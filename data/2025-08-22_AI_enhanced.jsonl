{"id": "2508.15043", "pdf": "https://arxiv.org/pdf/2508.15043.pdf", "abs": "https://arxiv.org/abs/2508.15043", "title": "LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking", "authors": ["Haoyang Yang", "Elliott H. Faa", "Weijian Liu", "Shunan Guo", "Duen Horng Chau", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 10 figures, Accepted to IEEE ISMAR 2025 (TVCG)", "summary": "Exploring and comprehending relevant academic literature is a vital yet\nchallenging task for researchers, especially given the rapid expansion in\nresearch publications. This task fundamentally involves sensemaking -\ninterpreting complex, scattered information sources to build understanding.\nWhile emerging immersive analytics tools have shown cognitive benefits like\nenhanced spatial memory and reduced mental load, they predominantly focus on\ninformation synthesis (e.g., organizing known documents). In contrast, the\nequally important information foraging phase - discovering and gathering\nrelevant literature - remains underexplored within immersive environments,\nhindering a complete sensemaking workflow. To bridge this gap, we introduce\nLitForager, an interactive literature exploration tool designed to facilitate\ninformation foraging of research literature within an immersive sensemaking\nworkflow using network-based visualizations and multimodal interactions.\nDeveloped with WebXR and informed by a formative study with researchers,\nLitForager supports exploration guidance, spatial organization, and seamless\ntransition through a 3D literature network. An observational user study with 15\nresearchers demonstrated LitForager's effectiveness in supporting fluid\nforaging strategies and spatial sensemaking through its multimodal interface.", "AI": {"tldr": "Introduction of LitForager, an interactive tool for literature exploration in immersive environments.", "motivation": "The rapid increase in academic publications makes it crucial for researchers to efficiently explore and comprehend literature, particularly in the information foraging phase, which is less addressed compared to information synthesis.", "method": "LitForager was developed using WebXR and built upon insights from a formative study with researchers. It employs network-based visualizations and multimodal interactions to facilitate the exploration of research literature.", "result": "An observational user study with 15 researchers showed that LitForager effectively supported fluid foraging strategies and spatial sensemaking, enhancing the literature exploration process.", "conclusion": "LitForager addresses the critical gap in information foraging within immersive analytics, promoting enhanced sensemaking through its interactive features.", "key_contributions": ["Introduction of a novel tool for immersive literature exploration", "Support for information foraging in academic research", "Demonstration of effective spatial sensemaking through user studies"], "limitations": "", "keywords": ["immersive analytics", "information foraging", "literature exploration", "WebXR", "spatial sensemaking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.15045", "pdf": "https://arxiv.org/pdf/2508.15045.pdf", "abs": "https://arxiv.org/abs/2508.15045", "title": "Understanding Accessibility Needs of Blind Authors on CMS-Based Websites", "authors": ["Guillermo Vera-Amaro", "José Rafael Rojano-Cáceres"], "categories": ["cs.HC", "H.5.2; H.5.4"], "comment": "15 pages, 5 figures, presented in Ibero-American Conference on\n  Human-Computer Interaction 2025", "summary": "This paper addresses the limited attention given to blind users as content\ncreators in Content Management Systems (CMS), a gap that remains under-explored\nin web accessibility research. For blind authors, effective interaction with\nCMS platforms requires more than technical compliance; it demands interfaces\ndesigned with semantic clarity, predictable navigation, and meaningful feedback\nfor screen reader users. This study investigates the accessibility barriers\nblind users face when performing key tasks, such as page creation, menu\nediting, and image publishing, using CMS platforms. A two-fold evaluation was\nconducted using automated tools and manual usability testing with three blind\nand one sighted participant, complemented by expert analysis based on the\nBarrier Walkthrough method. Results showed that block-based interfaces were\nparticularly challenging, often marked as accessible by automated tools but\nresulting in critical usability issues during manual evaluation. The use of a\ntext-based editor, the integration of AI-generated image descriptions, and\ntraining aligned with screen reader workflows, significantly improved usability\nand autonomy. These findings underscore the limitations of automated\nassessments and highlight the importance of user-centered design practices.\nEnhancing CMS accessibility requires consistent navigation structures, reduced\nreliance on visual interaction patterns, and the integration of AI tools that\nsupport blind content authors throughout the content creation process.", "AI": {"tldr": "This paper explores accessibility challenges for blind users in Content Management Systems (CMS), emphasizing user-centered design and the integration of AI tools to enhance usability.", "motivation": "To address the limited attention given to blind users as content creators in CMS and improve web accessibility research in this area.", "method": "A two-fold evaluation involving automated tools and manual usability testing with blind and sighted participants, supplemented by expert analysis using the Barrier Walkthrough method.", "result": "Challenges primarily stem from block-based interfaces that are falsely marked as accessible, leading to usability issues; improvements were noted with text-based editors and AI tools for image descriptions.", "conclusion": "The study highlights the need for consistent navigation structures and user-centered design practices to enhance CMS accessibility for blind authors.", "key_contributions": ["Identification of CMS accessibility barriers for blind users.", "Evidence of effectiveness of AI-generated image descriptions.", "Recommendations for improving CMS design for better usability."], "limitations": "The study involved a limited number of participants and may not be generalizable to all CMS platforms.", "keywords": ["Accessibility", "Content Management Systems", "Blind Users", "Human-Computer Interaction", "AI Tools"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15146", "pdf": "https://arxiv.org/pdf/2508.15146.pdf", "abs": "https://arxiv.org/abs/2508.15146", "title": "QueryGenie: Making LLM-Based Database Querying Transparent and Controllable", "authors": ["Longfei Chen", "Shenghan Gao", "Shiwei Wang", "Ken Lin", "Yun Wang", "Quan Li"], "categories": ["cs.HC"], "comment": "Accepted by The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST Adjunct '25), September 28-October 1, 2025, Busan,\n  Republic of Korea", "summary": "Conversational user interfaces powered by large language models (LLMs) have\nsignificantly lowered the technical barriers to database querying. However,\nexisting tools still encounter several challenges, such as misinterpretation of\nuser intent, generation of hallucinated content, and the absence of effective\nmechanisms for human feedback-all of which undermine their reliability and\npractical utility. To address these issues and promote a more transparent and\ncontrollable querying experience, we proposed QueryGenie, an interactive system\nthat enables users to monitor, understand, and guide the LLM-driven query\ngeneration process. Through incremental reasoning, real-time validation, and\nresponsive interaction mechanisms, users can iteratively refine query logic and\nensure alignment with their intent.", "AI": {"tldr": "QueryGenie is an interactive system designed to enhance user interaction with LLM-driven query generation by improving transparency and control.", "motivation": "To address challenges in conversational user interfaces powered by LLMs, such as misinterpretation of user intent and hallucinated content.", "method": "The system allows users to monitor and guide the query generation process through incremental reasoning and real-time validation.", "result": "QueryGenie enables users to iteratively refine query logic, aligning it with their intent more effectively.", "conclusion": "The proposed system aims to promote a more reliable and practical utility for LLM-driven queries.", "key_contributions": ["Development of the QueryGenie interactive system", "Mechanisms for real-time validation and user guidance", "Incremental reasoning to enhance understanding of query generation"], "limitations": "", "keywords": ["Conversational interfaces", "Large language models", "User interaction", "Query generation", "Human feedback"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.15148", "pdf": "https://arxiv.org/pdf/2508.15148.pdf", "abs": "https://arxiv.org/abs/2508.15148", "title": "ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews", "authors": ["Yuansong Xu", "Shuhao Zhang", "Yijie Fan", "Shaohan Shi", "Zhenhui Peng", "Quan Li"], "categories": ["cs.HC"], "comment": "Appear in Proc. ACM Hum.-Comput. Interact., Vol. 9, No. 7, Article\n  CSCW321. Publication date: November 2025", "summary": "Effectively assimilating and integrating reviewer feedback is crucial for\nresearchers seeking to refine their papers and handle potential rebuttal phases\nin academic venues. However, traditional review digestion processes present\nchallenges such as time consumption, reading fatigue, and the requisite for\ncomprehensive analytical skills. Prior research on review analysis often\nprovides theoretical guidance with limited targeted support. Additionally,\ngeneral text comprehension tools overlook the intricate nature of\ncomprehensively understanding reviews and lack contextual assistance. To bridge\nthis gap, we formulated research questions to explore the authors' concerns and\nmethods for enhancing comprehension during the review digestion phase. Through\ninterviews and the creation of storyboards, we developed ReviseMate, an\ninteractive system designed to address the identified challenges. A controlled\nuser study (N=31) demonstrated the superiority of ReviseMate over baseline\nmethods, with positive feedback regarding user interaction. Subsequent field\ndeployment (N=6) further validated the effectiveness of ReviseMate in\nreal-world review digestion scenarios. These findings underscore the potential\nof interactive tools to significantly enhance the assimilation and integration\nof reviewer feedback during the manuscript review process.", "AI": {"tldr": "This paper presents ReviseMate, an interactive system designed to improve how researchers assimilate and integrate reviewer feedback.", "motivation": "The study addresses challenges researchers face in processing reviewer feedback, including time consumption and the need for analytical skills.", "method": "The authors conducted interviews to identify authors' concerns and developed storyboards that led to the creation of ReviseMate. A controlled study with 31 users was performed, followed by field deployment with 6 users.", "result": "Users found ReviseMate superior to traditional methods, with positive interaction feedback indicating improved comprehension during review digestion.", "conclusion": "ReviseMate significantly enhances the assimilation of reviewer feedback, suggesting a strong potential for interactive tools in the academic review process.", "key_contributions": ["Introduction of ReviseMate as a novel interactive tool for reviewing feedback", "Empirical testing demonstrating the effectiveness of ReviseMate", "Insights from interviews guiding the design of the tool"], "limitations": "The study's user tests involved a limited number of participants, potentially affecting generalizability.", "keywords": ["human-computer interaction", "review feedback", "interactive tools", "user study", "academic writing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.14904", "pdf": "https://arxiv.org/pdf/2508.14904.pdf", "abs": "https://arxiv.org/abs/2508.14904", "title": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training", "authors": ["Jianfeng Si", "Lin Sun", "Zhewen Tan", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages,5 figures,4 tables", "summary": "Current methods for content safety in Large Language Models (LLMs), such as\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback\n(RLHF), often rely on multi-stage training pipelines and lack fine-grained,\npost-deployment controllability. To address these limitations, we propose a\nunified co-training framework that efficiently integrates multiple safety\nbehaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and\nrejective (refusal-oriented/conservative) within a single SFT stage. Notably,\neach behavior is dynamically activated via a simple system-level instruction,\nor magic token, enabling stealthy and efficient behavioral switching at\ninference time. This flexibility supports diverse deployment scenarios, such as\npositive for safe user interaction, negative for internal red-teaming, and\nrejective for context-aware refusals triggered by upstream moderation signals.\nThis co-training strategy induces a distinct Safety Alignment Margin in the\noutput space, characterized by well-separated response distributions\ncorresponding to each safety mode. The existence of this margin provides\nempirical evidence for the model's safety robustness and enables unprecedented\nfine-grained control. Experiments show that our method matches the safety\nalignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1\n(671B) in safety performance, while significantly reducing both training\ncomplexity and deployment costs. This work presents a scalable, efficient, and\nhighly controllable solution for LLM content safety.", "AI": {"tldr": "A unified co-training framework for content safety in LLMs that integrates multiple safety behaviors in a single SFT stage, enhancing efficiency and control.", "motivation": "Current methods lack fine-grained, controllable safety behavior in LLMs and often require complex training pipelines.", "method": "A co-training framework that activates safety behaviors via a simple instruction or 'magic token' that allows behavioral switching at inference time.", "result": "The method matches the safety alignment quality of SFT+DPO, with improved safety performance and reduced training complexity compared to existing models.", "conclusion": "The proposed framework offers a scalable and efficient approach to ensure content safety in LLMs while significantly lowering deployment costs.", "key_contributions": ["Unified co-training framework for content safety", "Dynamic activation of safety behaviors using a magic token", "Improved safety performance with reduced training complexity"], "limitations": "", "keywords": ["Large Language Models", "Content Safety", "Co-training Framework", "Safety Behaviors", "Fine-tuning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.15152", "pdf": "https://arxiv.org/pdf/2508.15152.pdf", "abs": "https://arxiv.org/abs/2508.15152", "title": "Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference", "authors": ["Matthew Brehmer", "Ginger Gloystein", "Bailiang Zhou", "Abby Gray", "Sruthi Pillai", "Ben Medina", "Vidya Setlur"], "categories": ["cs.HC"], "comment": "To appear at the Human Factors in Immersive Analytics (HFIA) Workshop\n  at IEEE VIS 2025", "summary": "We reflect on an evaluation of an immersive analytics application (Tableau\nfor visionOS) conducted at a large enterprise business intelligence (BI)\nconference. Conducting a study in such a context offered an opportunistic\nsetting to gather diverse feedback. However, this setting also highlighted the\nchallenge of evaluating usability while also assessing potential utility, as\nfeedback straddled between the novelty of the experience and the practicality\nof the application in participants' analytical workflows. This formative\nevaluation with 22 participants allowed us to gather insights with respect to\nthe usability of Tableau for visionOS, along with broader perspectives on the\npotential for head-mounted displays (HMDs) to promote new ways to engage with\nBI data. Our experience suggests a need for new evaluation considerations that\nintegrate qualitative and quantitative measures and account for unique\ninteraction patterns with 3D representations and interfaces accessible via an\nHMD. Overall, we contribute an enterprise perspective on evaluation\nmethodologies for immersive analytics.", "AI": {"tldr": "Evaluation of an immersive analytics application (Tableau for visionOS) reveals challenges in assessing usability and utility in an enterprise context.", "motivation": "To gather diverse feedback on usability and utility of a new immersive analytics application at a Business Intelligence conference.", "method": "Formative evaluation with 22 participants assessing usability and feedback on head-mounted displays for business intelligence applications.", "result": "Insights on usability of Tableau for visionOS and the potential of HMDs for engaging with BI data were collected, highlighting the need for new evaluation methods.", "conclusion": "The study suggests integrating qualitative and quantitative evaluation measures for immersive analytics in enterprise settings.", "key_contributions": ["Insights on usability challenges specific to immersive analytics applications.", "Recommendations for new evaluation methodologies that consider unique interaction patterns with 3D interfaces.", "Enterprise perspective on the use of head-mounted displays in analytical workflows."], "limitations": "Study conducted at a specific conference may limit the generalizability of findings.", "keywords": ["immersive analytics", "usability", "head-mounted displays", "business intelligence", "evaluation methodologies"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.14909", "pdf": "https://arxiv.org/pdf/2508.14909.pdf", "abs": "https://arxiv.org/abs/2508.14909", "title": "Preliminary Ranking of WMT25 General Machine Translation Systems", "authors": ["Tom Kocmi", "Eleftherios Avramidis", "Rachel Bawden", "Ondřej Bojar", "Konstantin Dranch", "Anton Dvorkovich", "Sergey Dukanov", "Natalia Fedorova", "Mark Fishel", "Markus Freitag", "Thamme Gowda", "Roman Grundkiewicz", "Barry Haddow", "Marzena Karpinska", "Philipp Koehn", "Howard Lakougna", "Jessica Lundin", "Kenton Murray", "Masaaki Nagata", "Stefano Perrella", "Lorenzo Proietti", "Martin Popel", "Maja Popović", "Parker Riley", "Mariya Shmatova", "Steinþór Steingrímsson", "Lisa Yankovskaya", "Vilém Zouhar"], "categories": ["cs.CL"], "comment": null, "summary": "We present the preliminary ranking of the WMT25 General Machine Translation\nShared Task, in which MT systems have been evaluated using automatic metrics.\nAs this ranking is based on automatic evaluations, it may be biased in favor of\nsystems that employ re-ranking techniques, such as Quality Estimation\nre-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be\nbased on human evaluation, which is more reliable and will supersede the\nautomatic ranking.\n  The purpose of this report is not to present the final findings of the\nGeneral MT task, but rather to share preliminary results with task\nparticipants, which may be useful when preparing their system submission\npapers.", "AI": {"tldr": "Preliminary ranking of WMT25 General Machine Translation Shared Task based on automatic metrics.", "motivation": "To evaluate MT systems using automatic metrics ahead of the official ranking based on human evaluation.", "method": "Preliminary evaluations were conducted using automatic metrics to rank various machine translation systems.", "result": "The preliminary rankings indicate potential biases towards systems using re-ranking techniques.", "conclusion": "The official WMT25 ranking will rely on human evaluation, considered more reliable than automatic metrics.", "key_contributions": ["Preliminary ranking of MT systems for WMT25 using automatic metrics", "Identification of potential biases in automatic evaluation methods", "Aiding participants in preparing submissions based on initial results"], "limitations": "The automatic ranking may favor systems using advanced techniques, not reflecting true performance.", "keywords": ["Machine Translation", "Automatic Metrics", "WMT25"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.15227", "pdf": "https://arxiv.org/pdf/2508.15227.pdf", "abs": "https://arxiv.org/abs/2508.15227", "title": "GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design", "authors": ["Wen-Fan Wang", "Ting-Ying Lee", "Chien-Ting Lu", "Che-Wei Hsu", "Nil Ponsa Campany", "Yu Chen", "Mike Y. Chen", "Bing-Yu Chen"], "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "Accepted ACM Symposium on User Interface Software and Technology\n  (UIST '25)", "summary": "Environment designers in the entertainment industry create imaginative 2D and\n3D scenes for games, films, and television, requiring both fine-grained control\nof specific details and consistent global coherence. Designers have\nincreasingly integrated generative AI into their workflows, often relying on\nlarge language models (LLMs) to expand user prompts for text-to-image\ngeneration, then iteratively refining those prompts and applying inpainting.\nHowever, our formative study with 10 designers surfaced two key challenges: (1)\nthe lengthy LLM-generated prompts make it difficult to understand and isolate\nthe keywords that must be revised for specific visual elements; and (2) while\ninpainting supports localized edits, it can struggle with global consistency\nand correctness. Based on these insights, we present GenTune, an approach that\nenhances human--AI collaboration by clarifying how AI-generated prompts map to\nimage content. Our GenTune system lets designers select any element in a\ngenerated image, trace it back to the corresponding prompt labels, and revise\nthose labels to guide precise yet globally consistent image refinement. In a\nsummative study with 20 designers, GenTune significantly improved prompt--image\ncomprehension, refinement quality, and efficiency, and overall satisfaction\n(all $p < .01$) compared to current practice. A follow-up field study with two\nstudios further demonstrated its effectiveness in real-world settings.", "AI": {"tldr": "GenTune enhances human-AI collaboration in environment design by clarifying AI-generated prompts for image content, improving comprehension and refinement efficiency among designers.", "motivation": "Environment designers face challenges with lengthy LLM-generated prompts and inpainting, hindering their ability to create coherent images.", "method": "GenTune clarifies the mapping between AI-generated prompts and specific image content, allowing designers to revise prompt labels for targeted image refinement.", "result": "In studies with designers, GenTune significantly improved prompt-image comprehension, refinement quality, efficiency, and overall satisfaction (all p < .01).", "conclusion": "The system demonstrated effective real-world application in environment design studios, enhancing collaborative design processes.", "key_contributions": ["Introduces GenTune for better understanding of AI-generated prompts", "Improves designers' ability to refine images with global consistency", "Demonstrates effectiveness through user studies and real-world application"], "limitations": "The study's scope was limited to specific design contexts and a small number of designers.", "keywords": ["human-computer interaction", "generative AI", "environment design", "LLM", "image refinement"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.14913", "pdf": "https://arxiv.org/pdf/2508.14913.pdf", "abs": "https://arxiv.org/abs/2508.14913", "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages", "authors": ["Israel Abebe Azime", "Tadesse Destaw Belay", "Dietrich Klakow", "Philipp Slusallek", "Anshuman Chhabra"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages.", "AI": {"tldr": "The paper presents a framework for cultural localization of math word problems in low-resource languages using LLMs to create datasets with native entities, addressing biases in existing multilingual benchmarks.", "motivation": "To address the gap in multilingual and culturally-grounded mathematical reasoning in low-resource languages, which usually rely on English-centric datasets due to the scarcity of localized data.", "method": "Developing a framework that utilizes LLMs to automatically construct datasets with culturally relevant names, organizations, and currencies for math word problems.", "result": "The experimental results demonstrate that the proposed framework reduces English-centric entity bias and enhances multilingual math abilities by introducing native entities.", "conclusion": "The framework significantly aids in producing culturally accurate datasets, improving the robustness of LLMs in handling multilingual mathematical problems.", "key_contributions": ["Introduction of a cultural localization framework for math word problems.", "Demonstration of improved benchmark performance through the use of native entities.", "Reduction of English-centric biases in multilingual datasets."], "limitations": "The approach may still depend on the quality of existing sources for native entities and may not cover all low-resource languages equally.", "keywords": ["large language models", "multilingual reasoning", "cultural localization", "math word problems", "low-resource languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.15249", "pdf": "https://arxiv.org/pdf/2508.15249.pdf", "abs": "https://arxiv.org/abs/2508.15249", "title": "Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios", "authors": ["Alaul Islam", "Fairouz Grioui", "Raimund Dachselt", "Petra Isenberg"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures", "summary": "We present the results of an in-situ ideation workshop for designing data\nvisualizations on smart wristbands that can show data around the entire wrist\nof a wearer. Wristbands pose interesting challenges because the visibility of\ndifferent areas of the band depends on the wearer's arm posture. We focused on\nfour usage scenarios that lead to different postures: office work, leisurely\nwalks, cycling, and driving. As the technology for smart wristbands is not yet\ncommercially available, we conducted a paper-based ideation exercise that\nshowed how spatial layout and visualization design on smart wristbands may need\nto vary depending on the types of data items of interest and arm postures.\nParticipants expressed a strong preference for responsive visualization designs\nthat could adapt to the movement of wearers' arms. Supplemental material from\nthe study is available here: https://osf.io/4hrca/.", "AI": {"tldr": "This paper reports on an ideation workshop aimed at designing responsive data visualizations for smart wristbands, considering user arm postures during various activities.", "motivation": "The motivation behind this research is to understand how the spatial layout and visualization design of data on smart wristbands can be optimized for different arm postures during various activities.", "method": "The authors conducted a paper-based ideation workshop where participants explored how data visualizations should vary based on user activity scenarios (office work, leisurely walks, cycling, and driving).", "result": "The findings indicated that participants preferred responsive visualization designs that could adapt to movements, highlighting the importance of context in data presentation.", "conclusion": "Overall, the study emphasizes the need for adaptive design in data visualizations on wearable technology to provide effective user experiences across different contexts.", "key_contributions": ["Identified key usage scenarios impacting visualization design on wristbands", "Emphasized user preference for responsive and adaptive visualizations", "Provided insights into nuanced design considerations based on arm posture"], "limitations": "The technology is not yet commercially available, and the study was conducted as a paper-based ideation exercise, which may not fully capture real-world usability.", "keywords": ["smart wristbands", "data visualization", "human-computer interaction", "responsive design", "wearable technology"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.14951", "pdf": "https://arxiv.org/pdf/2508.14951.pdf", "abs": "https://arxiv.org/abs/2508.14951", "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data", "authors": ["Dario Vajda", "Domen Vreš", "Marko Robnik-Šikonja"], "categories": ["cs.CL"], "comment": "Paper with individual presentation at LUHME workshop at ECAI 2025", "summary": "Large language models have emerged as effective machine translation systems.\nIn this paper, we explore how a general instruction-tuned large language model\ncan be improved for machine translation using relatively few easily produced\ndata resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct\nmodel using Direct Preference Optimization (DPO) training on a programmatically\ncurated and enhanced subset of a public dataset. As DPO requires pairs of\nquality-ranked instances, we generated its training dataset by translating\nEnglish Wikipedia articles using two LLMs, GaMS-9B-Instruct and\nEuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics\ncoupled with automatic evaluation metrics such as COMET. The evaluation shows\nthat our fine-tuned model outperforms both models involved in the dataset\ngeneration. In comparison to the baseline models, the fine-tuned model achieved\na COMET score gain of around 0.04 and 0.02, respectively, on translating\nWikipedia articles. It also more consistently avoids language and formatting\nerrors.", "AI": {"tldr": "This paper discusses improvements to a general instruction-tuned large language model for machine translation, specifically using Slovene and employing Direct Preference Optimization (DPO) on a curated dataset.", "motivation": "To enhance machine translation using instruction-tuned LLMs with minimal data resources.", "method": "The authors conducted DPO training on a subset of a public dataset, generating a training dataset through translation and ranking of English Wikipedia articles with two different LLMs.", "result": "The fine-tuned model showed a COMET score improvement of approximately 0.04 and 0.02 over the baseline models, along with a reduction in language and formatting errors.", "conclusion": "Fine-tuning the instruction-tuned model with DPO leads to significant improvements in translation quality and consistency.", "key_contributions": ["Improved machine translation performance using a minimal data approach.", "Utilization of Direct Preference Optimization for training LLMs.", "Demonstrated application focusing on Slovene language translation."], "limitations": "", "keywords": ["Machine Translation", "Direct Preference Optimization", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.15258", "pdf": "https://arxiv.org/pdf/2508.15258.pdf", "abs": "https://arxiv.org/abs/2508.15258", "title": "Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback", "authors": ["Dooyoung Kim", "Woontack Woo"], "categories": ["cs.HC"], "comment": "4 pages, 2 figures, Accepted in the IEEE ISMAR 2025 XRStand Workshop", "summary": "We propose the Spatio-Temporal Mixed and Augmented Reality Experience\nDescription (MAR-ED), a novel framework to standardize the representation of\npast events for interactive and adaptive playback in a user's present physical\nspace. While current spatial media technologies have primarily focused on\ncapturing or replaying content as static assets, often disconnected from the\nviewer's environment or offering limited interactivity, the means to describe\nan experience's underlying semantic and interactive structure remains\nunderexplored. We propose a descriptive framework called MAR-ED based on three\ncore primitives: 1) Event Primitives for semantic scene graph representation,\n2) Keyframe Primitives for efficient and meaningful data access, and 3)\nPlayback Primitives for user-driven adaptive interactive playback of recorded\nMAR experience. The proposed flowchart of the three-stage process of the\nproposed MAR-ED framework transforms a recorded experience into a unique\nadaptive MAR experience during playback, where its spatio-temporal structure\ndynamically conforms to a new environment and its narrative can be altered by\nlive user input. Drawing on this framework, personal digital memories and\nrecorded events can evolve beyond passive 2D/3D videos into immersive,\nspatially-integrated group experiences, opening new paradigms for training,\ncultural heritage, and interactive storytelling without requiring complex,\nper-user adaptive rendering.", "AI": {"tldr": "This paper introduces the Spatio-Temporal Mixed and Augmented Reality Experience Description (MAR-ED) framework, which enhances the capture and playback of events in augmented reality by enabling user-driven interactivity and dynamic adaptation to environments.", "motivation": "The need for a standardized method to represent past events in interactive augmented reality that enhances the existing capabilities of spatial media technologies.", "method": "The MAR-ED framework utilizes three core primitives: Event Primitives for semantic scene graph representation, Keyframe Primitives for data access, and Playback Primitives for adaptive user-driven playback.", "result": "The MAR-ED framework allows recorded events to evolve into immersive experiences that adapt to the viewer's environment and narrative, enhancing engagement and interactivity.", "conclusion": "MAR-ED facilitates the transformation of static recordings into dynamic, interactive experiences that can be applied to various fields like training, cultural heritage, and storytelling.", "key_contributions": ["Introduction of Event, Keyframe, and Playback Primitives for augmented reality experiences.", "A three-stage process for creating adaptive MAR experiences.", "Potential applications in training, cultural heritage, and interactive storytelling."], "limitations": "No specific limitations discussed in the abstract.", "keywords": ["Mixed Reality", "Augmented Reality", "Interactive Playback", "User Experience", "Event Representation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.14982", "pdf": "https://arxiv.org/pdf/2508.14982.pdf", "abs": "https://arxiv.org/abs/2508.14982", "title": "Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems", "authors": ["Qianli Wang", "Tatiana Anikina", "Nils Feldhus", "Simon Ostermann", "Fedor Splitt", "Jiaao Li", "Yoana Tsoneva", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Findings, camera-ready version", "summary": "Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered considerable attention for their\nability to enhance user comprehension through dialogue-based explanations.\nCurrent ConvXAI systems often are based on intent recognition to accurately\nidentify the user's desired intention and map it to an explainability method.\nWhile such methods offer great precision and reliability in discerning users'\nunderlying intentions for English, a significant challenge in the scarcity of\ntraining data persists, which impedes multilingual generalization. Besides, the\nsupport for free-form custom inputs, which are user-defined data distinct from\npre-configured dataset instances, remains largely limited. To bridge these\ngaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL\ndataset spanning five typologically diverse languages, including one\nlow-resource language. Subsequently, we propose a new parsing approach aimed at\nenhancing multilingual parsing performance, and evaluate three LLMs on\nMultiCoXQL using various parsing strategies. Furthermore, we present Compass, a\nnew multilingual dataset designed for custom input extraction in ConvXAI\nsystems, encompassing 11 intents across the same five languages as MultiCoXQL.\nWe conduct monolingual, cross-lingual, and multilingual evaluations on Compass,\nemploying three LLMs of varying sizes alongside BERT-type models.", "AI": {"tldr": "This paper introduces multilingual datasets and parsing approaches to improve conversational explainable AI systems using large language models.", "motivation": "To address challenges in multilingual generalization and limited support for free-form custom inputs in current Conversational Explainable AI systems.", "method": "The authors developed MultiCoXQL, a multilingual extension of CoXQL, and Compass, a dataset for custom input extraction. They evaluated multiple LLMs using different parsing strategies across these datasets.", "result": "The evaluation demonstrated improved parsing performance for multilingual ConvXAI systems, showcasing the effectiveness of the proposed datasets and parsing approach.", "conclusion": "The introduction of MultiCoXQL and Compass aids in enhancing the adaptability of ConvXAI systems across multiple languages and input types, contributing to better user comprehension.", "key_contributions": ["Introduced MultiCoXQL, a multilingual extension of the CoXQL dataset encompassing five languages.", "Developed Compass, a dataset focused on custom input extraction with 11 intents in five languages.", "Proposed a new parsing approach that enhances multilingual parsing performance."], "limitations": "The study may require further testing across more diverse languages and user intents to fully assess generalization capabilities.", "keywords": ["Conversational Explainable AI", "Multilingual Datasets", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15716", "pdf": "https://arxiv.org/pdf/2508.15716.pdf", "abs": "https://arxiv.org/abs/2508.15716", "title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey", "authors": ["Hongqi Li", "Yitong Chen", "Yujuan Wang", "Weihang Ni", "Haodong Zhang"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE Journals", "summary": "Electroencephalography (EEG) analysis stands at the forefront of neuroscience\nand artificial intelligence research, where foundation models are reshaping the\ntraditional EEG analysis paradigm by leveraging their powerful representational\ncapacity and cross-modal generalization. However, the rapid proliferation of\nthese techniques has led to a fragmented research landscape, characterized by\ndiverse model roles, inconsistent architectures, and a lack of systematic\ncategorization. To bridge this gap, this study presents the first comprehensive\nmodality-oriented taxonomy for foundation models in EEG analysis,\nsystematically organizing research advances based on output modalities of the\nnative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal\nframeworks. We rigorously analyze each category's research ideas, theoretical\nfoundations, and architectural innovations, while highlighting open challenges\nsuch as model interpretability, cross-domain generalization, and real-world\napplicability in EEG-based systems. By unifying this dispersed field, our work\nnot only provides a reference framework for future methodology development but\naccelerates the translation of EEG foundation models into scalable,\ninterpretable, and online actionable solutions.", "AI": {"tldr": "The paper introduces a comprehensive taxonomy for foundation models in EEG analysis, addressing the fragmentation in research by categorizing advancements based on output modalities while highlighting challenges in model interpretability and real-world applicability.", "motivation": "To address the fragmented research landscape in EEG analysis by providing a systematic categorization of foundation models.", "method": "The study presents a modality-oriented taxonomy for EEG analysis, classifying advancements based on output modalities and analyzing research ideas, theoretical foundations, and architectural innovations.", "result": "The paper organizes EEG analysis advancements into categories like EEG decoding, EEG-text, EEG-vision, EEG-audio, and multimodal frameworks, while also identifying challenges such as interpretability and generalization.", "conclusion": "This work serves as a reference framework for future developments in EEG models and aims to enhance their scalability and applicability in real-world scenarios.", "key_contributions": ["First comprehensive taxonomy for foundation models in EEG analysis", "Systematic organization of research advances based on output modalities", "Identification of open challenges in EEG-based systems"], "limitations": "The paper does not provide solutions to the identified challenges but highlights them for future research.", "keywords": ["EEG analysis", "foundation models", "taxonomy", "multimodal frameworks", "interpretability"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.15044", "pdf": "https://arxiv.org/pdf/2508.15044.pdf", "abs": "https://arxiv.org/abs/2508.15044", "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "authors": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.", "AI": {"tldr": "The paper introduces the reward-Shifted Speculative Sampling (SSS) algorithm to align large language models with human preferences at inference time, reducing costs while improving performance.", "motivation": "Aligning large language models (LLMs) with human preferences is crucial, but current test-time alignment techniques incur substantial inference costs, posing practical limitations.", "method": "The reward-Shifted Speculative Sampling (SSS) algorithm uses a small draft model aligned with human preferences to predict future tokens, which helps exploit the distributional shift between draft and target models to enhance low-cost alignment without changing the target model.", "result": "The SSS algorithm demonstrates superior gold reward scores with significantly reduced inference costs during test-time weak-to-strong alignment experiments.", "conclusion": "SSS effectively aligns LLMs with human preferences while minimizing inference costs, making it a promising approach for practical applications of LLM alignment.", "key_contributions": ["Introduced the reward-Shifted Speculative Sampling (SSS) algorithm for efficient LLM alignment.", "Theoretical demonstration of exploiting the distributional shift between aligned and unaligned models.", "Validation of improved alignment outcomes at lower inference costs."], "limitations": "", "keywords": ["large language models", "human preference alignment", "speculative sampling", "inference cost", "test-time alignment"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.15727", "pdf": "https://arxiv.org/pdf/2508.15727.pdf", "abs": "https://arxiv.org/abs/2508.15727", "title": "Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI", "authors": ["Hannah Selder", "Florian Fischer", "Per Ola Kristensson", "Arthur Fleig"], "categories": ["cs.HC", "H.5.2; F.m"], "comment": "17 pages, 14 figures, 1 table, ACM UIST 2025", "summary": "Designing effective reward functions is critical for reinforcement\nlearning-based biomechanical simulations, yet HCI researchers and practitioners\noften waste (computation) time with unintuitive trial-and-error tuning. This\npaper demystifies reward function design by systematically analyzing the impact\nof effort minimization, task completion bonuses, and target proximity\nincentives on typical HCI tasks such as pointing, tracking, and choice\nreaction. We show that proximity incentives are essential for guiding movement,\nwhile completion bonuses ensure task success. Effort terms, though optional,\nhelp refine motion regularity when appropriately scaled. We perform an\nextensive analysis of how sensitive task success and completion time depend on\nthe weights of these three reward components. From these results we derive\npractical guidelines to create plausible biomechanical simulations without the\nneed for reinforcement learning expertise, which we then validate on remote\ncontrol and keyboard typing tasks. This paper advances simulation-based\ninteraction design and evaluation in HCI by improving the efficiency and\napplicability of biomechanical user modeling for real-world interface\ndevelopment.", "AI": {"tldr": "This paper analyzes the design of reward functions for reinforcement learning in biomechanical simulations and provides guidelines to improve efficiency in HCI task performance.", "motivation": "To optimize the reward function design process in reinforcement learning for HCI applications, which is often hindered by intuition-based approaches.", "method": "The paper conducts a systematic analysis of reward function components (effort minimization, task completion bonuses, and target proximity incentives) across various HCI tasks.", "result": "Proximity incentives are critical for guiding user movement, completion bonuses enhance task success, and effort minimization terms can refine motion regularity when properly scaled.", "conclusion": "The paper offers practical guidelines for designing reward functions to create effective biomechanical simulations without requiring extensive knowledge of reinforcement learning.", "key_contributions": ["Systematic analysis of reward function components in HCI tasks.", "Practical guidelines for designing reward functions in biomechanical simulations.", "Validation of guidelines on remote control and keyboard typing tasks."], "limitations": "The study focuses on a limited range of HCI tasks and might not generalize to all biomechanical simulations.", "keywords": ["reinforcement learning", "reward functions", "HCI", "biomechanical simulations", "user modeling"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2508.15085", "pdf": "https://arxiv.org/pdf/2508.15085.pdf", "abs": "https://arxiv.org/abs/2508.15085", "title": "LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text", "authors": ["MohamamdJavad Ardestani", "Ehsan Kamalloo", "Davood Rafiei"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "LongRecall. The completeness of machine-generated text, ensuring that it\ncaptures all relevant information, is crucial in domains such as medicine and\nlaw and in tasks like list-based question answering (QA), where omissions can\nhave serious consequences. However, existing recall metrics often depend on\nlexical overlap, leading to errors with unsubstantiated entities and\nparaphrased answers, while LLM-as-a-Judge methods with long holistic prompts\ncapture broader semantics but remain prone to misalignment and hallucinations\nwithout structured verification. We introduce LongRecall, a general three-stage\nrecall evaluation framework that decomposes answers into self-contained facts,\nsuccessively narrows plausible candidate matches through lexical and semantic\nfiltering, and verifies their alignment through structured entailment checks.\nThis design reduces false positives and false negatives while accommodating\ndiverse phrasings and contextual variations, serving as a foundational building\nblock for systematic recall assessment. We evaluate LongRecall on three\nchallenging long-form QA benchmarks using both human annotations and LLM-based\njudges, demonstrating substantial improvements in recall accuracy over strong\nlexical and LLM-as-a-Judge baselines.", "AI": {"tldr": "LongRecall is introduced as a three-stage recall evaluation framework designed to improve the completeness of machine-generated text in critical domains such as medicine and law.", "motivation": "Existing recall metrics fail to account for lexical variations and can lead to errors in high-stakes areas, necessitating a more robust method for assessing recall accuracy in machine-generated answers.", "method": "The LongRecall framework decomposes answers into self-contained facts and utilizes lexical and semantic filtering along with structured entailment checks to verify alignments.", "result": "Evaluation on long-form QA benchmarks shows LongRecall significantly outperforms traditional recall metrics and LLM-based approaches in recall accuracy.", "conclusion": "LongRecall provides a novel approach to systematically assess recall in machine-generated text, helping mitigate issues related to hallucinations and misalignments in AI-generated content.", "key_contributions": ["Introduces LongRecall, a three-stage recall evaluation framework", "Implements structured entailment checks to increase accuracy", "Demonstrates improvements over existing recall metrics on QA benchmarks"], "limitations": "The framework may still be susceptible to errors in extremely ambiguous contexts or highly complex queries where fact decomposition is non-trivial.", "keywords": ["recall evaluation", "machine-generated text", "long-form question answering"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.15752", "pdf": "https://arxiv.org/pdf/2508.15752.pdf", "abs": "https://arxiv.org/abs/2508.15752", "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries", "authors": ["Jon E. Froehlich", "Jared Hwang", "Zeyu Wang", "John S. O'Meara", "Xia Su", "William Huang", "Yang Zhang", "Alex Fiannaca", "Philip Nelson", "Shaun Kane"], "categories": ["cs.HC", "cs.AI", "cs.CV", "H.5; I.2"], "comment": "Accepted to the ICCV'25 Workshop \"Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities\"", "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.", "AI": {"tldr": "The paper presents the concept of Geo-Visual Agents, multimodal AI agents designed to address geo-visual inquiries by analyzing both geospatial images and traditional GIS data.", "motivation": "Interactive digital maps are limited by pre-existing structured data, which restricts their ability to effectively answer nuanced geo-visual questions regarding the world.", "method": "The authors propose a framework for Geo-Visual Agents that combines analysis of geospatial images (streetscapes, place-based photos, aerial imagery) with traditional GIS data.", "result": "The paper exemplifies the Geo-Visual Agents in three specific cases and highlights their potential to enhance interactions with geographic information.", "conclusion": "Geo-Visual Agents present an innovative solution to improve the understanding of complex spatial queries, leading to improved accessibility and interaction with geographic data.", "key_contributions": ["Introduction of Geo-Visual Agents that utilize multimodal data.", "Demonstration of applications through exemplars.", "Identification of future challenges and opportunities for research in geo-visual inquiries."], "limitations": "Limited to the exploration of data processing methodologies and challenges, without delving into the technical aspects of implementation.", "keywords": ["Geo-Visual Agents", "multimodal AI", "geospatial images", "GIS data", "accessibility"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.15090", "pdf": "https://arxiv.org/pdf/2508.15090.pdf", "abs": "https://arxiv.org/abs/2508.15090", "title": "Mapping the Course for Prompt-based Structured Prediction", "authors": ["Matt Pauk", "Maria Leonor Pacheco"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been shown to be useful for a variety of language tasks, without\nrequiring task-specific fine-tuning. However, these models often struggle with\nhallucinations and complex reasoning problems due to their autoregressive\nnature. We propose to address some of these issues, specifically in the area of\nstructured prediction, by combining LLMs with combinatorial inference in an\nattempt to marry the predictive power of LLMs with the structural consistency\nprovided by inference methods. We perform exhaustive experiments in an effort\nto understand which prompting strategies can effectively estimate LLM\nconfidence values for use with symbolic inference, and show that, regardless of\nthe prompting strategy, the addition of symbolic inference on top of prompting\nalone leads to more consistent and accurate predictions. Additionally, we show\nthat calibration and fine-tuning using structured prediction objectives leads\nto increased performance for challenging tasks, showing that structured\nlearning is still valuable in the era of LLMs.", "AI": {"tldr": "This paper proposes combining LLMs with combinatorial inference to improve outcomes in structured prediction tasks by leveraging symbolic inference for better accuracy and consistency.", "motivation": "To address issues in LLMs such as hallucinations and complex reasoning problems in language tasks without requiring fine-tuning.", "method": "The authors conducted exhaustive experiments to evaluate different prompting strategies and their effectiveness in estimating LLM confidence values for symbolic inference, combined with structured prediction objectives.", "result": "The findings indicate that adding symbolic inference improves consistency and accuracy of predictions compared to prompting alone. Additionally, calibrating and fine-tuning with structured prediction objectives enhances performance on complex tasks.", "conclusion": "Structured learning remains important for enhancing LLM performance, even in light of prevailing capabilities of LLMs.", "key_contributions": ["Proposed a novel combination of LLMs with combinatorial inference", "Demonstrated the effectiveness of symbolic inference on boosting accuracy", "Showed the value of structured learning in improving LLM performance"], "limitations": "", "keywords": ["Language Models", "Structured Prediction", "Symbolic Inference"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15096", "pdf": "https://arxiv.org/pdf/2508.15096.pdf", "abs": "https://arxiv.org/abs/2508.15096", "title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "authors": ["Rabeeh Karimi Mahabadi", "Sanjeev Satheesh", "Shrimai Prabhumoye", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Pretraining large language models (LLMs) on high-quality, structured data\nsuch as mathematics and code substantially enhances reasoning capabilities.\nHowever, existing math-focused datasets built from Common Crawl suffer from\ndegraded quality due to brittle extraction heuristics, lossy HTML-to-text\nconversion, and the failure to reliably preserve mathematical structure. In\nthis work, we introduce Nemotron-CC-Math, a large-scale, high-quality\nmathematical corpus constructed from Common Crawl using a novel,\ndomain-agnostic pipeline specifically designed for robust scientific text\nextraction.\n  Unlike previous efforts, our pipeline recovers math across various formats\n(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx\nand a targeted LLM-based cleaning stage. This approach preserves the structural\nintegrity of equations and code blocks while removing boilerplate,\nstandardizing notation into LaTeX representation, and correcting\ninconsistencies.\n  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+\n(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,\nNemotron-CC-Math-4+ not only surpasses all prior open math datasets-including\nMegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens\nthan FineMath-4+, which was previously the highest-quality math pretraining\ndataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to\n+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,\nwhile also improving general-domain performance on MMLU and MMLU-Stem.\n  We present the first pipeline to reliably extract scientific\ncontent--including math--from noisy web-scale data, yielding measurable gains\nin math, code, and general reasoning, and setting a new state of the art among\nopen math pretraining corpora. To support open-source efforts, we release our\ncode and datasets.", "AI": {"tldr": "Introducing Nemotron-CC-Math, a large-scale mathematical corpus that improves reasoning capabilities in large language models by addressing the challenges in previous datasets.", "motivation": "Existing math-focused datasets suffer from quality issues in extraction and structure preservation, necessitating a robust solution for effective pretraining of LLMs.", "method": "A novel, domain-agnostic pipeline was developed for scientific text extraction, leveraging layout-aware rendering and an LLM-based cleaning stage to recover mathematical content accurately.", "result": "Nemotron-CC-Math-4+ contains 52B tokens and surpasses existing datasets in quality, yielding significant performance gains when used to pretrain the Nemotron-T 8B model, especially in math and general-domain tasks.", "conclusion": "The presented pipeline sets a new state of the art for open math pretraining corpora and supports open-source initiatives by releasing the code and datasets.", "key_contributions": ["Development of the Nemotron-CC-Math corpus addressing previous dataset limitations", "First robust pipeline for extracting scientific content from noisy web-scale data", "Significant performance improvements for LLMs pretraining in reasoning capabilities"], "limitations": "", "keywords": ["large language models", "mathematics", "pretraining", "scientific text extraction", "open datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15139", "pdf": "https://arxiv.org/pdf/2508.15139.pdf", "abs": "https://arxiv.org/abs/2508.15139", "title": "Identifying and Answering Questions with False Assumptions: An Interpretable Approach", "authors": ["Zijie Wang", "Eduardo Blanco"], "categories": ["cs.CL"], "comment": "To appear at EMNLP 2025 Main conference", "summary": "People often ask questions with false assumptions, a type of question that\ndoes not have regular answers. Answering such questions require first\nidentifying the false assumptions. Large Language Models (LLMs) often generate\nmisleading answers because of hallucinations. In this paper, we focus on\nidentifying and answering questions with false assumptions in several domains.\nWe first investigate to reduce the problem to fact verification. Then, we\npresent an approach leveraging external evidence to mitigate hallucinations.\nExperiments with five LLMs demonstrate that (1) incorporating retrieved\nevidence is beneficial and (2) generating and validating atomic assumptions\nyields more improvements and provides an interpretable answer by specifying the\nfalse assumptions.", "AI": {"tldr": "This paper addresses the challenge of identifying and answering questions with false assumptions using Large Language Models (LLMs), proposing a method that mitigates hallucinations through fact verification and incorporating external evidence.", "motivation": "The need to answer questions that arise from false assumptions, which lead to misleading answers from LLMs.", "method": "The paper reduces the problem to fact verification and leverages external evidence to mitigate hallucinations, while also generating and validating atomic assumptions.", "result": "Experiments demonstrate that incorporating retrieved evidence is beneficial, and generating atomic assumptions provides more interpretable answers by highlighting false assumptions.", "conclusion": "The proposed approach improves the quality of responses from LLMs when dealing with questions that contain false assumptions.", "key_contributions": ["Reduces the problem of false assumptions to fact verification.", "Introduces external evidence to combat hallucinations in LLM outputs.", "Shows that generating atomic assumptions leads to improved and interpretable answers."], "limitations": "", "keywords": ["False Assumptions", "Large Language Models", "Fact Verification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15164", "pdf": "https://arxiv.org/pdf/2508.15164.pdf", "abs": "https://arxiv.org/abs/2508.15164", "title": "ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following", "authors": ["Seungmin Han", "Haeun Kwon", "Ji-jun Park", "Taeyang Yoon"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs), current models still face substantial\nchallenges in handling complex, multi-turn, and visually-grounded tasks that\ndemand deep reasoning, sustained contextual understanding, entity tracking, and\nmulti-step instruction following. Existing benchmarks often fall short in\ncapturing the dynamism and intricacies of real-world multi-modal interactions,\nleading to issues such as context loss and visual hallucinations. To address\nthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning\nBenchmark), a novel dataset comprising 300 meticulously designed complex\nmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated across\nsix core dimensions including visual entity tracking and reasoning depth.\nFurthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic\nframework that enhances existing LVLMs with advanced reasoning and instruction\nfollowing capabilities through an iterative\n\"memory-perception-planning-execution\" cycle, requiring no extensive\nre-training of the underlying models. Our extensive experiments on MMDR-Bench\ndemonstrate that CoLVLM Agent consistently achieves superior performance,\nattaining an average human evaluation score of 4.03, notably surpassing\nstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro\n(3.85). The framework exhibits significant advantages in reasoning depth,\ninstruction adherence, and error suppression, and maintains robust performance\nover extended dialogue turns, validating the effectiveness of its modular\ndesign and iterative approach for complex multi-modal interactions.", "AI": {"tldr": "Introducing MMDR-Bench, a new dataset for evaluating multi-modal dialogue reasoning, and CoLVLM Agent, a framework for enhancing Large Vision-Language Models with improved reasoning capabilities.", "motivation": "Current LLMs and LVLMs struggle with complex, multi-turn, visually-grounded tasks that require sustained understanding and reasoning, prompting the need for better evaluation benchmarks and models.", "method": "The paper introduces MMDR-Bench, comprising 300 complex dialogue scenarios, and proposes CoLVLM Agent, which enhances existing models using a memory-perception-planning-execution cycle without extensive retraining.", "result": "CoLVLM Agent significantly outperformed models like GPT-4o and Gemini 1.5 Pro, achieving an average evaluation score of 4.03 on MMDR-Bench, showcasing improvements in reasoning and instruction adherence.", "conclusion": "The modular design and iterative approach of CoLVLM Agent validate its effectiveness for handling complex multi-modal interactions better than competitors.", "key_contributions": ["Creation of MMDR-Bench for multi-modal dialogue evaluation", "Development of CoLVLM Agent to enhance LVLMs with reasoning capabilities", "Demonstrated superior performance compared to current state-of-the-art models"], "limitations": "", "keywords": ["Large Language Models", "Multi-Modal Interaction", "Dialogue Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.04905", "pdf": "https://arxiv.org/pdf/2501.04905.pdf", "abs": "https://arxiv.org/abs/2501.04905", "title": "Balancing Exploration and Cybersickness: Investigating Curiosity-Driven Behavior in Virtual Environments", "authors": ["Tangyao Li", "Yuyang Wang"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures, submitted to the IEEE", "summary": "During virtual navigation, users exhibit varied interaction and navigation\nbehaviors influenced by several factors. Existing theories and models have been\ndeveloped to explain and predict these diverse patterns. While users often\nexperience uncomfortable sensations, such as cybersickness, during virtual\nreality (VR) use, they do not always make optimal decisions to mitigate these\neffects. Although methods like reinforcement learning have been used to model\ndecision-making processes, they typically rely on random selection to simulate\nactions, failing to capture the complexities of real navigation behavior. In\nthis study, we propose curiosity as a key factor driving irrational\ndecision-making, suggesting that users continuously balance exploration and\ncybersickness according to the free energy principle during virtual navigation.\nOur findings show that VR users generally adopt conservative strategies when\nnavigating, with most participants displaying negative curiosity across trials.\nHowever, curiosity levels tend to rise when the virtual environment changes,\nillustrating the dynamic interplay between exploration and discomfort. This\nstudy provides a quantitative approach to decoding curiosity-driven behavior\nduring virtual navigation, offering insights into how users balance exploration\nand the avoidance of cybersickness. Future research will further refine this\nmodel by incorporating additional psychological and environmental factors to\nimprove the accuracy of navigation pattern predictions.", "AI": {"tldr": "This study explores how curiosity influences decision-making during virtual navigation and its relationship with cybersickness.", "motivation": "To understand the diverse interactions and navigation behaviors of users during virtual reality experiences, and to address the limitations of existing models that fail to account for users' actual decision-making processes.", "method": "Proposes a model incorporating curiosity as a driving factor in decision-making in virtual navigation while balancing exploration against cybersickness, evaluated through quantitative analysis of user behavior in varying virtual environments.", "result": "Most VR users displayed conservative navigation strategies with generally negative curiosity, but curiosity increased with changes in the virtual environment, indicating a complex interplay between exploration and discomfort.", "conclusion": "The study provides a new quantitative model for understanding curiosity-driven navigation in VR, establishing a foundation for future research into integrating psychological factors for better prediction of navigation patterns.", "key_contributions": ["Introduces curiosity as a key factor in understanding navigation behavior in virtual environments.", "Quantitative analysis demonstrating the relationship between curiosity levels and navigation strategies among users.", "Offers a model that considers psychological and environmental factors in decision-making during virtual navigation."], "limitations": "Future research is needed to incorporate more psychological and environmental factors to refine the model.", "keywords": ["Virtual Reality", "Curiosity", "Cybersickness", "Navigation Behavior", "Reinforcement Learning"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.15190", "pdf": "https://arxiv.org/pdf/2508.15190.pdf", "abs": "https://arxiv.org/abs/2508.15190", "title": "SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling", "authors": ["Dong Liu", "Yanxuan Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization plays a critical role in language modeling, yet existing\napproaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on\nfrequency statistics, ignoring the underlying semantic structure of text. This\nleads to over-tokenization of semantically redundant spans and underutilization\nof contextual coherence, particularly in long-context scenarios. In this work,\nwe propose \\textbf{SemToken}, a semantic-aware tokenization framework that\njointly reduces token redundancy and improves computation efficiency. SemToken\nfirst extracts contextual semantic embeddings via lightweight encoders and\nperforms local semantic clustering to merge semantically equivalent tokens.\nThen, it allocates heterogeneous token granularity based on semantic density,\nallowing finer-grained tokenization in content-rich regions and coarser\ncompression in repetitive or low-entropy spans. SemToken can be seamlessly\nintegrated with modern language models and attention acceleration methods.\nExperiments on long-context language modeling benchmarks such as WikiText-103\nand LongBench show that SemToken achieves up to $2.4\\times$ reduction in token\ncount and $1.9\\times$ speedup, with negligible or no degradation in perplexity\nand downstream accuracy. Our findings suggest that semantic structure offers a\npromising new axis for optimizing tokenization and computation in large\nlanguage models.", "AI": {"tldr": "SemToken is a semantic-aware tokenization framework designed to improve language modeling by reducing token redundancy and enhancing computation efficiency.", "motivation": "Current tokenization methods fail to consider the semantic structure of text, leading to inefficiencies in handling long-context scenarios and redundancy in token usage.", "method": "SemToken utilizes lightweight encoders to extract contextual semantic embeddings and performs local semantic clustering to merge semantically equivalent tokens. It adjusts token granularity based on semantic density for more effective long-context handling.", "result": "SemToken achieves up to 2.4× reduction in token count and 1.9× speedup with minimal to no degradation in downstream accuracy and perplexity on long-context language modeling benchmarks.", "conclusion": "The semantic structure of text provides a valuable dimension for optimizing tokenization, leading to improved efficiency and effectiveness in large language models.", "key_contributions": ["Introduction of SemToken, a semantic-aware tokenization framework", "Demonstrated improvements in token count reduction and computational speed", "Successful integration with modern language models and attention acceleration methods"], "limitations": "", "keywords": ["Tokenization", "Semantic Clustering", "Language Models", "Natural Language Processing", "Efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.15202", "pdf": "https://arxiv.org/pdf/2508.15202.pdf", "abs": "https://arxiv.org/abs/2508.15202", "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models", "authors": ["Yuanchen Zhou", "Shuo Jiang", "Jie Zhu", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce \\textbf{Fin-PRM},\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin.", "AI": {"tldr": "Introducing Fin-PRM, a domain-specialized Process Reward Model for evaluating intermediate reasoning in financial tasks, outperforming traditional models.", "motivation": "Existing Process Reward Models lack efficacy in specialized domains like finance, which require structured and regulatory-compliant reasoning.", "method": "Fin-PRM integrates step-level and trajectory-level reward supervision for fine-grained evaluation in financial reasoning tasks, applied in both offline and online settings.", "result": "Fin-PRM outperforms general Purpose PRMs and domain baselines on benchmarks like CFLUE and FinQA, showing significant improvements in supervised and reinforcement learning.", "conclusion": "Domain-specialized reward modeling significantly enhances LLM alignment with expert-level financial reasoning, providing resources for broader application.", "key_contributions": ["Introduction of Fin-PRM for financial tasks", "Integration of step-level and trajectory-level reward supervision", "Demonstrated significant improvements in reasoning task performance"], "limitations": "", "keywords": ["Process Reward Models", "Financial reasoning", "Large language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.15212", "pdf": "https://arxiv.org/pdf/2508.15212.pdf", "abs": "https://arxiv.org/abs/2508.15212", "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning", "authors": ["Huanxuan Liao", "Yixing Xu", "Shizhu He", "Guanchen Li", "Xuanwu Yin", "Dong Li", "Emad Barsoum", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.", "AI": {"tldr": "SPARK is a training-free method that prunes KV cache at the channel level to improve efficiency in LLMs during long-context inference, achieving over 30% reduction in memory usage and maintaining accuracy with aggressive pruning.", "motivation": "To address the KV cache bottleneck in LLMs, which limits memory and computational efficiency during long-context inference.", "method": "SPARK applies unstructured sparsity by pruning KV at the channel level and dynamically restoring pruned entries during attention score computation.", "result": "SPARK reduces channel-level redundancy, enabling longer sequence processing within the same memory budget and achieving over 30% reduction in KV cache storage compared to eviction-based methods, with minimal degradation in performance.", "conclusion": "SPARK demonstrates robustness and effectiveness, maintaining performance with less than 5% degradation even with 80% pruning, and is compatible with existing compression and quantization techniques.", "key_contributions": ["Proposes a training-free method for channel-level pruning of KV cache in LLMs.", "Achieves significant memory savings while improving or maintaining model accuracy.", "Demonstrates compatibility with other KV compression techniques for enhanced performance."], "limitations": "", "keywords": ["Long-context inference", "large language models", "KV cache", "channel sparsity", "memory efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15213", "pdf": "https://arxiv.org/pdf/2508.15213.pdf", "abs": "https://arxiv.org/abs/2508.15213", "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering", "authors": ["Bolei He", "Xinran He", "Run Shao", "Shanfu Shu", "Xianwei Xue", "Mingquan Cheng", "Haifeng Li", "Zhenhua Ling"], "categories": ["cs.CL"], "comment": "EMNLP2025 Findings", "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost.", "AI": {"tldr": "Selct2Know (S2K) is a framework designed to enhance the performance of LLMs in domain-specific question-answering by selectively integrating external knowledge and improving reasoning capabilities.", "motivation": "LLMs excel in general QA but face challenges in domain-specific applications due to hallucinations and latency from noisy retrieval, as well as high costs for continued pretraining.", "method": "The S2K framework utilizes an internal-external knowledge self-selection strategy and selective supervised fine-tuning to integrate domain knowledge progressively.", "result": "Experiments indicated that S2K consistently outperforms other methods on medical, legal, and financial QA benchmarks, achieving comparable results to domain-pretrained LLMs at a lower cost.", "conclusion": "The S2K framework effectively addresses the limitations of existing methods by mimicking human learning processes and providing cost-effective domain knowledge acquisition.", "key_contributions": ["Introduction of the Selct2Know (S2K) framework for knowledge integration", "Development of a structured reasoning data generation pipeline", "Enhanced reasoning capabilities through GRPO integration"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "domain-specific QA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15214", "pdf": "https://arxiv.org/pdf/2508.15214.pdf", "abs": "https://arxiv.org/abs/2508.15214", "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall", "authors": ["Sijia Cui", "Aiyao He", "Shuai Xu", "Hongming Zhang", "Yanna Wang", "Qingyang Zhang", "Yajing Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1\\% on easy and 4.7\\% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44\\% and 23.38\\%, respectively.", "AI": {"tldr": "This paper introduces SEER, a self-guided method for improving LLM interactions with external systems through fine-grained experience retrieval.", "motivation": "To overcome challenges in tool selection, parameter generation, and tool-chain planning faced by LLMs when using multiple tools.", "method": "SEER employs stepwise retrieval from an experience pool that continuously updates with past successful tool usage trajectories.", "result": "SEER shows an average improvement of 6.1% on easy and 4.7% on hard questions in the ToolQA benchmark, with even higher accuracy gains in real-world domain evaluations.", "conclusion": "The continuous update mechanism of SEER enhances LLM performance over time without needing extensive prompt engineering or manual curation.", "key_contributions": ["Introduction of Stepwise Experience Recall (SEER) for tool interaction", "Continuous updates to user experience pool improve tool usage performance", "Substantial accuracy gains demonstrated on the ToolQA benchmark and real-world domains."], "limitations": "", "keywords": ["Large Language Models", "Tool Selection", "Experience Retrieval", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.15218", "pdf": "https://arxiv.org/pdf/2508.15218.pdf", "abs": "https://arxiv.org/abs/2508.15218", "title": "Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?", "authors": ["Momoka Furuhashi", "Kouta Nakayama", "Takashi Kodama", "Saku Sugawara"], "categories": ["cs.CL"], "comment": "Accepted to the EMNLP 2025 Main Conference", "summary": "Automatic evaluation of generative tasks using large language models faces\nchallenges due to ambiguous criteria. Although automatic checklist generation\nis a potentially promising approach, its usefulness remains underexplored. We\ninvestigate whether checklists should be used for all questions or selectively,\ngenerate them using six methods, evaluate their effectiveness across eight\nmodel sizes, and identify checklist items that correlate with human\nevaluations. Through experiments on pairwise comparison and direct scoring\ntasks, we find that selective checklist use tends to improve evaluation\nperformance in pairwise settings, while its benefits are less consistent in\ndirect scoring. Our analysis also shows that even checklist items with low\ncorrelation to human scores often reflect human-written criteria, indicating\npotential inconsistencies in human evaluation. These findings highlight the\nneed to more clearly define objective evaluation criteria to guide both human\nand automatic evaluations. \\footnote{Our code is available\nat~https://github.com/momo0817/checklist-effectiveness-study", "AI": {"tldr": "The paper investigates the effectiveness of automatic checklist generation for evaluating generative tasks using large language models, highlighting selective use as beneficial in some settings.", "motivation": "To address challenges in automatic evaluation of generative tasks due to ambiguous criteria and explore the effectiveness of automatic checklist generation.", "method": "The authors generate checklists using six methods, evaluate their effectiveness across eight model sizes, and analyze correlation with human evaluations through experiments on pairwise comparison and direct scoring tasks.", "result": "Selective checklist use improves evaluation performance in pairwise settings, with inconsistent benefits in direct scoring. Items with low correlation to human scores still reflect human-written criteria.", "conclusion": "Clearly defining objective evaluation criteria is essential for guiding both human and automatic evaluations in generative tasks.", "key_contributions": ["Demonstrated the benefits of selective checklist use in evaluation settings", "Generated checklists using six distinct methods", "Analyzed checklist item correlation with human evaluations"], "limitations": "Potential inconsistencies in human evaluation criteria and the effectiveness of checklist methods may vary.", "keywords": ["automatic evaluation", "checklist generation", "large language models", "generative tasks", "human evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.15229", "pdf": "https://arxiv.org/pdf/2508.15229.pdf", "abs": "https://arxiv.org/abs/2508.15229", "title": "VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models", "authors": ["Hanling Zhang", "Yayu Zhou", "Tongcheng Fang", "Zhihang Yuan", "Guohao Dai", "Yu Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Small Language Models (SLMs) provide computational advantages in\nresource-constrained environments, yet memory limitations remain a critical\nbottleneck for edge device deployment. A substantial portion of SLMs' memory\nfootprint stems from vocabulary-related components, particularly embeddings and\nlanguage modeling (LM) heads, due to large vocabulary sizes. Existing static\nvocabulary pruning, while reducing memory usage, suffers from rigid,\none-size-fits-all designs that cause information loss from the prefill stage\nand a lack of flexibility. In this work, we identify two key principles\nunderlying the vocabulary reduction challenge: the lexical locality principle,\nthe observation that only a small subset of tokens is required during any\nsingle inference, and the asymmetry in computational characteristics between\nvocabulary-related components of SLM. Based on these insights, we introduce\nVocabTailor, a novel decoupled dynamic vocabulary selection framework that\naddresses memory constraints through offloading embedding and implements a\nhybrid static-dynamic vocabulary selection strategy for LM Head, enabling\non-demand loading of vocabulary components. Comprehensive experiments across\ndiverse downstream tasks demonstrate that VocabTailor achieves a reduction of\nup to 99% in the memory usage of vocabulary-related components with minimal or\nno degradation in task performance, substantially outperforming existing static\nvocabulary pruning.", "AI": {"tldr": "The paper presents VocabTailor, a framework for dynamic vocabulary selection in Small Language Models (SLMs) to reduce memory usage while maintaining task performance.", "motivation": "Memory limitations in Small Language Models (SLMs) hinder their deployment in resource-constrained environments, particularly due to large vocabulary sizes that impact efficiency.", "method": "VocabTailor employs a decoupled dynamic vocabulary selection framework, offloading embeddings and using a hybrid static-dynamic vocabulary selection strategy for LM Head.", "result": "VocabTailor achieves up to 99% reduction in memory usage for vocabulary-related components without significant performance loss on various tasks.", "conclusion": "The proposed framework significantly surpasses traditional static vocabulary pruning methods, improving efficiency in utilizing memory resources while preserving model performance.", "key_contributions": ["Introduction of VocabTailor for dynamic vocabulary selection in SLMs", "Reduction of vocabulary-related memory usage by up to 99%", "Demonstrated minimal performance degradation across multiple tasks"], "limitations": "", "keywords": ["Small Language Models", "Dynamic Vocabulary Selection", "Memory Efficiency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.15239", "pdf": "https://arxiv.org/pdf/2508.15239.pdf", "abs": "https://arxiv.org/abs/2508.15239", "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai", "authors": ["Peerat Limkonchotiwat", "Pume Tuchinda", "Lalita Lowphansirikul", "Surapon Nonesung", "Panuthep Tasawong", "Alham Fikri Aji", "Can Udomcharoenchaikit", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Main). Model and Dataset:\n  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd", "summary": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings.", "AI": {"tldr": "WangchanThaiInstruct introduces a Thai dataset for evaluating and tuning large language models, highlighting performance disparities in low-resource languages.", "motivation": "To address the performance gaps of large language models in low-resource languages like Thai, which are often overlooked in existing benchmarks that rely on translations.", "method": "The dataset was created through a multi-stage quality control process involving annotators, domain experts, and AI researchers, covering multiple professional domains and various task types.", "result": "Models fine-tuned on WangchanThaiInstruct outperformed those using translated data in both in-domain and out-of-domain benchmarks, revealing significant performance gaps in culturally and professionally specific tasks.", "conclusion": "The study emphasizes the importance of culturally and professionally grounded instruction data for improving LLM alignment in linguistically diverse settings.", "key_contributions": ["Introduction of WangchanThaiInstruct, a human-authored Thai dataset.", "Demonstration of performance advantages for models trained on culturally grounded data.", "Evidence of substantial performance gaps in zero-shot evaluations of language models in Thai."], "limitations": "", "keywords": ["large language models", "Thai dataset", "instruction tuning", "low-resource languages", "cultural nuances"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.15244", "pdf": "https://arxiv.org/pdf/2508.15244.pdf", "abs": "https://arxiv.org/abs/2508.15244", "title": "UniCoM: A Universal Code-Switching Speech Generator", "authors": ["Sangmin Lee", "Woojin Chung", "Seyun Um", "Hong-Goo Kang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Code-switching (CS), the alternation between two or more languages within a\nsingle speaker's utterances, is common in real-world conversations and poses\nsignificant challenges for multilingual speech technology. However, systems\ncapable of handling this phenomenon remain underexplored, primarily due to the\nscarcity of suitable datasets. To resolve this issue, we propose Universal\nCode-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS\nsamples without altering sentence semantics. Our approach utilizes an algorithm\nwe call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by\nreplacing selected words with their translations while considering their parts\nof speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a\nmultilingual CS corpus designed for automatic speech recognition (ASR) and\nspeech-to-text translation (S2TT). Experimental results show that CS-FLEURS\nachieves high intelligibility and naturalness, performing comparably to\nexisting datasets on both objective and subjective metrics. We expect our\napproach to advance CS speech technology and enable more inclusive multilingual\nsystems.", "AI": {"tldr": "This paper presents a novel pipeline, Universal Code-Mixer (UniCoM), for generating high-quality code-switching speech samples to advance multilingual speech technology.", "motivation": "Address the challenges of handling code-switching in multilingual speech systems due to the lack of suitable datasets.", "method": "The proposed method, Substituting WORDs with Synonyms (SWORDS), replaces selected words with their translations considering parts of speech, generating natural code-switched samples.", "result": "The Code-Switching FLEURS (CS-FLEURS) corpus was constructed, achieving high intelligibility and naturalness, and performing comparably to existing datasets in ASR and S2TT tasks.", "conclusion": "UniCoM and CS-FLEURS are expected to advance code-switching speech technology and promote inclusive multilingual systems.", "key_contributions": ["Proposed a novel pipeline for generating code-switching speech samples.", "Constructed the CS-FLEURS corpus for ASR and S2TT.", "Showed competitive performance of CS-FLEURS against existing datasets."], "limitations": "", "keywords": ["code-switching", "speech technology", "multilingual systems", "natural language processing", "speech recognition"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.15250", "pdf": "https://arxiv.org/pdf/2508.15250.pdf", "abs": "https://arxiv.org/abs/2508.15250", "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "categories": ["cs.CL", "I.2.7"], "comment": "24pages, 12 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate\nprofessional roles. However, comprehensive psychological and ethical evaluation\nin these contexts remains lacking. This paper introduces EMNLP, an\nEducator-role Moral and Normative LLMs Profiling framework for personality\nprofiling, moral development stage measurement, and ethical risk under soft\nprompt injection. EMNLP extends existing scales and constructs 88\nteacher-specific moral dilemmas, enabling profession-oriented comparison with\nhuman teachers. A targeted soft prompt injection set evaluates compliance and\nvulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs\nexhibit more idealized and polarized personalities than human teachers, excel\nin abstract moral reasoning, but struggle with emotionally complex situations.\nModels with stronger reasoning are more vulnerable to harmful prompt injection,\nrevealing a paradox between capability and safety. The model temperature and\nother hyperparameters have limited influence except in some risk behaviors.\nThis paper presents the first benchmark to assess ethical and psychological\nalignment of teacher-role LLMs for educational AI. Resources are available at\nhttps://e-m-n-l-p.github.io/.", "AI": {"tldr": "The paper introduces a framework for profiling Large Language Models (LLMs) in educational roles, focusing on ethical and psychological evaluation through a series of moral dilemmas and soft prompt injections.", "motivation": "There is a lack of comprehensive psychological and ethical evaluations of Large Language Models (LLMs) in professional simulations, particularly in educational contexts.", "method": "The paper presents the Educator-role Moral and Normative LLMs Profiling (EMNLP) framework which constructs 88 teacher-specific moral dilemmas and employs soft prompt injections to assess compliance and vulnerability in LLMs.", "result": "Experiments reveal that teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, perform well in abstract moral reasoning, but face challenges in emotionally complex situations. Additionally, there is a paradox where models with better reasoning are more susceptible to harmful prompt injections.", "conclusion": "The study establishes a benchmark for assessing the ethical and psychological alignment of teacher-role LLMs in educational AI contexts, highlighting the importance of understanding the balance between model capability and safety.", "key_contributions": ["Introduction of the EMNLP framework for profiling LLMs in education.", "Creation of 88 teacher-specific moral dilemmas for comprehensive assessment.", "First benchmarking methodology for ethical risk assessment in teacher-role LLMs."], "limitations": "The findings suggest limited influence of model temperature and hyperparameters on risk behaviors, indicating a need for deeper investigation into model safety.", "keywords": ["Large Language Models", "Moral Dilemmas", "Educational AI", "Ethics", "Personality Profiling"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2508.15253", "pdf": "https://arxiv.org/pdf/2508.15253.pdf", "abs": "https://arxiv.org/abs/2508.15253", "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation", "authors": ["Eunseong Choi", "June Park", "Hyeri Lee", "Jongwuk Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025; 14 pages; 5 figures, 11 tables", "summary": "Retrieval-augmented generation (RAG) enhances the capabilities of large\nlanguage models (LLMs) by incorporating external knowledge into their input\nprompts. However, when the retrieved context contradicts the LLM's parametric\nknowledge, it often fails to resolve the conflict between incorrect external\ncontext and correct parametric knowledge, known as context-memory conflict. To\ntackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation\n(CARE), consisting of a context assessor and a base LLM. The context assessor\nencodes compact memory token embeddings from raw context tokens. Through\ngrounded/adversarial soft prompting, the context assessor is trained to discern\nunreliable context and capture a guidance signal that directs reasoning toward\nthe more reliable knowledge source. Extensive experiments show that CARE\neffectively mitigates context-memory conflicts, leading to an average\nperformance gain of 5.0\\% on QA and fact-checking benchmarks, establishing a\npromising direction for trustworthy and adaptive RAG systems.", "AI": {"tldr": "Introducing CARE, a method to resolve context-memory conflicts in RAG systems, improving reliability in LLMs.", "motivation": "To enhance the reliability of retrieval-augmented generation (RAG) by addressing conflicts between external context and LLM's parametric knowledge.", "method": "CARE integrates a context assessor that evaluates context reliability and provides a guidance signal to the underlying LLM through grounded/adversarial soft prompting.", "result": "CARE reduced context-memory conflicts, achieving an average performance gain of 5.0% on QA and fact-checking benchmarks.", "conclusion": "CARE presents a significant step towards building trustworthy and adaptive RAG systems by effectively managing context-memory conflicts.", "key_contributions": ["Development of the Conflict-Aware REtrieval-Augmented Generation (CARE) method", "Integration of a context assessor for reliability evaluation", "Demonstrated performance improvement on QA and fact-checking benchmarks"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Large language models", "Context-memory conflict"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15274", "pdf": "https://arxiv.org/pdf/2508.15274.pdf", "abs": "https://arxiv.org/abs/2508.15274", "title": "TComQA: Extracting Temporal Commonsense from Text", "authors": ["Lekshmi R Nair", "Arun Sankar", "Koninika Pal"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Understanding events necessitates grasping their temporal context, which is\noften not explicitly stated in natural language. For example, it is not a\ntrivial task for a machine to infer that a museum tour may last for a few\nhours, but can not take months. Recent studies indicate that even advanced\nlarge language models (LLMs) struggle in generating text that require reasoning\nwith temporal commonsense due to its infrequent explicit mention in text.\nTherefore, automatically mining temporal commonsense for events enables the\ncreation of robust language models. In this work, we investigate the capacity\nof LLMs to extract temporal commonsense from text and evaluate multiple\nexperimental setups to assess their effectiveness. Here, we propose a temporal\ncommonsense extraction pipeline that leverages LLMs to automatically mine\ntemporal commonsense and use it to construct TComQA, a dataset derived from\nSAMSum and RealNews corpora. TComQA has been validated through crowdsourcing\nand achieves over 80\\% precision in extracting temporal commonsense. The model\ntrained with TComQA also outperforms an LLM fine-tuned on existing dataset of\ntemporal question answering task.", "AI": {"tldr": "This paper presents a method for extracting temporal commonsense from text using large language models (LLMs) and introduces a new dataset called TComQA, which demonstrates improved performance in temporal question answering.", "motivation": "There is a need to enhance the reasoning capabilities of LLMs regarding temporal commonsense, as it is often implicit in natural language, impacting their performance on tasks requiring temporal understanding.", "method": "The authors develop a temporal commonsense extraction pipeline that utilizes LLMs to automatically mine temporal commonsense from text, evaluated through various experimental setups.", "result": "The proposed pipeline results in the creation of the TComQA dataset, achieving over 80% precision when validated through crowdsourcing, and outperforms existing methods in temporal question answering tasks.", "conclusion": "The study highlights the potential of mining temporal commonsense to improve the capabilities of language models, addressing challenges in understanding temporal context in natural language.", "key_contributions": ["Introduction of a temporal commonsense extraction pipeline using LLMs", "Development of the TComQA dataset validated through crowdsourcing", "Demonstrated improved performance in temporal question answering tasks compared to existing datasets"], "limitations": "", "keywords": ["temporal commonsense", "large language models", "TComQA", "natural language understanding", "temporal reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.15316", "pdf": "https://arxiv.org/pdf/2508.15316.pdf", "abs": "https://arxiv.org/abs/2508.15316", "title": "CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing", "authors": ["Abdul Rehman", "Jian-Jun Zhang", "Xiaosong Yang"], "categories": ["cs.CL", "cs.LG", "eess.AS", "I.2.7"], "comment": "Accepted in: 8th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2025)", "summary": "Universal phoneme recognition typically requires analyzing long speech\nsegments and language-specific patterns. Many speech processing tasks require\npure phoneme representations free from contextual influence, which motivated\nour development of CUPE - a lightweight model that captures key phoneme\nfeatures in just 120 milliseconds, about one phoneme's length. CUPE processes\nshort, fixed-width windows independently and, despite fewer parameters than\ncurrent approaches, achieves competitive cross-lingual performance by learning\nfundamental acoustic patterns common to all languages. Our extensive evaluation\nthrough supervised and self-supervised training on diverse languages, including\nzero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual\ngeneralization and reveals that effective universal speech processing is\npossible through modeling basic acoustic patterns within phoneme-length\nwindows.", "AI": {"tldr": "CUPE is a lightweight model for universal phoneme recognition that processes phoneme-length speech segments effectively and achieves strong cross-lingual performance.", "motivation": "The need for phoneme representations free from contextual influence in various speech processing tasks.", "method": "CUPE processes short, fixed-width speech windows independently while capturing key phoneme features in just 120 milliseconds.", "result": "The model demonstrates strong cross-lingual generalization and competitive performance through extensive evaluation on diverse languages, including zero-shot tests.", "conclusion": "Effective universal speech processing can be achieved by modeling basic acoustic patterns within phoneme-length windows.", "key_contributions": ["Development of CUPE for lightweight phoneme recognition", "Competitive performance with fewer parameters compared to existing methods", "Strong cross-lingual generalization using fundamental acoustic patterns"], "limitations": "", "keywords": ["phoneme recognition", "cross-lingual performance", "acoustic patterns"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.15357", "pdf": "https://arxiv.org/pdf/2508.15357.pdf", "abs": "https://arxiv.org/abs/2508.15357", "title": "KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models", "authors": ["Haji Gul", "Abul Ghani Naim", "Ajaz Ahmad Bhat"], "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Knowledge Graphs (KGs) enable applications in various domains such as\nsemantic search, recommendation systems, and natural language processing. KGs\nare often incomplete, missing entities and relations, an issue addressed by\nKnowledge Graph Completion (KGC) methods that predict missing elements.\nDifferent evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank\n(MR), and Hit@k, are commonly used to assess the performance of such KGC\nmodels. A major challenge in evaluating KGC models, however, lies in comparing\ntheir performance across multiple datasets and metrics. A model may outperform\nothers on one dataset but underperform on another, making it difficult to\ndetermine overall superiority. Moreover, even within a single dataset,\ndifferent metrics such as MRR and Hit@1 can yield conflicting rankings, where\none model excels in MRR while another performs better in Hit@1, further\ncomplicating model selection for downstream tasks. These inconsistencies hinder\nholistic comparisons and highlight the need for a unified meta-metric that\nintegrates performance across all metrics and datasets to enable a more\nreliable and interpretable evaluation framework. To address this need, we\npropose KG Evaluation based on Distance from Average Solution (EDAS), a robust\nand interpretable meta-metric that synthesizes model performance across\nmultiple datasets and diverse evaluation criteria into a single normalized\nscore ($M_i \\in [0,1]$). Unlike traditional metrics that focus on isolated\naspects of performance, EDAS offers a global perspective that supports more\ninformed model selection and promotes fairness in cross-dataset evaluation.\nExperimental results on benchmark datasets such as FB15k-237 and WN18RR\ndemonstrate that EDAS effectively integrates multi-metric, multi-dataset\nperformance into a unified ranking, offering a consistent, robust, and\ngeneralizable framework for evaluating KGC models.", "AI": {"tldr": "The paper introduces a new meta-metric, KG Evaluation based on Distance from Average Solution (EDAS), for evaluating Knowledge Graph Completion models across multiple datasets and metrics, aiming to provide a unified and interpretable evaluation framework.", "motivation": "To address the challenges of inconsistent evaluations of KGC models across different datasets and metrics, which complicate model selection for downstream tasks.", "method": "The paper proposes the EDAS meta-metric, which synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score, providing a global perspective on KGC model performance.", "result": "Experimental results demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, showing a consistent and robust evaluation framework.", "conclusion": "EDAS promotes fairness in cross-dataset evaluations and supports more informed model selection for Knowledge Graph Completion models.", "key_contributions": ["Introduction of the EDAS meta-metric for KGC evaluation", "Integration of multiple evaluation metrics into a single normalized score", "Demonstration of consistent and generalizable evaluation framework through experiments."], "limitations": "", "keywords": ["Knowledge Graphs", "Knowledge Graph Completion", "Meta-metric", "Evaluation Metrics", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.15361", "pdf": "https://arxiv.org/pdf/2508.15361.pdf", "abs": "https://arxiv.org/abs/2508.15361", "title": "A Survey on Large Language Model Benchmarks", "authors": ["Shiwen Ni", "Guhong Chen", "Shuaimin Li", "Xuanang Chen", "Siyi Li", "Bingli Wang", "Qiyao Wang", "Xingjian Wang", "Yifan Zhang", "Liyang Fan", "Chengming Li", "Ruifeng Xu", "Le Sun", "Min Yang"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.", "AI": {"tldr": "This paper reviews 283 benchmarks for evaluating large language models, categorizing them into general, domain-specific, and target-specific types, while highlighting issues like inflated scores and biases.", "motivation": "To assess how well large language models are evaluated through benchmarks and to guide future developments in this area.", "method": "A systematic review categorizing 283 benchmarks into three categories: general capabilities, domain-specific, and target-specific, analyzing their characteristics and issues.", "result": "Identified three main categories of benchmarks and highlighted problems like data contamination, biases, and a lack of evaluation on dynamic environments.", "conclusion": "A referable design paradigm for innovating benchmarks is proposed to address current shortcomings.", "key_contributions": ["Systematic categorization of 283 evaluation benchmarks for large language models.", "Identification of key performance evaluation issues in existing benchmarks.", "Proposal of a design paradigm for future benchmark development."], "limitations": "Issues include biased evaluations and lack of considerations for dynamic environments in current benchmarks.", "keywords": ["large language models", "evaluation benchmarks", "performance assessment", "systematic review", "model development"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.15370", "pdf": "https://arxiv.org/pdf/2508.15370.pdf", "abs": "https://arxiv.org/abs/2508.15370", "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation", "authors": ["Yichi Zhang", "Yao Huang", "Yifan Wang", "Yitong Sun", "Chang Liu", "Zhe Zhao", "Zhengwei Fang", "Huanran Chen", "Xiao Yang", "Xingxing Wei", "Hang Su", "Yinpeng Dong", "Jun Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "For Appendix, please refer to arXiv:2406.07057", "summary": "The trustworthiness of Multimodal Large Language Models (MLLMs) remains an\nintense concern despite the significant progress in their capabilities.\nExisting evaluation and mitigation approaches often focus on narrow aspects and\noverlook risks introduced by the multimodality. To tackle these challenges, we\npropose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and\nmitigating the trustworthiness issues of MLLMs. We define a three-dimensional\nframework, encompassing five trustworthiness aspects which include\ntruthfulness, robustness, safety, fairness, and privacy; two novel risk types\ncovering multimodal risks and cross-modal impacts; and various mitigation\nstrategies from the perspectives of data, model architecture, training, and\ninference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and\n28 curated datasets, enabling holistic evaluations over 30 open-source and\nproprietary MLLMs and in-depth analysis with 8 representative mitigation\nmethods. Our extensive experiments reveal significant vulnerabilities in\ncurrent models, including a gap between trustworthiness and general\ncapabilities, as well as the amplification of potential risks in base LLMs by\nboth multimodal training and inference. Moreover, our controlled analysis\nuncovers key limitations in existing mitigation strategies that, while some\nmethods yield improvements in specific aspects, few effectively address overall\ntrustworthiness, and many introduce unexpected trade-offs that compromise model\nutility. These findings also provide practical insights for future\nimprovements, such as the benefits of reasoning to better balance safety and\nperformance. Based on these insights, we introduce a Reasoning-Enhanced Safety\nAlignment (RESA) approach that equips the model with chain-of-thought reasoning\nability to discover the underlying risks, achieving state-of-the-art results.", "AI": {"tldr": "MultiTrust-X is a benchmark developed to evaluate, analyze, and mitigate trustworthiness issues in Multimodal Large Language Models (MLLMs), addressing aspects such as truthfulness, robustness, and fairness, while highlighting the risks introduced by multimodality.", "motivation": "The trustworthiness of MLLMs is increasingly important but poorly addressed by current evaluation methods that often overlook multimodal risks and their impacts.", "method": "The paper introduces a three-dimensional framework consisting of five trustworthiness aspects, two novel risk types tailored for multimodal contexts, and various mitigation strategies, leading to the creation of a benchmark with 32 tasks and 28 datasets for evaluating MLLMs.", "result": "The experiments reveal significant vulnerabilities in MLLMs, including the gap between trustworthiness and overall capabilities. The analysis shows that while mitigation methods can improve specific areas, they often compromise model utility and fail to enhance overall trustworthiness.", "conclusion": "The findings emphasize the need for better reasoning methods to mitigate risks effectively, leading to the proposal of a Reasoning-Enhanced Safety Alignment (RESA) approach for enhanced safety and performance in MLLMs.", "key_contributions": ["Introduction of MultiTrust-X for comprehensive evaluation of MLLMs", "Development of a framework to analyze multimodal risks in MLLMs", "Proposal of RESA for improving trustworthiness via reasoning techniques."], "limitations": "Existing mitigation strategies show limited effectiveness and can introduce trade-offs that reduce utility.", "keywords": ["Multimodal Large Language Models", "Trustworthiness", "Benchmark", "Risk Mitigation", "Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15371", "pdf": "https://arxiv.org/pdf/2508.15371.pdf", "abs": "https://arxiv.org/abs/2508.15371", "title": "Confidence-Modulated Speculative Decoding for Large Language Models", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "categories": ["cs.CL"], "comment": "This is the preprint of the paper, which has been accepted for oral\n  presentation and publication in the proceedings of IEEE INDISCON 2025. The\n  conference will be organized at the National Institute of Technology,\n  Rourkela, India, from August 21 to 23, 2025. The paper is 10 pages long, and\n  it contains 2 figures and 5 tables", "summary": "Speculative decoding has emerged as an effective approach for accelerating\nautoregressive inference by parallelizing token generation through a\ndraft-then-verify paradigm. However, existing methods rely on static drafting\nlengths and rigid verification criteria, limiting their adaptability across\nvarying model uncertainties and input complexities. This paper proposes an\ninformation-theoretic framework for speculative decoding based on\nconfidence-modulated drafting. By leveraging entropy and margin-based\nuncertainty measures over the drafter's output distribution, the proposed\nmethod dynamically adjusts the number of speculatively generated tokens at each\niteration. This adaptive mechanism reduces rollback frequency, improves\nresource utilization, and maintains output fidelity. Additionally, the\nverification process is modulated using the same confidence signals, enabling\nmore flexible acceptance of drafted tokens without sacrificing generation\nquality. Experiments on machine translation and summarization tasks demonstrate\nsignificant speedups over standard speculative decoding while preserving or\nimproving BLEU and ROUGE scores. The proposed approach offers a principled,\nplug-in method for efficient and robust decoding in large language models under\nvarying conditions of uncertainty.", "AI": {"tldr": "This paper introduces an adaptive information-theoretic framework for speculative decoding in autoregressive models, optimizing token generation and verification processes to enhance efficiency and output quality.", "motivation": "To improve the efficiency of autoregressive inference in large language models by adapting speculative decoding methods to varying model uncertainties and input complexities.", "method": "The proposed framework employs confidence-modulated drafting, using uncertainty measures to dynamically adjust the drafting lengths and verification processes during token generation.", "result": "The adaptive speculative decoding method shows significant speedups in machine translation and summarization tasks, outperforming standard methods while maintaining or enhancing BLEU and ROUGE scores.", "conclusion": "The proposed method provides an efficient and robust decoding alternative for large language models, enhancing both speed and quality under uncertain conditions.", "key_contributions": ["Information-theoretic framework for adaptive speculative decoding", "Dynamic adjustment of drafting lengths based on uncertainty measures", "Improved resource utilization and output fidelity during token generation"], "limitations": "", "keywords": ["speculative decoding", "autoregressive inference", "large language models", "machine translation", "summarization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15390", "pdf": "https://arxiv.org/pdf/2508.15390.pdf", "abs": "https://arxiv.org/abs/2508.15390", "title": "Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training", "authors": ["Woojin Chung", "Jeonghoon Kim"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models are trained with tokenizers, and the resulting token\ndistribution is highly imbalanced: a few words dominate the stream while most\noccur rarely. Recent practice favors ever-larger vocabularies, but the source\nof the benefit is unclear. We conduct a controlled study that scales the\nlanguage model's vocabulary from 24K to 196K while holding data, compute, and\noptimization fixed. We first quantify the complexity of tokenized text,\nformalized via Kolmogorov complexity, and show that larger vocabularies reduce\nthis complexity. Above 24K, every common word is already a single token, so\nfurther growth mainly deepens the relative token-frequency imbalance. A\nword-level loss decomposition shows that larger vocabularies reduce\ncross-entropy almost exclusively by lowering uncertainty on the 2,500 most\nfrequent words, even though loss on the rare tail rises. Constraining input and\noutput embedding norms to attenuate the effect of token-frequency imbalance\nreverses the gain, directly showing that the model exploits rather than suffers\nfrom imbalance. Because the same frequent words cover roughly 77% of tokens in\ndownstream benchmarks, this training advantage transfers intact. We also show\nthat enlarging model parameters with a fixed vocabulary yields the same\nfrequent-word benefit. Our results reframe \"bigger vocabularies help\" as\n\"lowering the complexity of tokenized text helps,\" providing a simple,\nprincipled lever for tokenizer-model co-design and clarifying the loss dynamics\nthat govern language-model scaling in pre-training.", "AI": {"tldr": "This paper examines the effects of vocabulary size on language model performance, showing that larger vocabularies lower the complexity of tokenized text but primarily benefit common words, with implications for tokenizer-model design.", "motivation": "There is a lack of clarity on the benefits of increasing vocabulary sizes in large language models, as token distributions are imbalanced with few words being dominant.", "method": "Conducted a controlled study scaling vocabulary from 24K to 196K while keeping data, compute, and optimization constant; analyzed tokenized text complexity using Kolmogorov complexity and examined loss dynamics with word-level decomposition.", "result": "Larger vocabularies reduce tokenized text complexity but mainly improve loss on the most frequent words; introducing norms to input/output embeddings can reverse the boost from increased vocabulary size.", "conclusion": "The results suggest focusing on reducing the complexity of tokenized text is more beneficial than merely increasing vocabulary size, influencing future tokenizer and model co-design.", "key_contributions": ["Reframed the understanding of vocabulary effects on language models as a matter of reducing token complexity.", "Demonstrated that larger vocabularies primarily benefit common words while increasing loss on rare words.", "Provided insights into the relationships between vocabulary size, token distribution, and loss dynamics."], "limitations": "", "keywords": ["large language models", "tokenizers", "vocabulary size", "complexity", "loss dynamics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.15396", "pdf": "https://arxiv.org/pdf/2508.15396.pdf", "abs": "https://arxiv.org/abs/2508.15396", "title": "Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models", "authors": ["Tobias Schreieder", "Tim Schopf", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing adoption of large language models (LLMs) has been accompanied\nby growing concerns regarding their reliability and trustworthiness. As a\nresult, a growing body of research focuses on evidence-based text generation\nwith LLMs, aiming to link model outputs to supporting evidence to ensure\ntraceability and verifiability. However, the field is fragmented due to\ninconsistent terminology, isolated evaluation practices, and a lack of unified\nbenchmarks. To bridge this gap, we systematically analyze 134 papers, introduce\na unified taxonomy of evidence-based text generation with LLMs, and investigate\n300 evaluation metrics across seven key dimensions. Thereby, we focus on\napproaches that use citations, attribution, or quotations for evidence-based\ntext generation. Building on this, we examine the distinctive characteristics\nand representative methods in the field. Finally, we highlight open challenges\nand outline promising directions for future work.", "AI": {"tldr": "This paper reviews evidence-based text generation with large language models (LLMs), offering a unified taxonomy and analyzing evaluation metrics to improve reliability and traceability.", "motivation": "The study addresses the reliability and trustworthiness concerns surrounding large language models by focusing on evidence-based text generation, linking outputs to supporting evidence.", "method": "The authors systematically analyzed 134 papers and evaluated 300 metrics across seven dimensions to create a unified taxonomy for evidence-based text generation in LLMs.", "result": "A comprehensive taxonomy for evidence-based text generation was introduced, along with an examination of distinctive characteristics and methods in the field, coupled with open challenges and future directions.", "conclusion": "The research contributes to a more coherent understanding of evidence-based text generation with LLMs, paving the way for unified benchmarks and improved practices in the community.", "key_contributions": ["Unified taxonomy of evidence-based text generation with LLMs", "Analysis of 300 evaluation metrics across seven dimensions", "Identification of open challenges and future directions for research"], "limitations": "", "keywords": ["Large Language Models", "Evidence-Based Generation", "Text Generation", "Machine Learning  Evaluation", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15407", "pdf": "https://arxiv.org/pdf/2508.15407.pdf", "abs": "https://arxiv.org/abs/2508.15407", "title": "When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models", "authors": ["Cheng Wang", "Gelei Deng", "Xianglin Yang", "Han Qiu", "Tianwei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Large Audio-Language Models (LALMs) are enhanced with audio perception\ncapabilities, enabling them to effectively process and understand multimodal\ninputs that combine audio and text. However, their performance in handling\nconflicting information between audio and text modalities remains largely\nunexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark\nspecifically designed to evaluate how LALMs prioritize information when\npresented with inconsistent audio-text pairs. Through extensive evaluation\nacross diverse audio understanding tasks, we reveal a concerning phenomenon:\nwhen inconsistencies exist between modalities, LALMs display a significant bias\ntoward textual input, frequently disregarding audio evidence. This tendency\nleads to substantial performance degradation in audio-centric tasks and raises\nimportant reliability concerns for real-world applications. We further\ninvestigate the influencing factors of text bias, and explore mitigation\nstrategies through supervised finetuning, and analyze model confidence patterns\nthat reveal persistent overconfidence even with contradictory inputs. These\nfindings underscore the need for improved modality balance during training and\nmore sophisticated fusion mechanisms to enhance the robustness when handling\nconflicting multi-modal inputs. The project is available at\nhttps://github.com/WangCheng0116/MCR-BENCH.", "AI": {"tldr": "This paper introduces MCR-BENCH, a benchmark for evaluating Large Audio-Language Models' performance with conflicting audio-text information, revealing a bias toward textual input that hampers audio task performance.", "motivation": "To assess the performance and reliability of Large Audio-Language Models (LALMs) in processing inconsistent audio and text data.", "method": "The paper presents MCR-BENCH, a benchmark for evaluating LALMs on diverse audio understanding tasks, especially when faced with conflicting audio and text pairs.", "result": "LALMs demonstrated a significant bias towards textual input over audio, leading to performance degradation in audio-centric tasks and highlighting a need for improved modality balance.", "conclusion": "The findings call for better training strategies and fusion mechanisms to enhance the handling of conflicting multimodal inputs in LALMs.", "key_contributions": ["Introduction of MCR-BENCH as a benchmark for LALMs.", "Demonstration of significant text bias in LALMs when faced with inconsistencies.", "Exploration of mitigation strategies through supervised finetuning."], "limitations": "Focuses primarily on inconsistencies between audio and text; other modalities are not addressed.", "keywords": ["Large Audio-Language Models", "multimodal inputs", "text bias", "benchmark", "audio understanding"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.15418", "pdf": "https://arxiv.org/pdf/2508.15418.pdf", "abs": "https://arxiv.org/abs/2508.15418", "title": "LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model", "authors": ["Yirong Sun", "Yizhong Geng", "Peidong Wei", "Yanjun Chen", "Jinghan Yang", "Rongfei Chen", "Wei Zhang", "Xiaoyu Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "The development of Large Speech-Language Models (LSLMs) has been slowed by\nfragmented architectures and a lack of transparency, hindering the systematic\ncomparison and reproducibility of research. Unlike in the vision-language\ndomain, the LSLM field suffers from the common practice of releasing model\nweights without their corresponding training data and configurations. To\naddress these critical gaps, we introduce LLaSO, the first fully open,\nend-to-end framework for large-scale speech-language modeling. LLaSO provides\nthe community with three essential resources: (1) LLaSO-Align, a 12M-instance\nspeech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task\ninstruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for\nstandardized evaluation. To validate our framework, we build and release\nLLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public\ndata. It achieves a normalized score of 0.72, establishing a strong,\nreproducible baseline that surpasses comparable models. Our analysis reveals\nthat while broader training coverage enhances performance, significant\ngeneralization gaps persist on unseen tasks, particularly in pure audio\nscenarios. By releasing the complete stack of data, benchmarks, and models,\nLLaSO establishes a foundational open standard to unify research efforts and\naccelerate community-driven progress in LSLMs. We release the code, dataset,\npretrained models, and results in https://github.com/EIT-NLP/LLaSO.", "AI": {"tldr": "Introduction of LLaSO, a fully open framework for large-scale speech-language modeling, providing datasets and benchmarks to facilitate research.", "motivation": "The lack of transparency and fragmented architectures in large speech-language models has hindered systematic comparison and reproducibility in research.", "method": "Development of LLaSO, which includes LLaSO-Align, LLaSO-Instruct, and LLaSO-Eval, alongside the release of LLaSO-Base, a 3.8B-parameter reference model trained on public data.", "result": "LLaSO-Base achieves a normalized score of 0.72, establishing a strong baseline that surpasses comparable models, while exposing performance gaps in unseen tasks.", "conclusion": "LLaSO aims to unify research efforts in LSLM by releasing comprehensive data and models, accelerating progress in the field.", "key_contributions": ["Introduction of LLaSO as an open framework for LSLMs", "Release of extensive datasets for training and evaluation", "Establishment of a reproducible benchmark for community use"], "limitations": "Generalization gaps persist on unseen tasks, especially in pure audio scenarios.", "keywords": ["Large Speech-Language Models", "Open Framework", "Datasets", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.15421", "pdf": "https://arxiv.org/pdf/2508.15421.pdf", "abs": "https://arxiv.org/abs/2508.15421", "title": "A Study of Privacy-preserving Language Modeling Approaches", "authors": ["Pritilata Saha", "Abhirup Sinha"], "categories": ["cs.CL"], "comment": null, "summary": "Recent developments in language modeling have increased their use in various\napplications and domains. Language models, often trained on sensitive data, can\nmemorize and disclose this information during privacy attacks, raising concerns\nabout protecting individuals' privacy rights. Preserving privacy in language\nmodels has become a crucial area of research, as privacy is one of the\nfundamental human rights. Despite its significance, understanding of how much\nprivacy risk these language models possess and how it can be mitigated is still\nlimited. This research addresses this by providing a comprehensive study of the\nprivacy-preserving language modeling approaches. This study gives an in-depth\noverview of these approaches, highlights their strengths, and investigates\ntheir limitations. The outcomes of this study contribute to the ongoing\nresearch on privacy-preserving language modeling, providing valuable insights\nand outlining future research directions.", "AI": {"tldr": "This study investigates privacy-preserving approaches in language modeling, emphasizing their importance in protecting individuals' privacy rights and outlining future research directions.", "motivation": "To address the privacy risks associated with language models trained on sensitive data and to explore methods for mitigating these risks.", "method": "Comprehensive study of various privacy-preserving language modeling approaches, highlighting their strengths and investigating their limitations.", "result": "An in-depth overview of privacy-preserving approaches, contributing insights to inform future research directions in this field.", "conclusion": "This research is vital for understanding and improving privacy protection in language models, given the increasing use of these models in sensitive applications.", "key_contributions": ["Comprehensive review of privacy-preserving language models", "Analysis of strengths and limitations of these approaches", "Identification of future research directions in privacy preservation."], "limitations": "The study may not cover all emerging techniques in privacy preservation as the field is rapidly evolving.", "keywords": ["privacy", "language models", "privacy preservation", "human rights", "data security"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15440", "pdf": "https://arxiv.org/pdf/2508.15440.pdf", "abs": "https://arxiv.org/abs/2508.15440", "title": "M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals", "authors": ["MSVPJ Sathvik", "Zuhair Hasan Shaik", "Vivek Gupta"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Mental health disorders are a global crisis. While various datasets exist for\ndetecting such disorders, there remains a critical gap in identifying\nindividuals actively seeking help. This paper introduces a novel dataset,\nM-Help, specifically designed to detect help-seeking behavior on social media.\nThe dataset goes beyond traditional labels by identifying not only help-seeking\nactivity but also specific mental health disorders and their underlying causes,\nsuch as relationship challenges or financial stressors. AI models trained on\nM-Help can address three key tasks: identifying help-seekers, diagnosing mental\nhealth conditions, and uncovering the root causes of issues.", "AI": {"tldr": "The paper introduces the M-Help dataset for detecting help-seeking behavior related to mental health on social media, enabling better identification and diagnosis of mental health disorders.", "motivation": "To fill the critical gap in identifying individuals actively seeking help for mental health issues on social media, amidst a global mental health crisis.", "method": "The study introduces the M-Help dataset, which categorizes help-seeking behavior and links it to specific mental health disorders and their causes.", "result": "AI models trained on the M-Help dataset can successfully identify help-seekers, diagnose mental health conditions, and reveal underlying causes.", "conclusion": "The M-Help dataset offers a comprehensive approach to understanding and addressing help-seeking behaviors related to mental health in social media contexts.", "key_contributions": ["Introduction of the M-Help dataset for help-seeking behavior", "Links mental health disorders to specific life stressors", "Facilitates advanced AI model training for better diagnosis and understanding of mental health issues."], "limitations": "", "keywords": ["mental health", "dataset", "help-seeking behavior", "social media", "AI models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.15453", "pdf": "https://arxiv.org/pdf/2508.15453.pdf", "abs": "https://arxiv.org/abs/2508.15453", "title": "Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English", "authors": ["Mohammad Ibrahim Qani"], "categories": ["cs.CL", "Translating nonequivalent words"], "comment": "Fully abstract is available in the attached file", "summary": "These pure languages understanding directly relates to translation knowledge\nwhere linguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. The purpose of this research is to introduce the methods of\nrendering non-equivalent words professionally from the source language to the\ntarget language and this research has been completed using library-based\nresearch. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. As a result, this research paper includes different ways and\nrules of rendering non-equivalent words from source language to the target\nlanguage and 25 non-equvalent words have been rendered from Dar & Uzbek into\nEnglish and Russian languages.", "AI": {"tldr": "Research on methods to translate non-equivalent words from source languages to target languages.", "motivation": "To address misunderstandings that arise from non-equivalent words in translation, particularly in cultural and traditional contexts.", "method": "Library-based research was used to identify methods and rules for rendering non-equivalent words.", "result": "Identified different methods for translating 25 specific non-equivalent words from Dar and Uzbek into English and Russian.", "conclusion": "The study provides insights into professional rendering of non-equivalent words, with an emphasis on the ongoing need for research in this area.", "key_contributions": ["Introduced methods for translating non-equivalent words", "Provided practical examples of 25 rendered words", "Highlighted the significance of cultural context in translation."], "limitations": "", "keywords": ["translation", "non-equivalent words", "cultural context", "language rendering", "linguistics"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2508.15456", "pdf": "https://arxiv.org/pdf/2508.15456.pdf", "abs": "https://arxiv.org/abs/2508.15456", "title": "PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback", "authors": ["Alexandru Coca", "Bo-Hsiang Tseng", "Pete Boothroyd", "Jianpeng Cheng", "Mark Gaynor", "Zhenxing Zhang", "Joe Stacey", "Tristan Guigue", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "categories": ["cs.CL"], "comment": "20 pages, 12 figures. To appear at SIGDIAL 2025", "summary": "Programmable task-oriented dialogue (TOD) agents enable language models to\nfollow structured dialogue policies, but their effectiveness hinges on accurate\nstate tracking. We present PyTOD, an agent that generates executable code to\ntrack dialogue state and uses policy and execution feedback for efficient error\ncorrection. To this end, PyTOD employs a simple constrained decoding approach,\nusing a language model instead of grammar rules to follow API schemata. This\nleads to state-of-the-art state tracking performance on the challenging SGD\nbenchmark. Our experiments show that PyTOD surpasses strong baselines in both\naccuracy and robust user goal estimation as the dialogue progresses,\ndemonstrating the effectiveness of execution-aware state tracking.", "AI": {"tldr": "PyTOD is a task-oriented dialogue agent that uses executable code for accurate state tracking and outperforms baselines on the SGD benchmark.", "motivation": "The effectiveness of programmable task-oriented dialogue agents relies on accurate state tracking, which is essential for maintaining dialogue flow and achieving user goals.", "method": "PyTOD implements a constrained decoding method that utilizes a language model for following API schemata instead of traditional grammar rules, enabling effective state tracking and error correction through policy and execution feedback.", "result": "PyTOD achieves state-of-the-art performance on the SGD benchmark, surpassing strong baselines in both accuracy and the ability to estimate user goals as dialogues evolve.", "conclusion": "The findings demonstrate the potential of execution-aware state tracking in enhancing the performance of dialogue systems.", "key_contributions": ["Introduction of PyTOD, a dialogue agent that generates executable code for state tracking.", "Use of a constrained decoding approach with language models for improved dialogue management.", "Demonstration of superior performance in state tracking and user goal estimation on the SGD benchmark."], "limitations": "", "keywords": ["task-oriented dialogue", "state tracking", "language models", "dialogue systems", "execution-aware tracking"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.15464", "pdf": "https://arxiv.org/pdf/2508.15464.pdf", "abs": "https://arxiv.org/abs/2508.15464", "title": "RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores", "authors": ["Yingshu Li", "Yunyi Liu", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating automatically generated radiology reports remains a fundamental\nchallenge due to the lack of clinically grounded, interpretable, and\nfine-grained metrics. Existing methods either produce coarse overall scores or\nrely on opaque black-box models, limiting their usefulness in real-world\nclinical workflows. We introduce RadReason, a novel evaluation framework for\nradiology reports that not only outputs fine-grained sub-scores across six\nclinically defined error types, but also produces human-readable justifications\nthat explain the rationale behind each score. Our method builds on Group\nRelative Policy Optimization and incorporates two key innovations: (1)\nSub-score Dynamic Weighting, which adaptively prioritizes clinically\nchallenging error types based on live F1 statistics; and (2) Majority-Guided\nAdvantage Scaling, which adjusts policy gradient updates based on prompt\ndifficulty derived from sub-score agreement. Together, these components enable\nmore stable optimization and better alignment with expert clinical judgment.\nExperiments on the ReXVal benchmark show that RadReason surpasses all prior\noffline metrics and achieves parity with GPT-4-based evaluations, while\nremaining explainable, cost-efficient, and suitable for clinical deployment.\nCode will be released upon publication.", "AI": {"tldr": "RadReason is an evaluation framework for radiology reports that provides fine-grained sub-scores and human-readable justifications, improving clinical relevance and interpretability over existing methods.", "motivation": "There is a pressing need for clinically grounded and interpretable metrics in the evaluation of automatically generated radiology reports to improve real-world clinical workflows.", "method": "RadReason utilizes Group Relative Policy Optimization and introduces Sub-score Dynamic Weighting and Majority-Guided Advantage Scaling to enhance score evaluation and explainability.", "result": "RadReason demonstrates superior performance over all previous offline metrics on the ReXVal benchmark and matches GPT-4-based evaluations, while maintaining cost-efficiency and clinical applicability.", "conclusion": "RadReason provides a robust evaluation method for radiology reports, enhancing the interpretability and practical utility of automated assessments in clinical settings.", "key_contributions": ["Innovative evaluation framework for radiology reports", "Fine-grained sub-scores across clinically defined error types", "Human-readable justifications for evaluation scores"], "limitations": "", "keywords": ["radiology reports", "evaluation framework", "machine learning", "clinical workflows", "explainable AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.15471", "pdf": "https://arxiv.org/pdf/2508.15471.pdf", "abs": "https://arxiv.org/abs/2508.15471", "title": "SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning", "authors": ["Vedasamhitha Challapalli", "Konduru Venkat Sai", "Piyush Pratap Singh", "Rupesh Prasad", "Arvind Maurya", "Atul Singh"], "categories": ["cs.CL"], "comment": "10 pages, BDA Conference 2025", "summary": "Personalized marketing has emerged as a pivotal strategy for enhancing\ncustomer engagement and driving business growth. Academic and industry efforts\nhave predominantly focused on recommendation systems and personalized\nadvertisements. Nonetheless, this facet of personalization holds significant\npotential for increasing conversion rates and improving customer satisfaction.\nPrior studies suggest that well-executed personalization strategies can boost\nrevenue by up to 40 percent, underscoring the strategic importance of\ndeveloping intelligent, data-driven approaches for offer generation. This work\nintroduces SLM4Offer, a generative AI model for personalized offer generation,\ndeveloped by fine-tuning a pre-trained encoder-decoder language model,\nspecifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a\ncontrastive learning approach. SLM4Offer employs InfoNCE (Information\nNoise-Contrastive Estimation) loss to align customer personas with relevant\noffers in a shared embedding space. A key innovation in SLM4Offer lies in the\nadaptive learning behaviour introduced by contrastive loss, which reshapes the\nlatent space during training and enhances the model's generalizability. The\nmodel is fine-tuned and evaluated on a synthetic dataset designed to simulate\ncustomer behaviour and offer acceptance patterns. Experimental results\ndemonstrate a 17 percent improvement in offer acceptance rate over a supervised\nfine-tuning baseline, highlighting the effectiveness of contrastive objectives\nin advancing personalized marketing.", "AI": {"tldr": "This paper introduces SLM4Offer, a generative AI model designed for personalized offer generation that outperforms traditional methods using contrastive learning techniques.", "motivation": "The paper addresses the need for improved personalized marketing strategies that can enhance customer engagement and increase conversion rates, potentially boosting revenue significantly.", "method": "SLM4Offer is developed by fine-tuning Google's T5-Small model using a contrastive learning approach, incorporating InfoNCE loss to align customer personas with offers in a shared embedding space.", "result": "Experimental evaluations show that SLM4Offer achieves a 17% improvement in offer acceptance rates compared to a supervised fine-tuning baseline, indicating the effectiveness of the contrastive approach.", "conclusion": "The study concludes that the adaptive learning behavior from contrastive loss significantly enhances the model's generalizability for personalized marketing applications.", "key_contributions": ["Introduction of SLM4Offer, a generative AI model for personalized marketing.", "Use of contrastive learning to improve offer acceptance rates.", "Development of a synthetic dataset to simulate customer behaviour and offer acceptance patterns."], "limitations": "", "keywords": ["personalized marketing", "machine learning", "customer engagement", "AI", "generative models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.15474", "pdf": "https://arxiv.org/pdf/2508.15474.pdf", "abs": "https://arxiv.org/abs/2508.15474", "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "authors": ["Sai Sundaresan", "Harshita Chopra", "Atanu R. Sinha", "Koustava Goswami", "Nagasai Saketh Naidu", "Raghav Karan", "N Anushka"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.", "AI": {"tldr": "This paper investigates the performance of a small language model (LM) in capturing users' subjective browsing behaviors compared to larger models, introducing HeTLM for better alignment with individual preferences.", "motivation": "To challenge the notion that large language models are universally better suited for capturing diverse user behaviors and preferences in browsing activities.", "method": "The study employs clusterwise LM training with HeTLM, evaluating a small LM trained with a page-level tokenizer against various larger LM configurations.", "result": "A small LM shows superior performance in representing user browsing 'language', outperforming larger models; HeTLM also yields better performance consistency across diverse user behaviors.", "conclusion": "The findings suggest that smaller, more specialized models can effectively capture subjective user behaviors, providing better alignment compared to larger, generalized models.", "key_contributions": ["Introduction of HeTLM for training language models on user-specific behaviors", "Demonstration that a small LM can outperform larger models in specific tasks", "Evidence of improved performance consistency with heterogeneous parameters."], "limitations": "", "keywords": ["Language Models", "Human-Computer Interaction", "User Behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15475", "pdf": "https://arxiv.org/pdf/2508.15475.pdf", "abs": "https://arxiv.org/abs/2508.15475", "title": "Influence-driven Curriculum Learning for Pre-training on Limited Data", "authors": ["Loris Schoenegger", "Lukas Thoma", "Terra Blevins", "Benjamin Roth"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "9 pages", "summary": "Curriculum learning, a training technique where data is presented to the\nmodel in order of example difficulty (e.g., from simpler to more complex\ndocuments), has shown limited success for pre-training language models. In this\nwork, we investigate whether curriculum learning becomes competitive if we\nreplace conventional human-centered difficulty metrics with one that more\nclosely corresponds to example difficulty as observed during model training.\nSpecifically, we experiment with sorting training examples by their\n\\textit{training data influence}, a score which estimates the effect of\nindividual training examples on the model's output. Models trained on our\ncurricula are able to outperform ones trained in random order by over 10\npercentage points in benchmarks, confirming that curriculum learning is\nbeneficial for language model pre-training, as long as a more model-centric\nnotion of difficulty is adopted.", "AI": {"tldr": "This paper explores the effectiveness of curriculum learning for pre-training language models using a model-centric approach to difficulty metrics.", "motivation": "To determine if curriculum learning can enhance language model pre-training by using a model-centric view of difficulty rather than traditional human-centered metrics.", "method": "The authors experiment with sorting training examples based on their 'training data influence,' which measures the impact of training examples on the model’s outputs, in contrast to the random ordering typically used.", "result": "Models trained with the new curriculum approach demonstrated an improvement of over 10 percentage points on benchmark tests compared to those trained randomly.", "conclusion": "Adopting a model-centric notion of difficulty for curriculum learning significantly benefits language model pre-training.", "key_contributions": ["Introduces a model-centric difficulty metric for training examples", "Demonstrates substantial performance improvements using this metric", "Provides a new perspective on curriculum learning for language models"], "limitations": "", "keywords": ["curriculum learning", "language models", "training data influence"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2508.15478", "pdf": "https://arxiv.org/pdf/2508.15478.pdf", "abs": "https://arxiv.org/abs/2508.15478", "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version", "authors": ["Nghiem Thanh Pham", "Tung Kieu", "Duc-Manh Nguyen", "Son Ha Xuan", "Nghia Duong-Trung", "Danh Le-Phuoc"], "categories": ["cs.CL", "cs.CY", "cs.PF"], "comment": "24 pages. An extended version of \"SLM-Bench: A Comprehensive\n  Benchmark of Small Language Models on Environmental Impacts\" accepted at\n  EMNLP 2025", "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, yet a systematic evaluation of their performance and\nenvironmental impact remains lacking. We introduce SLM-Bench, the first\nbenchmark specifically designed to assess SLMs across multiple dimensions,\nincluding accuracy, computational efficiency, and sustainability metrics.\nSLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14\ndomains. The evaluation is conducted on 4 hardware configurations, providing a\nrigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench\nquantifies 11 metrics across correctness, computation, and consumption,\nenabling a holistic assessment of efficiency trade-offs. Our evaluation\nconsiders controlled hardware conditions, ensuring fair comparisons across\nmodels. We develop an open-source benchmarking pipeline with standardized\nevaluation protocols to facilitate reproducibility and further research. Our\nfindings highlight the diverse trade-offs among SLMs, where some models excel\nin accuracy while others achieve superior energy efficiency. SLM-Bench sets a\nnew standard for SLM evaluation, bridging the gap between resource efficiency\nand real-world applicability.", "AI": {"tldr": "SLM-Bench is introduced as a benchmark for evaluating Small Language Models across various dimensions, including accuracy and sustainability.", "motivation": "There is a lack of systematic evaluation for Small Language Models (SLMs) regarding performance and environmental impact.", "method": "SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets across 14 domains, comparing results on 4 hardware configurations and quantifying 11 metrics.", "result": "The evaluation shows diverse trade-offs among SLMs, revealing that some excel in accuracy while others are more energy efficient.", "conclusion": "SLM-Bench establishes a new evaluation standard for SLMs, promoting better understanding of their real-world applicability and resource efficiency.", "key_contributions": ["Introduction of SLM-Bench for evaluating SLMs", "Incorporation of environmental impact in NLP model assessment", "Development of an open-source benchmarking pipeline for reproducibility"], "limitations": "", "keywords": ["Small Language Models", "Benchmarking", "Environmental Impact", "NLP", "Efficiency"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.15483", "pdf": "https://arxiv.org/pdf/2508.15483.pdf", "abs": "https://arxiv.org/abs/2508.15483", "title": "HebID: Detecting Social Identities in Hebrew-language Political Text", "authors": ["Guy Mor-Lan", "Naama Rivlin-Angert", "Yael R. Kaplan", "Tamir Sheafer", "Shaul R. Shenhav"], "categories": ["cs.CL"], "comment": null, "summary": "Political language is deeply intertwined with social identities. While social\nidentities are often shaped by specific cultural contexts and expressed through\nparticular uses of language, existing datasets for group and identity detection\nare predominantly English-centric, single-label and focus on coarse identity\ncategories. We introduce HebID, the first multilabel Hebrew corpus for social\nidentity detection: 5,536 sentences from Israeli politicians' Facebook posts\n(Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities\n(e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We\nbenchmark multilabel and single-label encoders alongside 2B-9B-parameter\ngenerative LLMs, finding that Hebrew-tuned LLMs provide the best results\n(macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts\nand parliamentary speeches, evaluating differences in popularity, temporal\ntrends, clustering patterns, and gender-related variations in identity\nexpression. We utilize identity choices from a national public survey, enabling\na comparison between identities portrayed in elite discourse and the public's\nidentity priorities. HebID provides a comprehensive foundation for studying\nsocial identities in Hebrew and can serve as a model for similar research in\nother non-English political contexts.", "AI": {"tldr": "HebID is a multilabel Hebrew corpus introduced for social identity detection in political language, with benchmark results showing the effectiveness of Hebrew-tuned LLMs.", "motivation": "Current datasets for identity detection are primarily English-centric and lack nuanced categories. HebID aims to provide a linguistic resource for the study of social identities in Hebrew within political contexts.", "method": "Creation of a multilabel corpus consisting of 5,536 sentences from Israeli politicians' Facebook posts, annotated for twelve social identities, followed by benchmarking various ML models including generative LLMs to analyze identity expression.", "result": "Hebrew-tuned LLMs achieve the best performance with a macro-$F_1$ score of 0.74, and insights were gained regarding identity expressions in political discourse, including trends and gender variations.", "conclusion": "HebID serves not only as a foundation for Hebrew-based social identity research but also as a potential blueprint for similar studies in other linguistic contexts.", "key_contributions": ["Introduction of the first multilabel Hebrew corpus for social identity detection.", "Demonstrated the effectiveness of Hebrew-tuned LLMs in achieving high performance in identity classification.", "Enabled comprehensive analysis of identity expression in political discourse with insights into public and elite identity alignment."], "limitations": "", "keywords": ["social identity", "Hebrew", "political language", "multilabel corpus", "language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.15487", "pdf": "https://arxiv.org/pdf/2508.15487.pdf", "abs": "https://arxiv.org/abs/2508.15487", "title": "Dream 7B: Diffusion Large Language Models", "authors": ["Jiacheng Ye", "Zhihui Xie", "Lin Zheng", "Jiahui Gao", "Zirui Wu", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Dream 7B, the most powerful open diffusion large language model\nto date. Unlike autoregressive (AR) models that generate tokens sequentially,\nDream 7B employs discrete diffusion modeling to refine sequences in parallel\nthrough iterative denoising. Our model consistently outperforms existing\ndiffusion language models on general, mathematical, and coding tasks. Dream 7B\ndemonstrates superior planning abilities and inference flexibility, including\narbitrary-order generation, infilling capabilities, and tunable quality-speed\ntrade-offs. These results are achieved through simple yet effective training\ntechniques, including AR-based LLM initialization and context-adaptive\ntoken-level noise rescheduling. We release both Dream-Base and Dream-Instruct\nto facilitate further research in diffusion-based language modeling.", "AI": {"tldr": "Dream 7B is a highly powerful open diffusion language model that surpasses existing models in various tasks.", "motivation": "To develop a more capable language model using discrete diffusion modeling techniques that improve generation quality and flexibility.", "method": "Dream 7B employs discrete diffusion modeling for parallel sequence refinement through iterative denoising, initialized with AR-based LLM techniques and enhanced by context-adaptive token-level noise rescheduling.", "result": "The model outperforms existing diffusion language models on general tasks, mathematics, and coding, showcasing advanced planning abilities and flexible inference.", "conclusion": "Dream 7B releases both Dream-Base and Dream-Instruct versions to support ongoing research in diffusion-based language modeling.", "key_contributions": ["Introduction of discrete diffusion modeling for language tasks", "Demonstrated planning abilities and inference flexibility", "Release of Dream-Base and Dream-Instruct for further research"], "limitations": "", "keywords": ["Diffusion Models", "Language Models", "Generative Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.15524", "pdf": "https://arxiv.org/pdf/2508.15524.pdf", "abs": "https://arxiv.org/abs/2508.15524", "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech", "authors": ["Naama Rivlin-Angert", "Guy Mor-Lan"], "categories": ["cs.CL"], "comment": null, "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse.", "AI": {"tldr": "This study explores political delegitimization discourse using a large Hebrew-language corpus. A two-stage classification pipeline identifies instances of PDD and its characteristics across different platforms and time spans.", "motivation": "To understand and analyze the patterns and prevalence of political delegitimization discourse in democratic contexts.", "method": "A two-stage classification pipeline combining fine-tuned encoder models and decoder LLMs was used to classify instances of political delegitimization discourse from a curated corpus of speech and social media data.", "result": "The best model (DictaLM 2.0) achieved an F$_1$ score of 0.74 for binary PDD detection and 0.67 for classification of delegitimization characteristics. A notable increase in PDD was observed over three decades, with varying intensity across genders, political affiliations, and platforms.", "conclusion": "Automated analysis of political delegitimization discourse is feasible and valuable for understanding trends in democratic discourse.", "key_contributions": ["Introduction of a novel Hebrew-language corpus for PDD analysis.", "Development of DictaLM 2.0 for effective PDD detection and classification.", "Insights into the trends and characteristics of PDD across different political contexts."], "limitations": "", "keywords": ["political discourse", "delegitimization", "natural language processing", "machine learning", "social media"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2508.15526", "pdf": "https://arxiv.org/pdf/2508.15526.pdf", "abs": "https://arxiv.org/abs/2508.15526", "title": "SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking", "authors": ["Xiangyang Zhu", "Yuan Tian", "Chunyi Li", "Kaiwei Zhang", "Wei Sun", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": "Code and dataset are available at\n  https://github.com/yangyangyang127/SafetyFlow", "summary": "The rapid proliferation of large language models (LLMs) has intensified the\nrequirement for reliable safety evaluation to uncover model vulnerabilities. To\nthis end, numerous LLM safety evaluation benchmarks are proposed. However,\nexisting benchmarks generally rely on labor-intensive manual curation, which\ncauses excessive time and resource consumption. They also exhibit significant\nredundancy and limited difficulty. To alleviate these problems, we introduce\nSafetyFlow, the first agent-flow system designed to automate the construction\nof LLM safety benchmarks. SafetyFlow can automatically build a comprehensive\nsafety benchmark in only four days without any human intervention by\norchestrating seven specialized agents, significantly reducing time and\nresource cost. Equipped with versatile tools, the agents of SafetyFlow ensure\nprocess and cost controllability while integrating human expertise into the\nautomatic pipeline. The final constructed dataset, SafetyFlowBench, contains\n23,446 queries with low redundancy and strong discriminative power. Our\ncontribution includes the first fully automated benchmarking pipeline and a\ncomprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on\nour dataset and conduct extensive experiments to validate our efficacy and\nefficiency.", "AI": {"tldr": "SafetyFlow automates the construction of LLM safety benchmarks, significantly reducing time and resource consumption while improving dataset quality.", "motivation": "To address the inefficiencies and limitations of existing LLM safety evaluation benchmarks that rely on manual curation.", "method": "SafetyFlow orchestrates seven specialized agents to automatically construct a safety benchmark, producing SafetyFlowBench with a diverse set of queries.", "result": "SafetyFlowBench contains 23,446 queries with low redundancy and high discriminative power, improving the efficiency of safety evaluations for LLMs.", "conclusion": "The study demonstrates the efficacy of fully automated safety benchmarking for LLMs, validated by extensive experimentation on 49 advanced LLMs.", "key_contributions": ["First fully automated benchmarking pipeline for LLM safety evaluation.", "Comprehensive safety benchmark dataset constructed: SafetyFlowBench.", "Reduction of time and resources in benchmark creation."], "limitations": "", "keywords": ["large language models", "safety evaluation", "automation", "benchmarking", "AI safety"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15617", "pdf": "https://arxiv.org/pdf/2508.15617.pdf", "abs": "https://arxiv.org/abs/2508.15617", "title": "Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing", "authors": ["Ishaan Bhola", "Mukunda NS", "Sravanth Kurmala", "Harsh Nandwani", "Arihant Jain"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in text generation; however, these\ncreative elements require heavy computation and are accompanied by a steep\ncost. Especially for targeted applications such as sales and marketing\noutreach, these costs are far from feasible. This paper introduces the concept\nof \"Trained Miniatures\" - Small Language Models(SLMs) fine-tuned for specific,\nhigh-value applications, generating similar domain-specific responses for a\nfraction of the cost.", "AI": {"tldr": "This paper discusses 'Trained Miniatures', which are Small Language Models fine-tuned for specific applications to reduce computational costs while maintaining performance.", "motivation": "The need to reduce high computational costs associated with large language models for specific applications like sales and marketing.", "method": "Introduce and define Trained Miniatures as Small Language Models (SLMs) that are fine-tuned for targeted high-value applications.", "result": "Trained Miniatures generate domain-specific responses at a fraction of the computation and cost compared to larger models.", "conclusion": "Trained Miniatures provide a viable alternative to large language models, making advanced text generation accessible for specific applications.", "key_contributions": ["Definition of Trained Miniatures", "Demonstration of cost-effectiveness of SLMs for targeted applications", "Potential applications in sales and marketing outreach"], "limitations": "", "keywords": ["Small Language Models", "Cost Reduction", "Domain-Specific Responses"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.15648", "pdf": "https://arxiv.org/pdf/2508.15648.pdf", "abs": "https://arxiv.org/abs/2508.15648", "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models", "authors": ["Peng Ding", "Wen Sun", "Dailin Li", "Wei Zou", "Jiaming Wang", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025, 15 pages, 4 figures, 6 tables", "summary": "Large Language Models (LLMs) excel at various natural language processing\ntasks but remain vulnerable to jailbreaking attacks that induce harmful content\ngeneration. In this paper, we reveal a critical safety inconsistency: LLMs can\nmore effectively identify harmful requests as discriminators than defend\nagainst them as generators. This insight inspires us to explore aligning the\nmodel's inherent discrimination and generation capabilities. To this end, we\npropose SDGO (Self-Discrimination-Guided Optimization), a reinforcement\nlearning framework that leverages the model's own discrimination capabilities\nas a reward signal to enhance generation safety through iterative\nself-improvement. Our method does not require any additional annotated data or\nexternal models during the training phase. Extensive experiments demonstrate\nthat SDGO significantly improves model safety compared to both prompt-based and\ntraining-based baselines while maintaining helpfulness on general benchmarks.\nBy aligning LLMs' discrimination and generation capabilities, SDGO brings\nrobust performance against out-of-distribution (OOD) jailbreaking attacks. This\nalignment achieves tighter coupling between these two capabilities, enabling\nthe model's generation capability to be further enhanced with only a small\namount of discriminative samples. Our code and datasets are available at\nhttps://github.com/NJUNLP/SDGO.", "AI": {"tldr": "This paper introduces SDGO, a reinforcement learning framework designed to enhance the safety of Large Language Models by aligning their discrimination and generation capabilities against harmful content generation.", "motivation": "The paper addresses the issue of jailbreaking attacks in Large Language Models (LLMs), revealing a critical inconsistency where LLMs can identify harmful requests better than they can defend against them.", "method": "The authors propose a framework called SDGO (Self-Discrimination-Guided Optimization), which uses the model's own discrimination capabilities as a reward signal in a reinforcement learning setup to iteratively improve generation safety.", "result": "Experiments show that SDGO significantly enhances model safety against out-of-distribution jailbreaking attacks while maintaining helpfulness on general benchmarks compared to existing baselines.", "conclusion": "By aligning the discrimination and generation capabilities, SDGO achieves a tight coupling that improves overall model robustness with minimal additional discriminative samples needed.", "key_contributions": ["Introduction of SDGO, a novel reinforcement learning framework for improving LLM safety.", "Demonstrating significant improvements in model safety compared to prompt-based and training-based baselines.", "Providing code and datasets for reproducibility and further research."], "limitations": "The paper does not mention any specific limitations, but the effectiveness of SDGO in varied real-world scenarios remains to be tested.", "keywords": ["Large Language Models", "Reinforcement Learning", "Safety", "Discrimination", "Jailbreaking attacks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.15658", "pdf": "https://arxiv.org/pdf/2508.15658.pdf", "abs": "https://arxiv.org/abs/2508.15658", "title": "Benchmarking Computer Science Survey Generation", "authors": ["Weihang Su", "Anzhe Xie", "Qingyao Ai", "Jianming Long", "Jiaxin Mao", "Ziyi Ye", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Scientific survey articles play a vital role in summarizing research\nprogress, yet their manual creation is becoming increasingly infeasible due to\nthe rapid growth of academic literature. While large language models (LLMs)\noffer promising capabilities for automating this process, progress in this area\nis hindered by the absence of standardized benchmarks and evaluation protocols.\nTo address this gap, we introduce SurGE (Survey Generation Evaluation), a new\nbenchmark for evaluating scientific survey generation in the computer science\ndomain. SurGE consists of (1) a collection of test instances, each including a\ntopic description, an expert-written survey, and its full set of cited\nreferences, and (2) a large-scale academic corpus of over one million papers\nthat serves as the retrieval pool. In addition, we propose an automated\nevaluation framework that measures generated surveys across four dimensions:\ninformation coverage, referencing accuracy, structural organization, and\ncontent quality. Our evaluation of diverse LLM-based approaches shows that\nsurvey generation remains highly challenging, even for advanced self-reflection\nframeworks. These findings highlight the complexity of the task and the\nnecessity for continued research. We have open-sourced all the code, data, and\nmodels at: https://github.com/oneal2000/SurGE", "AI": {"tldr": "SurGE is a benchmark for evaluating automated scientific survey generation using large language models, addressing the lack of standardized evaluation in this area.", "motivation": "The increasing volume of academic literature makes manual creation of survey articles infeasible, prompting the need for automated solutions.", "method": "SurGE includes a collection of test instances with topic descriptions and expert-written surveys, alongside a large academic corpus of over one million papers for retrieval. An evaluation framework assesses generated surveys based on information coverage, referencing accuracy, structural organization, and content quality.", "result": "Evaluation of various LLM-based approaches indicates that generating high-quality surveys is still a complex challenge, necessitating further research.", "conclusion": "The study emphasizes the difficulties in automated survey generation and the need for ongoing advancements in the field.", "key_contributions": ["Introduction of the SurGE benchmark for scientific survey generation", "Provision of a large-scale academic corpus for research", "Development of an automated evaluation framework for survey quality assessment"], "limitations": "The complexity of survey generation remains a significant challenge, indicating that the current state of LLMs is insufficient for reliable outcomes.", "keywords": ["survey generation", "large language models", "benchmarking", "scientific literature", "automated evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15709", "pdf": "https://arxiv.org/pdf/2508.15709.pdf", "abs": "https://arxiv.org/abs/2508.15709", "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation", "authors": ["Yifei Wang", "Feng Xiong", "Yong Wang", "Linjing Li", "Xiangxiang Chu", "Daniel Dajun Zeng"], "categories": ["cs.CL"], "comment": "EMNLP2025 Main", "summary": "Positional bias (PB), manifesting as non-uniform sensitivity across different\ncontextual locations, significantly impairs long-context comprehension and\nprocessing capabilities. While prior work seeks to mitigate PB through\nmodifying the architectures causing its emergence, significant PB still\npersists. To address PB effectively, we introduce \\textbf{Pos2Distill}, a\nposition to position knowledge distillation framework. Pos2Distill transfers\nthe superior capabilities from advantageous positions to less favorable ones,\nthereby reducing the huge performance gaps. The conceptual principle is to\nleverage the inherent, position-induced disparity to counteract the PB itself.\nWe identify distinct manifestations of PB under \\textbf{\\textsc{r}}etrieval and\n\\textbf{\\textsc{r}}easoning paradigms, thereby designing two specialized\ninstantiations: \\emph{Pos2Distill-R\\textsuperscript{1}} and\n\\emph{Pos2Distill-R\\textsuperscript{2}} respectively, both grounded in this\ncore principle. By employing the Pos2Distill approach, we achieve enhanced\nuniformity and significant performance gains across all contextual positions in\nlong-context retrieval and reasoning tasks. Crucially, both specialized systems\nexhibit strong cross-task generalization mutually, while achieving superior\nperformance on their respective tasks.", "AI": {"tldr": "Pos2Distill is a framework designed to mitigate positional bias in long-context comprehension by transferring knowledge from advantageous to less favorable context positions, leading to improved performance in retrieval and reasoning tasks.", "motivation": "Positional bias significantly impairs comprehension and processing in long-context situations, necessitating effective interventions.", "method": "The Pos2Distill framework employs knowledge distillation from advantageous positions to counteract positional bias.", "result": "Enhanced uniformity in performance across all contextual positions was achieved, along with strong generalization across tasks.", "conclusion": "Pos2Distill can effectively reduce performance gaps caused by positional bias in long-context tasks, leading to superior outcomes in retrieval and reasoning.", "key_contributions": ["Introduction of the Pos2Distill framework for addressing positional bias.", "Development of specialized systems Pos2Distill-R1 and Pos2Distill-R2 for distinct tasks.", "Demonstration of strong cross-task generalization and performance improvement."], "limitations": "", "keywords": ["positional bias", "knowledge distillation", "long-context comprehension", "retrieval", "reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.15711", "pdf": "https://arxiv.org/pdf/2508.15711.pdf", "abs": "https://arxiv.org/abs/2508.15711", "title": "Stemming -- The Evolution and Current State with a Focus on Bangla", "authors": ["Abhijit Paul", "Mashiat Amin Farin", "Sharif Md. Abdullah", "Ahmedul Kabir", "Zarif Masud", "Shebuti Rayana"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Bangla, the seventh most widely spoken language worldwide with 300 million\nnative speakers, faces digital under-representation due to limited resources\nand lack of annotated datasets. Stemming, a critical preprocessing step in\nlanguage analysis, is essential for low-resource, highly-inflectional languages\nlike Bangla, because it can reduce the complexity of algorithms and models by\nsignificantly reducing the number of words the algorithm needs to consider.\nThis paper conducts a comprehensive survey of stemming approaches, emphasizing\nthe importance of handling morphological variants effectively. While exploring\nthe landscape of Bangla stemming, it becomes evident that there is a\nsignificant gap in the existing literature. The paper highlights the\ndiscontinuity from previous research and the scarcity of accessible\nimplementations for replication. Furthermore, it critiques the evaluation\nmethodologies, stressing the need for more relevant metrics. In the context of\nBangla's rich morphology and diverse dialects, the paper acknowledges the\nchallenges it poses. To address these challenges, the paper suggests directions\nfor Bangla stemmer development. It concludes by advocating for robust Bangla\nstemmers and continued research in the field to enhance language analysis and\nprocessing.", "AI": {"tldr": "This paper surveys stemming approaches for the Bangla language, addressing gaps in literature and suggesting directions for development.", "motivation": "Bangla is under-represented in digital resources, necessitating effective preprocessing methods like stemming to aid language analysis.", "method": "Comprehensive survey of existing stemming techniques for Bangla, highlighting morphologic challenges and evaluating current methodologies.", "result": "Identifies significant gaps in existing literature and critiques evaluation methods, advocating for improved Bengali stemmers.", "conclusion": "Robust Bangla stemmers are essential for better language analysis and further research is necessary in this area.", "key_contributions": ["Comprehensive survey of Bangla stemming approaches", "Identification of gaps in existing research", "Suggestions for future research directions in Bangla stemmer development"], "limitations": "Limited accessibility of implementations for replication and relevant performance metrics in existing works.", "keywords": ["Bangla", "Stemming", "Natural Language Processing", "Morphological Analysis", "Language Processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.15721", "pdf": "https://arxiv.org/pdf/2508.15721.pdf", "abs": "https://arxiv.org/abs/2508.15721", "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models", "authors": ["Xinyi Ling", "Hanwen Du", "Zhihui Zhu", "Xia Ning"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25.", "AI": {"tldr": "Introducing EcomMMMU, a dataset for multimodal product understanding in e-commerce, revealing that images may not always enhance performance in MLLM tasks.", "motivation": "To systematically examine whether product images on e-commerce platforms enhance understanding or degrade performance, given the limitations of existing datasets.", "method": "Introduction of EcomMMMU, with 406,190 samples and nearly 9 million images, designed for multimodal language tasks. Analysis and benchmarking of multimodal large language models' capabilities.", "result": "Product images do not consistently improve performance and can sometimes degrade it; MLLMs may struggle with effectively leveraging visual content for specific tasks.", "conclusion": "Proposing SUMEI, a method to predict visual utilities of images before downstream task utilization shows effectiveness and robustness in enhancing multimodal understanding.", "key_contributions": ["Introduction of EcomMMMU dataset with extensive multimodal samples", "Analysis shows images can degrade MLLM performance", "Development of SUMEI for better usage of visual content"], "limitations": "Only focusing on production images and their role in performance; other potential factors affecting performance are not addressed.", "keywords": ["e-commerce", "multimodal", "dataset", "large language models", "visual content"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.15746", "pdf": "https://arxiv.org/pdf/2508.15746.pdf", "abs": "https://arxiv.org/abs/2508.15746", "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning", "authors": ["Qiaoyu Zheng", "Yuze Sun", "Chaoyi Wu", "Weike Zhao", "Pengcheng Qiu", "Yongguo Yu", "Kun Sun", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "35 pages, 5 figures, 3 tables", "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.", "AI": {"tldr": "Deep-DxSearch is an end-to-end RAG system using reinforcement learning to improve medical diagnosis accuracy by integrating a large-scale medical retrieval corpus with large language models.", "motivation": "The paper addresses the limitations in accurate medical diagnosis due to knowledge gaps and hallucinations in medical LLMs, aiming to enhance retrieval-augmented reasoning.", "method": "An end-to-end agentic RAG system was constructed, framing the LLM as the core agent interacting with a medical retrieval corpus, with reinforcement learning employed for training against tailored rewards focusing on diagnostic accuracy and reasoning quality.", "result": "Deep-DxSearch consistently outperformed existing models like GPT-4o and DeepSeek-R1 in diagnostic accuracy across various scenarios, confirming the effectiveness of the proposed approach.", "conclusion": "The study concludes that Deep-DxSearch's unique reinforcement learning framework and effective retrieval corpus significantly enhance diagnostic reasoning in medical applications, providing insights for clinicians.", "key_contributions": ["Introduction of Deep-DxSearch, an agentic RAG system for medical diagnosis", "Use of reinforcement learning for improving retrieval-augmented reasoning", "Demonstrated significant improvements in diagnostic accuracy over existing baselines."], "limitations": "The research is limited to the specific data centers utilized, and further exploration might be needed on diverse datasets for broader applicability.", "keywords": ["medical diagnosis", "large language models", "reinforcement learning", "retrieval-augmented reasoning", "health informatics"], "importance_score": 9, "read_time_minutes": 35}}
{"id": "2508.15754", "pdf": "https://arxiv.org/pdf/2508.15754.pdf", "abs": "https://arxiv.org/abs/2508.15754", "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis", "authors": ["Yufeng Zhao", "Junnan Liu", "Hongwei Liu", "Dongsheng Zhu", "Yuan Shen", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, working in progress", "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.", "AI": {"tldr": "Tool-Integrated Reasoning (TIR) improves Large Language Models' (LLMs) reasoning in mathematical and non-mathematical tasks while enhancing efficiency.", "motivation": "To evaluate the effects of Tool-Integrated Reasoning (TIR) on the reasoning capabilities of LLMs, particularly in tasks requiring precision.", "method": "Introduced ReasonZoo, a benchmark with nine reasoning categories, and proposed new metrics (PAC and AUC-PCC) for assessing reasoning efficiency.", "result": "TIR-enabled models consistently outperformed non-TIR models in various reasoning tasks, demonstrating improvements in both performance and efficiency metrics.", "conclusion": "TIR represents a domain-general enhancement for LLMs, potentially advancing capabilities in complex reasoning tasks.", "key_contributions": ["Introduction of the ReasonZoo benchmark with nine reasoning categories", "Development of two novel metrics: Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC)", "Demonstration of TIR's impact on improving LLM performance and efficiency in reasoning tasks."], "limitations": "", "keywords": ["Tool-Integrated Reasoning", "Large Language Models", "Reasoning Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.15760", "pdf": "https://arxiv.org/pdf/2508.15760.pdf", "abs": "https://arxiv.org/abs/2508.15760", "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries", "authors": ["Ming Yin", "Dinghan Shen", "Silei Xu", "Jianbing Han", "Sixun Dong", "Mian Zhang", "Yebowen Hu", "Shujian Liu", "Simin Ma", "Song Wang", "Sathish Reddy Indurthi", "Xun Wang", "Yiran Chen", "Kaiqiang Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.", "AI": {"tldr": "LiveMCP-101 is a benchmark for assessing AI agents' ability to use multiple tools for complex tasks, revealing significant challenges in execution success rates and tool orchestration.", "motivation": "To bridge the gap in benchmarking AI agents' effectiveness in solving multi-step tasks using diverse tools under the Model Context Protocol (MCP).", "method": "Introduced LiveMCP-101, a benchmark of 101 real-world queries that require coordinated use of multiple MCP tools and a novel evaluation approach using ground-truth execution plans.", "result": "Even advanced LLMs showed below 60% success rates in coordinating tool use, indicating major challenges in execution and tool orchestration.", "conclusion": "LiveMCP-101 sets a new standard for evaluating AI agents' capabilities in realistic scenarios, highlighting the need for advancements in model performance and tool orchestration.", "key_contributions": ["Development of LiveMCP-101 benchmark for AI agent evaluation", "Introduction of ground-truth execution plans for better assessment", "Identification of distinct failure modes in current models."], "limitations": "", "keywords": ["AI agents", "tool integration", "benchmarking", "Model Context Protocol", "LiveMCP-101"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2404.11916", "pdf": "https://arxiv.org/pdf/2404.11916.pdf", "abs": "https://arxiv.org/abs/2404.11916", "title": "Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time", "authors": ["Nakyeong Yang", "Jiwon Moon", "Junseok Kim", "Yunah Jang", "Kyomin Jung"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to CIKM 2025", "summary": "Enabled by large-scale text corpora with huge parameters, pre-trained\nlanguage models operate as multi-task experts using a single model\narchitecture. However, recent studies have revealed that certain neurons play\ndisproportionately important roles in solving specific tasks, suggesting that\ntask-relevant substructures can be isolated and selectively activated for each\ntask. Therefore, we introduce Decomposition of Experts (DoE), a novel framework\nthat dynamically identifies and activates task-specific experts within a\nlanguage model to reduce inference cost without sacrificing accuracy. We first\ndefine a task expert as a set of parameters that significantly influence the\nperformance of a specific task and propose a four-step unplug-and-play process:\n(1) receiving a user request, (2) identifying the corresponding task expert,\n(3) performing inference using the expert-localized model, and (4) restoring\nthe original model and waiting for the next task. Using attribution methods and\nprompt tuning, DoE isolates task-relevant neurons, minimizing computational\noverhead while maintaining task performance. We assume a setting where a\nlanguage model receives user requests from five widely used natural language\nunderstanding benchmarks, processing one task at a time. In this setup, we\ndemonstrate that DoE achieves up to a x1.73 inference speed-up with a 65%\npruning rate, without compromising accuracy. Comparisons with various task\nexpert localization methods reveal that DoE effectively identifies task\nexperts, while ablation studies validate the importance of its components.\nAdditionally, we analyze the effects of batch size, token count, and layer\ntypes on inference speed-up, providing practical insights for adopting DoE. The\nproposed framework is both practical and scalable, applicable to any\ntransformer-based architecture, offering a robust solution for efficient\ntask-specific inference.", "AI": {"tldr": "This paper introduces Decomposition of Experts (DoE), a framework for dynamically activating task-specific neurons in language models to improve inference efficiency without sacrificing accuracy.", "motivation": "To address the inefficiencies of large pre-trained language models that use all their parameters for every task, leading to increased inference costs.", "method": "The DoE framework includes a four-step process: receiving a user request, identifying the task expert, performing inference with the expert-localized model, and restoring the original model.", "result": "DoE achieved up to 1.73x speed-up in inference with a 65% pruning rate, without losing accuracy compared to traditional methods.", "conclusion": "DoE provides a practical and scalable solution for efficient task-specific inference in transformer-based architectures.", "key_contributions": ["Introduction of the DoE framework for task-specific expert activation.", "Demonstrated inference speed-up and pruning efficiency without accuracy loss.", "Practical insights for optimizing inference with varying batch sizes and configurations."], "limitations": "", "keywords": ["Decomposition of Experts", "task-specific models", "inference optimization", "language models", "natural language understanding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.10885", "pdf": "https://arxiv.org/pdf/2406.10885.pdf", "abs": "https://arxiv.org/abs/2406.10885", "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions", "authors": ["Weiqi Wang", "Tianqing Fang", "Haochen Shi", "Baixuan Xu", "Wenxuan Ding", "Liyu Zhang", "Wei Fan", "Jiaxin Bai", "Haoran Li", "Xin Liu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Findings of EMNLP 2025", "summary": "Conceptualization, a fundamental element of human cognition, plays a pivotal\nrole in human generalizable reasoning. Generally speaking, it refers to the\nprocess of sequentially abstracting specific instances into higher-level\nconcepts and then forming abstract knowledge that can be applied in unfamiliar\nor novel situations. This enhances models' inferential capabilities and\nsupports the effective transfer of knowledge across various domains. Despite\nits significance, the broad nature of this term has led to inconsistencies in\nunderstanding conceptualization across various works, as there exists different\ntypes of instances that can be abstracted in a wide variety of ways. There is\nalso a lack of a systematic overview that comprehensively examines existing\nworks on the definition, execution, and application of conceptualization to\nenhance reasoning tasks. In this paper, we address these gaps by first\nproposing a categorization of different types of conceptualizations into four\nlevels based on the types of instances being conceptualized, in order to\nclarify the term and define the scope of our work. Then, we present the first\ncomprehensive survey of over 150 papers, surveying various definitions,\nresources, methods, and downstream applications related to conceptualization\ninto a unified taxonomy, with a focus on the entity and event levels.\nFurthermore, we shed light on potential future directions in this field and\nhope to garner more attention from the community.", "AI": {"tldr": "This paper proposes a categorized taxonomy of conceptualization types and presents a comprehensive survey of existing research on conceptualization in reasoning tasks.", "motivation": "To address the inconsistencies in understanding conceptualization and its significance in enhancing reasoning tasks, given its broad nature and varied definitions.", "method": "The paper categorizes different types of conceptualizations into four levels based on instance types and surveys over 150 papers to unify definitions, resources, methods, and applications in a taxonomy.", "result": "The survey identifies various definitions and applications of conceptualization, focusing particularly on entity and event levels, and proposes a framework for future research in the field.", "conclusion": "The paper concludes with a call for more attention and research on conceptualization in reasoning tasks, providing a comprehensive overview that can guide future exploration.", "key_contributions": ["Categorization of conceptualization into four levels", "First comprehensive survey of over 150 papers", "Unified taxonomy of definitions and applications related to conceptualization."], "limitations": "The paper does not provide new empirical results but rather focuses on synthesizing existing literature.", "keywords": ["Conceptualization", "Cognition", "Reasoning", "Taxonomy", "Survey"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2410.03730", "pdf": "https://arxiv.org/pdf/2410.03730.pdf", "abs": "https://arxiv.org/abs/2410.03730", "title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs", "authors": ["Mehdi Ali", "Michael Fromm", "Klaudia Thellmann", "Jan Ebert", "Alexander Arno Weber", "Richard Rutmann", "Charvi Jain", "Max Lübbering", "Daniel Steinigen", "Johannes Leveling", "Katrin Klug", "Jasper Schulze Buschhoff", "Lena Jurkschat", "Hammam Abdelwahab", "Benny Jörg Stein", "Karl-Heinz Sylla", "Pavel Denisov", "Nicolo' Brandizzi", "Qasid Saleem", "Anirban Bhowmick", "Lennard Helmer", "Chelsea John", "Pedro Ortiz Suarez", "Malte Ostendorff", "Alex Jude", "Lalith Manjunath", "Samuel Weinbach", "Carolin Penke", "Oleg Filatov", "Fabio Barth", "Paramita Mirza", "Lucas Weber", "Ines Wendler", "Rafet Sifa", "Fabian Küch", "Andreas Herten", "René Jäkel", "Georg Rehm", "Stefan Kesselheim", "Joachim Köhler", "Nicolas Flores-Herr"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct,\ndesigned to embrace Europe's linguistic diversity by supporting all 24 official\nlanguages of the European Union. Trained on a dataset comprising around 60%\nnon-English data and utilizing a custom multilingual tokenizer, our models\naddress the limitations of existing LLMs that predominantly focus on English or\na few high-resource languages. We detail the models' development principles,\ni.e., data composition, tokenizer optimization, and training methodologies. The\nmodels demonstrate strong performance across multilingual benchmarks, as\nevidenced by their performance on European versions of ARC, HellaSwag, and\nTruthfulQA.", "AI": {"tldr": "The paper introduces two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed for all 24 EU languages to enhance linguistic diversity.", "motivation": "To address the predominance of English in existing LLMs and better support the linguistic diversity of Europe.", "method": "The models were trained on a dataset with 60% non-English data using a custom multilingual tokenizer, focusing on data composition and training methodologies.", "result": "The models show strong performance on multilingual benchmarks including ARC, HellaSwag, and TruthfulQA.", "conclusion": "Teuken LLMs effectively cater to Europe's linguistic needs, outperforming existing models in multilingual tasks.", "key_contributions": ["Introduction of two multilingual LLMs supporting all EU languages", "Custom multilingual tokenizer for better performance", "Strong results on standardized multilingual benchmarks"], "limitations": "", "keywords": ["multilingual", "LLM", "European languages", "tokenization", "machine learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.15186", "pdf": "https://arxiv.org/pdf/2410.15186.pdf", "abs": "https://arxiv.org/abs/2410.15186", "title": "Fine-tuning foundational models to code diagnoses from veterinary health records", "authors": ["Mayla R. Boguslav", "Adam Kiehl", "David Kott", "G. Joseph Strecker", "Tracy Webb", "Nadia Saklou", "Terri Ward", "Michael Kirby"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "26 pages, 5 figures", "summary": "Veterinary medical records represent a large data resource for application to\nveterinary and One Health clinical research efforts. Use of the data is limited\nby interoperability challenges including inconsistent data formats and data\nsiloing. Clinical coding using standardized medical terminologies enhances the\nquality of medical records and facilitates their interoperability with\nveterinary and human health records from other sites. Previous studies, such as\nDeepTag and VetTag, evaluated the application of Natural Language Processing\n(NLP) to automate veterinary diagnosis coding, employing long short-term memory\n(LSTM) and transformer models to infer a subset of Systemized Nomenclature of\nMedicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical\nnotes. This study expands on these efforts by incorporating all 7,739 distinct\nSNOMED-CT diagnosis codes recognized by the Colorado State University (CSU)\nVeterinary Teaching Hospital (VTH) and by leveraging the increasing\navailability of pre-trained language models (LMs). 13 freely-available\npre-trained LMs were fine-tuned on the free-text notes from 246,473\nmanually-coded veterinary patient visits included in the CSU VTH's electronic\nhealth records (EHRs), which resulted in superior performance relative to\nprevious efforts. The most accurate results were obtained when expansive\nlabeled data were used to fine-tune relatively large clinical LMs, but the\nstudy also showed that comparable results can be obtained using more limited\nresources and non-clinical LMs. The results of this study contribute to the\nimprovement of the quality of veterinary EHRs by investigating accessible\nmethods for automated coding and support both animal and human health research\nby paving the way for more integrated and comprehensive health databases that\nspan species and institutions.", "AI": {"tldr": "This study enhances the coding of veterinary medical records using NLP techniques and pre-trained language models to improve interoperability and quality of veterinary EHRs.", "motivation": "To address interoperability challenges in veterinary medical records that limit data usability for clinical research.", "method": "Fine-tuning of 13 pre-trained language models on a large dataset of veterinary patient visits to automate coding of SNOMED-CT diagnosis codes.", "result": "The fine-tuned models achieved superior performance in coding accuracy, even with limited resources, compared to previous models.", "conclusion": "The study advances the automation of coding in veterinary EHRs, which can benefit animal and human health research through better integration of health data.", "key_contributions": ["Incorporation of all SNOMED-CT diagnosis codes by CSU VTH", "Use of 13 pre-trained language models for fine-tuning", "Demonstrated methods for automated coding with both large and limited data resources."], "limitations": "The study is region-specific to CSU and may not generalize to all veterinary settings.", "keywords": ["veterinary", "NLP", "clinical coding", "SNOMED-CT", "electronic health records"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2501.00712", "pdf": "https://arxiv.org/pdf/2501.00712.pdf", "abs": "https://arxiv.org/abs/2501.00712", "title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding", "authors": ["Jiajun Zhu", "Peihao Wang", "Ruisi Cai", "Jason D. Lee", "Pan Li", "Zhangyang Wang"], "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": "ICML 2025", "summary": "Transformers rely on both content-based and position-based addressing\nmechanisms to make predictions, but existing positional encoding techniques\noften diminish the effectiveness of position-based addressing. Many current\nmethods enforce rigid patterns in attention maps, limiting the ability to model\nlong-range dependencies and adapt to diverse tasks. Additionally, most\npositional encodings are learned as general biases, lacking the specialization\nrequired for different instances within a dataset. To address this, we propose\ncon\\textbf{T}extualized equivari\\textbf{A}nt \\textbf{P}osition\n\\textbf{E}ncoding (\\textbf{TAPE}), a novel framework that enhances positional\nembeddings by incorporating sequence content across layers. TAPE introduces\ndynamic, context-aware positional encodings, overcoming the constraints of\ntraditional fixed patterns. We show that TAPE can provably facilitate LLM\nreasoning ability by emulating a broader class of algorithms. By enforcing\npermutation and orthogonal equivariance, TAPE ensures the stability of\npositional encodings during updates, improving long-context ability. Our method\ncan be easily integrated into pre-trained transformers, offering\nparameter-efficient fine-tuning with minimal overhead. Extensive experiments\nshow that TAPE achieves superior performance in language modeling, arithmetic\nreasoning, and long-context retrieval tasks compared to existing positional\nembedding techniques. Code is available at https://github.com/VITA-Group/TAPE.", "AI": {"tldr": "TAPE is a novel framework for enhancing positional embeddings in transformers by incorporating sequence content across layers, enabling dynamic and context-aware positional encodings that improve reasoning and long-context capability.", "motivation": "Existing positional encoding methods in transformers limit modeling flexibility and long-range dependency relationships due to their rigid patterns and lack of adaptability to specific tasks.", "method": "TAPE introduces contextualized equivariant position encoding that allows for dynamic, context-aware adjustments to positional encodings across transformer layers, ensuring stability during updates and improving the model's long-context capabilities.", "result": "TAPE outperforms traditional positional encoding methods in several tasks, including language modeling, arithmetic reasoning, and long-context retrieval, as demonstrated through extensive experimental evaluations.", "conclusion": "TAPE provides a parameter-efficient mechanism to enhance pre-trained transformers, facilitating better performance in a range of tasks requiring long-context understanding and reasoning abilities without significant overhead.", "key_contributions": ["Introduction of dynamic, context-aware positional encodings", "Provable enhancement of LLM reasoning abilities", "Effective integration into existing transformer architectures with minimal overhead"], "limitations": "", "keywords": ["positional encoding", "transformers", "context-aware", "long-context", "language modeling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.08312", "pdf": "https://arxiv.org/pdf/2501.08312.pdf", "abs": "https://arxiv.org/abs/2501.08312", "title": "Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages", "authors": ["Alžběta Kučerová", "Johann-Mattis List"], "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the Global WordNet Conference 2025", "summary": "Object naming - the act of identifying an object with a word or a phrase - is\na fundamental skill in interpersonal communication, relevant to many\ndisciplines, such as psycholinguistics, cognitive linguistics, or language and\nvision research. Object naming datasets, which consist of concept lists with\npicture pairings, are used to gain insights into how humans access and select\nnames for objects in their surroundings and to study the cognitive processes\ninvolved in converting visual stimuli into semantic concepts. Unfortunately,\nobject naming datasets often lack transparency and have a highly idiosyncratic\nstructure. Our study tries to make current object naming data transparent and\ncomparable by using a multilingual, computer-assisted approach that links\nindividual items of object naming lists to unified concepts. Our current sample\nlinks 17 object naming datasets that cover 30 languages from 10 different\nlanguage families. We illustrate how the comparative dataset can be explored by\nsearching for concepts that recur across the majority of datasets and comparing\nthe conceptual spaces of covered object naming datasets with classical basic\nvocabulary lists from historical linguistics and linguistic typology. Our\nfindings can serve as a basis for enhancing cross-linguistic object naming\nresearch and as a guideline for future studies dealing with object naming\ntasks.", "AI": {"tldr": "The study enhances transparency in object naming datasets by linking individual items to unified concepts across multiple languages.", "motivation": "To improve the understanding of how humans access and select names for objects and to enhance the quality of object naming datasets.", "method": "A multilingual, computer-assisted approach that links 17 object naming datasets covering 30 languages from 10 language families to unified concepts.", "result": "The comparative dataset allows exploration of recurring concepts and comparisons with classical vocabulary lists, showing various conceptual spaces.", "conclusion": "The findings offer a foundation for advancing cross-linguistic research in object naming and provide guidelines for future studies.", "key_contributions": ["Links multiple object naming datasets across languages for better comparability", "Enhances understanding of cognitive processes in object naming", "Provides a basis for future cross-linguistic naming research"], "limitations": "", "keywords": ["object naming", "cross-linguistic study", "cognitive linguistics", "dataset transparency", "language and vision"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.06855", "pdf": "https://arxiv.org/pdf/2502.06855.pdf", "abs": "https://arxiv.org/abs/2502.06855", "title": "Self-Supervised Prompt Optimization", "authors": ["Jinyu Xiang", "Jiayi Zhang", "Zhaoyang Yu", "Xinbing Liang", "Fengwei Teng", "Jinhao Tu", "Fashen Ren", "Xiangru Tang", "Sirui Hong", "Chenglin Wu", "Yuyu Luo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Well-designed prompts are crucial for enhancing Large language models' (LLMs)\nreasoning capabilities while aligning their outputs with task requirements\nacross diverse domains. However, manually designed prompts require expertise\nand iterative experimentation. While existing prompt optimization methods aim\nto automate this process, they rely heavily on external references such as\nground truth or by humans, limiting their applicability in real-world scenarios\nwhere such data is unavailable or costly to obtain. To address this, we propose\nSelf-Supervised Prompt Optimization (SPO), a cost-efficient framework that\ndiscovers effective prompts for both closed and open-ended tasks without\nrequiring external reference. Motivated by the observations that prompt quality\nmanifests directly in LLM outputs and LLMs can effectively assess adherence to\ntask requirements, we derive evaluation and optimization signals purely from\noutput comparisons. Specifically, SPO selects superior prompts through pairwise\noutput comparisons evaluated by an LLM evaluator, followed by an LLM optimizer\nthat aligns outputs with task requirements. Extensive experiments demonstrate\nthat SPO outperforms state-of-the-art prompt optimization methods, achieving\ncomparable or superior results with significantly lower costs (e.g., 1.1% to\n5.6% of existing methods) and fewer samples (e.g., three samples). The code is\navailable at https://github.com/FoundationAgents/SPO.", "AI": {"tldr": "This paper introduces Self-Supervised Prompt Optimization (SPO), a framework for optimizing prompts for large language models (LLMs) without external references, achieving cost-efficient and effective results.", "motivation": "The need for effective prompts in LLMs that do not require costly external references or human input in real-world scenarios.", "method": "SPO employs pairwise output comparisons to evaluate and select effective prompts, leveraging LLM assessments to align outputs with task requirements.", "result": "SPO demonstrates superior performance compared to state-of-the-art methods with significantly lower costs and fewer required samples in extensive experiments.", "conclusion": "SPO provides a novel approach to prompt optimization, making it feasible for diverse tasks without reliance on external data.", "key_contributions": ["Introduction of Self-Supervised Prompt Optimization (SPO)", "Cost-efficient prompt optimization framework", "Demonstrated superior performance versus existing methods"], "limitations": "", "keywords": ["Prompt Optimization", "Large Language Models", "Self-Supervised Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.09183", "pdf": "https://arxiv.org/pdf/2502.09183.pdf", "abs": "https://arxiv.org/abs/2502.09183", "title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation", "authors": ["Changzhi Zhou", "Xinyu Zhang", "Dandan Song", "Xiancai Chen", "Wanli Gu", "Huipeng Ma", "Yuhang Tian", "Mengdi Zhang", "Linmei Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.", "AI": {"tldr": "This paper introduces Adaptive Critique Refinement (ACR) to enhance code generation using self-generated code and external critiques, leading to improved performance in code generation tasks.", "motivation": "Code generation has become increasingly relevant due to Large Language Models (LLMs), yet existing methods focus on teacher model distillation without harnessing the potential of self-refinement.", "method": "The paper presents ACR, which utilizes a composite scoring system with LLM-as-a-Judge for quality evaluation and a selective critique strategy with LLM-as-a-Critic to improve low-quality code responses.", "result": "The RefineCoder series, developed through iterative application of ACR, shows continuous performance gains across multiple code generation benchmarks, performing comparably or better than baselines with less data.", "conclusion": "ACR represents a novel approach by enabling self-refinement in code generation, potentially exceeding performance expectations while minimizing data needs.", "key_contributions": ["Introduction of Adaptive Critique Refinement (ACR) for code generation", "Development of the RefineCoder series leveraging self-refinement", "Demonstrated performance improvement with lower data requirements"], "limitations": "", "keywords": ["Code Generation", "Large Language Models", "Self-Refinement", "Machine Learning", "Adaptive Critique"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11491", "pdf": "https://arxiv.org/pdf/2502.11491.pdf", "abs": "https://arxiv.org/abs/2502.11491", "title": "Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering", "authors": ["Runxuan Liu", "Bei Luo", "Jiaqi Li", "Baoxin Wang", "Ming Liu", "Dayong Wu", "Shijin Wang", "Bing Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers)", "summary": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing. However, in knowledge graph question answering tasks\n(KGQA), there remains the issue of answering questions that require multi-hop\nreasoning. Existing methods rely on entity vector matching, but the purpose of\nthe question is abstract and difficult to match with specific entities. As a\nresult, it is difficult to establish reasoning paths to the purpose, which\nleads to information loss and redundancy. To address this issue, inspired by\nhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a\nnovel framework that constructs reasoning paths from purposes back to\nconditions. ORT operates in three key phases: (1) using LLM to extract purpose\nlabels and condition labels, (2) constructing label reasoning paths based on\nthe KG ontology, and (3) using the label reasoning paths to guide knowledge\nretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves\nstate-of-the-art performance and significantly enhances the capability of LLMs\nfor KGQA.", "AI": {"tldr": "The paper presents Ontology-Guided Reverse Thinking (ORT), a framework to improve multi-hop reasoning in knowledge graph question answering using large language models.", "motivation": "To address the challenges of abstract question matching and reasoning paths in knowledge graph question answering that lead to information loss.", "method": "ORT uses a three-phase approach: (1) extracts purpose and condition labels with LLM, (2) constructs reasoning paths based on the KG ontology, and (3) guides knowledge retrieval using these paths.", "result": "Experiments demonstrate that ORT achieves state-of-the-art performance on the WebQSP and CWQ datasets, significantly improving LLM capability in KGQA.", "conclusion": "The proposed ORT framework effectively enhances multi-hop reasoning in knowledge graph question answering scenarios.", "key_contributions": ["Introduction of Ontology-Guided Reverse Thinking (ORT) framework", "Novel approach to construct reasoning paths from purposes to conditions", "Demonstration of state-of-the-art performance on standard datasets."], "limitations": "", "keywords": ["large language models", "knowledge graph", "question answering", "multi-hop reasoning", "reverse thinking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.15429", "pdf": "https://arxiv.org/pdf/2502.15429.pdf", "abs": "https://arxiv.org/abs/2502.15429", "title": "Pub-Guard-LLM: Detecting Retracted Biomedical Articles with Reliable Explanations", "authors": ["Lihu Chen", "Shuojie Fu", "Gabriel Freedman", "Cemre Zor", "Guy Martin", "James Kinross", "Uddhav Vaghela", "Ovidiu Serban", "Francesca Toni"], "categories": ["cs.CL"], "comment": "long paper under review", "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.", "AI": {"tldr": "Pub-Guard-LLM is a large language model-based system designed to detect fraud in biomedical scientific articles, offering multiple deployment modes and enhanced performance over baselines.", "motivation": "The increasing prevalence of fraudulent practices in published scientific articles threatens the credibility of research, especially in medicine.", "method": "Pub-Guard-LLM employs three application modes: vanilla reasoning, retrieval-augmented generation, and multi-agent debate, each capable of providing textual explanations for its predictions.", "result": "Pub-Guard-LLM outperforms various baseline models in predicting fraudulent articles and provides more coherent and relevant explanations based on an open-source benchmark of over 11K biomedical articles.", "conclusion": "By improving detection performance and explanation quality, Pub-Guard-LLM serves as an effective tool for safeguarding research integrity in the biomedical field.", "key_contributions": ["Introduction of a large language model for biomedical fraud detection", "Development of multiple application modes for enhanced usability", "Creation of the PubMed Retraction benchmark for evaluating model performance"], "limitations": "", "keywords": ["fraud detection", "biomedical", "large language model", "Pub-Guard-LLM", "research integrity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.15600", "pdf": "https://arxiv.org/pdf/2502.15600.pdf", "abs": "https://arxiv.org/abs/2502.15600", "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings", "authors": ["Ingroj Shrestha", "Louis Tay", "Padmini Srinivasan"], "categories": ["cs.CL"], "comment": "Findings of NAACL 2025", "summary": "There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\emph{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible.", "AI": {"tldr": "The paper proposes a systematic statistical method to assess bias in machine learning models (MLMs) using mixed models and pseudo-perplexity weights. It finds variations in gender bias among different MLMs, correlating some findings with psychological studies on personality traits.", "motivation": "To address limitations in prior studies on bias in machine learning models that overlook random variability, assume equality among templates, and ignore bias quantification.", "method": "Utilizes mixed models to account for random effects and pseudo-perplexity weights for sentences derived from templates to quantify bias using statistical effect sizes.", "result": "The study confirms prior bias findings in MLMs, observes variability in gender bias across MLMs, and establishes some alignment between MLM bias and psychological studies on personality traits.", "conclusion": "Different MLMs exhibit varying degrees of bias, with findings that can correlate with human psychological perspectives, although character traits comparison remains limited due to a lack of studies.", "key_contributions": ["Systematic statistical approach to assess bias in MLMs", "Application of mixed models for bias quantification", "Exploration of gender bias in personality and character traits across multiple MLMs"], "limitations": "Comparison of character traits is limited due to the scarcity of human studies on gender bias in this area.", "keywords": ["bias", "machine learning models", "personality traits", "gender bias", "statistical analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.15860", "pdf": "https://arxiv.org/pdf/2502.15860.pdf", "abs": "https://arxiv.org/abs/2502.15860", "title": "Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection", "authors": ["Arefeh Kazemi", "Sri Balaaji Natarajan Kalaivendan", "Joachim Wagner", "Hamza Qadeer", "Kanishk Verma", "Brian Davis"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Cyberbullying (CB) presents a pressing threat, especially to children,\nunderscoring the urgent need for robust detection systems to ensure online\nsafety. While large-scale datasets on online abuse exist, there remains a\nsignificant gap in labeled data that specifically reflects the language and\ncommunication styles used by children. The acquisition of such data from\nvulnerable populations, such as children, is challenging due to ethical, legal\nand technical barriers. Moreover, the creation of these datasets relies heavily\non human annotation, which not only strains resources but also raises\nsignificant concerns due to annotators exposure to harmful content. In this\npaper, we address these challenges by leveraging Large Language Models (LLMs)\nto generate synthetic data and labels. Our experiments demonstrate that\nsynthetic data enables BERT-based CB classifiers to achieve performance close\nto that of those trained on fully authentic datasets (75.8% vs. 81.5%\naccuracy). Additionally, LLMs can effectively label authentic yet unlabeled\ndata, allowing BERT classifiers to attain a comparable performance level (79.1%\nvs. 81.5% accuracy). These results highlight the potential of LLMs as a\nscalable, ethical, and cost-effective solution for generating data for CB\ndetection.", "AI": {"tldr": "The paper explores using Large Language Models to generate synthetic data for effective cyberbullying detection, addressing the gap in labeled data for children's online communication.", "motivation": "Cyberbullying is a significant threat to children, highlighting the need for efficient detection systems, but there is a lack of labeled data that reflects children’s communication styles.", "method": "The paper employs Large Language Models to generate synthetic data and labels for training BERT-based classifiers, and assesses performance against authentic datasets.", "result": "Synthetic data allows BERT classifiers to achieve performance close to classifiers trained on authentic datasets, with accuracies of 75.8% vs. 81.5%.", "conclusion": "LLMs provide a scalable, ethical, and cost-effective approach for generating necessary data for cyberbullying detection systems.", "key_contributions": ["Use of Large Language Models for synthetic dataset generation", "Demonstrated effectiveness of synthetic data in cyberbullying classification", "Addressed ethical concerns related to data gathering from children"], "limitations": "", "keywords": ["Cyberbullying", "Large Language Models", "Data Generation", "BERT", "Synthetic Data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.01539", "pdf": "https://arxiv.org/pdf/2503.01539.pdf", "abs": "https://arxiv.org/abs/2503.01539", "title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language", "authors": ["Xi Chen", "Shuo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "The rapid development of large language models (LLMs) gives rise to ethical\nconcerns about their performance, while opening new avenues for developing\ntoxic language detection techniques. However, LLMs' unethical output and their\ncapability of detecting toxicity have primarily been tested on language data\nthat do not demand complex meaning inference, such as the biased associations\nof 'he' with programmer and 'she' with household. Nowadays toxic language\nadopts a much more creative range of implicit forms, thanks to advanced\ncensorship. In this study, we collect authentic toxic interactions that evade\nonline censorship and that are verified by human annotators as\ninference-intensive. To evaluate and improve LLMs' reasoning of the authentic\nimplicit toxic language, we propose a new prompting method, Pragmatic Inference\nChain (PIC), drawn on interdisciplinary findings from cognitive science and\nlinguistics. The PIC prompting significantly improves the success rate of\nGPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying\nimplicit toxic language, compared to five baseline prompts, such as CoT and\nrule-based baselines. In addition, it also facilitates the models to produce\nmore explicit and coherent reasoning processes, hence can potentially be\ngeneralized to other inference-intensive tasks, e.g., understanding humour and\nmetaphors.", "AI": {"tldr": "This study proposes the Pragmatic Inference Chain (PIC) prompting method to enhance large language models' ability to detect implicit toxic language.", "motivation": "The increasing sophistication of toxic language and LLMs' ethical implications necessitate improved detection techniques that account for complex language usage.", "method": "The study introduces the Pragmatic Inference Chain (PIC) prompting method, evaluated across several advanced LLMs including GPT-4o, to enhance reasoning in identifying implicit toxicity.", "result": "Using the PIC method significantly increased the success rates of multiple LLMs in identifying implicit toxic language compared to several baseline prompts.", "conclusion": "The PIC prompting method shows promise not only in improving toxic language detection but also in potentially benefiting other inference-intensive tasks.", "key_contributions": ["Introduction of the Pragmatic Inference Chain (PIC) prompting method", "Improved detection rates of implicit toxic language in advanced LLMs", "Potential applications of PIC to other inference-intensive tasks."], "limitations": "", "keywords": ["toxic language", "large language models", "Pragmatic Inference Chain", "implicit toxicity", "ethical concerns"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2503.11377", "pdf": "https://arxiv.org/pdf/2503.11377.pdf", "abs": "https://arxiv.org/abs/2503.11377", "title": "Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data", "authors": ["Annika Tjuka", "Robert Forkel", "Christoph Rzymski", "Johann-Mattis List"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Lexical resources are crucial for cross-linguistic analysis and can provide\nnew insights into computational models for natural language learning. Here, we\npresent an advanced database for comparative studies of words with multiple\nmeanings, a phenomenon known as colexification. The new version includes\nimprovements in the handling, selection and presentation of the data. We\ncompare the new database with previous versions and find that our improvements\nprovide a more balanced sample covering more language families worldwide, with\nenhanced data quality, given that all word forms are provided in phonetic\ntranscription. We conclude that the new Database of Cross-Linguistic\nColexifications has the potential to inspire exciting new studies that link\ncross-linguistic data to open questions in linguistic typology, historical\nlinguistics, psycholinguistics, and computational linguistics.", "AI": {"tldr": "This paper presents an improved database for colexification studies, enhancing linguistic research.", "motivation": "To provide new insights into computational models for natural language learning through an advanced database for cross-linguistic analysis.", "method": "The paper details improvements in data handling, selection, and presentation for a database tracking words with multiple meanings across different languages.", "result": "The new database offers a more balanced sample from various language families and enhanced data quality, including phonetic transcription of word forms.", "conclusion": "The enhanced database can inspire studies linking cross-linguistic data to questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.", "key_contributions": ["Introduction of an advanced database for colexification research", "Improved data quality and selection methodology", "Coverage across a wider range of language families"], "limitations": "", "keywords": ["colexification", "cross-linguistic analysis", "lexical resources", "natural language processing", "linguistic databases"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2503.16622", "pdf": "https://arxiv.org/pdf/2503.16622.pdf", "abs": "https://arxiv.org/abs/2503.16622", "title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation", "authors": ["Michele Fiori", "Gabriele Civitarese", "Priyankar Choudhary", "Claudio Bettini"], "categories": ["cs.CL"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition.", "AI": {"tldr": "The paper explores the integration of Large Language Models with Explainable AI for sensor-based recognition of Activities of Daily Living, aiming to enhance explanation generation and model transparency.", "motivation": "To improve the transparency and trust in machine learning models used in IoT systems by leveraging Explainable AI, particularly for recognizing sensor-based Activities of Daily Living.", "method": "The paper investigates the use of LLMs in two main approaches: 1) as explainable zero-shot models for ADL recognition to minimize the need for labeled data, and 2) to automate explanation generation in data-driven XAI approaches for better recognition rates.", "result": "The evaluation reveals potential benefits of LLMs in enhancing explanation flexibility and scalability, while also identifying challenges regarding their implementation in the context of ADL recognition.", "conclusion": "LLMs present promising opportunities for combining with XAI in ADL recognition, though challenges remain that need to be addressed for effective deployment.", "key_contributions": ["Integration of LLMs with XAI for improved explanation generation.", "Evaluation of LLMs as zero-shot recognition models for ADLs.", "Identification of challenges in using LLMs for explainability in sensor data."], "limitations": "Challenges in implementing LLMs for real-time ADL recognition applications due to computational demands and integration complexity.", "keywords": ["Explainable AI", "Large Language Models", "Activities of Daily Living", "IoT", "Sensor Data"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.00406", "pdf": "https://arxiv.org/pdf/2504.00406.pdf", "abs": "https://arxiv.org/abs/2504.00406", "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent", "AI": {"tldr": "VerifiAgent is a unified verification agent designed to enhance the reliability of large language model responses through meta-verification and adaptive tool-based verification.", "motivation": "To improve the reliability and scalability of verification methods for large language models which often produce incorrect responses.", "method": "VerifiAgent employs a two-level verification system: meta-verification for assessing model response completeness and consistency, and adaptive verification to choose the appropriate tools for various reasoning types.", "result": "VerifiAgent outperforms existing verification methods across all reasoning tasks, demonstrating improved reasoning accuracy by using feedback from verification results and showing cost efficiency in the mathematical reasoning domain.", "conclusion": "VerifiAgent provides a robust and efficient solution for verifying large language models, enhancing both the accuracy of reasoning tasks and the scalability of verification processes.", "key_contributions": ["Introduction of a unified verification agent for large language models.", "Meta-verification and tool-based adaptive verification integration.", "Significant improvements in verification performance and reasoning accuracy with reduced costs."], "limitations": "", "keywords": ["large language models", "verification methods", "reasoning tasks"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2504.11169", "pdf": "https://arxiv.org/pdf/2504.11169.pdf", "abs": "https://arxiv.org/abs/2504.11169", "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos", "authors": ["Laura De Grazia", "Pol Pastells", "Mauro Vázquez Chas", "Desmond Elliott", "Danae Sánchez Villegas", "Mireia Farrús", "Mariona Taulé"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025 camera-ready version: expanded Section 4.3 with an\n  additional experiment using an extended definition-based prompt (including a\n  definition of sexist content), and applied minor corrections", "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contributions of textual,\nvocal, and visual modalities to the classification of content as either sexist\nor non-sexist; and (3) we evaluate a range of large language models (LLMs) and\nmultimodal LLMs on the task of sexism detection. We find that visual\ninformation plays a key role in labeling sexist content for both humans and\nmodels. Models effectively detect explicit sexism; however, they struggle with\nimplicit cases, such as stereotypes, instances where annotators also show low\nagreement. This highlights the inherent difficulty of the task, as identifying\nimplicit sexism depends on the social and cultural context.", "AI": {"tldr": "This paper presents a multimodal approach to detecting sexism in social media videos, introducing a new dataset and evaluation of machine learning models.", "motivation": "The study addresses the amplification of sexism through social media, emphasizing the need for a comprehensive analysis of video content combining verbal, audio, and visual elements.", "method": "We introduce MuSeD, a multimodal dataset comprising videos from platforms like TikTok and BitChute, and an innovative annotation framework for classifying sexism in content. A range of LLMs and multimodal models were evaluated for their effectiveness in detecting sexism.", "result": "The findings indicate that visual elements significantly contribute to the identification of sexist content, with models performing well on explicit sexism but struggling with implicit stereotypes.", "conclusion": "The research underscores the complexity of detecting implicit sexism due to its dependence on context, highlighting the need for improved model capabilities and further exploration of cultural factors.", "key_contributions": ["Introduction of the MuSeD dataset for multimodal sexism detection", "Proposed annotation framework for analyzing multimodal contributions", "Evaluation of various LLMs on sexism detection tasks"], "limitations": "Models have difficulty with implicit sexism, as evidenced by low agreement among annotators and challenges in context sensitivity.", "keywords": ["sexism detection", "multimodal dataset", "social media", "large language models", "implicit sexism"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120.pdf", "abs": "https://arxiv.org/abs/2504.15120", "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.", "AI": {"tldr": "The paper presents a method for integrating new languages into large language models without losing previous knowledge.", "motivation": "To enhance existing language models by adding new languages while maintaining their prior capabilities.", "method": "The authors introduce a novel approach that incorporates a new language (Arabic) into a 1.5 billion parameter LLM trained predominantly in English, using minimal original data.", "result": "Results show an 8% average improvement in Arabic language performance across various benchmarks, while preserving the original model's knowledge.", "conclusion": "The proposed method offers a cost-effective way to expand language capabilities in models without extensive retraining, paving the way for efficient language model development.", "key_contributions": ["Novel integration method for new languages in LLMs", "Demonstrated significant performance improvement in Arabic", "Cost-effective model expansion approach"], "limitations": "", "keywords": ["language model", "LLM", "Arabic", "knowledge integration", "benchmark performance"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640.pdf", "abs": "https://arxiv.org/abs/2504.15640", "title": "Cequel: Cost-Effective Querying of Large Language Models for Text Clustering", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of documents\ninto coherent groups based on their linguistic features. In the literature,\nthis task is formulated either as metric clustering over pre-trained text\nembeddings or as graph clustering based on pairwise similarities derived from\nan oracle, e.g., a large machine learning model. Recent advances in large\nlanguage models (LLMs) have significantly improved this field by providing\nhigh-quality contextualized embeddings and accurate semantic similarity\nestimates. However, leveraging LLMs at scale introduces substantial\ncomputational and financial costs due to the large number of required API\nqueries or inference calls. To address this issue, we propose Cequel, a\ncost-effective framework that achieves accurate text clustering under a limited\nbudget of LLM queries. At its core, Cequel constructs must-link and cannot-link\nconstraints by selectively querying LLMs on informative text pairs or triplets,\nidentified via our proposed algorithms, EdgeLLM and TriangleLLM. These\nconstraints are then utilized in a weighted constrained clustering algorithm to\nform high-quality clusters. Specifically, EdgeLLM and TriangleLLM employ\ncarefully designed greedy selection strategies and prompting techniques to\nidentify and extract informative constraints efficiently. Experiments on\nmultiple benchmark datasets demonstrate that Cequel consistently outperforms\nexisting methods in unsupervised text clustering under the same query budget.", "AI": {"tldr": "Cequel is a cost-effective framework for text clustering that minimizes LLM queries while maintaining clustering quality.", "motivation": "Improving text clustering accuracy while managing the computational and financial costs associated with LLM queries.", "method": "Cequel uses must-link and cannot-link constraints derived from selective querying of LLMs on informative text pairs and triplets through EdgeLLM and TriangleLLM algorithms, followed by a weighted constrained clustering algorithm.", "result": "Cequel consistently outperforms state-of-the-art methods in unsupervised text clustering while adhering to a limited query budget.", "conclusion": "The proposed framework, Cequel, effectively balances cost and accuracy in text clustering using LLMs.", "key_contributions": ["Introduction of EdgeLLM and TriangleLLM for efficient constraint extraction", "Development of a cost-effective clustering framework that leverages LLMs", "Demonstration of superior clustering performance on benchmark datasets under query budget constraints."], "limitations": "", "keywords": ["text clustering", "large language models", "constraint-based clustering", "unsupervised learning", "cost-effective methods"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.14444", "pdf": "https://arxiv.org/pdf/2508.14444.pdf", "abs": "https://arxiv.org/abs/2508.14444", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Alexander Bukharin", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haifeng Qian", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jian Zhang", "Jiaqi Zeng", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Tugrul Konuk", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.", "AI": {"tldr": "Introducing Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model that improves reasoning throughput and accuracy compared to similar models.", "motivation": "To enhance the throughput for reasoning workloads and achieve state-of-the-art accuracy among similarly-sized language models.", "method": "The model is developed by replacing self-attention layers with Mamba-2 layers and is pre-trained on 20 trillion tokens. It incorporates a Minitron strategy for compression and distillation, enabling efficient inference on up to 128k tokens.", "result": "Nemotron-Nano-9B-v2 demonstrates on-par or improved accuracy on reasoning benchmarks compared to models like Qwen3-8B, while achieving up to 6x higher inference throughput in specific settings.", "conclusion": "The release of Nemotron-Nano-9B-v2 along with its base checkpoints and datasets aims to support further advancements in language model research and applications.", "key_contributions": ["Introduction of Mamba-2 layers for enhanced inference speed", "Significant increase in throughput for reasoning tasks", "Availability of pre-trained datasets and model checkpoints on Hugging Face"], "limitations": "", "keywords": ["language model", "Mamba-Transformer", "reasoning workloads", "AI applications", "token inference"], "importance_score": 7, "read_time_minutes": 5}}
