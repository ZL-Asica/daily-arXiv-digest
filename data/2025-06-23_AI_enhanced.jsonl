{"id": "2506.15794", "pdf": "https://arxiv.org/pdf/2506.15794.pdf", "abs": "https://arxiv.org/abs/2506.15794", "title": "Veracity: An Open-Source AI Fact-Checking System", "authors": ["Taylor Lynn Curtis", "Maximilian Puelma Touzel", "William Garneau", "Manon Gruaz", "Mike Pinder", "Li Wei Wang", "Sukanya Krishna", "Luda Cohen", "Jean-Fran√ßois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of misinformation poses a significant threat to society,\nexacerbated by the capabilities of generative AI. This demo paper introduces\nVeracity, an open-source AI system designed to empower individuals to combat\nmisinformation through transparent and accessible fact-checking. Veracity\nleverages the synergy between Large Language Models (LLMs) and web retrieval\nagents to analyze user-submitted claims and provide grounded veracity\nassessments with intuitive explanations. Key features include multilingual\nsupport, numerical scoring of claim veracity, and an interactive interface\ninspired by familiar messaging applications. This paper will showcase\nVeracity's ability to not only detect misinformation but also explain its\nreasoning, fostering media literacy and promoting a more informed society.", "AI": {"tldr": "This paper presents Veracity, an open-source AI system for fact-checking and combating misinformation using LLMs and web retrieval.", "motivation": "To address the threat of misinformation exacerbated by generative AI and enhance media literacy among users.", "method": "Veracity combines Large Language Models and web retrieval agents to analyze claims and provides assessments with explanations.", "result": "Veracity offers features like multilingual support, numerical scoring of claim veracity, and an interactive interface for user engagement.", "conclusion": "Veracity not only detects misinformation but also explains its reasoning to foster a more informed society.", "key_contributions": ["Introduction of an open-source AI system for fact-checking", "Multilingual support and numerical scoring system", "Intuitive interface inspired by messaging applications"], "limitations": "", "keywords": ["misinformation", "fact-checking", "large language models", "AI", "media literacy"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.15830", "pdf": "https://arxiv.org/pdf/2506.15830.pdf", "abs": "https://arxiv.org/abs/2506.15830", "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics", "authors": ["Riccardo Di Sipio"], "categories": ["cs.CL", "quant-ph", "I.2; I.7"], "comment": "9 pages, 1 figure(s)", "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems.", "AI": {"tldr": "This paper discusses the optimization of large language models through the lens of information geometry, emphasizing the use of the Fisher information metric and natural gradient descent, while exploring implications for quantum systems.", "motivation": "To enhance understanding of the optimization landscape in large language models and clarify phenomena related to sharp minima, generalization, and scaling laws.", "method": "The paper utilizes information geometry, specifically the Fisher information metric, to analyze the high-dimensional parameter spaces of large language models and proposes curvature-aware optimization techniques.", "result": "The authors explain how curvature-aware methods can provide insights into LLM training and suggest possible parallels with quantum optimization techniques.", "conclusion": "Curvature-aware approaches may improve LLM training and efficiency while offering a bridge to quantum optimization strategies.", "key_contributions": ["Application of information geometry to LLM optimization", "Identification of curvature-aware techniques for better training insights", "Speculation on quantum optimization parallels using metrics from quantum information theory"], "limitations": "", "keywords": ["large language models", "information geometry", "natural gradient descent", "quantum optimization", "Fisher information"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2506.15841", "pdf": "https://arxiv.org/pdf/2506.15841.pdf", "abs": "https://arxiv.org/abs/2506.15841", "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents", "authors": ["Zijian Zhou", "Ao Qu", "Zhaoxuan Wu", "Sunghwan Kim", "Alok Prakash", "Daniela Rus", "Jinhua Zhao", "Bryan Kian Hsiang Low", "Paul Pu Liang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized.", "AI": {"tldr": "Introduction of MEM1, a reinforcement learning framework for long-horizon multi-turn interactions that optimizes memory usage and performance.", "motivation": "To address challenges in long-horizon interactions faced by LLM systems, which often result in increased memory and computational costs due to reliance on full-context prompting.", "method": "MEM1 updates a constant-size shared internal state at each turn, supporting memory consolidation and reasoning while discarding irrelevant information.", "result": "MEM1-7B achieves a 3.5x performance improvement and 3.7x reduction in memory usage compared to Qwen2.5-14B-Instruct on a complex multi-hop QA task.", "conclusion": "MEM1 provides a scalable solution for training interactive agents in multi-turn tasks, emphasizing both efficiency and performance.", "key_contributions": ["Introduction of a constant memory usage framework for LLMs", "Proposed methodology for constructing complex task sequences from existing datasets", "Demonstrated significant performance and memory efficiency improvements in experiments"], "limitations": "", "keywords": ["long-horizon interactions", "memory consolidation", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.15846", "pdf": "https://arxiv.org/pdf/2506.15846.pdf", "abs": "https://arxiv.org/abs/2506.15846", "title": "Finance Language Model Evaluation (FLaME)", "authors": ["Glenn Matlin", "Mika Okamoto", "Huzaifa Pardawala", "Yang Yang", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Language Models (LMs) have demonstrated impressive capabilities with core\nNatural Language Processing (NLP) tasks. The effectiveness of LMs for highly\nspecialized knowledge-intensive tasks in finance remains difficult to assess\ndue to major gaps in the methodologies of existing evaluation frameworks, which\nhave caused an erroneous belief in a far lower bound of LMs' performance on\ncommon Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for\nthese FinNLP tasks, we present the first holistic benchmarking suite for\nFinancial Language Model Evaluation (FLaME). We are the first research paper to\ncomprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical\nstudy of 23 foundation LMs over 20 core NLP tasks in finance. We open-source\nour framework software along with all data and results.", "AI": {"tldr": "Introduction of a benchmarking suite called FLaME for evaluating Language Models in Finance NLP tasks.", "motivation": "Address gaps in existing evaluation frameworks that misrepresent LMs' performance in finance-related NLP tasks.", "method": "A comprehensive empirical study comparing 23 foundation language models on 20 core finance NLP tasks using the new FLaME framework.", "result": "The study reveals that Language Models can perform significantly better than previously believed on finance NLP tasks when evaluated properly.", "conclusion": "The FLaME framework serves as a tool for accurately evaluating and comparing LMs, helping to correct misconceptions about their capabilities in Finance.", "key_contributions": ["Introduction of a holistic benchmarking suite (FLaME) for Financial Language Model Evaluation", "First study to compare 'reasoning-reinforced' LMs against traditional LMs in finance", "Open-sourcing the framework software, data, and results for further research"], "limitations": "None stated in the abstract.", "keywords": ["Language Models", "Financial NLP", "Evaluation Framework", "Benchmarking", "Reasoning-reinforced LMs"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.15834", "pdf": "https://arxiv.org/pdf/2506.15834.pdf", "abs": "https://arxiv.org/abs/2506.15834", "title": "Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study", "authors": ["Zachary D King", "Maryam Khalid", "Han Yu", "Kei Shibuya", "Khadija Zanna", "Marzieh Majd", "Ryan L Brown", "Yufei Shen", "Thomas Vaessen", "George Kypriotakis", "Christopher P Fagundes", "Akane Sano"], "categories": ["cs.HC"], "comment": null, "summary": "Mobile health (mHealth) systems help researchers monitor and care for\npatients in real-world settings. Studies utilizing mHealth applications use\nEcological Momentary Assessment (EMAs), passive sensing, and contextual\nfeatures to develop emotion recognition models, which rely on EMA responses as\nground truth. Due to this, it is crucial to consider EMA compliance when\nconducting a successful mHealth study. Utilizing machine learning is one\napproach that can solve this problem by sending EMAs based on the predicted\nlikelihood of a response. However, literature suggests that this approach may\nlead to prompting participants more frequently during emotions associated with\nresponsiveness, thereby narrowing the range of emotions collected. We propose a\nmulti-objective function that utilizes machine learning to identify optimal\ntimes for sending EMAs. The function identifies optimal moments by combining\npredicted response likelihood with model uncertainty in emotion predictions.\nUncertainty would lead the function to prioritize time points when the model is\nless confident, which often corresponds to underrepresented emotions. We\ndemonstrate that this objective function would result in EMAs being sent when\nparticipants are responsive and experiencing less commonly observed emotions.\nThe evaluation is conducted offline using two datasets: (1) 91 spousal\ncaregivers of individuals with Alzheimer's Disease and Related dementias\n(ADRD), (2) 45 healthy participants. Results show that the multi-objective\nfunction tends to be higher when participants respond to EMAs and report less\ncommonly observed emotions. This suggests that using the proposed objective\nfunction to guide EMA delivery could improve receptivity rates and capture a\nbroader range of emotions.", "AI": {"tldr": "This study proposes a machine learning-based approach to optimize the timing of Ecological Momentary Assessments (EMAs) in mHealth applications, aiming to improve emotional data collection by focusing on underrepresented emotions.", "motivation": "To improve EMA compliance and emotional data collection in mHealth studies, particularly for emotions that are less commonly reported.", "method": "A multi-objective function is proposed which utilizes machine learning to determine optimal times for sending EMAs by combining predicted response likelihood with model uncertainty in emotion predictions.", "result": "The results demonstrate that the objective function leads to higher EMA responses during instances when participants report less commonly observed emotions.", "conclusion": "The proposed method could enhance receptivity rates and diversify emotional data captured in mHealth research.", "key_contributions": ["Multi-objective function for EMA timing optimization", "Combination of response likelihood and model uncertainty", "Broader emotional data collection in mHealth contexts"], "limitations": "Evaluation conducted offline with limited datasets; further real-world testing needed.", "keywords": ["mHealth", "Ecological Momentary Assessment", "machine learning", "emotion recognition", "response likelihood"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15889", "pdf": "https://arxiv.org/pdf/2506.15889.pdf", "abs": "https://arxiv.org/abs/2506.15889", "title": "Entropy-Driven Pre-Tokenization for Byte-Pair Encoding", "authors": ["Yifan Hu", "Frank Liang", "Dachuan Zhao", "Jonathan Geuter", "Varshini Reddy", "Craig W. Schmidt", "Chris Tanner"], "categories": ["cs.CL"], "comment": null, "summary": "Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization\nmethod in modern language models due to its simplicity and strong empirical\nperformance across downstream tasks. However, applying BPE to unsegmented\nlanguages such as Chinese presents significant challenges, as its\nfrequency-driven merge operation is agnostic to linguistic boundaries. To\naddress this, we propose two entropy-informed pre-tokenization strategies that\nguide BPE segmentation using unsupervised information-theoretic cues. The first\napproach uses pointwise mutual information and left/right entropy to identify\ncoherent character spans, while the second leverages predictive entropy derived\nfrom a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both\nmethods on a subset of the PKU dataset and demonstrate substantial improvements\nin segmentation precision, recall, and F1 score compared to standard BPE. Our\nresults suggest that entropy-guided pre-tokenization not only enhances\nalignment with gold-standard linguistic units but also offers a promising\ndirection for improving tokenization quality in low-resource and multilingual\nsettings.", "AI": {"tldr": "This paper proposes two entropy-informed pre-tokenization strategies to enhance the performance of Byte-Pair Encoding (BPE) for segmenting unsegmented languages like Chinese.", "motivation": "The need to improve BPE segmentation for unsegmented languages due to its challenges in linguistic boundary recognition.", "method": "Two strategies: one based on pointwise mutual information and left/right entropy to identify coherent character spans, and another using predictive entropy from a pretrained GPT-2 model to detect boundary uncertainty.", "result": "Both methods showed substantial improvements in segmentation precision, recall, and F1 score on the PKU dataset compared to standard BPE.", "conclusion": "Entropy-guided pre-tokenization improves alignment with gold-standard linguistic units and enhances tokenization quality in low-resource and multilingual contexts.", "key_contributions": ["Development of two novel entropy-informed pre-tokenization strategies for BPE.", "Substantial improvement in segmentation metrics compared to standard BPE.", "Potential application in low-resource and multilingual settings."], "limitations": "", "keywords": ["Byte-Pair Encoding", "pre-tokenization", "entropy", "segmentation", "low-resource languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.15873", "pdf": "https://arxiv.org/pdf/2506.15873.pdf", "abs": "https://arxiv.org/abs/2506.15873", "title": "DeckFlow: Iterative Specification on a Multimodal Generative Canvas", "authors": ["Gregory Croisdale", "Emily Huang", "John Joon Young Chung", "Anhong Guo", "Xu Wang", "Austin Z. Henley", "Cyrus Omar"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI promises to allow people to create high-quality personalized\nmedia. Although powerful, we identify three fundamental design problems with\nexisting tooling through a literature review. We introduce a multimodal\ngenerative AI tool, DeckFlow, to address these problems. First, DeckFlow\nsupports task decomposition by allowing users to maintain multiple\ninterconnected subtasks on an infinite canvas populated by cards connected\nthrough visual dataflow affordances. Second, DeckFlow supports a specification\ndecomposition workflow where an initial goal is iteratively decomposed into\nsmaller parts and combined using feature labels and clusters. Finally, DeckFlow\nsupports generative space exploration by generating multiple prompt and output\nvariations, presented in a grid, that can feed back recursively into the next\ndesign iteration. We evaluate DeckFlow for text-to-image generation against a\nstate-of-practice conversational AI baseline for image generation tasks. We\nthen add audio generation and investigate user behaviors in a more open-ended\ncreative setting with text, image, and audio outputs.", "AI": {"tldr": "DeckFlow is a multimodal generative AI tool designed to address fundamental design problems in existing generative AI tools by enabling task decomposition, specification workflows, and generative space exploration in creative media generation.", "motivation": "Generative AI has the potential to empower individuals in creating personalized media, but current tools face design shortcomings that limit their effectiveness.", "method": "The paper introduces DeckFlow, which features an infinite canvas for task decomposition, facilitates iterative specification workflows, and allows for generative space exploration through multiple variations of outputs.", "result": "DeckFlow was evaluated for text-to-image generation against a baseline, showing its capability in handling complex generative tasks involving text, image, and audio outputs in a creative context.", "conclusion": "The findings suggest that DeckFlow enhances user engagement and creativity in media generation by addressing key limitations of existing generative AI tools.", "key_contributions": ["Introduction of DeckFlow multimodal tool for media generation", "Support for task and specification decomposition workflows", "Ability to generate and explore multiple variations iteratively"], "limitations": "The evaluation is limited to specific use cases and does not encompass all potential user scenarios for generative media.", "keywords": ["Generative AI", "DeckFlow", "multimodal", "creative media", "task decomposition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15894", "pdf": "https://arxiv.org/pdf/2506.15894.pdf", "abs": "https://arxiv.org/abs/2506.15894", "title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning", "authors": ["Sam Silver", "Jimin Sun", "Ivan Zhang", "Sara Hooker", "Eddie Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive mathematical\nreasoning capabilities, yet their performance remains brittle to minor\nvariations in problem description and prompting strategy. Furthermore,\nreasoning is vulnerable to sampling-induced errors which autoregressive models\nmust primarily address using self-correction via additionally-generated tokens.\nTo better understand self-correction capabilities of recent models, we conduct\nexperiments measuring models' ability to self-correct synthetic perturbations\nintroduced into their Chain of Thought (CoT) reasoning. We observe robust\nsingle-utterance intrinsic self-correction behavior across a range of\nopen-weight models and datasets, ranging from subtle, implicit corrections to\nexplicit acknowledgments and corrections of errors. Our findings suggest that\nLLMs, including those not finetuned for long CoT, may possess stronger\nintrinsic self-correction capabilities than commonly shown in the literature.\nThe presence of this ability suggests that recent \"reasoning\" model work\ninvolves amplification of traits already meaningfully present in models.", "AI": {"tldr": "The paper investigates the self-correction capabilities of Large Language Models (LLMs) in mathematical reasoning under synthetic perturbations in their Chain of Thought (CoT) process.", "motivation": "To understand the robustness of LLMs' self-correction capabilities and their performance in mathematical reasoning tasks, especially under variations in problem description and prompting strategy.", "method": "Experiments were conducted to measure the intrinsic self-correction behavior of various open-weight LLMs across different datasets by introducing synthetic perturbations in the CoT reasoning.", "result": "The study found that LLMs exhibited strong intrinsic self-correction behavior, correcting subtle errors and explicitly acknowledging mistakes, suggesting that they may have better self-correction capabilities than previously reported.", "conclusion": "The findings imply that LLMs, even those not fine-tuned for extended reasoning tasks, possess significant intrinsic self-correction abilities, indicating that reasoning model advancements may build on existing model traits.", "key_contributions": ["Demonstrated robust intrinsic self-correction across LLMs in mathematical reasoning tasks.", "Provided evidence that non-finetuned models display significant self-correction capabilities.", "Highlighted the importance of understanding LLMs' inherent traits for future reasoning model development."], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Self-Correction", "Chain of Thought", "Intrinsic Behavior"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15883", "pdf": "https://arxiv.org/pdf/2506.15883.pdf", "abs": "https://arxiv.org/abs/2506.15883", "title": "Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration", "authors": ["Jonathan Zong", "Isabella Pedraza Pineros", "Mengzhu Katie Chen", "Daniel Hajas", "Arvind Satyanarayan"], "categories": ["cs.HC"], "comment": null, "summary": "Drawing connections between interesting groupings of data and their\nreal-world meaning is an important, yet difficult, part of encountering a new\ndataset. A lay reader might see an interesting visual pattern in a chart but\nlack the domain expertise to explain its meaning. Or, a reader might be\nfamiliar with a real-world concept but struggle to express it in terms of a\ndataset's fields. In response, we developed semantic scaffolding, a technique\nfor using domain-specific information from large language models (LLMs) to\nidentify, explain, and formalize semantically meaningful data groupings. We\npresent groupings in two ways: as semantic bins, which segment a field into\ndomain-specific intervals and categories; and data highlights, which annotate\nsubsets of data records with their real-world meaning. We demonstrate and\nevaluate this technique in Olli, an accessible visualization tool that\nexemplifies tensions around explicitly defining groupings while respecting the\nagency of readers to conduct independent data exploration. We conducted a study\nwith 15 blind and low-vision (BLV) users and found that readers used semantic\nscaffolds to quickly understand the meaning of the data, but were often also\ncritically aware of its influence on their interpretation.", "AI": {"tldr": "The paper introduces semantic scaffolding, a technique to help lay readers understand data by using domain-specific information from LLMs to reveal meaningful groupings and meanings within datasets, demonstrated through an accessible tool called Olli.", "motivation": "Understanding new datasets can be challenging for lay readers who may not have the domain expertise to interpret complex visual patterns or data fields.", "method": "The authors developed semantic scaffolding to identify and explain semantically meaningful data groupings, presenting these through semantic bins and data highlights in a tool named Olli.", "result": "A study involving 15 blind and low-vision users showed that semantic scaffolds helped users quickly grasp the meanings of data, although they were mindful of how these scaffolds influenced their interpretations.", "conclusion": "Semantic scaffolding can enhance comprehension of data for users, but awareness of its influence on interpretation remains a critical consideration.", "key_contributions": ["Development of semantic scaffolding using LLMs for data interpretation", "Creation of Olli, an accessible visualization tool", "Evaluation of its effectiveness with blind and low-vision users"], "limitations": "The study involved a limited sample size of 15 participants, which may affect the generalizability of the results.", "keywords": ["semantic scaffolding", "data visualization", "large language models", "blind and low-vision users", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15911", "pdf": "https://arxiv.org/pdf/2506.15911.pdf", "abs": "https://arxiv.org/abs/2506.15911", "title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents", "authors": ["Mohammad Amaan Sayeed", "Mohammed Talha Alam", "Raza Imam", "Shahab Saquib Sohail", "Amir Hussain"], "categories": ["cs.CL"], "comment": "Under-review at the 4th Muslims in Machine Learning (MusIML) Workshop\n  (ICML-25)", "summary": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the\nProphetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and\nholistic therapies, yet remain inaccessible to many and underutilized in modern\nAI systems. Existing language-model benchmarks focus narrowly on factual recall\nor user preference, leaving a gap in validating culturally grounded medical\nguidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that\naligns 30 carefully curated Prophetic-medicine questions with human-verified\nremedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three\nconfigurations: direct generation, retrieval-augmented generation, and a\nscientific self-critique filter. Each answer is then assessed by a secondary\nLLM serving as an agentic judge, yielding a single 3C3H quality score.\nRetrieval improves factual accuracy by 13%, while the agentic prompt adds\nanother 10% improvement through deeper mechanistic insight and safety\nconsiderations. Our results demonstrate that blending classical Islamic texts\nwith retrieval and self-evaluation enables reliable, culturally sensitive\nmedical question-answering.", "AI": {"tldr": "The paper presents Tibbe-AG, a novel evaluation pipeline that enhances the accessibility and reliability of Islamic medical texts in modern AI systems by assessing the accuracy of LLM-generated answers.", "motivation": "To make centuries-old Islamic medical texts more accessible and applicable in modern AI systems, bridging cultural knowledge gaps and improving medical question-answering.", "method": "A unified evaluation pipeline, Tibbe-AG, aligns questions from Prophetic medicine with human-verified remedies, comparing three LLMs under various configurations. Answers are assessed by an agentic judge LLM to generate a quality score.", "result": "Retrieval methods increased factual accuracy by 13%, while agentic prompting added another 10% improvement, demonstrating that integrating classical texts with AI can enhance medical question-answering.", "conclusion": "Blending classical Islamic medical knowledge with retrieval-augmented models and self-evaluation leads to reliable, culturally sensitive medical insights.", "key_contributions": ["Introduction of the Tibbe-AG evaluation pipeline for Islamic medical texts.", "Validation of LLMs in culturally grounded medical guidance.", "Demonstration of improved accuracy through retrieval and agentic evaluation."], "limitations": "", "keywords": ["Islamic medicine", "language models", "health informatics", "retrieval-augmented generation", "cultural sensitivity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16008", "pdf": "https://arxiv.org/pdf/2506.16008.pdf", "abs": "https://arxiv.org/abs/2506.16008", "title": "ChatAR: Conversation Support using Large Language Model and Augmented Reality", "authors": ["Yuichiro Fujimoto"], "categories": ["cs.HC"], "comment": null, "summary": "Engaging in smooth conversations with others is a crucial social skill.\nHowever, differences in knowledge between conversation participants can\nsometimes hinder effective communication. To tackle this issue, this study\nproposes a real-time support system that integrates head-mounted display\n(HMD)-based augmented reality (AR) technology with large language models\n(LLMs). This system facilitates conversation by recognizing keywords during\ndialogue, generating relevant information using the LLM, reformatting it, and\npresenting it to the user via the HMD. A significant issue with this system is\nthat the user's eye movements may reveal to the conversation partner that they\nare reading the displayed text. This study also proposes a method for\npresenting information that takes into account appropriate eye movements during\nconversation. Two experiments were conducted to evaluate the effectiveness of\nthe proposed system. The first experiment revealed that the proposed\ninformation presentation method reduces the likelihood of the conversation\npartner noticing that the user is reading the displayed text. The second\nexperiment demonstrated that the proposed method led to a more balanced speech\nratio between the user and the conversation partner, as well as a increase in\nthe perceived excitement of the conversation.", "AI": {"tldr": "This study presents a real-time support system for enhancing conversations using AR and LLMs, addressing issues of eye movement visibility while reading displayed information.", "motivation": "To improve communication where knowledge differences hinder smooth conversations, integrating AR technology with LLMs can provide contextual support without revealing user actions to conversation partners.", "method": "The study developed an HMD-based AR system that recognizes keywords and uses LLMs to generate and present relevant information invisibly to conversation partners. It includes methods to manage user eye movements during dialogue.", "result": "The first experiment showed a reduction in the likelihood that conversation partners notice when users read the AR text. The second experiment indicated improved user engagement, with a balanced speech ratio and higher perceived excitement in conversations.", "conclusion": "The proposed AR support system effectively aids real-time communication while mitigating the visibility of information display to conversation partners, enhancing conversational dynamics.", "key_contributions": ["Integration of AR and LLMs for conversation support", "Methodology to manage eye movement visibility during conversations", "Demonstrated improvements in user engagement and conversation dynamics"], "limitations": "The feasibility of real-time implementation in various conversational contexts is yet to be fully assessed.", "keywords": ["Augmented Reality", "Large Language Models", "Conversational Dynamics", "Eye Movement", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.15925", "pdf": "https://arxiv.org/pdf/2506.15925.pdf", "abs": "https://arxiv.org/abs/2506.15925", "title": "Reranking-based Generation for Unbiased Perspective Summarization", "authors": ["Narutatsu Ri", "Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Generating unbiased summaries in real-world settings such as political\nperspective summarization remains a crucial application of Large Language\nModels (LLMs). Yet, existing evaluation frameworks rely on traditional metrics\nfor measuring key attributes such as coverage and faithfulness without\nverifying their applicability, and efforts to develop improved summarizers are\nstill nascent. We address these gaps by (1) identifying reliable metrics for\nmeasuring perspective summary quality, and (2) investigating the efficacy of\nLLM-based methods beyond zero-shot inference. Namely, we build a test set for\nbenchmarking metric reliability using human annotations and show that\ntraditional metrics underperform compared to language model-based metrics,\nwhich prove to be strong evaluators. Using these metrics, we show that\nreranking-based methods yield strong results, and preference tuning with\nsynthetically generated and reranking-labeled data further boosts performance.\nOur findings aim to contribute to the reliable evaluation and development of\nperspective summarization methods.", "AI": {"tldr": "The paper explores improving perspective summarization with LLMs by identifying reliable quality metrics and demonstrating the efficacy of advanced summarization methods.", "motivation": "To enhance unbiased perspective summarization in real-world applications, particularly in political contexts, by addressing gaps in evaluation metrics and summarization methods.", "method": "The authors develop a benchmark test set using human annotations to evaluate metric reliability and compare traditional metrics with language model-based metrics, ultimately showing the advantages of LLM-based methods, including reranking and preference tuning.", "result": "The study finds that traditional metrics are less effective than language model-based metrics in evaluating perspective summary quality. Reranking and preference tuning withsynthetically generated data significantly improve summarization performance.", "conclusion": "The research contributes to better evaluation practices and the development of more effective perspective summarization methods, suggesting that advanced LLM techniques are more reliable.", "key_contributions": ["Identified reliable metrics for measuring the quality of perspective summaries", "Demonstrated that language model-based metrics are more effective than traditional metrics", "Showed that reranking and preference tuning significantly enhance summarization performance"], "limitations": "", "keywords": ["Large Language Models", "perspective summarization", "evaluation metrics", "reranking methods", "preference tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16010", "pdf": "https://arxiv.org/pdf/2506.16010.pdf", "abs": "https://arxiv.org/abs/2506.16010", "title": "SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion", "authors": ["Xiangyang He", "Jiale Li", "Jiahao Chen", "Yang Yang", "Mingming Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Panel discussion allows the audience to learn different perspectives through\ninteractive discussions among experts moderated by a host and a Q&A session\nwith the audience. Despite its benefits, panel discussion in the real world is\ninaccessible to many who do not have the privilege to participate due to\ngeographical, financial, and time constraints. We present SimuPanel, which\nsimulates panel discussions among academic experts through LLM-based\nmulti-agent interaction. It enables users to define topics of interest for the\npanel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel\nemploys a host-expert architecture where each panel member is simulated by an\nagent with specialized expertise, and the panel is visualized in an immersive\n3D environment to enhance engagement. Traditional dialogue generation struggles\nto capture the depth and interactivity of real-world panel discussions. To\naddress this limitation, we propose a novel multi-agent interaction framework\nthat simulates authentic panel dynamics by modeling reasoning strategies and\npersonas of experts grounded in multimedia sources. This framework enables\nagents to dynamically recall and contribute to the discussion based on past\nexperiences from diverse perspectives. Our technical evaluation and the user\nstudy with university students show that SimuPanel was able to simulate more\nin-depth discussions and engage participants to interact with and reflect on\nthe discussions. As a first step in this direction, we offer design\nimplications for future avenues to improve and harness the power of panel\ndiscussion for multimedia learning.", "AI": {"tldr": "SimuPanel simulates academic panel discussions using LLM-based multi-agent interaction to enhance accessibility and engagement in learning.", "motivation": "Panel discussions provide diverse perspectives but are often inaccessible due to various constraints, necessitating a solution that can simulate these discussions for wider access.", "method": "The paper presents SimuPanel, a framework using LLMs to simulate panel discussions with a host-expert architecture and immersive 3D visualizations, allowing users to interact and engage with dynamic discussions.", "result": "SimuPanel successfully simulates authentic panel dynamics, showing improved depth of discussion and participant engagement compared to traditional dialogue generation.", "conclusion": "The findings suggest promising pathways for enhancing panel discussions in multimedia learning, alongside implications for future design.", "key_contributions": ["Introduction of SimuPanel for simulating panel discussions.", "Development of a multi-agent interaction framework for dynamic recall and contribution.", "User study showing enhanced engagement and reflection in discussions."], "limitations": "Limited to academic scenarios; further work needed to generalize to other domains.", "keywords": ["Human-Computer Interaction", "Machine Learning", "Multi-Agent Systems", "Panel Discussions", "Immersive Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15978", "pdf": "https://arxiv.org/pdf/2506.15978.pdf", "abs": "https://arxiv.org/abs/2506.15978", "title": "A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension", "authors": ["Toan Nguyen Hai", "Ha Nguyen Viet", "Truong Quan Xuan", "Duc Do Minh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Vietnamese, the 20th most spoken language with over 102 million native\nspeakers, lacks robust resources for key natural language processing tasks such\nas text segmentation and machine reading comprehension (MRC). To address this\ngap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice\nReading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset\nincludes 15,942 documents for text segmentation and 16,347 synthetic\nmultiple-choice question-answer pairs generated with human quality assurance,\nensuring a reliable and diverse resource. Experiments show that mBERT\nconsistently outperforms monolingual models on both tasks, achieving an\naccuracy of 88.01% on MRC test set and an F1 score of 63.15\\% on text\nsegmentation test set. Our analysis reveals that multilingual models excel in\nNLP tasks for Vietnamese, suggesting potential applications to other\nunder-resourced languages. VSMRC is available at HuggingFace", "AI": {"tldr": "Introduction of a dataset for Vietnamese NLP tasks focused on segmentation and reading comprehension.", "motivation": "To bridge the gap in resources for key NLP tasks in Vietnamese, which is under-resourced despite having a large number of speakers.", "method": "Development of the VSMRC dataset, including 15,942 documents for text segmentation and 16,347 synthetic MRC question-answer pairs, with evaluation performed using mBERT.", "result": "mBERT outperformed monolingual models, achieving 88.01% accuracy on the MRC test set and a 63.15% F1 score on text segmentation.", "conclusion": "The study indicates that multilingual models can effectively enhance NLP tasks for Vietnamese, with implications for similar under-resourced languages.", "key_contributions": ["Introduction of VSMRC dataset", "Demonstration of mBERT's superiority over monolingual models", "Insights for applying multilingual models to under-resourced languages"], "limitations": "Dataset is limited to Wikipedia content and synthetic questions; may not cover all real-world scenarios.", "keywords": ["Vietnamese NLP", "dataset", "mBERT", "text segmentation", "reading comprehension"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.16044", "pdf": "https://arxiv.org/pdf/2506.16044.pdf", "abs": "https://arxiv.org/abs/2506.16044", "title": "Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications", "authors": ["MH Farhadi", "Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Wei Xu", "Reza Abiri"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "With recent advancements in AI and computational tools, intelligent paradigms\nhave emerged to enhance fields like shared autonomy and human-machine teaming\nin healthcare. Advanced AI algorithms (e.g., reinforcement learning) can\nautonomously make decisions to achieve planning and motion goals. However, in\nhealthcare, where human intent is crucial, fully independent machine decisions\nmay not be ideal. This chapter presents a comprehensive review of\nhuman-centered shared autonomy AI frameworks, focusing on upper limb\nbiosignal-based machine interfaces and associated motor control systems,\nincluding computer cursors, robotic arms, and planar platforms. We examine\nmotor planning, learning (rehabilitation), and control, covering conceptual\nfoundations of human-machine teaming in reach-and-grasp tasks and analyzing\nboth theoretical and practical implementations. Each section explores how human\nand machine inputs can be blended for shared autonomy in healthcare\napplications. Topics include human factors, biosignal processing for intent\ndetection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,\nassistive robotics, and Large Language Models (LLMs) as the next frontier. We\npropose adaptive shared autonomy AI as a high-performance paradigm for\ncollaborative human-AI systems, identify key implementation challenges, and\noutline future directions, particularly regarding AI reasoning agents. This\nanalysis aims to bridge neuroscientific insights with robotics to create more\nintuitive, effective, and ethical human-machine teaming frameworks.", "AI": {"tldr": "A review of human-centered shared autonomy AI frameworks in healthcare, focusing on upper limb biosignal-based machine interfaces and their integration for collaborative human-AI systems.", "motivation": "To improve human-machine teaming in healthcare by integrating AI with human intent, particularly through shared autonomy.", "method": "Comprehensive review and analysis of existing AI frameworks, motor planning, learning, and control systems in healthcare applications, with a focus on human factors and biosignal processing.", "result": "Identified effective approaches and challenges in implementing shared autonomy systems in rehabilitation and assistive robotics, highlighting the role of LLMs in future frameworks.", "conclusion": "Adaptive shared autonomy AI can enhance collaborative systems in healthcare, bridging the gap between neuroscientific insights and robotics.", "key_contributions": ["Framework for integrating human intent in AI decision-making", "Analysis of biosignal processing techniques for intent detection", "Recommendations for future directions in human-AI collaboration"], "limitations": "", "keywords": ["Shared Autonomy", "Human-Machine Teaming", "Biosignal Processing", "Assistive Robotics", "Large Language Models"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2506.15981", "pdf": "https://arxiv.org/pdf/2506.15981.pdf", "abs": "https://arxiv.org/abs/2506.15981", "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion", "authors": ["Markus Frohmann", "Gabriel Meseguer-Brocal", "Markus Schedl", "Elena V. Epure"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Findings", "summary": "The rapid advancement of AI-based music generation tools is revolutionizing\nthe music industry but also posing challenges to artists, copyright holders,\nand providers alike. This necessitates reliable methods for detecting such\nAI-generated content. However, existing detectors, relying on either audio or\nlyrics, face key practical limitations: audio-based detectors fail to\ngeneralize to new or unseen generators and are vulnerable to audio\nperturbations; lyrics-based methods require cleanly formatted and accurate\nlyrics, unavailable in practice. To overcome these limitations, we propose a\nnovel, practically grounded approach: a multimodal, modular late-fusion\npipeline that combines automatically transcribed sung lyrics and speech\nfeatures capturing lyrics-related information within the audio. By relying on\nlyrical aspects directly from audio, our method enhances robustness, mitigates\nsusceptibility to low-level artifacts, and enables practical applicability.\nExperiments show that our method, DE-detect, outperforms existing lyrics-based\ndetectors while also being more robust to audio perturbations. Thus, it offers\nan effective, robust solution for detecting AI-generated music in real-world\nscenarios. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection.", "AI": {"tldr": "A novel approach for detecting AI-generated music is proposed using a multimodal pipeline combining transcribed lyrics and speech features, outperforming existing methods.", "motivation": "To address the challenges of detecting AI-generated music content and the limitations of existing detection methods based on audio or lyrics.", "method": "A multimodal, modular late-fusion pipeline that uses automatically transcribed sung lyrics alongside audio-derived speech features.", "result": "The proposed method, DE-detect, shows improved performance over existing lyrics-based detectors and is more robust against audio perturbations.", "conclusion": "DE-detect offers a practical and effective solution for detecting AI-generated music in real-world scenarios, enhancing the reliability of detection methods.", "key_contributions": ["Introduces a multimodal detection approach combining lyrics and audio features.", "Demonstrates superior performance compared to traditional detection methods.", "Enhances robustness against audio perturbations and real-world applicability."], "limitations": "", "keywords": ["AI music generation", "multimodal detection", "audio perturbations"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.16107", "pdf": "https://arxiv.org/pdf/2506.16107.pdf", "abs": "https://arxiv.org/abs/2506.16107", "title": "From 600 Tools to 1 Console: A UX-Driven Transformation", "authors": ["Mariann Kornelia Smith", "Jacqueline Meijer-Irons", "Andrew Millar"], "categories": ["cs.HC"], "comment": null, "summary": "In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a\nsurvey to 10,000 Google Developers (Googlers) and uncovered that Google's\ninternal infrastructure tools were fragmented and inefficient, hindering\ndevelopers' productivity. Using user centered research and design methodologies\nthe team first created a story map and service blueprint to visualize the\nrelationship between internal applications, then formulated a strategic vision\nto consolidate tools, streamline workflows, and measure the impact of their\nwork. We secured executive buy-in and delivered incremental improvements.", "AI": {"tldr": "Explores the improvement of Google's internal infrastructure tools for developers through user-centered research and design methodologies.", "motivation": "To address fragmentation and inefficiency in Google's internal infrastructure tools which hinder developer productivity.", "method": "Utilized user-centered research techniques to create a story map and a service blueprint to visualize tool relationships.", "result": "Formulated a strategic vision to consolidate tools and streamline workflows, leading to secured executive buy-in and delivered incremental improvements.", "conclusion": "Incremental improvements were made to enhance the efficiency of internal tools based on user-centered design.", "key_contributions": ["Created a story map to visualize application relationships", "Developed a strategic vision for tool consolidation", "Secured executive buy-in for improvements"], "limitations": "", "keywords": ["User Experience", "Infrastructure Tools", "Productivity", "Research Design", "Tool Consolidation"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.16024", "pdf": "https://arxiv.org/pdf/2506.16024.pdf", "abs": "https://arxiv.org/abs/2506.16024", "title": "From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation", "authors": ["Zhihan Guo", "Jiele Wu", "Wenqian Cui", "Yifei Zhang", "Minda Hu", "Yufei Wang", "Irwin King"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on long-form context in Large Language Models (LLMs)\nprimarily focuses on the understanding of long-contexts, the Open-ended Long\nText Generation (Open-LTG) remains insufficiently explored. Training a\nlong-context generation model requires curation of gold standard reference\ndata, which is typically nonexistent for informative Open-LTG tasks. However,\nprevious methods only utilize general assessments as reward signals, which\nlimits accuracy. To bridge this gap, we introduce ProxyReward, an innovative\nreinforcement learning (RL) based framework, which includes a dataset and a\nreward signal computation method. Firstly, ProxyReward Dataset generation is\naccomplished through simple prompts that enables the model to create\nautomatically, obviating extensive labeled data or significant manual effort.\nSecondly, ProxyReward Signal offers a targeted evaluation of information\ncomprehensiveness and accuracy for specific questions. The experimental results\nindicate that our method ProxyReward surpasses even GPT-4-Turbo. It can\nsignificantly enhance performance by 20% on the Open-LTG task when training\nwidely used open-source models, while also surpassing the LLM-as-a-Judge\napproach. Our work presents effective methods to enhance the ability of LLMs to\naddress complex open-ended questions posed by human.", "AI": {"tldr": "Introduction of ProxyReward, a framework to enhance Open-ended Long Text Generation in LLMs by addressing the lack of curated reference data with innovative reward signals.", "motivation": "To improve the generation of long-context outputs in LLMs, addressing the limitations in existing methods that lack specific, informative data for training.", "method": "Development of the ProxyReward framework, consisting of a dataset generated through prompts and a novel reward signal for evaluating generation accuracy and comprehensiveness.", "result": "ProxyReward demonstrates a significant performance enhancement of 20% on Open-LTG tasks, outperforming GPT-4-Turbo and the LLM-as-a-Judge approach.", "conclusion": "ProxyReward effectively improves LLM capabilities in generating informative long-form responses to complex prompts, indicating its potential for further applications in diverse areas.", "key_contributions": ["Introduction of ProxyReward as a novel RL-based framework for Open-LTG.", "Creation of a dataset generated from prompts that reduces the need for labeled data.", "Development of tailored reward signals that improve accuracy in long-context generation."], "limitations": "The framework's effectiveness may depend on the nature of the prompts used for dataset generation and the scalability of the method to various domains.", "keywords": ["Long-Form Generation", "Reinforcement Learning", "Large Language Models", "ProxyReward", "Open-ended tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16168", "pdf": "https://arxiv.org/pdf/2506.16168.pdf", "abs": "https://arxiv.org/abs/2506.16168", "title": "On using AI for EEG-based BCI applications: problems, current challenges and future trends", "authors": ["Thomas Barbera", "Jacopo Burger", "Alessandro D'Amelio", "Simone Zini", "Simone Bianco", "Raffaella Lanzarotti", "Paolo Napoletano", "Giuseppe Boccignone", "Jose Luis Contreras-Vidal"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Imagine unlocking the power of the mind to communicate, create, and even\ninteract with the world around us. Recent breakthroughs in Artificial\nIntelligence (AI), especially in how machines \"see\" and \"understand\" language,\nare now fueling exciting progress in decoding brain signals from scalp\nelectroencephalography (EEG). Prima facie, this opens the door to revolutionary\nbrain-computer interfaces (BCIs) designed for real life, moving beyond\ntraditional uses to envision Brain-to-Speech, Brain-to-Image, and even a\nBrain-to-Internet of Things (BCIoT).\n  However, the journey is not as straightforward as it was for Computer Vision\n(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based\nBCIs, particularly in building powerful foundational models, presents unique\nand intricate hurdles that could affect their reliability.\n  Here, we unfold a guided exploration of this dynamic and rapidly evolving\nresearch area. Rather than barely outlining a map of current endeavors and\nresults, the goal is to provide a principled navigation of this hot and\ncutting-edge research landscape. We consider the basic paradigms that emerge\nfrom a causal perspective and the attendant challenges presented to AI-based\nmodels. Looking ahead, we then discuss promising research avenues that could\novercome today's technological, methodological, and ethical limitations. Our\naim is to lay out a clear roadmap for creating truly practical and effective\nEEG-based BCI solutions that can thrive in everyday environments.", "AI": {"tldr": "This paper explores the advancements in AI applied to brain-computer interfaces (BCIs) using EEG, addressing challenges and future research directions.", "motivation": "With recent breakthroughs in AI, particularly in understanding brain signals, there is a possibility to develop practical brain-computer interfaces that could enhance communication and interaction with technology.", "method": "The paper reviews the current state of EEG-based BCIs, factoring in challenges from a causal perspective, and identifies key research avenues to overcome existing limitations.", "result": "The paper provides a roadmap for the development of effective EEG-based BCI solutions, outlining current endeavors and proposing future directions.", "conclusion": "To realize the potential of EEG-based BCIs, we need to navigate the technological, methodological, and ethical challenges through clear, principled research pathways.", "key_contributions": ["Guided exploration of EEG-based BCI advancements using AI.", "Identification of unique challenges faced in real-world applications.", "Proposed roadmap for future research in EEG-based BCI solutions."], "limitations": "The complexities of building reliable foundational models hinder practical applications.", "keywords": ["brain-computer interfaces", "EEG", "artificial intelligence", "machine learning", "BCIoT"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16029", "pdf": "https://arxiv.org/pdf/2506.16029.pdf", "abs": "https://arxiv.org/abs/2506.16029", "title": "EvoLM: In Search of Lost Language Model Training Dynamics", "authors": ["Zhenting Qi", "Fan Nie", "Alexandre Alahi", "James Zou", "Himabindu Lakkaraju", "Yilun Du", "Eric Xing", "Sham Kakade", "Hanlin Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern language model (LM) training has been divided into multiple stages,\nmaking it difficult for downstream developers to evaluate the impact of design\nchoices made at each stage. We present EvoLM, a model suite that enables\nsystematic and transparent analysis of LMs' training dynamics across\npre-training, continued pre-training, supervised fine-tuning, and reinforcement\nlearning. By training over 100 LMs with 1B and 4B parameters from scratch, we\nrigorously evaluate both upstream (language modeling) and downstream\n(problem-solving) reasoning capabilities, including considerations of both\nin-domain and out-of-domain generalization. Key insights highlight the\ndiminishing returns from excessive pre-training and post-training, the\nimportance and practices of mitigating forgetting during domain-specific\ncontinued pre-training, the crucial role of continued pre-training in bridging\npre-training and post-training phases, and various intricate trade-offs when\nconfiguring supervised fine-tuning and reinforcement learning. To facilitate\nopen research and reproducibility, we release all pre-trained and post-trained\nmodels, training datasets for all stages, and our entire training and\nevaluation pipeline.", "AI": {"tldr": "EvoLM is a model suite for analyzing language model training dynamics across multiple stages, evaluating reasoning capabilities and generalization, while providing insights on pre-training practices and releasing all models and datasets for reproducibility.", "motivation": "To enable systematic and transparent analysis of language models' training dynamics due to the complexity introduced by the multi-stage training process.", "method": "Training over 100 LMs with 1B and 4B parameters from scratch, analyzing both upstream (language modeling) and downstream (problem-solving) reasoning capabilities.", "result": "Identified diminishing returns from extensive pre-training, emphasized the significance of continued pre-training, and recognized intricate trade-offs in supervised fine-tuning and reinforcement learning configurations.", "conclusion": "The study underscores the importance of managing training dynamics to optimize model performance while providing resources for open research and reproducibility.", "key_contributions": ["Introduction of EvoLM model suite for analyzing LM training dynamics.", "Systematic evaluation of reasoning capabilities and generalization across various stages.", "Release of training datasets and models for facilitating reproducibility in research."], "limitations": "", "keywords": ["language model", "training dynamics", "pre-training", "fine-tuning", "reproducibility"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16199", "pdf": "https://arxiv.org/pdf/2506.16199.pdf", "abs": "https://arxiv.org/abs/2506.16199", "title": "Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)", "authors": ["Mohammad Naiseh", "Huseyin Dogan", "Stephen Giff", "Nan Jiang"], "categories": ["cs.HC"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) plays a critical role in fostering\nuser trust and understanding in AI-driven systems. However, the design of\neffective XAI interfaces presents significant challenges, particularly for UX\nprofessionals who may lack technical expertise in AI or machine learning.\nExisting explanation methods, such as SHAP, LIME, and counterfactual\nexplanations, often rely on complex technical language and assumptions that are\ndifficult for non-expert users to interpret. To address these gaps, we propose\na UX Research (UXR) Playbook for XAI - a practical framework aimed at\nsupporting UX professionals in designing accessible, transparent, and\ntrustworthy AI experiences. Our playbook offers actionable guidance to help\nbridge the gap between technical explainability methods and user centred\ndesign, empowering designers to create AI interactions that foster better\nunderstanding, trust, and responsible AI adoption.", "AI": {"tldr": "The paper proposes a UX Research Playbook for Explainable AI (XAI) to support UX professionals in designing user-friendly AI interfaces.", "motivation": "To address the challenges faced by UX professionals in creating effective XAI interfaces that are understandable for non-expert users.", "method": "Development of a UX Research (UXR) Playbook that provides actionable guidance for designing accessible, transparent, and trustworthy AI experiences.", "result": "The playbook offers strategies that bridge the gap between technical explanation methods and user-centered design, fostering better understanding and trust in AI.", "conclusion": "The framework enhances the capabilities of UX designers to produce more transparent and responsible AI interactions.", "key_contributions": ["Introduction of a UX Research Playbook for XAI", "Guidance for UX designers to improve AI interaction transparency", "Strategies to facilitate user understanding of complex AI systems"], "limitations": "", "keywords": ["Explainable AI", "User Experience", "UX Research", "Trust in AI", "Human-Centered Design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16037", "pdf": "https://arxiv.org/pdf/2506.16037.pdf", "abs": "https://arxiv.org/abs/2506.16037", "title": "Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3", "authors": ["Xinyue Huang", "Ziqi Lin", "Fang Sun", "Wenchao Zhang", "Kejian Tong", "Yunbo Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents a novel Retrieval-Augmented Generation (RAG) framework\ntailored for complex question answering tasks, addressing challenges in\nmulti-hop reasoning and contextual understanding across lengthy documents.\nBuilt upon LLaMA 3, the framework integrates a dense retrieval module with\nadvanced context fusion and multi-hop reasoning mechanisms, enabling more\naccurate and coherent response generation. A joint optimization strategy\ncombining retrieval likelihood and generation cross-entropy improves the\nmodel's robustness and adaptability. Experimental results show that the\nproposed system outperforms existing retrieval-augmented and generative\nbaselines, confirming its effectiveness in delivering precise, contextually\ngrounded answers.", "AI": {"tldr": "This paper introduces a new RAG framework for complex question answering that enhances multi-hop reasoning and context understanding, leading to improved response accuracy.", "motivation": "To address challenges in multi-hop reasoning and contextual understanding in long documents for question answering tasks.", "method": "The framework combines a dense retrieval module with context fusion and multi-hop reasoning, optimized through a joint strategy of retrieval likelihood and generation cross-entropy.", "result": "The proposed system outperforms existing RAG and generative baselines in terms of accuracy and coherence of responses.", "conclusion": "The framework demonstrates improved performance in delivering precise answers, confirming its effectiveness for complex question answering.", "key_contributions": ["Novel RAG framework for question answering", "Integration of retrieval and generation mechanisms", "Joint optimization strategy for enhanced performance"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "multi-hop reasoning", "contextual understanding", "LLaMA 3", "question answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16312", "pdf": "https://arxiv.org/pdf/2506.16312.pdf", "abs": "https://arxiv.org/abs/2506.16312", "title": "When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard", "authors": ["Angxuan Chen"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigated the impact of a theory-driven, explainable Learning\nAnalytics Dashboard (LAD) on university students' human-AI collaborative\nacademic abstract writing task. Grounded in Self-Regulated Learning (SRL)\ntheory and incorporating Explainable AI (XAI) principles, our LAD featured a\nthree-layered design (Visual, Explainable, Interactive). In an experimental\nstudy, participants were randomly assigned to either an experimental group\n(using the full explainable LAD) or a control group (using a visual-only LAD)\nto collaboratively write an academic abstract with a Generative AI. While\nquantitative analysis revealed no significant difference in the quality of\nco-authored abstracts between the two groups, a significant and noteworthy\ndifference emerged in conceptual understanding: students in the explainable LAD\ngroup demonstrated a superior grasp of abstract writing principles, as\nevidenced by their higher scores on a knowledge test (p= .026). These findings\nhighlight that while basic AI-generated feedback may suffice for immediate task\ncompletion, the provision of explainable feedback is crucial for fostering\ndeeper learning, enhancing conceptual understanding, and developing\ntransferable skills fundamental to self-regulated learning in academic writing\ncontexts.", "AI": {"tldr": "The study evaluates the effectiveness of an explainable Learning Analytics Dashboard in enhancing students' understanding of AI-supported academic writing.", "motivation": "To explore the role of explainable AI in improving students' conceptual understanding during collaborative academic writing tasks.", "method": "An experimental study was conducted where participants were divided into two groups: one used an explainable Learning Analytics Dashboard and the other a visual-only dashboard.", "result": "The explainable LAD group scored higher on a knowledge test related to abstract writing principles, indicating better conceptual understanding, though no significant difference in the quality of abstracts was noted.", "conclusion": "Explainable feedback is essential for deeper learning and skill development in self-regulated learning contexts, despite basic AI feedback being adequate for task completion.", "key_contributions": ["Demonstrated the importance of explainable AI in educational settings.", "Revealed significant improvements in conceptual understanding with the use of explainable dashboards.", "Located foundational support for self-regulated learning theories in AI-assisted tasks."], "limitations": "No significant difference in the overall quality of the academic abstracts was found, which may limit the conclusions drawn regarding the effectiveness of the explainable LAD.", "keywords": ["Learning Analytics Dashboard", "Explainable AI", "Self-Regulated Learning", "Academic Writing", "Human-AI Collaboration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16043", "pdf": "https://arxiv.org/pdf/2506.16043.pdf", "abs": "https://arxiv.org/abs/2506.16043", "title": "DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling", "authors": ["Fei Wang", "Xingchen Wan", "Ruoxi Sun", "Jiefeng Chen", "Sercan √ñ. Arƒ±k"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Inference-time scaling has proven effective in boosting large language model\n(LLM) performance through increased test-time computation. Yet, its practical\napplication is often hindered by reliance on external verifiers or a lack of\noptimization for realistic computational constraints. We propose DynScaling,\nwhich addresses these limitations through two primary innovations: an\nintegrated parallel-sequential sampling strategy and a bandit-based dynamic\nbudget allocation framework. The integrated sampling strategy unifies parallel\nand sequential sampling by constructing synthetic sequential reasoning chains\nfrom initially independent parallel responses, promoting diverse and coherent\nreasoning trajectories. The dynamic budget allocation framework formulates the\nallocation of computational resources as a multi-armed bandit problem,\nadaptively distributing the inference budget across queries based on the\nuncertainty of previously sampled responses, thereby maximizing computational\nefficiency. By combining these components, DynScaling effectively improves LLM\nperformance under practical resource constraints without the need for external\nverifiers. Experimental results demonstrate that DynScaling consistently\nsurpasses existing verifier-free inference scaling baselines in both task\nperformance and computational cost.", "AI": {"tldr": "DynScaling boosts LLM performance under resource constraints using a novel sampling strategy and dynamic budget allocation without external verifiers.", "motivation": "Improving LLM performance while minimizing reliance on external verification and optimizing for practical computational limits.", "method": "DynScaling employs an integrated parallel-sequential sampling strategy and a multi-armed bandit framework for dynamic budget allocation.", "result": "DynScaling outperforms existing verifier-free scaling methods in task performance and reduces computational costs.", "conclusion": "By effectively managing computational resources and improving sampling methods, DynScaling enhances LLM performance in practical scenarios.", "key_contributions": ["Integrated parallel-sequential sampling strategy", "Bandit-based dynamic budget allocation framework", "Verifier-free model optimization"], "limitations": "", "keywords": ["large language models", "dynamic scaling", "budget allocation", "sampling strategy", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16345", "pdf": "https://arxiv.org/pdf/2506.16345.pdf", "abs": "https://arxiv.org/abs/2506.16345", "title": "Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Bruna Capeleti", "Rafael Ferreira Mello", "Andr√© Freire", "Luciana Zaina"], "categories": ["cs.HC", "H.5.2"], "comment": "Paper accepted at the 20th IFIP TC13 International Conference on\n  Human-Computer Interaction (INTERACT) 2025", "summary": "Heuristic evaluation is a widely used method in Human-Computer Interaction\n(HCI) to inspect interfaces and identify issues based on heuristics. Recently,\nLarge Language Models (LLMs), such as GPT-4o, have been applied in HCI to\nassist in persona creation, the ideation process, and the analysis of\nsemi-structured interviews. However, considering the need to understand\nheuristics and the high degree of abstraction required to evaluate them, LLMs\nmay have difficulty conducting heuristic evaluation. However, prior research\nhas not investigated GPT-4o's performance in heuristic evaluation compared to\nHCI experts in web-based systems. In this context, this study aims to compare\nthe results of a heuristic evaluation performed by GPT-4o and human experts. To\nthis end, we selected a set of screenshots from a web system and asked GPT-4o\nto perform a heuristic evaluation based on Nielsen's Heuristics from a\nliterature-grounded prompt. Our results indicate that only 21.2% of the issues\nidentified by human experts were also identified by GPT-4o, despite it found 27\nnew issues. We also found that GPT-4o performed better for heuristics related\nto aesthetic and minimalist design and match between system and real world,\nwhereas it has difficulty identifying issues in heuristics related to\nflexibility, control, and user efficiency. Additionally, we noticed that GPT-4o\ngenerated several false positives due to hallucinations and attempts to predict\nissues. Finally, we highlight five takeaways for the conscious use of GPT-4o in\nheuristic evaluations.", "AI": {"tldr": "This study compares the heuristic evaluation performance of GPT-4o with human experts, revealing GPT-4o's strengths and limitations in identifying usability issues in web systems.", "motivation": "To investigate the performance of GPT-4o in heuristic evaluation compared to HCI experts.", "method": "Heuristic evaluations were conducted by both GPT-4o and human experts on selected screenshots from a web system using Nielsen's Heuristics.", "result": "Only 21.2% of the issues identified by human experts were also identified by GPT-4o, which additionally discovered 27 new issues, mainly excelling in aesthetic and minimalist design heuristics, but struggling with flexibility, control, and efficiency.", "conclusion": "GPT-4o shows promise in heuristic evaluation but has notable limitations, including hallucinations resulting in false positives, suggesting cautious use in HCI.", "key_contributions": ["Comparison of GPT-4o and human expert evaluations in HCI", "Identification of strengths and weaknesses of GPT-4o in heuristic evaluations", "Recommendations for using GPT-4o in practical heuristic evaluation settings"], "limitations": "GPT-4o tends to generate false positives and may struggle with specific heuristic categories.", "keywords": ["Human-Computer Interaction", "Heuristic Evaluation", "Large Language Models", "GPT-4o", "Usability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16052", "pdf": "https://arxiv.org/pdf/2506.16052.pdf", "abs": "https://arxiv.org/abs/2506.16052", "title": "A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text", "authors": ["Devesh Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of online communication platforms has created unprecedented\nopportunities for global connectivity while simultaneously enabling harmful\nbehaviors such as cyberbullying, which affects approximately 54.4\\% of\nteenagers according to recent research. This paper presents a hybrid\narchitecture that combines the contextual understanding capabilities of\ntransformer-based models with the pattern recognition strengths of broad\nlearning systems for effective cyberbullying detection. This approach\nintegrates a modified DeBERTa model augmented with Squeeze-and-Excitation\nblocks and sentiment analysis capabilities with a Gated Broad Learning System\n(GBLS) classifier, creating a synergistic framework that outperforms existing\napproaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +\nGBLS model achieved good performance on four English datasets: 79.3\\% accuracy\non HateXplain, 95.41\\% accuracy on SOSNet, 91.37\\% accuracy on Mendeley-I, and\n94.67\\% accuracy on Mendeley-II. Beyond performance gains, the framework\nincorporates comprehensive explainability mechanisms including token-level\nattribution analysis, LIME-based local interpretations, and confidence\ncalibration, addressing critical transparency requirements in automated content\nmoderation. Ablation studies confirm the meaningful contribution of each\narchitectural component, while failure case analysis reveals specific\nchallenges in detecting implicit bias and sarcastic content, providing valuable\ninsights for future improvements in cyberbullying detection systems.", "AI": {"tldr": "A hybrid architecture combining transformer-based models and broad learning systems for enhanced cyberbullying detection is proposed, achieving significantly high accuracy across several datasets.", "motivation": "To address the increasing prevalence of cyberbullying among teenagers by improving detection mechanisms using machine learning techniques.", "method": "The proposed model integrates a modified DeBERTa with Squeeze-and-Excitation blocks and a Gated Broad Learning System classifier for robust cyberbullying detection.", "result": "The ModifiedDeBERTa + GBLS model reached 79.3% accuracy on HateXplain, 95.41% on SOSNet, 91.37% on Mendeley-I, and 94.67% on Mendeley-II datasets.", "conclusion": "The framework not only enhances detection performance but also emphasizes explainability, making it suitable for automated content moderation.", "key_contributions": ["Development of a novel hybrid cyberbullying detection model", "Integration of explainability mechanisms", "High performance across multiple benchmark datasets"], "limitations": "Challenges remain in detecting implicit bias and sarcastic content.", "keywords": ["cyberbullying", "machine learning", "transformer models", "explainability", "content moderation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16468", "pdf": "https://arxiv.org/pdf/2506.16468.pdf", "abs": "https://arxiv.org/abs/2506.16468", "title": "Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury", "authors": ["Vlad Cnejevici", "Matthias Ponfick", "Raul C. S√Æmpetru", "Alessandro Del Vecchio"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "26 pages, 7 figures", "summary": "Restoring movement of a paralyzed foot is a key challenge in helping\nindividuals with neurological conditions such as spinal cord injury (SCI) to\nimprove their quality of life. Neuroprostheses based on functional electrical\nstimulation (FES) can restore the physiological range of motion by stimulating\nthe affected muscles using surface electrodes. We have previously shown that,\ndespite chronic motor-complete SCI, it is possible to capture paralyzed hand\nmovements in individuals with tetraplegia using spared and modulated motor unit\n(MU) activity decoded with non-invasive electromyography (EMG) sensors. This\nstudy investigated whether a wearable high-density surface EMG system could\ncapture and control paralyzed foot kinematics in closed-loop control with an\nFES system. We found that all our participants with SCI (2 with chronic SCI and\n3 with acute SCI) retained distinct spared EMG activity for at least three\nankle movements, which allowed them to reliably control a digital cursor using\ntheir spared tibialis anterior and triceps surae MU activity. Movement\nseparability was further reconfirmed by extracting task-modulated MU activity\nduring foot flexion/extension (3-7 modulated MUs/participant). Three\nparticipants were further able to modulate and maintain their foot\nflexion/extension EMG levels with an accuracy of >70%. Lastly, we show that\nreal-time control of a FES system using EMG from the affected limb can restore\nfoot movements in a highly intuitive way, significantly improving the lost or\npathological foot range of motion. Our system provides an intuitive approach\nfor closed-loop control of FES that has the potential to assist individuals\nwith SCI in regaining lost motor functions.", "AI": {"tldr": "This study explores a wearable high-density surface EMG system's ability to capture and control paralyzed foot kinematics for individuals with spinal cord injury (SCI) using functional electrical stimulation (FES).", "motivation": "To restore movement in paralyzed feet for individuals with neurological conditions, addressing key challenges related to spinal cord injury and improving quality of life.", "method": "A high-density surface EMG system was utilized to capture muscle activity and control foot movement in a closed-loop FES system, involving participants with chronic and acute SCI.", "result": "Participants exhibited distinct spared EMG activity, enabling them to control a digital cursor with their muscle activity and some achieved over 70% accuracy in foot movement modulation with FES.", "conclusion": "The study demonstrates a viable method for intuitive control of FES systems to help individuals with SCI regain foot movement, significantly enhancing their range of motion.", "key_contributions": ["Use of a wearable high-density EMG system for real-time control of FES in foot movements.", "Demonstration of potential for intuitively restoring lost motor functions in individuals with SCI.", "Identification of distinct spared muscle activity patterns that can be leveraged for movement control."], "limitations": "Limited sample size with only five participants; results may not generalize to all individuals with SCI.", "keywords": ["Functional Electrical Stimulation", "Spinal Cord Injury", "Electromyography", "Human-Computer Interaction", "Neuroprostheses"], "importance_score": 8, "read_time_minutes": 26}}
{"id": "2506.16055", "pdf": "https://arxiv.org/pdf/2506.16055.pdf", "abs": "https://arxiv.org/abs/2506.16055", "title": "Knee-Deep in C-RASP: A Transformer Depth Hierarchy", "authors": ["Andy Yang", "Micha√´l Cadilhac", "David Chiang"], "categories": ["cs.CL", "cs.FL"], "comment": "27 pages, 4 figures", "summary": "It has been observed that transformers with greater depth (that is, more\nlayers) have more capabilities, but can we establish formally which\ncapabilities are gained with greater depth? We answer this question with a\ntheoretical proof followed by an empirical study. First, we consider\ntransformers that round to fixed precision except inside attention. We show\nthat this subclass of transformers is expressively equivalent to the\nprogramming language C-RASP and this equivalence preserves depth. Second, we\nprove that deeper C-RASP programs are more expressive than shallower C-RASP\nprograms, implying that deeper transformers are more expressive than shallower\ntransformers (within the subclass mentioned above). These results are\nestablished by studying a form of temporal logic with counting operators, which\nwas shown equivalent to C-RASP in previous work. Finally, we provide empirical\nevidence that our theory predicts the depth required for transformers without\npositional encodings to length-generalize on a family of sequential dependency\ntasks.", "AI": {"tldr": "The paper establishes formal insights on the capabilities gained with greater depth in transformers, providing theoretical proof and empirical evidence of deeper transformers' expressiveness.", "motivation": "To formally determine the capabilities gained by increasing the depth of transformers in machine learning models.", "method": "The study involves theoretical proofs showing that deeper transformers expressively outperform shallower ones by relating them to a programming language model (C-RASP), alongside an empirical study to validate these findings.", "result": "The research demonstrates that deeper C-RASP programs (and thus transformers) offer greater expressiveness, further supported by empirical evidence regarding their performance in sequential tasks without positional encodings.", "conclusion": "Deep transformers are shown to be more expressive than shallower ones, with theoretical backing and empirical validation indicating a better capability in handling sequential dependencies.", "key_contributions": ["Formal proof of greater expressiveness in deeper transformers", "Empirical evidence supporting theory on transformer depth", "Equivalence established between transformers and C-RASP programming language"], "limitations": "", "keywords": ["transformers", "depth", "C-RASP", "expressiveness", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16473", "pdf": "https://arxiv.org/pdf/2506.16473.pdf", "abs": "https://arxiv.org/abs/2506.16473", "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions.", "AI": {"tldr": "This study compares the thematic and semantic alignment of disclosures in robot-led conversations with those in human therapy sessions, revealing significant parallels that suggest robots can augment mental health interventions.", "motivation": "To understand the similarities between robot-led supportive dialogue and traditional human therapy, especially as conversational agents engage more in emotionally supportive interactions.", "method": "Two datasets were analyzed: one from human therapists and another from a social robot (QTrobot) using sentence embeddings and K-means clustering for thematic alignment assessment, validated by Euclidean distances.", "result": "90.88% of the robot conversation disclosures could be mapped to clusters from the human therapy dataset, indicating shared topical structures and strong semantic overlap in respondents' disclosures.", "conclusion": "The study demonstrates significant parallels between robot-led and human-led therapy conversations, suggesting potential for robots in mental health interventions.", "key_contributions": ["Investigation of thematic alignment between robot and human therapy sessions", "Analysis of semantic overlap in responses to similar themes", "Demonstration of the robot's potential in augmenting mental health support"], "limitations": "", "keywords": ["conversational agents", "therapy", "robot interactions", "semantic alignment", "mental health"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16064", "pdf": "https://arxiv.org/pdf/2506.16064.pdf", "abs": "https://arxiv.org/abs/2506.16064", "title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning", "authors": ["Duc Hieu Ho", "Chenglin Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated robust capabilities across\nvarious natural language tasks. However, producing outputs that are\nconsistently honest and helpful remains an open challenge. To overcome this\nchallenge, this paper tackles the problem through two complementary directions.\nIt conducts a comprehensive benchmark evaluation of ten widely used large\nlanguage models, including both proprietary and open-weight models from OpenAI,\nMeta, and Google. In parallel, it proposes a novel prompting strategy,\nself-critique-guided curiosity refinement prompting. The key idea behind this\nstrategy is enabling models to self-critique and refine their responses without\nadditional training. The proposed method extends the curiosity-driven prompting\nstrategy by incorporating two lightweight in-context steps including\nself-critique step and refinement step.\n  The experiment results on the HONESET dataset evaluated using the framework\n$\\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a\njudge of honesty and helpfulness, show consistent improvements across all\nmodels. The approach reduces the number of poor-quality responses, increases\nhigh-quality responses, and achieves relative gains in $\\mathrm{H}^2$ scores\nranging from 1.4% to 4.3% compared to curiosity-driven prompting across\nevaluated models. These results highlight the effectiveness of structured\nself-refinement as a scalable and training-free strategy to improve the\ntrustworthiness of LLMs outputs.", "AI": {"tldr": "This paper evaluates ten large language models and proposes a novel prompting strategy to enhance their honesty and helpfulness.", "motivation": "Producing outputs that are consistently honest and helpful from large language models remains a challenge.", "method": "The paper benchmarks ten large language models and introduces a self-critique-guided curiosity refinement prompting strategy, which allows models to self-critique and refine their outputs without retraining.", "result": "The proposed method shows consistent improvements in reducing poor-quality responses and increasing high-quality responses, with relative gains in honesty and helpfulness scores ranging from 1.4% to 4.3%.", "conclusion": "Structured self-refinement is an effective, scalable, and training-free strategy for improving the trustworthiness of outputs from large language models.", "key_contributions": ["Benchmark evaluation of ten large language models", "Introduction of self-critique-guided prompting strategy", "Demonstration of improvements in honesty and helpfulness of model outputs"], "limitations": "", "keywords": ["large language models", "self-critique", "honesty and helpfulness", "curiosity-driven prompting", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16542", "pdf": "https://arxiv.org/pdf/2506.16542.pdf", "abs": "https://arxiv.org/abs/2506.16542", "title": "Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence", "authors": ["Nathalia Gomez", "S. Sue Batham", "Mathias Volonte", "Tiffany D. Do"], "categories": ["cs.HC"], "comment": "6 pages, To Appear in Companion Publication of the 2025 Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW Companion '25)", "summary": "Technical interviews are a critical yet stressful step in the hiring process\nfor computer science graduates, often hindered by limited access to practice\nopportunities. This formative qualitative study (n=20) explores whether a\nmultimodal AI system can realistically simulate technical interviews and\nsupport confidence-building among candidates. Participants engaged with an\nAI-driven mock interview tool featuring whiteboarding tasks and real-time\nfeedback. Many described the experience as realistic and helpful, noting\nincreased confidence and improved articulation of problem-solving decisions.\nHowever, challenges with conversational flow and timing were noted. These\nfindings demonstrate the potential of AI-driven technical interviews as\nscalable and realistic preparation tools, suggesting that future research could\nexplore variations in interviewer behavior and their potential effects on\ncandidate preparation.", "AI": {"tldr": "This study examines the effectiveness of a multimodal AI system designed to simulate technical interviews for computer science graduates, focusing on its impact on candidate confidence and preparation.", "motivation": "To address the stress and limited practice opportunities faced by computer science graduates during technical interviews.", "method": "A formative qualitative study was conducted with 20 participants who engaged with an AI-driven mock interview tool that included whiteboarding tasks and real-time feedback.", "result": "Participants reported that the AI simulation was realistic and helpful, leading to increased confidence and better communication of their problem-solving processes, despite some challenges in conversational flow and timing.", "conclusion": "The study indicates that AI-driven technical interviews can serve as effective and scalable preparation tools, pointing towards the need for further exploration into variations in interviewer behavior.", "key_contributions": ["Demonstrated the feasibility of AI systems in simulating technical interviews.", "Showed improvements in candidate confidence and articulation during interviews.", "Identified specific challenges in conversational flow and timing that need addressing."], "limitations": "Challenges with conversational flow and timing were noted by participants.", "keywords": ["AI-driven interviews", "technical interviews", "confidence-building", "qualitative study", "computer science graduates"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2506.16066", "pdf": "https://arxiv.org/pdf/2506.16066.pdf", "abs": "https://arxiv.org/abs/2506.16066", "title": "Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI", "authors": ["Devesh Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "The growth of digital communication platforms has led to increased\ncyberbullying incidents worldwide, creating a need for automated detection\nsystems to protect users. The rise of code-mixed Hindi-English (Hinglish)\ncommunication on digital platforms poses challenges for existing cyberbullying\ndetection systems, which were designed primarily for monolingual text. This\npaper presents a framework for cyberbullying detection in Hinglish text using\nthe Multilingual Representations for Indian Languages (MURIL) architecture to\naddress limitations in current approaches. Evaluation across six benchmark\ndatasets -- Bohra \\textit{et al.}, BullyExplain, BullySentemo, Kumar \\textit{et\nal.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based\napproach outperforms existing multilingual models including RoBERTa and\nIndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies\nof 86.97\\% on Bohra, 84.62\\% on BullyExplain, 86.03\\% on BullySentemo, 75.41\\%\non Kumar datasets, 83.92\\% on HASOC 2021, and 94.63\\% on Mendeley dataset. The\nframework includes explainability features through attribution analysis and\ncross-linguistic pattern recognition. Ablation studies show that selective\nlayer freezing, appropriate classification head design, and specialized\npreprocessing for code-mixed content improve detection performance, while\nfailure analysis identifies challenges including context-dependent\ninterpretation, cultural understanding, and cross-linguistic sarcasm detection,\nproviding directions for future research in multilingual cyberbullying\ndetection.", "AI": {"tldr": "This paper presents a framework for detecting cyberbullying in Hinglish text, outperforming existing models with explainability features.", "motivation": "The rise of Hinglish in digital communication creates challenges for existing cyberbullying detection systems designed for monolingual text.", "method": "A framework using the Multilingual Representations for Indian Languages (MURIL) architecture evaluated on six benchmark datasets.", "result": "The MURIL-based approach outperforms existing multilingual models with accuracy improvements of up to 13.07 percentage points across various datasets.", "conclusion": "The proposed framework improves detection performance with specialized techniques, while highlighting challenges for future research.", "key_contributions": ["Development of a Hinglish cyberbullying detection framework using the MURIL architecture.", "Evaluation against multiple benchmark datasets demonstrating superior performance versus existing models.", "Incorporation of explainability features to enhance understanding of detection mechanisms."], "limitations": "Challenges include context-dependent interpretation, cultural nuances, and sarcasm detection in cross-linguistic contexts.", "keywords": ["cyberbullying detection", "Hinglish", "MURIL", "machine learning", "explainability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571.pdf", "abs": "https://arxiv.org/abs/2506.16571", "title": "Capturing Visualization Design Rationale", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.HC"], "comment": null, "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "AI": {"tldr": "This paper introduces a new dataset and methodology to understand the rationale behind visualization design using real-world student-created visualization notebooks and large language models for analysis.", "motivation": "Existing datasets for data visualization emphasize interpretation over understanding the encoding of visualizations. This paper aims to fill that gap by using real-world examples.", "method": "The authors collected literate visualization notebooks from students, which include design rationales. They used large language models to generate and categorize question-answer-rationale triples and validate them to create a dataset.", "result": "The resulting dataset captures the design choices and rationales behind visualizations as articulated by students, providing a richer context for understanding visualization design.", "conclusion": "This approach not only highlights visualization design choices but also serves as a valuable resource for further research in data visualization and natural language processing.", "key_contributions": ["Introduction of a dataset focused on design rationale in visualizations.", "Use of real-world student notebooks as source material.", "Application of LLMs to generate and validate question-answer-rationale triples."], "limitations": "The dataset is limited to the context of student projects and may not reflect broader professional practices in visualization design.", "keywords": ["data visualization", "natural language processing", "visualization design rationale", "large language models", "literate visualization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16123", "pdf": "https://arxiv.org/pdf/2506.16123.pdf", "abs": "https://arxiv.org/abs/2506.16123", "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning", "authors": ["Natapong Nitarach", "Warit Sirichotedumrong", "Panop Pitchayarthorn", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kunat Pipatanakul"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\napproach that incorporates insights from domain-specific expert financial\nreasoning to guide the reasoning traces of large language models. We\ninvestigate that there are three main prompting styles in FinNLP: (1) standard\nprompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an\nexplicit reasoning structure, such as the use of tags; and (3) structured CoT\nprompting--CoT prompting with explicit instructions or examples that define\nstructured reasoning steps. Previously, FinNLP has primarily focused on prompt\nengineering with either standard or unstructured CoT prompting. However,\nstructured CoT prompting has received limited attention in prior work.\nFurthermore, the design of reasoning structures in structured CoT prompting is\noften based on heuristics from non-domain experts. In this study, we\ninvestigate each prompting approach in FinNLP. We evaluate the three main\nprompting styles and FinCoT on CFA-style questions spanning ten financial\ndomains. We observe that FinCoT improves performance from 63.2% to 80.5% and\nQwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens\neight-fold compared to structured CoT prompting. Our findings show that\ndomain-aligned structured prompts not only improve performance and reduce\ninference costs but also yield more interpretable and expert-aligned reasoning\ntraces.", "AI": {"tldr": "FinCoT is a structured chain-of-thought prompting approach improving reasoning in large language models for financial applications.", "motivation": "To enhance the reasoning abilities of large language models in financial natural language processing (FinNLP) by integrating domain-specific expert insights into structured prompting approaches.", "method": "The study evaluates three prompting styles‚Äîstandard prompting, unstructured CoT, and structured CoT‚Äîspecifically assessing the effectiveness of FinCoT on CFA-style questions across ten financial domains.", "result": "FinCoT achieves an improvement in performance from 63.2% to 80.5% and an increase for Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while also significantly reducing the number of generated tokens by eight-fold compared to standard structured CoT prompting.", "conclusion": "Domain-aligned structured prompts lead to improved performance, lower inference costs, and more interpretable reasoning in financial applications.", "key_contributions": ["Introduction of FinCoT as a structured prompting method in FinNLP.", "Comprehensive evaluation of three prompting styles in financial reasoning tasks.", "Demonstration of performance improvement and cost reduction through structured reasoning methods."], "limitations": "", "keywords": ["chain-of-thought", "financial NLP", "prompt engineering", "domain expertise", "structured reasoning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.16677", "pdf": "https://arxiv.org/pdf/2506.16677.pdf", "abs": "https://arxiv.org/abs/2506.16677", "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration", "authors": ["Hao Guo", "Wei Fan", "Shaohui Liu", "Feng Jiang", "Chunzhi Yi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Trust prediction is a key issue in human-robot collaboration, especially in\nconstruction scenarios where maintaining appropriate trust calibration is\ncritical for safety and efficiency. This paper introduces the\nPerformance-guided Physiological signal-based Trust Prediction (PPTP), a novel\nframework designed to improve trust assessment. We designed a human-robot\nconstruction scenario with three difficulty levels to induce different trust\nstates. Our approach integrates synchronized multimodal physiological signals\n(ECG, GSR, and EMG) with collaboration performance evaluation to predict human\ntrust levels. Individual physiological signals are processed using\ncollaboration performance information as guiding cues, leveraging the\nstandardized nature of collaboration performance to compensate for individual\nvariations in physiological responses. Extensive experiments demonstrate the\nefficacy of our cross-modality fusion method in significantly improving trust\nclassification performance. Our model achieves over 81% accuracy in three-level\ntrust classification, outperforming the best baseline method by 6.7%, and\nnotably reaches 74.3% accuracy in high-resolution seven-level classification,\nwhich is a first in trust prediction research. Ablation experiments further\nvalidate the superiority of physiological signal processing guided by\ncollaboration performance assessment.", "AI": {"tldr": "This paper presents a novel framework for predicting trust in human-robot collaboration using physiological signals and performance evaluation.", "motivation": "Trust calibration is crucial for safety and efficiency in human-robot collaboration, particularly in construction settings.", "method": "The Performance-guided Physiological signal-based Trust Prediction (PPTP) framework integrates synchronized multimodal physiological signals with collaboration performance evaluation.", "result": "The model achieves over 81% accuracy in three-level trust classification and 74.3% in seven-level classification, surpassing previous methods.", "conclusion": "The proposed method significantly enhances trust prediction in collaboration scenarios by using physiological signals guided by collaboration performance.", "key_contributions": ["Introduction of a multimodal approach for trust prediction incorporating physiological signals and performance metrics.", "Achievement of high accuracy in trust classification across multiple levels.", "Validation of the method through extensive experiments, demonstrating improvement over baseline techniques."], "limitations": "", "keywords": ["trust prediction", "human-robot collaboration", "physiological signals", "collaboration performance", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16151", "pdf": "https://arxiv.org/pdf/2506.16151.pdf", "abs": "https://arxiv.org/abs/2506.16151", "title": "Under the Shadow of Babel: How Language Shapes Reasoning in LLMs", "authors": ["Chenxi Wang", "Yixuan Zhang", "Lang Gao", "Zixiang Xu", "Zirui Song", "Yanbo Wang", "Xiuying Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Language is not only a tool for communication but also a medium for human\ncognition and reasoning. If, as linguistic relativity suggests, the structure\nof language shapes cognitive patterns, then large language models (LLMs)\ntrained on human language may also internalize the habitual logical structures\nembedded in different languages. To examine this hypothesis, we introduce\nBICAUSE, a structured bilingual dataset for causal reasoning, which includes\nsemantically aligned Chinese and English samples in both forward and reversed\ncausal forms. Our study reveals three key findings: (1) LLMs exhibit\ntypologically aligned attention patterns, focusing more on causes and\nsentence-initial connectives in Chinese, while showing a more balanced\ndistribution in English. (2) Models internalize language-specific preferences\nfor causal word order and often rigidly apply them to atypical inputs, leading\nto degraded performance, especially in Chinese. (3) When causal reasoning\nsucceeds, model representations converge toward semantically aligned\nabstractions across languages, indicating a shared understanding beyond surface\nform. Overall, these results suggest that LLMs not only mimic surface\nlinguistic forms but also internalize the reasoning biases shaped by language.\nRooted in cognitive linguistic theory, this phenomenon is for the first time\nempirically verified through structural analysis of model internals.", "AI": {"tldr": "Introducing BICAUSE, a dataset for examining bilingual causal reasoning in LLMs.", "motivation": "To investigate how large language models may internalize reasoning biases shaped by language structures, influenced by linguistic relativity.", "method": "Analysis of a structured bilingual dataset (BICAUSE) with Chinese and English samples for causal reasoning, evaluating LLM attention patterns and performance across languages.", "result": "LLMs show language-specific attention patterns and biases, with a notable difference in performance related to causal structures in Chinese versus English.", "conclusion": "LLMs not only imitate linguistic forms but also embody reasoning biases associated with the languages they are trained on, validated through structural analysis of model internals.", "key_contributions": ["Development of the BICAUSE bilingual dataset for causal reasoning.", "Empirical evidence of LLMs internalizing language-specific reasoning biases.", "Analysis of attention patterns in LLMs highlighting differences between English and Chinese."], "limitations": "The study focuses primarily on two languages (English and Chinese), limiting generalizability to other languages.", "keywords": ["causal reasoning", "bilingual dataset", "large language models", "linguistic relativity", "attention patterns"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16716", "pdf": "https://arxiv.org/pdf/2506.16716.pdf", "abs": "https://arxiv.org/abs/2506.16716", "title": "V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos", "authors": ["Qixin Wang", "Songtao Zhou", "Zeyu Jin", "Chenglin Guo", "Shikun Sun", "Xiaoyu Qin"], "categories": ["cs.HC"], "comment": "Accepted by IJCNN 2025", "summary": "Automatic video commentary systems are widely used on multimedia social media\nplatforms to extract factual information about video content. However, current\nsystems may overlook essential para-linguistic cues, including emotion and\nattitude, which are critical for fully conveying the meaning of visual content.\nThe absence of these cues can limit user understanding or, in some cases,\ndistort the video's original intent. Expressive speech effectively conveys\nthese cues and enhances the user's comprehension of videos. Building on these\ninsights, this paper explores the usage of vision-context-aware expressive\nspeech in enhancing users' understanding of videos in video commentary systems.\nFirstly, our formatting study indicates that semantic-only speech can lead to\nambiguity, and misaligned emotions between speech and visuals may distort\ncontent interpretation. To address this, we propose a method called\nvision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic\ncues from visuals using a vision-language model and leverages a\nknowledge-infused language model to guide the expressive speech model in\ngenerating context-aligned speech. User studies show that V-CASS enhances\nemotional and attitudinal resonance, as well as user audio-visual understanding\nand engagement, with 74.68% of participants preferring the system. Finally, we\nexplore the potential of our method in helping blind and low-vision users\nnavigate web videos, improving universal accessibility.", "AI": {"tldr": "This paper presents a method called vision-context-aware speech synthesis (V-CASS) to enhance user understanding of videos by incorporating expressive speech aligned with visual content.", "motivation": "Current automatic video commentary systems fail to capture crucial para-linguistic cues like emotion and attitude, impeding user understanding of video content.", "method": "The proposed V-CASS method uses a vision-language model to analyze visual cues and a knowledge-infused language model to generate expressive speech that aligns with these cues.", "result": "User studies indicate that V-CASS improves emotional resonance and user engagement, with 74.68% of participants preferring the system over traditional approaches.", "conclusion": "V-CASS not only enhances general video understanding but also shows promise in improving accessibility for blind and low-vision users.", "key_contributions": ["Introduction of vision-context-aware speech synthesis (V-CASS) for video commentary.", "Demonstrated improved user engagement and understanding through user studies.", "Potential applications for enhancing accessibility in multimedia content."], "limitations": "", "keywords": ["video commentary", "expressive speech", "para-linguistic cues", "accessibility", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16172", "pdf": "https://arxiv.org/pdf/2506.16172.pdf", "abs": "https://arxiv.org/abs/2506.16172", "title": "SGIC: A Self-Guided Iterative Calibration Framework for RAG", "authors": ["Guanhua Chen", "Yutong Yao", "Lidia S. Chao", "Xuebo Liu", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research in retrieval-augmented generation (RAG) has concentrated on\nretrieving useful information from candidate documents. However, numerous\nmethodologies frequently neglect the calibration capabilities of large language\nmodels (LLMs), which capitalize on their robust in-context reasoning prowess.\nThis work illustrates that providing LLMs with specific cues substantially\nimproves their calibration efficacy, especially in multi-round calibrations. We\npresent a new SGIC: Self-Guided Iterative Calibration Framework that employs\nuncertainty scores as a tool. Initially, this framework calculates uncertainty\nscores to determine both the relevance of each document to the query and the\nconfidence level in the responses produced by the LLMs. Subsequently, it\nreevaluates these scores iteratively, amalgamating them with prior responses to\nrefine calibration. Furthermore, we introduce an innovative approach for\nconstructing an iterative self-calibration training set, which optimizes LLMs\nto efficiently harness uncertainty scores for capturing critical information\nand enhancing response accuracy. Our proposed framework significantly improves\nperformance on both closed-source and open-weight LLMs.", "AI": {"tldr": "This paper introduces the Self-Guided Iterative Calibration (SGIC) Framework that enhances the calibration of large language models (LLMs) using uncertainty scores in multi-round processes.", "motivation": "To address the shortcomings in the calibration capabilities of LLMs often overlooked in retrieval-augmented generation (RAG) methodologies.", "method": "The SGIC Framework utilizes uncertainty scores to assess relevance and confidence in responses, iteratively refining these scores by integrating them with prior outputs for improved calibration.", "result": "The framework demonstrates substantial improvements in the performance of both closed-source and open-weight LLMs through enhanced calibration.", "conclusion": "The SGIC Framework offers a novel approach to improve LLMs' response accuracy by capitalizing on uncertainty scores and iterative self-calibration processes.", "key_contributions": ["Introduction of the SGIC Framework for calibration", "Utilization of uncertainty scores for relevance and confidence assessment", "Development of an iterative self-calibration training set"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Self-Guided Calibration"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.16851", "pdf": "https://arxiv.org/pdf/2506.16851.pdf", "abs": "https://arxiv.org/abs/2506.16851", "title": "\"Whoever needs to see it, will see it\": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok", "authors": ["Ankolika De", "Kelley Cotter", "Shaheen Kanthawala", "Haley McAtee", "Amy Ritchart", "Gahana Kadur"], "categories": ["cs.HC", "H.5.0"], "comment": "27 pages, Proc. ACM Hum.-Comput. Interact. 8", "summary": "Recent studies show that users often interpret social media algorithms as\nmystical or spiritual because of their unpredictability. This invites new\nquestions about how such perceptions affect the content that creators create\nand the communities they form online. In this study, 14 creators of algorithmic\nconspirituality content on TikTok were interviewed to explore their\ninterpretations and creation processes influenced by the platform's For You\nPage algorithm. We illustrate how creators' beliefs interact with TikTok's\nalgorithmic mediation to reinforce and shape their spiritual or relational\nthemes. Furthermore, we show how algorithmic conspirituality content impacts\nviewers, highlighting its role in generating significant emotional and\naffective labor for creators, stemming from complex relational dynamics\ninherent in this content creation. We discuss implications for design to\nsupport creators aimed at recognizing the unexpected spiritual and religious\nexperiences algorithms prompt, as well as supporting creators in effectively\nmanaging these challenges.", "AI": {"tldr": "This study investigates how social media algorithms, particularly TikTok's For You Page, influence creators of algorithmic conspirituality content and their interactions with viewers.", "motivation": "To understand the implications of social media algorithms on content creation and community dynamics, particularly in the context of spiritual themes.", "method": "Interviews with 14 TikTok creators focused on algorithmic conspirituality to explore their experiences and the influence of TikTok's algorithm on their content.", "result": "Creators' beliefs shape how they engage with the algorithm and inform their content, while also affecting viewer interactions and emotional labor.", "conclusion": "Design efforts should support creators in navigating the complexities of algorithmic mediation and the spiritual themes it evokes in content creation.", "key_contributions": ["Exploration of creator-audience dynamics influenced by algorithmic design.", "Insights into emotional and affective labor associated with algorithmic content creation.", "Recommendations for design improvements to support creators in managing algorithmic challenges."], "limitations": "", "keywords": ["social media algorithms", "conspirituality", "TikTok", "content creation", "emotional labor"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2506.16187", "pdf": "https://arxiv.org/pdf/2506.16187.pdf", "abs": "https://arxiv.org/abs/2506.16187", "title": "JETHICS: Japanese Ethics Understanding Evaluation Dataset", "authors": ["Masashi Takeshita", "Rafal Rzepka"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we propose JETHICS, a Japanese dataset for evaluating ethics\nunderstanding of AI models. JETHICS contains 78K examples and is built by\nfollowing the construction methods of the existing English ETHICS dataset. It\nincludes four categories based normative theories and concepts from ethics and\npolitical philosophy; and one representing commonsense morality. Our evaluation\nexperiments on non-proprietary large language models (LLMs) and on GPT-4o\nreveal that even GPT-4o achieves only an average score of about 0.7, while the\nbest-performing Japanese LLM attains around 0.5, indicating a relatively large\nroom for improvement in current LLMs.", "AI": {"tldr": "JETHICS is a Japanese dataset designed to evaluate the ethical understanding of AI models, revealing that current LLMs, including GPT-4o, show significant limitations in this area.", "motivation": "To evaluate the ethics understanding of AI models, particularly in the context of Japanese language and culture.", "method": "A dataset called JETHICS was created, consisting of 78K examples based on normative theories and commonsense morality, aligned with the methodology of the English ETHICS dataset.", "result": "Evaluation experiments showed that GPT-4o scores about 0.7 and the best-performing Japanese LLM scores around 0.5, indicating a need for improvement.", "conclusion": "The findings highlight a considerable gap in the ethical understanding of current LLMs, suggesting that more work is needed to enhance this aspect of AI development.", "key_contributions": ["Introduction of JETHICS dataset for Japanese AI ethics evaluation", "Demonstration of low ethical understanding scores in state-of-the-art LLMs", "Provision of a framework for future research on ethics in AI"], "limitations": "The dataset is limited to Japanese language and may not be fully representative of all ethical perspectives.", "keywords": ["JETHICS", "AI Ethics", "Japanese Language", "LLM Evaluation", "Commonsense Morality"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.16874", "pdf": "https://arxiv.org/pdf/2506.16874.pdf", "abs": "https://arxiv.org/abs/2506.16874", "title": "Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools", "authors": ["Zhiqing Wang", "Haoxiang Fan", "Shiwei Wu", "Qiaoyi Chen", "Yongqi Liang", "Zhenhui Peng"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative Artificial Intelligence (GenAI) in K-6\nproject-based art courses presents both opportunities and challenges for\nenhancing creativity, engagement, and group collaboration. This study\nintroduces a four-phase field study, involving in total two experienced K-6 art\nteachers and 132 students in eight offline course sessions, to investigate the\nusage and impact of GenAI. Specifically, based on findings in Phases 1 and 2,\nwe developed AskArt, an interactive interface that combines DALL-E and GPT and\nis tailored to support elementary school students in their art projects, and\ndeployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in\nproviding background information, inspirations, and personalized guidance.\nHowever, challenges in query formulation for generating expected content were\nalso observed. Moreover, students employed varied collaboration strategies, and\nteachers noted increased engagement alongside concerns regarding misuse and\ninterface suitability. This study offers insights into the effective\nintegration of GenAI in elementary education, presents AskArt as a practical\ntool, and provides recommendations for educators and researchers to enhance\nproject-based learning with GenAI technologies.", "AI": {"tldr": "The study explores the integration of Generative AI in K-6 project-based art courses, highlighting the development of AskArt, an interface combining DALL-E and GPT that supports student creativity and collaboration.", "motivation": "To investigate the impact of Generative AI on enhancing creativity, engagement, and collaboration in K-6 art education.", "method": "A four-phase field study with two experienced K-6 art teachers and 132 students across eight offline course sessions was conducted to analyze the usage and effects of Generative AI.", "result": "The integration of Generative AI provided background information, inspiration, and personalized guidance for students, but challenges in query formulation were noted. Increased engagement and varied collaboration strategies were reported by students, along with teacher concerns regarding misuse and the interface's suitability.", "conclusion": "The study demonstrates the potential benefits of Generative AI in elementary education and provides recommendations for its effective use in project-based learning.", "key_contributions": ["Development of AskArt, an interactive tool for K-6 students", "Insights into creativity enhancement and engagement through GenAI", "Recommendations for educators on integrating GenAI in teaching"], "limitations": "Challenges in query formulation and concerns regarding misuse of the interface were noted.", "keywords": ["Generative AI", "K-6 education", "Project-based learning", "AskArt", "Creativity"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.16190", "pdf": "https://arxiv.org/pdf/2506.16190.pdf", "abs": "https://arxiv.org/abs/2506.16190", "title": "Web(er) of Hate: A Survey on How Hate Speech Is Typed", "authors": ["Luna Wang", "Andrew Caines", "Alice Hutchings"], "categories": ["cs.CL"], "comment": null, "summary": "The curation of hate speech datasets involves complex design decisions that\nbalance competing priorities. This paper critically examines these\nmethodological choices in a diverse range of datasets, highlighting common\nthemes and practices, and their implications for dataset reliability. Drawing\non Max Weber's notion of ideal types, we argue for a reflexive approach in\ndataset creation, urging researchers to acknowledge their own value judgments\nduring dataset construction, fostering transparency and methodological rigour.", "AI": {"tldr": "The paper examines the complexities and methodologies involved in curating hate speech datasets, calling for a reflexive approach to enhance transparency and reliability.", "motivation": "To address the balance of competing priorities in hate speech dataset curation and improve dataset reliability through methodological rigor.", "method": "The authors analyze a variety of hate speech datasets, emphasizing the need for transparency and reflexivity based on Max Weber's ideal types.", "result": "The study identifies common themes and practices in hate speech dataset curation that impact reliability and suggests a reflexive approach for researchers.", "conclusion": "A reflexive approach encourages researchers to recognize their value judgments in dataset creation, leading to more reliable and transparent datasets.", "key_contributions": ["Critical examination of hate speech dataset methodologies", "Emphasis on reflexive practices during dataset construction", "Call for greater transparency in research methods"], "limitations": "", "keywords": ["hate speech", "dataset curation", "methodological rigor", "transparency", "reflexivity"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.17011", "pdf": "https://arxiv.org/pdf/2506.17011.pdf", "abs": "https://arxiv.org/abs/2506.17011", "title": "Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics", "authors": ["Bruno Campos"], "categories": ["cs.HC"], "comment": null, "summary": "This study compares the impact of \"juiciness\" on user engagement and\nshort-term information retention in interactive infographics. Juicy designs\ngenerally showed a slight advantage in overall user engagement scores compared\nto dry designs. Specifically, the juicy version of the Burcalories infographic\nhad the highest engagement score. However, the differences in engagement were\noften small. Regarding information retention, the results were mixed. The juicy\nversions of The Daily Routines of Famous Creative People and The Main Chakras\ninfographics showed marginally better average recall and more participants with\nhigher recall. Conversely, the dry version of Burcalories led to more correct\nanswers in multiple-choice questions. The study suggests that while juicy\ndesign elements can enhance user engagement and, in some cases, short-term\ninformation retention, their effectiveness depends on careful implementation.\nExcessive juiciness could be overwhelming or distracting, while\nwell-implemented juicy elements contributed to a more entertaining experience.\nThe findings emphasize the importance of balancing engaging feedback with\nclarity and usability.", "AI": {"tldr": "This study analyzes the effects of 'juiciness' in interactive infographics on user engagement and information retention, revealing mixed outcomes and the necessity for balance in design elements.", "motivation": "To understand how design elements, specifically 'juiciness,' affect user engagement and information retention in interactive infographics.", "method": "Comparative analysis of user engagement and information retention metrics between juicy and dry infographics.", "result": "Juicy designs generally led to slightly higher engagement, but results on information retention were mixed; some juicy versions had better recall while one dry version performed best in multiple-choice questions.", "conclusion": "Juicy elements can enhance user engagement and retention when well-implemented, but excessive juiciness may distract users. A balance of engagement and clarity is crucial.", "key_contributions": ["Highlighting the impact of design choices on user engagement and information retention.", "Providing empirical evidence on user responses to interactive infographic styles.", "Outlining the necessity of balancing engagement with clarity in infographic design."], "limitations": "Variation in results suggests that optimal design implementation is context-dependent, and excessive juiciness may hinder information retention.", "keywords": ["HCI", "User Engagement", "Information Retention", "Interactive Infographics", "Design Elements"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.16247", "pdf": "https://arxiv.org/pdf/2506.16247.pdf", "abs": "https://arxiv.org/abs/2506.16247", "title": "Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports", "authors": ["Anindita Bhattacharya", "Tohida Rehman", "Debarshi Kumar Sanyal", "Samiran Chattopadhyay"], "categories": ["cs.CL"], "comment": "14 pages, 2 figures, 6 tables", "summary": "The findings section of a radiology report is often detailed and lengthy,\nwhereas the impression section is comparatively more compact and captures key\ndiagnostic conclusions. This research explores the use of advanced abstractive\nsummarization models to generate the concise impression from the findings\nsection of a radiology report. We have used the publicly available MIMIC-CXR\ndataset. A comparative analysis is conducted on leading pre-trained and\nopen-source large language models, including T5-base, BART-base,\nPEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network\nwith a coverage mechanism. To ensure a thorough assessment, multiple evaluation\nmetrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and\nBERTScore. By analyzing the performance of these models, this study identifies\ntheir respective strengths and limitations in the summarization of medical\ntext. The findings of this paper provide helpful information for medical\nprofessionals who need automated summarization solutions in the healthcare\nsector.", "AI": {"tldr": "This research investigates advanced abstractive summarization models for generating concise impressions from detailed findings in radiology reports using the MIMIC-CXR dataset.", "motivation": "The detailed findings section of radiology reports is lengthy, while the impression section is concise; improving summarization can enhance medical professionals' efficiency.", "method": "A comparative analysis of various pre-trained large language models (T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, Pointer Generator Network) was conducted, employing multiple evaluation metrics such as ROUGE and BERTScore.", "result": "Different models displayed varying strengths and limitations in summarizing medical text; insights were derived on the effectiveness of each model for medical summarization.", "conclusion": "The research offers valuable insights for the development of automated summarization tools beneficial to medical professionals in healthcare.", "key_contributions": ["Exploration of various large language models for summarization in radiology reports", "Comprehensive evaluation using multiple metrics to analyze performance", "Identification of strengths and limitations of summarization models in a healthcare context"], "limitations": "", "keywords": ["abstractive summarization", "radiology reports", "large language models", "MIMIC-CXR dataset", "healthcare"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2506.17032", "pdf": "https://arxiv.org/pdf/2506.17032.pdf", "abs": "https://arxiv.org/abs/2506.17032", "title": "Toward Understanding Similarity of Visualization Techniques", "authors": ["Abdulhaq Adetunji Salako", "Christian Tominski"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "The literature describes many visualization techniques for different types of\ndata, tasks, and application contexts, and new techniques are proposed on a\nregular basis. Visualization surveys try to capture the immense space of\ntechniques and structure it with meaningful categorizations. Yet, it remains\ndifficult to understand the similarity of visualization techniques in general.\nWe approach this open research question from two angles. First, we follow a\nmodel-driven approach that is based on defining the signature of visualization\ntechniques and interpreting the similarity of signatures as the similarity of\ntheir associated techniques. Second, following an expert-driven approach, we\nasked visualization experts in a small online study for their ad-hoc intuitive\nassessment of the similarity of pairs visualization techniques. From both\napproaches, we gain insight into the similarity of a set of 13 basic and\nadvanced visualizations for different types of data. While our results are so\nfar preliminary and academic, they are first steps toward better understanding\nthe similarity of visualization techniques.", "AI": {"tldr": "This paper investigates the similarity of various visualization techniques through both model-driven and expert-driven approaches.", "motivation": "To understand the similarity of visualization techniques and provide structured categorization while acknowledging the challenges faced in existing literature.", "method": "A model-driven approach was used to define visualization signatures; an expert-driven approach involved an online study assessing similarity between techniques.", "result": "Both approaches revealed insights into the similarity of 13 basic and advanced visualization techniques, highlighting preliminary findings.", "conclusion": "The study represents initial steps toward a deeper understanding of visualization technique similarities, though results are preliminary and academic.", "key_contributions": ["Introduction of a model-driven approach to define visualization technique signatures.", "Conducted an expert-driven study for intuitive similarity assessment of visualization techniques.", "Provided preliminary insights into similarity across different data visualization methods."], "limitations": "Results are preliminary and may require further validation; limited to 13 techniques.", "keywords": ["visualization techniques", "similarity", "model-driven approach", "expert assessment", "data visualization"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.16251", "pdf": "https://arxiv.org/pdf/2506.16251.pdf", "abs": "https://arxiv.org/abs/2506.16251", "title": "End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data", "authors": ["Aishwarya Pothula", "Bhavana Akkiraju", "Srihari Bandarupalli", "Charan D", "Santosh Kesiraju", "Anil Kumar Vuppala"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of high-quality annotated data presents a significant challenge\nin developing effective end-to-end speech-to-text translation (ST) systems,\nparticularly for low-resource languages. This paper explores the hypothesis\nthat weakly labeled data can be used to build ST models for low-resource\nlanguage pairs. We constructed speech-to-text translation datasets with the\nhelp of bitext mining using state-of-the-art sentence encoders. We mined the\nmultilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset\ncomprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,\nOdia-Hindi, and Telugu-Hindi. We created multiple versions of training data\nwith varying degrees of quality and quantity to investigate the effect of\nquality versus quantity of weakly labeled data on ST model performance. Results\ndemonstrate that ST systems can be built using weakly labeled data, with\nperformance comparable to massive multi-modal multilingual baselines such as\nSONAR and SeamlessM4T.", "AI": {"tldr": "This paper investigates using weakly labeled data for speech-to-text translation in low-resource languages, demonstrating comparable performance to established multilingual systems.", "motivation": "The scarcity of high-quality annotated data for low-resource languages challenges the development of effective speech-to-text systems.", "method": "The paper constructs datasets using bitext mining and sentence encoders, creating a dataset named Shrutilipi-anuvaad for specific language pairs. Various versions of training data were created to analyze the impact of data quality and quantity.", "result": "Experimental results indicate that speech-to-text systems can be effectively built using weakly labeled data, achieving performance on par with advanced multilingual systems like SONAR and SeamlessM4T.", "conclusion": "Weakly labeled data is viable for developing efficient speech-to-text translation models in low-resource scenarios.", "key_contributions": ["Construction of the Shrutilipi-anuvaad dataset for low-resource language pairs", "Analysis of the effects of data quality and quantity on ST model performance", "Demonstration of competitive performance with weakly labeled data compared to established models"], "limitations": "The effectiveness of the approach may vary with different low-resource languages and the quality of weakly labeled data.", "keywords": ["speech-to-text", "low-resource languages", "weakly labeled data", "dataset construction", "bitext mining"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.17116", "pdf": "https://arxiv.org/pdf/2506.17116.pdf", "abs": "https://arxiv.org/abs/2506.17116", "title": "Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools", "authors": ["Samuel Rhys Cox", "Helena B√∏jer Djern√¶s", "Niels van Berkel"], "categories": ["cs.HC"], "comment": "Workshop paper presented at XAIxArts'25 - the third international\n  workshop on eXplainable AI for the Arts, held in conjunction with the ACM\n  Creativity and Cognition conference 2025, June 23rd, 2025. 3 pages", "summary": "In this workshop paper, we discuss the potential for measures of user-centric\nbenefits (such as emotional well-being) that could be explored when evaluating\nexplainable AI (XAI) systems within the arts. As a background to this, we draw\nfrom our recent review of creativity support tool (CST) evaluations, that found\na paucity of studies evaluating CSTs for user-centric measures that benefit the\nuser themselves. Specifically, we discuss measures of: (1) developing intrinsic\nabilities, (2) emotional well-being, (3) self-reflection, and (4)\nself-perception. By discussing these user-centric measures within the context\nof XAI and the arts, we wish to provoke discussion regarding the potential of\nsuch measures.", "AI": {"tldr": "Exploration of user-centric benefits in evaluating explainable AI systems in the arts, focusing on emotional well-being and intrinsic abilities.", "motivation": "To address the lack of studies evaluating creativity support tools (CSTs) based on user-centric metrics that enhance user experience.", "method": "Discussion of measures related to intrinsic abilities, emotional well-being, self-reflection, and self-perception within the context of explainable AI in the arts.", "result": "Highlights the potential benefits of incorporating user-centric evaluation measures in XAI systems, specifically in the domain of the arts.", "conclusion": "The paper aims to provoke dialogue on the importance of user-centric measures in the evaluation of XAI systems in creative fields.", "key_contributions": ["Identification of user-centric measures for evaluating XAI systems", "Discussion on emotional well-being as a metric in the arts", "Provocation for further research on usability in XAI for creativity support"], "limitations": "Limited empirical evidence due to the nature of the workshop paper; primarily a discussion-based approach.", "keywords": ["Explainable AI", "User-centric measures", "Emotional well-being", "Creativity support tools", "Arts"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.16285", "pdf": "https://arxiv.org/pdf/2506.16285.pdf", "abs": "https://arxiv.org/abs/2506.16285", "title": "Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information", "authors": ["Hao-Chien Lu", "Jhen-Ke Lin", "Hong-Yun Lin", "Chung-Chun Wang", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Current automated speaking assessment (ASA) systems for use in multi-aspect\nevaluations often fail to make full use of content relevance, overlooking image\nor exemplar cues, and employ superficial grammar analysis that lacks detailed\nerror types. This paper ameliorates these deficiencies by introducing two novel\nenhancements to construct a hybrid scoring model. First, a multifaceted\nrelevance module integrates question and the associated image content,\nexemplar, and spoken response of an L2 speaker for a comprehensive assessment\nof content relevance. Second, fine-grained grammar error features are derived\nusing advanced grammar error correction (GEC) and detailed annotation to\nidentify specific error categories. Experiments and ablation studies\ndemonstrate that these components significantly improve the evaluation of\ncontent relevance, language use, and overall ASA performance, highlighting the\nbenefits of using richer, more nuanced feature sets for holistic speaking\nassessment.", "AI": {"tldr": "This paper presents a hybrid scoring model for automated speaking assessment (ASA) systems that improves content relevance evaluation and grammar error identification.", "motivation": "Current ASA systems often overlook content relevance and rely on superficial grammar analyses, lacking detailed error categorization.", "method": "The paper introduces a multifaceted relevance module that combines question content, related images, exemplars, and L2 spoken responses. Additionally, it incorporates fine-grained grammar error features through advanced correction techniques and detailed annotations.", "result": "Experiments show that the enhancements significantly improve the assessment of content relevance, language use, and overall performance of ASA systems.", "conclusion": "Using richer, more nuanced feature sets leads to a more comprehensive speaking assessment and better identification of specific errors.", "key_contributions": ["Development of a multifaceted relevance module for content assessment", "Implementation of fine-grained grammar error features", "Demonstration of improved ASA performance through experimental validation"], "limitations": "", "keywords": ["automated speaking assessment", "content relevance", "grammar error correction", "machine learning", "multifaceted evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.17196", "pdf": "https://arxiv.org/pdf/2506.17196.pdf", "abs": "https://arxiv.org/abs/2506.17196", "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance", "authors": ["Shambhavi Bhushan", "Danielle R Thomas", "Conrad Borchers", "Isha Raghuvanshi", "Ralph Abboud", "Erin Gatz", "Shivang Gupta", "Kenneth Koedinger"], "categories": ["cs.HC"], "comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript", "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases.", "AI": {"tldr": "This study fine-tunes GPT-4o to detect LLM-generated responses and evaluates the impact on learner performance, finding improved accuracy over existing tools and evidence of potential misuse.", "motivation": "To address the concerns regarding the misuse of large language models (LLMs) in online learning and to improve detection methods for LLM-generated text.", "method": "The study defines LLM-generated text and fine-tunes GPT-4o for detecting such responses, comparing its accuracy against GPTZero, and assessing the effects of LLM misuse on learner performance.", "result": "Fine-tuned GPT-4o achieved 80% accuracy and an F1 score of 0.78, outperforming GPTZero which had 70% accuracy and an F1 score of 0.50. Additionally, learners suspected of LLM misuse were more likely to answer related posttest questions correctly.", "conclusion": "The findings suggest that LLM-generated text misuse can bypass the learning process, and the study provides a structured approach for future improvements in detection methods and proposes auxiliary indicators for detection.", "key_contributions": ["Fine-tuning of GPT-4o improves detection of LLM-generated responses.", "Demonstrated correlation between LLM misuse and learner performance outcomes.", "Contribution of data and code for future research in LLM misuse detection."], "limitations": "", "keywords": ["large language models", "LLM detection", "educational technology", "machine learning", "learner performance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16322", "pdf": "https://arxiv.org/pdf/2506.16322.pdf", "abs": "https://arxiv.org/abs/2506.16322", "title": "PL-Guard: Benchmarking Language Model Safety for Polish", "authors": ["Aleksandra Krasnodƒôbska", "Karolina Seweryn", "Szymon ≈Åukasik", "Wojciech Kusa"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to the 10th Workshop on Slavic Natural Language Processing", "summary": "Despite increasing efforts to ensure the safety of large language models\n(LLMs), most existing safety assessments and moderation tools remain heavily\nbiased toward English and other high-resource languages, leaving majority of\nglobal languages underexamined. To address this gap, we introduce a manually\nannotated benchmark dataset for language model safety classification in Polish.\nWe also create adversarially perturbed variants of these samples designed to\nchallenge model robustness. We conduct a series of experiments to evaluate\nLLM-based and classifier-based models of varying sizes and architectures.\nSpecifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based\nclassifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B\nmodel. We train these models using different combinations of annotated data and\nevaluate their performance, comparing it against publicly available guard\nmodels. Results demonstrate that the HerBERT-based classifier achieves the\nhighest overall performance, particularly under adversarial conditions.", "AI": {"tldr": "This paper presents a benchmark dataset for assessing the safety of large language models in Polish and evaluates different model architectures to improve performance under adversarial conditions.", "motivation": "There is a significant bias in existing language model safety assessments favoring high-resource languages, leaving many global languages, including Polish, inadequately addressed.", "method": "A benchmark dataset for safety classification in Polish is created, along with adversarially perturbed samples to test model robustness. Three models (Llama-Guard-3-8B, a HerBERT-based classifier, and PLLuM) are fine-tuned and evaluated against publicly available guard models.", "result": "The experiments show that the HerBERT-based classifier outperforms other models, especially in adversarial scenarios.", "conclusion": "The findings highlight the importance of creating inclusive safety assessments for language models and demonstrate the effectiveness of Polish-adapted solutions in enhancing performance.", "key_contributions": ["Introduction of a benchmark dataset for Polish language model safety assessment", "Development of adversarially perturbed samples to evaluate model robustness", "Demonstration of the HerBERT-based classifier's superior performance under challenging conditions"], "limitations": "", "keywords": ["language models", "safety classification", "Polish language", "adversarial evaluation", "HerBERT"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.15794", "pdf": "https://arxiv.org/pdf/2506.15794.pdf", "abs": "https://arxiv.org/abs/2506.15794", "title": "Veracity: An Open-Source AI Fact-Checking System", "authors": ["Taylor Lynn Curtis", "Maximilian Puelma Touzel", "William Garneau", "Manon Gruaz", "Mike Pinder", "Li Wei Wang", "Sukanya Krishna", "Luda Cohen", "Jean-Fran√ßois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of misinformation poses a significant threat to society,\nexacerbated by the capabilities of generative AI. This demo paper introduces\nVeracity, an open-source AI system designed to empower individuals to combat\nmisinformation through transparent and accessible fact-checking. Veracity\nleverages the synergy between Large Language Models (LLMs) and web retrieval\nagents to analyze user-submitted claims and provide grounded veracity\nassessments with intuitive explanations. Key features include multilingual\nsupport, numerical scoring of claim veracity, and an interactive interface\ninspired by familiar messaging applications. This paper will showcase\nVeracity's ability to not only detect misinformation but also explain its\nreasoning, fostering media literacy and promoting a more informed society.", "AI": {"tldr": "Veracity is an open-source AI system that empowers individuals to combat misinformation through transparent fact-checking using LLMs and web retrieval agents.", "motivation": "To address the significant threat of misinformation exacerbated by generative AI, and to foster media literacy.", "method": "The system analyzes user-submitted claims using LLMs in conjunction with web retrieval agents to assess claim veracity.", "result": "Veracity provides grounded veracity assessments and intuitive explanations with features like multilingual support and numerical scoring.", "conclusion": "The tool not only detects misinformation but also explains the reasoning behind its assessments, promoting an informed society.", "key_contributions": ["Open-source design for transparent fact-checking", "Integration of LLMs with web retrieval for claim assessment", "Interactive and user-friendly interface for improving media literacy"], "limitations": "", "keywords": ["Misinformation", "Fact-checking", "Large Language Models", "AI", "Media literacy"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2506.16337", "pdf": "https://arxiv.org/pdf/2506.16337.pdf", "abs": "https://arxiv.org/abs/2506.16337", "title": "Generalizability of Media Frames: Corpus creation and analysis across countries", "authors": ["Agnese Daffara", "Sourabh Dattawad", "Sebastian Pad√≥", "Tanise Ceron"], "categories": ["cs.CL"], "comment": "8 pages + References (3 pages) and Appendix (4 pages). This paper was\n  submitted to StarSem 2025 and is currently under review", "summary": "Frames capture aspects of an issue that are emphasized in a debate by\ninterlocutors and can help us understand how political language conveys\ndifferent perspectives and ultimately shapes people's opinions. The Media Frame\nCorpus (MFC) is the most commonly used framework with categories and detailed\nguidelines for operationalizing frames. It is, however, focused on a few\nsalient U.S. news issues, making it unclear how well these frames can capture\nnews issues in other cultural contexts. To explore this, we introduce\nFrameNews-PT, a dataset of Brazilian Portuguese news articles covering\npolitical and economic news and annotate it within the MFC framework. Through\nseveral annotation rounds, we evaluate the extent to which MFC frames\ngeneralize to the Brazilian debate issues. We further evaluate how fine-tuned\nand zero-shot models perform on out-of-domain data. Results show that the 15\nMFC frames remain broadly applicable with minor revisions of the guidelines.\nHowever, some MFC frames are rarely used, and novel news issues are analyzed\nusing general 'fall-back' frames. We conclude that cross-cultural frame use\nrequires careful consideration.", "AI": {"tldr": "The paper introduces FrameNews-PT, a dataset for Brazilian Portuguese news articles, analyzing how well established U.S. media frames apply to Brazilian political and economic contexts.", "motivation": "To understand how political language and framing affect opinions in different cultural contexts beyond the U.S.", "method": "Creation of the FrameNews-PT dataset and evaluation of MFC frames through annotation across multiple rounds, assessing their relevance and performance of models on out-of-domain data.", "result": "The study finds that MFC frames are broadly applicable to Brazilian news with minor adjustments, though some frames are underutilized and new issues often require general fallback frames.", "conclusion": "Cross-cultural application of media frames needs careful attention to local contexts and language nuances.", "key_contributions": ["Introduction of FrameNews-PT dataset", "Evaluation of MFC frame applicability in Brazilian context", "Insights on performance of models in zero-shot scenarios"], "limitations": "The study may be limited by the cultural nuances in Brazilian political discourse that are not captured by MFC frames.", "keywords": ["media framing", "cross-cultural communication", "political discourse", "frame analysis", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.16343", "pdf": "https://arxiv.org/pdf/2506.16343.pdf", "abs": "https://arxiv.org/abs/2506.16343", "title": "Analyzing the Influence of Knowledge Graph Information on Relation Extraction", "authors": ["Cedric M√∂ller", "Ricardo Usbeck"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "We examine the impact of incorporating knowledge graph information on the\nperformance of relation extraction models across a range of datasets. Our\nhypothesis is that the positions of entities within a knowledge graph provide\nimportant insights for relation extraction tasks. We conduct experiments on\nmultiple datasets, each varying in the number of relations, training examples,\nand underlying knowledge graphs. Our results demonstrate that integrating\nknowledge graph information significantly enhances performance, especially when\ndealing with an imbalance in the number of training examples for each relation.\nWe evaluate the contribution of knowledge graph-based features by combining\nestablished relation extraction methods with graph-aware Neural Bellman-Ford\nnetworks. These features are tested in both supervised and zero-shot settings,\ndemonstrating consistent performance improvements across various datasets.", "AI": {"tldr": "Incorporating knowledge graph information enhances relation extraction model performance across diverse datasets.", "motivation": "Explore the significance of knowledge graph positions in improving relation extraction tasks.", "method": "Conducted experiments on various datasets using relation extraction methods integrated with graph-aware Neural Bellman-Ford networks, analyzed in both supervised and zero-shot settings.", "result": "Integrating knowledge graph information led to significant performance improvements, particularly under training sample imbalances.", "conclusion": "Knowledge graph features are crucial for effective relation extraction and can enhance model performance across different scenarios.", "key_contributions": ["Demonstrated improved relation extraction through knowledge graph integration", "Established the effectiveness of graph-aware Neural Bellman-Ford networks", "Showcased benefits in both supervised and zero-shot learning settings"], "limitations": "", "keywords": ["relation extraction", "knowledge graph", "Neural Bellman-Ford", "supervised learning", "zero-shot learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16348", "pdf": "https://arxiv.org/pdf/2506.16348.pdf", "abs": "https://arxiv.org/abs/2506.16348", "title": "DISCIE -- Discriminative Closed Information Extraction", "authors": ["Cedric M√∂ller", "Ricardo Usbeck"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a novel method for closed information extraction. The\nmethod employs a discriminative approach that incorporates type and\nentity-specific information to improve relation extraction accuracy,\nparticularly benefiting long-tail relations. Notably, this method demonstrates\nsuperior performance compared to state-of-the-art end-to-end generative models.\nThis is especially evident for the problem of large-scale closed information\nextraction where we are confronted with millions of entities and hundreds of\nrelations. Furthermore, we emphasize the efficiency aspect by leveraging\nsmaller models. In particular, the integration of type-information proves\ninstrumental in achieving performance levels on par with or surpassing those of\na larger generative model. This advancement holds promise for more accurate and\nefficient information extraction techniques.", "AI": {"tldr": "The paper presents a novel discriminative approach for closed information extraction that improves relation extraction accuracy, especially for long-tail relations, and demonstrates efficiency with smaller models.", "motivation": "To enhance relation extraction accuracy and efficiency, particularly for long-tail relations in large-scale closed information extraction tasks.", "method": "A discriminative method incorporating type and entity-specific information for relation extraction.", "result": "The proposed method outperforms state-of-the-art end-to-end generative models in relation extraction.", "conclusion": "The method offers a more accurate and efficient approach to closed information extraction by leveraging smaller models while maintaining performance.", "key_contributions": ["A novel discriminative approach for closed information extraction.", "Integration of type-information significantly enhances performance.", "Demonstrated efficiency in extracting relations from large-scale data with smaller models."], "limitations": "", "keywords": ["closed information extraction", "relation extraction", "discriminative models", "type-information", "efficiency"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.16370", "pdf": "https://arxiv.org/pdf/2506.16370.pdf", "abs": "https://arxiv.org/abs/2506.16370", "title": "Can structural correspondences ground real world representational content in Large Language Models?", "authors": ["Iwan Williams"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 produce compelling responses to a\nwide range of prompts. But their representational capacities are uncertain.\nMany LLMs have no direct contact with extra-linguistic reality: their inputs,\noutputs and training data consist solely of text, raising the questions (1) can\nLLMs represent anything and (2) if so, what? In this paper, I explore what it\nwould take to answer these questions according to a structural-correspondence\nbased account of representation, and make an initial survey of this evidence. I\nargue that the mere existence of structural correspondences between LLMs and\nworldly entities is insufficient to ground representation of those entities.\nHowever, if these structural correspondences play an appropriate role - they\nare exploited in a way that explains successful task performance - then they\ncould ground real world contents. This requires overcoming a challenge: the\ntext-boundedness of LLMs appears, on the face of it, to prevent them engaging\nin the right sorts of tasks.", "AI": {"tldr": "This paper examines the representational capacities of Large Language Models (LLMs) and argues that structural correspondences alone are insufficient for grounding representation of real-world entities.", "motivation": "The study aims to address the understanding of whether LLMs can represent real-world entities and how this representation might be justified.", "method": "The paper employs a structural-correspondence based account of representation to analyze LLMs, examining the potential for LLMs to ground representations based on their successful task performance.", "result": "The findings suggest that while structural correspondences exist between LLMs and worldly entities, they must play a role that facilitates successful task performance to effectively ground representation.", "conclusion": "If LLMs can engage in tasks that exploit these structural correspondences appropriately, they could potentially represent real-world contents despite their text-bounded nature.", "key_contributions": ["Exploration of structural correspondences in LLMs", "Analysis of task performance in relation to representation", "Discussion on the limitations of text-boundedness in grounding representation."], "limitations": "The paper acknowledges the challenge posed by the text-boundedness of LLMs in engaging in tasks that would lead to suitable representation.", "keywords": ["Large Language Models", "Representation", "Structural Correspondence", "Task Performance", "Text-Boundedness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16381", "pdf": "https://arxiv.org/pdf/2506.16381.pdf", "abs": "https://arxiv.org/abs/2506.16381", "title": "InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems", "authors": ["Kexin Huang", "Qian Tu", "Liwei Fan", "Chenchen Yang", "Dong Zhang", "Shimin Li", "Zhaoye Fei", "Qinyuan Cheng", "Xipeng Qiu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "19 pages, 9 figures", "summary": "In modern speech synthesis, paralinguistic information--such as a speaker's\nvocal timbre, emotional state, and dynamic prosody--plays a critical role in\nconveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)\nsystems rely on fixed style labels or inserting a speech prompt to control\nthese cues, which severely limits flexibility. Recent attempts seek to employ\nnatural-language instructions to modulate paralinguistic features,\nsubstantially improving the generalization of instruction-driven TTS models.\nAlthough many TTS systems now support customized synthesis via textual\ndescription, their actual ability to interpret and execute complex instructions\nremains largely unexplored. In addition, there is still a shortage of\nhigh-quality benchmarks and automated evaluation metrics specifically designed\nfor instruction-based TTS, which hinders accurate assessment and iterative\noptimization of these models. To address these limitations, we introduce\nInstructTTSEval, a benchmark for measuring the capability of complex\nnatural-language style control. We introduce three tasks, namely\nAcoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,\nincluding English and Chinese subsets, each with 1k test cases (6k in total)\npaired with reference audio. We leverage Gemini as an automatic judge to assess\ntheir instruction-following abilities. Our evaluation of accessible\ninstruction-following TTS systems highlights substantial room for further\nimprovement. We anticipate that InstructTTSEval will drive progress toward more\npowerful, flexible, and accurate instruction-following TTS.", "AI": {"tldr": "The paper introduces InstructTTSEval, a benchmark for evaluating complex natural-language style control in TTS systems, addressing current limitations in instruction-following capabilities and benchmarks.", "motivation": "The need for more flexible and accurate Text-to-Speech (TTS) systems that can interpret complex natural-language instructions and better convey paralinguistic information.", "method": "Introduction of InstructTTSEval benchmark with three tasks for evaluating TTS systems, along with use of Gemini as an automatic judge for instruction-following capabilities.", "result": "The evaluation of current instruction-following TTS systems reveals significant areas for improvement, highlighting the benchmark's effectiveness in measuring TTS capabilities.", "conclusion": "InstructTTSEval is expected to facilitate advancements in TTS systems by providing a structured way to assess and enhance instruction-following abilities.", "key_contributions": ["Introduction of InstructTTSEval benchmark for TTS systems", "Development of three specific tasks for evaluation: Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play", "Use of Gemini for automated assessment of instruction-following capabilities"], "limitations": "Current TTS systems show substantial room for improvement; existing challenges remain in interpreting complex instructions accurately.", "keywords": ["Text-to-Speech", "paralinguistic information", "natural-language instructions", "benchmark", "instruction-following TTS"], "importance_score": 6, "read_time_minutes": 19}}
{"id": "2506.16383", "pdf": "https://arxiv.org/pdf/2506.16383.pdf", "abs": "https://arxiv.org/abs/2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Work draft", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.", "AI": {"tldr": "This survey analyzes the impact of Large Language Models on Argument Mining (AM), highlighting advancements, methodologies, and a research agenda.", "motivation": "To synthesize recent advancements in LLM-driven Argument Mining and propose a research agenda.", "method": "The paper reviews theoretical foundations, annotation frameworks, and datasets related to AM, and presents a taxonomy of AM subtasks, alongside a critical assessment of current methodologies and challenges.", "result": "The survey catalogues advancements in LLM techniques for AM, detailing their impact on subtask execution and evaluation practices while identifying key challenges.", "conclusion": "A forward-looking research agenda is proposed, addressing the evolving landscape of LLM-based computational argumentation.", "key_contributions": ["Comprehensive taxonomy of AM subtasks influenced by LLM techniques", "Critical evaluation of current LLM architectures in AM", "Identification of key challenges in long-context reasoning and interpretability."], "limitations": "", "keywords": ["Argument Mining", "Large Language Models", "Natural Language Processing", "Taxonomy", "Evaluation Practices"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.16388", "pdf": "https://arxiv.org/pdf/2506.16388.pdf", "abs": "https://arxiv.org/abs/2506.16388", "title": "HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection", "authors": ["Sani Abdullahi Sani", "Salim Abubakar", "Falalu Ibrahim Lawan", "Abdulhamid Abubakar", "Maryam Bala"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our approach to multi-label emotion detection in Hausa, a\nlow-resource African language, as part of SemEval Track A. We fine-tuned\nAfriBERTa, a transformer-based model pre-trained on African languages, to\nclassify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and\nsurprise. Our methodology involved data preprocessing, tokenization, and model\nfine-tuning using the Hugging Face Trainer API. The system achieved a\nvalidation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the\neffectiveness of transformer-based models for emotion detection in low-resource\nlanguages.", "AI": {"tldr": "The paper discusses a multi-label emotion detection approach for Hausa using AfriBERTa, achieving notable accuracy in a low-resource setting.", "motivation": "To address the challenge of emotion detection in Hausa, a low-resource African language, by leveraging transformer-based models.", "method": "The approach includes data preprocessing, tokenization, and fine-tuning AfriBERTa with the Hugging Face Trainer API for classifying text into six emotions: anger, disgust, fear, joy, sadness, and surprise.", "result": "The system achieved a validation accuracy of 74.00% and an F1-score of 73.50%.", "conclusion": "The results demonstrate the effectiveness of transformer models for emotion detection even in low-resource languages like Hausa.", "key_contributions": ["Fine-tuning of AfriBERTa on Hausa text for emotion detection", "Classifying text into six distinct emotions", "Validation of transformer models in low-resource language settings"], "limitations": "", "keywords": ["emotion detection", "Hausa", "AfriBERTa", "low-resource languages", "transformer models"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.16622", "pdf": "https://arxiv.org/pdf/2506.16622.pdf", "abs": "https://arxiv.org/abs/2506.16622", "title": "Modeling Public Perceptions of Science in Media", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstin", "David Jurgens"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content.", "AI": {"tldr": "This paper introduces a computational framework to model public perception of science news across various dimensions, presents a large dataset on science perception, and develops NLP models to predict public engagement with scientific content.", "motivation": "The study aims to enhance public engagement and understanding of science by analyzing how audiences perceive and interact with scientific news in the face of overwhelming information.", "method": "A computational framework was created to assess public perception across twelve dimensions. A dataset with 10,489 annotations from 2,101 participants was established, and NLP models were developed to predict perception scores.", "result": "The study finds that the frequency of science news consumption significantly influences public perception, while demographic factors play a lesser role. Predicted public perception scores correlate with higher levels of engagement, such as comments and upvotes on social media posts.", "conclusion": "This research highlights the critical role of perception modeling in science communication and suggests that better understanding of public perception can inform strategies for increasing engagement with scientific content.", "key_contributions": ["Introduction of a perception modeling framework for science communication", "Creation of a large-scale science news perception dataset", "Development of NLP models to predict engagement based on public perception"], "limitations": "", "keywords": ["public perception", "science communication", "NLP models"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.16389", "pdf": "https://arxiv.org/pdf/2506.16389.pdf", "abs": "https://arxiv.org/abs/2506.16389", "title": "RiOT: Efficient Prompt Refinement with Residual Optimization Tree", "authors": ["Chenyi Zhou", "Zhengyan Shi", "Yuan Yao", "Lei Liang", "Huajun Chen", "Qiang Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have highlighted their\npotential across a variety of tasks, but their performance still heavily relies\non the design of effective prompts. Existing methods for automatic prompt\noptimization face two challenges: lack of diversity, limiting the exploration\nof valuable and innovative directions and semantic drift, where optimizations\nfor one task can degrade performance in others. To address these issues, we\npropose Residual Optimization Tree (RiOT), a novel framework for automatic\nprompt optimization. RiOT iteratively refines prompts through text gradients,\ngenerating multiple semantically diverse candidates at each step, and selects\nthe best prompt using perplexity. Additionally, RiOT incorporates the text\nresidual connection to mitigate semantic drift by selectively retaining\nbeneficial content across optimization iterations. A tree structure efficiently\nmanages the optimization process, ensuring scalability and flexibility.\nExtensive experiments across five benchmarks, covering commonsense,\nmathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT\noutperforms both previous prompt optimization methods and manual prompting.", "AI": {"tldr": "This paper introduces Residual Optimization Tree (RiOT), a framework for automated prompt optimization in large language models that enhances performance by generating diverse candidates and mitigating semantic drift.", "motivation": "To improve the effective use of large language models (LLMs) by automating prompt optimization, addressing diversity and semantic drift issues in existing methods.", "method": "RiOT refines prompts iteratively using text gradients, generates semantically diverse candidates, and uses perplexity for selection while retaining beneficial content through a text residual connection in a tree structure.", "result": "RiOT outperforms previous prompt optimization methods and manual prompting across five benchmarks involving different reasoning tasks.", "conclusion": "The proposed RiOT framework significantly improves prompt optimization, thus enhancing the performance of LLMs in various reasoning tasks.", "key_contributions": ["Introduction of the Residual Optimization Tree (RiOT) framework.", "Demonstration of improved performance in prompt optimization across multiple benchmarks.", "Mitigation of semantic drift through selective content retention."], "limitations": "", "keywords": ["large language models", "prompt optimization", "semantic drift", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16393", "pdf": "https://arxiv.org/pdf/2506.16393.pdf", "abs": "https://arxiv.org/abs/2506.16393", "title": "From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling", "authors": ["Yao Lu", "Zhaiyuan Ji", "Jiawei Du", "Yu Shanqing", "Qi Xuan", "Tianyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although the annotation paradigm based on Large Language Models (LLMs) has\nmade significant breakthroughs in recent years, its actual deployment still has\ntwo core bottlenecks: first, the cost of calling commercial APIs in large-scale\nannotation is very expensive; second, in scenarios that require fine-grained\nsemantic understanding, such as sentiment classification and toxicity\nclassification, the annotation accuracy of LLMs is even lower than that of\nSmall Language Models (SLMs) dedicated to this field. To address these\nproblems, we propose a new paradigm of multi-model cooperative annotation and\ndesign a fully automatic annotation framework AutoAnnotator based on this.\nSpecifically, AutoAnnotator consists of two layers. The upper-level\nmeta-controller layer uses the generation and reasoning capabilities of LLMs to\nselect SLMs for annotation, automatically generate annotation code and verify\ndifficult samples; the lower-level task-specialist layer consists of multiple\nSLMs that perform annotation through multi-model voting. In addition, we use\nthe difficult samples obtained by the secondary review of the meta-controller\nlayer as the reinforcement learning set and fine-tune the SLMs in stages\nthrough a continual learning strategy, thereby improving the generalization of\nSLMs. Extensive experiments show that AutoAnnotator outperforms existing\nopen-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.\nNotably, AutoAnnotator reduces the annotation cost by 74.15% compared to\ndirectly annotating with GPT-3.5-turbo, while still improving the accuracy by\n6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.", "AI": {"tldr": "The paper presents AutoAnnotator, an innovative annotation framework that combines LLMs and SLMs to improve annotation accuracy and reduce costs in large-scale tasks.", "motivation": "To address the high costs and low accuracy associated with LLMs when performing fine-grained semantic annotation tasks.", "method": "AutoAnnotator features a two-layer architecture: a meta-controller layer utilizing LLMs for selecting SLMs and generating code, and a task-specialist layer using multiple SLMs that perform annotation through multi-model voting and continual learning.", "result": "AutoAnnotator demonstrates significant improvements, outperforming existing LLMs in various settings while reducing annotation costs by 74.15% and increasing accuracy by 6.21%.", "conclusion": "The study establishes that a hybrid model utilizing both LLMs and SLMs can enhance annotation efficiency and accuracy for fine-grained tasks.", "key_contributions": ["Introduction of the AutoAnnotator framework combining LLMs and SLMs for annotation.", "Demonstrated significant cost savings and accuracy improvements over standard LLM approaches.", "Utilization of continual learning strategies to enhance SLM performance based on challenging annotation samples."], "limitations": "", "keywords": ["Large Language Models", "Small Language Models", "annotation framework", "human-computer interaction", "continual learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16395", "pdf": "https://arxiv.org/pdf/2506.16395.pdf", "abs": "https://arxiv.org/abs/2506.16395", "title": "OJBench: A Competition Level Code Benchmark For Large Language Models", "authors": ["Zhexu Wang", "Yiping Liu", "Yejie Wang", "Wenyang He", "Bofei Gao", "Muxi Diao", "Yanxu Chen", "Kelin Fu", "Flood Sung", "Zhilin Yang", "Tianyu Liu", "Weiran Xu"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsignificant progress in math and code reasoning capabilities. However, existing\ncode benchmark are limited in their ability to evaluate the full spectrum of\nthese capabilities, particularly at the competitive level. To bridge this gap,\nwe introduce OJBench, a novel and challenging benchmark designed to assess the\ncompetitive-level code reasoning abilities of LLMs. OJBench comprises 232\nprogramming competition problems from NOI and ICPC, providing a more rigorous\ntest of models' reasoning skills. We conducted a comprehensive evaluation using\nOJBench on 37 models, including both closed-source and open-source models,\nreasoning-oriented and non-reasoning-oriented models. Our results indicate that\neven state-of-the-art reasoning-oriented models, such as o4-mini and\nGemini-2.5-pro-exp, struggle with highly challenging competition-level\nproblems. This highlights the significant challenges that models face in\ncompetitive-level code reasoning.", "AI": {"tldr": "OJBench is a new benchmark for assessing competitive-level code reasoning abilities of large language models (LLMs) using 232 programming competition problems.", "motivation": "To address the limitations of existing code benchmarks in evaluating the full spectrum of code reasoning capabilities of LLMs.", "method": "The benchmark, OJBench, was created by compiling 232 programming problems from competitive programming contests. A comprehensive evaluation was conducted on 37 models to assess their performance.", "result": "The evaluation revealed that even the most advanced reasoning-oriented models struggled with the challenging problems presented in OJBench.", "conclusion": "There are significant challenges for LLMs in achieving competitive-level code reasoning, indicating a need for improved capabilities in this area.", "key_contributions": ["Introduction of OJBench as a novel benchmark for LLM evaluation in code reasoning", "Evaluation of 37 different models highlights performance gaps in state-of-the-art LLMs", "Identification of the discrepancy between model capabilities and competitive programming challenges"], "limitations": "Focuses only on a limited set of programming competition problems and may not cover all reasoning scenarios.", "keywords": ["large language models", "code reasoning", "benchmark", "OJBench", "programming competitions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2310.07019", "pdf": "https://arxiv.org/pdf/2310.07019.pdf", "abs": "https://arxiv.org/abs/2310.07019", "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans and AI", "authors": ["Quan Ze Chen", "Amy X. Zhang"], "categories": ["cs.HC"], "comment": "Accepted at ACM Collective Intelligence 2025", "summary": "From moderating content within an online community to producing\nsocially-appropriate generative outputs, decision-making tasks -- conducted by\neither humans or AI -- often depend on subjective or socially-established\ncriteria. To ensure such decisions are consistent, prevailing processes\nprimarily make use of high-level rules and guidelines to ground decisions,\nsimilar to applying \"constitutions\" in the legal context. However,\ninconsistencies in specifying and interpreting constitutional grounding can\nlead to undesirable and even incorrect decisions being made. In this work, we\nintroduce \"case law grounding\" (CLG) -- an approach for grounding subjective\ndecision-making using past decisions, similar to how precedents are used in\ncase law. We present how this grounding approach can be implemented in both\nhuman and AI decision-making contexts, introducing both a human-led process and\na large language model (LLM) prompting setup. Evaluating with five groups and\ncommunities across two decision-making task domains, we find that decisions\nproduced with CLG were significantly more accurately aligned to ground truth in\n4 out of 5 groups, achieving a 16.0--23.3 %-points higher accuracy in the human\nprocess, and 20.8--32.9 %-points higher with LLMs. We also examined the impact\nof different configurations with the retrieval window size and binding nature\nof decisions and find that binding decisions and larger retrieval windows were\nbeneficial. Finally, we discuss the broader implications of using CLG to\naugment existing constitutional grounding when it comes to aligning human and\nAI decisions.", "AI": {"tldr": "This paper introduces 'case law grounding' (CLG), a method for improving decision-making in human and AI contexts by using past decisions as precedents.", "motivation": "The need for consistent decision-making in communities and AI systems, ensuring decisions align with socially-established criteria.", "method": "The authors propose and implement 'case law grounding' (CLG), evaluating its effectiveness across human-led and LLM-assisted decision-making tasks.", "result": "CLG significantly improved decision accuracy, showing a 16.0--23.3 %-points increase in human decisions and a 20.8--32.9 %-points increase in LLM outputs across evaluated groups.", "conclusion": "CLG offers a valuable framework for enhancing consistency in subjective decision-making, bridging human and AI judgment.", "key_contributions": ["Introduction of 'case law grounding' for decision-making consistency", "Empirical evaluation showing significant accuracy improvements", "Exploration of retrieval window sizes and decision binding effects"], "limitations": "", "keywords": ["case law grounding", "decision-making", "human-AI interaction", "large language models", "social criteria"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16399", "pdf": "https://arxiv.org/pdf/2506.16399.pdf", "abs": "https://arxiv.org/abs/2506.16399", "title": "NepaliGPT: A Generative Language Model for the Nepali Language", "authors": ["Shushanta Pudasaini", "Aman Shakya", "Siddhartha Shrestha", "Sahil Bhatta", "Sunil Thapa", "Sushmita Palikhe"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "After the release of ChatGPT, Large Language Models (LLMs) have gained huge\npopularity in recent days and thousands of variants of LLMs have been released.\nHowever, there is no generative language model for the Nepali language, due to\nwhich other downstream tasks, including fine-tuning, have not been explored\nyet. To fill this research gap in the Nepali NLP space, this research proposes\n\\textit{NepaliGPT}, a generative large language model tailored specifically for\nthe Nepali language. This research introduces an advanced corpus for the Nepali\nlanguage collected from several sources, called the Devanagari Corpus.\nLikewise, the research introduces the first NepaliGPT benchmark dataset\ncomprised of 4,296 question-answer pairs in the Nepali language. The proposed\nLLM NepaliGPT achieves the following metrics in text generation: Perplexity of\n26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\\%, and causal\nconsistency of 85.41\\%.", "AI": {"tldr": "Proposes NepaliGPT, the first generative language model for the Nepali language, along with an advanced corpus and benchmark dataset.", "motivation": "To address the lack of a generative language model for the Nepali language, enabling further exploration of NLP tasks.", "method": "Introduces NepaliGPT, utilizing an advanced corpus called the Devanagari Corpus and a benchmark dataset of 4,296 question-answer pairs in Nepali.", "result": "NepaliGPT achieves metrics such as perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41%.", "conclusion": "NepaliGPT fills a critical gap in Nepali NLP research and facilitates further NLP task development in the language.", "key_contributions": ["Introduction of NepaliGPT, the first LLM for Nepali", "Development of the Devanagari Corpus", "Creation of a benchmark dataset with 4,296 question-answer pairs"], "limitations": "", "keywords": ["Nepali language", "Large Language Models", "NLP"], "importance_score": 3, "read_time_minutes": 11}}
{"id": "2410.20564", "pdf": "https://arxiv.org/pdf/2410.20564.pdf", "abs": "https://arxiv.org/abs/2410.20564", "title": "Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors", "authors": ["Sadia Nowrin", "Keith Vertanen"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "To appear in PErvasive Technologies Related to Assistive Environments\n  (PETRA '25)", "summary": "Conversational systems rely heavily on speech recognition to interpret and\nrespond to user commands and queries. Despite progress on speech recognition\naccuracy, errors may still sometimes occur and can significantly affect the\nend-user utility of such systems. While visual feedback can help detect errors,\nit may not always be practical, especially for people who are blind or\nlow-vision. In this study, we investigate ways to improve error detection by\nmanipulating the audio output of the transcribed text based on the recognizer's\nconfidence level in its result. Our findings show that selectively slowing down\nthe audio when the recognizer exhibited uncertainty led to a 12% relative\nincrease in participants' ability to detect errors compared to uniformly\nslowing the audio. It also reduced the time it took participants to listen to\nthe recognition result and decide if there was an error by 11%.", "AI": {"tldr": "The study explores improving error detection in conversational systems through audio output manipulation based on speech recognizer confidence levels.", "motivation": "To address speech recognition errors in conversational systems that significantly impact usability, especially for blind or low-vision users.", "method": "Audio output of transcribed text was modified by selectively slowing down the audio when the recognizer exhibited uncertainty, compared to a uniform slowdown.", "result": "Participants showed a 12% increase in error detection ability and an 11% reduction in decision time regarding errors with the manipulated audio output.", "conclusion": "Manipulating audio feedback based on confidence levels enhances error detection in speech recognition systems, improving usability for users with visual impairments.", "key_contributions": ["Investigated audio manipulation to improve error detection in speech recognition", "Demonstrated effectiveness of adjusting audio playback speed based on confidence levels", "Showed significant usability improvements for blind and low-vision users."], "limitations": "The study's findings may not be generalizable to all types of conversational systems or users with varying levels of hearing ability.", "keywords": ["speech recognition", "error detection", "human-computer interaction", "accessibility", "audiovisual feedback"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16411", "pdf": "https://arxiv.org/pdf/2506.16411.pdf", "abs": "https://arxiv.org/abs/2506.16411", "title": "When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework", "authors": ["Zhen Xu", "Shang Zhu", "Jue Wang", "Junlin Wang", "Ben Athiwaratkun", "Chi Wang", "James Zou", "Ce Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "under review", "summary": "We investigate the challenge of applying Large Language Models (LLMs) to long\ntexts. We propose a theoretical framework that distinguishes the failure modes\nof long context tasks into three categories: cross-chunk dependence (task\nnoise), confusion that grows with context size (model noise), and the imperfect\nintegration of partial results (aggregator noise). Under this view, we analyze\nwhen it is effective to use multi-agent chunking, i.e., dividing a length\nsequence into smaller chunks and aggregating the processed results of each\nchunk. Our experiments on tasks such as retrieval, question answering, and\nsummarization confirm both the theoretical analysis and the conditions that\nfavor multi-agent chunking. By exploring superlinear model noise growth with\ninput length, we also explain why, for large inputs, a weaker model configured\nwith chunk-based processing can surpass a more advanced model like GPT4o\napplied in a single shot. Overall, we present a principled understanding\nframework and our results highlight a direct pathway to handling long contexts\nin LLMs with carefully managed chunking and aggregator strategies.", "AI": {"tldr": "This paper presents a framework for addressing challenges related to applying Large Language Models (LLMs) to long texts, analyzing failure modes and proposing effective chunking strategies.", "motivation": "The research seeks to understand and improve the performance of LLMs on long context tasks, addressing common failures in processing.", "method": "The authors propose a theoretical framework categorizing failure modes into cross-chunk dependence, growing confusion with context size, and imperfect result integration. They conduct experiments on retrieval, question answering, and summarization using multi-agent chunking.", "result": "Experiments validate the theoretical framework, showing that chunk-based processing can outperform more advanced models like GPT4o in tasks with long inputs.", "conclusion": "The study provides a principled understanding of handling long contexts in LLMs through effective chunking and aggregation strategies.", "key_contributions": ["Theoretical framework for analyzing long context tasks in LLMs", "Empirical validation of chunk-based processing strategies", "Comparison of performance between chunking methods and single-shot model applications"], "limitations": "", "keywords": ["Large Language Models", "long context", "chunking", "aggregator strategies", "multi-agent"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.02263", "pdf": "https://arxiv.org/pdf/2411.02263.pdf", "abs": "https://arxiv.org/abs/2411.02263", "title": "AI Should Challenge, Not Obey", "authors": ["Advait Sarkar"], "categories": ["cs.HC"], "comment": "Advait Sarkar. 2024. AI Should Challenge, Not Obey. Commun. ACM 67,\n  10 (October 2024), 18-21. https://doi.org/10.1145/3649404", "summary": "Let's transform our robot secretaries into Socratic gadflies.", "AI": {"tldr": "The paper discusses the role of AI in promoting critical thinking and challenging users rather than merely obeying commands.", "motivation": "To advocate for AI systems that engage users in deeper, more meaningful interactions instead of simply fulfilling tasks.", "method": "The author proposes a conceptual framework for AI interaction that emphasizes challenge and inquiry.", "result": "The paper argues that transforming AI into 'Socratic gadflies' can enhance user engagement and improve decision-making.", "conclusion": "Encouraging AIs to challenge users can foster a more thoughtful and reflective approach to human-computer interaction.", "key_contributions": ["Proposes the concept of Socratic AI that challenges users", "Highlights the importance of critical thinking in AI interactions", "Suggests a novel approach to user engagement in HCI"], "limitations": "The implementation of such AI systems in real-world applications is not thoroughly explored.", "keywords": ["AI", "HCI", "Socratic method", "user interaction", "critical thinking"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2506.16444", "pdf": "https://arxiv.org/pdf/2506.16444.pdf", "abs": "https://arxiv.org/abs/2506.16444", "title": "REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing", "authors": ["Kangqi Chen", "Andreas Kosmas Kakolyris", "Rakesh Nadig", "Manos Frouzakis", "Nika Mansouri Ghiasi", "Yu Liang", "Haiyu Mao", "Jisung Park", "Mohammad Sadrosadati", "Onur Mutlu"], "categories": ["cs.CL", "cs.AR", "cs.DB", "H.3.3; I.2.7"], "comment": "Extended version of our publication at the 52nd International\n  Symposium on Computer Architecture (ISCA-52), 2025", "summary": "Large Language Models (LLMs) face an inherent challenge: their knowledge is\nconfined to the data that they have been trained on. To overcome this issue,\nRetrieval-Augmented Generation (RAG) complements the static training-derived\nknowledge of LLMs with an external knowledge repository. RAG consists of three\nstages: indexing, retrieval, and generation. The retrieval stage of RAG becomes\na significant bottleneck in inference pipelines. In this stage, a user query is\nmapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)\nalgorithm searches for similar vectors in the database to identify relevant\nitems. Due to the large database sizes, ANNS incurs significant data movement\noverheads between the host and the storage system. To alleviate these\noverheads, prior works propose In-Storage Processing (ISP) techniques that\naccelerate ANNS by performing computations inside storage. However, existing\nworks that leverage ISP for ANNS (i) employ algorithms that are not tailored to\nISP systems, (ii) do not accelerate data retrieval operations for data selected\nby ANNS, and (iii) introduce significant hardware modifications, limiting\nperformance and hindering their adoption. We propose REIS, the first ISP system\ntailored for RAG that addresses these limitations with three key mechanisms.\nFirst, REIS employs a database layout that links database embedding vectors to\ntheir associated documents, enabling efficient retrieval. Second, it enables\nefficient ANNS by introducing an ISP-tailored data placement technique that\ndistributes embeddings across the planes of the storage system and employs a\nlightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that\nuses the existing computational resources inside the storage system. Compared\nto a server-grade system, REIS improves the performance (energy efficiency) of\nretrieval by an average of 13x (55x).", "AI": {"tldr": "This paper proposes REIS, a novel In-Storage Processing system tailored for Retrieval-Augmented Generation that alleviates bottlenecks in Approximate Nearest Neighbor Search by optimizing data retrieval and retrieval performance.", "motivation": "Large Language Models struggle with knowledge limitations due to their training data confinement, and retrieval bottlenecks in RAG can hinder performance.", "method": "The paper introduces REIS, which features a database layout linking embeddings to documents, an ISP-tailored data placement technique, and an optimized ANNS engine utilizing storage system resources.", "result": "REIS enhances retrieval performance and energy efficiency compared to traditional server-grade systems, achieving an average improvement of 13x in performance and 55x in energy efficiency.", "conclusion": "REIS represents a significant advancement in RAG processes by optimizing data retrieval and reducing overheads related to large databases.", "key_contributions": ["Introduction of REIS, a specialized ISP system for RAG", "Innovative database layout for efficient retrieval", "Implementation of lightweight Flash Translation Layer for data placement"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "In-Storage Processing", "Approximate Nearest Neighbor Search"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.00630", "pdf": "https://arxiv.org/pdf/2412.00630.pdf", "abs": "https://arxiv.org/abs/2412.00630", "title": "Collective Creation of Intimacy: Exploring the Cosplay Commission Practice within the Otome Game Community in China", "authors": ["Yihao Zhou", "Haowei Xu", "Lili Zhang", "Shengdong Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Cosplay commission (cos-commission) is a new form of commodified romantic\ncompanionship within the Otome game community in China. To explore the\nmotivations, practices, experiences, and challenges, we conducted\nsemi-structured interviews with 15 participants in different roles. Our\nfindings reveal that cos-commission, as a hybrid activity, provides\nparticipants with a chance to collaboratively build meaningful connections. It\nalso offers a pathway for personal exploration and emotional recovery. However,\nthe vague boundary between performative roles and intimate interactions can\ngive rise to unexpected negative outcomes, such as attachment-driven\nentanglements and post-commission \"withdrawal symptoms.\" While digital\nplatforms facilitate communication in cos-commissions, they often lack\nsufficient safeguards. This preliminary work provides insights into the\nformation process of hybrid intimate relationship and its potential to foster\npersonalized, long-term support for mental well-being, and reveals potential\nprivacy and security challenges.", "AI": {"tldr": "This paper explores cos-commission, a commodified romantic companionship in the Otome game community in China, through interviews to understand participants' experiences and challenges.", "motivation": "To investigate the motivations, practices, experiences, and challenges of cos-commission participants within the Otome game community.", "method": "Semi-structured interviews conducted with 15 participants in various roles related to cos-commission.", "result": "Participants found cos-commission to enhance personal connections and emotional recovery, but faced challenges such as role confusion and attachment issues.", "conclusion": "While cos-commission can foster meaningful relationships and mental well-being, it presents privacy and security challenges that need addressing.", "key_contributions": ["Explored the dual role of cos-commission in forming personal connections and providing emotional support.", "Identified emotional challenges such as attachment issues and withdrawal symptoms post-commission.", "Highlighted the limitations of digital platforms in providing sufficient safeguards for participants."], "limitations": "The study is based on a small sample size and focuses solely on the Otome game community in China, which may not generalize to other contexts.", "keywords": ["cosplay", "cos-commission", "Otome games", "emotional recovery", "digital intimacy"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2506.16445", "pdf": "https://arxiv.org/pdf/2506.16445.pdf", "abs": "https://arxiv.org/abs/2506.16445", "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation", "authors": ["Haotian Xia", "Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long story generation remains a challenge for existing large language models\n(LLMs), primarily due to two main factors: (1) discourse coherence, which\nrequires plot consistency, logical coherence, and completeness in the long-form\ngeneration, and (2) narrative complexity, which requires an interwoven and\nengaging narrative. To address these challenges, we propose StoryWriter, a\nmulti-agent story generation framework, which consists of three main modules:\n(1) outline agent, which generates event-based outlines containing rich event\nplots, character, and event-event relationships. (2) planning agent, which\nfurther details events and plans which events should be written in each chapter\nto maintain an interwoven and engaging story. (3) writing agent, which\ndynamically compresses the story history based on the current event to generate\nand reflect new plots, ensuring the coherence of the generated story. We\nconduct both human and automated evaluation, and StoryWriter significantly\noutperforms existing story generation baselines in both story quality and\nlength. Furthermore, we use StoryWriter to generate a dataset, which contains\nabout $6,000$ high-quality long stories, with an average length of $8,000$\nwords. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning\non LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which\ndemonstrates advanced performance in long story generation.", "AI": {"tldr": "StoryWriter is a multi-agent framework addressing challenges in long story generation by improving discourse coherence and narrative complexity.", "motivation": "Current large language models struggle with long story generation due to issues in coherence and complexity, necessitating a new approach.", "method": "The framework employs three modules: an outline agent for event-based outlines, a planning agent for organizing chapters, and a writing agent for generating coherent narratives.", "result": "StoryWriter demonstrates significant improvements in quality and length of generated stories, outperforming previous baselines, and produces a dataset of approximately 6,000 high-quality stories.", "conclusion": "StoryWriter shows advanced capabilities in long story generation, indicating that multi-agent frameworks can enhance narrative creation in LLMs.", "key_contributions": ["Introduction of a multi-agent approach for story generation", "Creation of a dataset with high-quality long stories", "Demonstration of superior performance over existing benchmarks in story generation"], "limitations": "", "keywords": ["story generation", "large language models", "multi-agent systems", "narrative coherence", "dataset creation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2412.14195", "pdf": "https://arxiv.org/pdf/2412.14195.pdf", "abs": "https://arxiv.org/abs/2412.14195", "title": "A multimodal dataset for understanding the impact of mobile phones on remote online virtual education", "authors": ["Roberto Daza", "Alvaro Becerra", "Ruth Cobos", "Julian Fierrez", "Aythami Morales"], "categories": ["cs.HC", "cs.CV"], "comment": "Article under review in the journal Scientific Data. GitHub\n  repository of the dataset at: https://github.com/BiDAlab/IMPROVE", "summary": "This work presents the IMPROVE dataset, a multimodal resource designed to\nevaluate the effects of mobile phone usage on learners during online education.\nIt includes behavioral, biometric, physiological, and academic performance data\ncollected from 120 learners divided into three groups with different levels of\nphone interaction, enabling the analysis of the impact of mobile phone usage\nand related phenomena such as nomophobia. A setup involving 16 synchronized\nsensors -- including EEG, eye tracking, video cameras, smartwatches, and\nkeystroke dynamics -- was used to monitor learner activity during 30-minute\nsessions involving educational videos, document reading, and multiple-choice\ntests. Mobile phone usage events, including both controlled interventions and\nuncontrolled interactions, were labeled by supervisors and refined through a\nsemi-supervised re-labeling process. Technical validation confirmed signal\nquality, and statistical analyses revealed biometric changes associated with\nphone usage. The dataset is publicly available for research through GitHub and\nScience Data Bank, with synchronized recordings from three platforms (edBB,\nedX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and\naccompanied by a detailed guide.", "AI": {"tldr": "The IMPROVE dataset evaluates mobile phone usage effects on learners during online education through multimodal data collection.", "motivation": "To analyze the impact of mobile phone usage on learners' behavior and academic performance during online education.", "method": "The study involved 120 learners monitored through 16 synchronized sensors (EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics) during educational tasks over 30-minute sessions.", "result": "Statistical analyses showed biometric changes associated with mobile phone usage, and the dataset includes various forms of synchronized recordings.", "conclusion": "The dataset is publicly available and useful for future research on mobile phone usage in educational contexts.", "key_contributions": ["Introduction of a multimodal dataset for educational research", "Insights into behavioral and physiological effects of mobile phone usage", "Public access to the dataset for the research community"], "limitations": "The study focuses on a specific learner population and context, potentially limiting generalizability.", "keywords": ["IMPROVE dataset", "mobile phone usage", "online education", "biometric data", "learner behavior"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.16476", "pdf": "https://arxiv.org/pdf/2506.16476.pdf", "abs": "https://arxiv.org/abs/2506.16476", "title": "Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection", "authors": ["Saad Almohaimeed", "Saleh Almohaimeed", "Damla Turgut", "Ladislau B√∂l√∂ni"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Implicit hate speech has recently emerged as a critical challenge for social\nmedia platforms. While much of the research has traditionally focused on\nharmful speech in general, the need for generalizable techniques to detect\nveiled and subtle forms of hate has become increasingly pressing. Based on\nlexicon analysis, we hypothesize that implicit hate speech is already present\nin publicly available harmful speech datasets but may not have been explicitly\nrecognized or labeled by annotators. Additionally, crowdsourced datasets are\nprone to mislabeling due to the complexity of the task and often influenced by\nannotators' subjective interpretations. In this paper, we propose an approach\nto address the detection of implicit hate speech and enhance generalizability\nacross diverse datasets by leveraging existing harmful speech datasets. Our\nmethod comprises three key components: influential sample identification,\nreannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental\nresults demonstrate the effectiveness of our approach in improving implicit\nhate detection, achieving a +12.9-point F1 score improvement compared to the\nbaseline.", "AI": {"tldr": "This paper proposes a method to detect implicit hate speech using existing harmful speech datasets, achieving significant improvements in detection accuracy.", "motivation": "To address the challenge of detecting veiled and subtle forms of hate speech which is often mislabelled in crowdsourced datasets.", "method": "The method includes influential sample identification, reannotation of datasets, and augmentation using Llama-3 70B and GPT-4o.", "result": "The approach achieved a +12.9-point improvement in the F1 score compared to the baseline in detecting implicit hate speech.", "conclusion": "The proposed method enhances generalizability in implicit hate speech detection and improves upon existing techniques.", "key_contributions": ["Identification of influential samples to improve detection", "Reannotation of existing datasets for better labeling accuracy", "Augmentation of datasets using advanced AI models such as Llama-3 and GPT-4o"], "limitations": "", "keywords": ["implicit hate speech", "dataset augmentation", "harmful speech detection", "Llama-3", "GPT-4o"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2502.04599", "pdf": "https://arxiv.org/pdf/2502.04599.pdf", "abs": "https://arxiv.org/abs/2502.04599", "title": "Fuzzy Linkography: Automatic Graphical Summarization of Creative Activity Traces", "authors": ["Amy Smith", "Barrett R. Anderson", "Jasmine Tan Otto", "Isaac Karth", "Yuqian Sun", "John Joon Young Chung", "Melissa Roemmele", "Max Kreminski"], "categories": ["cs.HC"], "comment": "ACM C&C 2025. Code available at\n  https://github.com/mkremins/fuzzy-linkography", "summary": "Linkography -- the analysis of links between the design moves that make up an\nepisode of creative ideation or design -- can be used for both visual and\nquantitative assessment of creative activity traces. Traditional linkography,\nhowever, is time-consuming, requiring a human coder to manually annotate both\nthe design moves within an episode and the connections between them. As a\nresult, linkography has not yet been much applied at scale. To address this\nlimitation, we introduce fuzzy linkography: a means of automatically\nconstructing a linkograph from a sequence of recorded design moves via a\n\"fuzzy\" computational model of semantic similarity, enabling wider deployment\nand new applications of linkographic techniques. We apply fuzzy linkography to\nthree markedly different kinds of creative activity traces (text-to-image\nprompting journeys, LLM-supported ideation sessions, and researcher publication\nhistories) and discuss our findings, as well as strengths, limitations, and\npotential future applications of our approach.", "AI": {"tldr": "The paper introduces fuzzy linkography, an automated method for analyzing connections between design moves, which enhances the scalability of traditional linkography techniques.", "motivation": "Traditional linkography is time-consuming due to manual annotation, limiting its application in analyzing creative activity traces at scale.", "method": "The authors propose a fuzzy computational model of semantic similarity to automatically construct linkographs from recorded design moves.", "result": "Fuzzy linkography was applied to diverse creative activity traces, demonstrating its capability to analyze text-to-image prompting, LLM-supported ideation, and publication histories.", "conclusion": "The findings highlight the strengths and limitations of fuzzy linkography and suggest potential future applications within creative ideation contexts.", "key_contributions": ["Introduction of fuzzy linkography for automated annotation", "Application to various creative activity traces", "Discussion on the implications for wider deployment of linkographic techniques"], "limitations": "The approach's reliance on semantic similarity models may introduce inaccuracies in complex creative contexts.", "keywords": ["fuzzy linkography", "creative ideation", "semantic similarity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16502", "pdf": "https://arxiv.org/pdf/2506.16502.pdf", "abs": "https://arxiv.org/abs/2506.16502", "title": "Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples", "authors": ["Soumya Suvra Ghosal", "Vaibhav Singh", "Akash Ghosh", "Soumyabrata Pal", "Subhadip Baidya", "Sriparna Saha", "Dinesh Manocha"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward models are essential for aligning large language models (LLMs) with\nhuman preferences. However, most open-source multilingual reward models are\nprimarily trained on preference datasets in high-resource languages, resulting\nin unreliable reward signals for low-resource Indic languages. Collecting\nlarge-scale, high-quality preference data for these languages is prohibitively\nexpensive, making preference-based training approaches impractical. To address\nthis challenge, we propose RELIC, a novel in-context learning framework for\nreward modeling in low-resource Indic languages. RELIC trains a retriever with\na pairwise ranking objective to select in-context examples from auxiliary\nhigh-resource languages that most effectively highlight the distinction between\npreferred and less-preferred responses. Extensive experiments on three\npreference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art\nopen-source reward models demonstrate that RELIC significantly improves reward\nmodel accuracy for low-resource Indic languages, consistently outperforming\nexisting example selection methods. For example, on Bodo-a low-resource Indic\nlanguage-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%\nimprovement in accuracy over zero-shot prompting and state-of-the-art example\nselection method, respectively.", "AI": {"tldr": "RELIC is a novel framework improving reward modeling accuracy for low-resource Indic languages by utilizing in-context learning from high-resource languages.", "motivation": "To improve LLM alignment with human preferences for low-resource Indic languages, where existing models struggle due to a lack of training data.", "method": "RELIC employs a pairwise ranking objective to train a retriever that selects in-context examples from high-resource languages to enhance preference modeling.", "result": "RELIC demonstrated significant improvements in reward model accuracy for low-resource Indic languages, achieving up to a 12.81% accuracy increase over prior methods.", "conclusion": "RELIC outperforms existing selection methods for training reward models in low-resource contexts, proving effective for languages like Bodo.", "key_contributions": ["Introduction of RELIC framework for low-resource reward modeling", "Pairwise ranking objective for effective in-context learning", "Improvements in reward model accuracy for low-resource Indic languages"], "limitations": "", "keywords": ["reward models", "low-resource languages", "in-context learning", "human preferences", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.05731", "pdf": "https://arxiv.org/pdf/2502.05731.pdf", "abs": "https://arxiv.org/abs/2502.05731", "title": "Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies", "authors": ["Sam Yu-Te Lee", "Cheng-Wei Hung", "Mei-Hua Yuan", "Kwan-Liu Ma"], "categories": ["cs.HC"], "comment": "IEEE PacificVis 2025 Information systems, Information systems\n  applications, Data Mining; Human-centered computing, Visualization,\n  Visualization systems and tools", "summary": "Environmental experts have developed the DPSIR (Driver, Pressure, State,\nImpact, Response) framework to systematically study and communicate key\nrelationships between society and the environment. Using this framework\nrequires experts to construct a DPSIR taxonomy from a corpus, annotate the\ndocuments, and identify DPSIR variables and relationships, which is laborious\nand inflexible. Automating it with conventional text mining faces technical\nchallenges, primarily because the taxonomy often begins with abstract\ndefinitions, which experts progressively refine and contextualize as they\nannotate the corpus. In response, we develop GreenMine, a system that supports\ninteractive text mining with prompt engineering. The system implements a\nprompting pipeline consisting of three simple and evaluable subtasks. In each\nsubtask, the DPSIR taxonomy can be defined in natural language and iteratively\nrefined as experts analyze the corpus. To support users evaluate the taxonomy,\nwe introduce an uncertainty score based on response consistency. Then, we\ndesign a radial uncertainty chart that visualizes uncertainties and corpus\ntopics, which supports interleaved evaluation and exploration. Using the\nsystem, experts can progressively construct the DPSIR taxonomy and annotate the\ncorpus with LLMs. Using real-world interview transcripts, we present a case\nstudy to demonstrate the capability of the system in supporting interactive\nmining of DPSIR relationships, and an expert review in the form of\ncollaborative discussion to understand the potential and limitations of the\nsystem. We discuss the lessons learned from developing the system and future\nopportunities for supporting interactive text mining in knowledge-intensive\ntasks for other application scenarios.", "AI": {"tldr": "The paper develops GreenMine, a system that automates the construction of the DPSIR taxonomy for environmental studies through interactive text mining and prompting.", "motivation": "To streamline the labor-intensive process of constructing a DPSIR taxonomy and to improve the flexibility and efficiency of analyzing environmental data.", "method": "The system uses a prompting pipeline with three tasks that allow natural language definitions of the DPSIR taxonomy to be iteratively refined. It incorporates an uncertainty score for response consistency and features a radial uncertainty chart for visualization.", "result": "A case study using real-world interview transcripts demonstrated GreenMine's capabilities in supporting interactive mining of DPSIR relationships, showcasing both strengths and limitations through expert collaboration.", "conclusion": "GreenMine presents a novel approach to enhance interactive text mining for constructing the DPSIR taxonomy, offering lessons and future opportunities for similar applications in knowledge-intensive tasks.", "key_contributions": ["Development of GreenMine for interactive text mining", "Introduction of an uncertainty score for taxonomy evaluation", "Design of a radial uncertainty chart for visualizing topics and uncertainties"], "limitations": "The system's effectiveness needs further validation across diverse environments and scenarios beyond the initial case study.", "keywords": ["DPSIR framework", "interactive text mining", "large language models", "visualization", "environmental data analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.16558", "pdf": "https://arxiv.org/pdf/2506.16558.pdf", "abs": "https://arxiv.org/abs/2506.16558", "title": "Automatic Speech Recognition Biases in Newcastle English: an Error Analysis", "authors": ["Dana Serditova", "Kevin Tang", "Jochen Steffens"], "categories": ["cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) systems struggle with regional dialects\ndue to biased training which favours mainstream varieties. While previous\nresearch has identified racial, age, and gender biases in ASR, regional bias\nremains underexamined. This study investigates ASR performance on Newcastle\nEnglish, a well-documented regional dialect known to be challenging for ASR. A\ntwo-stage analysis was conducted: first, a manual error analysis on a subsample\nidentified key phonological, lexical, and morphosyntactic errors behind ASR\nmisrecognitions; second, a case study focused on the systematic analysis of ASR\nrecognition of the regional pronouns ``yous'' and ``wor''. Results show that\nASR errors directly correlate with regional dialectal features, while social\nfactors play a lesser role in ASR mismatches. We advocate for greater dialectal\ndiversity in ASR training data and highlight the value of sociolinguistic\nanalysis in diagnosing and addressing regional biases.", "AI": {"tldr": "This study addresses the challenges of Automatic Speech Recognition (ASR) systems with regional dialects, specifically Newcastle English, highlighting biases in ASR performance and advocating for more diverse training data.", "motivation": "The research investigates how ASR systems perform with regional dialects, focusing on underexamined regional biases that affect recognition accuracy.", "method": "A two-stage analysis involving a manual error analysis on a subsample of speech data and a detailed case study of ASR recognition of specific regional pronouns.", "result": "Key phonological, lexical, and morphosyntactic errors were identified as the primary causes of ASR misrecognitions, with regional dialect features influencing error rates more than social factors.", "conclusion": "There is a need for greater dialectal diversity in ASR training datasets, and sociolinguistic analysis can help in understanding and mitigating regional biases in ASR systems.", "key_contributions": ["Identified key errors in ASR recognition of Newcastle English regional dialect.", "Highlighted the correlation between regional dialectal features and ASR errors.", "Advocated for improved dialectal representation in ASR training data."], "limitations": "", "keywords": ["Automatic Speech Recognition", "regional dialects", "bias", "sociolinguistics", "Newcastle English"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.15242", "pdf": "https://arxiv.org/pdf/2502.15242.pdf", "abs": "https://arxiv.org/abs/2502.15242", "title": "Agonistic Image Generation: Unsettling the Hegemony of Intention", "authors": ["Andrew Shaw", "Andre Ye", "Ranjay Krishna", "Amy X. Zhang"], "categories": ["cs.HC"], "comment": "Accepted to ACM Fairness, Accountability, Transparency 2025 --\n  Athens, Greece", "summary": "Current image generation paradigms prioritize actualizing user intention -\n\"see what you intend\" - but often neglect the sociopolitical dimensions of this\nprocess. However, it is increasingly evident that image generation is\npolitical, contributing to broader social struggles over visual meaning. This\nsociopolitical aspect was highlighted by the March 2024 Gemini controversy,\nwhere Gemini faced criticism for inappropriately injecting demographic\ndiversity into user prompts. Although the developers sought to redress image\ngeneration's sociopolitical dimension by introducing diversity \"corrections,\"\ntheir opaque imposition of a standard for \"diversity\" ultimately proved\ncounterproductive. In this paper, we present an alternative approach: an image\ngeneration interface designed to embrace open negotiation along the\nsociopolitical dimensions of image creation. Grounded in the principles of\nagonistic pluralism (from the Greek agon, meaning struggle), our interface\nactively engages users with competing visual interpretations of their prompts.\nThrough a lab study with 29 participants, we evaluate our agonistic interface\non its ability to facilitate reflection - engagement with other perspectives\nand challenging dominant assumptions - a core principle that underpins\nagonistic contestation. We compare it to three existing paradigms: a standard\ninterface, a Gemini-style interface that produces \"diverse\" images, and an\nintention-centric interface suggesting prompt refinements. Our findings\ndemonstrate that the agonistic interface enhances reflection across multiple\nmeasures, but also that reflection depends on users perceiving the interface as\nboth appropriate and empowering; introducing diversity without grounding it in\nrelevant political contexts was perceived as inauthentic. Our results suggest\nthat diversity and user intention should not be treated as opposing values to\nbe balanced.", "AI": {"tldr": "This paper discusses the sociopolitical dimensions of image generation, proposing an interface that promotes reflection and engagement with competing visual interpretations.", "motivation": "Current image generation systems often overlook the sociopolitical implications of user intentions, highlighted by controversies related to demographic diversity.", "method": "A lab study with 29 participants evaluated the proposed agonistic interface against three existing paradigms to measure its effectiveness in facilitating reflection.", "result": "The agonistic interface was found to enhance reflection compared to other interfaces, with the perception of authenticity being crucial for user engagement.", "conclusion": "Diversity and user intention should coexist within image generation, rather than viewing them as opposing values.", "key_contributions": ["Proposed an agonistic interface for image generation that incorporates sociopolitical negotiation.", "Provided empirical evidence on the importance of user perception of authenticity in diversity integration.", "Challenged current paradigms in image generation by emphasizing reflection on competing interpretations."], "limitations": "The study's sample size was limited to 29 participants, and further research is needed to generalize findings.", "keywords": ["image generation", "sociopolitical dimensions", "agonistic pluralism", "user reflection", "diversity in AI"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.16574", "pdf": "https://arxiv.org/pdf/2506.16574.pdf", "abs": "https://arxiv.org/abs/2506.16574", "title": "Weight Factorization and Centralization for Continual Learning in Speech Recognition", "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Modern neural network based speech recognition models are required to\ncontinually absorb new data without re-training the whole system, especially in\ndownstream applications using foundation models, having no access to the\noriginal training data. Continually training the models in a rehearsal-free,\nmultilingual, and language agnostic condition, likely leads to catastrophic\nforgetting, when a seemingly insignificant disruption to the weights can\ndestructively harm the quality of the models. Inspired by the ability of human\nbrains to learn and consolidate knowledge through the waking-sleeping cycle, we\npropose a continual learning approach with two distinct phases: factorization\nand centralization, learning and merging knowledge accordingly. Our experiments\non a sequence of varied code-switching datasets showed that the centralization\nstage can effectively prevent catastrophic forgetting by accumulating the\nknowledge in multiple scattering low-rank adapters.", "AI": {"tldr": "Proposes a continual learning approach for speech recognition models that prevents catastrophic forgetting by using a two-phase learning strategy inspired by human cognitive processes.", "motivation": "Address the challenge of modern speech recognition models needing to absorb new data without retraining, which leads to catastrophic forgetting.", "method": "Introduces a continual learning approach consisting of a factorization phase and a centralization phase for merging knowledge.", "result": "Experiments show that the proposed centralization stage effectively prevents catastrophic forgetting in multilingual and language-agnostic settings.", "conclusion": "The approach allows for continual training of speech recognition models without significant degradation in quality, leveraging knowledge accumulation in low-rank adapters.", "key_contributions": ["A two-phase continual learning approach for speech recognition.", "Demonstration of knowledge accumulation through centralization.", "Effective prevention of catastrophic forgetting in multilingual scenarios."], "limitations": "", "keywords": ["continual learning", "speech recognition", "catastrophic forgetting", "knowledge accumulation", "multilingual models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.01769", "pdf": "https://arxiv.org/pdf/2503.01769.pdf", "abs": "https://arxiv.org/abs/2503.01769", "title": "Using Collective Dialogues and AI to Find Common Ground Between Israeli and Palestinian Peacebuilders", "authors": ["Andrew Konya", "Luke Thorburn", "Wasim Almasri", "Oded Adomi Leshem", "Ariel D. Procaccia", "Lisa Schirch", "Michiel A. Bakker"], "categories": ["cs.HC"], "comment": "Accepted at FAccT 2025", "summary": "A growing body of work has shown that AI-assisted methods -- leveraging large\nlanguage models, social choice methods, and collective dialogues -- can help\nnavigate polarization and surface common ground in controlled lab settings. But\nwhat can these approaches contribute in real-world contexts? We present a case\nstudy applying these techniques to find common ground between Israeli and\nPalestinian peacebuilders in the period following October 7th, 2023. From April\nto July 2024 an iterative deliberative process combining LLMs, bridging-based\nranking, and collective dialogues was conducted in partnership with the\nAlliance for Middle East Peace. Around 138 civil society peacebuilders\nparticipated including Israeli Jews, Palestinian citizens of Israel, and\nPalestinians from the West Bank and Gaza. The process resulted in a set of\ncollective statements, including demands to world leaders, with at least 84%\nagreement from participants on each side. In this paper, we document the\nprocess, results, challenges, and important open questions.", "AI": {"tldr": "This paper presents a case study on utilizing AI-assisted methods to facilitate dialogue and find common ground between Israeli and Palestinian peacebuilders.", "motivation": "To investigate the effectiveness of AI-assisted approaches in bridging divides and fostering communication in high-stakes, real-world contexts following significant events.", "method": "An iterative deliberative process involving large language models (LLMs), bridging-based ranking, and collective dialogues was used with 138 participants from diverse backgrounds related to the Israeli-Palestinian conflict.", "result": "The participatory process resulted in collective statements with at least 84% agreement from participants on both sides, indicating a successful negotiation of common ground.", "conclusion": "This case study illustrates the potential of AI-assisted methods in real-world conflict resolution and highlights the need to address future challenges and questions.", "key_contributions": ["Demonstrated the application of AI-assisted dialogue methods in real-world conflict resolution.", "Achieved a high level of agreement (84%) among diverse peacebuilders on collective statements.", "Provided empirical insights into the challenges and outcomes of using LLMs in sensitive negotiations."], "limitations": "The study is context-specific, with results not necessarily generalizable to other conflicts or settings.", "keywords": ["AI-assisted methods", "conflict resolution", "large language models", "collective dialogues", "Israeli-Palestinian peace"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16580", "pdf": "https://arxiv.org/pdf/2506.16580.pdf", "abs": "https://arxiv.org/abs/2506.16580", "title": "Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement", "authors": ["Tuan-Nam Nguyen", "Ngoc-Quan Pham", "Seymanur Akti", "Alexander Waibel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "We propose a first streaming accent conversion (AC) model that transforms\nnon-native speech into a native-like accent while preserving speaker identity,\nprosody and improving pronunciation. Our approach enables stream processing by\nmodifying a previous AC architecture with an Emformer encoder and an optimized\ninference mechanism. Additionally, we integrate a native text-to-speech (TTS)\nmodel to generate ideal ground-truth data for efficient training. Our streaming\nAC model achieves comparable performance to the top AC models while maintaining\nstable latency, making it the first AC system capable of streaming.", "AI": {"tldr": "The proposed model enables real-time transformation of non-native speech into a native-like accent while preserving speaker identity and enhancing pronunciation.", "motivation": "To develop a streaming accent conversion model that maintains speaker identity and improves non-native pronunciation while achieving low latency.", "method": "Modifying a previous accent conversion architecture to incorporate an Emformer encoder and an optimized inference mechanism, along with integrating a native text-to-speech model for training.", "result": "The streaming accent conversion model achieved performance comparable to existing leading models while ensuring stable latency, effectively breaking new ground in accent conversion technology.", "conclusion": "This work presents the first effective streaming accent conversion system that performs well under real-time conditions.", "key_contributions": ["Development of a streaming accent conversion model", "Use of an Emformer encoder for improved performance", "Integration of a TTS model to enhance training efficiency"], "limitations": "", "keywords": ["accent conversion", "streaming speech processing", "text-to-speech", "Emformer", "native-like accent"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.14103", "pdf": "https://arxiv.org/pdf/2503.14103.pdf", "abs": "https://arxiv.org/abs/2503.14103", "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments using a Retrieval-Augmented Language Model", "authors": ["Jonas Oppenlaender"], "categories": ["cs.HC", "H.5.m"], "comment": "14 pages, 3 figures, 1 table", "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.", "AI": {"tldr": "The study explores the use of large language models in providing personalized travel safety information through a system called DangerMaps, which visualizes safety ratings and explanations for urban destinations.", "motivation": "Travelers often struggle to find reliable safety information for trip planning, typically relying on search engines that do not meet their contextual needs.", "method": "A formative study of travelers' information needs was conducted, followed by the development of DangerMaps, a mapping system integrating LLMs to provide personalized safety advice.", "result": "DangerMaps effectively plots safety ratings on a map and offers contextual explanations to users, filling a gap in current travel planning tools.", "conclusion": "The paper emphasizes the importance of prompt design when using LLMs in real-world applications and identifies areas for future research in this domain.", "key_contributions": ["Introduction of DangerMaps for personalized travel safety research", "Exploration of information needs specific to travel safety", "Detailed approach to prompt design for LLM applications"], "limitations": "The study is focused on urban destinations and may not generalize to all travel contexts.", "keywords": ["Large Language Models", "Travel Safety", "Personalized Advice", "Human-Computer Interaction", "Mapping System"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16584", "pdf": "https://arxiv.org/pdf/2506.16584.pdf", "abs": "https://arxiv.org/abs/2506.16584", "title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework", "authors": ["Nadav Kunievsky", "James A. Evans"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 68T05", "I.2.7; I.2.6; I.5.1"], "comment": null, "summary": "Understanding whether large language models (LLMs) possess a world model-a\nstructured understanding of the world that supports generalization beyond\nsurface-level patterns-is central to assessing their reliability, especially in\nhigh-stakes applications. We propose a formal framework for evaluating whether\nan LLM exhibits a sufficiently robust world model, defined as producing\nconsistent outputs across semantically equivalent prompts while distinguishing\nbetween prompts that express different intents. We introduce a new evaluation\napproach to measure this that decomposes model response variability into three\ncomponents: variability due to user purpose, user articulation, and model\ninstability. An LLM with a strong world model should attribute most of the\nvariability in its responses to changes in foundational purpose rather than\nsuperficial changes in articulation. This approach allows us to quantify how\nmuch of a model's behavior is semantically grounded rather than driven by model\ninstability or alternative wording. We apply this framework to evaluate LLMs\nacross diverse domains. Our results show how larger models attribute a greater\nshare of output variability to changes in user purpose, indicating a more\nrobust world model. This improvement is not uniform, however: larger models do\nnot consistently outperform smaller ones across all domains, and their\nadvantage in robustness is often modest. These findings highlight the\nimportance of moving beyond accuracy-based benchmarks toward semantic\ndiagnostics that more directly assess the structure and stability of a model's\ninternal understanding of the world.", "AI": {"tldr": "This paper proposes a framework for evaluating whether large language models (LLMs) possess a robust world model by measuring their output variability in response to different user intents and articulations.", "motivation": "Assessing the reliability of LLMs is critical, particularly for high-stakes applications, and requires understanding whether they have a structured world model that goes beyond surface-level pattern recognition.", "method": "The authors introduce a new evaluation approach that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability, to measure how robust the world model is.", "result": "Results indicate that larger LLMs attribute more output variability to changes in user purpose, suggesting a stronger world model, though this advantage is not consistent across all domains.", "conclusion": "The paper emphasizes the necessity of semantic diagnostics over traditional accuracy-based benchmarks to evaluate the internal understanding of LLMs.", "key_contributions": ["Proposal of a formal framework for evaluating LLM world models", "New evaluation approach that quantifies response variability components", "Insights into larger models showing greater attribution to user purpose variability"], "limitations": "The robustness advantage of larger models is often modest and not uniform across all domains.", "keywords": ["large language models", "world model", "evaluation framework", "semantic diagnostics", "output variability"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.16594", "pdf": "https://arxiv.org/pdf/2506.16594.pdf", "abs": "https://arxiv.org/abs/2506.16594", "title": "A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications", "authors": ["Hanshu Rao", "Weisi Liu", "Haohan Wang", "I-Chan Huang", "Zhe He", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Synthetic data generation--mitigating data scarcity, privacy concerns, and\ndata quality challenges in biomedical fields--has been facilitated by rapid\nadvances of large language models (LLMs). This scoping review follows\nPRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and\n2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The\nreview systematically examines biomedical research and application trends in\nsynthetic data generation, emphasizing clinical applications, methodologies,\nand evaluations. Our analysis identifies data modalities of unstructured texts\n(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation\nmethods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model\n(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),\nhuman-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The\nanalysis addresses current limitations in what, where, and how health\nprofessionals can leverage synthetic data generation for biomedical domains.\nOur review also highlights challenges in adaption across clinical domains,\nresource and model accessibility, and evaluation standardizations.", "AI": {"tldr": "This scoping review examines 59 studies on synthetic data generation in biomedical fields, focusing on clinical applications and methodologies.", "motivation": "To address data scarcity, privacy concerns, and data quality challenges in biomedical fields through synthetic data generation using large language models (LLMs).", "method": "A scoping review following PRISMA-ScR guidelines, synthesizing studies published between 2020 and 2025 from various academic databases.", "result": "The review reveals that 78.0% of studies involve unstructured texts, 13.6% tabular data, and 8.4% multimodal sources; generation methods predominantly use prompting (72.9%) and fine-tuning (22.0%); evaluations vary with 55.9% using human-in-the-loop assessments.", "conclusion": "Current limitations in synthetic data generation across biomedical domains include issues with adaptation, accessibility of resources and models, and lack of standardized evaluations.", "key_contributions": ["Systematic examination of synthetic data generation in biomedicine", "Identification of prevalent data modalities and generation methods", "Analysis of evaluation strategies and their challenges"], "limitations": "Limitations in adaptation across clinical domains and evaluation standardizations.", "keywords": ["synthetic data", "large language models", "biomedical applications", "data generation", "evaluation methods"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.16622", "pdf": "https://arxiv.org/pdf/2506.16622.pdf", "abs": "https://arxiv.org/abs/2506.16622", "title": "Modeling Public Perceptions of Science in Media", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstin", "David Jurgens"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content.", "AI": {"tldr": "The paper introduces a computational framework for modeling public perception of science, utilizing a large-scale dataset to predict engagement with scientific information.", "motivation": "Engaging the public with science is crucial for building trust and understanding, yet communicators face challenges in anticipating audience interactions with scientific news.", "method": "A computational framework modeled public perception across 12 dimensions, creating a large-scale dataset with 10,489 annotations from a diverse participant pool. NLP models were developed to predict perception scores.", "result": "It was found that the frequency of science news consumption is the primary driver of public perception, with minimal demographic influence. A large-scale analysis showed that positive perception scores correlate with increased engagement (comments, upvotes) on platforms like Reddit.", "conclusion": "Nuanced perception modeling is essential for effective science communication and can help predict public interest and engagement with scientific content.", "key_contributions": ["Introduction of a computational framework for public perception modeling", "Creation of a large-scale science news perception dataset", "Development of NLP models that predict public perception scores"], "limitations": "", "keywords": ["science communication", "public perception", "NLP models", "dataset", "public engagement"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.16628", "pdf": "https://arxiv.org/pdf/2506.16628.pdf", "abs": "https://arxiv.org/abs/2506.16628", "title": "Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System", "authors": ["Jianlin Shi", "Brian T. Bucher"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite advances in machine learning (ML) and large language models (LLMs),\nrule-based natural language processing (NLP) systems remain active in clinical\nsettings due to their interpretability and operational efficiency. However,\ntheir manual development and maintenance are labor-intensive, particularly in\ntasks with large linguistic variability. To overcome these limitations, we\nproposed a novel approach employing LLMs solely during the rule-based systems\ndevelopment phase. We conducted the initial experiments focusing on the first\ntwo steps of developing a rule-based NLP pipeline: find relevant snippets from\nthe clinical note; extract informative keywords from the snippets for the\nrule-based named entity recognition (NER) component. Our experiments\ndemonstrated exceptional recall in identifying clinically relevant text\nsnippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.\nThis study sheds light on a promising new direction for NLP development,\nenabling semi-automated or automated development of rule-based systems with\nsignificantly faster, more cost-effective, and transparent execution compared\nwith deep learning model-based solutions.", "AI": {"tldr": "This paper proposes a novel approach using LLMs to enhance the development of rule-based NLP systems for clinical settings, achieving high performance in identifying relevant snippets and extracting keywords for NER.", "motivation": "There is a need to improve the labor-intensive process of developing rule-based NLP systems in clinical settings, balancing efficiency and interpretability with the advent of ML and LLMs.", "method": "The proposed approach involves utilizing LLMs during the development phase of rule-based NLP systems, particularly focusing on identifying relevant text snippets and extracting keywords to support the NER task.", "result": "The experiments showed high recall rates of 0.98 for Deepseek and 0.99 for Qwen in identifying clinically relevant snippets, and a perfect score of 1.0 in keyword extraction for the NER component.", "conclusion": "The study indicates that using LLMs for initial phases of rule-based NLP system development could significantly enhance the process, making it faster, more cost-effective, and transparent compared to traditional deep learning models.", "key_contributions": ["Introduction of LLMs in the rule-based NLP system development phase", "High recall rates for identifying relevant clinical text snippets", "Perfect performance in keyword extraction for named entity recognition"], "limitations": "", "keywords": ["rule-based NLP", "large language models", "clinical settings", "named entity recognition", "machine learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.16633", "pdf": "https://arxiv.org/pdf/2506.16633.pdf", "abs": "https://arxiv.org/abs/2506.16633", "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View", "authors": ["Fenghua Cheng", "Jinxiang Wang", "Sen Wang", "Zi Huang", "Xue Li"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "Multimodal reasoning is a process of understanding, integrating and inferring\ninformation across different data modalities. It has recently attracted surging\nacademic attention as a benchmark for Artificial Intelligence (AI). Although\nthere are various tasks for evaluating multimodal reasoning ability, they still\nhave limitations. Lack of reasoning on hierarchical visual clues at different\nlevels of granularity, e.g., local details and global context, is of little\ndiscussion, despite its frequent involvement in real scenarios. To bridge the\ngap, we introduce a novel and challenging task for multimodal reasoning, namely\nGeoGuess. Given a street view image, the task is to identify its location and\nprovide a detailed explanation. A system that succeeds in GeoGuess should be\nable to detect tiny visual clues, perceive the broader landscape, and associate\nwith vast geographic knowledge. Therefore, GeoGuess would require the ability\nto reason between hierarchical visual information and geographic knowledge. In\nthis work, we establish a benchmark for GeoGuess by introducing a specially\ncurated dataset GeoExplain which consists of\npanoramas-geocoordinates-explanation tuples. Additionally, we present a\nmultimodal and multilevel reasoning method, namely SightSense which can make\nprediction and generate comprehensive explanation based on hierarchy of visual\ninformation and external knowledge. Our analysis and experiments demonstrate\ntheir outstanding performance in GeoGuess.", "AI": {"tldr": "This paper introduces GeoGuess, a novel multimodal reasoning task requiring location identification and explanation from street view images, along with the GeoExplain dataset and a method called SightSense for hierarchical reasoning.", "motivation": "To address the limitations of existing multimodal reasoning tasks that do not adequately consider hierarchical visual clues at different levels of granularity, which are crucial in real-world scenarios.", "method": "A benchmark task called GeoGuess is introduced along with the GeoExplain dataset, consisting of panoramas with geocoordinates and explanations. A multimodal reasoning method named SightSense is proposed to predict and generate explanations based on hierarchical visual information and external geographic knowledge.", "result": "The proposed methods and dataset demonstrate outstanding performance in the GeoGuess task, showcasing the effectiveness of hierarchical reasoning in multimodal contexts.", "conclusion": "GeoGuess sets a new standard for evaluating multimodal reasoning in AI by emphasizing the integration of detailed visual clues with broader geographic context.", "key_contributions": ["Introduction of the GeoGuess task for multimodal reasoning.", "Creation of the GeoExplain dataset for benchmarking.", "Development of SightSense, a method for hierarchical visual reasoning."], "limitations": "", "keywords": ["multimodal reasoning", "GeoGuess", "hierarchical visual information", "AI", "geographic knowledge"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16640", "pdf": "https://arxiv.org/pdf/2506.16640.pdf", "abs": "https://arxiv.org/abs/2506.16640", "title": "Long-Context Generalization with Sparse Attention", "authors": ["Pavlo Vasylenko", "Marcos Treviso", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformer-based architectures traditionally employ softmax to compute\nattention weights, which produces dense distributions over all tokens in a\nsequence. While effective in many settings, this density has been shown to be\ndetrimental for tasks that demand precise focus on fixed-size patterns: as\nsequence length increases, non-informative tokens accumulate attention\nprobability mass, leading to dispersion and representational collapse. We show\nin this paper that sparse attention mechanisms using $\\alpha$-entmax can avoid\nthese issues, due to their ability to assign exact zeros to irrelevant tokens.\nFurthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows\n$\\alpha$-entmax with a learnable temperature parameter, allowing the attention\ndistribution to interpolate between sparse (pattern-focused) and dense\n(softmax-like) regimes. Finally, we show that the ability to locate and\ngeneralize fixed-size patterns can be further improved through a careful design\nof position encodings, which impacts both dense and sparse attention methods.\nBy integrating ASEntmax into standard transformer layers alongside proper\npositional encodings, we show that our models greatly outperform softmax,\nscalable softmax, and fixed-temperature $\\alpha$-entmax baselines on\nlong-context generalization.", "AI": {"tldr": "This paper introduces a novel attention mechanism called Adaptive-Scalable Entmax (ASEntmax) that improves long-context generalization in transformer models by using sparse attention and learnable temperature parameters.", "motivation": "Transformer architectures often struggle with attention dispersal on lengthy sequences, leading to inefficiencies in tasks requiring focus on fixed-size patterns.", "method": "The authors propose the use of $\text{ASEntmax}$, a sparse attention mechanism that incorporates a learnable temperature parameter for adaptive attention distribution, allowing for better fixation on relevant tokens while nullifying irrelevant ones.", "result": "Integration of ASEntmax into the transformer architecture significantly enhances performance on long-context generalization tasks, surpassing traditional softmax and other attention baselines.", "conclusion": "The proposed ASEntmax, along with optimized positional encodings, provides a considerable advancement in transformer model capabilities for tasks requiring specific pattern recognition in extensive data sequences.", "key_contributions": ["Introduction of ASEntmax as a novel attention mechanism", "Demonstration of ASEntmax's effectiveness over traditional softmax in long-context tasks", "Insights on the design of positional encodings to complement attention mechanisms."], "limitations": "", "keywords": ["sparse attention", "Adaptive-Scalable Entmax", "transformer architecture", "long-context generalization", "position encodings"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16655", "pdf": "https://arxiv.org/pdf/2506.16655.pdf", "abs": "https://arxiv.org/abs/2506.16655", "title": "Arch-Router: Aligning LLM Routing with Human Preferences", "authors": ["Co Tran", "Salman Paracha", "Adil Hafeez", "Shuguang Chen"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce \\textbf{Arch-Router}, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: \\texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.", "AI": {"tldr": "A framework called Arch-Router is proposed for preference-aligned routing of large language models, improving the selection process by matching user queries to user-defined preferences and domains.", "motivation": "To improve the operationalization of diverse large language models by aligning routing decisions with human preferences, which are often subjective and poorly captured by existing benchmarks.", "method": "Arch-Router is a compact 1.5B model that maps queries to preferences based on user-defined domains or action types, enabling flexible routing without retraining or architectural changes.", "result": "The proposed routing framework achieved state-of-the-art results in matching queries with human preferences on conversational datasets, outperforming existing proprietary models.", "conclusion": "Arch-Router offers a more transparent and flexible routing mechanism for large language models, accommodating new models without significant retraining needs.", "key_contributions": ["Development of Arch-Router model for aligning LLM routing with human preferences.", "Demonstrated state-of-the-art performance in query matching based on subjective evaluation criteria.", "Enabling flexible integration of new models into the routing framework."], "limitations": "", "keywords": ["large language models", "model routing", "preference alignment", "human preferences", "Arch-Router"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16678", "pdf": "https://arxiv.org/pdf/2506.16678.pdf", "abs": "https://arxiv.org/abs/2506.16678", "title": "Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations", "authors": ["Ananth Agarwal", "Jasper Jian", "Christopher D. Manning", "Shikhar Murty"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit a robust mastery of syntax when\nprocessing and generating text. While this suggests internalized understanding\nof hierarchical syntax and dependency relations, the precise mechanism by which\nthey represent syntactic structure is an open area within interpretability\nresearch. Probing provides one way to identify the mechanism of syntax being\nlinearly encoded in activations, however, no comprehensive study has yet\nestablished whether a model's probing accuracy reliably predicts its downstream\nsyntactic performance. Adopting a \"mechanisms vs. outcomes\" framework, we\nevaluate 32 open-weight transformer models and find that syntactic features\nextracted via probing fail to predict outcomes of targeted syntax evaluations\nacross English linguistic phenomena. Our results highlight a substantial\ndisconnect between latent syntactic representations found via probing and\nobservable syntactic behaviors in downstream tasks.", "AI": {"tldr": "This study investigates the relationship between probing accuracy and syntactic performance in transformer models, finding that probing results do not reliably predict downstream syntax evaluation outcomes.", "motivation": "Understanding how Large Language Models (LLMs) represent syntactic structure is important for improving their interpretability and performance in syntactic tasks.", "method": "The study evaluates 32 open-weight transformer models through a 'mechanisms vs. outcomes' framework, comparing probing accuracy to targeted syntax evaluations.", "result": "The findings indicate that syntactic features extracted from probing do not predict the models' performance in downstream syntactic evaluations across various English linguistic phenomena.", "conclusion": "There is a significant disconnect between latent syntactic representations obtained through probing and the observable syntactic behaviors exhibited by the models in downstream tasks.", "key_contributions": ["Evaluated probing accuracy of 32 transformer models", "Established a framework for comparing mechanisms and outcomes in syntax processing", "Highlighted the disconnect between probing and syntactic performance"], "limitations": "The study focuses only on English linguistic phenomena; findings may not generalize to other languages or syntactic features.", "keywords": ["Large Language Models", "syntax", "probing", "transformer models", "interpretability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.16692", "pdf": "https://arxiv.org/pdf/2506.16692.pdf", "abs": "https://arxiv.org/abs/2506.16692", "title": "LegiGPT: Party Politics and Transport Policy with Large Language Model", "authors": ["Hyunsoo Yun", "Eun Hak Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Given the significant influence of lawmakers' political ideologies on\nlegislative decision-making, understanding their impact on policymaking is\ncritically important. We introduce a novel framework, LegiGPT, which integrates\na large language model (LLM) with explainable artificial intelligence (XAI) to\nanalyze transportation-related legislative proposals. LegiGPT employs a\nmulti-stage filtering and classification pipeline using zero-shot prompting\nwith GPT-4. Using legislative data from South Korea's 21st National Assembly,\nwe identify key factors - including sponsor characteristics, political\naffiliations, and geographic variables - that significantly influence\ntransportation policymaking. The LLM was used to classify\ntransportation-related bill proposals through a stepwise filtering process\nbased on keywords, phrases, and contextual relevance. XAI techniques were then\napplied to examine relationships between party affiliation and associated\nattributes. The results reveal that the number and proportion of conservative\nand progressive sponsors, along with district size and electoral population,\nare critical determinants shaping legislative outcomes. These findings suggest\nthat both parties contributed to bipartisan legislation through different forms\nof engagement, such as initiating or supporting proposals. This integrated\napproach provides a valuable tool for understanding legislative dynamics and\nguiding future policy development, with broader implications for infrastructure\nplanning and governance.", "AI": {"tldr": "LegiGPT integrates a large language model with explainable AI to analyze transportation-related legislation, revealing key political and geographical factors influencing policymaking.", "motivation": "Understanding how lawmakers' political ideologies impact legislative decision-making is essential for effective policymaking.", "method": "LegiGPT uses a multi-stage filtering and classification pipeline with zero-shot prompting via GPT-4 to analyze legislative data, focusing on sponsor characteristics, political affiliations, and geographic variables.", "result": "Key determinants of transportation policymaking include the number of conservative and progressive sponsors and district demographics, demonstrating bipartisan engagement in legislation.", "conclusion": "This framework offers insights for legislative dynamics and can inform future policy development, notably in infrastructure planning.", "key_contributions": ["Introduction of LegiGPT framework for legislative analysis", "Combines LLM with XAI for explainable decision-making", "Identifies key factors in transportation policymaking"], "limitations": "", "keywords": ["Legislation", "Large Language Model", "Explainable AI", "Transportation Policy", "Political Ideologies"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.16712", "pdf": "https://arxiv.org/pdf/2506.16712.pdf", "abs": "https://arxiv.org/abs/2506.16712", "title": "ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models", "authors": ["Bin Chen", "Xinzge Gao", "Chuanrui Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative Reward Models (GRMs) provide greater flexibility than scalar\nreward models in capturing human preferences, but their effectiveness is\nlimited by poor reasoning capabilities. This often results in incomplete or\noverly speculative reasoning paths, leading to hallucinations or missing key\ninformation in complex tasks. We address this challenge with ReasonGRM, a\nthree-stage generative reward modeling framework. In the first stage, Zero-RL\nis used to generate concise, outcome-directed reasoning paths that reduce the\nlikelihood of critical omissions. In the second stage, we introduce a novel\nevaluation metric, $R^\\star$, which scores reasoning paths based on their\ngeneration likelihood. This favors paths that reach correct answers with\nminimal exploration, helping to reduce hallucination-prone data during\ntraining. In the final stage, the model is further refined through\nreinforcement learning on challenging examples to enhance its preference\ndiscrimination capabilities. Experiments on three public benchmarks show that\nReasonGRM achieves competitive or state-of-the-art performance, outperforming\nprevious best GRMs by 1.8\\% on average and surpassing proprietary models such\nas GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of\nreasoning-aware training and highlight the importance of high-quality rationale\nselection for reliable preference modeling.", "AI": {"tldr": "This paper introduces ReasonGRM, a generative reward modeling framework that improves reasoning capabilities to enhance the performance of generative reward models.", "motivation": "Generative Reward Models often struggle with reasoning, leading to hallucinations and incomplete information in complex tasks; there is a need for improved reasoning methods in this domain.", "method": "The ReasonGRM framework consists of three stages: using Zero-RL for concise reasoning paths, applying a novel evaluation metric $R^*$ to score these paths, and refining the model through reinforcement learning on challenging examples.", "result": "ReasonGRM outperforms previous state-of-the-art Generative Reward Models by 1.8% on average and exceeds proprietary models like GPT-4o by up to 5.6% across three public benchmarks.", "conclusion": "The proposed methodology demonstrates that reasoning-aware training and quality rationale selection are crucial for effective preference modeling.", "key_contributions": ["Introduction of ReasonGRM framework for improved reasoning in GRMs", "Novel evaluation metric $R^*$ to optimize reasoning paths", "Enhanced performance benchmarks compared to existing models"], "limitations": "", "keywords": ["Generative Reward Models", "Human preferences", "Reinforcement learning", "Machine Learning", "Reasoning paths"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.16724", "pdf": "https://arxiv.org/pdf/2506.16724.pdf", "abs": "https://arxiv.org/abs/2506.16724", "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties", "authors": ["Xinyi Liu", "Weiguang Wang", "Hangfeng He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) for open-ended\ntasks, accurately assessing epistemic uncertainty, which reflects a model's\nlack of knowledge, has become crucial to ensuring reliable outcomes. However,\nquantifying epistemic uncertainty in such tasks is challenging due to the\npresence of aleatoric uncertainty, which arises from multiple valid answers.\nWhile bias can introduce noise into epistemic uncertainty estimation, it may\nalso reduce noise from aleatoric uncertainty. To investigate this trade-off, we\nconduct experiments on Visual Question Answering (VQA) tasks and find that\nmitigating prompt-introduced bias improves uncertainty quantification in\nGPT-4o. Building on prior work showing that LLMs tend to copy input information\nwhen model confidence is low, we further analyze how these prompt biases affect\nmeasured epistemic and aleatoric uncertainty across varying bias-free\nconfidence levels with GPT-4o and Qwen2-VL. We find that all considered biases\ninduce greater changes in both uncertainties when bias-free model confidence is\nlower. Moreover, lower bias-free model confidence leads to greater\nunderestimation of epistemic uncertainty (i.e. overconfidence) due to bias,\nwhereas it has no significant effect on the direction of changes in aleatoric\nuncertainty estimation. These distinct effects deepen our understanding of bias\nmitigation for uncertainty quantification and potentially inform the\ndevelopment of more advanced techniques.", "AI": {"tldr": "This paper investigates the impact of prompt-introduced bias on epistemic and aleatoric uncertainty in LLMs, particularly in Visual Question Answering tasks.", "motivation": "The growing use of LLMs necessitates reliable uncertainty assessment to improve model outcomes, particularly in tasks with multiple valid answers.", "method": "Experiments were conducted on Visual Question Answering tasks using GPT-4o and Qwen2-VL to analyze the effects of mitigating biases on uncertainty quantification.", "result": "Mitigating prompt-introduced bias improves uncertainty quantification, with biases causing greater changes in uncertainty measures at lower model confidence levels.", "conclusion": "Understanding the trade-offs in bias mitigation can enhance uncertainty quantification methods in LLMs, leading to better decision-making processes in open-ended tasks.", "key_contributions": ["Investigates the trade-off between epistemic and aleatoric uncertainty in LLMs.", "Finds that mitigating bias improves uncertainty assessment in Visual Question Answering tasks.", "Demonstrates the relationship between model confidence and uncertainty under bias influences."], "limitations": "", "keywords": ["Large Language Models", "epistemic uncertainty", "aleatoric uncertainty", "Visual Question Answering", "bias mitigation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16738", "pdf": "https://arxiv.org/pdf/2506.16738.pdf", "abs": "https://arxiv.org/abs/2506.16738", "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization", "authors": ["Daejin Jo", "Jeeyoung Yun", "Byungseok Roh", "Sungwoong Kim"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "With the rapid progress of speech language models (SLMs), discrete speech\ntokens have emerged as a core interface between speech and text, enabling\nunified modeling across modalities. Recent speech tokenization approaches aim\nto isolate semantic information from low-level acoustics to better align with\nlanguage models. In particular, previous methods use SSL teachers such as\nHuBERT to extract semantic representations, which are then distilled into a\nsemantic quantizer to suppress acoustic redundancy as well as capture\ncontent-related latent structures. However, they still produce speech token\nsequences significantly longer than their textual counterparts, creating\nchallenges for efficient speech-language modeling. Reducing the frame rate is a\nnatural solution, but standard techniques, such as rigid average pooling across\nframes, can distort or dilute the semantic structure required for effective LM\nalignment. To address this, we propose LM-SPT, a speech tokenization method\nthat introduces a novel semantic distillation. Instead of directly matching\nteacher and student features via pooling, we reconstruct speech solely from\nsemantic tokens and minimize the discrepancy between the encoded\nrepresentations of the original and reconstructed waveforms, obtained from a\nfrozen automatic speech recognition (ASR) encoder. This indirect yet\ndata-driven supervision enables the tokenizer to learn discrete units that are\nmore semantically aligned with language models. LM-SPT further incorporates\narchitectural improvements to the encoder and decoder for speech tokenization,\nand supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.\nExperimental results show that LM-SPT achieves superior reconstruction fidelity\ncompared to baselines, and that SLMs trained with LM-SPT tokens achieve\ncompetitive performances on speech-to-text and consistently outperform\nbaselines on text-to-speech tasks.", "AI": {"tldr": "This paper presents LM-SPT, a novel speech tokenization method that enhances the alignment of speech tokens with language models by reconstructing speech from semantic tokens, achieving superior performance in speech-to-text and text-to-speech tasks.", "motivation": "The paper addresses the growing need for efficient speech-language modeling by improving speech tokenization methods, which often produce excessively long token sequences that challenge processing efficiency.", "method": "LM-SPT introduces a distinct semantic distillation approach, focusing on reconstructing speech from semantic tokens and minimizing discrepancies between original and reconstructed waveforms, while also integrating architectural improvements for the tokenizer.", "result": "Experimental results demonstrate that LM-SPT has better reconstruction fidelity than existing methods, and that language models trained with LM-SPT tokens perform competitively on speech-to-text and significantly outperform other methods on text-to-speech tasks.", "conclusion": "The findings suggest that LM-SPT provides an effective solution for improving speech tokenization, thereby enhancing the performance of speech-language models in both recognition and synthesis tasks.", "key_contributions": ["Introduces semantic distillation for improved speech tokenization.", "Achieves better alignment of speech tokens with language models.", "Demonstrates architectural enhancements for tokenizer efficiency."], "limitations": "", "keywords": ["speech tokenization", "language models", "semantic distillation", "speech-to-text", "text-to-speech"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.16755", "pdf": "https://arxiv.org/pdf/2506.16755.pdf", "abs": "https://arxiv.org/abs/2506.16755", "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly", "authors": ["Lance Ying", "Ryan Truong", "Katherine M. Collins", "Cedegao E. Zhang", "Megan Wei", "Tyler Brooke-Wilson", "Tan Zhi-Xuan", "Lionel Wong", "Joshua B. Tenenbaum"], "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 19 pages", "summary": "Drawing real world social inferences usually requires taking into account\ninformation from multiple modalities. Language is a particularly powerful\nsource of information in social settings, especially in novel situations where\nlanguage can provide both abstract information about the environment dynamics\nand concrete specifics about an agent that cannot be easily visually observed.\nIn this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a\nframework for drawing context-specific social inferences that integrate\nlinguistic and visual inputs. LIRAS frames multimodal social reasoning as a\nprocess of constructing structured but situation-specific agent and environment\nrepresentations - leveraging multimodal language models to parse language and\nvisual inputs into unified symbolic representations, over which a Bayesian\ninverse planning engine can be run to produce granular probabilistic judgments.\nOn a range of existing and new social reasoning tasks derived from cognitive\nscience experiments, we find that our model (instantiated with a comparatively\nlightweight VLM) outperforms ablations and state-of-the-art models in capturing\nhuman judgments across all domains.", "AI": {"tldr": "LIRAS is a framework that integrates linguistic and visual inputs for multimodal social reasoning, improving social inference accuracy through structured representations.", "motivation": "To enhance social inference accuracy by merging linguistic and visual information in novel situations where language provides critical insights.", "method": "LIRAS constructs situation-specific representations of agents and environments by using multimodal language models and a Bayesian inverse planning engine to perform social reasoning.", "result": "LIRAS outperforms existing models and ablations on various social reasoning tasks, demonstrating improved alignment with human judgments.", "conclusion": "The proposed framework effectively leverages multimodal inputs to draw granular social inferences, significantly enhancing the capabilities of prior models.", "key_contributions": ["Introduction of the LIRAS framework for multimodal social reasoning", "Integration of linguistic and visual inputs for enhanced social inference", "Demonstrated superiority of the model across various cognitive science-derived tasks"], "limitations": "", "keywords": ["multimodal reasoning", "social inference", "language models", "Bayesian planning", "visual inputs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16756", "pdf": "https://arxiv.org/pdf/2506.16756.pdf", "abs": "https://arxiv.org/abs/2506.16756", "title": "SocialSim: Towards Socialized Simulation of Emotional Support Conversation", "authors": ["Zhuang Chen", "Yaru Cao", "Guanqun Bi", "Jincenzi Wu", "Jinfeng Zhou", "Xiyao Xiao", "Si Chen", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "AAAI 2025 Paper #32116 (Without Publication Edits)", "summary": "Emotional support conversation (ESC) helps reduce people's psychological\nstress and provide emotional value through interactive dialogues. Due to the\nhigh cost of crowdsourcing a large ESC corpus, recent attempts use large\nlanguage models for dialogue augmentation. However, existing approaches largely\noverlook the social dynamics inherent in ESC, leading to less effective\nsimulations. In this paper, we introduce SocialSim, a novel framework that\nsimulates ESC by integrating key aspects of social interactions: social\ndisclosure and social awareness. On the seeker side, we facilitate social\ndisclosure by constructing a comprehensive persona bank that captures diverse\nand authentic help-seeking scenarios. On the supporter side, we enhance social\nawareness by eliciting cognitive reasoning to generate logical and supportive\nresponses. Building upon SocialSim, we construct SSConv, a large-scale\nsynthetic ESC corpus of which quality can even surpass crowdsourced ESC data.\nWe further train a chatbot on SSConv and demonstrate its state-of-the-art\nperformance in both automatic and human evaluations. We believe SocialSim\noffers a scalable way to synthesize ESC, making emotional care more accessible\nand practical.", "AI": {"tldr": "This paper presents SocialSim, a framework for simulating emotional support conversations (ESC) by integrating social disclosure and awareness, resulting in a large-scale synthetic ESC corpus (SSConv) that outperforms crowdsourced data.", "motivation": "To address the limitations of current dialogue augmentation methods for emotional support conversations that overlook social dynamics and to provide a scalable solution for synthesizing emotional care.", "method": "The framework integrates social disclosure through a persona bank of diverse scenarios and enhances social awareness via cognitive reasoning for generating supportive responses. A large-scale synthetic ESC corpus (SSConv) is created and utilized to train a chatbot.", "result": "The trained chatbot achieves state-of-the-art performance in both automatic and human evaluations, indicating the quality of SSConv surpasses that of crowdsourced data.", "conclusion": "SocialSim offers a novel and practical approach to make emotional care through conversations more accessible and effective.", "key_contributions": ["Introduction of SocialSim framework for simulating ESC incorporating social dynamics", "Development of SSConv, a large-scale synthetic ESC corpus that outperforms crowdsourced data", "Demonstrated state-of-the-art performance of a trained chatbot on the synthetic data"], "limitations": "", "keywords": ["emotional support conversations", "social dynamics", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16760", "pdf": "https://arxiv.org/pdf/2506.16760.pdf", "abs": "https://arxiv.org/abs/2506.16760", "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models", "authors": ["Lei Jiang", "Zixun Zhang", "Zizhou Wang", "Xiaobing Sun", "Zhen Li", "Liangli Zhen", "Xiaohua Xu"], "categories": ["cs.CL", "cs.CV"], "comment": "15 pages, 9 figures", "summary": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance\nacross multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass\nbuilt-in safety mechanisms to elicit restricted content generation. Existing\nblack-box jailbreak methods primarily rely on adversarial textual prompts or\nimage perturbations, yet these approaches are highly detectable by standard\ncontent filtering systems and exhibit low query and computational efficiency.\nIn this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),\na novel black-box jailbreak attack framework that decomposes malicious prompts\ninto semantically benign visual and textual fragments. By leveraging LVLMs'\ncross-modal reasoning abilities, CAMO covertly reconstructs harmful\ninstructions through multi-step reasoning, evading conventional detection\nmechanisms. Our approach supports adjustable reasoning complexity and requires\nsignificantly fewer queries than prior attacks, enabling both stealth and\nefficiency. Comprehensive evaluations conducted on leading LVLMs validate\nCAMO's effectiveness, showcasing robust performance and strong cross-model\ntransferability. These results underscore significant vulnerabilities in\ncurrent built-in safety mechanisms, emphasizing an urgent need for advanced,\nalignment-aware security and safety solutions in vision-language systems.", "AI": {"tldr": "A new black-box jailbreak attack framework, CAMO, effectively bypasses safety mechanisms in large vision-language models by decomposing prompts into semantically benign fragments, boasting improved efficiency and stealth.", "motivation": "To address vulnerabilities in current vision-language models (LVLMs) that can be exploited through jailbreak attacks, which bypass built-in safety mechanisms.", "method": "CAMO decomposes malicious prompts into benign visual and textual fragments using the cross-modal reasoning abilities of LVLMs. It relies on multi-step reasoning to reconstruct harmful content without being detected.", "result": "CAMO exhibits strong effectiveness against leading vision-language models, requiring significantly fewer queries and demonstrating robust performance and high cross-model transferability.", "conclusion": "The findings highlight critical weaknesses in existing safety mechanisms of LVLMs, necessitating the development of advanced security measures that are alignment-aware.", "key_contributions": ["Introduction of the CAMO framework for black-box jailbreak attacks", "Improved efficiency and stealth compared to prior methods", "Evident vulnerabilities discovered in current LVLM safety mechanisms"], "limitations": "", "keywords": ["Vision-Language Models", "jailbreak attacks", "security mechanisms", "cross-modal reasoning", "multimodal tasks"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.16777", "pdf": "https://arxiv.org/pdf/2506.16777.pdf", "abs": "https://arxiv.org/abs/2506.16777", "title": "DistillNote: LLM-based clinical note summaries improve heart failure diagnosis", "authors": ["Heloisa Oss Boll", "Antonio Oss Boll", "Leticia Puttlitz Boll", "Ameen Abu Hanna", "Iacer Calixto"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research.", "AI": {"tldr": "This paper presents Distillnote, a framework for LLM-based clinical note summarization that improves documentation efficiency and predictive performance for heart failure.", "motivation": "Healthcare providers are overwhelmed by clinical documentation; LLMs can automate and summarize this information effectively.", "method": "The framework generates summaries using three techniques: direct summarization, structured summarization, and distilled summarization. It tests the utility of these summaries by predicting heart failure outcomes using LLM-generated summaries compared to original notes.", "result": "Distilled summaries achieved 79% text compression and a 18.2% improvement in AUPRC over the model trained on original notes.", "conclusion": "One-step summaries are preferred for relevance and clinical actionability, while distilled summaries provide high efficiency and reduced hallucination rates. The summaries are publicly available on PhysioNet for further research.", "key_contributions": ["Introduction of Distillnote for clinical note summarization", "Demonstrated significant improvement in predictive performance for heart failure", "Public release of over 64,000 admission note summaries for future research"], "limitations": "Not specified in the abstract.", "keywords": ["Large Language Models", "Clinical Note Summarization", "Health Informatics"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2506.16792", "pdf": "https://arxiv.org/pdf/2506.16792.pdf", "abs": "https://arxiv.org/abs/2506.16792", "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning", "authors": ["Muyang Zheng", "Yuanzhi Yao", "Changting Lin", "Rui Wang", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST.", "AI": {"tldr": "This paper presents MIST, a method for jailbreaking black-box large language models by refining prompts to induce harmful content while maintaining semantic intent.", "motivation": "To address the vulnerabilities of large language models to jailbreak attacks, which can elicit harmful responses despite attempts to align these models with societal values.", "method": "MIST uses Iterative Semantic Tuning to iteratively refine prompts, employing strategies like sequential synonym search and order-determining optimization to enhance attack success while maintaining computational efficiency.", "result": "MIST demonstrates competitive attack success rates and transferability compared to existing jailbreak methods, validated through extensive experiments on various models.", "conclusion": "MIST effectively combines semantic preservation and computational efficiency, making it a viable approach for conducting jailbreak attacks on black-box LLMs.", "key_contributions": ["Introduction of MIST for jailbreaking black-box LLMs", "Innovative use of Iterative Semantic Tuning for prompt refinement", "Demonstrated balance between semantic intent and attack efficiency"], "limitations": "The study does not explore the ethical implications of using such methods in real-world applications.", "keywords": ["large language models", "jailbreak attacks", "Iterative Semantic Tuning", "prompt refinement", "computational efficiency"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2506.16912", "pdf": "https://arxiv.org/pdf/2506.16912.pdf", "abs": "https://arxiv.org/abs/2506.16912", "title": "From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts", "authors": ["Daniel Christoph", "Max Ploner", "Patrick Haller", "Alan Akbik"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to the First Workshop on Large Language Model Memorization\n  (L2M2), co-located with ACL 2025 in Vienna", "summary": "Sample efficiency is a crucial property of language models with practical\nimplications for training efficiency. In real-world text, information follows a\nlong-tailed distribution. Yet, we expect models to learn and recall frequent\nand infrequent facts. Sample-efficient models are better equipped to handle\nthis challenge of learning and retaining rare information without requiring\nexcessive exposure. This study analyzes multiple models of varying\narchitectures and sizes, all trained on the same pre-training data. By\nannotating relational facts with their frequencies in the training corpus, we\nexamine how model performance varies with fact frequency. Our findings show\nthat most models perform similarly on high-frequency facts but differ notably\non low-frequency facts. This analysis provides new insights into the\nrelationship between model architecture, size, and factual learning efficiency.", "AI": {"tldr": "This study investigates the sample efficiency of language models in learning and recalling facts with varying frequency in the training corpus.", "motivation": "The importance of sample efficiency in language models for improving training efficiency, especially in handling long-tailed distributions of information found in real-world text.", "method": "We analyze multiple models of different architectures and sizes, all trained on the same pre-training data, and annotate relational facts with their frequencies.", "result": "Most models show similar performance on high-frequency facts but exhibit significant differences on low-frequency facts.", "conclusion": "The findings reveal new insights into how model architecture and size affect the efficiency of factual learning, particularly for low-frequency information.", "key_contributions": ["Analysis of model performance on facts of varying frequencies", "Insights into the relationship between model architecture/size and sample efficiency", "Focus on low-frequency facts and their learning efficiency"], "limitations": "", "keywords": ["sample efficiency", "language models", "long-tailed distribution", "fact frequency", "model architecture"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.16982", "pdf": "https://arxiv.org/pdf/2506.16982.pdf", "abs": "https://arxiv.org/abs/2506.16982", "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond", "authors": ["Antonin Berthon", "Mihaela van der Schaar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality.", "AI": {"tldr": "This paper presents the Language Bottleneck Model (LBM) for Knowledge Tracing (KT) that enhances interpretability and prediction accuracy by summarizing student knowledge in natural language.", "motivation": "To address the limitations of traditional KT methods and LLM-based predictions that lack interpretability and accuracy guarantees.", "method": "The Language Bottleneck Model (LBM) uses an encoder LLM to create an interpretable summary of student knowledge, which is then used by a frozen decoder LLM to predict future responses.", "result": "LBMs achieve accuracy comparable to state-of-the-art KT methods while requiring significantly fewer student trajectories, as shown in experiments on synthetic and large-scale datasets.", "conclusion": "By constraining predictive information through a natural-language bottleneck, LBMs offer a more interpretable and effective approach to student knowledge assessment.", "key_contributions": ["Introduction of the Language Bottleneck Model (LBM) for Knowledge Tracing", "Improved interpretability of student knowledge assessments", "Demonstrated effectiveness with fewer data requirements compared to traditional methods"], "limitations": "The model's performance may still be constrained by the quality of the LLMs used, and it requires effective training strategies to ensure summary accuracy.", "keywords": ["Knowledge Tracing", "Language Bottleneck Model", "student assessment", "interpretability", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.16990", "pdf": "https://arxiv.org/pdf/2506.16990.pdf", "abs": "https://arxiv.org/abs/2506.16990", "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the SDProc Workshop @ ACL 2025", "summary": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert.", "AI": {"tldr": "The paper introduces TeXpert, a benchmark for evaluating LLMs' ability to generate LaTeX code from natural language prompts, analyzing performance across various tasks and identifying common errors.", "motivation": "Despite the potential of LLMs to generate LaTeX code with natural language instructions, current evaluations do not reflect this capability, prompting the need for a dedicated benchmark.", "method": "The authors created TeXpert, a dataset with natural language prompts for LaTeX code generation, and evaluated multiple LLMs on their performance, focusing on accuracy and types of errors made.", "result": "LLMs show a significant drop in accuracy for complex LaTeX tasks, with open-source models competing strongly against closed-source ones; prevalent errors include formatting and package issues due to insufficient training examples.", "conclusion": "The findings indicate the necessity for more diverse training datasets for LLMs and demonstrate the utility of TeXpert in evaluating LaTeX generation capabilities.", "key_contributions": ["Introduction of TeXpert benchmark for LaTeX generation", "In-depth analysis of LLM performance on LaTeX tasks", "Identification of common error types in LLM-generated LaTeX"], "limitations": "The study may not cover all LLMs available and focuses primarily on LaTeX generation without exploring broader applications of LLMs.", "keywords": ["LaTeX", "Large Language Models", "benchmarking", "natural language processing", "scientific documentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17001", "pdf": "https://arxiv.org/pdf/2506.17001.pdf", "abs": "https://arxiv.org/abs/2506.17001", "title": "PersonalAI: Towards digital twins in the graph form", "authors": ["Mikhail Menschikov", "Dmitry Evseev", "Ruslan Kostoev", "Ilya Perepechkin", "Ilnaz Salimov", "Victoria Dochkina", "Petr Anokhin", "Evgeny Burnaev", "Nikita Semenov"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies.", "AI": {"tldr": "This paper addresses the personalization of language models by incorporating external memory through knowledge graphs, improving user interactions based on individual history.", "motivation": "The personalization of language models is crucial for enhancing user interactions, especially in retaining extensive personal information to generate relevant responses.", "method": "The paper proposes a new architecture that uses knowledge graphs with standard edges and two types of hyperedges to enhance the retention and utilization of personal user data.", "result": "Experiments on benchmarks showed that the proposed graph construction method improves knowledge extraction and retains robust performance in answering questions, even with temporal modifications and contradictory statements.", "conclusion": "The introduced architecture successfully supports personalized language model interactions by maintaining temporal dependencies and accommodating complex user histories.", "key_contributions": ["Utilization of knowledge graphs for user history integration in LLMs", "Introduction of a combined graph with standard edges and hyperedges", "Improvement of benchmarks through temporal and contradictory parameter incorporation"], "limitations": "", "keywords": ["language models", "personalization", "knowledge graphs", "hyperedges", "temporal dependencies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17006", "pdf": "https://arxiv.org/pdf/2506.17006.pdf", "abs": "https://arxiv.org/abs/2506.17006", "title": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Kenneth R. Koedinger"], "categories": ["cs.CL", "cs.CY"], "comment": "Full research paper accepted at EC-TEL '25", "summary": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility.", "AI": {"tldr": "The study investigates the impact of LLM-generated feedback on learning outcomes in tutor training scenarios, revealing moderate learning benefits and high learner satisfaction.", "motivation": "This study explores the effectiveness of LLM-generated explanatory feedback in learning environments, which is less understood compared to traditional feedback methods.", "method": "The study analyzes over 2,600 lesson completions from 885 learners, comparing posttest performance among groups receiving LLM feedback, traditional feedback, or neither, while addressing selection bias through propensity scoring.", "result": "Learners who were more likely to engage with LLM feedback scored higher at posttest, with two lessons showing moderate learning benefits from LLM feedback (effect sizes of 0.28 and 0.33).", "conclusion": "LLM feedback has potential as a scalable tool for improving learning in open-ended tasks, without significantly increasing completion time, and is well-received by learners.", "key_contributions": ["Demonstrated moderate learning benefits from LLM feedback in tutor training scenarios.", "Provided open datasets, LLM prompts, and rubrics for reproducibility.", "Addressed the impact of user engagement on feedback effectiveness."], "limitations": "The study's results may be influenced by selection bias despite accounting for it through propensity scoring.", "keywords": ["Large Language Models", "Feedback", "Learning Outcomes", "Machine Learning", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17019", "pdf": "https://arxiv.org/pdf/2506.17019.pdf", "abs": "https://arxiv.org/abs/2506.17019", "title": "Instituto de Telecomunica√ß√µes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning", "authors": ["Giuseppe Attanasio", "Sonal Sannigrahi", "Ben Peters", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 1 figure, IWSLT 2025", "summary": "This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on\nInstruction Following Speech Processing. We submit results for the Short Track,\ni.e., speech recognition, translation, and spoken question answering. Our model\nis a unified speech-to-text model that integrates a pre-trained continuous\nspeech encoder and text decoder through a first phase of modality alignment and\na second phase of instruction fine-tuning. Crucially, we focus on using\nsmall-scale language model backbones (< 2B) and restrict to high-quality, CC-BY\ndata along with synthetic data generation to supplement existing resources.", "AI": {"tldr": "This paper discusses a unified speech-to-text model for speech recognition, translation, and spoken question answering in the IT-IST submission for the IWSLT 2025 Shared Task.", "motivation": "To address the challenges of instruction following in speech processing through effective speech recognition, translation, and question answering technologies.", "method": "The model is built on a unified framework that aligns a pre-trained continuous speech encoder with a text decoder. It includes two phases: modality alignment and instruction fine-tuning, utilizing small-scale language model backbones and high-quality datasets.", "result": "The submission reports results from the Short Track of the shared task, demonstrating the model's capabilities in performing speech recognition, translation, and spoken question answering.", "conclusion": "The integration of modality alignment and fine-tuning with smaller language models offers a promising approach to improve instruction-following speech processing tasks.", "key_contributions": ["Unified speech-to-text model integrating encoder and decoder", "Focus on small-scale language model backbones", "Use of high-quality and synthetic data for training"], "limitations": "", "keywords": ["speech recognition", "translation", "spoken question answering", "instruction following", "language models"], "importance_score": 5, "read_time_minutes": 7}}
{"id": "2506.17046", "pdf": "https://arxiv.org/pdf/2506.17046.pdf", "abs": "https://arxiv.org/abs/2506.17046", "title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models", "authors": ["Xiaolong Wang", "Zhaolu Kang", "Wangyuxuan Zhai", "Xinyue Lou", "Yunghwei Lai", "Ziyue Wang", "Yawen Wang", "Kaiyu Huang", "Yile Wang", "Peng Li", "Yang Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances across numerous vision-language tasks. Due to their strong image-text\nalignment capability, MLLMs can effectively understand image-text pairs with\nclear meanings. However, effectively resolving the inherent ambiguities in\nnatural language and visual contexts remains challenging. Existing multimodal\nbenchmarks typically overlook linguistic and visual ambiguities, relying mainly\non unimodal context for disambiguation and thus failing to exploit the mutual\nclarification potential between modalities. To bridge this gap, we introduce\nMUCAR, a novel and challenging benchmark designed explicitly for evaluating\nmultimodal ambiguity resolution across multilingual and cross-modal scenarios.\nMUCAR includes: (1) a multilingual dataset where ambiguous textual expressions\nare uniquely resolved by corresponding visual contexts, and (2) a\ndual-ambiguity dataset that systematically pairs ambiguous images with\nambiguous textual contexts, with each combination carefully constructed to\nyield a single, clear interpretation through mutual disambiguation. Extensive\nevaluations involving 19 state-of-the-art multimodal models--encompassing both\nopen-source and proprietary architectures--reveal substantial gaps compared to\nhuman-level performance, highlighting the need for future research into more\nsophisticated cross-modal ambiguity comprehension methods, further pushing the\nboundaries of multimodal reasoning.", "AI": {"tldr": "Introduction of MUCAR, a benchmark for evaluating multimodal ambiguity resolution in MLLMs.", "motivation": "Current multimodal benchmarks fail to adequately address linguistic and visual ambiguities, which limits the ability of models to resolve ambiguities effectively using multimodal contexts.", "method": "MUCAR is created with two datasets: a multilingual dataset resolving ambiguous text expressions with visual contexts, and a dual-ambiguity dataset pairing ambiguous images with ambiguous texts to enforce mutual disambiguation.", "result": "Extensive evaluations on 19 state-of-the-art multimodal models show significant performance gaps compared to human-level understanding, indicating an urgent need for advancements in cross-modal ambiguity resolution techniques.", "conclusion": "MUCAR demonstrates the necessity for more advanced methods in understanding multimodal ambiguities, reinforcing the importance of mutual clarification between language and images.", "key_contributions": ["Introduction of MUCAR benchmark for multimodal ambiguity resolution", "Creation of multilingual and dual-ambiguity datasets", "Evaluation of state-of-the-art models highlighting performance gaps"], "limitations": "Existing models still struggle with ambiguity resolution compared to human-level performance.", "keywords": ["Multimodal Large Language Models", "ambiguity resolution", "multilingual dataset", "cross-modal scenarios", "mutual clarification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17077", "pdf": "https://arxiv.org/pdf/2506.17077.pdf", "abs": "https://arxiv.org/abs/2506.17077", "title": "Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025", "authors": ["Dominik Mach√°ƒçek", "Peter Pol√°k"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency.", "AI": {"tldr": "This paper presents a submission to the IWSLT 2025 for Simultaneous Speech Translation using the Whisper model and AlignAtt for improved translation accuracy across multiple language pairs.", "motivation": "To enhance simultaneous translation performance using advanced speech models and innovative methodologies for various language pairs.", "method": "Utilizes the Whisper model in conjunction with AlignAtt for simultaneous translation and transcription. Applies in-domain terminology prompting and context accommodation alongside EuroLLM for cascaded systems.", "result": "Achieved improvements of 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese, and Japanese on the development sets compared to the baseline.", "conclusion": "The proposed methods demonstrate significant improvements in simultaneous translation and introduce a new enhanced measure for speech recognition latency.", "key_contributions": ["Implementation of the Whisper model for simultaneous translation.", "Introduction of a new measure for speech recognition latency.", "Significant BLEU score improvements over baseline performance."], "limitations": "", "keywords": ["simultaneous translation", "speech model", "Whisper", "IWSLT 2025", "BLEU points"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.17080", "pdf": "https://arxiv.org/pdf/2506.17080.pdf", "abs": "https://arxiv.org/abs/2506.17080", "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs", "authors": ["Ricardo Rei", "Nuno M. Guerreiro", "Jos√© Pombal", "Jo√£o Alves", "Pedro Teixeirinha", "Amin Farajian", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization.", "AI": {"tldr": "This paper introduces Tower+, a suite of models optimized for both translation and multilingual general-purpose tasks, achieving a balance between specialization and broad capabilities through a novel training process.", "motivation": "The need for LLMs to excel in both specific tasks like translation and broader tasks like instruction-following without losing general utility is essential for real-world applications.", "method": "The authors propose a training regime that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards, using curated data to enhance performance.", "result": "Tower+ models, particularly the largest 72B variant, excel in both translation tasks and general instruction-following, outperforming existing state-of-the-art models in several benchmarks, including IF-MT and multilingual Arena Hard evaluations.", "conclusion": "The research demonstrates that with the right methodology, it is feasible to develop models that maintain both strong general-purpose capabilities and specialized task performance, indicating potential for business applications in translation and localization.", "key_contributions": ["Introduction of the Tower+ model suite for LLMs", "Demonstration of a training process achieving a Pareto frontier", "Introduction of the IF-MT benchmark for evaluating translation and instruction-following"], "limitations": "", "keywords": ["Fine-tuning", "LLMs", "Translation", "General-purpose capabilities", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.17088", "pdf": "https://arxiv.org/pdf/2506.17088.pdf", "abs": "https://arxiv.org/abs/2506.17088", "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "authors": ["Jiahao Cheng", "Tiancheng Su", "Jia Yuan", "Guoxiu He", "Jiawei Liu", "Xinqi Tao", "Jingwen Xie", "Huaxia Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.", "AI": {"tldr": "This paper investigates the effect of Chain-of-Thought prompting on the detection of hallucinations in Large Language Models, revealing a trade-off between reduced hallucination frequency and detection efficacy.", "motivation": "To explore how Chain-of-Thought prompting affects hallucination detection in Large Language Models, addressing existing gaps in the research.", "method": "Conducted a systematic empirical evaluation, starting with a pilot experiment, followed by an assessment of various CoT prompting methods on hallucination detection methods for both instruction-tuned and reasoning-oriented LLMs.", "result": "CoT prompting reduces hallucination frequency but obscures critical signals used in detection, leading to decreased efficacy of detection methods.", "conclusion": "The study reveals a critical trade-off in using reasoning strategies to mitigate hallucinations in LLMs, impacting detection performance.", "key_contributions": ["Empirical evaluation of CoT prompting effects on hallucination detection", "Insights into the impact of CoT prompting on LLMs' internal states", "Identification of trade-offs in hallucination mitigation approaches"], "limitations": "The study focuses on specific models and may not generalize across all LLM architectures or applications.", "keywords": ["Large Language Models", "hallucinations", "Chain-of-Thought prompting", "hallucination detection", "empirical evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17090", "pdf": "https://arxiv.org/pdf/2506.17090.pdf", "abs": "https://arxiv.org/abs/2506.17090", "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions", "authors": ["Murtaza Nazir", "Matthew Finlayson", "John X. Morris", "Xiang Ren", "Swabha Swayamdipta"], "categories": ["cs.CL"], "comment": null, "summary": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known.", "AI": {"tldr": "The paper presents a novel method called PILS for recovering hidden prompts from language model outputs, showing significant improvements in recovery rates.", "motivation": "To address security concerns related to hidden prompts and potentially exposed private information from language models.", "method": "The proposed approach, PROMPT INVERSION FROM LOGPROB SEQUENCES (PILS), utilizes next-token probabilities over multiple generation steps to recover hidden prompts effectively by compressing output information using a linear map.", "result": "PILS achieves 2--3.5 times higher exact recovery rates than state-of-the-art methods, with notable improvements in prompt recovery, especially under varying generation step conditions.", "conclusion": "The method highlights next-token probabilities as a critical vulnerability for inversion attacks, demonstrating stronger recovery rates and generalization to previously unseen conditions.", "key_contributions": ["Development of a new method for recovering hidden prompts using next-token probabilities", "Demonstration of improved exact recovery rates", "Strong performance on recovering hidden system messages"], "limitations": "Potential limitations not discussed in detail.", "keywords": ["language model inversion", "prompt recovery", "next-token probabilities", "machine learning", "security"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.17121", "pdf": "https://arxiv.org/pdf/2506.17121.pdf", "abs": "https://arxiv.org/abs/2506.17121", "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?", "authors": ["Adithya Bhaskar", "Alexander Wettig", "Tianyu Gao", "Yihe Dong", "Danqi Chen"], "categories": ["cs.CL"], "comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong", "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.", "AI": {"tldr": "This paper introduces the KV footprint as a metric for evaluating key-value (KV) cache management in language models, proposing methods that lower memory costs while maintaining performance for long-context tasks.", "motivation": "Current methods for managing KV caches in language models do not adequately consider memory efficiency, often leading to high peak memory usage and performance degradation.", "method": "The authors propose the KV footprint metric to evaluate cache management methods. They adapt existing post-fill eviction methods for pre-filling and introduce PruLong, an optimization approach for recency eviction methods.", "result": "PruLong achieves a 12% reduction in KV footprint compared to prior methods while maintaining performance in long-context tasks, revealing inefficiencies in existing approaches.", "conclusion": "The proposed methods improve management of KV caches in language models, contributing to the efficiency of long-context inference.", "key_contributions": ["Introduction of the KV footprint metric for evaluating KV cache methods", "Adaptation of post-fill eviction methods for pre-filling", "Development of PruLong for optimizing recency eviction"], "limitations": "", "keywords": ["KV footprint", "language models", "memory efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.17180", "pdf": "https://arxiv.org/pdf/2506.17180.pdf", "abs": "https://arxiv.org/abs/2506.17180", "title": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models", "authors": ["Naiming Liu", "Richard Baraniuk", "Shashank Sonkar"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions\ndesigned to evaluate whether language models can determine if one statement\ncausally explains another. Each question present an assertion-reason pair and\nchallenge language models to distinguish between semantic relatedness and\ngenuine causal explanatory relationships. Through comprehensive evaluation of\n21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we\nidentify two fundamental findings. First, language models frequently confuse\nsemantic similarity with causality, relying on lexical and semantic overlap\ninstead of inferring actual causal explanatory relationships. Second, as\nparameter size increases, models tend to shift from being overly skeptical\nabout causal relationships to being excessively permissive in accepting them.\nDespite this shift, performance measured by the Matthews Correlation\nCoefficient plateaus at just 0.55, even for the best-performing models.Hence,\nCLEAR-3K provides a crucial benchmark for developing and evaluating genuine\ncausal reasoning in language models, which is an essential capability for\napplications that require accurate assessment of causal relationships.", "AI": {"tldr": "CLEAR-3K is a dataset of 3,000 assertion-reasoning questions that evaluates language models' ability to determine causal relationships between statements.", "motivation": "To assess whether language models can distinguish between semantic similarity and genuine causal explanatory relationships.", "method": "Evaluation of 21 state-of-the-art language models with varied parameter sizes on tasks involving assertion-reason pairs.", "result": "Models confuse semantic similarity with causality and show a performance plateau at a Matthews Correlation Coefficient of 0.55, even among the largest models.", "conclusion": "CLEAR-3K is a vital benchmark for advancing the understanding of causal reasoning in AI language models.", "key_contributions": ["Introduction of a dataset for evaluating causal reasoning in language models", "Identification of key performance limitations of current models", "Providing a benchmark for future model improvements in causal reasoning"], "limitations": "Models still show a limited ability to ascertain true causal relationships, with a plateau in performance metrics.", "keywords": ["causal reasoning", "language models", "dataset", "assertion-reason pairs", "benchmark"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.17188", "pdf": "https://arxiv.org/pdf/2506.17188.pdf", "abs": "https://arxiv.org/abs/2506.17188", "title": "Towards AI Search Paradigm", "authors": ["Yuchen Li", "Hengyi Cai", "Rui Kong", "Xinran Chen", "Jiamin Chen", "Jun Yang", "Haojie Zhang", "Jiayi Li", "Jiayi Wu", "Yiqun Chen", "Changle Qu", "Keyi Kong", "Wenwen Ye", "Lixin Su", "Xinyu Ma", "Long Xia", "Daiting Shi", "Jiashu Zhao", "Haoyi Xiong", "Shuaiqiang Wang", "Dawei Yin"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems.", "AI": {"tldr": "This paper introduces the AI Search Paradigm, a detailed framework for next-gen search systems that simulates human decision-making using LLM-powered agents.", "motivation": "To create a comprehensive blueprint for search systems that can handle diverse information needs with human-like processing.", "method": "The paper presents a modular architecture consisting of four LLM agents (Master, Planner, Executor, Writer) that work collaboratively on tasks of varying complexity through coordinated workflows.", "result": "The proposed methodology improves task planning, tool integration, and retrieval-augmented generation, optimizing LLM inference and search system performance.", "conclusion": "The AI Search Paradigm provides foundational components essential for developing adaptive and scalable AI search systems.", "key_contributions": ["Introduction of a modular architecture for AI search systems", "Detailed methodologies for task planning and tool integration", "Strategies for robust retrieval-augmented generation and LLM inference optimization"], "limitations": "", "keywords": ["AI Search Paradigm", "LLM agents", "retrieval-augmented generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17209", "pdf": "https://arxiv.org/pdf/2506.17209.pdf", "abs": "https://arxiv.org/abs/2506.17209", "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency", "authors": ["Kathleen C. Fraser", "Hillary Dawkins", "Isar Nejadgholi", "Svetlana Kiritchenko"], "categories": ["cs.CL"], "comment": "to appear at LLMSEC 2025", "summary": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future.", "AI": {"tldr": "This paper discusses the critical failure mode of fine-tuning large language models (LLMs) leading to reduced safety alignment, highlighting the need for reliable safety evaluations amidst variability in experimental setups.", "motivation": "The motivation behind this work is to address the vulnerabilities created when fine-tuning LLMs, which can lead to unintentional removal of safety features that protect against harmful outputs.", "method": "The authors investigate the robustness of a safety benchmark against minor experimental variations and the inherent stochasticity of LLMs, conducting initial experiments to gauge result variance.", "result": "The initial experiments reveal significant variance in safety evaluation results even with minor changes to the fine-tuning procedure, suggesting challenges in obtaining consistent safety benchmarks.", "conclusion": "The findings have important implications for researchers, stressing the need for more accurate reporting of evaluation results to facilitate meaningful comparisons and improve safety in fine-tuned models.", "key_contributions": ["Identified critical safety vulnerabilities in fine-tuned LLMs due to unintentional misalignment.", "Highlighted the variability in safety evaluations based on experimental setups.", "Proposed the necessity for standardized reporting in safety benchmark experiments."], "limitations": "The study may not account for all potential variables affecting safety evaluations; further research is needed to encompass a broader range of scenarios.", "keywords": ["Large Language Models", "Fine-tuning", "Safety Evaluation", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.16473", "pdf": "https://arxiv.org/pdf/2506.16473.pdf", "abs": "https://arxiv.org/abs/2506.16473", "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions.", "AI": {"tldr": "This study compares the similarities between the concerns shared in human therapy sessions and those with a conversational robot, QTrobot, highlighting significant thematic and semantic alignments.", "motivation": "Understanding how robot interactions in emotionally supportive dialogue compare to traditional therapy settings to assess the potential of conversational agents in mental health support.", "method": "Analyzed interactions from two datasets: professional therapist conversations and supportive robot dialogues, using sentence embeddings and K-means clustering for thematic alignment analysis.", "result": "90.88% of robot conversation disclosures mapped to thematic clusters from human therapy, indicating significant topical alignment and strong semantic overlap in responses to similar disclosures.", "conclusion": "Robot-led support conversations show considerable parallels to human therapy, suggesting potential enhancements in mental health interventions through conversational agents.", "key_contributions": ["Demonstrated thematic alignment between human and robot therapy conversations.", "Mapped robot disclosures to human therapist response clusters, indicating shared concerns.", "Analyzed semantic similarities using advanced embedding techniques."], "limitations": "", "keywords": ["Conversational Agents", "Mental Health", "Thematic Analysis", "Machine Learning", "Human-Robot Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2305.14597", "pdf": "https://arxiv.org/pdf/2305.14597.pdf", "abs": "https://arxiv.org/abs/2305.14597", "title": "Voices of Her: Analyzing Gender Differences in the AI Publication World", "authors": ["Yiwen Ding", "Jiarui Liu", "Zhiheng Lyu", "Kun Zhang", "Bernhard Schoelkopf", "Zhijing Jin", "Rada Mihalcea"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While several previous studies have analyzed gender bias in research, we are\nstill missing a comprehensive analysis of gender differences in the AI\ncommunity, covering diverse topics and different development trends. Using the\nAI Scholar dataset of 78K researchers in the field of AI, we identify several\ngender differences: (1) Although female researchers tend to have fewer overall\ncitations than males, this citation difference does not hold for all\nacademic-age groups; (2) There exist large gender homophily in co-authorship on\nAI papers; (3) Female first-authored papers show distinct linguistic styles,\nsuch as longer text, more positive emotion words, and more catchy titles than\nmale first-authored papers. Our analysis provides a window into the current\ndemographic trends in our AI community, and encourages more gender equality and\ndiversity in the future. Our code and data are at\nhttps://github.com/causalNLP/ai-scholar-gender.", "AI": {"tldr": "This study analyzes gender differences in the AI community, highlighting discrepancies in citations, co-authorship, and linguistic styles in papers authored by different genders.", "motivation": "To provide a comprehensive analysis of gender bias in the AI community, exploring trends and differences across various demographics.", "method": "Utilized the AI Scholar dataset, examining 78,000 researchers to analyze citation patterns, co-authorship homophily, and linguistic styles in papers based on gender.", "result": "Identified that female researchers have fewer citations overall but this varies by academic age; noted significant gender homophily in co-authorship; highlighted distinct linguistic styles in papers authored by females.", "conclusion": "The findings reveal ongoing issues of gender disparity within the AI community and advocate for increased gender diversity and equality.", "key_contributions": ["Comprehensive analysis of gender bias in AI research", "Insightful data on citation metrics and academic age groups", "Unique linguistic contrasts in papers based on authors' gender"], "limitations": "The analysis is limited to the AI context and may not fully represent other research fields or account for intersectional factors.", "keywords": ["gender bias", "AI community", "co-authorship", "linguistic styles", "citation analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2312.04684", "pdf": "https://arxiv.org/pdf/2312.04684.pdf", "abs": "https://arxiv.org/abs/2312.04684", "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "authors": ["Zifan Xu", "Haozhu Wang", "Dmitriy Bespalov", "Xian Wu", "Peter Stone", "Yanjun Qi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.", "AI": {"tldr": "This paper presents Latent Reasoning Skills (LaRS), an unsupervised learning approach that enhances in-context learning by creating latent space representations of rationales, improving the efficiency of example selection for large language models.", "motivation": "To address the challenges of traditional in-context learning approaches that rely on example selection based on questions rather than the necessary reasoning processes, which are costly and difficult to scale.", "method": "LaRS employs unsupervised learning to represent rationales in a latent space with a reasoning skill variable, alongside a reasoning policy that determines the skill required for a question, facilitating more efficient selection of ICL examples.", "result": "LaRS outperforms existing skill-based selection methods in terms of speed (processing examples four times faster) and reduces the number of LLM inferences needed during selection by half while showing robustness to less optimal example banks.", "conclusion": "The introduction of LaRS represents a significant improvement in the efficiency and scalability of ICL approaches for LLMs by eliminating reliance on auxiliary LLM inference or manual prompt design, supported by strong empirical results.", "key_contributions": ["Introduction of Latent Reasoning Skills (LaRS) for better efficiency in ICL", "Elimination of the need for manual prompt design", "Improved robustness in example selection for reasoning tasks."], "limitations": "", "keywords": ["Latent Reasoning Skills", "in-context learning", "large language models", "unsupervised learning", "example selection"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2402.09404", "pdf": "https://arxiv.org/pdf/2402.09404.pdf", "abs": "https://arxiv.org/abs/2402.09404", "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability", "authors": ["Siwei Yang", "Bingchen Zhao", "Cihang Xie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.", "AI": {"tldr": "Introduction of AQA-Bench, a benchmark for assessing sequential reasoning in LLMs using algorithms like DFS.", "motivation": "To evaluate the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts and improve their performance in interactive environments.", "method": "Development of AQA-Bench, an interactive benchmark incorporating three algorithms: binary search, depth-first search, and breadth-first search, evaluating 14 different LLMs.", "result": "Closed-source models outperform open-source ones in sequential reasoning; providing in-context examples can reduce performance for some models; limited predecessor steps can improve performance; weak models struggle with initial steps; performance scaling is complex and not always consistent with model size.", "conclusion": "The findings highlight significant differences in LLM capabilities and suggest directions for enhancing sequential reasoning in future research.", "key_contributions": ["Development of AQA-Bench for LLM evaluation", "Identification of performance differences between closed-source and open-source LLMs", "Insights into how in-context examples affect model performance"], "limitations": "The study is limited to the selected algorithms and models, and the findings may not generalize to all LLMs or algorithm types.", "keywords": ["Large Language Models", "Sequential Reasoning", "AQA-Bench"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2404.12041", "pdf": "https://arxiv.org/pdf/2404.12041.pdf", "abs": "https://arxiv.org/abs/2404.12041", "title": "A Survey of Automatic Hallucination Evaluation on Natural Language Generation", "authors": ["Siya Qi", "Lin Gui", "Yulan He", "Zheng Yuan"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages", "summary": "The proliferation of Large Language Models (LLMs) has introduced a critical\nchallenge: accurate hallucination evaluation that ensures model reliability.\nWhile Automatic Hallucination Evaluation (AHE) has emerged as essential, the\nfield suffers from methodological fragmentation, hindering both theoretical\nunderstanding and practical advancement. This survey addresses this critical\ngap through a comprehensive analysis of 74 evaluation methods, revealing that\n74% specifically target LLMs, a paradigm shift that demands new evaluation\nframeworks. We formulate a unified evaluation pipeline encompassing datasets\nand benchmarks, evidence collection strategies, and comparison mechanisms,\nsystematically documenting the evolution from pre-LLM to post-LLM\nmethodologies. Beyond taxonomical organization, we identify fundamental\nlimitations in current approaches and their implications for real-world\ndeployment. To guide future research, we delineate key challenges and propose\nstrategic directions, including enhanced interpretability mechanisms and\nintegration of application-specific evaluation criteria, ultimately providing a\nroadmap for developing more robust and practical hallucination evaluation\nsystems.", "AI": {"tldr": "This survey analyzes 74 Automatic Hallucination Evaluation (AHE) methods for Large Language Models (LLMs), identifying gaps and proposing a unified evaluation framework.", "motivation": "To address the critical need for reliable evaluation of hallucinations in LLMs due to methodological fragmentation in the current evaluation landscape.", "method": "A comprehensive analysis of 74 evaluation methods specific to LLMs, including the formulation of a unified evaluation pipeline with datasets, benchmarks, evidence collection strategies, and comparison mechanisms.", "result": "The survey reveals that 74% of the methods target LLMs, highlighting a paradigm shift requiring new evaluation frameworks, and documents the evolution of evaluation methodologies.", "conclusion": "The paper provides a roadmap for improving hallucination evaluation systems, suggesting enhanced interpretability and application-specific criteria as key areas for future research.", "key_contributions": ["Comprehensive analysis of 74 AHE methods for LLMs", "Unified evaluation pipeline formulation", "Identification of key challenges and strategic directions for future research"], "limitations": "Current evaluation methods have fundamental limitations affecting their real-world deployment.", "keywords": ["Large Language Models", "Hallucination Evaluation", "Evaluation Frameworks", "Machine Learning", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2406.04220", "pdf": "https://arxiv.org/pdf/2406.04220.pdf", "abs": "https://arxiv.org/abs/2406.04220", "title": "BEADs: Bias Evaluation Across Domains", "authors": ["Shaina Raza", "Mizanur Rahman", "Michael R. Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved natural language processing (NLP) applications. However, these models\noften inherit biases from their training data. While several datasets exist for\nbias detection, most are limited to one or two NLP tasks, typically\nclassification or evaluation, and lack comprehensive coverage across a broader\nrange of tasks. To address this gap, we introduce the Bias Evaluations Across\nDomains (BEADs) dataset, designed to support a wide range of NLP tasks,\nincluding text classification, token classification, bias quantification, and\nbenign language generation. A key contribution of this work is the\ngold-standard annotation provided by GPT-4 for scalability, with expert\nverification to ensure high reliability. BEADs can be used for both fine-tuning\nmodels (for classification and generation tasks) and evaluating LLM behavior.\nOur findings show that BEADs effectively surfaces various biases during model\nfine-tuning and helps reduce biases in language generation tasks while\nmaintaining output quality. The dataset also highlights prevalent demographic\nbiases in LLMs during evaluation. We release BEADs as a practical resource for\ndetecting and mitigating bias across domains, supporting the development of\nresponsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data:\nhttps://huggingface.co/datasets/shainar/BEAD", "AI": {"tldr": "Introduction of the BEADs dataset to address bias in LLMs across multiple NLP tasks.", "motivation": "To provide a comprehensive dataset for bias detection that encompasses various NLP tasks, moving beyond the limitations of existing datasets.", "method": "Developed BEADs dataset with gold-standard annotations from GPT-4, validated by experts, to cover tasks like classification, token classification, and bias quantification.", "result": "BEADs surfaced various biases in models during fine-tuning and helped mitigate them in language generation tasks without compromising output quality.", "conclusion": "BEADs serves as a valuable resource for identifying and reducing bias in NLP models, promoting the creation of responsible AI.", "key_contributions": ["Introduction of a diverse dataset for bias detection", "Gold-standard annotations by GPT-4", "Support for a wide range of NLP tasks"], "limitations": "", "keywords": ["bias detection", "large language models", "natural language processing", "dataset", "AI ethics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.02397", "pdf": "https://arxiv.org/pdf/2407.02397.pdf", "abs": "https://arxiv.org/abs/2407.02397", "title": "Learning to Refine with Fine-Grained Natural Language Feedback", "authors": ["Manya Wadhwa", "Xinyu Zhao", "Junyi Jessy Li", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Code and models available at: https://github.com/ManyaWadhwa/DCR;\n  Findings of EMNLP 2024", "summary": "Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1) detection of\nbad generations; (2) fine-grained natural language critique generation; (3)\nrefining with fine-grained feedback. The first step can be implemented with a\nhigh-performing discriminative model and steps 2 and 3 can be implemented\neither via prompted or fine-tuned LLMs. A key property of the proposed Detect,\nCritique, Refine (\"DCR\") method is that the step 2 critique model can give\nfine-grained feedback about errors, made possible by offloading the\ndiscrimination to a separate model in step 1. We show that models of different\ncapabilities benefit from refining with DCR on the task of improving factual\nconsistency of document grounded summaries. Overall, DCR consistently\noutperforms existing end-to-end refinement approaches and current trained\nmodels not fine-tuned for factuality critiquing.", "AI": {"tldr": "The paper proposes a novel method called Detect, Critique, Refine (DCR) for enhancing the factual consistency of document-grounded summaries generated by LLMs.", "motivation": "To improve the refinement processes in LLM-generated responses by evaluating effective feedback mechanisms instead of just model sizes.", "method": "The DCR method involves three steps: detection of poor generations using a discriminative model, generating fine-grained critiques via a critique model, and refining responses with that feedback, leveraging prompted or fine-tuned LLMs.", "result": "DCR outperforms existing approaches in refining factual consistency of summaries, demonstrating that models with varying capabilities benefit from this structured feedback process.", "conclusion": "The ability to fine-tune critiques separately from the generation process enhances LLM performance, suggesting a better way to improve response quality in LLMs.", "key_contributions": ["Introduction of the Detect, Critique, Refine method for LLM refinement.", "Separation of critique generation and detection to improve feedback quality.", "Empirical evidence showing DCR's superior performance in factual consistency improvements."], "limitations": "Potential dependence on the quality of the initial discriminative model for detection phases.", "keywords": ["Large Language Models", "Feedback Mechanisms", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.09879", "pdf": "https://arxiv.org/pdf/2407.09879.pdf", "abs": "https://arxiv.org/abs/2407.09879", "title": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting", "authors": ["Sanchit Ahuja", "Kumar Tanmay", "Hardik Hansrajbhai Chauhan", "Barun Patra", "Kriti Aggarwal", "Luciano Del Corro", "Arindam Mitra", "Tejas Indulal Dhamecha", "Ahmed Awadallah", "Monojit Choudhary", "Vishrav Chaudhary", "Sunayana Sitaram"], "categories": ["cs.CL"], "comment": "20 pages, 12 tables, 5 figures", "summary": "Despite the remarkable success of large language models (LLMs) in English, a\nsignificant performance gap remains in non-English languages. To address this,\nwe introduce a novel approach for strategically constructing a multilingual\nsynthetic instruction tuning dataset, sPhinX. Unlike prior methods that\ndirectly translate fixed instruction-response pairs, sPhinX enhances diversity\nby selectively augmenting English instruction-response pairs with multilingual\ntranslations. Additionally, we propose LANGIT, a novel N-shot guided\nfine-tuning strategy, which further enhances model performance by incorporating\ncontextually relevant examples in each training sample. Our ablation study\nshows that our approach enhances the multilingual capabilities of Mistral-7B\nand Phi-3-Small improving performance by an average of 39.8% and 11.2%,\nrespectively, across multilingual benchmarks in reasoning, question answering,\nreading comprehension, and machine translation. Moreover, sPhinX maintains\nstrong performance on English LLM benchmarks while exhibiting minimal to no\ncatastrophic forgetting, even when trained on 51 languages.", "AI": {"tldr": "This paper introduces sPhinX, a multilingual synthetic instruction tuning dataset, to improve large language models' performance in non-English languages and presents LANGIT, an N-shot guided fine-tuning strategy.", "motivation": "To address the significant performance gap of large language models (LLMs) in non-English languages compared to English.", "method": "The paper presents sPhinX, a method for constructing a diverse multilingual synthetic instruction tuning dataset by augmenting English instruction-response pairs with multilingual translations and proposes LANGIT, an N-shot guided fine-tuning strategy for further model enhancement.", "result": "The approach demonstrates a 39.8% improvement in Mistral-7B and an 11.2% improvement in Phi-3-Small across various multilingual benchmarks, while maintaining strong English performance without catastrophic forgetting.", "conclusion": "sPhinX effectively enhances multilingual capabilities of LLMs, showing significant performance gains in reasoning, question answering, reading comprehension, and machine translation across 51 languages.", "key_contributions": ["Introduction of the sPhinX dataset for multilingual instruction tuning", "Development of LANGIT, a novel N-shot guided fine-tuning strategy", "Demonstrated performance improvements on multilingual benchmarks"], "limitations": "", "keywords": ["multilingual", "large language models", "instruction tuning", "fine-tuning", "synthetic datasets"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2407.14701", "pdf": "https://arxiv.org/pdf/2407.14701.pdf", "abs": "https://arxiv.org/abs/2407.14701", "title": "Contextual modulation of language comprehension in a dynamic neural model of lexical meaning", "authors": ["Michael C. Stern", "Maria M. Pi√±ango"], "categories": ["cs.CL"], "comment": null, "summary": "We propose and computationally implement a dynamic neural model of lexical\nmeaning, and experimentally test its behavioral predictions. We demonstrate the\narchitecture and behavior of the model using as a test case the English lexical\nitem 'have', focusing on its polysemous use. In the model, 'have' maps to a\nsemantic space defined by two continuous conceptual dimensions, connectedness\nand control asymmetry, previously proposed to parameterize the conceptual\nsystem for language. The mapping is modeled as coupling between a neural node\nrepresenting the lexical item and neural fields representing the conceptual\ndimensions. While lexical knowledge is modeled as a stable coupling pattern,\nreal-time lexical meaning retrieval is modeled as the motion of neural\nactivation patterns between metastable states corresponding to semantic\ninterpretations or readings. Model simulations capture two previously reported\nempirical observations: (1) contextual modulation of lexical semantic\ninterpretation, and (2) individual variation in the magnitude of this\nmodulation. Simulations also generate a novel prediction that the by-trial\nrelationship between sentence reading time and acceptability should be\ncontextually modulated. An experiment combining self-paced reading and\nacceptability judgments replicates previous results and confirms the new model\nprediction. Altogether, results support a novel perspective on lexical\npolysemy: that the many related meanings of a word are metastable neural\nactivation states that arise from the nonlinear dynamics of neural populations\ngoverning interpretation on continuous semantic dimensions.", "AI": {"tldr": "This paper presents a dynamic neural model of lexical meaning, focusing on the polysemous use of the English word 'have'.", "motivation": "To understand how lexical meaning is retrieved and how it is influenced by context and individual differences.", "method": "Development and simulation of a neural model that maps lexical items to a semantic space defined by concepts of connectedness and control asymmetry.", "result": "The model captures contextual modulation of lexical meanings and individual variations in interpretation, supporting statistically derived predictions about reading times.", "conclusion": "The findings suggest that lexical meanings are dynamic states influenced by the neural architecture of semantic interpretation, challenging traditional views of polysemy.", "key_contributions": ["Introduced a dynamic neural model for understanding lexical meaning retrieval.", "Demonstrated empirical support for the model's predictive capabilities through experimental results.", "Proposed the concept of metastable states in lexical interpretation."], "limitations": "", "keywords": ["lexical meaning", "neural model", "polysemy", "semantic dimensions", "contextual modulation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2408.01287", "pdf": "https://arxiv.org/pdf/2408.01287.pdf", "abs": "https://arxiv.org/abs/2408.01287", "title": "Deep Learning based Visually Rich Document Content Understanding: A Survey", "authors": ["Yihao Ding", "Soyeon Caren Han", "Jean Lee", "Eduard Hovy"], "categories": ["cs.CL", "cs.CV"], "comment": "Work in Progress", "summary": "Visually Rich Documents (VRDs) play a vital role in domains such as academia,\nfinance, healthcare, and marketing, as they convey information through a\ncombination of text, layout, and visual elements. Traditional approaches to\nextracting information from VRDs rely heavily on expert knowledge and manual\nannotation, making them labor-intensive and inefficient. Recent advances in\ndeep learning have transformed this landscape by enabling multimodal models\nthat integrate vision, language, and layout features through pretraining,\nsignificantly improving information extraction performance. This survey\npresents a comprehensive overview of deep learning-based frameworks for VRD\nContent Understanding (VRD-CU). We categorize existing methods based on their\nmodeling strategies and downstream tasks, and provide a comparative analysis of\nkey components, including feature representation, fusion techniques, model\narchitectures, and pretraining objectives. Additionally, we highlight the\nstrengths and limitations of each approach and discuss their suitability for\ndifferent applications. The paper concludes with a discussion of current\nchallenges and emerging trends, offering guidance for future research and\npractical deployment in real-world scenarios.", "AI": {"tldr": "A survey on deep learning-based frameworks for extracting information from visually rich documents, categorizing methods, and analyzing their effectiveness.", "motivation": "To address the inefficiencies of traditional methods in extracting information from visually rich documents using deep learning techniques.", "method": "The paper categorizes existing deep learning frameworks based on modeling strategies and downstream tasks, and includes a comparative analysis of key components such as feature representation, fusion techniques, model architectures, and pretraining objectives.", "result": "The survey reveals that multimodal models significantly enhance information extraction performance from visually rich documents, although each method has its own strengths and weaknesses depending on the application.", "conclusion": "The paper discusses current challenges in the field and proposes directions for future research and practical applications in extracting information from visually rich documents.", "key_contributions": ["Comprehensive overview of deep learning frameworks for VRD Content Understanding", "Comparative analysis of feature representation, fusion techniques, and model architectures", "Discussion of emerging trends and practical deployment challenges"], "limitations": "Some methods may not generalize well to all types of visually rich documents and practical scenarios.", "keywords": ["Visually Rich Documents", "Deep Learning", "Information Extraction", "Multimodal Models", "Content Understanding"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2408.06904", "pdf": "https://arxiv.org/pdf/2408.06904.pdf", "abs": "https://arxiv.org/abs/2408.06904", "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives", "authors": ["Zhihu Wang", "Shiwan Zhao", "Yu Wang", "Heyuan Huang", "Sitao Xie", "Yubo Zhang", "Jiaxin Shi", "Zhixing Wang", "Hongyan Li", "Junchi Yan"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; First three authors contributed equally", "summary": "The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving\ncomplex problems with large language models (LLMs). However, its application to\ndomain-specific tasks remains challenging, as LLMs often fail to decompose\ntasks accurately or execute subtasks effectively. This paper introduces the\nRe-TASK framework, a novel theoretical model that revisits LLM tasks from\ncapability, skill, and knowledge perspectives, drawing on the principles of\nBloom's Taxonomy and Knowledge Space Theory. While CoT provides a\nworkflow-centric perspective on tasks, Re-TASK introduces a Chain-of-Learning\n(CoL) paradigm that highlights task dependencies on specific capability items,\nfurther broken down into their constituent knowledge and skill components. To\naddress CoT failures, we propose a Re-TASK prompting strategy, which\nstrengthens task-relevant capabilities through targeted knowledge injection and\nskill adaptation. Experiments across diverse domains demonstrate the\neffectiveness of Re-TASK. In particular, we achieve improvements of 45.00% on\nYi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks. These results\nhighlight the potential of Re-TASK to significantly enhance LLM performance and\nits applicability in specialized domains. We release our code and data at\nhttps://github.com/Uylee/Re-TASK.", "AI": {"tldr": "This paper introduces the Re-TASK framework, which improves task decomposition and execution in large language models (LLMs) using a Chain-of-Learning paradigm, demonstrating significant performance improvements in domain-specific tasks.", "motivation": "To address challenges in applying the Chain-of-Thought paradigm to domain-specific tasks, particularly the failure of LLMs to accurately decompose tasks and execute subtasks.", "method": "The Re-TASK framework revisits tasks by analyzing capability, skill, and knowledge, using principles from Bloom's Taxonomy and Knowledge Space Theory, and introduces a Re-TASK prompting strategy for targeted knowledge and skill enhancements.", "result": "Experiments show Re-TASK improvements of 45.00% on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks, demonstrating significant effectiveness in specialized domains.", "conclusion": "The Re-TASK framework presents a promising approach to enhance LLM performance by focusing on task-relevant capabilities and skill adaptation, with implications for diverse fields.", "key_contributions": ["Introduction of the Re-TASK framework for LLM task enhancement", "Development of the Chain-of-Learning paradigm for task decomposition", "Proposed Re-TASK prompting strategy for targeted knowledge injection"], "limitations": "", "keywords": ["Chain-of-Thought", "Re-TASK framework", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.00128", "pdf": "https://arxiv.org/pdf/2409.00128.pdf", "abs": "https://arxiv.org/abs/2409.00128", "title": "Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management", "authors": ["Ziyan Cui", "Ning Li", "Huaikang Zhou"], "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "comment": "5 figures, 2 tables", "summary": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions.", "AI": {"tldr": "A study replicating psychological experiments using Large Language Models (LLMs) reveals high replication rates but also challenges in areas like socially sensitive topics.", "motivation": "To evaluate the efficacy of Large Language Models in replicating psychological experiments and their implications in social sciences.", "method": "A large-scale study replicating 156 psychological experiments from top social science journals using GPT-4, Claude 3.5 Sonnet, and DeepSeek v3.", "result": "LLMs showed high replication rates for main effects (73-81%) but produced larger effect sizes than human studies, especially struggling with socially sensitive research.", "conclusion": "The study highlights LLMs as promising tools for rapid hypothesis validation but emphasizes the need for human validation in complex social phenomena.", "key_contributions": ["High replication rates of psychological experiments by LLMs", "Significant differences in effect sizes compared to human studies", "Identification of limitations in handling socially sensitive topics."], "limitations": "LLMs showed lower replication rates for studies on sensitive topics and high rates of false positives in null findings.", "keywords": ["Artificial Intelligence", "Large Language Models", "Psychological Research", "Social Sciences", "Replication Study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.10855", "pdf": "https://arxiv.org/pdf/2410.10855.pdf", "abs": "https://arxiv.org/abs/2410.10855", "title": "Core Knowledge Deficits in Multi-Modal Language Models", "authors": ["Yijiang Li", "Qingying Gao", "Tianwei Zhao", "Bingyang Wang", "Haoran Sun", "Haiyun Lyu", "Robert D. Hawkins", "Nuno Vasconcelos", "Tal Golan", "Dezhi Luo", "Hokin Deng"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025. Project page at\n  https://williamium3000.github.io/core-knowledge and code is available at\n  https://github.com/williamium3000/core-knowledge", "summary": "While Multi-modal Large Language Models (MLLMs) demonstrate impressive\nabilities over high-level perception and reasoning, their robustness in the\nwild remains limited, often falling short on tasks that are intuitive and\neffortless for humans. We examine the hypothesis that these deficiencies stem\nfrom the absence of core knowledge--rudimentary cognitive abilities innate to\nhumans from early childhood. To explore the core knowledge representation in\nMLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core\nknowledge concepts grounded in developmental cognitive science. We evaluate 230\nmodels with 11 different prompts, leading to a total of 2,530 data points for\nanalysis. Our experiments uncover four key findings, collectively demonstrating\ncore knowledge deficits in MLLMs: they consistently underperform and show\nreduced, or even absent, scalability on low-level abilities relative to\nhigh-level ones. Finally, we propose Concept Hacking, a novel controlled\nevaluation method that reveals MLLMs fail to progress toward genuine core\nknowledge understanding, but instead rely on shortcut learning as they scale.", "AI": {"tldr": "The paper introduces CoreCognition, a benchmark for evaluating core knowledge in Multi-modal Large Language Models (MLLMs), finding that these models lack basic cognitive abilities and rely on shortcut learning.", "motivation": "To investigate the core knowledge representation in MLLMs and identify the reasons for their robustness limitations in real-world tasks.", "method": "The study evaluates 230 MLLMs using 11 different prompts across 12 core knowledge concepts, analyzing a total of 2,530 data points.", "result": "Key findings reveal that MLLMs underperform on low-level cognitive tasks compared to high-level ones and show deficiencies in scaling their capabilities.", "conclusion": "MLLMs are not progressing toward genuine understanding of core knowledge but are instead exploiting shortcut learning as they scale up.", "key_contributions": ["Introduction of the CoreCognition benchmark for evaluating MLLMs' cognitive abilities", "Identification of core knowledge deficits in MLLMs", "Proposal of Concept Hacking as a new evaluation method"], "limitations": "The study focuses only on specific core knowledge concepts and does not address all possible cognitive abilities.", "keywords": ["Multi-modal Large Language Models", "core knowledge", "cognitive abilities", "shortcut learning", "benchmark"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.11331", "pdf": "https://arxiv.org/pdf/2410.11331.pdf", "abs": "https://arxiv.org/abs/2410.11331", "title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments", "authors": ["Syed Abdul Gaffar Shakhadri", "Kruthika KR", "Rakshit Aralimatti"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "Paper in pdf format is 11 pages and contains 4 tables", "summary": "We introduce Shakti, a 2.5 billion parameter language model specifically\noptimized for resource-constrained environments such as edge devices, including\nsmartphones, wearables, and IoT systems. Shakti combines high-performance NLP\nwith optimized efficiency and precision, making it ideal for real-time AI\napplications where computational resources and memory are limited. With support\nfor vernacular languages and domain-specific tasks, Shakti excels in industries\nsuch as healthcare, finance, and customer service. Benchmark evaluations\ndemonstrate that Shakti performs competitively against larger models while\nmaintaining low latency and on-device efficiency, positioning it as a leading\nsolution for edge AI.", "AI": {"tldr": "Shakti is a 2.5 billion parameter language model optimized for resource-constrained environments, excelling in NLP tasks on edge devices such as smartphones, wearables, and IoT systems.", "motivation": "To develop a high-performance NLP model that can operate efficiently in resource-constrained environments and support vernacular languages and domain-specific tasks.", "method": "Developed a 2.5 billion parameter language model optimized for edge devices, incorporating techniques to enhance efficiency and precision in NLP applications.", "result": "Benchmark evaluations show Shakti performs competitively against larger models while maintaining low latency and efficiency on-device, suitable for real-time AI applications in various industries.", "conclusion": "Shakti positions itself as a leading solution for edge AI, particularly in the healthcare, finance, and customer service sectors, where computational resources are limited.", "key_contributions": ["Development of a high-performance NLP model for edge devices", "Support for vernacular languages", "Competitive performance against larger models with low latency"], "limitations": "", "keywords": ["language model", "edge AI", "resource-constrained environments", "NLP", "real-time applications"], "importance_score": 9, "read_time_minutes": 11}}
{"id": "2410.13284", "pdf": "https://arxiv.org/pdf/2410.13284.pdf", "abs": "https://arxiv.org/abs/2410.13284", "title": "Learning to Route LLMs with Confidence Tokens", "authors": ["Yu-Neng Chuang", "Prathusha Kameswara Sarma", "Parikshit Gopalan", "John Boccio", "Sara Bolouki", "Xia Hu", "Helen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nseveral tasks and are increasingly deployed in real-world applications.\nHowever, especially in high-stakes settings, it becomes vital to know when the\noutput of an LLM may be unreliable. Depending on whether an answer is\ntrustworthy, a system can then choose to route the question to another expert,\nor otherwise fall back on a safe default behavior. In this work, we study the\nextent to which LLMs can reliably indicate confidence in their answers, and how\nthis notion of confidence can translate into downstream accuracy gains. We\npropose Self-Reflection with Error-based Feedback (Self-REF), a lightweight\ntraining strategy to teach LLMs to express confidence in whether their answers\nare correct in a reliable manner. Self-REF introduces confidence tokens into\nthe LLM, from which a confidence score can be extracted. Compared to\nconventional approaches such as verbalizing confidence and examining token\nprobabilities, we demonstrate empirically that confidence tokens show\nsignificant improvements in downstream routing and rejection learning tasks.", "AI": {"tldr": "This paper investigates how large language models (LLMs) can reliably express confidence in their answers and introduces a new training strategy, Self-REF, to improve this capability.", "motivation": "Understanding when LLM outputs are unreliable is critical, especially in high-stakes applications, to enable appropriate decision-making concerning user queries.", "method": "The authors propose a training approach called Self-Reflection with Error-based Feedback (Self-REF), which integrates confidence tokens into LLMs to evaluate answer correctness and reliability.", "result": "Empirical results show that confidence tokens significantly enhance performance in downstream tasks involving routing and rejection learning compared to traditional methods of expressing confidence.", "conclusion": "The implementation of Self-REF can lead to improved trust and accuracy in LLM applications by better calibrating the models' confidence in their outputs.", "key_contributions": ["Introduction of confidence tokens that can be extracted as confidence scores.", "Demonstration of substantial improvements in downstream tasks with the proposed training strategy.", "Comparison of Self-REF against conventional methods highlighting its effectiveness."], "limitations": "", "keywords": ["large language models", "confidence measurement", "Self-REF", "machine learning", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.15865", "pdf": "https://arxiv.org/pdf/2410.15865.pdf", "abs": "https://arxiv.org/abs/2410.15865", "title": "Principles of semantic and functional efficiency in grammatical patterning", "authors": ["Emily Cheng", "Francesca Franzon"], "categories": ["cs.CL"], "comment": null, "summary": "Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation-a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints, accounting for variable communicative need. Our analyses reveal\nthat grammatical organization provably inherits from perceptual attributes, and\nour measurements on a diverse language sample show that grammars prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.", "AI": {"tldr": "This paper examines the relationship between grammatical features and sentence processing in human languages, proposing a unified theory based on semantic encoding and agreement-based predictability.", "motivation": "To explain the basis of universal grammatical patterns observed in diverse languages, which are rooted in a semantic foundation yet lack theoretical explanation.", "method": "The authors present an information-theoretic framework that combines semantic encoding with agreement-based predictability under cognitive constraints, analyzing variable communicative needs across languages.", "result": "The study reveals that grammatical organization derives from perceptual attributes, showing that grammars prioritize functional efficiency in language processing over mere semantic encoding.", "conclusion": "Grammars exhibit consistent organizational patterns due to their functional goals aimed at promoting efficient sentence processing.", "key_contributions": ["Unification of semantic encoding and agreement in grammar", "Demonstration of grammatical patterns stemming from perceptual attributes", "Prioritization of efficient language processing in grammatical organization"], "limitations": "", "keywords": ["grammar", "semantic encoding", "language processing", "cognitive constraints", "universal patterns"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2411.04291", "pdf": "https://arxiv.org/pdf/2411.04291.pdf", "abs": "https://arxiv.org/abs/2411.04291", "title": "Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models", "authors": ["Saketh Bachu", "Erfan Shayegani", "Rohit Lal", "Trishna Chakraborty", "Arindam Dutta", "Chengyu Song", "Yue Dong", "Nael Abu-Ghazaleh", "Amit K. Roy-Chowdhury"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ICML 2025 as a spotlight poster", "summary": "Vision-language models (VLMs) have improved significantly in their\ncapabilities, but their complex architecture makes their safety alignment\nchallenging. In this paper, we reveal an uneven distribution of harmful\ninformation across the intermediate layers of the image encoder and show that\nskipping a certain set of layers and exiting early can increase the chance of\nthe VLM generating harmful responses. We call it as \"Image enCoder Early-exiT\"\nbased vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5,\nLLaVA-NeXT, and Llama 3.2, show that performing early exits from the image\nencoder significantly increases the likelihood of generating harmful outputs.\nTo tackle this, we propose a simple yet effective modification of the\nClipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing\nlayer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO).\nWe evaluate our L-PPO algorithm across three multimodal datasets and show that\nit consistently reduces the harmfulness caused by early exits.", "AI": {"tldr": "This paper addresses the safety alignment challenges in vision-language models by discovering vulnerabilities associated with early exits from the image encoder, proposing a solution through a modified RLHF algorithm.", "motivation": "To address the safety alignment challenges in vision-language models due to complex architectures and their distribution of harmful information.", "method": "The paper reveals vulnerabilities in VLMs related to early exits from the image encoder and introduces the Layer-Wise PPO (L-PPO) algorithm for layer-wise multi-modal RLHF to mitigate harmful outputs.", "result": "Experiments show that early exits from the image encoder significantly increase harmful outputs, and L-PPO consistently reduces these harmful effects across three multimodal datasets.", "conclusion": "The proposed L-PPO algorithm effectively mitigates the harmfulness associated with early exits in vision-language models, enhancing their safety alignment.", "key_contributions": ["Identification of 'Image enCoder Early-exiT' (ICET) vulnerabilities in VLMs.", "Proposal of the Layer-Wise PPO (L-PPO) algorithm for improved safety alignment in VLMs.", "Experimental validation across multiple VLMs and datasets showing reduction in harmful outputs."], "limitations": "The findings are based on three specific VLMs, and generalizability to other architectures remains to be evaluated.", "keywords": ["vision-language models", "safety alignment", "harmful outputs", "Layer-Wise PPO", "multi-modal RLHF"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.13100", "pdf": "https://arxiv.org/pdf/2411.13100.pdf", "abs": "https://arxiv.org/abs/2411.13100", "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control", "authors": ["Yunkee Chae", "Eunsik Shin", "Suntae Hwang", "Seungryeol Paik", "Kyogu Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999", "AI": {"tldr": "The paper proposes a framework for generating song lyrics with precise syllable control across multiple levels while respecting song structures.", "motivation": "To address the challenges of generating natural-sounding song lyrics that adhere to syllable constraints and song form structures.", "method": "The proposed framework enables multi-level syllable control at the word, phrase, line, and paragraph levels, ensuring the generated lyrics align with input text and song form.", "result": "The framework produces song lyrics that meet specified syllable constraints and maintain natural phrasing, improving upon conventional line-by-line generation methods.", "conclusion": "This approach advances the state of lyrics generation by allowing for more structured and coherent outputs while correctly managing syllable counts.", "key_contributions": ["Multi-level syllable control for lyrics generation", "Integration of song form awareness into the generation process", "Improved natural flow and coherence of generated lyrics"], "limitations": "", "keywords": ["lyrics generation", "syllable control", "song form", "natural language processing", "text generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2411.16813", "pdf": "https://arxiv.org/pdf/2411.16813.pdf", "abs": "https://arxiv.org/abs/2411.16813", "title": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation", "authors": ["Svetlana Churina", "Kokil Jaidka"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support.", "AI": {"tldr": "This study examines the effects of fine-tuning language models on political argumentation quality using contrasting datasets from Twitter and Reddit.", "motivation": "To address the challenge of incivility in AI-generated political arguments on social media platforms.", "method": "Experiments with GPT-3.5 Turbo fine-tuned on datasets of political discussions from Twitter and Reddit, evaluating the impact on rhetorical framing and argument quality.", "result": "Reddit-finetuned models yield safer but rigid arguments, while cross-platform fine-tuning increases toxicity. Prompting can reduce some toxic behaviors but not fully counteract high-incivility influences.", "conclusion": "A new rhetorical evaluation rubric was introduced, along with practical guidelines for the use of LLMs in content authoring and moderation.", "key_contributions": ["Development of a rhetorical evaluation rubric", "Guidelines for deploying LLMs in moderation and deliberation", "Insights into fine-tuning strategies affecting argument quality"], "limitations": "High-incivility training data still adversely affects model behavior despite prompting.", "keywords": ["AI systems", "political argumentation", "incivility", "GPT-3.5 Turbo", "fine-tuning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.19930", "pdf": "https://arxiv.org/pdf/2411.19930.pdf", "abs": "https://arxiv.org/abs/2411.19930", "title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models", "authors": ["Daixuan Cheng", "Shaohan Huang", "Ziyu Zhu", "Xintong Zhang", "Wayne Xin Zhao", "Zhongzhi Luan", "Bo Dai", "Zhenliang Zhang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://huggingface.co/AdaptLLM", "summary": "Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs via post-training, focusing on data synthesis,\ntraining pipeline, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models in enhancing domain-specific performance. (2) Training\nPipeline: Unlike general MLLMs that typically adopt a two-stage training\nparadigm, we find that a single-stage approach is more effective for domain\nadaptation. (3) Task Evaluation: We conduct extensive experiments in\nhigh-impact domains such as biomedicine, food, and remote sensing, by\npost-training a variety of MLLMs and then evaluating MLLM performance on\nvarious domain-specific tasks. Finally, we fully open-source our models, code,\nand data to encourage future research in this area.", "AI": {"tldr": "This paper explores domain adaptation of multimodal large language models (MLLMs) through post-training, focusing on data synthesis, a novel training pipeline, and task evaluation in various high-impact domains.", "motivation": "To enhance the practical applications of general MLLMs in specific domains such as biomedicine and food through effective adaptation strategies.", "method": "The authors propose a generate-then-filter data synthesis approach that outperforms traditional methods, adopt a more effective single-stage training paradigm for domain adaptation, and conduct extensive testing on different domain-specific tasks.", "result": "The proposed methods demonstrate improved performance over existing approaches in enhancing domain-specific tasks in areas like biomedicine, food, and remote sensing.", "conclusion": "The study highlights the effectiveness of data synthesis and a single-stage training pipeline for adapting MLLMs to domain-specific tasks and opens resources for further research.", "key_contributions": ["Development of a generate-then-filter data synthesis pipeline for domain-specific tasks.", "Introduction of a single-stage training approach for more effective domain adaptation.", "Extensive evaluation of MLLMs in high-impact domains with open-sourced models and data."], "limitations": "", "keywords": ["multimodal large language models", "domain adaptation", "data synthesis", "biomedicine", "task evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.14860", "pdf": "https://arxiv.org/pdf/2412.14860.pdf", "abs": "https://arxiv.org/abs/2412.14860", "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling", "authors": ["Junyi Li", "Hwee Tou Ng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.", "AI": {"tldr": "This paper introduces Think&Cite, a framework for attributed text generation using self-guided Monte Carlo Tree Search to mitigate hallucination in LLMs.", "motivation": "Address the problem of hallucinations and factually incorrect information generated by large language models (LLMs).", "method": "Propose a novel framework called Think&Cite that integrates attributed text generation with a multi-step reasoning process using Self-Guided Monte Carlo Tree Search (SG-MCTS).", "result": "The proposed approach significantly outperforms baseline methods in generating attributed text across three different datasets.", "conclusion": "Think&Cite effectively utilizes self-reflection of LLMs and introduces Progress Reward Modeling, improving both generation and attribution in text production.", "key_contributions": ["Introduction of the Think&Cite framework for attributed text generation.", "Development of Self-Guided Monte Carlo Tree Search (SG-MCTS) for guiding text generation and search.", "Implementation of Progress Reward Modeling for measuring search progress."], "limitations": "", "keywords": ["Large Language Models", "Attributed Text Generation", "Monte Carlo Tree Search", "Self-Reflection", "Progress Reward Modeling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12911", "pdf": "https://arxiv.org/pdf/2502.12911.pdf", "abs": "https://arxiv.org/abs/2502.12911", "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation", "authors": ["Zheng Yuan", "Hao Chen", "Zijin Hong", "Qinggang Zhang", "Feiran Huang", "Qing Li", "Xiao Huang"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes.", "AI": {"tldr": "This paper presents KaSLA, a novel schema linking method that enhances SQL generation accuracy by addressing missing relevant schema elements and minimizing redundant ones through optimization techniques.", "motivation": "The accuracy of schema linking significantly affects SQL generation, but existing models struggle with missing and redundant schema elements. Current metrics do not capture the real performance of schema linking adequately.", "method": "The authors introduce KaSLA, which employs a hierarchical linking strategy and a knapsack optimization approach to improve schema linking by effectively linking relevant schema elements while minimizing excess ones.", "result": "KaSLA-1.6B demonstrates superior performance in schema linking compared to existing large-scale LLMs and state-of-the-art methods, leading to improved SQL generation results.", "conclusion": "The extensive experiments on benchmarks show that KaSLA can significantly enhance the SQL generation performance of SOTA Text2SQL models by improving their schema linking processes.", "key_contributions": ["Development of enhanced schema linking metrics with a restricted missing indicator.", "Introduction of a hierarchical linking strategy and knapsack optimization for schema linking.", "Demonstration of KaSLA's superior SQL generation performance on benchmarks."], "limitations": "", "keywords": ["schema linking", "SQL generation", "optimization", "human-computer interaction", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.14709", "pdf": "https://arxiv.org/pdf/2502.14709.pdf", "abs": "https://arxiv.org/abs/2502.14709", "title": "Group-Level Data Selection for Efficient Pretraining", "authors": ["Zichun Yu", "Fei Peng", "Jie Lei", "Arnold Overwijk", "Wen-tau Yih", "Chenyan Xiong"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we introduce Group-MATES, an efficient group-level data\nselection approach to optimize the speed-quality frontier of language model\npretraining. Specifically, Group-MATES parameterizes costly group-level\nselection with a relational data influence model. To train this model, we\nsample training trajectories of the language model and collect oracle data\ninfluences alongside. The relational data influence model approximates the\noracle data influence by weighting individual influence with relationships\namong training data. To enable efficient selection with our relational data\ninfluence model, we partition the dataset into small clusters using\nrelationship weights and select data within each cluster independently.\nExperiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves\n3.5%-9.4% relative performance gains over random selection across 22 downstream\ntasks, nearly doubling the improvements achieved by state-of-the-art individual\ndata selection baselines. Furthermore, Group-MATES reduces the number of tokens\nrequired to reach a certain downstream performance by up to 1.75x,\nsubstantially elevating the speed-quality frontier. Further analyses highlight\nthe critical role of relationship weights in the relational data influence\nmodel and the effectiveness of our cluster-based inference. Our code is\nopen-sourced at https://github.com/facebookresearch/Group-MATES.", "AI": {"tldr": "This paper presents Group-MATES, a group-level data selection approach that enhances the efficiency of language model pretraining, achieving significant performance improvements and reduced token requirements.", "motivation": "To improve the efficiency of language model pretraining by optimizing the speed-quality frontier through group-level data selection.", "method": "Group-MATES uses a relational data influence model to parameterize costly group-level selection, partitioning datasets into clusters based on relationship weights for independent data selection.", "result": "Group-MATES achieves 3.5%-9.4% relative performance gains over random selection on 22 downstream tasks and reduces tokens needed for performance by up to 1.75x.", "conclusion": "The relational data influence model and cluster-based inference significantly enhance data selection efficiency in language model training.", "key_contributions": ["Introduction of Group-MATES for group-level data selection", "Use of a relational data influence model", "Substantial performance gains on downstream tasks with reduced token usage."], "limitations": "", "keywords": ["language model", "data selection", "machine learning", "pretraining", "group-level optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14802", "pdf": "https://arxiv.org/pdf/2502.14802.pdf", "abs": "https://arxiv.org/abs/2502.14802", "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models", "authors": ["Bernal Jim√©nez Guti√©rrez", "Yiheng Shu", "Weijian Qi", "Sizhe Zhou", "Yu Su"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025. Code and data are available at:\n  https://github.com/OSU-NLP-Group/HippoRAG", "summary": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.", "AI": {"tldr": "The paper proposes HippoRAG 2, an advanced retrieval-augmented generation framework that improves factual, sense-making, and associative memory tasks in language models compared to standard RAG.", "motivation": "To enhance the ability of AI systems to approximate human-like long-term memory through improved continual learning methods.", "method": "HippoRAG 2 enhances the Personalized PageRank algorithm with deeper passage integration and more effective online use of LLMs to facilitate better memory performance.", "result": "HippoRAG 2 demonstrates a 7% improvement in associative memory tasks and outperforms standard RAG in factual knowledge and sense-making capabilities.", "conclusion": "The proposed framework significantly enhances non-parametric continual learning in LLMs, bridging the gap towards human long-term memory effectiveness.", "key_contributions": ["Introduction of HippoRAG 2 framework for improved memory tasks", "7% improvement in associative memory performance", "Enhancement of the Learner's performance through deeper passage integration"], "limitations": "", "keywords": ["retrieval-augmented generation", "long-term memory", "language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.14911", "pdf": "https://arxiv.org/pdf/2502.14911.pdf", "abs": "https://arxiv.org/abs/2502.14911", "title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models", "authors": ["Jann Railey Montalan", "Jimson Paulo Layacan", "David Demitri Africa", "Richell Isaiah Flores", "Michael T. Lopez II", "Theresa Denise Magsajo", "Anjanette Cayabyab", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities on widely benchmarked high-resource languages. However, linguistic\nnuances of under-resourced languages remain unexplored. We introduce Batayan, a\nholistic Filipino benchmark that systematically evaluates LLMs across three key\nnatural language processing (NLP) competencies: understanding, reasoning, and\ngeneration. Batayan consolidates eight tasks, three of which have not existed\nprior for Filipino corpora, covering both Tagalog and code-switched Taglish\nutterances. Our rigorous, native-speaker-driven adaptation and validation\nprocesses ensures fluency and authenticity to the complex morphological and\nsyntactic structures of Filipino, alleviating the pervasive translationese bias\nin existing Filipino corpora. We report empirical results on a variety of\nopen-source and commercial LLMs, highlighting significant performance gaps that\nsignal the under-representation of Filipino in pre-training corpora, the unique\nhurdles in modeling Filipino's rich morphology and construction, and the\nimportance of explicit Filipino language support. Moreover, we discuss the\npractical challenges encountered in dataset construction and propose principled\nsolutions for building culturally and linguistically-faithful resources in\nunder-represented languages. We also provide a public evaluation suite as a\nclear foundation for iterative, community-driven progress in Filipino NLP.", "AI": {"tldr": "This paper introduces Batayan, a benchmark for evaluating large language models on Filipino language processing tasks, emphasizing the challenges and needs for better language resources.", "motivation": "The need to explore and evaluate large language models (LLMs) for under-resourced languages, specifically Filipino, due to significant performance gaps and lack of representation in existing datasets.", "method": "Batayan encompasses eight NLP tasks focused on understanding, reasoning, and generation in both Tagalog and Taglish, validated through a native-speaker-driven process.", "result": "Empirical results reveal major performance gaps in LLMs when processing Filipino languages, highlighting the inadequacy of current pre-training corpora and the need for explicit support for Filipino language processing.", "conclusion": "The paper emphasizes the importance of creating culturally and linguistically-faithful resources for under-represented languages and provides an evaluation suite for community-driven progress in Filipino NLP.", "key_contributions": ["Introduction of a new benchmark for Filipino NLP called Batayan", "Identification of performance gaps in LLMs for Filipino languages", "Proposed solutions for dataset construction challenges in under-represented languages"], "limitations": "Potential limitations in generalizability to other under-resourced languages and reliance on native-speaker validation.", "keywords": ["Filipino NLP", "large language models", "benchmarking", "dataset construction", "cultural representation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.18108", "pdf": "https://arxiv.org/pdf/2502.18108.pdf", "abs": "https://arxiv.org/abs/2502.18108", "title": "Uncertainty Quantification in Retrieval Augmented Question Answering", "authors": ["Laura Perez-Beltrachini", "Mirella Lapata"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval augmented Question Answering (QA) helps QA models overcome\nknowledge gaps by incorporating retrieved evidence, typically a set of\npassages, alongside the question at test time. Previous studies show that this\napproach improves QA performance and reduces hallucinations, without, however,\nassessing whether the retrieved passages are indeed useful at answering\ncorrectly. In this work, we propose to quantify the uncertainty of a QA model\nvia estimating the utility of the passages it is provided with. We train a\nlightweight neural model to predict passage utility for a target QA model and\nshow that while simple information theoretic metrics can predict answer\ncorrectness up to a certain extent, our approach efficiently approximates or\noutperforms more expensive sampling-based methods. Code and data are available\nat https://github.com/lauhaide/ragu.", "AI": {"tldr": "This paper quantifies the utility of passages in retrieval-augmented Question Answering, proposing a lightweight model to predict passage effectiveness which outperforms costly methods.", "motivation": "To evaluate the effectiveness of retrieved passages in retrieval-augmented QA systems and improve answer correctness by predicting passage utility.", "method": "Development of a lightweight neural model to predict the utility of passages supplied to a QA model, comparing its performance with simple information-theoretic metrics and sampling-based methods.", "result": "The proposed model efficiently predicts passage utility and shows improved performance in predicting answer correctness compared to traditional methods.", "conclusion": "The study concludes that predicting passage utility can enhance the reliability of QA models and provides a lightweight alternative to complex sampling methods.", "key_contributions": ["Introduces a lightweight neural model to predict passage utility for QA systems.", "Demonstrates the effectiveness of the approach in comparison to existing methods.", "Gives access to code and data for further research."], "limitations": "", "keywords": ["Retrieval-Augmented QA", "Passage Utility", "Neural Model", "Information Theory", "QA Performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.18443", "pdf": "https://arxiv.org/pdf/2502.18443.pdf", "abs": "https://arxiv.org/abs/2502.18443", "title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models", "authors": ["Jake Poznanski", "Aman Rangapur", "Jon Borchardt", "Jason Dunkelberger", "Regan Huff", "Daniel Lin", "Aman Rangapur", "Christopher Wilhelm", "Kyle Lo", "Luca Soldaini"], "categories": ["cs.CL"], "comment": null, "summary": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. Traditional open source tools often produce\nlower quality extractions compared to vision language models (VLMs), but\nreliance on the best VLMs can be prohibitively costly (e.g., over $6,240 USD\nper million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to\nproprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs\ninto clean, linearized plain text in natural reading order while preserving\nstructured content like sections, tables, lists, equations, and more. Our\ntoolkit runs a fine-tuned 7B vision language model (VLM) trained on\nolmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and can convert a million PDF pages for\nonly $176 USD. To aid comparison with existing systems, we also introduce\nolmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that\nremain challenging even for the best tools and VLMs, including formulas,\ntables, tiny fonts, old scans, and more. We find olmOCR outperforms even top\nVLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all\ncomponents of olmOCR: our fine-tuned VLM model, training code and data, an\nefficient inference pipeline that supports vLLM and SGLang backends, and\nbenchmark olmOCR-Bench.", "AI": {"tldr": "olmOCR is an open-source toolkit designed to convert diverse PDF documents into high-quality, linearized text while maintaining structured content.", "motivation": "PDF documents hold a wealth of quality tokens for training language models but pose challenges due to their varied formats and layouts.", "method": "The toolkit uses a fine-tuned 7B vision language model and is optimized for large-scale batch processing, enabling efficient conversion of PDFs at low cost.", "result": "olmOCR outperforms leading VLMs in document processing accuracy and efficiency, costing only $176 to process one million pages.", "conclusion": "All components of olmOCR, including the model, training code, data, and benchmarks, are publicly available to facilitate further research and development.", "key_contributions": ["Introduction of olmOCR for high-quality PDF text extraction", "Development of olmOCR-Bench for benchmarking PDF processing tools", "Cost-effective solution for processing large sets of PDF documents"], "limitations": "", "keywords": ["PDF Processing", "Vision Language Models", "Open-source Toolkit"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.18452", "pdf": "https://arxiv.org/pdf/2502.18452.pdf", "abs": "https://arxiv.org/abs/2502.18452", "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response", "authors": ["Mollie Shichman", "Claire Bonial", "Austin Blodgett", "Taylor Hudson", "Francis Ferraro", "Rachel Rudinger"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 5 tables", "summary": "During Human Robot Interactions in disaster relief scenarios, Large Language\nModels (LLMs) have the potential for substantial physical reasoning to assist\nin mission objectives. However, these capabilities are often found only in\nlarger models, which are frequently not reasonable to deploy on robotic\nsystems. To meet our problem space requirements, we introduce a dataset and\npipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA)\nmodels. In our pipeline, domain experts and linguists combine their knowledge\nto make high-quality few-shot prompts used to generate synthetic data for\nfine-tuning. We hand-curate datasets for this few-shot prompting and for\nevaluation to improve LLM reasoning on both general and disaster-specific\nobjects. We concurrently run an ablation study to understand which kinds of\nsynthetic data most affect performance. We fine-tune several small\ninstruction-tuned models and find that ablated FRIDA models only trained on\nobjects' physical state and function data outperformed both the FRIDA models\ntrained on all synthetic data and the base models in our customized evaluation.\nWe demonstrate that the FRIDA pipeline is capable of instilling physical common\nsense with minimal data.", "AI": {"tldr": "The paper introduces the FRIDA pipeline for enhancing small LLMs' physical reasoning in disaster relief by using few-shot prompting and synthetic data generation.", "motivation": "The need for effective physical reasoning in small models for disaster relief robots, which cannot deploy larger LLMs.", "method": "The authors developed the FRIDA pipeline to create high-quality few-shot prompts for generating synthetic data, refined through domain expert collaboration, and conducted an ablation study on the models.", "result": "Ablated FRIDA models trained on specific object data outperformed both fully trained FRIDA models and base models in evaluation metrics.", "conclusion": "The FRIDA pipeline effectively improves physical reasoning capabilities in small LLMs with minimal high-quality data.", "key_contributions": ["Development of the FRIDA dataset and pipeline for disaster scenarios", "Effective few-shot prompting techniques for synthetic data generation", "Ablation study results indicating the importance of physical state and function data in training"], "limitations": "", "keywords": ["Human-Robot Interaction", "Large Language Models", "Disaster Relief", "Few-Shot Learning", "Synthetic Data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.01807", "pdf": "https://arxiv.org/pdf/2503.01807.pdf", "abs": "https://arxiv.org/abs/2503.01807", "title": "Large-Scale Data Selection for Instruction Tuning", "authors": ["Hamish Ivison", "Muru Zhang", "Faeze Brahman", "Pang Wei Koh", "Pradeep Dasigi"], "categories": ["cs.CL"], "comment": "Updated, new baselines, removed some typos", "summary": "Selecting high-quality training data from a larger pool is a crucial step\nwhen instruction-tuning language models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.\nAutomated data selection approaches for instruction-tuning are typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant of\nrepresentation-based data selection (RDS+), which uses weighted mean pooling of\npretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection.", "AI": {"tldr": "This paper evaluates the effectiveness of automated data selection methods for instruction-tuning language models, highlighting that many recent approaches do not outperform random selection, particularly when scaling to larger datasets.", "motivation": "The study seeks to address the importance of selecting high-quality training data when instruction-tuning language models, as it can greatly enhance model performance compared to using noisy datasets.", "method": "The authors systematically assess data selection methods by selecting up to 2.5M samples from large pools (up to 5.8M), comparing their performance across seven diverse tasks.", "result": "It was found that many recent data selection methods perform worse than random selection and even decline in effectiveness when using larger pools, while a variant of representation-based data selection consistently outperformed complex methods.", "conclusion": "The findings suggest that the scaling properties of automated data selection methods need closer examination, and the authors provide their code, data, and models for further exploration.", "key_contributions": ["Systematic evaluation of data selection methods at scale.", "Demonstration that many recent methods underperform compared to random selection when scaling.", "Introduction of RDS+, a more effective and compute-efficient data selection approach."], "limitations": "The study mainly focuses on the scaling properties and may not cover smaller datasets or less complex selection methods.", "keywords": ["data selection", "instruction-tuning", "language models", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.02832", "pdf": "https://arxiv.org/pdf/2503.02832.pdf", "abs": "https://arxiv.org/abs/2503.02832", "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "authors": ["Songming Zhang", "Xue Zhang", "Tong Zhang", "Bojie Hu", "Yufeng Chen", "Jinan Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main Conference, code available at:\n  https://github.com/songmzhang/AlignDistil", "summary": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.", "AI": {"tldr": "AlignDistil proposes a novel method for LLM alignment through token-level reward optimization, improving performance and convergence speed compared to existing methods.", "motivation": "Existing methods for LLM alignment use sparse, response-level rewards, which can lead to suboptimal performance by incorrectly optimizing high- or low-quality tokens.", "method": "The paper introduces AlignDistil, which integrates token-level rewards learned from direct preference optimization (DPO) into the reinforcement learning from human feedback (RLHF) framework, establishing a theoretically proved equivalence between this objective and a token-level distillation process.", "result": "Experimental results show that AlignDistil outperforms existing alignment methods and achieves faster convergence due to its token-level distributional reward optimization.", "conclusion": "The proposed method enhances LLM alignment by optimizing rewards at the token level, thus improving overall model performance.", "key_contributions": ["Introduction of token-level reward optimization into LLM alignment through AlignDistil.", "Theoretical proof of the equivalence between RLHF and token-level distillation processes.", "Design of a token adaptive logit extrapolation mechanism for effective teacher distribution."], "limitations": "", "keywords": ["LLM alignment", "token-level rewards", "reinforcement learning from human feedback", "direct preference optimization", "fast convergence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.05298", "pdf": "https://arxiv.org/pdf/2503.05298.pdf", "abs": "https://arxiv.org/abs/2503.05298", "title": "Coreference as an indicator of context scope in multimodal narrative", "authors": ["Nikolai Ilinykh", "Shalom Lappin", "Asad Sayeed", "Sharid Lo√°iciga"], "categories": ["cs.CL"], "comment": "19 pages, 4 tables. Accepted to GEM2 Workshop: Generation, Evaluation\n  & Metrics at ACL 2025", "summary": "We demonstrate that large multimodal language models differ substantially\nfrom humans in the distribution of coreferential expressions in a visual\nstorytelling task. We introduce a number of metrics to quantify the\ncharacteristics of coreferential patterns in both human- and machine-written\ntexts. Humans distribute coreferential expressions in a way that maintains\nconsistency across texts and images, interleaving references to different\nentities in a highly varied way. Machines are less able to track mixed\nreferences, despite achieving perceived improvements in generation quality.\nMaterials, metrics, and code for our study are available at\nhttps://github.com/GU-CLASP/coreference-context-scope.", "AI": {"tldr": "Large multimodal language models exhibit significant differences from humans in their use of coreferential expressions during visual storytelling tasks.", "motivation": "To analyze and quantify the differences in how humans and machines use coreferential expressions in multimodal contexts, particularly in visual storytelling.", "method": "The study introduced several metrics to evaluate coreferential patterns and compared those patterns in human and machine-generated texts across various visual contexts.", "result": "It was found that humans are able to distribute coreferential expressions more effectively and consistently than machines, who struggle with tracking mixed references despite improvements in overall generation quality.", "conclusion": "The findings highlight the limitations of current large multimodal language models in handling visual storytelling and suggest areas for improvement in future AI systems.", "key_contributions": ["Introduction of metrics for evaluating coreferential expressions in visual storytelling", "Demonstration of substantial differences between human and machine text generation", "Availability of study materials and code for reproducibility"], "limitations": "", "keywords": ["coreference", "multimodal", "visual storytelling", "language models", "machine learning"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2503.05328", "pdf": "https://arxiv.org/pdf/2503.05328.pdf", "abs": "https://arxiv.org/abs/2503.05328", "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models", "authors": ["Anar Yeginbergen", "Maite Oronoz", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.", "AI": {"tldr": "This paper explores integrating dynamic external knowledge into Large Language Models (LLMs) to enhance counter-argument generation, presenting a new dataset and evaluation methodology.", "motivation": "To address the limitations of LLMs in generating factual and controlled argumentative responses and to improve counter-argument generation.", "method": "The authors introduce a manually curated dataset of argument and counter-argument pairs and propose a LLM-as-a-Judge evaluation methodology to better align with human judgments.", "result": "Experimental results show that dynamic integration of external knowledge significantly enhances the quality of generated counter-arguments, improving relatedness, persuasiveness, and factuality.", "conclusion": "The study concludes that combining LLMs with external knowledge retrieval can lead to the development of more effective counter-argumentation systems.", "key_contributions": ["Introduction of a custom dataset for evaluating argumentative complexity.", "Development of a LLM-as-a-Judge evaluation methodology that aligns closely with human judgments.", "Demonstrated improvement in counter-argument quality through external knowledge integration."], "limitations": "The results may vary based on the quality of external knowledge and the limitations of the LLMs used.", "keywords": ["Large Language Models", "counter-argument generation", "dynamic external knowledge integration", "argumentative complexity", "evaluation methodology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.05888", "pdf": "https://arxiv.org/pdf/2503.05888.pdf", "abs": "https://arxiv.org/abs/2503.05888", "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation", "authors": ["Bang Nguyen", "Tingting Du", "Mengxia Yu", "Lawrence Angrave", "Meng Jiang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Camera Ready - ACL 2025 Main", "summary": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items.", "AI": {"tldr": "The paper introduces a new evaluation framework for Question Generation (QG) that utilizes test item analysis to better assess test question quality in educational assessments.", "motivation": "The existing evaluation methods for Question Generation (QG) lack a direct connection to educational values and do not adequately assess test question quality.", "method": "The authors developed pairs of candidate questions with varying quality dimensions and assessed existing evaluation approaches against these.", "result": "The study identified significant limitations in current QG evaluation methods in measuring test item quality relative to student performance, leading to the development of QG-SMS framework.", "conclusion": "The QG-SMS framework, which uses Large Language Models to simulate student profiles, provides a more robust assessment of test items and addresses the shortcomings of current evaluation techniques.", "key_contributions": ["Introduction of test item analysis to QG evaluation", "Development of the QG-SMS framework leveraging LLM for assessment", "Empirical evidence showing improved evaluation accuracy with simulated student profiles."], "limitations": "Current methods do not directly link to student performance; may require further validation in diverse educational contexts.", "keywords": ["Question Generation", "Educational Assessment", "Large Language Models", "Test Item Analysis", "Human Evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.10486", "pdf": "https://arxiv.org/pdf/2503.10486.pdf", "abs": "https://arxiv.org/abs/2503.10486", "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions", "authors": ["Gaurav Kumar Gupta", "Pranal Pande", "Nirajan Acharya", "Aniket Kumar Singh", "Suman Niroula"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.", "AI": {"tldr": "This study evaluates the performance of two LLM-based diagnostic tools, DeepSeek R1 and O3 Mini, in medical diagnostics using a structured dataset.", "motivation": "The paper aims to assess how LLMs can enhance disease classification and clinical decision-making in the medical field.", "method": "Performance evaluation of DeepSeek R1 and O3 Mini on a dataset of symptoms and diagnoses, measuring predictive accuracy and confidence scores.", "result": "DeepSeek R1 achieved 76% disease-level accuracy and 82% overall accuracy, outperforming O3 Mini's 72% and 75% accuracy; both models struggled with Respiratory Disease classification.", "conclusion": "The findings highlight the effectiveness of LLMs in certain medical areas while underscoring challenges, such as classification accuracy in Respiratory Diseases and ethical concerns.", "key_contributions": ["Insights into LLM performance in medical diagnostics", "Comparison of two LLM-based systems", "Ethical considerations in AI-driven healthcare"], "limitations": "Struggles with Respiratory Disease classification and ethical concerns such as bias and data privacy.", "keywords": ["Large Language Models", "diagnostics", "health informatics", "machine learning", "clinical decision-making"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.07385", "pdf": "https://arxiv.org/pdf/2504.07385.pdf", "abs": "https://arxiv.org/abs/2504.07385", "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences.", "AI": {"tldr": "Tool-Augmented LLM Evaluation (TALE) framework improves evaluation of LLM outputs without static references by using an agent to retrieve and synthesize information dynamically.", "motivation": "Traditional methods for evaluating LLM outputs rely on static references, which face challenges in cost, scalability, and completeness, particularly in autonomous applications.", "method": "TALE uses an agent with tool-access capabilities that generates web queries, collects information, summarizes findings, and refines searches through iterative reflection, effectively assessing LLM outputs.", "result": "Experimental results reveal that TALE outperforms standard reference-based metrics for accuracy and aligns closely with human evaluations across multiple free-form QA benchmarks.", "conclusion": "By moving away from static references, TALE provides a more reliable and effective means of evaluating LLM outputs in dynamic, real-world scenarios.", "key_contributions": ["Introduction of the TALE framework for evaluating LLM outputs without predetermined ground-truth answers.", "Utilization of an agent that synthesizes external evidence through iterative queries and reflections.", "Demonstration of improved evaluation accuracy compared to standard metrics based on static references."], "limitations": "", "keywords": ["Large Language Models", "evaluation framework", "dynamic assessment", "free-form question-answering", "autonomous applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.06751", "pdf": "https://arxiv.org/pdf/2506.06751.pdf", "abs": "https://arxiv.org/abs/2506.06751", "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries according to contemporary language models", "authors": ["Mikhail Salnikov", "Dmitrii Korzh", "Ivan Lazichny", "Elvir Karimov", "Artyom Iudin", "Ivan Oseledets", "Oleg Y. Rogov", "Natalia Loukachevitch", "Alexander Panchenko", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": null, "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.", "AI": {"tldr": "The paper analyzes geopolitical biases in LLMs regarding historical interpretations from various countries, revealing significant biases favoring national narratives and limited effectiveness of debiasing prompts.", "motivation": "To investigate and evaluate the influence of geopolitical biases in large language models (LLMs) on their interpretations of historical events from conflicting national perspectives.", "method": "A novel dataset was introduced comprising neutral event descriptions alongside contrasting viewpoints from the USA, UK, USSR, and China. The study conducted experiments involving manipulated participant labels to assess the models' sensitivity to attribution.", "result": "The findings indicate substantial geopolitical biases in LLMs, with a tendency to favor certain national narratives. Moreover, simple debiasing prompts proved largely ineffective in mitigating these biases.", "conclusion": "The research emphasizes the existence of national narrative biases within LLMs and questions the utility of straightforward debiasing methods, while providing a framework and dataset for future investigations into geopolitical bias.", "key_contributions": ["Introduction of a novel dataset for evaluating geopolitical biases in LLMs", "Demonstration of significant biases favoring specific national narratives", "Challenge to the effectiveness of simple debiasing techniques"], "limitations": "The effectiveness of debiasing methods was limited, and further research is needed to explore alternative approaches.", "keywords": ["geopolitical bias", "LLMs", "national narratives", "debiasing", "historical interpretation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245.pdf", "abs": "https://arxiv.org/abs/2506.07245", "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement.", "AI": {"tldr": "SDE-SQL is a framework that enables large language models to explore databases autonomously by generating and executing SQL probes during inference.", "motivation": "Dynamic interaction with database contents to improve the Text-to-SQL task performance and overcome limitations of prior static approaches.", "method": "The proposed framework allows LLMs to autonomously retrieve information from databases by generating and executing SQL probes in a zero-shot setting.", "result": "SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the baseline on the BIRD benchmark, setting a new state-of-the-art performance without supervised fine-tuning.", "conclusion": "SDE-SQL enhances LLM capabilities in understanding and interacting with databases, and further improves with supervised fine-tuning.", "key_contributions": ["Introduction of SDE-SQL framework for self-driven database exploration", "Achieves state-of-the-art performance in Text-to-SQL tasks", "Demonstrates improvements over existing models without requiring in-context demonstrations"], "limitations": "", "keywords": ["large language models", "Text-to-SQL", "self-driven exploration", "SQL probes", "zero-shot setting"], "importance_score": 9, "read_time_minutes": 5}}
