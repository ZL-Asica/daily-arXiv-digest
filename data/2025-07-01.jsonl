{"id": "2506.22520", "pdf": "https://arxiv.org/pdf/2506.22520.pdf", "abs": "https://arxiv.org/abs/2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity."}
{"id": "2506.22583", "pdf": "https://arxiv.org/pdf/2506.22583.pdf", "abs": "https://arxiv.org/abs/2506.22583", "title": "Supra-threshold control of peripheral LOD", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Level of detail (LOD) is widely used to control visual feedback in\ninteractive applications. LOD control is typically based on perception at\nthreshold - the conditions in which a stimulus first becomes perceivable. Yet\nmost LOD manipulations are quite perceivable and occur well above threshold.\nMoreover, research shows that supra-threshold perception differs drastically\nfrom perception at threshold. In that case, should supra-threshold LOD control\nalso differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual\nperiphery and find that indeed, it should differ drastically from LOD control\nat threshold. Specifically, we find that LOD must support a task-dependent\nlevel of reliable perceptibility. Above that level, perceptibility of LOD\ncontrol manipulations should be minimized, and detail contrast is a better\npredictor of perceptibility than detail size. Below that level, perceptibility\nmust be maximized, and LOD should be improved as eccentricity rises or contrast\ndrops. This directly contradicts prevailing threshold-based LOD control\nschemes, and strongly suggests a reexamination of LOD control for foveal\ndisplay."}
{"id": "2506.22597", "pdf": "https://arxiv.org/pdf/2506.22597.pdf", "abs": "https://arxiv.org/abs/2506.22597", "title": "A tangible user interface for assessing cognitive mapping ability", "authors": ["Ehud Sharlin", "Benjamin Watson", "Steve Sutphen", "Lili Liu", "Robert Lederer", "John Frazer"], "categories": ["cs.HC"], "comment": null, "summary": "Wayfinding, the ability to recall the environment and navigate through it, is\nan essential cognitive skill relied upon almost every day in a person's life. A\ncrucial component of wayfinding is the construction of cognitive maps, mental\nrepresentations of the environments through which a person travels. Age,\ndisease or injury can severely affect cognitive mapping, making assessment of\nthis basic survival skill particularly important to clinicians and therapists.\nCognitive mapping has also been the focus of decades of basic research by\ncognitive psychologists. Both communities have evolved a number of techniques\nfor assessing cognitive mapping ability. We present the Cognitive Map Probe\n(CMP), a new computerized tool for assessment of cognitive mapping ability that\nincreases consistency and promises improvements in flexibility, accessibility,\nsensitivity and control. The CMP uses a tangible user interface that affords\nspatial manipulation. We describe the design of the CMP, and find that it is\nsensitive to factors known to affect cognitive mapping performance in extensive\nexperimental testing."}
{"id": "2506.22674", "pdf": "https://arxiv.org/pdf/2506.22674.pdf", "abs": "https://arxiv.org/abs/2506.22674", "title": "Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China", "authors": ["Weiyin Xie", "Chunxi Huang", "Jiyao Wang", "Dengbo He"], "categories": ["cs.HC", "cs.CY", "stat.AP"], "comment": null, "summary": "Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs),\ngiven some unique characteristics of EVs, for example, the low air pollution\nand maintenance cost. However, the increasing prevalence of EVs is accompanied\nby widespread complaints regarding the high likelihood of motion sickness (MS)\ninduction, especially when compared to FVs, which has become one of the major\nobstacles to the acceptance and popularity of EVs. Despite the prevalence of\nsuch complaints online and among EV users, the association between vehicle type\n(i.e., EV versus FV) and MS prevalence and severity has not been quantified.\nThus, this study aims to investigate the existence of EV-induced MS and explore\nthe potential factors leading to it. A survey study was conducted to collect\npassengers' MS experience in EVs and FVs in the past one year. In total, 639\nvalid responses were collected from mainland China. The results show that FVs\nwere associated with a higher frequency of MS, while EVs were found to induce\nmore severe MS symptoms. Further, we found that passengers' MS severity was\nassociated with individual differences (i.e., age, gender, sleep habits,\nsusceptibility to motion-induced MS), in-vehicle activities (i.e., chatting\nwith others and watching in-vehicle displays), and road conditions (i.e.,\ncongestion and slope), while the MS frequency was associated with the vehicle\nownership and riding frequency. The results from this study can guide the\ndirections of future empirical studies that aim to quantify the inducers of MS\nin EVs and FVs, as well as the optimization of EVs to reduce MS."}
{"id": "2506.22439", "pdf": "https://arxiv.org/pdf/2506.22439.pdf", "abs": "https://arxiv.org/abs/2506.22439", "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "authors": ["Javier Conde", "Miguel González", "María Grandury", "Gonzalo Martínez", "Pedro Reviriego", "Mar Brysbaert"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets."}
{"id": "2506.22741", "pdf": "https://arxiv.org/pdf/2506.22741.pdf", "abs": "https://arxiv.org/abs/2506.22741", "title": "Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India", "authors": ["Akshay Nayak Kolgar", "Yash Prakash", "Sampath Jayarathna", "Hae-Na Lee", "Vikas Ashok"], "categories": ["cs.HC"], "comment": null, "summary": "Significant changes in the digital employment landscape, driven by rapid\ntechnological advancements and the COVID-19 pandemic, have introduced new\nopportunities for blind and visually impaired (BVI) individuals in developing\ncountries like India. However, a significant portion of the BVI population in\nIndia remains unemployed despite extensive accessibility advancements and job\nsearch interventions. Therefore, we conducted semi-structured interviews with\n20 BVI persons who were either pursuing or recently sought employment in the\ndigital industry. Our findings reveal that despite gaining digital literacy and\nextensive training, BVI individuals struggle to meet industry requirements for\nfulfilling job openings. While they engage in self-reflection to identify\nshortcomings in their approach and skills, they lack constructive feedback from\npeers and recruiters. Moreover, the numerous job intervention tools are limited\nin their ability to meet the unique needs of BVI job seekers. Our results\ntherefore provide key insights that inform the design of future collaborative\nintervention systems that offer personalized feedback for BVI individuals,\neffectively guiding their self-reflection process and subsequent job search\nbehaviors, and potentially leading to improved employment outcomes."}
{"id": "2506.22485", "pdf": "https://arxiv.org/pdf/2506.22485.pdf", "abs": "https://arxiv.org/abs/2506.22485", "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext."}
{"id": "2506.22815", "pdf": "https://arxiv.org/pdf/2506.22815.pdf", "abs": "https://arxiv.org/abs/2506.22815", "title": "Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents", "authors": ["Haichang Li"], "categories": ["cs.HC", "H.5.0"], "comment": "Position Paper for workshop. This is an initial version for\n  discussion purposes", "summary": "This position paper aims to rethink the role and design of memory in Large\nLanguage Model (LLM)-based agent systems. We observe that while current memory\npractices have begun to transcend the limitations of single interactions, they\nremain conceptually grounded in \"bound memory\" in terms of design concept-where\nmemory is treated as local state attached to specific context or entities,\nforming \"memory silos\" that impede cross-entity collaboration. To overcome this\narchitectural bottleneck, this paper proposes the timely design perspective of\n\"Memory as a Service\" (MaaS). MaaS advocates decoupling memory from its\nconventional role as an interaction byproduct and encapsulating it as a modular\nservice that can be independently callable, dynamically composable, and finely\ngoverned. At its core, MaaS leverages the duality of memory-its inherently\nprivate nature and its potential for public service-to grant memory controlled,\non-demand interoperability across entities. This paper introduces a\ntwo-dimensional design space defined by entity structure and service type,\nillustrating how MaaS aligns with current memory practices while naturally\nextending them to cross-entity collaborative scenarios. Finally, we outline an\nopen research agenda spanning governance, security, and ethical ecosystems, and\ncall upon the broader research community to explore this shift toward\nservice-oriented memory for collaborative agents operating across entity\nboundaries."}
{"id": "2506.22486", "pdf": "https://arxiv.org/pdf/2506.22486.pdf", "abs": "https://arxiv.org/abs/2506.22486", "title": "Hallucination Detection with Small Language Models", "authors": ["Ming Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications."}
{"id": "2506.22841", "pdf": "https://arxiv.org/pdf/2506.22841.pdf", "abs": "https://arxiv.org/abs/2506.22841", "title": "Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation", "authors": ["George Bell", "Alma Cantu"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures. Conditionally accepted to IEEE VIS 2025 (pending\n  final review)", "summary": "Adjusting transparency is a common method of mitigating occlusion but is\noften detrimental for understanding the relative depth relationships between\nobjects as well as removes potentially important information from the occluding\nobject. We propose using dichoptic opacity, a novel method for occlusion\nmanagement that contrasts the transparency of occluders presented to each eye.\nThis allows for better simultaneous understanding of both occluder and\noccluded. A user study highlights the technique's potential, showing strong\nuser engagement and a clear preference for dichoptic opacity over traditional\npresentations. While it does not determine optimal transparency values, it\nreveals promising trends in both percentage and range that merit further\ninvestigation."}
{"id": "2506.22491", "pdf": "https://arxiv.org/pdf/2506.22491.pdf", "abs": "https://arxiv.org/abs/2506.22491", "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology."}
{"id": "2506.22926", "pdf": "https://arxiv.org/pdf/2506.22926.pdf", "abs": "https://arxiv.org/abs/2506.22926", "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions", "authors": ["Qixuan Liu", "Shi Qiu", "Yinqiao Wang", "Xiwen Wu", "Kenneth Siu Ho Chok", "Chi-Wing Fu", "Pheng-Ann Heng"], "categories": ["cs.HC", "cs.GR", "cs.MM"], "comment": "IEEE VIS 2025 Short Paper", "summary": "Volumetric medical imaging technologies produce detailed 3D representations\nof anatomical structures. However, effective medical data visualization and\nexploration pose significant challenges, especially for individuals with\nlimited medical expertise. We introduce a novel XR-based system with two key\ninnovations: (1) a coordinated visualization module integrating Multi-layered\nMulti-planar Reconstruction with 3D mesh models and (2) a multimodal\ninteraction framework combining hand gestures with LLM-enabled voice commands.\nWe conduct preliminary evaluations, including a 15-participant user study and\nexpert interviews, to demonstrate the system's abilities to enhance spatial\nunderstanding and reduce cognitive load. Experimental results show notable\nimprovements in task completion times, usability metrics, and interaction\neffectiveness enhanced by LLM-driven voice control. While identifying areas for\nfuture refinement, our findings highlight the potential of this immersive\nvisualization system to advance medical training and clinical practice. Our\ndemo application and supplemental materials are available for download at:\nhttps://osf.io/bpjq5/."}
{"id": "2506.22508", "pdf": "https://arxiv.org/pdf/2506.22508.pdf", "abs": "https://arxiv.org/abs/2506.22508", "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth."}
{"id": "2506.22932", "pdf": "https://arxiv.org/pdf/2506.22932.pdf", "abs": "https://arxiv.org/abs/2506.22932", "title": "Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives", "authors": ["Zoe Anastasiadou", "Andreas Lanitis"], "categories": ["cs.HC"], "comment": "13 pages, 2 figures", "summary": "The increase of the percentage of elderly population in modern societies\ndictates the use of emerging technologies as a means of supporting elder\nmembers of the society. Within this scope, Extended Reality (XR) technologies\npose as a promising technology for improving the daily lives of the elderly\npopulation. This paper presents a literature review that describes the most\ncommon characteristics of the physical and mental state of the elderly,\nallowing readers, and specifically XR developers, to understand the main\ndifficulties faced by elderly users of extended reality applications so they\ncan develop accessible, user friendly and engaging applications for the target\naudience. Furthermore, a review of existing extended reality applications that\ntarget the elder population is presented, allowing readers to get acquainted\nwith existing design paradigms that can inspire future developments."}
{"id": "2506.22510", "pdf": "https://arxiv.org/pdf/2506.22510.pdf", "abs": "https://arxiv.org/abs/2506.22510", "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score."}
{"id": "2506.22937", "pdf": "https://arxiv.org/pdf/2506.22937.pdf", "abs": "https://arxiv.org/abs/2506.22937", "title": "GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework", "authors": ["Tianrun Qiu", "Changxin Chen", "Sizhe Cheng", "Yiming Yang", "Yixiao Guo", "Zhicong Lu", "Yuxin Ma"], "categories": ["cs.HC", "H.5.2"], "comment": "19 pages, 9 figures", "summary": "Blind and low-vision (BLV) players encounter critical challenges in engaging\nwith video games due to the inaccessibility of visual elements, difficulties in\nnavigating interfaces, and limitations in sending interaction input. Moreover,\nthe development of specialized accessibility features typically requires\nsubstantial programming effort and is often implemented on a game-by-game\nbasis. To address these challenges, we introduce \\textit{GamerAstra}, a\ngeneralized accessibility framework that leverages a multi-agent design to\nfacilitate access to video games for BLV players. It integrates multi-modal\ntechniques including large language models and vision-language models, enabling\ninteraction with games lacking native accessibility support. The framework\nfurther incorporates customizable assistance granularities to support varying\ndegrees of visual impairment and enhances interface navigation through multiple\ninput modalities. The evaluation through technical assessments and user studies\nindicate that \\textit{GamerAstra} effectively enhances playability and delivers\na more immersive gaming experience for BLV players. These findings also\nunderscore potential avenues for advancing intelligent accessibility frameworks\nin the gaming domain."}
{"id": "2506.22516", "pdf": "https://arxiv.org/pdf/2506.22516.pdf", "abs": "https://arxiv.org/abs/2506.22516", "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "authors": ["Jingkai Li"], "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163."}
{"id": "2506.22940", "pdf": "https://arxiv.org/pdf/2506.22940.pdf", "abs": "https://arxiv.org/abs/2506.22940", "title": "Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools", "authors": ["Varun Sangwan", "Heidi Makitalo"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "This paper investigates how collaborative AI systems can enhance user agency\nin identifying and evaluating misinformation on social media platforms.\nTraditional methods, such as personal judgment or basic fact-checking, often\nfall short when faced with emotionally charged or context-deficient content. To\naddress this, we designed and evaluated an interactive interface that\nintegrates collaborative AI features, including real-time explanations, source\naggregation, and debate-style interaction. These elements aim to support\ncritical thinking by providing contextual cues and argumentative reasoning in a\ntransparent, user-centered format. In a user study with 14 participants, 79%\nfound the debate mode more effective than standard chatbot interfaces, and the\nmultiple-source view received an average usefulness rating of 4.6 out of 5. Our\nfindings highlight the potential of context-rich, dialogic AI systems to\nimprove media literacy and foster trust in digital information environments. We\nargue that future tools for misinformation mitigation should prioritize ethical\ndesign, explainability, and interactive engagement to empower users in a\npost-truth era."}
{"id": "2506.22518", "pdf": "https://arxiv.org/pdf/2506.22518.pdf", "abs": "https://arxiv.org/abs/2506.22518", "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%."}
{"id": "2506.22941", "pdf": "https://arxiv.org/pdf/2506.22941.pdf", "abs": "https://arxiv.org/abs/2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 4 figures, with appendix", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem."}
{"id": "2506.22529", "pdf": "https://arxiv.org/pdf/2506.22529.pdf", "abs": "https://arxiv.org/abs/2506.22529", "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "categories": ["cs.CL"], "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms."}
{"id": "2506.22968", "pdf": "https://arxiv.org/pdf/2506.22968.pdf", "abs": "https://arxiv.org/abs/2506.22968", "title": "Against 'softmaxing' culture", "authors": ["Daniel Mwesigwa"], "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,\" and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking \"what is culture?\" at the start of system\nevaluations, I propose beginning with the question: \"when is culture?\" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture."}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598.pdf", "abs": "https://arxiv.org/abs/2506.22598", "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance."}
{"id": "2506.23016", "pdf": "https://arxiv.org/pdf/2506.23016.pdf", "abs": "https://arxiv.org/abs/2506.23016", "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "authors": ["Tomás Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "José Santos-Victor"], "categories": ["cs.HC", "cs.CV"], "comment": "13 pages, 5 figures", "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task."}
{"id": "2506.22623", "pdf": "https://arxiv.org/pdf/2506.22623.pdf", "abs": "https://arxiv.org/abs/2506.22623", "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method."}
{"id": "2506.23017", "pdf": "https://arxiv.org/pdf/2506.23017.pdf", "abs": "https://arxiv.org/abs/2506.23017", "title": "Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age", "authors": ["Noverah Khan", "Hira Eiraj Daud", "Suleman Shahid"], "categories": ["cs.HC"], "comment": null, "summary": "This paper addresses the critical issue of deceptive design elements\nprevalent in technology, and their potential impact on children. Recent\nresearch highlights the impact of dark patterns on adults and adolescents,\nwhile studies involving children are scarce. In an era where children wield\ngreater independence with digital devices, their vulnerability to dark patterns\namplifies without early education. Our findings show a significant positive\nimpact of dark pattern education on children's awareness, revealing that\nheightened awareness considerably alters children's navigation of social media,\nvideo games, and streaming platforms. To this end, we developed a gamified\napplication aimed at instructing children on identifying and responding to\nvarious dark patterns. Our evaluation results emphasize the critical role of\nearly education in empowering children to recognize and counter deceptive\ndesign, thereby cultivating a digitally literate generation capable of making\ninformed choices in the complex landscape of digital technology."}
{"id": "2506.22644", "pdf": "https://arxiv.org/pdf/2506.22644.pdf", "abs": "https://arxiv.org/abs/2506.22644", "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "categories": ["cs.CL", "cs.IR"], "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762."}
{"id": "2506.23075", "pdf": "https://arxiv.org/pdf/2506.23075.pdf", "abs": "https://arxiv.org/abs/2506.23075", "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "authors": ["Yuchen Zhou", "Jiamin Wu", "Zichen Ren", "Zhouheng Yao", "Weiheng Lu", "Kunyu Peng", "Qihao Zheng", "Chunfeng Song", "Wanli Ouyang", "Chao Gou"], "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research."}
{"id": "2506.22679", "pdf": "https://arxiv.org/pdf/2506.22679.pdf", "abs": "https://arxiv.org/abs/2506.22679", "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "categories": ["cs.CL"], "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data."}
{"id": "2506.23116", "pdf": "https://arxiv.org/pdf/2506.23116.pdf", "abs": "https://arxiv.org/abs/2506.23116", "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences", "authors": ["Wei Xu"], "categories": ["cs.HC"], "comment": null, "summary": "User experience (UX) practices have evolved in stages and are entering a\ntransformative phase (UX 3.0), driven by AI technologies and shifting user\nneeds. Human-centered AI (HCAI) experiences are emerging, necessitating new UX\napproaches to support UX practices in the AI era. We propose a UX 3.0 paradigm\nframework to respond and guide UX practices in developing HCAI systems."}
{"id": "2506.22694", "pdf": "https://arxiv.org/pdf/2506.22694.pdf", "abs": "https://arxiv.org/abs/2506.22694", "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct."}
{"id": "2506.23180", "pdf": "https://arxiv.org/pdf/2506.23180.pdf", "abs": "https://arxiv.org/abs/2506.23180", "title": "ImprovMate: Multimodal AI Assistant for Improv Actor Training", "authors": ["Riccardo Drago", "Yotam Sechayk", "Mustafa Doga Dogan", "Andrea Sanna", "Takeo Igarashi"], "categories": ["cs.HC", "H.5.0; H.5.2"], "comment": "ACM DIS '25", "summary": "Improvisation training for actors presents unique challenges, particularly in\nmaintaining narrative coherence and managing cognitive load during\nperformances. Previous research on AI in improvisation performance often\npredates advances in large language models (LLMs) and relies on human\nintervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate\nthe generation of narrative stimuli and cues, allowing actors to focus on\ncreativity without keeping track of plot or character continuity. Based on\ninsights from professional improvisers, ImprovMate incorporates exercises that\nmimic live training, such as abrupt story resolution and reactive thinking\nexercises, while maintaining coherence via reference tables. By balancing\nrandomness and structured guidance, ImprovMate provides a groundbreaking tool\nfor improv training. Our pilot study revealed that actors might embrace AI\ntechniques if the latter mirrors traditional practices, and appreciate the\nfresh twist introduced by our approach with the AI-generated cues."}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698.pdf", "abs": "https://arxiv.org/abs/2506.22698", "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "authors": ["Emily Dux Speltz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration."}
{"id": "2506.23253", "pdf": "https://arxiv.org/pdf/2506.23253.pdf", "abs": "https://arxiv.org/abs/2506.23253", "title": "Vibe coding: programming through conversation with artificial intelligence", "authors": ["Advait Sarkar", "Ian Drosos"], "categories": ["cs.HC"], "comment": null, "summary": "We examine \"vibe coding\": an emergent programming paradigm where developers\nprimarily write code by interacting with code-generating large language models\nrather than writing code directly. We analysed a curated set of videos\ndepicting extended vibe coding sessions with rich think-aloud reflections.\nUsing framework analysis, we investigated programmers' goals, workflows,\nprompting techniques, debugging approaches, and challenges encountered. We find\nthat vibe coding follows iterative goal satisfaction cycles where developers\nalternate between prompting AI, evaluating generated code through rapid\nscanning and application testing, and manual editing. Prompting strategies\nblend vague, high-level directives with detailed technical specifications.\nDebugging remains a hybrid process combining AI assistance with manual\npractices. Critically, vibe coding does not eliminate the need for programming\nexpertise but rather redistributes it toward context management, rapid code\nevaluation, and decisions about when to transition between AI-driven and manual\nmanipulation of code. Trust in AI tools during vibe coding is dynamic and\ncontextual, developed through iterative verification rather than blanket\nacceptance. Vibe coding is an evolution of AI-assisted programming that\nrepresents an early manifestation of \"material disengagement\", where\npractitioners orchestrate code production and manipulation, mediated through\nAI, while maintaining selective and strategic oversight."}
{"id": "2506.22724", "pdf": "https://arxiv.org/pdf/2506.22724.pdf", "abs": "https://arxiv.org/abs/2506.22724", "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs."}
{"id": "2506.23443", "pdf": "https://arxiv.org/pdf/2506.23443.pdf", "abs": "https://arxiv.org/abs/2506.23443", "title": "Accessible Data Access and Analysis by People who are Blind or Have Low Vision", "authors": ["Samuel Reinders", "Munazza Zaib", "Matthew Butler", "Bongshin Lee", "Ingrid Zukerman", "Lizhen Qu", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Poster presented at the 1st Workshop on Accessible Data\n  Visualization, IEEE VIS 2024", "summary": "Our work aims to develop new assistive technologies that enable blind or low\nvision (BLV) people to explore and analyze data readily. At present, barriers\nexist for BLV people to explore and analyze data, restricting access to\ngovernment, health and personal data, and limiting employment opportunities.\nThis work explores the co-design and development of an innovative system to\nsupport data access, with a focus on the use of refreshable tactile displays\n(RTDs) and conversational agents. The envisaged system will use a combination\nof tactile graphics and speech to communicate with BLV users, and proactively\nassist with data analysis tasks. As well as addressing significant equity gaps,\nour work expects to produce innovations in assistive technology, multimodal\ninterfaces, dialogue systems, and natural language understanding and\ngeneration."}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760.pdf", "abs": "https://arxiv.org/abs/2506.22760", "title": "Jan-nano Technical Report", "authors": ["Alan Dao", "Dinh Bach Vu"], "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy."}
{"id": "2506.23457", "pdf": "https://arxiv.org/pdf/2506.23457.pdf", "abs": "https://arxiv.org/abs/2506.23457", "title": "Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path", "authors": ["Yuya Ide", "Hailong Liu", "Takahiro Wada"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous personal mobility vehicles (APMVs) are small mobility devices\ndesigned for individual automated transportation in shared spaces. In such\nenvironments, frequent pedestrian avoidance maneuvers may cause rapid steering\nadjustments and passive postural responses from passengers, thereby increasing\nthe risk of motion sickness. This study investigated the effects of providing\npath information on 16 passengers' head movement behavior and motion sickness\nwhile riding an APMV. Through a controlled experiment comparing manual driving\n(MD), autonomous driving without path information (AD w/o path), and autonomous\ndriving with path information (AD w/ path), we found that providing path cues\nsignificantly reduced MISC scores and delayed the onset of motion sickness\nsymptoms. In addition, participants were more likely to proactively align their\nhead movements with the direction of vehicle rotation in both MD and AD w/ path\nconditions. Although a small correlation was observed between the delay in yaw\nrotation of the passenger's head relative to the vehicle and the occurrence of\nmotion sickness, the underlying physiological mechanism remains to be\nelucidated."}
{"id": "2506.22777", "pdf": "https://arxiv.org/pdf/2506.22777.pdf", "abs": "https://arxiv.org/abs/2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems."}
{"id": "2506.23458", "pdf": "https://arxiv.org/pdf/2506.23458.pdf", "abs": "https://arxiv.org/abs/2506.23458", "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs", "authors": ["Xiaoxiao Yang", "Chan Feng", "Jiancheng Chen"], "categories": ["cs.HC", "cs.LG"], "comment": "2 pages short paper", "summary": "Portable and wearable consumer-grade electroencephalography (EEG) devices,\nlike Muse headbands, offer unprecedented mobility for daily brain-computer\ninterface (BCI) applications, including cognitive load detection. However, the\nexacerbated non-stationarity in portable EEG signals constrains data fidelity\nand decoding accuracy, creating a fundamental trade-off between portability and\nperformance. To mitigate such limitation, we propose MuseCogNet (Muse-based\nCognitive Network), a unified joint learning framework integrating\nself-supervised and supervised training paradigms. In particular, we introduce\nan EEG-grounded self-supervised reconstruction loss based on average pooling to\ncapture robust neurophysiological patterns, while cross-entropy loss refines\ntask-specific cognitive discriminants. This joint learning framework resembles\nthe bottom-up and top-down attention in humans, enabling MuseCogNet to\nsignificantly outperform state-of-the-art methods on a publicly available Muse\ndataset and establish an implementable pathway for neurocognitive monitoring in\necological settings."}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791.pdf", "abs": "https://arxiv.org/abs/2506.22791", "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."}
{"id": "2506.23545", "pdf": "https://arxiv.org/pdf/2506.23545.pdf", "abs": "https://arxiv.org/abs/2506.23545", "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research", "authors": ["Barbara Karpowicz", "Maciej Grzeszczuk", "Adam Kuzdraliński", "Monika Kornacka", "Aliaksandr Marozau", "Wiktor Stawski", "Pavlo Zinevych", "Grzegorz Marcin Wójcik", "Tomasz Kowalewski", "Grzegorz Pochwatko", "Wiesław Kopeć"], "categories": ["cs.HC", "cs.CE"], "comment": "8 pages, 1 figure", "summary": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are\nincreasingly recognized for their applications in training, diagnostics, and\npsychological research, particularly in high-risk and highly regulated\nenvironments. In this panel we discuss how immersive systems enhance human\nperformance across multiple domains, including clinical psychology, space\nexploration, and medical education. In psychological research and training, XR\ncan offer a controlled yet ecologically valid setting for measuring cognitive\nand affective processes. In space exploration, we discuss the development of\nVR-based astronaut training and diagnostic systems, allowing astronauts to\nperform real-time health assessments. In medical education and rehabilitation,\nwe cover procedural training and patient engagement. From virtual surgical\nsimulations to gamified rehabilitation exercises, immersive environments\nenhance both learning outcomes and treatment adherence."}
{"id": "2506.22808", "pdf": "https://arxiv.org/pdf/2506.22808.pdf", "abs": "https://arxiv.org/abs/2506.22808", "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA."}
{"id": "2506.23678", "pdf": "https://arxiv.org/pdf/2506.23678.pdf", "abs": "https://arxiv.org/abs/2506.23678", "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models", "authors": ["Rock Yuren Pang", "K. J. Kevin Feng", "Shangbin Feng", "Chu Li", "Weijia Shi", "Yulia Tsvetkov", "Jeffrey Heer", "Katharina Reinecke"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses."}
{"id": "2506.22813", "pdf": "https://arxiv.org/pdf/2506.22813.pdf", "abs": "https://arxiv.org/abs/2506.22813", "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework."}
{"id": "2506.23694", "pdf": "https://arxiv.org/pdf/2506.23694.pdf", "abs": "https://arxiv.org/abs/2506.23694", "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons", "authors": ["Patrick Stadler", "Christopher Lazik", "Christopher Katins", "Thomas Kosch"], "categories": ["cs.HC"], "comment": null, "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask."}
{"id": "2506.22846", "pdf": "https://arxiv.org/pdf/2506.22846.pdf", "abs": "https://arxiv.org/abs/2506.22846", "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "authors": ["Duygu Altinok"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead."}
{"id": "2506.23815", "pdf": "https://arxiv.org/pdf/2506.23815.pdf", "abs": "https://arxiv.org/abs/2506.23815", "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "authors": ["Patrick Stokkink"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."}
{"id": "2506.22852", "pdf": "https://arxiv.org/pdf/2506.22852.pdf", "abs": "https://arxiv.org/abs/2506.22852", "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea."}
{"id": "2506.23850", "pdf": "https://arxiv.org/pdf/2506.23850.pdf", "abs": "https://arxiv.org/abs/2506.23850", "title": "Email as the Interface to Generative AI Models: Seamless Administrative Automation", "authors": ["Andres Navarro", "Carlos de Quinto", "José Alberto Hernández"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient."}
{"id": "2506.22853", "pdf": "https://arxiv.org/pdf/2506.22853.pdf", "abs": "https://arxiv.org/abs/2506.22853", "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/."}
{"id": "2506.23952", "pdf": "https://arxiv.org/pdf/2506.23952.pdf", "abs": "https://arxiv.org/abs/2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Bermúdez"], "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action."}
{"id": "2506.22858", "pdf": "https://arxiv.org/pdf/2506.22858.pdf", "abs": "https://arxiv.org/abs/2506.22858", "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "authors": ["Duygu Altinok"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks."}
{"id": "2506.24057", "pdf": "https://arxiv.org/pdf/2506.24057.pdf", "abs": "https://arxiv.org/abs/2506.24057", "title": "Access InContext: Futuring Accessible Prototyping Tools and Methods", "authors": ["Patricia Piedade", "Peter A Hayton", "Cynthia Bennett", "Anna R L Carter", "Clara Crivellaro", "Alan Dix", "Jess McGowan", "Katta Spiel", "Miriam Sturdee", "Garreth W. Tigwell", "Hugo Nicolau"], "categories": ["cs.HC"], "comment": null, "summary": "The popularity of accessibility research has grown recently, improving\ndigital inclusion for people with disabilities. However, researchers, including\nthose who have disabilities, have attempted to include people with disabilities\nin all aspects of design, and they have identified a myriad of practical\naccessibility barriers posed by tools and methods leveraged by human-computer\ninteraction (HCI) researchers during prototyping. To build a more inclusive\ntechnological landscape, we must question the effectiveness of existing\nprototyping tools and methods, repurpose/retrofit existing resources, and build\nnew tools and methods to support the participation of both researchers and\npeople with disabilities within the prototyping design process of novel\ntechnologies. This full-day workshop at CHI 2025 will provide a platform for\nHCI researchers, designers, and practitioners to discuss barriers and\nopportunities for creating accessible prototyping and promote hands-on ideation\nand fabrication exercises aimed at futuring accessible prototyping."}
{"id": "2506.22957", "pdf": "https://arxiv.org/pdf/2506.22957.pdf", "abs": "https://arxiv.org/abs/2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM."}
{"id": "2506.24104", "pdf": "https://arxiv.org/pdf/2506.24104.pdf", "abs": "https://arxiv.org/abs/2506.24104", "title": "Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities", "authors": ["Mariia Ershova", "Graziano Blasilli"], "categories": ["cs.HC"], "comment": "Submitted to: Workshop on Visual Analytics in Healthcare (VAHC 2025)", "summary": "Digital twins (DT) are increasingly used in healthcare to model patients,\nprocesses, and physiological systems. While recent solutions leverage\nvisualization, visual analytics, and user interaction, these systems rarely\nincorporate structured service design methodologies. Bridging service design\nwith visual analytics and visualization can be valuable for the healthcare DT\ncommunity. This paper aims to introduce the service design discipline to\nvisualization researchers by framing this integration gap and suggesting\nresearch directions to enhance the real-world applicability of DT solutions."}
{"id": "2506.22977", "pdf": "https://arxiv.org/pdf/2506.22977.pdf", "abs": "https://arxiv.org/abs/2506.22977", "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "categories": ["cs.CL", "cs.LG"], "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task."}
{"id": "2506.22443", "pdf": "https://arxiv.org/pdf/2506.22443.pdf", "abs": "https://arxiv.org/abs/2506.22443", "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "categories": ["cs.LG", "cs.HC"], "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems."}
{"id": "2506.22978", "pdf": "https://arxiv.org/pdf/2506.22978.pdf", "abs": "https://arxiv.org/abs/2506.22978", "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs."}
{"id": "2506.22462", "pdf": "https://arxiv.org/pdf/2506.22462.pdf", "abs": "https://arxiv.org/abs/2506.22462", "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living."}
{"id": "2506.23046", "pdf": "https://arxiv.org/pdf/2506.23046.pdf", "abs": "https://arxiv.org/abs/2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions."}
{"id": "2506.22464", "pdf": "https://arxiv.org/pdf/2506.22464.pdf", "abs": "https://arxiv.org/abs/2506.22464", "title": "Golden Ratio Assisted Localization for Wireless Sensor Network", "authors": ["Hitesh Mohapatra"], "categories": ["cs.NI", "cs.HC", "B.4"], "comment": "6", "summary": "This paper presents a novel localization algorithm for wireless sensor\nnetworks (WSNs) called Golden Ratio Localization (GRL), which leverages the\nmathematical properties of the golden ratio (phi 1.618) to optimize both node\nplacement and communication range. GRL introduces phi-based anchor node\ndeployment and hop-sensitive weighting using phi-exponents to improve\nlocalization accuracy while minimizing energy consumption. Through extensive\nsimulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10\nanchors, GRL achieved an average localization error of 2.35 meters,\noutperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of\nenergy efficiency, GRL reduced localization energy consumption to 1.12 microJ\nper node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.\nThese results confirm that GRL provides a more balanced and efficient\nlocalization approach, making it especially suitable for energy-constrained and\nlarge-scale WSN deployments."}
{"id": "2506.23051", "pdf": "https://arxiv.org/pdf/2506.23051.pdf", "abs": "https://arxiv.org/abs/2506.23051", "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "authors": ["João Lucas Luz Lima Sarcinelli", "Marina Lages Gonçalves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset."}
{"id": "2506.22476", "pdf": "https://arxiv.org/pdf/2506.22476.pdf", "abs": "https://arxiv.org/abs/2506.22476", "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "comment": null, "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states."}
{"id": "2506.23056", "pdf": "https://arxiv.org/pdf/2506.23056.pdf", "abs": "https://arxiv.org/abs/2506.23056", "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE."}
{"id": "2506.22604", "pdf": "https://arxiv.org/pdf/2506.22604.pdf", "abs": "https://arxiv.org/abs/2506.22604", "title": "Bootstrapping Human-Like Planning via LLMs", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance."}
{"id": "2506.23071", "pdf": "https://arxiv.org/pdf/2506.23071.pdf", "abs": "https://arxiv.org/abs/2506.23071", "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "categories": ["cs.CL"], "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL."}
{"id": "2506.22803", "pdf": "https://arxiv.org/pdf/2506.22803.pdf", "abs": "https://arxiv.org/abs/2506.22803", "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU."}
{"id": "2506.23101", "pdf": "https://arxiv.org/pdf/2506.23101.pdf", "abs": "https://arxiv.org/abs/2506.23101", "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "authors": ["Yue Xu", "Wenjie Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation."}
{"id": "2506.22893", "pdf": "https://arxiv.org/pdf/2506.22893.pdf", "abs": "https://arxiv.org/abs/2506.22893", "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users."}
{"id": "2506.23111", "pdf": "https://arxiv.org/pdf/2506.23111.pdf", "abs": "https://arxiv.org/abs/2506.23111", "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context."}
{"id": "2506.23092", "pdf": "https://arxiv.org/pdf/2506.23092.pdf", "abs": "https://arxiv.org/abs/2506.23092", "title": "Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics", "authors": ["Arisa Cowe", "Tyson Neuroth", "Qi Wu", "Martin Rieth", "Jacqueline Chen", "Myoungkyu Lee", "Kwan-Liu Ma"], "categories": ["cs.GR", "cs.HC"], "comment": "15 pages (13 pages without references)", "summary": "Many scientific and engineering problems involving multi-physics span a wide\nrange of scales. Understanding the interactions across these scales is\nessential for fully comprehending such complex problems. However, visualizing\nmultivariate, multiscale data within an integrated view where correlations\nacross space, scales, and fields are easily perceived remains challenging. To\naddress this, we introduce a novel local spatial statistical visualization of\nflow fields across multiple fields and turbulence scales. Our method leverages\nthe curvelet transform for scale decomposition of fields of interest, a\nlevel-set-restricted centroidal Voronoi tessellation to partition the spatial\ndomain into local regions for statistical aggregation, and a set of glyph\ndesigns that combines information across scales and fields into a single, or\nreduced set of perceivable visual representations. Each glyph represents data\naggregated within a Voronoi region and is positioned at the Voronoi site for\ndirect visualization in a 3D view centered around flow features of interest. We\nimplement and integrate our method into an interactive visualization system\nwhere the glyph-based technique operates in tandem with linked 3D spatial views\nand 2D statistical views, supporting a holistic analysis. We demonstrate with\ncase studies visualizing turbulent combustion data--multi-scalar compressible\nflows--and turbulent incompressible channel flow data. This new capability\nenables scientists to better understand the interactions between multiple\nfields and length scales in turbulent flows."}
{"id": "2506.23122", "pdf": "https://arxiv.org/pdf/2506.23122.pdf", "abs": "https://arxiv.org/abs/2506.23122", "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content."}
{"id": "2506.23549", "pdf": "https://arxiv.org/pdf/2506.23549.pdf", "abs": "https://arxiv.org/abs/2506.23549", "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios."}
{"id": "2506.23127", "pdf": "https://arxiv.org/pdf/2506.23127.pdf", "abs": "https://arxiv.org/abs/2506.23127", "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization."}
{"id": "2506.23682", "pdf": "https://arxiv.org/pdf/2506.23682.pdf", "abs": "https://arxiv.org/abs/2506.23682", "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?", "authors": ["Maysara Alhindi", "Joseph Hallett"], "categories": ["cs.CR", "cs.AR", "cs.HC"], "comment": null, "summary": "A digital security-by-design computer architecture, like CHERI, lets you\nprogram without fear of buffer overflows or other memory safety errors, but\nCHERI also rewrites some of the assumptions about how C works and how\nfundamental types (such as pointers) are implemented in hardware. We conducted\na usability study to examine how developers react to the changes required by\nCHERI when porting software to run on it. We find that developers struggle with\nCHERI's display of warnings and errors and a lack of diverse documentation."}
{"id": "2506.23133", "pdf": "https://arxiv.org/pdf/2506.23133.pdf", "abs": "https://arxiv.org/abs/2506.23133", "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness."}
{"id": "2506.23721", "pdf": "https://arxiv.org/pdf/2506.23721.pdf", "abs": "https://arxiv.org/abs/2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings."}
{"id": "2506.23136", "pdf": "https://arxiv.org/pdf/2506.23136.pdf", "abs": "https://arxiv.org/abs/2506.23136", "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "categories": ["cs.CL"], "comment": "29 Pages, 11 Tables", "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context."}
{"id": "2506.23739", "pdf": "https://arxiv.org/pdf/2506.23739.pdf", "abs": "https://arxiv.org/abs/2506.23739", "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen Müller"], "categories": ["cs.RO", "cs.CE", "cs.HC"], "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments."}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137.pdf", "abs": "https://arxiv.org/abs/2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results."}
{"id": "2506.23774", "pdf": "https://arxiv.org/pdf/2506.23774.pdf", "abs": "https://arxiv.org/abs/2506.23774", "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management", "authors": ["Ewelina Gajewska", "Michal Wawer", "Katarzyna Budzynska", "Jarosław A. Chudziak"], "categories": ["cs.CY", "cs.HC", "H.1.2"], "comment": "8 pages, 1 figure", "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings."}
{"id": "2506.23139", "pdf": "https://arxiv.org/pdf/2506.23139.pdf", "abs": "https://arxiv.org/abs/2506.23139", "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation."}
{"id": "2506.23826", "pdf": "https://arxiv.org/pdf/2506.23826.pdf", "abs": "https://arxiv.org/abs/2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Lluís C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "comment": "24 pages, 9 figures", "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs."}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146.pdf", "abs": "https://arxiv.org/abs/2506.23146", "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success."}
{"id": "2506.23851", "pdf": "https://arxiv.org/pdf/2506.23851.pdf", "abs": "https://arxiv.org/abs/2506.23851", "title": "Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan", "authors": ["Israel Fianyi", "Soonja Yeom", "Ju-Hyun Shin"], "categories": ["cs.CY", "cs.ET", "cs.HC"], "comment": null, "summary": "The integration of cloud computing in education can revolutionise learning in\nadvanced (Australia & South Korea) and middle-income (Ghana & Nigeria)\ncountries, while offering scalable, cost-effective and equitable access to\nadaptive learning systems. This paper explores how cloud computing and adaptive\nlearning technologies are deployed across different socio-economic and\ninfrastructure contexts. The study identifies enabling factors and systematic\nchallenges, providing insights into how cloud-based education can be tailored\nto bridge the digital and educational divide globally."}
{"id": "2506.23149", "pdf": "https://arxiv.org/pdf/2506.23149.pdf", "abs": "https://arxiv.org/abs/2506.23149", "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis."}
{"id": "2506.24039", "pdf": "https://arxiv.org/pdf/2506.24039.pdf", "abs": "https://arxiv.org/abs/2506.24039", "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "categories": ["cs.CV", "cs.HC"], "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging."}
{"id": "2506.23192", "pdf": "https://arxiv.org/pdf/2506.23192.pdf", "abs": "https://arxiv.org/abs/2506.23192", "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SIGIR'23", "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext."}
{"id": "2506.24046", "pdf": "https://arxiv.org/pdf/2506.24046.pdf", "abs": "https://arxiv.org/abs/2506.24046", "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation."}
{"id": "2506.23235", "pdf": "https://arxiv.org/pdf/2506.23235.pdf", "abs": "https://arxiv.org/abs/2506.23235", "title": "Generalist Reward Models: Found Inside Large Language Models", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models."}
{"id": "2503.18243", "pdf": "https://arxiv.org/pdf/2503.18243.pdf", "abs": "https://arxiv.org/abs/2503.18243", "title": "A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal", "authors": ["Guy Laban", "Julie Wang", "Hatice Gunes"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Emotion regulation is a crucial skill for managing emotions in everyday life,\nyet finding a constructive and accessible method to support these processes\nremains challenging due to their cognitive demands. In this study, we explore\nhow regular interactions with a social robot, conducted in a structured yet\nfamiliar environment within university halls and departments, can provide\neffective support for emotion regulation through cognitive reappraisal.\nTwenty-one students participated in a five-session study at a university hall\nor department, where the robot, powered by a large language model (GPT-3.5),\nfacilitated structured conversations, encouraging the students to reinterpret\nemotionally charged situations they shared with the robot. Quantitative and\nqualitative results indicate significant improvements in emotion\nself-regulation, with participants reporting better understanding and control\nof their emotions. The intervention led to significant changes in constructive\nemotion regulation tendencies and positive effects on mood and sentiment after\neach session. The findings also demonstrate that repeated interactions with the\nrobot encouraged greater emotional expressiveness, including longer speech\ndisclosures, increased use of affective language, and heightened facial\narousal. Notably, expressiveness followed structured patterns aligned with the\nreappraisal process, with expression peaking during key reappraisal moments,\nparticularly when participants were prompted to reinterpret negative\nexperiences. The qualitative feedback further highlighted how the robot\nfostered introspection and provided a supportive space for discussing emotions,\nenabling participants to confront long-avoided emotional challenges. These\nfindings demonstrate the potential of robots to effectively assist in emotion\nregulation in familiar environments, offering both emotional support and\ncognitive guidance."}
{"id": "2506.23288", "pdf": "https://arxiv.org/pdf/2506.23288.pdf", "abs": "https://arxiv.org/abs/2506.23288", "title": "Two Spelling Normalization Approaches Based on Large Language Models", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "categories": ["cs.CL"], "comment": null, "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task."}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677.pdf", "abs": "https://arxiv.org/abs/2504.17677", "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted author version for the D-SAIL Workshop - Transformative\n  Curriculum Design: Digitalisation, Sustainability, and AI Literacy for 21st\n  Century Learning, July 22, 2025, Palermo, Italy", "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience."}
{"id": "2506.23293", "pdf": "https://arxiv.org/pdf/2506.23293.pdf", "abs": "https://arxiv.org/abs/2506.23293", "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "authors": ["P. Myles Eugenio"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models."}
{"id": "2506.07193", "pdf": "https://arxiv.org/pdf/2506.07193.pdf", "abs": "https://arxiv.org/abs/2506.07193", "title": "earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor", "authors": ["Tobias King", "Michael Knierim", "Philipp Lepold", "Christopher Clarke", "Hans Gellersen", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.HC"], "comment": "12 pages", "summary": "Eye tracking technology is frequently utilized to diagnose eye and\nneurological disorders, assess sleep and fatigue, study human visual\nperception, and enable novel gaze-based interaction methods. However,\ntraditional eye tracking methodologies are constrained by bespoke hardware that\nis often cumbersome to wear, complex to apply, and demands substantial\ncomputational resources. To overcome these limitations, we investigated\nElectrooculography (EOG) eye tracking using 14 electrodes positioned around the\nears, integrated into a custom-built headphone form factor device. In a\ncontrolled experiment, 16 participants tracked stimuli designed to induce\nsmooth pursuits and saccades. Data analysis identified optimal electrode pairs\nfor vertical and horizontal eye movement tracking, benchmarked against\ngold-standard EOG and camera-based methods. The electrode montage nearest the\neyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG\nshowed high correlation with gold-standard measures ($r_{\\mathrm{EOG}} = 0.81,\np = 0.01$; $r_{\\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were\nweakly correlated ($r_{\\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\\mathrm{CAM}} =\n0.35, p = 0.05$). Voltage deflections when performing saccades showed strong\ncorrelation in the horizontal direction ($r_{\\mathrm{left}} = 0.99, p = 0.0$;\n$r_{\\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical\ndirection ($r_{\\mathrm{up}} = 0.6, p = 0.23$; $r_{\\mathrm{down}} = 0.19, p =\n0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating\nits potential effectiveness, while vertical earEOG results were poor,\nsuggesting limited feasibility in our current setup."}
{"id": "2506.23315", "pdf": "https://arxiv.org/pdf/2506.23315.pdf", "abs": "https://arxiv.org/abs/2506.23315", "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively."}
{"id": "2506.19107", "pdf": "https://arxiv.org/pdf/2506.19107.pdf", "abs": "https://arxiv.org/abs/2506.19107", "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education", "authors": ["Ruiwei Xiao", "Xinying Hou", "Runlong Ye", "Majeed Kazemitabaar", "Nicholas Diana", "Michael Liut", "John Stamper"], "categories": ["cs.HC", "cs.AI"], "comment": "Under review for Elsevier Journal. Journal policy allows submitting\n  as preprint", "summary": "With the proliferation of large language model (LLM) applications since 2022,\ntheir use in education has sparked both excitement and concern. Recent studies\nconsistently highlight students' (mis)use of LLMs can hinder learning outcomes.\nThis work aims to teach students how to effectively prompt LLMs to improve\ntheir learning. We first proposed pedagogical prompting, a\ntheoretically-grounded new concept to elicit learning-oriented responses from\nLLMs. To move from concept design to a proof-of-concept learning intervention\nin real educational settings, we selected early undergraduate CS education\n(CS1/CS2) as the example context. We began with a formative survey study with\ninstructors (N=36) teaching early-stage undergraduate-level CS courses to\ninform the instructional design based on classroom needs. Based on their\ninsights, we designed and developed a learning intervention through an\ninteractive system with scenario-based instruction to train pedagogical\nprompting skills. Finally, we evaluated its instructional effectiveness through\na user study with CS novice students (N=22) using pre/post-tests. Through mixed\nmethods analyses, our results indicate significant improvements in learners'\nLLM-based pedagogical help-seeking skills, along with positive attitudes toward\nthe system and increased willingness to use pedagogical prompts in the future.\nOur contributions include (1) a theoretical framework of pedagogical prompting;\n(2) empirical insights into current instructor attitudes toward pedagogical\nprompting; and (3) a learning intervention design with an interactive learning\ntool and scenario-based instruction leading to promising results on teaching\nLLM-based help-seeking. Our approach is scalable for broader implementation in\nclassrooms and has the potential to be integrated into tools like ChatGPT as an\non-boarding experience to encourage learning-oriented use of generative AI."}
{"id": "2506.23340", "pdf": "https://arxiv.org/pdf/2506.23340.pdf", "abs": "https://arxiv.org/abs/2506.23340", "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages."}
{"id": "2206.00535", "pdf": "https://arxiv.org/pdf/2206.00535.pdf", "abs": "https://arxiv.org/abs/2206.00535", "title": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines", "authors": ["Camilo Fosco", "Emilie Josephs", "Alex Andonian", "Aude Oliva"], "categories": ["cs.CV", "cs.HC", "cs.SI"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Deepfakes can fuel online misinformation. As deepfakes get harder to\nrecognize with the naked eye, human users become more reliant on deepfake\ndetection models to help them decide whether a video is real or fake.\nCurrently, models yield a prediction for a video's authenticity, but do not\nintegrate a method for alerting a human user. We introduce a framework for\namplifying artifacts in deepfake videos to make them more detectable by people.\nWe propose a novel, semi-supervised Artifact Attention module, which is trained\non human responses to create attention maps that highlight video artifacts, and\nmagnify them to create a novel visual indicator we call \"Deepfake Caricatures\".\nIn a user study, we demonstrate that Caricatures greatly increase human\ndetection, across video presentation times and user engagement levels. We also\nintroduce a deepfake detection model that incorporates the Artifact Attention\nmodule to increase its accuracy and robustness. Overall, we demonstrate the\nsuccess of a human-centered approach to designing deepfake mitigation methods."}
{"id": "2506.23342", "pdf": "https://arxiv.org/pdf/2506.23342.pdf", "abs": "https://arxiv.org/abs/2506.23342", "title": "ATGen: A Framework for Active Text Generation", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 System Demonstrations", "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group"}
{"id": "2409.01754", "pdf": "https://arxiv.org/pdf/2409.01754.pdf", "abs": "https://arxiv.org/abs/2409.01754", "title": "Empirical evidence of Large Language Model's influence on human spoken communication", "authors": ["Hiromu Yakura", "Ezequiel Lopez-Lopez", "Levin Brinkmann", "Ignacio Serna", "Prateek Gupta", "Ivan Soraperra", "Iyad Rahwan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."}
{"id": "2506.23377", "pdf": "https://arxiv.org/pdf/2506.23377.pdf", "abs": "https://arxiv.org/abs/2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective."}
{"id": "2411.15240", "pdf": "https://arxiv.org/pdf/2411.15240.pdf", "abs": "https://arxiv.org/abs/2411.15240", "title": "Foundation Models for Wearable Movement Data in Mental Health Research", "authors": ["Franklin Y. Ruan", "Aiwei Zhang", "Jenny Y. Oh", "SouYoung Jin", "Nicholas C. Jacobson"], "categories": ["cs.LG", "cs.AI", "cs.HC", "q-bio.QM"], "comment": null, "summary": "Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/"}
{"id": "2506.23393", "pdf": "https://arxiv.org/pdf/2506.23393.pdf", "abs": "https://arxiv.org/abs/2506.23393", "title": "Hierarchical Memory Organization for Wikipedia Generation", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios."}
{"id": "2506.00924", "pdf": "https://arxiv.org/pdf/2506.00924.pdf", "abs": "https://arxiv.org/abs/2506.00924", "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison", "authors": ["Parsa Hassani Shariat Panahi", "Amir Hossein Jalilvand", "M. Hassan Najafi"], "categories": ["cs.NI", "cs.AI", "cs.HC"], "comment": "19 ppages, 13 figures", "summary": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates."}
{"id": "2506.23411", "pdf": "https://arxiv.org/pdf/2506.23411.pdf", "abs": "https://arxiv.org/abs/2506.23411", "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community."}
{"id": "2506.21490", "pdf": "https://arxiv.org/pdf/2506.21490.pdf", "abs": "https://arxiv.org/abs/2506.21490", "title": "Ad-Hoc Human-AI Coordination Challenge", "authors": ["Tin Dizdarević", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Darius Muglich", "Johannes Forkel", "Jakob Nicolaus Foerster"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": "Published at ICML 2025", "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}."}
{"id": "2506.23423", "pdf": "https://arxiv.org/pdf/2506.23423.pdf", "abs": "https://arxiv.org/abs/2506.23423", "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "authors": ["Felipe Nuti", "Tim Franzmeyer", "João Henriques"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa."}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431.pdf", "abs": "https://arxiv.org/abs/2506.23431", "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption."}
{"id": "2506.23463", "pdf": "https://arxiv.org/pdf/2506.23463.pdf", "abs": "https://arxiv.org/abs/2506.23463", "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "authors": ["Jang Won June"], "categories": ["cs.CL", "I.2.7"], "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks."}
{"id": "2506.23485", "pdf": "https://arxiv.org/pdf/2506.23485.pdf", "abs": "https://arxiv.org/abs/2506.23485", "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA."}
{"id": "2506.23508", "pdf": "https://arxiv.org/pdf/2506.23508.pdf", "abs": "https://arxiv.org/abs/2506.23508", "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages (Preprint. Work in progress)", "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels."}
{"id": "2506.23524", "pdf": "https://arxiv.org/pdf/2506.23524.pdf", "abs": "https://arxiv.org/abs/2506.23524", "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC."}
{"id": "2506.23527", "pdf": "https://arxiv.org/pdf/2506.23527.pdf", "abs": "https://arxiv.org/abs/2506.23527", "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?", "authors": ["Jan Kvapil", "Martin Fajcik"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities."}
{"id": "2506.23601", "pdf": "https://arxiv.org/pdf/2506.23601.pdf", "abs": "https://arxiv.org/abs/2506.23601", "title": "Semantic-guided Diverse Decoding for Large Language Model", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%."}
{"id": "2506.23610", "pdf": "https://arxiv.org/pdf/2506.23610.pdf", "abs": "https://arxiv.org/abs/2506.23610", "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "categories": ["cs.CL", "cs.CY"], "comment": "pre-print version - paper actually under submission", "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents."}
{"id": "2506.23661", "pdf": "https://arxiv.org/pdf/2506.23661.pdf", "abs": "https://arxiv.org/abs/2506.23661", "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "categories": ["cs.CL"], "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack"}
{"id": "2506.23662", "pdf": "https://arxiv.org/pdf/2506.23662.pdf", "abs": "https://arxiv.org/abs/2506.23662", "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "authors": ["Philip Lippmann", "Jie Yang"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments."}
{"id": "2506.23667", "pdf": "https://arxiv.org/pdf/2506.23667.pdf", "abs": "https://arxiv.org/abs/2506.23667", "title": "L0: Reinforcement Learning to Become General Agents", "authors": ["Junjie Zhang", "Jingyi Xi", "Zhuoyang Song", "Junyu Lu", "Yuhua Ke", "Ting Sun", "Yukun Yang", "Jiaxing Zhang", "Songxin Zhang", "Zejian Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0)."}
{"id": "2506.23735", "pdf": "https://arxiv.org/pdf/2506.23735.pdf", "abs": "https://arxiv.org/abs/2506.23735", "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "authors": ["JiaRu Wu", "Mingwei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval."}
{"id": "2506.23743", "pdf": "https://arxiv.org/pdf/2506.23743.pdf", "abs": "https://arxiv.org/abs/2506.23743", "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "categories": ["cs.CL"], "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct."}
{"id": "2506.23840", "pdf": "https://arxiv.org/pdf/2506.23840.pdf", "abs": "https://arxiv.org/abs/2506.23840", "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model."}
{"id": "2506.23864", "pdf": "https://arxiv.org/pdf/2506.23864.pdf", "abs": "https://arxiv.org/abs/2506.23864", "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It", "authors": ["Seyed Mahed Mousavi", "Edoardo Cecchinato", "Lucia Hornikova", "Giuseppe Riccardi"], "categories": ["cs.CL"], "comment": null, "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning."}
{"id": "2506.23888", "pdf": "https://arxiv.org/pdf/2506.23888.pdf", "abs": "https://arxiv.org/abs/2506.23888", "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting", "authors": ["André de Souza Loureiro", "Jorge Valverde-Rebaza", "Julieta Noguez", "David Escarcega", "Ricardo Marcacini"], "categories": ["cs.CL"], "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance."}
{"id": "2506.23921", "pdf": "https://arxiv.org/pdf/2506.23921.pdf", "abs": "https://arxiv.org/abs/2506.23921", "title": "The Trilemma of Truth in Large Language Models", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge."}
{"id": "2506.23929", "pdf": "https://arxiv.org/pdf/2506.23929.pdf", "abs": "https://arxiv.org/abs/2506.23929", "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies", "authors": ["Mohammed J. Saeed", "Tommi Vehvilainen", "Evgeny Fedoseev", "Sevil Caliskan", "Tatiana Vodolazova"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework."}
{"id": "2506.23930", "pdf": "https://arxiv.org/pdf/2506.23930.pdf", "abs": "https://arxiv.org/abs/2506.23930", "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time."}
{"id": "2506.23940", "pdf": "https://arxiv.org/pdf/2506.23940.pdf", "abs": "https://arxiv.org/abs/2506.23940", "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs."}
{"id": "2506.23951", "pdf": "https://arxiv.org/pdf/2506.23951.pdf", "abs": "https://arxiv.org/abs/2506.23951", "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders", "authors": ["Mathis Le Bail", "Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features."}
{"id": "2506.23979", "pdf": "https://arxiv.org/pdf/2506.23979.pdf", "abs": "https://arxiv.org/abs/2506.23979", "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation", "authors": ["Renren Jin", "Tianhao Shen", "Xinwei Wu", "Dan Shi", "Haoran Sun", "Wuwei Huang", "Quandong Wang", "Wei Liu", "Jian Luan", "Bin Wang", "Deyi Xiong"], "categories": ["cs.CL"], "comment": "33 pages, 15 tables, 11 figures", "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger."}
{"id": "2506.23990", "pdf": "https://arxiv.org/pdf/2506.23990.pdf", "abs": "https://arxiv.org/abs/2506.23990", "title": "Machine Understanding of Scientific Language", "authors": ["Dustin Wright"], "categories": ["cs.CL", "cs.LG"], "comment": "PhD Thesis, 210 pages", "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process"}
{"id": "2506.23998", "pdf": "https://arxiv.org/pdf/2506.23998.pdf", "abs": "https://arxiv.org/abs/2506.23998", "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "categories": ["cs.CL"], "comment": "Presented at ACL 2025 SRW", "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts."}
{"id": "2506.24006", "pdf": "https://arxiv.org/pdf/2506.24006.pdf", "abs": "https://arxiv.org/abs/2506.24006", "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Seßler", "Brian Greer", "Lieven Verschaffel"], "categories": ["cs.CL", "math.HO"], "comment": null, "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms."}
{"id": "2506.24016", "pdf": "https://arxiv.org/pdf/2506.24016.pdf", "abs": "https://arxiv.org/abs/2506.24016", "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025 Findings", "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT."}
{"id": "2506.24068", "pdf": "https://arxiv.org/pdf/2506.24068.pdf", "abs": "https://arxiv.org/abs/2506.24068", "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."}
{"id": "2506.24106", "pdf": "https://arxiv.org/pdf/2506.24106.pdf", "abs": "https://arxiv.org/abs/2506.24106", "title": "On the Predictive Power of Representation Dispersion in Language Models", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach."}
{"id": "2506.24117", "pdf": "https://arxiv.org/pdf/2506.24117.pdf", "abs": "https://arxiv.org/abs/2506.24117", "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models", "authors": ["David M. Smiley"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies."}
{"id": "2506.22449", "pdf": "https://arxiv.org/pdf/2506.22449.pdf", "abs": "https://arxiv.org/abs/2506.22449", "title": "Computational Analysis of Climate Policy", "authors": ["Carolyn Hicks"], "categories": ["cs.CY", "cs.CL"], "comment": "Master's thesis", "summary": "This thesis explores the impact of the Climate Emergency movement on local\ngovernment climate policy, using computational methods. The Climate Emergency\nmovement sought to accelerate climate action at local government level through\nthe mechanism of Climate Emergency Declarations (CEDs), resulting in a series\nof commitments from councils to treat climate change as an emergency. With the\naim of assessing the potential of current large language models to answer\ncomplex policy questions, I first built and configured a system named PALLM\n(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.\nThis system is designed to apply a conceptual framework for climate emergency\nresponse plans to a dataset of climate policy documents. I validated the\nperformance of this system with the help of local government policymakers, by\ngenerating analyses of the climate policies of 11 local governments in Victoria\nand assessing the policymakers' level of agreement with PALLM's responses.\nHaving established that PALLM's performance is satisfactory, I used it to\nconduct a large-scale analysis of current policy documents from local\ngovernments in the state of Victoria, Australia. This thesis presents the\nmethodology and results of this analysis, comparing the results for councils\nwhich have passed a CED to those which did not. This study finds that GPT-4 is\ncapable of high-level policy analysis, with limitations including a lack of\nreliable attribution, and can also enable more nuanced analysis by researchers.\nIts use in this research shows that councils which have passed a CED are more\nlikely to have a recent and climate-specific policy, and show more attention to\nurgency, prioritisation, and equity and social justice, than councils which\nhave not. It concludes that the ability to assess policy documents at scale\nopens up exciting new opportunities for policy researchers."}
{"id": "2506.22481", "pdf": "https://arxiv.org/pdf/2506.22481.pdf", "abs": "https://arxiv.org/abs/2506.22481", "title": "Theories of \"Sexuality\" in Natural Language Processing Bias Research", "authors": ["Jacob Hobbs"], "categories": ["cs.CY", "cs.CL"], "comment": "17 pages, 9 tables, undergraduate senior thesis, submitted to The\n  Spectra: The Virginia Engineering and Science Research Journal", "summary": "In recent years, significant advancements in the field of Natural Language\nProcessing (NLP) have positioned commercialized language models as\nwide-reaching, highly useful tools. In tandem, there has been an explosion of\nmultidisciplinary research examining how NLP tasks reflect, perpetuate, and\namplify social biases such as gender and racial bias. A significant gap in this\nscholarship is a detailed analysis of how queer sexualities are encoded and\n(mis)represented by both NLP systems and practitioners. Following previous work\nin the field of AI fairness, we document how sexuality is defined and\noperationalized via a survey and analysis of 55 articles that quantify\nsexuality-based NLP bias. We find that sexuality is not clearly defined in a\nmajority of the literature surveyed, indicating a reliance on assumed or\nnormative conceptions of sexual/romantic practices and identities. Further, we\nfind that methods for extracting biased outputs from NLP technologies often\nconflate gender and sexual identities, leading to monolithic conceptions of\nqueerness and thus improper quantifications of bias. With the goal of improving\nsexuality-based NLP bias analyses, we conclude with recommendations that\nencourage more thorough engagement with both queer communities and\ninterdisciplinary literature."}
{"id": "2506.22493", "pdf": "https://arxiv.org/pdf/2506.22493.pdf", "abs": "https://arxiv.org/abs/2506.22493", "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs."}
{"id": "2506.22496", "pdf": "https://arxiv.org/pdf/2506.22496.pdf", "abs": "https://arxiv.org/abs/2506.22496", "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety", "authors": ["Y. Du"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems."}
{"id": "2506.22666", "pdf": "https://arxiv.org/pdf/2506.22666.pdf", "abs": "https://arxiv.org/abs/2506.22666", "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "authors": ["Anamika Lochab", "Lu Yan", "Patrick Pynadath", "Xiangyu Zhang", "Ruqi Zhang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation."}
{"id": "2506.22696", "pdf": "https://arxiv.org/pdf/2506.22696.pdf", "abs": "https://arxiv.org/abs/2506.22696", "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "authors": ["Brian Mak", "Jeffrey Flanigan"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer."}
{"id": "2506.22716", "pdf": "https://arxiv.org/pdf/2506.22716.pdf", "abs": "https://arxiv.org/abs/2506.22716", "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor Rühle"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop."}
{"id": "2506.22783", "pdf": "https://arxiv.org/pdf/2506.22783.pdf", "abs": "https://arxiv.org/abs/2506.22783", "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution."}
{"id": "2506.22809", "pdf": "https://arxiv.org/pdf/2506.22809.pdf", "abs": "https://arxiv.org/abs/2506.22809", "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "authors": ["Cooper Doyle"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making."}
{"id": "2506.22864", "pdf": "https://arxiv.org/pdf/2506.22864.pdf", "abs": "https://arxiv.org/abs/2506.22864", "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICMR 2025", "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods."}
{"id": "2506.22900", "pdf": "https://arxiv.org/pdf/2506.22900.pdf", "abs": "https://arxiv.org/abs/2506.22900", "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR."}
{"id": "2506.22992", "pdf": "https://arxiv.org/pdf/2506.22992.pdf", "abs": "https://arxiv.org/abs/2506.22992", "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbić", "Michael Moor"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps."}
{"id": "2506.23049", "pdf": "https://arxiv.org/pdf/2506.23049.pdf", "abs": "https://arxiv.org/abs/2506.23049", "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks."}
{"id": "2506.23115", "pdf": "https://arxiv.org/pdf/2506.23115.pdf", "abs": "https://arxiv.org/abs/2506.23115", "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Homepage: https://haon-chen.github.io/MoCa/", "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB."}
{"id": "2506.23219", "pdf": "https://arxiv.org/pdf/2506.23219.pdf", "abs": "https://arxiv.org/abs/2506.23219", "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ICCV 2025", "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA."}
{"id": "2506.23225", "pdf": "https://arxiv.org/pdf/2506.23225.pdf", "abs": "https://arxiv.org/abs/2506.23225", "title": "Masked Gated Linear Unit", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline."}
{"id": "2506.23276", "pdf": "https://arxiv.org/pdf/2506.23276.pdf", "abs": "https://arxiv.org/abs/2506.23276", "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim"}
{"id": "2506.23322", "pdf": "https://arxiv.org/pdf/2506.23322.pdf", "abs": "https://arxiv.org/abs/2506.23322", "title": "GaussMaster: An LLM-based Database Copilot System", "authors": ["Wei Zhou", "Ji Sun", "Xuanhe Zhou", "Guoliang Li", "Luyang Liu", "Hao Wu", "Tianyuan Wang"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster", "summary": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster."}
{"id": "2506.23366", "pdf": "https://arxiv.org/pdf/2506.23366.pdf", "abs": "https://arxiv.org/abs/2506.23366", "title": "Density, asymmetry and citation dynamics in scientific literature", "authors": ["Nathaniel Imel", "Zachary Hafen"], "categories": ["cs.DL", "cs.CL", "cs.SI"], "comment": null, "summary": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward."}
{"id": "2506.23367", "pdf": "https://arxiv.org/pdf/2506.23367.pdf", "abs": "https://arxiv.org/abs/2506.23367", "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals."}
{"id": "2506.23394", "pdf": "https://arxiv.org/pdf/2506.23394.pdf", "abs": "https://arxiv.org/abs/2506.23394", "title": "Teaching a Language Model to Speak the Language of Tools", "authors": ["Simeon Emanuilov"], "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "comment": null, "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems."}
{"id": "2506.23517", "pdf": "https://arxiv.org/pdf/2506.23517.pdf", "abs": "https://arxiv.org/abs/2506.23517", "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools."}
{"id": "2506.23563", "pdf": "https://arxiv.org/pdf/2506.23563.pdf", "abs": "https://arxiv.org/abs/2506.23563", "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason."}
{"id": "2506.23578", "pdf": "https://arxiv.org/pdf/2506.23578.pdf", "abs": "https://arxiv.org/abs/2506.23578", "title": "Reachability in symmetric VASS", "authors": ["Łukasz Kamiński", "Sławomir Lasota"], "categories": ["cs.FL", "cs.CL"], "comment": null, "summary": "We investigate the reachability problem in symmetric vector addition systems\nwith states (VASS), where transitions are invariant under a group of\npermutations of coordinates. One extremal case, the trivial groups, yields\ngeneral VASS. In another extremal case, the symmetric groups, we show that the\nreachability problem can be solved in PSPACE, regardless of the dimension of\ninput VASS (to be contrasted with Ackermannian complexity in general VASS). We\nalso consider other groups, in particular alternating and cyclic ones.\nFurthermore, motivated by the open status of the reachability problem in data\nVASS, we estimate the gain in complexity when the group arises as a combination\nof the trivial and symmetric groups."}
{"id": "2506.23670", "pdf": "https://arxiv.org/pdf/2506.23670.pdf", "abs": "https://arxiv.org/abs/2506.23670", "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration."}
{"id": "2506.23706", "pdf": "https://arxiv.org/pdf/2506.23706.pdf", "abs": "https://arxiv.org/abs/2506.23706", "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1."}
{"id": "2506.23714", "pdf": "https://arxiv.org/pdf/2506.23714.pdf", "abs": "https://arxiv.org/abs/2506.23714", "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkilä", "Mourad Oussalah"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries."}
{"id": "2506.23845", "pdf": "https://arxiv.org/pdf/2506.23845.pdf", "abs": "https://arxiv.org/abs/2506.23845", "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences."}
{"id": "2506.23978", "pdf": "https://arxiv.org/pdf/2506.23978.pdf", "abs": "https://arxiv.org/abs/2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security."}
{"id": "2506.24019", "pdf": "https://arxiv.org/pdf/2506.24019.pdf", "abs": "https://arxiv.org/abs/2506.24019", "title": "Ella: Embodied Social Agents with Lifelong Memory", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/."}
{"id": "2506.24056", "pdf": "https://arxiv.org/pdf/2506.24056.pdf", "abs": "https://arxiv.org/abs/2506.24056", "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models", "authors": ["Tung-Ling Li", "Hongliang Liu"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations."}
{"id": "2506.24086", "pdf": "https://arxiv.org/pdf/2506.24086.pdf", "abs": "https://arxiv.org/abs/2506.24086", "title": "MotionGPT3: Human Motion as a Second Modality", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "categories": ["cs.CV", "cs.CL"], "comment": "21 pages, 8 figures", "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner."}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119.pdf", "abs": "https://arxiv.org/abs/2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."}
{"id": "2305.16264", "pdf": "https://arxiv.org/pdf/2305.16264.pdf", "abs": "https://arxiv.org/abs/2305.16264", "title": "Scaling Data-Constrained Language Models", "authors": ["Niklas Muennighoff", "Alexander M. Rush", "Boaz Barak", "Teven Le Scao", "Aleksandra Piktus", "Nouamane Tazi", "Sampo Pyysalo", "Thomas Wolf", "Colin Raffel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "50 pages (9 main), 39 figures, 15 tables", "summary": "The current trend of scaling language models involves increasing both\nparameter count and training dataset size. Extrapolating this trend suggests\nthat training dataset size may soon be limited by the amount of text data\navailable on the internet. Motivated by this limit, we investigate scaling\nlanguage models in data-constrained regimes. Specifically, we run a large set\nof experiments varying the extent of data repetition and compute budget,\nranging up to 900 billion training tokens and 9 billion parameter models. We\nfind that with constrained data for a fixed compute budget, training with up to\n4 epochs of repeated data yields negligible changes to loss compared to having\nunique data. However, with more repetition, the value of adding compute\neventually decays to zero. We propose and empirically validate a scaling law\nfor compute optimality that accounts for the decreasing value of repeated\ntokens and excess parameters. Finally, we experiment with approaches mitigating\ndata scarcity, including augmenting the training dataset with code data or\nremoving commonly used filters. Models and datasets from our 400 training runs\nare freely available at https://github.com/huggingface/datablations."}
{"id": "2404.19543", "pdf": "https://arxiv.org/pdf/2404.19543.pdf", "abs": "https://arxiv.org/abs/2404.19543", "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing", "authors": ["Yucheng Hu", "Yuxing Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 7 figures. Draft version 1", "summary": "Large Language Models (LLMs) have catalyzed significant advancements in\nNatural Language Processing (NLP), yet they encounter challenges such as\nhallucination and the need for domain-specific knowledge. To mitigate these,\nrecent methodologies have integrated information retrieved from external\nresources with LLMs, substantially enhancing their performance across NLP\ntasks. This survey paper addresses the absence of a comprehensive overview on\nRetrieval-Augmented Language Models (RALMs), both Retrieval-Augmented\nGeneration (RAG) and Retrieval-Augmented Understanding (RAU), providing an\nin-depth examination of their paradigm, evolution, taxonomy, and applications.\nThe paper discusses the essential components of RALMs, including Retrievers,\nLanguage Models, and Augmentations, and how their interactions lead to diverse\nmodel structures and applications. RALMs demonstrate utility in a spectrum of\ntasks, from translation and dialogue systems to knowledge-intensive\napplications. The survey includes several evaluation methods of RALMs,\nemphasizing the importance of robustness, accuracy, and relevance in their\nassessment. It also acknowledges the limitations of RALMs, particularly in\nretrieval quality and computational efficiency, offering directions for future\nresearch. In conclusion, this survey aims to offer a structured insight into\nRALMs, their potential, and the avenues for their future development in NLP.\nThe paper is supplemented with a Github Repository containing the surveyed\nworks and resources for further study:\nhttps://github.com/2471023025/RALM_Survey."}
{"id": "2405.01299", "pdf": "https://arxiv.org/pdf/2405.01299.pdf", "abs": "https://arxiv.org/abs/2405.01299", "title": "The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation", "authors": ["Maja Pavlovic", "Massimo Poesio"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LREC-COLING NLPerspectives workshop", "summary": "Large Language Models (LLMs) have emerged as powerful support tools across\nvarious natural language tasks and a range of application domains. Recent\nstudies focus on exploring their capabilities for data annotation. This paper\nprovides a comparative overview of twelve studies investigating the potential\nof LLMs in labelling data. While the models demonstrate promising cost and\ntime-saving benefits, there exist considerable limitations, such as\nrepresentativeness, bias, sensitivity to prompt variations and English language\npreference. Leveraging insights from these studies, our empirical analysis\nfurther examines the alignment between human and GPT-generated opinion\ndistributions across four subjective datasets. In contrast to the studies\nexamining representation, our methodology directly obtains the opinion\ndistribution from GPT. Our analysis thereby supports the minority of studies\nthat are considering diverse perspectives when evaluating data annotation tasks\nand highlights the need for further research in this direction."}
{"id": "2406.15627", "pdf": "https://arxiv.org/pdf/2406.15627.pdf", "abs": "https://arxiv.org/abs/2406.15627", "title": "Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph", "authors": ["Roman Vashurin", "Ekaterina Fadeeva", "Artem Vazhentsev", "Lyudmila Rvanova", "Akim Tsvigun", "Daniil Vasilev", "Rui Xing", "Abdelrahman Boda Sadallah", "Kirill Grishchenkov", "Sergey Petrakov", "Alexander Panchenko", "Timothy Baldwin", "Preslav Nakov", "Maxim Panov", "Artem Shelmanov"], "categories": ["cs.CL", "cs.LG"], "comment": "Published at TACL 2025, presented at ACL 2025. Roman Vashurin,\n  Ekaterina Fadeeva, Artem Vazhentsev contributed equally", "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph"}
{"id": "2407.01461", "pdf": "https://arxiv.org/pdf/2407.01461.pdf", "abs": "https://arxiv.org/abs/2407.01461", "title": "Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement", "authors": ["Xiaohua Wang", "Zisu Huang", "Feiran Zhang", "Zhibo Xu", "Cenyuan Zhang", "Qi Qian", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The capacity of large language models (LLMs) to generate honest, harmless,\nand helpful responses heavily relies on the quality of user prompts. However,\nthese prompts often tend to be brief and vague, thereby significantly limiting\nthe full potential of LLMs. Moreover, harmful prompts can be meticulously\ncrafted and manipulated by adversaries to jailbreak LLMs, inducing them to\nproduce potentially toxic content. To enhance the capabilities of LLMs while\nmaintaining strong robustness against harmful jailbreak inputs, this study\nproposes a transferable and pluggable framework that refines user prompts\nbefore they are input into LLMs. This strategy improves the quality of the\nqueries, empowering LLMs to generate more truthful, benign and useful\nresponses. Specifically, a lightweight query refinement model is introduced and\ntrained using a specially designed reinforcement learning approach that\nincorporates multiple objectives to enhance particular capabilities of LLMs.\nExtensive experiments demonstrate that the refinement model not only improves\nthe quality of responses but also strengthens their robustness against\njailbreak attacks. Code is available at:\nhttps://github.com/Huangzisu/query-refinement ."}
{"id": "2407.12749", "pdf": "https://arxiv.org/pdf/2407.12749.pdf", "abs": "https://arxiv.org/abs/2407.12749", "title": "ChipXplore: Natural Language Exploration of Hardware Designs and Libraries", "authors": ["Manar Abdelatty", "Jacob Rosenstein", "Sherief Reda"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Hardware design workflows rely on Process Design Kits (PDKs) from different\nfabrication nodes, each containing standard cell libraries optimized for speed,\npower, or density. Engineers typically navigate between the design and target\nPDK to make informed decisions, such as selecting gates for area optimization\nor enhancing the speed of the critical path. However, this process is often\nmanual, time-consuming, and prone to errors. To address this, we present\nChipXplore, a multi-agent collaborative framework powered by large language\nmodels that enables engineers to query hardware designs and PDKs using natural\nlanguage. By exploiting the structured nature of PDK and hardware design data,\nChipXplore retrieves relevant information through text-to-SQL and\ntext-to-Cypher customized workflows. The framework achieves an execution\naccuracy of 97.39\\% in complex natural language queries and improves\nproductivity by making retrieval 5.63x faster while reducing errors by 5.25x in\nuser studies. Compared to generic workflows, ChipXplore's customized workflow\nis capable of orchestrating reasoning and planning over multiple databases,\nimproving accuracy by 29.78\\%. ChipXplore lays the foundation for building\nautonomous agents capable of tackling diverse physical design tasks that\nrequire PDK and hardware design awareness."}
{"id": "2407.21536", "pdf": "https://arxiv.org/pdf/2407.21536.pdf", "abs": "https://arxiv.org/abs/2407.21536", "title": "Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition", "authors": ["Jiang Li", "Xiaoping Wang", "Zhigang Zeng"], "categories": ["cs.CL"], "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Multimodal emotion recognition in conversation (MERC) has garnered\nsubstantial research attention recently. Existing MERC methods face several\nchallenges: (1) they fail to fully harness direct inter-modal cues, possibly\nleading to less-than-thorough cross-modal modeling; (2) they concurrently\nextract information from the same and different modalities at each network\nlayer, potentially triggering conflicts from the fusion of multi-source data;\n(3) they lack the agility required to detect dynamic sentimental changes,\nperhaps resulting in inaccurate classification of utterances with abrupt\nsentiment shifts. To address these issues, a novel approach named GraphSmile is\nproposed for tracking intricate emotional cues in multimodal dialogues.\nGraphSmile comprises two key components, i.e., GSF and SDP modules. GSF\ningeniously leverages graph structures to alternately assimilate inter-modal\nand intra-modal emotional dependencies layer by layer, adequately capturing\ncross-modal cues while effectively circumventing fusion conflicts. SDP is an\nauxiliary task to explicitly delineate the sentiment dynamics between\nutterances, promoting the model's ability to distinguish sentimental\ndiscrepancies. GraphSmile is effortlessly applied to multimodal sentiment\nanalysis in conversation (MSAC), thus enabling simultaneous execution of MERC\nand MSAC tasks. Empirical results on multiple benchmarks demonstrate that\nGraphSmile can handle complex emotional and sentimental patterns, significantly\noutperforming baseline models."}
{"id": "2408.06576", "pdf": "https://arxiv.org/pdf/2408.06576.pdf", "abs": "https://arxiv.org/abs/2408.06576", "title": "CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization", "authors": ["Wei Peng", "Junmei Ding", "Wei Wang", "Lei Cui", "Wei Cai", "Zhiyu Hao", "Xiaochun Yun"], "categories": ["cs.CL"], "comment": null, "summary": "Cyber Threat Intelligence (CTI) summarization involves generating concise and\naccurate highlights from web intelligence data, which is critical for providing\ndecision-makers with actionable insights to swiftly detect and respond to cyber\nthreats in the cybersecurity domain. Despite that, the development of efficient\ntechniques for summarizing CTI reports, comprising facts, analytical insights,\nattack processes, and more, has been hindered by the lack of suitable datasets.\nTo address this gap, we introduce CTISum, a new benchmark dataset designed for\nthe CTI summarization task. Recognizing the significance of understanding\nattack processes, we also propose a novel fine-grained subtask: attack process\nsummarization, which aims to help defenders assess risks, identify security\ngaps, and uncover vulnerabilities. Specifically, a multi-stage annotation\npipeline is designed to collect and annotate CTI data from diverse web sources,\nalongside a comprehensive benchmarking of CTISum using both extractive,\nabstractive and LLMs-based summarization methods. Experimental results reveal\nthat current state-of-the-art models face significant challenges when applied\nto CTISum, highlighting that automatic summarization of CTI reports remains an\nopen research problem. The code and example dataset can be made publicly\navailable at https://github.com/pengwei-iie/CTISum."}
{"id": "2408.11189", "pdf": "https://arxiv.org/pdf/2408.11189.pdf", "abs": "https://arxiv.org/abs/2408.11189", "title": "Emotional RAG LLMs: Reading Comprehension for the Open Internet", "authors": ["Benjamin Reichman", "Adar Avsian", "Kartik Talamadupula", "Toshish Jawale", "Larry Heck"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia-like texts written in a neutral and factual tone. However, real-world\nRAG applications often retrieve internet-based text with diverse tones and\nlinguistic styles, posing challenges for downstream tasks. This paper\nintroduces (a) a dataset that transforms RAG-retrieved passages into\nemotionally inflected and sarcastic text, (b) an emotion translation model for\nadapting text to different tones, and (c) a prompt-based method to improve\nLLMs' pragmatic interpretation of retrieved text."}
{"id": "2409.01524", "pdf": "https://arxiv.org/pdf/2409.01524.pdf", "abs": "https://arxiv.org/abs/2409.01524", "title": "S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners", "authors": ["Yuchen Yan", "Jin Jiang", "Yang Liu", "Yixin Cao", "Xin Xu", "Mengdi Zhang", "Xunliang Cai", "Jian Shao"], "categories": ["cs.CL", "cs.AI"], "comment": "AAAI 2025: https://ojs.aaai.org/index.php/AAAI/article/view/34749", "summary": "Self-correction is a novel method that can stimulate the potential reasoning\nabilities of large language models (LLMs). It involves detecting and correcting\nerrors during the inference process when LLMs solve reasoning problems.\nHowever, recent works do not regard self-correction as a spontaneous and\nintrinsic capability of LLMs. Instead, such correction is achieved through\npost-hoc generation, external knowledge introduction, multi-model\ncollaboration, and similar techniques. In this paper, we propose a series of\nmathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous\nStep-level Self-correction for Mathematical reasoning. This capability helps\nLLMs to recognize whether their ongoing inference tends to contain errors and\nsimultaneously correct these errors to produce a more reliable response. We\nproposed a method, which employs a step-level sampling approach to construct\nstep-wise self-correction data for achieving such ability. Additionally, we\nimplement a training strategy that uses above constructed data to equip LLMs\nwith spontaneous step-level self-correction capacities. Our data and methods\nhave been demonstrated to be effective across various foundation LLMs,\nconsistently showing significant progress in evaluations on GSM8K, MATH, and\nother mathematical benchmarks. To the best of our knowledge, we are the first\nto introduce the spontaneous step-level self-correction ability of LLMs in\nmathematical reasoning."}
{"id": "2409.15380", "pdf": "https://arxiv.org/pdf/2409.15380.pdf", "abs": "https://arxiv.org/abs/2409.15380", "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino", "authors": ["Jann Railey Montalan", "Jian Gang Ngui", "Wei Qi Leong", "Yosephine Susanto", "Hamsawardhini Rengarajan", "Alham Fikri Aji", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for presentation at Paclic 38, 2024", "summary": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs."}
{"id": "2410.03145", "pdf": "https://arxiv.org/pdf/2410.03145.pdf", "abs": "https://arxiv.org/abs/2410.03145", "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "authors": ["Kyuyoung Kim", "Ah Jeong Seo", "Hao Liu", "Jinwoo Shin", "Kimin Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings", "summary": "Large language models (LLMs) fine-tuned with alignment techniques, such as\nreinforcement learning from human feedback, have been instrumental in\ndeveloping some of the most capable AI systems to date. Despite their success,\nexisting methods typically rely on simple binary labels, such as those\nindicating preferred outputs in pairwise preferences, which fail to capture the\nsubtle differences in relative quality between pairs. To address this\nlimitation, we introduce an approach called Margin Matching Preference\nOptimization (MMPO), which incorporates relative quality margins into\noptimization, leading to improved LLM policies and reward models. Specifically,\ngiven quality margins in pairwise preferences, we design soft target\nprobabilities based on the Bradley-Terry model, which are then used to train\nmodels with the standard cross-entropy objective. Experiments with both human\nand AI feedback data demonstrate that MMPO consistently outperforms baseline\nmethods, often by a substantial margin, on popular benchmarks including\nMT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves\nstate-of-the-art performance on RewardBench as of June 2024, outperforming\nother models of the same scale. Our analysis also shows that MMPO is more\nrobust to overfitting, leading to better-calibrated models."}
{"id": "2410.06735", "pdf": "https://arxiv.org/pdf/2410.06735.pdf", "abs": "https://arxiv.org/abs/2410.06735", "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "authors": ["Fumiya Uchiyama", "Takeshi Kojima", "Andrew Gambardella", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2024", "summary": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs."}
{"id": "2410.08174", "pdf": "https://arxiv.org/pdf/2410.08174.pdf", "abs": "https://arxiv.org/abs/2410.08174", "title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models", "authors": ["Qingni Wang", "Tiantian Geng", "Zhiyuan Wang", "Teng Wang", "Bo Fu", "Feng Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted by ICLR 2025 Spotlights", "summary": "Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels."}
{"id": "2410.10360", "pdf": "https://arxiv.org/pdf/2410.10360.pdf", "abs": "https://arxiv.org/abs/2410.10360", "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "authors": ["Yongxin Xu", "Ruizhe Zhang", "Xinke Jiang", "Yujie Feng", "Yuzhen Xiao", "Xinyu Ma", "Runchuan Zhu", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, existing methods lack effective control mechanisms for integrating\ninternal and external knowledge. Inspired by human cognitive processes, we\npropose Parenting, a novel framework that decouples, identifies, and\npurposefully optimizes parameter subspaces related to adherence and robustness.\nSpecifically, Parenting utilizes a key parameter mining method that combines\nforward and backward propagation signals to localize subspaces representing\ndifferent capabilities. Then, Parenting employs a type-tailored tuning\nstrategy, applying specific and appropriate optimizations to different\nsubspaces, aiming to achieve a balanced enhancement of both adherence and\nrobustness. Extensive experiments on various datasets and models validate the\neffectiveness and generalizability of our method."}
{"id": "2410.17711", "pdf": "https://arxiv.org/pdf/2410.17711.pdf", "abs": "https://arxiv.org/abs/2410.17711", "title": "Beware of Calibration Data for Pruning Large Language Models", "authors": ["Yixin Ji", "Yang Xiang", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters. Recent\nresearch has enhanced post-training pruning from different aspects but few of\nthem systematically explore the effects of calibration data, and it is unclear\nif there exist better calibration data construction strategies. We fill this\nblank and surprisingly observe that calibration data is also crucial to\npost-training pruning, especially for high sparsity. Through controlled\nexperiments on important influence factors of calibration data, including the\npruning settings, the amount of data, and its similarity with pre-training\ndata, we observe that a small size of data is adequate, and more similar data\nto its pre-training stage can yield better performance. As pre-training data is\nusually inaccessible for advanced LLMs, we further provide a self-generating\ncalibration data synthesis strategy to construct feasible calibration data.\nExperimental results on recent strong open-source LLMs (e.g., DCLM, and\nLLaMA-3) show that the proposed strategy can enhance the performance of strong\npruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to $2.68\\%$).\nCode is available at https://github.com/Dereck0602/calibration_data."}
{"id": "2411.06660", "pdf": "https://arxiv.org/pdf/2411.06660.pdf", "abs": "https://arxiv.org/abs/2411.06660", "title": "Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation", "authors": ["Qiao Qiao", "Yuepei Li", "Qing Wang", "Kang Zhou", "Qi Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge graph completion (KGC) is a task of inferring missing triples based\non existing Knowledge Graphs (KGs). Both structural and semantic information\nare vital for successful KGC. However, existing methods only use either the\nstructural knowledge from the KG embeddings or the semantic information from\npre-trained language models (PLMs), leading to suboptimal model performance.\nMoreover, since PLMs are not trained on KGs, directly using PLMs to encode\ntriples may be inappropriate. To overcome these limitations, we propose a novel\nframework called Bridge, which jointly encodes structural and semantic\ninformation of KGs. Specifically, we strategically encode entities and\nrelations separately by PLMs to better utilize the semantic knowledge of PLMs\nand enable structured representation learning via a structural learning\nprinciple. Furthermore, to bridge the gap between KGs and PLMs, we employ a\nself-supervised representation learning method called BYOL to fine-tune PLMs\nwith two different views of a triple. Unlike BYOL, which uses augmentation\nmethods to create two semantically similar views of the same image, potentially\naltering the semantic information. We strategically separate the triple into\ntwo parts to create different views, thus avoiding semantic alteration.\nExperiments demonstrate that Bridge outperforms the SOTA models on three\nbenchmark datasets."}
{"id": "2411.08870", "pdf": "https://arxiv.org/pdf/2411.08870.pdf", "abs": "https://arxiv.org/abs/2411.08870", "title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "authors": ["Daniel P. Jeong", "Pranav Mani", "Saurabh Garg", "Zachary C. Lipton", "Michael Oberst"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes\n  additional results on clinical note QA tasks and supervised fine-tuning\n  evaluations", "summary": "Several recent works seek to adapt general-purpose large language models\n(LLMs) and vision-language models (VLMs) for medical applications through\ncontinued pretraining on publicly available biomedical corpora. These works\ntypically claim that such domain-adaptive pretraining improves performance on\nvarious downstream medical tasks, such as answering medical exam questions. In\nthis paper, we compare ten \"medical\" LLMs and two VLMs against their\ncorresponding base models, arriving at a different conclusion: all medical VLMs\nand nearly all medical LLMs fail to consistently improve over their base models\nin the zero-/few-shot prompting and supervised fine-tuning regimes for medical\nquestion answering (QA). For instance, on clinical-note-based QA tasks in the\n3-shot setting, medical LLMs outperform their base models in only 26.7% of\ncases, reach a (statistical) tie in 16.7% of cases, and perform significantly\nworse in the remaining 56.7% of cases. Our conclusions are based on (i)\ncomparing each medical model directly against its base model; (ii) optimizing\nthe prompts for each model separately in zero-/few-shot prompting; and (iii)\naccounting for statistical uncertainty in comparisons. Our findings suggest\nthat state-of-the-art general-domain models may already exhibit strong medical\nknowledge and reasoning capabilities, and offer recommendations to strengthen\nthe conclusions of future studies."}
{"id": "2411.10557", "pdf": "https://arxiv.org/pdf/2411.10557.pdf", "abs": "https://arxiv.org/abs/2411.10557", "title": "MLAN: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models", "authors": ["Jianhong Tu", "Zhuohao Ni", "Nicholas Crispino", "Zihao Yu", "Michael Bendersky", "Beliz Gunel", "Ruoxi Jia", "Xin Liu", "Lingjuan Lyu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel visual instruction tuning strategy to improve the\nzero-shot task generalization of multimodal large language models by building a\nfirm text-only knowledge base. Existing work lacks sufficient experimentation\non the importance of each modality in the instruction tuning stage, often using\na majority of vision-language data while keeping text-only data limited and\nfixing mixtures of modalities. By incorporating diverse text-only data in the\nvisual instruction tuning stage, we vary vision-language data in various\ncontrolled experiments to investigate the importance of modality in visual\ninstruction tuning. Our comprehensive evaluation shows that the text-heavy\ninstruction tuning approach is able to perform on-par with traditional\nvision-heavy mixtures on both modalities across 12 general datasets while using\nas low as half the total training tokens. We find that simply increasing\nsufficiently diverse text-only data enables transfer of instruction following\nability and domain knowledge across modalities while being more efficient than\nthe vision-language approach."}
{"id": "2412.01131", "pdf": "https://arxiv.org/pdf/2412.01131.pdf", "abs": "https://arxiv.org/abs/2412.01131", "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Simone Teufel", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": "This manuscript is currently under review at Language Resources and\n  Evaluation", "summary": "Recently, much work has concerned itself with the enigma of what exactly PLMs\n(pretrained language models) learn about different aspects of language, and how\nthey learn it. One stream of this type of research investigates the knowledge\nthat PLMs have about semantic relations. However, many aspects of semantic\nrelations were left unexplored. Only one relation was considered, namely\nhypernymy. Furthermore, previous work did not measure humans' performance on\nthe same task as that solved by the PLMs. This means that at this point in\ntime, there is only an incomplete view of models' semantic relation knowledge.\nTo address this gap, we introduce a comprehensive evaluation framework covering\nfive relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy,\nand synonymy. We use six metrics (two newly introduced here) for recently\nuntreated aspects of semantic relation knowledge, namely soundness,\ncompleteness, symmetry, asymmetry, prototypicality, and distinguishability and\nfairly compare humans and models on the same task. Our extensive experiments\ninvolve 16 PLMs, eight masked and eight causal language models. Up to now only\nmasked language models had been tested although causal and masked language\nmodels treat context differently. Our results reveal a significant knowledge\ngap between humans and models for almost all semantic relations. Antonymy is\nthe outlier relation where all models perform reasonably well. In general,\nmasked language models perform significantly better than causal language\nmodels. Nonetheless, both masked and causal language models are likely to\nconfuse non-antonymy relations with antonymy."}
{"id": "2412.04205", "pdf": "https://arxiv.org/pdf/2412.04205.pdf", "abs": "https://arxiv.org/abs/2412.04205", "title": "A Context-aware Framework for Translation-mediated Conversations", "authors": ["José Pombal", "Sweta Agrawal", "Patrick Fernandes", "Emmanouil Zaranis", "André F. T. Martins"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic translation systems offer a powerful solution to bridge language\nbarriers in scenarios where participants do not share a common language.\nHowever, these systems can introduce errors leading to misunderstandings and\nconversation breakdown. A key issue is that current systems fail to incorporate\nthe rich contextual information necessary to resolve ambiguities and omitted\ndetails, resulting in literal, inappropriate, or misaligned translations. In\nthis work, we present a framework to improve large language model-based\ntranslation systems by incorporating contextual information in bilingual\nconversational settings during training and inference. We validate our proposed\nframework on two task-oriented domains: customer chat and user-assistant\ninteraction. Across both settings, the system produced by our\nframework-TowerChat-consistently results in better translations than\nstate-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple\nautomatic translation quality metrics on several language pairs. We also show\nthat the resulting model leverages context in an intended and interpretable\nway, improving consistency between the conveyed message and the generated\ntranslations."}
{"id": "2412.10266", "pdf": "https://arxiv.org/pdf/2412.10266.pdf", "abs": "https://arxiv.org/abs/2412.10266", "title": "Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media", "authors": ["Jiaqing Yuan", "Ruijie Xi", "Munindar P. Singh"], "categories": ["cs.CL"], "comment": "Accepted by ACM Hypertext 2025", "summary": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media."}
{"id": "2412.12386", "pdf": "https://arxiv.org/pdf/2412.12386.pdf", "abs": "https://arxiv.org/abs/2412.12386", "title": "Interpretable LLM-based Table Question Answering", "authors": ["Giang Nguyen", "Ivan Brugere", "Shubham Sharma", "Sanjay Kariyappa", "Anh Totti Nguyen", "Freddy Lecue"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR) in\n  06/2025. Reviews at: https://openreview.net/forum?id=2eTsZBoU2W", "summary": "Interpretability in Table Question Answering (Table QA) is critical,\nespecially in high-stakes domains like finance and healthcare. While recent\nTable QA approaches based on Large Language Models (LLMs) achieve high\naccuracy, they often produce ambiguous explanations of how answers are derived.\n  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's\ndecision-making process interpretable. POS decomposes a question into a\nsequence of atomic steps, each directly translated into an executable SQL\ncommand on the table, thereby ensuring that every intermediate result is\ntransparent. Through extensive experiments, we show that: First, POS generates\nthe highest-quality explanations among compared methods, which markedly\nimproves the users' ability to simulate and verify the model's decisions.\nSecond, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and\nFeTaQA), POS achieves QA accuracy that is competitive to existing methods,\nwhile also offering greater efficiency-requiring significantly fewer LLM calls\nand table database queries (up to 25x fewer)-and more robust performance on\nlarge-sized tables. Finally, we observe high agreement (up to 90.59% in forward\nsimulation) between LLMs and human users when making decisions based on the\nsame explanations, suggesting that LLMs could serve as an effective proxy for\nhumans in evaluating Table QA explanations."}
{"id": "2412.17063", "pdf": "https://arxiv.org/pdf/2412.17063.pdf", "abs": "https://arxiv.org/abs/2412.17063", "title": "Computational Analysis of Character Development in Holocaust Testimonies", "authors": ["Esther Shizgal", "Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives."}
{"id": "2501.01644", "pdf": "https://arxiv.org/pdf/2501.01644.pdf", "abs": "https://arxiv.org/abs/2501.01644", "title": "Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs", "authors": ["Tien Dang", "Viet Thanh Duy Nguyen", "Minh Tuan Le", "Truong-Son Hy"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Biomedical Knowledge Graphs (BKGs) integrate diverse datasets to elucidate\ncomplex relationships within the biomedical field. Effective link prediction on\nthese graphs can uncover valuable connections, such as potential novel\ndrug-disease relations. We introduce a novel multimodal approach that unifies\nembeddings from specialized Language Models (LMs) with Graph Contrastive\nLearning (GCL) to enhance intra-entity relationships while employing a\nKnowledge Graph Embedding (KGE) model to capture inter-entity relationships for\neffective link prediction. To address limitations in existing BKGs, we present\nPrimeKG++, an enriched knowledge graph incorporating multimodal data, including\nbiological sequences and textual descriptions for each entity type. By\ncombining semantic and relational information in a unified representation, our\napproach demonstrates strong generalizability, enabling accurate link\npredictions even for unseen nodes. Experimental results on PrimeKG++ and the\nDrugBank drug-target interaction dataset demonstrate the effectiveness and\nrobustness of our method across diverse biomedical datasets. Our source code,\npre-trained models, and data are publicly available at\nhttps://github.com/HySonLab/BioMedKG"}
{"id": "2501.03124", "pdf": "https://arxiv.org/pdf/2501.03124.pdf", "abs": "https://arxiv.org/abs/2501.03124", "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models", "authors": ["Mingyang Song", "Zhaochen Su", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Main. Project Page: https://prmbench.github.io/", "summary": "Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development."}
{"id": "2501.10316", "pdf": "https://arxiv.org/pdf/2501.10316.pdf", "abs": "https://arxiv.org/abs/2501.10316", "title": "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling", "authors": ["Suvodip Dey", "Yi-Jyun Sun", "Gokhan Tur", "Dilek Hakkani-Tur"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance."}
{"id": "2502.04397", "pdf": "https://arxiv.org/pdf/2502.04397.pdf", "abs": "https://arxiv.org/abs/2502.04397", "title": "Multimodal Medical Code Tokenizer", "authors": ["Xiaorui Su", "Shvat Messica", "Yepeng Huang", "Ruth Johnson", "Lukas Fesser", "Shanghua Gao", "Faryad Sahneh", "Marinka Zitnik"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML'25", "summary": "Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models."}
{"id": "2502.05489", "pdf": "https://arxiv.org/pdf/2502.05489.pdf", "abs": "https://arxiv.org/abs/2502.05489", "title": "Mechanistic Interpretability of Emotion Inference in Large Language Models", "authors": ["Ala N. Tak", "Amin Banayeeanzade", "Anahita Bolourani", "Mina Kian", "Robin Jia", "Jonathan Gratch"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 camera-ready version. First two authors contributed equally", "summary": "Large language models (LLMs) show promising capabilities in predicting human\nemotions from text. However, the mechanisms through which these models process\nemotional stimuli remain largely unexplored. Our study addresses this gap by\ninvestigating how autoregressive LLMs infer emotions, showing that emotion\nrepresentations are functionally localized to specific regions in the model.\nOur evaluation includes diverse model families and sizes and is supported by\nrobustness checks. We then show that the identified representations are\npsychologically plausible by drawing on cognitive appraisal theory, a\nwell-established psychological framework positing that emotions emerge from\nevaluations (appraisals) of environmental stimuli. By causally intervening on\nconstrued appraisal concepts, we steer the generation and show that the outputs\nalign with theoretical and intuitive expectations. This work highlights a novel\nway to causally intervene and precisely shape emotional text generation,\npotentially benefiting safety and alignment in sensitive affective domains."}
{"id": "2502.05651", "pdf": "https://arxiv.org/pdf/2502.05651.pdf", "abs": "https://arxiv.org/abs/2502.05651", "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy", "authors": ["Hyunjong Kim", "Suyeon Lee", "Yeongjae Cho", "Eunseo Ryu", "Yohan Jo", "Suran Seong", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL 2025 Main Conference", "summary": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI."}
{"id": "2502.07004", "pdf": "https://arxiv.org/pdf/2502.07004.pdf", "abs": "https://arxiv.org/abs/2502.07004", "title": "Demystifying Singular Defects in Large Language Models", "authors": ["Haoqi Wang", "Tong Zhang", "Mathieu Salzmann"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Large transformer models are known to produce high-norm tokens. In vision\ntransformers (ViTs), such tokens have been mathematically modeled through the\nsingular vectors of the linear approximations of layers. However, in large\nlanguage models (LLMs), the underlying causes of high-norm tokens remain\nlargely unexplored, and their different properties from those of ViTs require a\nnew analysis framework. In this paper, we provide both theoretical insights and\nempirical validation across a range of recent models, leading to the following\nobservations: i) The layer-wise singular direction predicts the abrupt\nexplosion of token norms in LLMs. ii) The negative eigenvalues of a layer\nexplain its sudden decay. iii) The computational pathways leading to high-norm\ntokens differ between initial and noninitial tokens. iv) High-norm tokens are\ntriggered by the right leading singular vector of the matrix approximating the\ncorresponding modules. We showcase two practical applications of these\nfindings: the improvement of quantization schemes and the design of LLM\nsignatures. Our findings not only advance the understanding of singular defects\nin LLMs but also open new avenues for their application. We expect that this\nwork will stimulate further research into the internal mechanisms of LLMs. Code\nis released at https://github.com/haoqiwang/singular_defect."}
{"id": "2502.10341", "pdf": "https://arxiv.org/pdf/2502.10341.pdf", "abs": "https://arxiv.org/abs/2502.10341", "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation", "authors": ["Alexander Wettig", "Kyle Lo", "Sewon Min", "Hannaneh Hajishirzi", "Danqi Chen", "Luca Soldaini"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025. Project page: https://weborganizer.allen.ai", "summary": "Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation."}
{"id": "2502.13010", "pdf": "https://arxiv.org/pdf/2502.13010.pdf", "abs": "https://arxiv.org/abs/2502.13010", "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge", "authors": ["Mohammad Reza Rezaei", "Reza Saadati Fard", "Jayson L. Parker", "Rahul G. Krishnan", "Milad Lankarany"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."}
{"id": "2502.16825", "pdf": "https://arxiv.org/pdf/2502.16825.pdf", "abs": "https://arxiv.org/abs/2502.16825", "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization", "authors": ["Yao Xiao", "Hai Ye", "Linyao Chen", "Hwee Tou Ng", "Lidong Bing", "Xiaoli Li", "Roy Ka-wei Lee"], "categories": ["cs.CL"], "comment": "ACL25 Main", "summary": "Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\n\\emph{scale up} the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a \\emph{decline} in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 ($C_7^2$) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position $\\mu - 2\\sigma$ rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases."}
{"id": "2502.18282", "pdf": "https://arxiv.org/pdf/2502.18282.pdf", "abs": "https://arxiv.org/abs/2502.18282", "title": "Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases", "authors": ["Shanshan Xu", "T. Y. S. S Santosh", "Yanai Elazar", "Quirin Vogel", "Barbara Plank", "Matthias Grabmair"], "categories": ["cs.CL"], "comment": null, "summary": "Recent works have shown that Large Language Models (LLMs) have a tendency to\nmemorize patterns and biases present in their training data, raising important\nquestions about how such memorized content influences model behavior. One such\nconcern is the emergence of political bias in LLM outputs. In this paper, we\ninvestigate the extent to which LLMs' political leanings reflect memorized\npatterns from their pretraining corpora. We propose a method to quantitatively\nevaluate political leanings embedded in the large pretraining corpora.\nSubsequently we investigate to whom are the LLMs' political leanings more\naligned with, their pretrainig corpora or the surveyed human opinions. As a\ncase study, we focus on probing the political leanings of LLMs in 32 US Supreme\nCourt cases, addressing contentious topics such as abortion and voting rights.\nOur findings reveal that LLMs strongly reflect the political leanings in their\ntraining data, and no strong correlation is observed with their alignment to\nhuman opinions as expressed in surveys. These results underscore the importance\nof responsible curation of training data, and the methodology for auditing the\nmemorization in LLMs to ensure human-AI alignment."}
{"id": "2502.18435", "pdf": "https://arxiv.org/pdf/2502.18435.pdf", "abs": "https://arxiv.org/abs/2502.18435", "title": "What Makes the Preferred Thinking Direction for LLMs in Multiple-choice Questions?", "authors": ["Yizhe Zhang", "Richard Bai", "Zijin Gu", "Ruixiang Zhang", "Jiatao Gu", "Emmanuel Abbe", "Samy Bengio", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "comment": "10 pages for the main text", "summary": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability, and directional conditional entropy. We\nanalyze the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous. Our code and checkpoints are released at\nhttps://github.com/apple/ml-reversal-blessing."}
{"id": "2502.18968", "pdf": "https://arxiv.org/pdf/2502.18968.pdf", "abs": "https://arxiv.org/abs/2502.18968", "title": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles", "authors": ["Kuang Wang", "Xianfei Li", "Shenghao Yang", "Li Zhou", "Feng Jiang", "Haizhou Li"], "categories": ["cs.CL"], "comment": "9 pages. Accepted to ACL 2025. Camera-ready version", "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, current role-playing\nmethods face challenges such as a lack of utterance-level authenticity and\nuser-level diversity, often hindered by role confusion and dependence on\npredefined profiles of well-known figures. In contrast, direct simulation\nfocuses solely on text, neglecting implicit user traits like personality and\nconversation-level consistency. To address these issues, we introduce the User\nSimulator with Implicit Profiles (USP), a framework that infers implicit user\nprofiles from human-machine interactions to simulate personalized and realistic\ndialogues. We first develop an LLM-driven extractor with a comprehensive\nprofile schema, then refine the simulation using conditional supervised\nfine-tuning and reinforcement learning with cycle consistency, optimizing at\nboth the utterance and conversation levels. Finally, a diverse profile sampler\ncaptures the distribution of real-world user profiles. Experimental results\nshow that USP outperforms strong baselines in terms of authenticity and\ndiversity while maintaining comparable consistency. Additionally, using USP to\nevaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications."}
{"id": "2503.01875", "pdf": "https://arxiv.org/pdf/2503.01875.pdf", "abs": "https://arxiv.org/abs/2503.01875", "title": "Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement", "authors": ["Yaxuan Kong", "Yiyuan Yang", "Yoontae Hwang", "Wenjie Du", "Stefan Zohren", "Zhangyang Wang", "Ming Jin", "Qingsong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Annual Meeting of the Association for Computational Linguistics (ACL\n  2025, Main)", "summary": "Time series data are foundational in finance, healthcare, and energy domains.\nHowever, most existing methods and datasets remain focused on a narrow spectrum\nof tasks, such as forecasting or anomaly detection. To bridge this gap, we\nintroduce Time Series Multi-Task Question Answering (Time-MQA), a unified\nframework that enables natural language queries across multiple time series\ntasks - numerical analytical tasks and open-ended question answering with\nreasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset\ncontaining $\\sim$200k question-answer pairs derived from diverse time series\nspanning environment, traffic, etc. This comprehensive resource covers various\ntime series lengths and promotes robust model development. We further\ndemonstrate how continually pre-training large language models (Mistral 7B,\nLlama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning\ncapabilities, moving beyond mere numeric tasks and enabling more advanced and\nintuitive interactions with temporal data. The complete TSQA dataset, models,\nuser study questionnaires for evaluation, and other related materials have been\nopen-sourced."}
{"id": "2503.04722", "pdf": "https://arxiv.org/pdf/2503.04722.pdf", "abs": "https://arxiv.org/abs/2503.04722", "title": "Enough Coin Flips Can Make LLMs Act Bayesian", "authors": ["Ritwik Gupta", "Rodolfo Corona", "Jiaxin Ge", "Eric Wang", "Dan Klein", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs use ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner."}
{"id": "2503.10135", "pdf": "https://arxiv.org/pdf/2503.10135.pdf", "abs": "https://arxiv.org/abs/2503.10135", "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding", "authors": ["Jinze Li", "Yixing Xu", "Haiduo Huang", "Xuanwu Yin", "Dong Li", "Edith C. H. Ngai", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025). Code: https://github.com/AMD-AIG-AIMA/Gumiho", "summary": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness."}
{"id": "2503.10995", "pdf": "https://arxiv.org/pdf/2503.10995.pdf", "abs": "https://arxiv.org/abs/2503.10995", "title": "TigerLLM -- A Family of Bangla Large Language Models", "authors": ["Nishat Raihan", "Marcos Zampieri"], "categories": ["cs.CL"], "comment": null, "summary": "The development of Large Language Models (LLMs) remains heavily skewed\ntowards English and a few other high-resource languages. This linguistic\ndisparity is particularly evident for Bangla - the 5th most spoken language. A\nfew initiatives attempted to create open-source Bangla LLMs with performance\nstill behind high-resource languages and limited reproducibility. To address\nthis gap, we introduce TigerLLM - a family of Bangla LLMs. Our results\ndemonstrate that these models surpass all open-source alternatives and also\noutperform larger proprietary models like GPT3.5 across standard benchmarks,\nestablishing TigerLLM as the new baseline for future Bangla language modeling."}
{"id": "2503.11655", "pdf": "https://arxiv.org/pdf/2503.11655.pdf", "abs": "https://arxiv.org/abs/2503.11655", "title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning", "authors": ["Donghao Huang", "Zhaoxia Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE\n  journal", "summary": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative."}
{"id": "2503.16334", "pdf": "https://arxiv.org/pdf/2503.16334.pdf", "abs": "https://arxiv.org/abs/2503.16334", "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "authors": ["Ying Shen", "Lifu Huang"], "categories": ["cs.CL"], "comment": "ACL 2025, 16 pages, 2 figures", "summary": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."}
{"id": "2503.17222", "pdf": "https://arxiv.org/pdf/2503.17222.pdf", "abs": "https://arxiv.org/abs/2503.17222", "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models", "authors": ["Sonish Sivarajkumar", "Kimia Ameri", "Chuqin Li", "Yanshan Wang", "Min Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies."}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745.pdf", "abs": "https://arxiv.org/abs/2504.04745", "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Accepted to XLLM Workshop at ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more\nevident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81% in\nthe best-case scenario."}
{"id": "2504.12563", "pdf": "https://arxiv.org/pdf/2504.12563.pdf", "abs": "https://arxiv.org/abs/2504.12563", "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation", "authors": ["Haris Riaz", "Sourav Bhabesh", "Vinayak Arannil", "Miguel Ballesteros", "Graham Horwood"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 17 figures. Findings of ACL 2025", "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth."}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154.pdf", "abs": "https://arxiv.org/abs/2504.14154", "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted by ACL 2025 Main", "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks."}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084.pdf", "abs": "https://arxiv.org/abs/2504.16084", "title": "TTRL: Test-Time Reinforcement Learning", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Li Sheng", "Shang Qu", "Ganqu Cui", "Xuekai Zhu", "Haozhan Li", "Yuchen Zhang", "Xinwei Long", "Ermo Hua", "Biqing Qi", "Youbang Sun", "Zhiyuan Ma", "Lifan Yuan", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182.pdf", "abs": "https://arxiv.org/abs/2505.12182", "title": "Truth Neurons", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271.pdf", "abs": "https://arxiv.org/abs/2505.13271", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\\% execution\naccuracy, while the 32B model achieves 73.67\\%. The code has been open sourced\nat https://github.com/CycloneBoy/csc_sql."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648.pdf", "abs": "https://arxiv.org/abs/2505.22648", "title": "WebDancer: Towards Autonomous Information Seeking Agency", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Gang Fu", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302.pdf", "abs": "https://arxiv.org/abs/2505.24302", "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging."}
{"id": "2506.07160", "pdf": "https://arxiv.org/pdf/2506.07160.pdf", "abs": "https://arxiv.org/abs/2506.07160", "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "authors": ["Yikun Wang", "Yibin Wang", "Dianyi Wang", "Zimian Peng", "Qipeng Guo", "Dacheng Tao", "Jiaqi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks."}
{"id": "2506.08686", "pdf": "https://arxiv.org/pdf/2506.08686.pdf", "abs": "https://arxiv.org/abs/2506.08686", "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths", "authors": ["Soham Poddar", "Paramita Koley", "Janardan Misra", "Sanjay Podder", "Navveen Balani", "Niloy Ganguly", "Saptarshi Ghosh"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to appear at the ACL 2025 findings", "summary": "A significant portion of the energy consumed by Large Language Models (LLMs)\narises from their inference processes; hence developing energy-efficient\nmethods for inference is crucial. While several techniques exist for inference\noptimization, output compression remains relatively unexplored, with only a few\npreliminary efforts addressing this aspect. In this work, we first benchmark 12\ndecoder-only LLMs across 5 datasets, revealing that these models often produce\nresponses that are substantially longer than necessary. We then conduct a\ncomprehensive quality assessment of LLM responses, formally defining six\ninformation categories present in LLM responses. We show that LLMs often tend\nto include redundant or additional information besides the minimal answer. To\naddress this issue of long responses by LLMs, we explore several simple and\nintuitive prompt-engineering strategies. Empirical evaluation shows that\nappropriate prompts targeting length reduction and controlling information\ncontent can achieve significant energy optimization between 25-60\\% by reducing\nthe response length while preserving the quality of LLM responses."}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428.pdf", "abs": "https://arxiv.org/abs/2506.09428", "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "authors": ["Fei Ding", "Baiqiao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is a critical step for enhancing the\ninstruction-following capabilities of Large Language Models (LLMs) and adapting\nthem to specialized domains. However, SFT often leads to a degradation of the\nmodel's general abilities, a phenomenon known as catastrophic forgetting. This\nproblem is exacerbated when third-party practitioners fine-tune open-source\nmodels, as the original SFT data is typically not available. To address this\nchallenge, we propose a novel and cost-effective SFT method that effectively\nmitigates catastrophic forgetting without requiring access to the original SFT\ndata. Our approach first reconstructs the likely instruction distribution of\nthe base model. It then employs a multi-model generation and filtering pipeline\nto synthesize a high-quality general-purpose dataset. This synthetic dataset is\nmixed with new, domain-specific data for fine-tuning. Experimental results show\nthat our method not only preserves the model's capabilities in general domains\nbut also improves task-specific performance, outperforming baselines that use\npublicly available SFT datasets."}
{"id": "2506.12446", "pdf": "https://arxiv.org/pdf/2506.12446.pdf", "abs": "https://arxiv.org/abs/2506.12446", "title": "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment", "authors": ["Bin Xie", "Bingbing Xu", "Yige Yuan", "Shengmao Zhu", "Huawei Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Inference-time alignment methods have gained significant attention for their\nefficiency and effectiveness in aligning large language models (LLMs) with\nhuman preferences. However, existing dominant approaches using reward-guided\nsearch (RGS) primarily rely on outcome reward models (ORMs), which suffer from\na critical granularity mismatch: ORMs are designed to provide outcome rewards\nfor complete responses, while RGS methods rely on process rewards to guide the\npolicy, leading to inconsistent scoring and suboptimal alignment. To address\nthis challenge, we introduce process reward models (PRMs) into RGS and argue\nthat an ideal PRM should satisfy two objectives: Score Consistency, ensuring\ncoherent evaluation across partial and complete responses, and Preference\nConsistency, aligning partial sequence assessments with human preferences.\nBased on these, we propose SP-PRM, a novel dual-consistency framework\nintegrating score consistency-based and preference consistency-based partial\nevaluation modules without relying on human annotation. Extensive experiments\non dialogue, summarization, and reasoning tasks demonstrate that SP-PRM\nsubstantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement\nin GPT-4 evaluation scores across all tasks."}
{"id": "2506.12494", "pdf": "https://arxiv.org/pdf/2506.12494.pdf", "abs": "https://arxiv.org/abs/2506.12494", "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation", "authors": ["Zhuocheng Zhang", "Yang Feng", "Min Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL 2025 Demo", "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."}
{"id": "2506.12576", "pdf": "https://arxiv.org/pdf/2506.12576.pdf", "abs": "https://arxiv.org/abs/2506.12576", "title": "Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders", "authors": ["Ananya Joshi", "Celia Cintas", "Skyler Speakman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work shows that Sparse Autoencoders (SAE) applied to large language\nmodel (LLM) layers have neurons corresponding to interpretable concepts. These\nSAE neurons can be modified to align generated outputs, but only towards\npre-identified topics and with some parameter tuning. Our approach leverages\nthe observational and modification properties of SAEs to enable alignment for\nany topic. This method 1) scores each SAE neuron by its semantic similarity to\nan alignment text and uses them to 2) modify SAE-layer-level outputs by\nemphasizing topic-aligned neurons. We assess the alignment capabilities of this\napproach on diverse public topic datasets including Amazon reviews, Medicine,\nand Sycophancy, across the currently available open-source LLMs and SAE pairs\n(GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to\nmedical prompts reveal several benefits over fine-tuning, including increased\naverage language acceptability (0.25 vs. 0.5), reduced training time across\nmultiple alignment topics (333.6s vs. 62s), and acceptable inference time for\nmany applications (+0.00092s/token). Our open-source code is available at\ngithub.com/IBM/sae-steering."}
{"id": "2506.15981", "pdf": "https://arxiv.org/pdf/2506.15981.pdf", "abs": "https://arxiv.org/abs/2506.15981", "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion", "authors": ["Markus Frohmann", "Gabriel Meseguer-Brocal", "Markus Schedl", "Elena V. Epure"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Findings", "summary": "The rapid advancement of AI-based music generation tools is revolutionizing\nthe music industry but also posing challenges to artists, copyright holders,\nand providers alike. This necessitates reliable methods for detecting such\nAI-generated content. However, existing detectors, relying on either audio or\nlyrics, face key practical limitations: audio-based detectors fail to\ngeneralize to new or unseen generators and are vulnerable to audio\nperturbations; lyrics-based methods require cleanly formatted and accurate\nlyrics, unavailable in practice. To overcome these limitations, we propose a\nnovel, practically grounded approach: a multimodal, modular late-fusion\npipeline that combines automatically transcribed sung lyrics and speech\nfeatures capturing lyrics-related information within the audio. By relying on\nlyrical aspects directly from audio, our method enhances robustness, mitigates\nsusceptibility to low-level artifacts, and enables practical applicability.\nExperiments show that our method, DE-detect, outperforms existing lyrics-based\ndetectors while also being more robust to audio perturbations. Thus, it offers\nan effective, robust solution for detecting AI-generated music in real-world\nscenarios. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."}
{"id": "2506.16692", "pdf": "https://arxiv.org/pdf/2506.16692.pdf", "abs": "https://arxiv.org/abs/2506.16692", "title": "LegiGPT: Party Politics and Transport Policy with Large Language Model", "authors": ["Hyunsoo Yun", "Eun Hak Lee"], "categories": ["cs.CL"], "comment": "Updated title to match published version. Added DOI and journal\n  reference to PDF", "summary": "Given the significant influence of lawmakers' political ideologies on\nlegislative decision-making, analyzing their impact on transportation-related\npolicymaking is of critical importance. This study introduces a novel framework\nthat integrates a large language model (LLM) with explainable artificial\nintelligence (XAI) to analyze transportation-related legislative proposals.\nLegislative bill data from South Korea's 21st National Assembly were used to\nidentify key factors shaping transportation policymaking. These include\npolitical affiliations and sponsor characteristics. The LLM was employed to\nclassify transportation-related bill proposals through a stepwise filtering\nprocess based on keywords, sentences, and contextual relevance. XAI techniques\nwere then applied to examine the relationships between political party\naffiliation and associated attributes. The results revealed that the number and\nproportion of conservative and progressive sponsors, along with district size\nand electoral population, were critical determinants shaping legislative\noutcomes. These findings suggest that both parties contributed to bipartisan\nlegislation through different forms of engagement, such as initiating or\nsupporting proposals. This integrated approach offers a valuable tool for\nunderstanding legislative dynamics and guiding future policy development, with\nbroader implications for infrastructure planning and governance."}
{"id": "2506.17525", "pdf": "https://arxiv.org/pdf/2506.17525.pdf", "abs": "https://arxiv.org/abs/2506.17525", "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning", "authors": ["Mingfei Lau", "Qian Chen", "Yeming Fang", "Tingting Xu", "Tongzhou Chen", "Pavel Golik"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and Vox Populi - shows that in some\nlanguages, these datasets suffer from significant quality issues, which may\nobfuscate downstream evaluation results while creating an illusion of success.\nWe divide these quality issues into two categories: micro-level and\nmacro-level. We find that macro-level issues are more prevalent in less\ninstitutionalized, often under-resourced languages. We provide a case analysis\nof Taiwanese Southern Min (nan_tw) that highlights the need for proactive\nlanguage planning (e.g. orthography prescriptions, dialect boundary definition)\nand enhanced data quality control in the dataset creation process. We conclude\nby proposing guidelines and recommendations to mitigate these issues in future\ndataset development, emphasizing the importance of sociolinguistic awareness\nand language planning principles. Furthermore, we encourage research into how\nthis creation process itself can be leveraged as a tool for community-led\nlanguage planning and revitalization."}
{"id": "2506.17609", "pdf": "https://arxiv.org/pdf/2506.17609.pdf", "abs": "https://arxiv.org/abs/2506.17609", "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting", "authors": ["Lincan Li", "Eren Erman Ozguven", "Yue Zhao", "Guang Wang", "Yiqun Xie", "Yushun Dong"], "categories": ["cs.CL", "cs.LG"], "comment": "Short research paper", "summary": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations."}
{"id": "2506.17728", "pdf": "https://arxiv.org/pdf/2506.17728.pdf", "abs": "https://arxiv.org/abs/2506.17728", "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo", "Haofen Wang", "Huajun Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition..."}
{"id": "2506.18501", "pdf": "https://arxiv.org/pdf/2506.18501.pdf", "abs": "https://arxiv.org/abs/2506.18501", "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance", "authors": ["Wael Etaiwi", "Bushra Alhijawi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements."}
{"id": "2506.19750", "pdf": "https://arxiv.org/pdf/2506.19750.pdf", "abs": "https://arxiv.org/abs/2506.19750", "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "categories": ["cs.CL"], "comment": null, "summary": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost."}
{"id": "2506.19753", "pdf": "https://arxiv.org/pdf/2506.19753.pdf", "abs": "https://arxiv.org/abs/2506.19753", "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis", "authors": ["Omar A. Essameldin", "Ali O. Elbeih", "Wael H. Gomaa", "Wael F. Elsersy"], "categories": ["cs.CL", "cs.AI"], "comment": "Email Typo Update", "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities."}
{"id": "2506.20199", "pdf": "https://arxiv.org/pdf/2506.20199.pdf", "abs": "https://arxiv.org/abs/2506.20199", "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?", "authors": ["Mengqi Wang", "Tiantian Feng", "Shrikanth Narayanan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing."}
{"id": "2506.20876", "pdf": "https://arxiv.org/pdf/2506.20876.pdf", "abs": "https://arxiv.org/abs/2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "categories": ["cs.CL"], "comment": "Flattened Figure 1 PDF for compatibility with Mac Preview", "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process."}
{"id": "2506.21521", "pdf": "https://arxiv.org/pdf/2506.21521.pdf", "abs": "https://arxiv.org/abs/2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations."}
{"id": "2506.21591", "pdf": "https://arxiv.org/pdf/2506.21591.pdf", "abs": "https://arxiv.org/abs/2506.21591", "title": "FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning", "authors": ["Shaoyu Dou", "Yutian Shen", "Mofan Chen", "Zixuan Wang", "Jiajie Xu", "Qi Guo", "Kailai Shao", "Chao Chen", "Haixiang Hu", "Haibo Shi", "Min Min", "Liwen Zhang"], "categories": ["cs.CL"], "comment": "The statistics included in the paper are incomplete (e.g., Tables 2\n  and 5 report only the results of a single run), which may lead readers to\n  misunderstand", "summary": "Large Language Models (LLMs) demonstrate significant potential but face\nchallenges in complex financial reasoning tasks requiring both domain knowledge\nand sophisticated reasoning. Current evaluation benchmarks often fall short by\nnot decoupling these capabilities indicators from single task performance and\nlack root cause analysis for task failure. To address this, we introduce\nFinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'\nknowledge and reasoning abilities independently, proposing distinct knowledge\nscore and reasoning score metrics. Inspired by cognitive science, we further\npropose a cognitive score based on Bloom's taxonomy to analyze capabilities in\nreasoning tasks across different cognitive levels. We also release a new\nopen-source Chinese financial reasoning dataset covering 22 subfields to\nsupport reproducible research and further advancements in financial reasoning.\nOur experimental results reveal that LLM reasoning ability and higher-order\ncognitive ability are the core factors influencing reasoning accuracy. We also\nspecifically find that even top models still face a bottleneck with knowledge\napplication. Furthermore, our analysis shows that specialized financial LLMs\ngenerally lag behind the top general large models across multiple metrics."}
{"id": "2404.09992", "pdf": "https://arxiv.org/pdf/2404.09992.pdf", "abs": "https://arxiv.org/abs/2404.09992", "title": "MMInA: Benchmarking Multihop Multimodal Internet Agents", "authors": ["Shulin Tian", "Ziniu Zhang", "Liangyu Chen", "Ziwei Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ACL 2025 findings. The live leaderboard is at\n  https://mmina.cliangyu.com/", "summary": "Autonomous embodied agents live on an Internet of multimedia websites. Can\nthey hop around multimodal websites to complete complex user tasks? Existing\nbenchmarks fail to assess them in a realistic, evolving environment for their\nembodiment across websites. To answer this question, we present MMInA, a\nmultihop and multimodal benchmark to evaluate the embodied agents for\ncompositional Internet tasks, with several appealing properties: 1) Evolving\nreal-world multimodal websites. Our benchmark uniquely operates on evolving\nreal-world websites, ensuring a high degree of realism and applicability to\nnatural user tasks. Our data includes 1,050 human-written tasks covering\nvarious domains such as shopping and travel, with each task requiring the agent\nto extract multimodal information from web pages as observations autonomously;\n2) Multihop web browsing. Our dataset features naturally compositional tasks\nthat require information from or actions on multiple websites to solve, to\nassess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.\nWe propose a novel protocol for evaluating an agent's progress in completing\nmultihop tasks. We experiment with both standalone (multimodal) language models\nand heuristic-based web agents. Extensive experiments demonstrate that while\nlong-chain multihop web tasks are easy for humans, they remain challenging for\nstate-of-the-art web agents. We identify that agents are more likely to fail on\nthe early hops when solving tasks with more hops, which results in lower task\nsuccess rates. To address this issue, we propose a simple memory augmentation\napproach that replays past action trajectories to reflect. Our method\nsignificantly improves the performance of both the single-hop and multihop web\nbrowsing abilities. Our code and data are available at\ngithub.com/shulin16/MMInA."}
{"id": "2406.12593", "pdf": "https://arxiv.org/pdf/2406.12593.pdf", "abs": "https://arxiv.org/abs/2406.12593", "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document Retrieval", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Yinwei Wei", "Trung Le", "Dragan Gasevic", "Yuan-Fang Li", "Thanh-Toan Do"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI", "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."}
{"id": "2407.10490", "pdf": "https://arxiv.org/pdf/2407.10490.pdf", "abs": "https://arxiv.org/abs/2407.10490", "title": "Learning Dynamics of LLM Finetuning", "authors": ["Yi Ren", "Danica J. Sutherland"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance."}
{"id": "2408.14419", "pdf": "https://arxiv.org/pdf/2408.14419.pdf", "abs": "https://arxiv.org/abs/2408.14419", "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for LLMs on Misleading Charts", "authors": ["Shubham Bharti", "Shiyun Cheng", "Jihyun Rho", "Jianrui Zhang", "Mu Cai", "Yong Jae Lee", "Martina Rau", "Xiaojin Zhu"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We introduce CHARTOM, a visual theory-of-mind benchmark designed to evaluate\nmultimodal large language models' capability to understand and reason about\nmisleading data visualizations though charts. CHARTOM consists of carefully\ndesigned charts and associated questions that require a language model to not\nonly correctly comprehend the factual content in the chart (the FACT question)\nbut also judge whether the chart will be misleading to a human readers (the\nMIND question), a dual capability with significant societal benefits. We detail\nthe construction of our benchmark including its calibration on human\nperformance and estimation of MIND ground truth called the Human Misleadingness\nIndex. We evaluated several leading LLMs -- including GPT, Claude, Gemini,\nQwen, Llama, and Llava series models -- on the CHARTOM dataset and found that\nit was challenging to all models both on FACT and MIND questions. This\nhighlights the limitations of current LLMs and presents significant opportunity\nfor future LLMs to improve on understanding misleading charts."}
{"id": "2409.01754", "pdf": "https://arxiv.org/pdf/2409.01754.pdf", "abs": "https://arxiv.org/abs/2409.01754", "title": "Empirical evidence of Large Language Model's influence on human spoken communication", "authors": ["Hiromu Yakura", "Ezequiel Lopez-Lopez", "Levin Brinkmann", "Ignacio Serna", "Prateek Gupta", "Ivan Soraperra", "Iyad Rahwan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."}
{"id": "2410.09432", "pdf": "https://arxiv.org/pdf/2410.09432.pdf", "abs": "https://arxiv.org/abs/2410.09432", "title": "FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Praneeth Vepakomma"], "categories": ["cs.DC", "cs.CL", "cs.CV"], "comment": "ACL 2025 - Oral. Raghav Singhal and Kaustubh Ponkshe contributed\n  equally to this work", "summary": "Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning\nof foundation models. However, applying LoRA in federated learning\nenvironments, where data is distributed across multiple clients, presents\nunique challenges. Existing methods rely on traditional federated averaging of\nLoRA adapters, resulting in inexact updates. To address this, we propose\nFederated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the\npretrained frozen weight matrix. Our approach achieves exact updates with\nminimal computational and communication overhead, preserving LoRA's efficiency.\nWe evaluate the method on various models across arithmetic reasoning,\ncommonsense reasoning, natural language understanding and natural language\ngeneration tasks, showing consistent performance gains over state-of-the-art\nmethods across multiple settings. Through extensive analysis, we quantify that\nthe deviations in updates from the ideal solution are significant, highlighting\nthe need for exact aggregation. Our method's simplicity, efficiency, and broad\napplicability position it as a promising solution for accurate and effective\nfederated fine-tuning of foundation models. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/fedex-lora."}
{"id": "2410.17218", "pdf": "https://arxiv.org/pdf/2410.17218.pdf", "abs": "https://arxiv.org/abs/2410.17218", "title": "Creativity in AI: Progresses and Challenges", "authors": ["Mete Ismayilzada", "Debjit Paul", "Antoine Bosselut", "Lonneke van der Plas"], "categories": ["cs.AI", "cs.CL"], "comment": "minor updates to content + contact information", "summary": "Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology."}
{"id": "2410.21896", "pdf": "https://arxiv.org/pdf/2410.21896.pdf", "abs": "https://arxiv.org/abs/2410.21896", "title": "Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models", "authors": ["Kaustubh Kislay", "Shlok Singh", "Soham Joshi", "Rohan Dutta", "Jay Shim", "George Flint", "Kevin Zhu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Symbolic Regression remains an NP-Hard problem, with extensive research\nfocusing on AI models for this task. Transformer models have shown promise in\nSymbolic Regression, but performance suffers with smaller datasets. We propose\napplying k-fold cross-validation to a transformer-based symbolic regression\nmodel trained on a significantly reduced dataset (15,000 data points, down from\n500,000). This technique partitions the training data into multiple subsets\n(folds), iteratively training on some while validating on others. Our aim is to\nprovide an estimate of model generalization and mitigate overfitting issues\nassociated with smaller datasets. Results show that this process improves the\nmodel's output consistency and generalization by a relative improvement in\nvalidation loss of 53.31%. Potentially enabling more efficient and accessible\nsymbolic regression in resource-constrained environments."}
{"id": "2411.02335", "pdf": "https://arxiv.org/pdf/2411.02335.pdf", "abs": "https://arxiv.org/abs/2411.02335", "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "authors": ["Yuqi Luo", "Chenyang Song", "Xu Han", "Yingfa Chen", "Chaojun Xiao", "Xiaojun Meng", "Liqun Deng", "Jiansheng Wei", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.7"], "comment": "23 pages, 13 figures, 6 tables", "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable."}
{"id": "2411.16750", "pdf": "https://arxiv.org/pdf/2411.16750.pdf", "abs": "https://arxiv.org/abs/2411.16750", "title": "PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation", "authors": ["Ziyao Zeng", "Jingcheng Ni", "Daniel Wang", "Patrick Rim", "Younjoon Chung", "Fengyu Yang", "Byung-Woo Hong", "Alex Wong"], "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Traditional monocular depth estimation suffers from inherent ambiguity and\nvisual nuisance. We argue that language prior can enhance monocular depth\nestimation by leveraging the inductive bias learned during the text-to-image\npre-training of diffusion models. The ability of these models to generate\nimages that align with text indicates that they have learned the spatial\nrelationships, size, and shape of specified objects, which can be applied to\nimprove depth estimation. Thus, we propose PriorDiffusion, using a pre-trained\ntext-to-image diffusion model that takes both images and corresponding text\ndescriptions to infer affine-invariant depth through a denoising process. We\nalso show that language prior enhances the model's perception of specific\nregions of images that users care about and describe. Simultaneously, language\nprior acts as a constraint to accelerate the convergence of both training and\nthe inference diffusion trajectory. By training on HyperSim and Virtual KITTI,\nwe achieve faster training convergence, fewer inference diffusion steps, and\nstate-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet.\nCode will be released upon acceptance."}
{"id": "2411.18797", "pdf": "https://arxiv.org/pdf/2411.18797.pdf", "abs": "https://arxiv.org/abs/2411.18797", "title": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?", "authors": ["Haomin Zhuang", "Yihua Zhang", "Kehan Guo", "Jinghan Jia", "Gaowen Liu", "Sijia Liu", "Xiangliang Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL'25", "summary": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters."}
{"id": "2501.02497", "pdf": "https://arxiv.org/pdf/2501.02497.pdf", "abs": "https://arxiv.org/abs/2501.02497", "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning", "authors": ["Yixin Ji", "Juntao Li", "Yang Xiang", "Hai Ye", "Kaixin Wu", "Kai Yao", "Jia Xu", "Linjian Mo", "Min Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out advanced topics and future\ndirections."}
{"id": "2501.17905", "pdf": "https://arxiv.org/pdf/2501.17905.pdf", "abs": "https://arxiv.org/abs/2501.17905", "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "authors": ["Mingkuan Feng", "Jinyang Wu", "Shuai Zhang", "Pengpeng Shao", "Ruihan Jin", "Zhengqi Wen", "Jianhua Tao", "Feihu Che"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved significant progress across\nvarious domains, but their increasing scale results in high computational and\nmemory costs. Recent studies have revealed that LLMs exhibit sparsity,\nproviding the potential to reduce model size through pruning techniques.\nHowever, existing pruning methods typically follow a prune-then-finetune\nparadigm. Since the pruned components still contain valuable information, their\ndirect removal often leads to irreversible performance degradation, imposing a\nsubstantial computational burden to recover performance during finetuning. In\nthis paper, we propose a novel paradigm that first applies regularization, then\nprunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a\nsimple and effective Data-driven Regularized Structured Streamlining method for\nLLMs. By leveraging a small amount of data to regularize the components to be\npruned, DReSS explicitly transfers the important information to the remaining\nparts of the model in advance. Compared to direct pruning, this can reduce the\ninformation loss caused by parameter removal, thereby enhancing its language\nmodeling capabilities. Experimental results demonstrate that DReSS\nsignificantly outperforms existing pruning methods even under extreme pruning\nratios, significantly reducing latency and increasing throughput."}
{"id": "2502.00306", "pdf": "https://arxiv.org/pdf/2502.00306.pdf", "abs": "https://arxiv.org/abs/2502.00306", "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation", "authors": ["Ali Naseh", "Yuefeng Peng", "Anshuman Suri", "Harsh Chaudhari", "Alina Oprea", "Amir Houmansadr"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "This is the full version (27 pages) of the paper 'Riddle Me This!\n  Stealthy Membership Inference for Retrieval-Augmented Generation' published\n  at CCS 2025", "summary": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference."}
{"id": "2502.15676", "pdf": "https://arxiv.org/pdf/2502.15676.pdf", "abs": "https://arxiv.org/abs/2502.15676", "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling", "authors": ["Zhining Zhang", "Chuanyang Jin", "Mung Yao Jia", "Shunchi Zhang", "Tianmin Shu"], "categories": ["cs.AI", "cs.CL"], "comment": "39 pages, 10 figures, 13 tables. Website at\n  https://chuanyangjin.com/AutoToM/", "summary": "Theory of Mind (ToM), the ability to understand people's minds based on their\nbehavior, is key to developing socially intelligent agents. Current approaches\nto ToM reasoning either rely on prompting Large Language Models (LLMs), which\nare prone to systematic errors, or use handcrafted, rigid agent models for\nmodel-based inference, which are more robust but fail to generalize across\ndomains. In this work, we introduce AutoToM, an automated agent modeling method\nfor scalable, robust, and interpretable mental inference. Given a ToM problem,\nAutoToM first proposes an initial agent model and then performs automated\nBayesian inverse planning based on this model, leveraging an LLM backend.\nGuided by inference uncertainty, it iteratively refines the model by\nintroducing additional mental variables and/or incorporating more timesteps in\nthe context. Across five diverse benchmarks, AutoToM outperforms existing ToM\nmethods and even large reasoning models. Additionally, we show that AutoToM can\nproduce human-like confidence estimates and enable online mental inference for\nembodied decision-making."}
{"id": "2503.00071", "pdf": "https://arxiv.org/pdf/2503.00071.pdf", "abs": "https://arxiv.org/abs/2503.00071", "title": "I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue", "authors": ["Esam Ghaleb", "Bulat Khaertdinov", "Aslı Özyürek", "Raquel Fernández"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Accepted to Findings of ACL 2025", "summary": "In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction."}
{"id": "2503.04734", "pdf": "https://arxiv.org/pdf/2503.04734.pdf", "abs": "https://arxiv.org/abs/2503.04734", "title": "What can large language models do for sustainable food?", "authors": ["Anna T. Thomas", "Adam Yee", "Andrew Mayne", "Maya B. Mathur", "Dan Jurafsky", "Kristina Gligorić"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "ICML camera ready version", "summary": "Food systems are responsible for a third of human-caused greenhouse gas\nemissions. We investigate what Large Language Models (LLMs) can contribute to\nreducing the environmental impacts of food production. We define a typology of\ndesign and prediction tasks based on the sustainable food literature and\ncollaboration with domain experts, and evaluate six LLMs on four tasks in our\ntypology. For example, for a sustainable protein design task, food science\nexperts estimated that collaboration with an LLM can reduce time spent by 45%\non average, compared to 22% for collaboration with another expert human food\nscientist. However, for a sustainable menu design task, LLMs produce suboptimal\nsolutions when instructed to consider both human satisfaction and climate\nimpacts. We propose a general framework for integrating LLMs with combinatorial\noptimization to improve reasoning capabilities. Our approach decreases\nemissions of food choices by 79% in a hypothetical restaurant while maintaining\nparticipants' satisfaction with their set of choices. Our results demonstrate\nLLMs' potential, supported by optimization techniques, to accelerate\nsustainable food development and adoption."}
{"id": "2503.13377", "pdf": "https://arxiv.org/pdf/2503.13377.pdf", "abs": "https://arxiv.org/abs/2503.13377", "title": "Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding", "authors": ["Ye Wang", "Ziheng Wang", "Boshen Xu", "Yang Du", "Kejun Lin", "Zihan Xiao", "Zihao Yue", "Jianzhong Ju", "Liang Zhang", "Dingyi Yang", "Xiangnan Fang", "Zewen He", "Zhenbo Luo", "Wenxuan Wang", "Junqi Lin", "Jian Luan", "Qin Jin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://xuboshen.github.io/Time-R1/", "summary": "Temporal Video Grounding (TVG), the task of locating specific video segments\nbased on language queries, is a core challenge in long-form video\nunderstanding. While recent Large Vision-Language Models (LVLMs) have shown\nearly promise in tackling TVG through supervised fine-tuning (SFT), their\nabilities to generalize remain limited. To address this, we propose a novel\npost-training framework that enhances the generalization capabilities of LVLMs\nvia reinforcement learning (RL). Specifically, our contributions span three key\ndirections: (1) Time-R1: we introduce a reasoning-guided post-training\nframework via RL with verifiable reward to enhance the capabilities of LVLMs on\nthe TVG task. (2) TimeRFT: we explore data-efficient post-training strategies\non our curated RL-friendly dataset, which trains the model to progressively\ncomprehend difficult samples, leading to better generalization. (3) TVGBench:\nwe carefully construct a small yet comprehensive benchmark for LVLM evaluation,\nassessing 11 types of queries and featuring balanced distributions across both\nvideos and queries. Extensive experiments demonstrate that Time-R1 achieves\nstate-of-the-art performance across multiple downstream datasets using only\n2.5K training data, while improving its general video understanding\ncapabilities."}
{"id": "2503.22968", "pdf": "https://arxiv.org/pdf/2503.22968.pdf", "abs": "https://arxiv.org/abs/2503.22968", "title": "Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models", "authors": ["Hanwool Lee", "Dasol Choi", "Sooyong Kim", "Ilgyun Jung", "Sangwon Baek", "Guijin Son", "Inseon Hwang", "Naeun Lee", "Seunghyeok Hong"], "categories": ["cs.CE", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development."}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947.pdf", "abs": "https://arxiv.org/abs/2504.03947", "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "authors": ["Chris Samarinas", "Hamed Zamani"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems."}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084.pdf", "abs": "https://arxiv.org/abs/2504.20084", "title": "AI Awareness", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness not as a philosophical question\nof consciousness, but as a measurable, functional capacity. AI awareness is a\ndouble-edged sword: it improves general capabilities, i.e., reasoning, safety,\nwhile also raising concerns around misalignment and societal risks, demanding\ncareful oversight as AI capabilities grow.\n  In this review, we explore the emerging landscape of AI awareness, which\nincludes metacognition (the ability to represent and reason about its own\ncognitive state), self-awareness (recognizing its own identity, knowledge,\nlimitations, inter alia), social awareness (modeling the knowledge, intentions,\nand behaviors of other agents and social norms), and situational awareness\n(assessing and responding to the context in which it operates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns."}
{"id": "2505.02811", "pdf": "https://arxiv.org/pdf/2505.02811.pdf", "abs": "https://arxiv.org/abs/2505.02811", "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", "authors": ["Diji Yang", "Linda Zeng", "Jinmeng Rao", "Yi Zhang"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR 2025", "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."}
{"id": "2505.08842", "pdf": "https://arxiv.org/pdf/2505.08842.pdf", "abs": "https://arxiv.org/abs/2505.08842", "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CR", "cs.CL"], "comment": "ACL 2025 Student Research Workshop and ICML 2025 TAIG Workshop", "summary": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166.pdf", "abs": "https://arxiv.org/abs/2505.20166", "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\ncrucial textual capabilities like instruction-following are lost after training\non audio data. In some cases, models may even hallucinate sounds that are not\npresent in the input audio, raising concerns about reliability. Second,\nachieving cross-modal alignment between audio and language typically relies on\nlarge collections of task-specific question-answer pairs for instruction\ntuning, making it resource-intensive. To address these issues, previous works\nhave leveraged the backbone LLMs to synthesize general-purpose, caption-style\nalignment data. In this paper, we propose a data generation framework that\nproduces contrastive-like training data, designed to enhance ALLMs' ability to\ndifferentiate between present and absent sounds. We further extend our approach\nto multi-audio scenarios, enabling the model to either explain differences\nbetween audio inputs or produce unified captions that describe all inputs,\nthereby enhancing audio-language alignment. We refer to the entire ALLM\ntraining framework as bootstrapping audio-language alignment via synthetic data\ngeneration from backbone LLMs (BALSa). Experimental results indicate that our\nmethod effectively mitigates audio hallucinations while reliably maintaining\nstrong performance on audio understanding and reasoning benchmarks, as well as\ninstruction-following skills. Moreover, incorporating multi-audio training\nfurther enhances the model's comprehension and reasoning capabilities. Overall,\nBALSa offers an efficient and scalable approach to developing ALLMs."}
{"id": "2506.10314", "pdf": "https://arxiv.org/pdf/2506.10314.pdf", "abs": "https://arxiv.org/abs/2506.10314", "title": "Detecting Sockpuppetry on Wikipedia Using Meta-Learning", "authors": ["Luc Raszewski", "Christine De Kock"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Malicious sockpuppet detection on Wikipedia is critical to preserving access\nto reliable information on the internet and preventing the spread of\ndisinformation. Prior machine learning approaches rely on stylistic and\nmeta-data features, but do not prioritise adaptability to author-specific\nbehaviours. As a result, they struggle to effectively model the behaviour of\nspecific sockpuppet-groups, especially when text data is limited. To address\nthis, we propose the application of meta-learning, a machine learning technique\ndesigned to improve performance in data-scarce settings by training models\nacross multiple tasks. Meta-learning optimises a model for rapid adaptation to\nthe writing style of a new sockpuppet-group. Our results show that\nmeta-learning significantly enhances the precision of predictions compared to\npre-trained models, marking an advancement in combating sockpuppetry on open\nediting platforms. We release a new dataset of sockpuppet investigations to\nfoster future research in both sockpuppetry and meta-learning fields."}
{"id": "2506.12484", "pdf": "https://arxiv.org/pdf/2506.12484.pdf", "abs": "https://arxiv.org/abs/2506.12484", "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization", "authors": ["Filip Sondej", "Yushi Yang", "Mikołaj Kniejski", "Marcel Windys"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning."}
{"id": "2506.18488", "pdf": "https://arxiv.org/pdf/2506.18488.pdf", "abs": "https://arxiv.org/abs/2506.18488", "title": "AI-Generated Song Detection via Lyrics Transcripts", "authors": ["Markus Frohmann", "Elena V. Epure", "Gabriel Meseguer-Brocal", "Markus Schedl", "Romain Hennequin"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Accepted to ISMIR 2025", "summary": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."}
{"id": "2506.19882", "pdf": "https://arxiv.org/pdf/2506.19882.pdf", "abs": "https://arxiv.org/abs/2506.19882", "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Yegor Denisov-Blanch", "Brando Miranda", "Matthias Gerstgrasser", "Susan Zhang", "Andreas Haupt", "Isha Gupta", "Elyas Obbad", "Jesse Dodge", "Jessica Zosa Forde", "Koustuv Sinha", "Francesco Orabona", "Sanmi Koyejo", "David Donoho"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Science progresses by iteratively advancing and correcting humanity's\nunderstanding of the world. In machine learning (ML) research, rapid\nadvancements have led to an explosion of publications, but have also led to\nmisleading, incorrect, flawed or perhaps even fraudulent studies being accepted\nand sometimes highlighted at ML conferences due to the fallibility of peer\nreview. While such mistakes are understandable, ML conferences do not offer\nrobust processes to help the field systematically correct when such errors are\nmade. This position paper argues that ML conferences should establish a\ndedicated \"Refutations and Critiques\" (R&C) Track. This R&C Track would provide\na high-profile, reputable platform to support vital research that critically\nchallenges prior research, thereby fostering a dynamic self-correcting research\necosystem. We discuss key considerations including track design, review\nprinciples, potential pitfalls, and provide an illustrative example submission\nconcerning a recent ICLR 2025 Oral. We conclude that ML conferences should\ncreate official, reputable mechanisms to help ML research self-correct."}
{"id": "2506.21546", "pdf": "https://arxiv.org/pdf/2506.21546.pdf", "abs": "https://arxiv.org/abs/2506.21546", "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation", "authors": ["Xinzhuo Li", "Adheesh Juvekar", "Xingyou Liu", "Muntasir Wahed", "Kiet A. Nguyen", "Ismini Lourentzou"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/", "summary": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity."}
