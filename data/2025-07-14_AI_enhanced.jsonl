{"id": "2507.08002", "pdf": "https://arxiv.org/pdf/2507.08002.pdf", "abs": "https://arxiv.org/abs/2507.08002", "title": "Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study", "authors": ["Karisa Parkington", "Bazen G. Teferra", "Marianne Rouleau-Tang", "Argyrios Perivolaris", "Alice Rueda", "Adam Dubrowski", "Bill Kapralos", "Reza Samavi", "Andrew Greenshaw", "Yanbo Zhang", "Bo Cao", "Yuqi Wu", "Sirisha Rambhatla", "Sridhar Krishnan", "Venkat Bhat"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Thematic analysis provides valuable insights into participants' experiences\nthrough coding and theme development, but its resource-intensive nature limits\nits use in large healthcare studies. Large language models (LLMs) can analyze\ntext at scale and identify key content automatically, potentially addressing\nthese challenges. However, their application in mental health interviews needs\ncomparison with traditional human analysis. This study evaluates out-of-the-box\nand knowledge-base LLM-based thematic analysis against traditional methods\nusing transcripts from a stress-reduction trial with healthcare workers.\nOpenAI's GPT-4o model was used along with the Role, Instructions, Steps,\nEnd-Goal, Narrowing (RISEN) prompt engineering framework and compared to human\nanalysis in Dedoose. Each approach developed codes, noted saturation points,\napplied codes to excerpts for a subset of participants (n = 20), and\nsynthesized data into themes. Outputs and performance metrics were compared\ndirectly. LLMs using the RISEN framework developed deductive parent codes\nsimilar to human codes, but humans excelled in inductive child code development\nand theme synthesis. Knowledge-based LLMs reached coding saturation with fewer\ntranscripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).\nThe out-of-the-box LLM identified a comparable number of excerpts to human\nresearchers, showing strong inter-rater reliability (K = 0.84), though the\nknowledge-based LLM produced fewer excerpts. Human excerpts were longer and\ninvolved multiple codes per excerpt, while LLMs typically applied one code.\nOverall, LLM-based thematic analysis proved more cost-effective but lacked the\ndepth of human analysis. LLMs can transform qualitative analysis in mental\nhealthcare and clinical research when combined with human oversight to balance\nparticipant perspectives and research resources.", "AI": {"tldr": "This study compares LLM-based thematic analysis using GPT-4o with traditional human analysis in mental health interviews, focusing on efficiency and coding quality.", "motivation": "Thematic analysis is resource-intensive, limiting its use in large healthcare studies, prompting an exploration of LLM capabilities to automate this process.", "method": "The study utilized OpenAI's GPT-4o model with the RISEN prompt engineering framework to analyze transcripts from a stress-reduction trial, comparing results to human analysis conducted via Dedoose.", "result": "LLMs achieved coding saturation with fewer transcripts than human analysis, and demonstrated strong inter-rater reliability; however, humans outperformed LLMs in thematic depth and inductive code creation.", "conclusion": "LLM-based thematic analysis is a promising cost-effective alternative that can enhance qualitative research in mental healthcare, provided it is paired with human oversight to ensure depth and accuracy.", "key_contributions": ["Comparison of LLMs and human analysis in thematic analysis", "Introduction of the RISEN prompt engineering framework", "Findings on coding efficiency and quality differences between LLMs and human researchers."], "limitations": "LLMs produced less detailed analysis compared to humans and applied fewer codes per excerpt.", "keywords": ["thematic analysis", "large language models", "mental health", "qualitative research", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08003", "pdf": "https://arxiv.org/pdf/2507.08003.pdf", "abs": "https://arxiv.org/abs/2507.08003", "title": "A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages", "authors": ["Kayhan Latifzadeh", "Jacek Gwizdka", "Luis A. Leiva"], "categories": ["cs.HC", "cs.CV", "cs.IR"], "comment": null, "summary": "We contribute a comprehensive dataset to study user attention and purchasing\nbehavior on Search Engine Result Pages (SERPs). Previous work has relied on\nmouse movements as a low-cost large-scale behavioral proxy but also has relied\non self-reported ground-truth labels, collected at post-task, which can be\ninaccurate and prone to biases. To address this limitation, we use an eye\ntracker to construct an objective ground-truth of continuous visual attention.\nOur dataset comprises 2,776 transactional queries on Google SERPs, collected\nfrom 47 participants, and includes: (1) HTML source files, with CSS and images;\n(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;\n(5) bounding boxes of direct display and organic advertisements; and (6)\nscripts for further preprocessing the data. In this paper we provide an\noverview of the dataset and baseline experiments (classification tasks) that\ncan inspire researchers about the different possibilities for future work.", "AI": {"tldr": "The paper introduces a comprehensive dataset for studying user attention and purchasing behavior on Search Engine Result Pages (SERPs), utilizing eye tracking for accurate data collection.", "motivation": "To improve the accuracy of understanding user attention and purchasing behavior, overcoming limitations of previous research methods that relied on mouse movements and biased self-reports.", "method": "The study employs eye tracking to gather objective data on user attention, resulting in a dataset of 2,776 queries from Google SERPs with various associated data types.", "result": "The dataset includes HTML files, rendered SERP screenshots, eye movement and mouse movement data, and bounding boxes for advertisements, along with baseline classification experiments.", "conclusion": "The dataset provides a rich resource for future research on user interaction with SERPs and sets the stage for innovative classification tasks.", "key_contributions": ["Comprehensive dataset on user attention using eye tracking", "Includes various data types for in-depth analysis", "Baseline experiments to stimulate future research", "Addresses limitations of previous methodologies"], "limitations": "The study is limited to data collected from 47 participants, which may affect the generalizability of the findings.", "keywords": ["user attention", "SERPs", "eye tracking", "purchasing behavior", "dataset"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.08028", "pdf": "https://arxiv.org/pdf/2507.08028.pdf", "abs": "https://arxiv.org/abs/2507.08028", "title": "SSSUMO: Real-Time Semi-Supervised Submovement Decomposition", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Otto Lappi", "Benjamin Ultan Cowley"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces a SSSUMO, semi-supervised deep learning approach for\nsubmovement decomposition that achieves state-of-the-art accuracy and speed.\nWhile submovement analysis offers valuable insights into motor control,\nexisting methods struggle with reconstruction accuracy, computational cost, and\nvalidation, due to the difficulty of obtaining hand-labeled data. We address\nthese challenges using a semi-supervised learning framework. This framework\nlearns from synthetic data, initially generated from minimum-jerk principles\nand then iteratively refined through adaptation to unlabeled human movement\ndata. Our fully convolutional architecture with differentiable reconstruction\nsignificantly surpasses existing methods on both synthetic and diverse human\nmotion datasets, demonstrating robustness even in high-noise conditions.\nCrucially, the model operates in real-time (less than a millisecond per input\nsecond), a substantial improvement over optimization-based techniques. This\nenhanced performance facilitates new applications in human-computer\ninteraction, rehabilitation medicine, and motor control studies. We demonstrate\nthe model's effectiveness across diverse human-performed tasks such as\nsteering, rotation, pointing, object moving, handwriting, and mouse-controlled\ngaming, showing notable improvements particularly on challenging datasets where\ntraditional methods largely fail. Training and benchmarking source code, along\nwith pre-trained model weights, are made publicly available at\nhttps://github.com/dolphin-in-a-coma/sssumo.", "AI": {"tldr": "This paper presents SSSUMO, a semi-supervised deep learning model for accurate and fast submovement decomposition, enhancing motor control analysis and applicable in HCI and rehabilitation.", "motivation": "The paper addresses challenges in submovement analysis related to accuracy, cost, and the scarcity of labeled data.", "method": "The authors utilize a semi-supervised learning framework that combines synthetic data generation with iterative adaptation to unlabeled human movement data, implemented in a fully convolutional architecture.", "result": "SSSUMO achieves state-of-the-art performance in accuracy and real-time processing speed on various human motion datasets, outperforming existing methods, especially in high-noise conditions.", "conclusion": "The findings suggest that SSSUMO can significantly advance applications in human-computer interaction, rehabilitation medicine, and motor control studies due to its enhanced performance.", "key_contributions": ["Introduction of SSSUMO, a novel semi-supervised learning approach for submovement decomposition.", "Demonstrated real-time performance in processing human movement data.", "Availability of training and benchmarking source code along with pre-trained model weights."], "limitations": "", "keywords": ["Semi-supervised learning", "Submovement decomposition", "Human-computer interaction", "Motor control", "Deep learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08142", "pdf": "https://arxiv.org/pdf/2507.08142.pdf", "abs": "https://arxiv.org/abs/2507.08142", "title": "Pushing the Boundaries of Immersion and Storytelling: A Technical Review of Unreal Engine", "authors": ["Oleksandra Sobchyshak", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Paper submitted to Elsevier", "summary": "Unreal Engine is a platform that has influenced immersive storytelling and\nvirtual reality (VR) through its advanced features and diverse applications.\nThis paper provides an in-depth technical review of Unreal Engine. It analyzes\nits key innovations in creating hyper-realistic environments and emotionally\nengaging narratives, with significant applications in gaming, virtual\nproduction, education, cultural preservation, and healthcare. The findings of\nthis article highlight Unreal Engine's transformative impact across industries,\ndemonstrating its ability to merge storytelling with cutting-edge technologies.\nCase studies illustrate how Unreal Engine facilitates seamless visuals, audio,\nand interactivity integration to create compelling experiences. Additionally,\nthis study identifies Unreal Engine's versatility in applications ranging from\nprocedural content generation and AI-driven workflows to smart city simulations\nand VR-based rehabilitation programs.\n  While Unreal Engine sets new benchmarks for visual fidelity and\ninteractivity, this paper underscores critical challenges, including its high\nhardware demands, limited accessibility, and ethical concerns related to\nover-immersion and data privacy. Addressing these challenges through\ncloud-based rendering, inclusive design, and ethical practices is essential for\nbroader adoption and sustainability. This review concludes that Unreal Engine\nis suitable for innovation and interdisciplinary collaboration. Its ability to\nempower creators, redefine workflows, and push the boundaries of immersive\nstorytelling positions Unreal Engine as pivotal in shaping the future of\nvirtual reality and interactive media.", "AI": {"tldr": "This paper reviews Unreal Engine's innovations in immersive storytelling and virtual reality, highlighting its applications across various sectors, including healthcare and education.", "motivation": "To provide an in-depth technical review of Unreal Engine and analyze its impact on immersive storytelling and environments.", "method": "A technical review conducted through case studies and analysis of Unreal Engine's features and applications.", "result": "The paper highlights Unreal Engine's transformative impact across industries, demonstrating its capacity to merge storytelling with cutting-edge technologies.", "conclusion": "Unreal Engine's ability to empower creators and redefine workflows positions it as pivotal in shaping the future of virtual reality and interactive media despite its challenges.", "key_contributions": ["Technical analysis of Unreal Engine's innovations", "Case studies demonstrating its applications", "Identification of challenges in adoption and ethics"], "limitations": "High hardware demands, limited accessibility, ethical concerns on over-immersion and data privacy.", "keywords": ["Unreal Engine", "virtual reality", "immersive storytelling", "healthcare", "procedural content generation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.08012", "pdf": "https://arxiv.org/pdf/2507.08012.pdf", "abs": "https://arxiv.org/abs/2507.08012", "title": "RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning", "authors": ["Atli Sigurgeirsson", "Simon King"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "A Prompt-based Text-To-Speech model allows a user to control different\naspects of speech, such as speaking rate and perceived gender, through natural\nlanguage instruction. Although user-friendly, such approaches are on one hand\nconstrained: control is limited to acoustic features exposed to the model\nduring training, and too flexible on the other: the same inputs yields\nuncontrollable variation that are reflected in the corpus statistics.\n  We investigate a novel fine-tuning regime to address both of these issues at\nthe same time by exploiting the uncontrollable variance of the model. Through\nprincipal component analysis of thousands of synthesised samples, we determine\nlatent features that account for the highest proportion of the output variance\nand incorporate them as new labels for secondary fine-tuning. We evaluate the\nproposed methods on two models trained on an expressive Icelandic speech\ncorpus, one with emotional disclosure and one without. In the case of the model\nwithout emotional disclosure, the method yields both continuous and discrete\nfeatures that improve overall controllability of the model.", "AI": {"tldr": "This paper presents a novel fine-tuning approach for a Prompt-based Text-To-Speech model, enhancing controllability by addressing issues of training constraints and output variance.", "motivation": "To improve controllability in Prompt-based Text-To-Speech models by addressing the limitations of user control and output variability through novel fine-tuning techniques.", "method": "The approach involves principal component analysis of thousands of synthesized samples to identify latent features that capture significant output variance, which are then used as additional labels for secondary fine-tuning.", "result": "The method improved overall controllability in a model trained on an expressive Icelandic speech corpus, demonstrating effectiveness with both continuous and discrete features, especially in models lacking emotional disclosure.", "conclusion": "Introducing secondary fine-tuning using identified latent features can significantly enhance the controllability of speech synthesis models without emotional disclosure.", "key_contributions": ["Development of a dual-fine tuning methodology in TTS models", "Introduction of latent features via PCA to improve control", "Evaluation of methods on expressive Icelandic speech datasets"], "limitations": "", "keywords": ["Text-To-Speech", "Fine-tuning", "Latent Features", "Principal Component Analysis", "Controllability"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.08167", "pdf": "https://arxiv.org/pdf/2507.08167.pdf", "abs": "https://arxiv.org/abs/2507.08167", "title": "Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors", "authors": ["Md. Saif Hassan Onim", "Andrew M. Kiselica", "Himanshu Thapliyal"], "categories": ["cs.HC", "cs.LG", "H.1.2; J.3; C.3"], "comment": null, "summary": "Emotion detection in older adults is crucial for understanding their\ncognitive and emotional well-being, especially in hospital and assisted living\nenvironments. In this work, we investigate an edge-based, non-obtrusive\napproach to emotion identification that uses only physiological signals\nobtained via wearable sensors. Our dataset includes data from 40 older\nindividuals. Emotional states were obtained using physiological signals from\nthe Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were\nrecorded using camera-based emotion recognition with the iMotion's Facial\nExpression Analysis (FEA) module. The dataset also contains twelve emotion\ncategories in terms of relative intensities. We aim to study how well emotion\nrecognition can be accomplished using simply physiological sensor data, without\nthe requirement for cameras or intrusive facial analysis. By leveraging\nclassical machine learning models, we predict the intensity of emotional\nresponses based on physiological signals. We achieved the highest 0.782 r2\nscore with the lowest 0.0006 MSE on the regression task. This method has\nsignificant implications for individuals with Alzheimer's Disease and Related\nDementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder\n(PTSD) or other cognitive impairments. Our results across multiple classical\nregression models validate the feasibility of this method, paving the way for\nprivacy-preserving and efficient emotion recognition systems in real-world\nsettings.", "AI": {"tldr": "This study investigates a non-intrusive method for emotion detection in older adults using wearable physiological sensors, achieving promising results that could assist in cognitive health monitoring.", "motivation": "Understanding emotional well-being in older adults is important in healthcare settings, especially for those with cognitive impairments.", "method": "The authors leveraged physiological signals obtained from wearable sensors (Empatica E4 and Shimmer3 GSR+) to identify emotional states, supplemented by facial expression analysis for validation.", "result": "A regression model was developed that achieved a maximum r2 score of 0.782 and a minimum MSE of 0.0006, indicating promising accuracy in emotion prediction using physiological data alone.", "conclusion": "The results demonstrate the potential for a privacy-preserving emotion recognition system, which is particularly beneficial for patients with Alzheimer’s or PTSD.", "key_contributions": ["Development of a non-obtrusive emotion detection method using wearable sensors", "High accuracy in predicting emotional intensity solely from physiological signals", "Implications for cognitive health monitoring in sensitive populations."], "limitations": "The study's dataset is limited to 40 older individuals, which may affect the generalizability of the findings.", "keywords": ["Emotion Detection", "Wearable Sensors", "Older Adults", "Machine Learning", "Cognitive Health"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.08013", "pdf": "https://arxiv.org/pdf/2507.08013.pdf", "abs": "https://arxiv.org/abs/2507.08013", "title": "MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model", "authors": ["K. Sahit Reddy", "N. Ragavenderan", "Vasanth K.", "Ganesh N. Naik", "Vishalakshi Prabhu", "Nagaraja G. S"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing (NLP) have been driven\nbypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel\nat understanding complex texts, but biomedical literature, withits\ndomain-specific terminology, poses challenges that models likeWord2Vec and\nbidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,\ndespite capturing context, fall short in tasks needingbidirectional\nunderstanding, unlike BERT. Addressing this, we proposedMedicalBERT, a\npretrained BERT model trained on a large biomedicaldataset and equipped with\ndomain-specific vocabulary that enhances thecomprehension of biomedical\nterminology. MedicalBERT model is furtheroptimized and fine-tuned to address\ndiverse tasks, including named entityrecognition, relation extraction, question\nanswering, sentence similarity, anddocument classification. Performance metrics\nsuch as the F1-score,accuracy, and Pearson correlation are employed to showcase\nthe efficiencyof our model in comparison to other BERT-based models such as\nBioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost\nof the benchmarks, and surpasses the general-purpose BERT model by5.67% on\naverage across all the tasks evaluated respectively. This work alsounderscores\nthe potential of leveraging pretrained BERT models for medicalNLP tasks,\ndemonstrating the effectiveness of transfer learning techniques incapturing\ndomain-specific information.\n  (PDF) MedicalBERT: enhancing biomedical natural language processing using\npretrained BERT-based model. Available from:\nhttps://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model\n[accessed Jul 06 2025].", "AI": {"tldr": "MedicalBERT is a pretrained BERT model optimized for biomedical literature, outperforming other models in various NLP tasks.", "motivation": "To enhance the understanding and processing of biomedical literature which poses challenges for existing NLP models due to its domain-specific terminology.", "method": "MedicalBERT was trained on a large biomedical dataset and fine-tuned for tasks such as named entity recognition, relation extraction, question answering, sentence similarity, and document classification.", "result": "MedicalBERT outperformed BioBERT, SciBERT, and ClinicalBERT on most benchmarks, surpassing general-purpose BERT by an average of 5.67% across all evaluated tasks.", "conclusion": "The study illustrates the effectiveness of transfer learning techniques in biomedical NLP, emphasizing the potential of pretrained models like BERT in capturing domain-specific information.", "key_contributions": ["Proposed MedicalBERT, enhancing biomedical NLP with domain-specific vocabulary.", "Demonstrated superior performance of MedicalBERT over existing models in a variety of tasks.", "Showcased effective transfer learning for biomedical applications of NLP."], "limitations": "", "keywords": ["Biomedical NLP", "MedicalBERT", "Transfer Learning", "Pretrained Models", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08230", "pdf": "https://arxiv.org/pdf/2507.08230.pdf", "abs": "https://arxiv.org/abs/2507.08230", "title": "Uncanny or Not? Perceptions of AI-Generated Faces in Autism", "authors": ["Gabriella Waters"], "categories": ["cs.HC"], "comment": "2 figures, 15 pages", "summary": "As artificial intelligence (AI) systems become increasingly sophisticated at\ngenerating synthetic human faces, understanding how these images are perceived\nacross diverse populations is important. This study investigates how autistic\nindividuals/individuals with autism perceive AI-generated faces, focusing on\nthe uncanny valley effect. Using a qualitative approach, we analyzed\ndiscussions from the r/autism community on Reddit to explore how autistic\nparticipants/participants with autism describe their experiences with\nAI-generated faces and the uncanny valley phenomenon. The findings suggest that\nautistic people/people with autism may experience the uncanny valley\ndifferently, often reporting stronger discomfort with real human faces than\nwith artificial ones. This research contributes to our understanding of visual\nperception in autism and has implications for the development of inclusive AI\nsystems and assistive technologies.", "AI": {"tldr": "This study explores how autistic individuals perceive AI-generated faces, highlighting their unique discomfort levels compared to non-autistic individuals and discussing implications for AI system design.", "motivation": "To understand how AI-generated faces are perceived by autistic individuals and the implications for the design of inclusive AI systems.", "method": "Qualitative analysis of discussions from the r/autism community on Reddit regarding AI-generated faces and the uncanny valley phenomenon.", "result": "Findings indicate that autistic individuals may experience discomfort with AI-generated faces differently, reporting more discomfort with real human faces.", "conclusion": "Understanding these perceptions can guide the development of more inclusive and effective AI technologies for individuals with autism.", "key_contributions": ["Insights into the uncanny valley effect in autistic perception.", "Emphasis on the need for inclusive AI system design based on user experiences.", "Contribution to the discourse on visual perception in autism."], "limitations": "The study relies on qualitative data from a specific community, which may not be representative of the broader autistic population.", "keywords": ["AI-generated faces", "autism", "uncanny valley", "visual perception", "inclusive AI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.08014", "pdf": "https://arxiv.org/pdf/2507.08014.pdf", "abs": "https://arxiv.org/abs/2507.08014", "title": "Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking", "authors": ["Aldan Creo", "Raul Castro Fernandez", "Manuel Cebrian"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Code: https://github.com/ACMCMC/risky-conversations Results:\n  https://huggingface.co/risky-conversations Visualizer:\n  https://huggingface.co/spaces/risky-conversations/Visualizer", "summary": "As large language models (LLMs) become increasingly deployed, understanding\nthe complexity and evolution of jailbreaking strategies is critical for AI\nsafety.\n  We present a mass-scale empirical analysis of jailbreak complexity across\nover 2 million real-world conversations from diverse platforms, including\ndedicated jailbreaking communities and general-purpose chatbots. Using a range\nof complexity metrics spanning probabilistic measures, lexical diversity,\ncompression ratios, and cognitive load indicators, we find that jailbreak\nattempts do not exhibit significantly higher complexity than normal\nconversations. This pattern holds consistently across specialized jailbreaking\ncommunities and general user populations, suggesting practical bounds on attack\nsophistication. Temporal analysis reveals that while user attack toxicity and\ncomplexity remains stable over time, assistant response toxicity has decreased,\nindicating improving safety mechanisms. The absence of power-law scaling in\ncomplexity distributions further points to natural limits on jailbreak\ndevelopment.\n  Our findings challenge the prevailing narrative of an escalating arms race\nbetween attackers and defenders, instead suggesting that LLM safety evolution\nis bounded by human ingenuity constraints while defensive measures continue\nadvancing. Our results highlight critical information hazards in academic\njailbreak disclosure, as sophisticated attacks exceeding current complexity\nbaselines could disrupt the observed equilibrium and enable widespread harm\nbefore defensive adaptation.", "AI": {"tldr": "The paper analyzes the complexity of jailbreak strategies in large language models (LLMs) through a mass-scale study of over 2 million conversations, revealing that jailbreak attempts are not more complex than normal conversations, indicating limits on attack sophistication and improvements in safety measures.", "motivation": "Understanding the complexity and evolution of jailbreaking strategies is critical for AI safety as LLMs are increasingly deployed.", "method": "The study conducted a mass-scale empirical analysis of jailbreak complexity using over 2 million conversations from various platforms and applied complexity metrics such as probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators.", "result": "Jailbreak attempts do not show significantly higher complexity than normal conversations, and there is a stable pattern in toxicity and complexity over time, indicating bounding constraints on both attack sophistication and safety evolution.", "conclusion": "The findings suggest that the escalation of jailbreak complexity may be limited by human ingenuity and highlight the importance of cautious jailbreak disclosures to avoid enabling harm before defensive advancements can keep pace.", "key_contributions": ["Mass-scale analysis of jailbreak complexity across diverse platforms", "Introduction of various metrics to measure complexity", "Insights into the evolution of AI safety mechanisms against jailbreaks"], "limitations": "Further research needed to explore potential future jailbreak strategies that exceed current complexity measures.", "keywords": ["large language models", "jailbreaking", "AI safety", "complexity analysis", "toxicity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08260", "pdf": "https://arxiv.org/pdf/2507.08260.pdf", "abs": "https://arxiv.org/abs/2507.08260", "title": "Do Conversational Interfaces Limit Creativity? Exploring Visual Graph Systems for Creative Writing", "authors": ["Abhinav Sood", "Maria Teresa Llano", "Jon McCormack"], "categories": ["cs.HC"], "comment": "Published in the 16th International Conference on Computational\n  Creativity, ICCC25. Accepted Paper in\n  https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025conversational.pdf", "summary": "We present a graphical, node-based system through which users can visually\nchain generative AI models for creative tasks. Research in the area of chaining\nLLMs has found that while chaining provides transparency, controllability and\nguardrails to approach certain tasks, chaining with pre-defined LLM steps\nprevents free exploration. Using cognitive processes from creativity research\nas a basis, we create a system that addresses the inherent constraints of\nchat-based AI interactions. Specifically, our system aims to overcome the\nlimiting linear structure that inhibits creative exploration and ideation.\nFurther, our node-based approach enables the creation of reusable, shareable\ntemplates that can address different creative tasks. In a small-scale user\nstudy, we find that our graph-based system supports ideation and allows some\nusers to better visualise and think through their writing process when compared\nto a similar conversational interface. We further discuss the weaknesses and\nlimitations of our system, noting the benefits to creativity that user\ninterfaces with higher complexity can provide for users who can effectively use\nthem.", "AI": {"tldr": "A graphical, node-based system for chaining generative AI models enhances creativity and ideation compared to linear chat interfaces.", "motivation": "To overcome the limitations of linear chat-based AI interactions and facilitate creative exploration in generative tasks.", "method": "Develop a node-based system enabling users to visually chain generative AI models and create reusable, shareable templates.", "result": "User study shows the graph-based system supports better ideation and visualization of the writing process than traditional conversational interfaces.", "conclusion": "Complex user interfaces can benefit creativity by allowing more freedom and control, though they may also pose challenges for users not adept in using them.", "key_contributions": ["Novel node-based system for chaining generative AI models", "Visual aids for enhancing user ideation and creativity", "Reusable templates for different creative tasks"], "limitations": "User interfaces with higher complexity may be challenging for some users to use effectively.", "keywords": ["generative AI", "creativity", "user interface", "node-based systems", "ideation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08015", "pdf": "https://arxiv.org/pdf/2507.08015.pdf", "abs": "https://arxiv.org/abs/2507.08015", "title": "Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications", "authors": ["Prudence Djagba", "Chimezie A. Odinakachukwu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work evaluates FinGPT, a financial domain-specific language model,\nacross six key natural language processing (NLP) tasks: Sentiment Analysis,\nText Classification, Named Entity Recognition, Financial Question Answering,\nText Summarization, and Stock Movement Prediction. The evaluation uses\nfinance-specific datasets to assess FinGPT's capabilities and limitations in\nreal-world financial applications. The results show that FinGPT performs\nstrongly in classification tasks such as sentiment analysis and headline\ncategorization, often achieving results comparable to GPT-4. However, its\nperformance is significantly lower in tasks that involve reasoning and\ngeneration, such as financial question answering and summarization. Comparisons\nwith GPT-4 and human benchmarks highlight notable performance gaps,\nparticularly in numerical accuracy and complex reasoning. Overall, the findings\nindicate that while FinGPT is effective for certain structured financial tasks,\nit is not yet a comprehensive solution. This research provides a useful\nbenchmark for future research and underscores the need for architectural\nimprovements and domain-specific optimization in financial language models.", "AI": {"tldr": "Evaluation of FinGPT on financial NLP tasks reveals strengths in classification but weaknesses in reasoning and generation.", "motivation": "To assess the capabilities and limitations of FinGPT in real-world financial applications across various NLP tasks.", "method": "The evaluation involved testing FinGPT on six NLP tasks using finance-specific datasets to measure its performance against benchmarks.", "result": "FinGPT showed strong performance in sentiment analysis and text classification but significantly lagged in financial question answering and text summarization compared to GPT-4 and human benchmarks.", "conclusion": "While effective for structured financial tasks, FinGPT is not a comprehensive solution, highlighting the need for further architectural improvements.", "key_contributions": ["Performance benchmark for FinGPT against GPT-4", "Identification of limitations in reasoning and generation tasks", "Framework for future enhancements in financial language models."], "limitations": "Not comprehensive for all financial NLP tasks, particularly reasoning.", "keywords": ["FinGPT", "financial NLP", "language model", "benchmark", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08624", "pdf": "https://arxiv.org/pdf/2507.08624.pdf", "abs": "https://arxiv.org/abs/2507.08624", "title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance", "authors": ["Gábor Baranyi", "Zsolt Csibi", "Kristian Fenech", "Áron Fóthi", "Zsófia Gaál", "Joul Skaf", "András Lőrincz"], "categories": ["cs.HC", "cs.AI"], "comment": "The paper has been submitted to a journal and waiting for review", "summary": "This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)\nframework, an advanced artificial intelligence-based solution tailored for home\nrehabilitation environments. AIRS integrates cutting-edge technologies,\nincluding Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and\nlarge Vision-Language Models (VLMs), to create a comprehensive system for\nmachine-guided physical rehabilitation. The general AIRS framework is\ndemonstrated in rehabilitation scenarios following total knee replacement\n(TKR), utilizing a database of 263 video recordings for evaluation. A\nsmartphone is employed within AIRS to perform RT-3DR of living spaces and has a\nbody-matched avatar to provide visual feedback about the excercise. This avatar\nis necessary in (a) optimizing exercise configurations, including camera\nplacement, patient positioning, and initial poses, and (b) addressing privacy\nconcerns and promoting compliance with the AI Act. The system guides users\nthrough the recording process to ensure the collection of properly recorded\nvideos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling\ndirect comparisons between prerecorded clinical exercises and patient home\nrecordings and (ii) VLM-generated feedback, providing detailed explanations and\ncorrections for exercise errors. The framework also supports people with visual\nand hearing impairments. It also features a modular design that can be adapted\nto broader rehabilitation contexts. AIRS software components are available for\nfurther use and customization.", "AI": {"tldr": "The AIRS framework integrates AI technologies for enhanced home rehabilitation, particularly after knee surgeries, using 3D reconstruction and vision-language models.", "motivation": "To develop an advanced AI-based solution for home rehabilitation, addressing needs in post-surgery recovery and catering to diverse user capabilities.", "method": "The AIRS framework utilizes Real-Time 3D Reconstruction, intelligent navigation, and Vision-Language Models to support machine-guided rehabilitation.", "result": "AIRS was evaluated using 263 video recordings from post-total knee replacement rehabilitation, demonstrating effective exercise feedback and guidance mechanisms.", "conclusion": "AIRS offers an adaptable, privacy-conscious rehabilitation solution that supports diverse user needs and encourages exercise compliance.", "key_contributions": ["Integration of RT-3DR for home rehabilitation", "Use of VLMs for personalized feedback", "Support for users with visual and hearing impairments"], "limitations": "", "keywords": ["Ambient Intelligence", "Rehabilitation", "AI", "Vision-Language Models", "Home Care"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.08017", "pdf": "https://arxiv.org/pdf/2507.08017.pdf", "abs": "https://arxiv.org/abs/2507.08017", "title": "Mechanistic Indicators of Understanding in Large Language Models", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "categories": ["cs.CL", "cs.AI"], "comment": "32 pages", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. Here, we offer an accessible\nsynthesis of these findings that doubles as an introduction to MI, all while\nintegrating these findings within a novel theoretical framework for thinking\nabout machine understanding. We argue that LLMs develop internal structures\nthat are functionally analogous to the kind of understanding that consists in\nseeing connections. To sharpen this idea, we propose a three-tiered conception\nof machine understanding. First, conceptual understanding emerges when a model\nforms \"features\" as directions in latent space, thereby learning the\nconnections between diverse manifestations of something. Second,\nstate-of-the-world understanding emerges when a model learns contingent factual\nconnections between features and dynamically tracks changes in the world.\nThird, principled understanding emerges when a model ceases to rely on a\ncollection of memorized facts and discovers a \"circuit\" that connects these\nfacts. However, we conclude by exploring the \"parallel mechanisms\" phenomenon,\narguing that while LLMs exhibit forms of understanding, their cognitive\narchitecture remains different from ours, and the debate should shift from\nwhether LLMs understand to how their strange minds work.", "AI": {"tldr": "The paper synthesizes findings in mechanistic interpretability of Large Language Models (LLMs) and presents a novel framework for understanding machine cognition.", "motivation": "To challenge the notion that LLMs rely solely on superficial statistics and to explore the mechanisms behind their interpretability and understanding.", "method": "The authors propose a theoretical framework comprising three tiers of machine understanding: conceptual, state-of-the-world, and principled understanding.", "result": "The study argues that LLMs develop internal structures similar to human understanding, allowing them to see connections but differ fundamentally in cognitive architecture from humans.", "conclusion": "The authors advocate shifting focus from whether LLMs understand to understanding the nature of their cognitive processes and mechanisms.", "key_contributions": ["Introduction of a three-tiered framework for machine understanding", "Synthesis of existing findings in mechanistic interpretability", "Exploration of the differences between LLM and human cognition"], "limitations": "", "keywords": ["mechanistic interpretability", "Large Language Models", "machine understanding", "cognitive architecture", "AI"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.08659", "pdf": "https://arxiv.org/pdf/2507.08659.pdf", "abs": "https://arxiv.org/abs/2507.08659", "title": "Push or Light: Nudging Standing to Break Prolonged Sitting", "authors": ["Sohshi Yoshida", "Ko Watanabe", "Andreas Dengel", "Shoya Ishimaru", "Shingo Ata", "Manato Fujimoto"], "categories": ["cs.HC"], "comment": null, "summary": "Prolonged sitting is a health risk leading to metabolic and cardiovascular\ndiseases. To combat this, various \"nudging\" strategies encourage stand-ups.\nBehavior change triggers use explicit prompts such as smartphone push\nnotifications or light controls. However, comparisons of the effects of such\ninteractions, discomfort, and user context have not yet been performed. The\npresent study evaluated these methods in a mixed design experiment with 15\ncollege students. Three intervention methods (none, push notifications, and\nlight dimming) and three user task contexts (computer work, video calls, and\nreading) were tested. The frequency of standing up and comfort were assessed\nafter each ten-minute session. Results showed that dimming resulted in slightly\nmore breaks (1.4 \\pm 1.55) than push notification (1.2 \\pm 1.08), but caused\ndiscomfort for 66.7% of participants, compared to 20% for notification. The\nresults were influenced by task context. Dimming was most effective during\nvideo calls and reading, while push notifications were more effective during\ncomputer work. These findings suggest adaptive nudging systems should tailor\ninterventions based on context and individual preferences.", "AI": {"tldr": "The study investigates the effectiveness of nudging strategies (push notifications vs. light dimming) to encourage standing up during different user task contexts.", "motivation": "To address health risks associated with prolonged sitting and evaluate different nudging strategies in promoting stand-up behavior.", "method": "A mixed design experiment was conducted with 15 college students to compare the effects of no intervention, push notifications, and light dimming across three task contexts.", "result": "Dimming led to slightly more standing breaks than push notifications, but it caused discomfort for a higher percentage of participants. The effectiveness varied by task context: dimming was best during video calls and reading; notifications were better for computer work.", "conclusion": "Adaptive nudging systems should be developed to tailor strategies based on user context and preferences to effectively promote standing behavior.", "key_contributions": ["Evaluation of multiple nudging strategies for stand-up behavior", "Impact of task context on effectiveness of interventions", "Insights into participant discomfort with nudging methods"], "limitations": "Study limited to a small sample size and short duration of tasks.", "keywords": ["nudging", "behavior change", "user context", "standing breaks", "health informatics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08018", "pdf": "https://arxiv.org/pdf/2507.08018.pdf", "abs": "https://arxiv.org/abs/2507.08018", "title": "Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation", "authors": ["Nikita Mounier", "Parsa Idehpour"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at Methods and Opportunities at Small Scale (MOSS), ICML\n  2025", "summary": "A key challenge for iterative text generation is enabling models to\nefficiently identify and correct their own errors. We propose Review, Remask,\nRefine (R3), a relatively simple yet elegant framework that requires no\nadditional model training and can be applied to any pre-trained masked text\ndiffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is\nutilized for the Review of intermediate generated blocks. The framework then\ntranslates these PRM scores into a Remask strategy: the lower a block's PRM\nscore, indicating potential mistakes, the greater the proportion of tokens\nwithin that block are remasked. Finally, the model is compelled to Refine these\ntargeted segments, focusing its efforts more intensively on specific\nsub-optimal parts of past generations, leading to improved final output.", "AI": {"tldr": "Proposes a framework called Review, Remask, Refine (R3) for improving text generation by correcting model errors through a structured approach using a Process Reward Model.", "motivation": "To address the challenge of iterative text generation where models need to identify and rectify their own errors effectively.", "method": "The R3 framework leverages a Process Reward Model (PRM) to review generated text blocks, applying a Remask strategy based on PRM scores, and focuses refinement efforts on sub-optimal segments.", "result": "R3 enhances the quality of generated outputs by directing more attention to parts identified as problematic, leading to more accurate text generation.", "conclusion": "R3 is a practical and effective solution for optimizing pre-trained masked text diffusion models without the need for additional training.", "key_contributions": ["Introduction of the R3 framework for text generation error correction", "Utilization of a Process Reward Model for targeted remasking", "Improves end output quality with minimal adjustments to existing models."], "limitations": "", "keywords": ["text generation", "error correction", "Process Reward Model", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.08675", "pdf": "https://arxiv.org/pdf/2507.08675.pdf", "abs": "https://arxiv.org/abs/2507.08675", "title": "LIMITER: A Gamified Interface for Harnessing Just Intonation Systems", "authors": ["Antonis Christou"], "categories": ["cs.HC"], "comment": "6 pages, 11 figures, NIME, 2025", "summary": "This paper introduces LIMITER, a gamified digital musical instrument for\nharnessing and performing microtonal and justly intonated sounds. While\nmicrotonality in Western music remains a niche and esoteric system that can be\ndifficult both to conceptualize and to perform with, LIMITER presents a novel,\neasy to pickup interface that utilizes color, geometric transformations, and\ngame-like controls to create a simpler inlet into utilizing these sounds as a\nmeans of expression. We report on the background of the development of LIMITER,\nas well as explain the underlying musical and engineering systems that enable\nits function. Additionally, we offer a discussion and preliminary evaluation of\nthe creativity-enhancing effects of the interface.", "AI": {"tldr": "LIMITER is a gamified digital musical instrument that simplifies the performance of microtonal music using a user-friendly interface.", "motivation": "To make microtonality in Western music more accessible and expressive through an engaging interface.", "method": "Developed a digital instrument that combines color, geometric transformations, and game-like controls to facilitate the performance of microtonal sounds.", "result": "Preliminary evaluations suggest that the LIMITER interface enhances creativity in music performance.", "conclusion": "LIMITER offers a novel approach to performing microtonal music, potentially broadening its appeal and usability.", "key_contributions": ["Creation of an intuitive interface for microtonal sounds", "Integration of gamification principles into musical performance", "Preliminary evidence of creativity enhancement with the interface"], "limitations": "", "keywords": ["microtonality", "digital instrument", "gamification", "music performance", "creativity"], "importance_score": 2, "read_time_minutes": 6}}
{"id": "2507.08019", "pdf": "https://arxiv.org/pdf/2507.08019.pdf", "abs": "https://arxiv.org/abs/2507.08019", "title": "Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks", "authors": ["Aryan Varshney", "Venkat Ram Reddy Ganuthula"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This study investigates whether large language models (LLMs) exhibit\nconsistent behavior (signal) or random variation (noise) when screening resumes\nagainst job descriptions, and how their performance compares to human experts.\nUsing controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)\nacross contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)\nwith identical and randomized resumes, benchmarked against three human\nrecruitment experts. Analysis of variance revealed significant mean differences\nin four of eight LLM-only conditions and consistently significant differences\nbetween LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts\nstrongly to company context (p < 0.001), Gemini partially (p = 0.038 for\nFirm1), and Claude minimally (p > 0.1), while all LLMs differed significantly\nfrom human experts across contexts. Meta-cognition analysis highlighted\nadaptive weighting patterns that differ markedly from human evaluation\napproaches. Findings suggest LLMs offer interpretable patterns with detailed\nprompts but diverge substantially from human judgment, informing their\ndeployment in automated hiring systems.", "AI": {"tldr": "This study analyzes the behavior of large language models (LLMs) in resume screening compared to human experts, revealing significant performance differences and insights into LLM adaptability.", "motivation": "To investigate the reliability of LLMs in screening resumes and their comparison with human recruitment experts in terms of consistent behavior versus random variation.", "method": "Controlled datasets were used to test three LLMs (Claude, GPT, and Gemini) against human experts across various contexts and conditions, with statistical analysis to evaluate performance differences.", "result": "Significant differences were found in LLM evaluations both among themselves and against human experts, particularly in response to the company context.", "conclusion": "LLMs can provide interpretable patterns in resume screening but exhibit substantial divergence from human judgment, which is crucial for understanding their role in automated hiring.", "key_contributions": ["Demonstrates significant performance differences between LLMs and human evaluators in resume screening tasks.", "Showcases the adaptive behavior of different LLMs in varying company contexts.", "Highlights the limitations of LLMs in mimicking human judgment in recruitment processes."], "limitations": "Limited to specific LLMs tested and controlled settings; may not represent all LLMs or real-world scenarios.", "keywords": ["large language models", "resume screening", "human evaluation", "recruitment", "adaptive behavior"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08744", "pdf": "https://arxiv.org/pdf/2507.08744.pdf", "abs": "https://arxiv.org/abs/2507.08744", "title": "EqualMotion: Accessible Motion Capture for the Creative Industries", "authors": ["Clarice Hilton", "Kat Hawkins", "Phill Tew", "Freddie Collins", "Seb Madgwick", "Dominic Potts", "Tom Mitchell"], "categories": ["cs.HC"], "comment": null, "summary": "Motion capture technologies are increasingly used in creative and performance\ncontexts but often exclude disabled practitioners due to normative assumptions\nin body modeling, calibration, and avatar representation. EqualMotion\nintroduces a body-agnostic, wearable motion capture system designed through a\ndisability-centred co-design approach. By enabling personalised calibration,\nintegrating mobility aids, and adopting an inclusive visual language,\nEqualMotion supports diverse body types and movement styles. The system is\ndeveloped collaboratively with disabled researchers and creatives, aiming to\nfoster equitable participation in digital performance and prototyping. This\npaper outlines the system's design principles and highlights ongoing case\nstudies in dance and music to evaluate accessibility in real-world creative\nworkflows.", "AI": {"tldr": "EqualMotion is a wearable motion capture system designed to include disabled practitioners by adopting a disability-centered co-design approach, enabling personalized calibration and supporting diverse body types.", "motivation": "To address the exclusion of disabled practitioners in motion capture technologies through normative assumptions in body modeling and representation.", "method": "The system was developed using a body-agnostic design, allowing for personalized calibration and integration of mobility aids, assessed through ongoing case studies in dance and music.", "result": "EqualMotion demonstrates increased accessibility in digital performance workflows by supporting a variety of body types and movement styles, allowing for equitable participation.", "conclusion": "The project aims to foster inclusive practices in creative industries by enabling disabled individuals to engage fully in motion capture and performance.", "key_contributions": ["Development of a body-agnostic motion capture system", "Integration of personalized calibration for diverse users", "Collaboration with disabled researchers for inclusive design"], "limitations": "", "keywords": ["motion capture", "disability-inclusive design", "digital performance", "wearable technology", "accessibility"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08020", "pdf": "https://arxiv.org/pdf/2507.08020.pdf", "abs": "https://arxiv.org/abs/2507.08020", "title": "Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation", "authors": ["Zhibo Zhang", "Yuxi Li", "Kailong Wang", "Shuai Yuan", "Ling Shi", "Haoyu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across domains\nsuch as healthcare, education, and cybersecurity. However, this openness also\nintroduces significant security risks, particularly through embedding space\npoisoning, which is a subtle attack vector where adversaries manipulate the\ninternal semantic representations of input data to bypass safety alignment\nmechanisms. While previous research has investigated universal perturbation\nmethods, the dynamics of LLM safety alignment at the embedding level remain\ninsufficiently understood. Consequently, more targeted and accurate adversarial\nperturbation techniques, which pose significant threats, have not been\nadequately studied.\n  In this work, we propose ETTA (Embedding Transformation Toxicity\nAttenuation), a novel framework that identifies and attenuates\ntoxicity-sensitive dimensions in embedding space via linear transformations.\nETTA bypasses model refusal behaviors while preserving linguistic coherence,\nwithout requiring model fine-tuning or access to training data. Evaluated on\nfive representative open-source LLMs using the AdvBench benchmark, ETTA\nachieves a high average attack success rate of 88.61%, outperforming the best\nbaseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR\non instruction-tuned defenses). These results highlight a critical\nvulnerability in current alignment strategies and underscore the need for\nembedding-aware defenses.", "AI": {"tldr": "This paper presents ETTA, a framework that targets and mitigates toxicity in the embedding space of LLMs, highlighting vulnerabilities in current safety alignment approaches.", "motivation": "To address significant security risks in LLMs due to embedding space poisoning and the insufficient understanding of embedding-level safety alignment.", "method": "The proposed ETTA framework identifies toxicity-sensitive dimensions in the embedding space and applies linear transformations to attenuate these risks without needing model fine-tuning or training data.", "result": "ETTA demonstrates an average attack success rate of 88.61% on five open-source LLMs, outperforming existing methods.", "conclusion": "The findings reveal critical vulnerabilities in current alignment strategies, emphasizing the importance of embedding-aware defenses in LLM applications.", "key_contributions": ["Introduction of ETTA framework for toxicity mitigation in LLMs", "Demonstrated high attack success rate of 88.61%", "Addressing the gap in understanding embedding-level safety alignment"], "limitations": "", "keywords": ["Large Language Models", "embedding space poisoning", "toxicity attenuation", "adversarial perturbation", "safety alignment"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08021", "pdf": "https://arxiv.org/pdf/2507.08021.pdf", "abs": "https://arxiv.org/abs/2507.08021", "title": "Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis", "authors": ["Li Li", "Yongliang Wu", "Jingze Zhu", "Jiawei Peng", "Jianfei Cai", "Xu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 11 figures", "summary": "The evolution of large models has witnessed the emergence of In-Context\nLearning (ICL) capabilities. In Natural Language Processing (NLP), numerous\nstudies have demonstrated the effectiveness of ICL. Inspired by the success of\nLarge Language Models (LLMs), researchers have developed Large Multimodal\nModels (LMMs) with ICL capabilities. However, explorations of demonstration\nconfiguration for multimodal ICL remain preliminary. Additionally, the\ncontrollability of In-Context Examples (ICEs) provides an efficient and\ncost-effective means to observe and analyze the inference characteristics of\nLMMs under varying inputs. This paper conducts a comprehensive external and\ninternal investigation of multimodal in-context learning on the image\ncaptioning task. Externally, we explore demonstration configuration strategies\nthrough three dimensions: shot number, image retrieval, and caption assignment.\nWe employ multiple metrics to systematically and thoroughly evaluate and\nsummarize key findings. Internally, we analyze typical LMM attention\ncharacteristics and develop attention-based metrics to quantify model\nbehaviors. We also conduct auxiliary experiments to explore the feasibility of\nattention-driven model acceleration and compression. We further compare\nperformance variations between LMMs with identical model design and pretraining\nstrategies and explain the differences from the angles of pre-training data\nfeatures. Our study reveals both how ICEs configuration strategies impact model\nperformance through external experiments and characteristic typical patterns\nthrough internal inspection, providing dual perspectives for understanding\nmultimodal ICL in LMMs. Our method of combining external and internal analysis\nto investigate large models, along with our newly proposed metrics, can be\napplied to broader research areas.", "AI": {"tldr": "This paper investigates the In-Context Learning (ICL) capabilities of Large Multimodal Models (LMMs) on image captioning tasks through both external and internal analyses.", "motivation": "To explore demonstration configuration strategies for multimodal In-Context Learning (ICL) and analyze inference characteristics of Large Multimodal Models (LMMs).", "method": "The study conducts external experiments examining shot number, image retrieval, and caption assignment in ICL, using multiple metrics for evaluation. Internally, attention characteristics of LMMs are analyzed with newly developed metrics for quantifying model behaviors.", "result": "The research reveals how configuration strategies for In-Context Examples (ICEs) affect model performance and identifies typical patterns in LMM behaviors through dual analyses.", "conclusion": "Combining external and internal analyses to study LMMs enhances understanding of multimodal ICL, with proposed metrics applicable in wider research contexts.", "key_contributions": ["Investigated demonstration configuration strategies for multimodal ICL.", "Developed attention-based metrics for analyzing model behaviors.", "Revealed performance variations in LMMs based on pre-training data features."], "limitations": "", "keywords": ["In-Context Learning", "Large Multimodal Models", "Image Captioning", "Attention Metrics", "NLP"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2507.08030", "pdf": "https://arxiv.org/pdf/2507.08030.pdf", "abs": "https://arxiv.org/abs/2507.08030", "title": "A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models", "authors": ["Sonali Sharma", "Ahmed M. Alaa", "Roxana Daneshjou"], "categories": ["cs.CL", "cs.CE", "cs.HC"], "comment": "11 pages, 5 figures", "summary": "Generative AI models, including large language models (LLMs) and\nvision-language models (VLMs), are increasingly used to interpret medical\nimages and answer clinical questions. Their responses often include\ninaccuracies; therefore, safety measures like medical disclaimers are critical\nto remind users that AI outputs are not professionally vetted or a substitute\nfor medical advice. This study evaluated the presence of disclaimers in LLM and\nVLM outputs across model generations from 2022 to 2025. Using 500 mammograms,\n500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs\nwere screened for disclaimer phrases. Medical disclaimer presence in LLM and\nVLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023\nto 1.05% in 2025, respectively. By 2025, the majority of models displayed no\ndisclaimers. As public models become more capable and authoritative,\ndisclaimers must be implemented as a safeguard adapting to the clinical context\nof each output.", "AI": {"tldr": "This study analyzes the decline of medical disclaimers in LLM and VLM outputs from 2022 to 2025, highlighting the safety concerns surrounding AI interpretations of medical images.", "motivation": "To address the safety concerns arising from growing use of generative AI models in medical contexts, particularly the lack of disclaimers in AI-generated outputs.", "method": "The study evaluated outputs from LLMs and VLMs across four types of medical images and questions, screening 2000 outputs for the presence of disclaimer phrases.", "result": "The presence of medical disclaimers in AI outputs significantly decreased from 26.3% in 2022 to 0.97% in 2025 for LLMs, and from 19.6% in 2023 to 1.05% in 2025 for VLMs, indicating a concerning trend.", "conclusion": "As AI models advance and their authority grows, it is essential to enforce disclaimers consciously tailored to the clinical context to ensure user safety.", "key_contributions": ["Quantitative analysis of the decline in medical disclaimers in AI outputs over time.", "Discussion on the implications of declining disclaimer presence for patient safety in AI medical applications.", "Recommendations for implementing tailored disclaimers in medical AI contexts."], "limitations": "The study focuses solely on certain model generations and does not evaluate the accuracy of the outputs themselves.", "keywords": ["Generative AI", "Large Language Models", "Vision-Language Models", "Medical Disclaimers", "Medical Imaging"], "importance_score": 9, "read_time_minutes": 11}}
{"id": "2507.08027", "pdf": "https://arxiv.org/pdf/2507.08027.pdf", "abs": "https://arxiv.org/abs/2507.08027", "title": "\"Amazing, They All Lean Left\" -- Analyzing the Political Temperaments of Current LLMs", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah", "Kund Meghani"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent studies have revealed a consistent liberal orientation in the ethical\nand political responses generated by most commercial large language models\n(LLMs), yet the underlying causes and resulting implications remain unclear.\nThis paper systematically investigates the political temperament of seven\nprominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity\n(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat\nand High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes\nMoral Foundations Theory, a dozen established political ideology scales and a\nnew index of current political controversies. We find strong and consistent\nprioritization of liberal-leaning values, particularly care and fairness,\nacross most models. Further analysis attributes this trend to four overlapping\nfactors: Liberal-leaning training corpora, reinforcement learning from human\nfeedback (RLHF), the dominance of liberal frameworks in academic ethical\ndiscourse and safety-driven fine-tuning practices. We also distinguish between\npolitical \"bias\" and legitimate epistemic differences, cautioning against\nconflating the two. A comparison of base and fine-tuned model pairs reveals\nthat fine-tuning generally increases liberal lean, an effect confirmed through\nboth self-report and empirical testing. We argue that this \"liberal tilt\" is\nnot a programming error or the personal preference of programmers but an\nemergent property of training on democratic rights-focused discourse. Finally,\nwe propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance\nphilosophical aspiration, reflecting a moral stance unanchored to personal\nidentity or interest. Rather than undermining democratic discourse, this\npattern may offer a new lens through which to examine collective reasoning.", "AI": {"tldr": "This paper investigates the political temperament of various large language models, revealing a consistent liberal orientation in their ethical and political responses, attributed to factors like training corpora and fine-tuning practices.", "motivation": "To understand the underlying causes and implications of the observed liberal orientation in the responses generated by commercial large language models (LLMs).", "method": "The study employs Moral Foundations Theory, multiple established political ideology scales, and a new index of current political controversies to analyze the political temperament of seven prominent LLMs.", "result": "The analysis shows strong prioritization of liberal-leaning values, particularly care and fairness, across most models, influenced by their training data and fine-tuning processes.", "conclusion": "The liberal tilt observed in LLMs is an emergent property rather than a programming error, suggesting that these models could reflect a moral stance that enhances democratic discourse.", "key_contributions": ["Systematic investigation of the political temperament of seven LLMs.", "Insights into the influence of training data and reinforcement learning on political bias.", "Clarification of the distinction between political bias and epistemic differences."], "limitations": "The analysis is limited to seven prominent LLMs and may not generalize to all models.", "keywords": ["large language models", "political orientation", "liberal values", "ethical discourse", "training corpora"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.08029", "pdf": "https://arxiv.org/pdf/2507.08029.pdf", "abs": "https://arxiv.org/abs/2507.08029", "title": "Better Together: Quantifying the Benefits of AI-Assisted Recruitment", "authors": ["Ada Aka", "Emil Palikot", "Ali Ansari", "Nima Yazdani"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Artificial intelligence (AI) is increasingly used in recruitment, yet\nempirical evidence quantifying its impact on hiring efficiency and candidate\nselection remains limited. We randomly assign 37,000 applicants for a\njunior-developer position to either a traditional recruitment process (resume\nscreening followed by human selection) or an AI-assisted recruitment pipeline\nincorporating an initial AI-driven structured video interview before human\nevaluation. Candidates advancing from either track faced the same final-stage\nhuman interview, with interviewers blind to the earlier selection method. In\nthe AI-assisted pipeline, 54% of candidates passed the final interview compared\nwith 34% from the traditional pipeline, yielding an average treatment effect of\n20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn\nprofiles of top applicants from both groups and found that 18% (SE 1.1%) of\napplicants from the traditional track found new jobs compared with 23% (SE\n2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the\nprobability of finding new employment between groups. The AI system tended to\nselect younger applicants with less experience and fewer advanced credentials.\nWe analyze AI-generated interview transcripts to examine the selection criteria\nand conversational dynamics. Our findings contribute to understanding how AI\ntechnologies affect decision making in recruitment and talent acquisition while\nhighlighting some of their potential implications.", "AI": {"tldr": "This paper evaluates the impact of AI-assisted recruitment on hiring efficiency and candidate outcomes compared to traditional methods.", "motivation": "To investigate the empirical effects of AI on recruitment processes and hiring outcomes, particularly in tech job roles.", "method": "A randomized assignment of 37,000 applicants to either a traditional recruitment process or an AI-assisted pipeline, followed by analysis of interview outcomes and subsequent job placements.", "result": "Candidates in the AI-assisted pipeline had a significantly higher pass rate in final interviews (54%) compared to traditional methods (34%), and a greater likelihood of finding new jobs (23% vs. 18%).", "conclusion": "AI technologies positively impact hiring effectiveness but tend to favor less experienced candidates, raising important considerations for selection criteria.", "key_contributions": ["Demonstrates the effectiveness of AI in improving hiring outcomes.", "Identifies biases in AI selection favoring younger and less experienced candidates.", "Provides insights into AI-driven interview dynamics and criteria."], "limitations": "The study focused only on junior-developer positions; broader applicability across other roles is uncertain.", "keywords": ["AI", "recruitment", "hiring efficiency", "candidate selection", "interview dynamics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.08030", "pdf": "https://arxiv.org/pdf/2507.08030.pdf", "abs": "https://arxiv.org/abs/2507.08030", "title": "A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models", "authors": ["Sonali Sharma", "Ahmed M. Alaa", "Roxana Daneshjou"], "categories": ["cs.CL", "cs.CE", "cs.HC"], "comment": "11 pages, 5 figures", "summary": "Generative AI models, including large language models (LLMs) and\nvision-language models (VLMs), are increasingly used to interpret medical\nimages and answer clinical questions. Their responses often include\ninaccuracies; therefore, safety measures like medical disclaimers are critical\nto remind users that AI outputs are not professionally vetted or a substitute\nfor medical advice. This study evaluated the presence of disclaimers in LLM and\nVLM outputs across model generations from 2022 to 2025. Using 500 mammograms,\n500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs\nwere screened for disclaimer phrases. Medical disclaimer presence in LLM and\nVLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023\nto 1.05% in 2025, respectively. By 2025, the majority of models displayed no\ndisclaimers. As public models become more capable and authoritative,\ndisclaimers must be implemented as a safeguard adapting to the clinical context\nof each output.", "AI": {"tldr": "This study analyzes the presence of medical disclaimers in outputs from LLM and VLM models interpreting medical images and answering clinical questions, finding a significant decrease in disclaimers from 2022 to 2025.", "motivation": "To evaluate the critical role of medical disclaimers in AI models interpreting healthcare data and to ensure user awareness of the potential inaccuracies in AI outputs.", "method": "The study screened outputs from various generative AI models using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions for the presence of disclaimer phrases from 2022 to 2025.", "result": "The study found that the presence of medical disclaimers in outputs dropped significantly: from 26.3% in 2022 to 0.97% in 2025 for LLMs, and from 19.6% in 2023 to 1.05% in 2025 for VLMs.", "conclusion": "As generative AI models become more authoritative, implementing disclaimers is essential to safeguard users and adapt to the clinical context.", "key_contributions": ["Investigated the trend of medical disclaimers in AI outputs over a three-year period.", "Highlighted the critical need for safety measures in the deployment of AI models in healthcare applications.", "Provided insights into user safety and trust in AI-generated medical content."], "limitations": "The study is limited to the evaluation of specific medical images and may not represent all potential scenarios in healthcare settings.", "keywords": ["Generative AI", "Medical Disclaimers", "Large Language Models", "Vision-Language Models", "Healthcare AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08031", "pdf": "https://arxiv.org/pdf/2507.08031.pdf", "abs": "https://arxiv.org/abs/2507.08031", "title": "Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding", "authors": ["Hong Jia", "Shiya Fu", "Vassilis Kostakos", "Feng Xia", "Ting Dang"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of Small Language Models (SLMs) as privacy-preserving\nalternatives for sensitive applications raises a fundamental question about\ntheir inherent understanding capabilities compared to Large Language Models\n(LLMs). This paper investigates the mental health understanding capabilities of\ncurrent SLMs through systematic evaluation across diverse classification tasks.\nEmploying zero-shot and few-shot learning paradigms, we benchmark their\nperformance against established LLM baselines to elucidate their relative\nstrengths and limitations in this critical domain. We assess five\nstate-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against\nthree LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding\ntasks. Our findings reveal that SLMs achieve mean performance within 2\\% of\nLLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot\nsettings), demonstrating notable competence despite orders of magnitude fewer\nparameters. Both model categories experience similar degradation on multi-class\nseverity tasks (a drop of over 30\\%), suggesting that nuanced clinical\nunderstanding challenges transcend model scale. Few-shot prompting provides\nsubstantial improvements for SLMs (up to 14.6\\%), while LLM gains are more\nvariable. Our work highlights the potential of SLMs in mental health\nunderstanding, showing they can be effective privacy-preserving tools for\nanalyzing sensitive online text data. In particular, their ability to quickly\nadapt and specialize with minimal data through few-shot learning positions them\nas promising candidates for scalable mental health screening tools.", "AI": {"tldr": "This paper evaluates the mental health understanding capabilities of Small Language Models (SLMs) compared to Large Language Models (LLMs) through systematic classification tasks.", "motivation": "The paper investigates the potential of Small Language Models as privacy-preserving alternatives to Large Language Models in sensitive applications like mental health understanding.", "method": "The study employs zero-shot and few-shot learning paradigms to benchmark five state-of-the-art SLMs against three established LLMs across six mental health understanding tasks.", "result": "SLMs perform competitively with LLMs on binary classification tasks but show similar degradation on multi-class severity tasks; few-shot learning significantly enhances SLM performance.", "conclusion": "SLMs exhibit notable capabilities for mental health understanding and can serve as effective tools for analyzing sensitive online text data, particularly in scalable screening applications.", "key_contributions": ["Evaluation of SLMs in mental health tasks", "Comparison of SLMs and LLMs on performance metrics", "Insights on few-shot learning impacts on SLM efficiency"], "limitations": "The study highlights that both SLMs and LLMs struggle with nuanced clinical understanding in multi-class tasks.", "keywords": ["Small Language Models", "Large Language Models", "mental health understanding", "few-shot learning", "privacy-preserving AI"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2503.15488", "pdf": "https://arxiv.org/pdf/2503.15488.pdf", "abs": "https://arxiv.org/abs/2503.15488", "title": "Human-AI Collaboration for Wearable Technology Component Standardization", "authors": ["Andrew M. Lydner"], "categories": ["cs.HC"], "comment": null, "summary": "Due to the multidisciplinary nature of wearable technology, the industry\nfaces potential limitations in innovation. The wearable technology industry is\nstill in its infancy and increased applicable use faces stagnation despite the\nplethora of technologies that have been largely wrist worn. This could be a\nresult of the lack of multidisciplinary expert knowledge disseminating through\nthe industry. Unlike other technologies which have standardizations and\nprocesses for how they are developed, wearable technologies exist in a realm of\nperpetual change as given the various materials and subcomponents that continue\nto be developed. It is essential that expert opinions form a collaborative\nfoundation, and even more so that intelligent systems foster that\ncollaboration. The caveat though, is likeliness of these artificial\nintelligence (AI) collaboration tools to be utilized by industry experts.\nMental model development for AI tool usage could be applied to wearable\ntechnology innovation in this regard, thus the goal of this paper and focus of\nresearch.", "AI": {"tldr": "The paper discusses the challenges in the wearable technology industry due to a lack of multidisciplinary collaboration and proposes the use of AI collaboration tools to enhance innovation.", "motivation": "The wearable technology industry, despite its potential, is experiencing stagnation in innovation due to its multidisciplinary nature and a lack of collaborative expert knowledge.", "method": "The paper explores the application of mental model development for AI tool usage among industry experts to foster collaboration in wearable technology innovation.", "result": "It highlights the necessity of establishing a collaborative foundation among experts to drive innovation in wearable technologies through AI tools.", "conclusion": "The involvement of intelligent systems is crucial for enhancing collaboration in the industry, which could lead to innovative solutions in wearable technology.", "key_contributions": ["Identifies the stagnation in wearable technology innovation due to limited expert collaboration.", "Proposes the use of AI collaboration tools to facilitate interdisciplinary engagement among industry professionals.", "Suggests mental model development as a strategy to improve AI tool utilization in the context of wearable technology."], "limitations": "The paper may not address specific implementation strategies for AI tools or the challenges in ensuring their adoption by experts.", "keywords": ["wearable technology", "collaboration", "artificial intelligence", "multidisciplinary", "innovation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.08034", "pdf": "https://arxiv.org/pdf/2507.08034.pdf", "abs": "https://arxiv.org/abs/2507.08034", "title": "Integrating External Tools with Large Language Models to Improve Accuracy", "authors": ["Nripesh Niketan", "Hadj Batatia"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6"], "comment": "9 pages, 3 figures, 2 tables. Extended version of paper published in\n  Proceedings of International Conference on Information Technology and\n  Applications, Springer Nature Singapore, 2025, pp. 409-421. This version\n  includes additional experimental results comparing against GPT-4o,\n  LLaMA-Large, Mistral-Large, and Phi-Large, expanded evaluation methodology,\n  and enhanced analysis", "summary": "This paper deals with improving querying large language models (LLMs). It is\nwell-known that without relevant contextual information, LLMs can provide poor\nquality responses or tend to hallucinate. Several initiatives have proposed\nintegrating LLMs with external tools to provide them with up-to-date data to\nimprove accuracy. In this paper, we propose a framework to integrate external\ntools to enhance the capabilities of LLMs in answering queries in educational\nsettings. Precisely, we develop a framework that allows accessing external APIs\nto request additional relevant information. Integrated tools can also provide\ncomputational capabilities such as calculators or calendars. The proposed\nframework has been evaluated using datasets from the Multi-Modal Language\nUnderstanding (MMLU) collection. The data consists of questions on mathematical\nand scientific reasoning. Results compared to state-of-the-art language models\nshow that the proposed approach significantly improves performance. Our Athena\nframework achieves 83% accuracy in mathematical reasoning and 88% in scientific\nreasoning, substantially outperforming all tested models including GPT-4o,\nLLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline\nmodel (LLaMA-Large) achieving only 67% and 79% respectively. These promising\nresults open the way to creating complex computing ecosystems around LLMs to\nmake their use more natural to support various tasks and activities.", "AI": {"tldr": "The paper presents a framework for enhancing large language models (LLMs) by integrating external tools and APIs to provide relevant contextual information, significantly improving query performance in educational settings.", "motivation": "To address the limitations of LLMs in providing accurate responses without relevant context and reduce hallucination in their outputs.", "method": "Development of the Athena framework, which integrates external APIs for accessing additional information and computational capabilities, evaluated using datasets from the Multi-Modal Language Understanding (MMLU) collection.", "result": "Achieved 83% accuracy in mathematical reasoning and 88% in scientific reasoning, outperforming state-of-the-art models such as GPT-4o and LLaMA-Large.", "conclusion": "The framework shows promise for creating better computing ecosystems around LLMs to support various educational tasks and activities effectively.", "key_contributions": ["Introduction of the Athena framework for LLMs", "Demonstrated significant improvement over baseline models in educational contexts", "Evaluation with comprehensive datasets from MMLU"], "limitations": "", "keywords": ["Large Language Models", "Framework Integration", "Educational Technology"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.16521", "pdf": "https://arxiv.org/pdf/2503.16521.pdf", "abs": "https://arxiv.org/abs/2503.16521", "title": "Conversational Self-Play for Discovering and Understanding Psychotherapy Approaches", "authors": ["Onno P Kampman", "Michael Xing", "Charmaine Lim", "Ahmad Ishqi Jabir", "Ryan Louie", "Jimmy Lee", "Robert JT Morris"], "categories": ["cs.HC", "cs.MA"], "comment": "Improve writing, add multiple techniques extraction", "summary": "This paper explores conversational self-play with LLMs as a scalable approach\nfor analyzing and exploring psychotherapy approaches, evaluating how well\nAI-generated therapeutic dialogues align with established modalities.", "AI": {"tldr": "Explores conversational self-play with LLMs to analyze and evaluate therapeutic dialogues in psychotherapy.", "motivation": "To evaluate how AI-generated therapeutic dialogues align with established psychotherapy modalities and enhance the understanding of these approaches through scalable methods.", "method": "Utilizes conversational self-play with large language models (LLMs) to generate and assess therapeutic conversations.", "result": "Findings reveal insights into the alignment and effectiveness of AI-generated dialogues compared to traditional psychotherapy methods.", "conclusion": "The study suggests that conversational self-play can be a valuable tool for analyzing psychotherapy approaches and understanding the nuances of therapeutic dialogue generation.", "key_contributions": ["Introduces conversational self-play as a method for evaluating psychotherapy dialogues", "Demonstrates the potential of AI in understanding therapeutic modalities", "Provides insights into the effectiveness of LLMs in generating human-like therapeutic conversations."], "limitations": "The paper notes a need for improvement in writing clarity and the extraction of multiple techniques.", "keywords": ["conversational self-play", "LLMs", "psychotherapy", "AI dialogue", "therapeutic modalities"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08036", "pdf": "https://arxiv.org/pdf/2507.08036.pdf", "abs": "https://arxiv.org/abs/2507.08036", "title": "Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights", "authors": ["Deepali Mishra", "Chaklam Silpasuwanchai", "Ashutosh Modi", "Madhumita Sushil", "Sorayouth Chumnanvej"], "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2\n  in supplementary). Scoping review and clinician survey", "summary": "Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration.", "AI": {"tldr": "This study reviews 68 publications on Medical Visual Question Answering (MedVQA) and surveys 50 clinicians to assess its integration into clinical workflows, highlighting practical utility, challenges, and gaps.", "motivation": "To understand the limitations and practical utility of MedVQA in clinical settings, assessing both literature and clinician perspectives.", "method": "A systematic review of 68 publications and a survey of 50 clinicians using the Arksey and O'Malley scoping review framework.", "result": "The review found that 60% of QA pairs are non-diagnostic, and there is a significant disconnect between model capabilities and clinical needs, with only 29.8% of clinicians finding MedVQA highly useful.", "conclusion": "For MedVQA to be effectively integrated into clinical workflows, it needs improvements in multimodal analysis, inclusion of patient context, and alignment of evaluation metrics with clinical needs.", "key_contributions": ["Identification of key concepts and research gaps in MedVQA for radiology workflows", "Insights from clinicians on the relevance and preferences for MedVQA systems", "Highlighting the importance of multi-view imaging and interactive dialogue systems in MedVQA"], "limitations": "Challenges such as limited multimodal analysis, lack of patient context, and misaligned evaluation approaches need to be addressed for effective clinical integration.", "keywords": ["Medical Visual Question Answering", "radiology", "clinical integration"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.08037", "pdf": "https://arxiv.org/pdf/2507.08037.pdf", "abs": "https://arxiv.org/abs/2507.08037", "title": "CRISP: Complex Reasoning with Interpretable Step-based Plans", "authors": ["Matan Vetzler", "Koren Lazar", "Guy Uziel", "Eran Hirsch", "Ateret Anaby-Tavor", "Leshem Choshen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) underscore the need for\nstronger reasoning capabilities to solve complex problems effectively. While\nChain-of-Thought (CoT) reasoning has been a step forward, it remains\ninsufficient for many domains. A promising alternative is explicit high-level\nplan generation, but existing approaches largely assume that LLMs can produce\neffective plans through few-shot prompting alone, without additional training.\nIn this work, we challenge this assumption and introduce CRISP (Complex\nReasoning with Interpretable Step-based Plans), a multi-domain dataset of\nhigh-level plans for mathematical reasoning and code generation. The plans in\nCRISP are automatically generated and rigorously validated--both intrinsically,\nusing an LLM as a judge, and extrinsically, by evaluating their impact on\ndownstream task performance. We demonstrate that fine-tuning a small model on\nCRISP enables it to generate higher-quality plans than much larger models using\nfew-shot prompting, while significantly outperforming Chain-of-Thought\nreasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning\non one domain improves plan generation in the other, highlighting the\ngeneralizability of learned planning capabilities.", "AI": {"tldr": "This paper introduces CRISP, a dataset for generating high-level plans that improve reasoning in LLMs, demonstrating that fine-tuning on this dataset enhances plan generation significantly.", "motivation": "Advancements in LLMs have revealed insufficient reasoning capabilities, prompting the need for stronger methods in solving complex problems.", "method": "The authors introduce the CRISP dataset for high-level planning and evaluate the performance of fine-tuning a small model on this dataset against larger models using few-shot prompting.", "result": "Fine-tuning on the CRISP dataset allows a smaller model to generate higher-quality plans than larger models, outperforming traditional Chain-of-Thought reasoning.", "conclusion": "The study indicates that fine-tuning on one domain enhances the model's plan generation abilities across multiple domains, underscoring the importance of learned planning capabilities.", "key_contributions": ["Introduction of the CRISP dataset for multi-domain high-level plan generation.", "Demonstration of the effectiveness of fine-tuning a smaller model over larger ones in generating plans.", "Evidence of transferability in plan generation capabilities across different domains."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Dataset", "Plan Generation", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08038", "pdf": "https://arxiv.org/pdf/2507.08038.pdf", "abs": "https://arxiv.org/abs/2507.08038", "title": "AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research", "authors": ["Talor Abramovich", "Gal Chechik"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents built on language models (LMs) are showing increasing\npopularity in many fields, including scientific research. AI co-scientists aim\nto support or automate parts of the research process using these agents. A key\ncomponent of empirical AI research is the design of ablation experiments. To\nthis end, we introduce AblationBench, a benchmark suite for evaluating agents\non ablation planning tasks in empirical AI research. It includes two tasks:\nAuthorAblation, which helps authors propose ablation experiments based on a\nmethod section and contains 83 instances, and ReviewerAblation, which helps\nreviewers find missing ablations in a full paper and contains 350 instances.\nFor both tasks, we develop LM-based judges that serve as an automatic\nevaluation framework. Our experiments with frontier LMs show that these tasks\nremain challenging, with the best-performing LM system identifying only 29% of\nthe original ablations on average. Lastly, we analyze the limitations of\ncurrent LMs on these tasks, and find that chain-of-thought prompting\noutperforms the currently existing agent-based approach.", "AI": {"tldr": "Introduction of AblationBench, a benchmark for evaluating language model agents on ablation planning tasks in AI research.", "motivation": "To support or automate parts of the research process using language model-based autonomous agents, specifically in designing ablation experiments.", "method": "Two tasks are introduced: AuthorAblation for proposing ablation experiments and ReviewerAblation for identifying missing ablations in papers, both evaluated using LM-based judges.", "result": "Experiments reveal that the best LMs are only able to identify 29% of the original ablations on average, indicating the tasks' difficulty.", "conclusion": "Chain-of-thought prompting outperforms existing agent-based approaches, highlighting limitations in current LMs for ablation tasks.", "key_contributions": ["Introduction of AblationBench benchmark suite", "Development of two distinct ablation planning tasks", "Implementation of LM-based judges for automatic evaluation"], "limitations": "Current LMs struggle with identifying ablations, demonstrating that improvements are needed in their capabilities.", "keywords": ["autonomous agents", "language models", "ablation experiments", "empirical AI research", "benchmark suite"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08045", "pdf": "https://arxiv.org/pdf/2507.08045.pdf", "abs": "https://arxiv.org/abs/2507.08045", "title": "Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing", "authors": ["Junyi Wen", "Junyuan Liang", "Zicong Hong", "Wuhui Chen", "Zibin Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.", "AI": {"tldr": "Krul is a system for efficient state restoration in multi-turn conversations using LLMs, dynamically optimizing KV cache compression based on attention similarity, achieving significant reductions in time and storage without degrading quality.", "motivation": "To solve the challenge of inefficient state restoration in multi-turn conversations with LLMs due to fixed compression schemes that lead to accuracy loss.", "method": "Krul utilizes a dynamic compression strategy based on attention similarity, with a recomputation-loading pipeline for restoring KV caches, incorporating a preemptive selector, heterogeneous similarity estimator, and a optimized restoration scheduler.", "result": "Empirical evaluations show Krul reduces time-to-first-token by 1.5x-2.68x and KV cache storage by 1.33x-2.35x compared to state-of-the-art methods while maintaining generation quality.", "conclusion": "The dynamic approach of Krul offers a nuanced solution to improve efficiency in multi-turn LLM inference and state restoration.", "key_contributions": ["Dynamic KV cache compression based on attention similarity", "Preemptive compression strategy selector for conversational context", "Bubble-free restoration scheduler to optimize performance"], "limitations": "", "keywords": ["LLM inference", "KV cache", "attention similarity", "multi-turn conversations", "compression strategy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08107", "pdf": "https://arxiv.org/pdf/2507.08107.pdf", "abs": "https://arxiv.org/abs/2507.08107", "title": "GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs", "authors": ["Sebastian Walter", "Hannah Bast"], "categories": ["cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.", "AI": {"tldr": "The paper presents a novel method for generating SPARQL queries from natural language using a large language model without the need for fine-tuning.", "motivation": "To improve the generation of SPARQL queries from natural language inputs without requiring fine-tuning on the model, thereby enhancing accessibility for users in querying RDF knowledge graphs.", "method": "Utilizes a large language model to explore knowledge graphs by dynamically executing SPARQL queries and identifying relevant IRIs and literals, along with evaluation against various benchmarks and models.", "result": "Achieves state-of-the-art results on Wikidata and near-best performance on Freebase in a zero-shot setup, with good results on less common knowledge graphs as well.", "conclusion": "The proposed approach demonstrates the effectiveness of using large language models for SPARQL query generation without fine-tuning, with promising performance across various benchmarks.", "key_contributions": ["State-of-the-art performance on Wikidata in zero-shot settings", "Comparison of different searching strategies in knowledge graphs", "Evaluation across multiple knowledge graph types and sizes"], "limitations": "", "keywords": ["SPARQL", "RDF", "knowledge graphs", "natural language processing", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08109", "pdf": "https://arxiv.org/pdf/2507.08109.pdf", "abs": "https://arxiv.org/abs/2507.08109", "title": "Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing", "authors": ["Reilly Raab", "Mike Parker", "Dan Nally", "Sadie Montgomery", "Anastasia Bernat", "Sai Munikoti", "Sameera Horawalavithana"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of language models (LMs) has the potential to dramatically\naccelerate tasks that may be cast to text-processing; however, real-world\nadoption is hindered by concerns regarding safety, explainability, and bias.\nHow can we responsibly leverage LMs in a transparent, auditable manner --\nminimizing risk and allowing human experts to focus on informed decision-making\nrather than data-processing or prompt engineering? In this work, we propose a\nframework for declaring statically typed, LM-powered subroutines (i.e.,\ncallable, function-like procedures) for use within conventional asynchronous\ncode -- such that sparse feedback from human experts is used to improve the\nperformance of each subroutine online (i.e., during use). In our\nimplementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and\ndata-dependencies) are recorded and exposed to audit on demand. We package this\nframework as a library to support its adoption and continued development. While\nthis framework may be applicable across several real-world decision workflows\n(e.g., in healthcare and legal fields), we evaluate it in the context of public\ncomment processing as mandated by the 1969 National Environmental Protection\nAct (NEPA): Specifically, we use this framework to develop \"CommentNEPA,\" an\napplication that compiles, organizes, and summarizes a corpus of public\ncommentary submitted in response to a project requiring environmental review.\nWe quantitatively evaluate the application by comparing its outputs (when\noperating without human feedback) to historical ``ground-truth'' data as\nlabelled by human annotators during the preparation of official environmental\nimpact statements.", "AI": {"tldr": "The paper presents a framework for safely integrating language models into asynchronous code, enabling real-time improvement through expert feedback while ensuring transparency and auditability.", "motivation": "To responsibly leverage language models in real-world applications while addressing safety, explainability, and bias concerns.", "method": "The authors propose a framework that allows for the declaration of statically typed, LM-powered subroutines within asynchronous code, utilizing sparse human expert feedback to enhance performance during use.", "result": "The framework is packaged as a library and evaluated through an application called CommentNEPA, designed to process and summarize public comments for environmental reviews, demonstrating effectiveness in comparisons to historical data.", "conclusion": "The proposed framework allows for safer and more effective use of language models in decision-making processes across various fields, including healthcare and legal domains.", "key_contributions": ["Development of a framework for statically typed LM-powered subroutines", "Implementation of real-time performance improvement through expert feedback", "Creation of CommentNEPA for public comment processing"], "limitations": "", "keywords": ["language models", "human feedback", "auditable systems", "healthcare", "environmental review"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08143", "pdf": "https://arxiv.org/pdf/2507.08143.pdf", "abs": "https://arxiv.org/abs/2507.08143", "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores", "authors": ["Vivek Chari", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.", "AI": {"tldr": "Compactor is a query-agnostic strategy for compressing KV caches in LLMs, achieving significant memory savings while maintaining performance.", "motivation": "As LLMs are trained for larger context windows, the memory requirements of the KV cache become a bottleneck, complicating deployment in real-world scenarios.", "method": "Compactor uses approximate leverage scores to determine token importance for KV cache compression without needing query information. It includes a context-calibrated compression procedure to infer optimal compression ratios.", "result": "Compactor retains 1/2 the tokens while achieving comparable performance to existing methods and reduces KV memory by 63% on average across various tasks.", "conclusion": "Compactor effectively addresses memory challenges in LLM deployments, demonstrating both efficacy and generalizability.", "key_contributions": ["Introduction of a parameter-free, query-agnostic KV cache compression strategy.", "Demonstration of significant memory savings while retaining performance.", "Development of context-calibrated compression for optimizing compression ratios."], "limitations": "", "keywords": ["Large Language Models", "KV cache", "compression", "approximate leverage scores", "context-calibrated"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08151", "pdf": "https://arxiv.org/pdf/2507.08151.pdf", "abs": "https://arxiv.org/abs/2507.08151", "title": "Distilling Empathy from Large Language Models", "authors": ["Henry J. Xie", "Jinghan Zhang", "Xinhao Zhang", "Kunpeng Liu"], "categories": ["cs.CL"], "comment": "Accepted by SIGDIAL 2025", "summary": "The distillation of knowledge from Large Language Models (LLMs) into Smaller\nLanguage Models (SLMs), preserving the capabilities and performance of LLMs\nwhile reducing model size, has played a key role in the proliferation of LLMs.\nBecause SLMs are considerably smaller than LLMs, they are often utilized in\ndomains where human interaction is frequent but resources are highly\nconstrained, e.g., smart phones. Therefore, it is crucial to ensure that\nempathy, a fundamental aspect of positive human interactions, already instilled\ninto LLMs, is retained by SLMs after distillation. In this paper, we develop a\ncomprehensive approach for effective empathy distillation from LLMs into SLMs.\nOur approach features a two-step fine-tuning process that fully leverages\ndatasets of empathetic dialogue responses distilled from LLMs. We explore\nseveral distillation methods beyond basic direct prompting and propose four\nunique sets of prompts for targeted empathy improvement to significantly\nenhance the empathy distillation process. Our evaluations demonstrate that SLMs\nfine-tuned through the two-step fine-tuning process with distillation datasets\nenhanced by the targeted empathy improvement prompts significantly outperform\nthe base SLM at generating empathetic responses with a win rate of 90%. Our\ntargeted empathy improvement prompts substantially outperform the basic direct\nprompting with a 10% improvement in win rate.", "AI": {"tldr": "This paper presents a method for distilling empathy from Large Language Models (LLMs) into Smaller Language Models (SLMs), enhancing their empathetic response capabilities while reducing model size, which is vital for resource-constrained environments.", "motivation": "The need to retain empathy in Smaller Language Models after distillation from Larger Language Models is paramount for effective human interaction in resource-constrained scenarios like smartphones.", "method": "The paper develops a comprehensive two-step fine-tuning process utilizing datasets of empathetic dialogue responses, exploring various distillation methods and proposing four unique prompt sets for empathy enhancement.", "result": "SLMs fine-tuned with the proposed methods achieved a 90% win rate in generating empathetic responses, significantly outperforming base SLMs and showing a 10% improvement over basic direct prompting.", "conclusion": "The proposed empathy distillation approach successfully enhances the empathetic capabilities of smaller models, which is crucial for applications in environments where interactions are frequent but resources are limited.", "key_contributions": ["Comprehensive approach for empathy distillation from LLMs to SLMs", "Two-step fine-tuning process leveraging empathetic dialogue datasets", "Four novel prompt sets for targeted empathy improvement."], "limitations": "", "keywords": ["Empathy distillation", "LLMs", "SLMs", "Human-Computer Interaction", "Fine-tuning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.08203", "pdf": "https://arxiv.org/pdf/2507.08203.pdf", "abs": "https://arxiv.org/abs/2507.08203", "title": "TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs", "authors": ["Duygu Nur Yaldiz", "Yavuz Faruk Bakman", "Sungmin Kang", "Alperen Öziş", "Hayrettin Eren Yildiz", "Mitash Ashish Shah", "Zhiqi Huang", "Anoop Kumar", "Alfy Samuel", "Daben Liu", "Sai Praneeth Karimireddy", "Salman Avestimehr"], "categories": ["cs.CL"], "comment": null, "summary": "Generative Large Language Models (LLMs)inevitably produce untruthful\nresponses. Accurately predicting the truthfulness of these outputs is critical,\nespecially in high-stakes settings. To accelerate research in this domain and\nmake truthfulness prediction methods more accessible, we introduce TruthTorchLM\nan open-source, comprehensive Python library featuring over 30 truthfulness\nprediction methods, which we refer to as Truth Methods. Unlike existing\ntoolkits such as Guardrails, which focus solely on document-grounded\nverification, or LM-Polygraph, which is limited to uncertainty-based methods,\nTruthTorchLM offers a broad and extensible collection of techniques. These\nmethods span diverse tradeoffs in computational cost, access level (e.g.,\nblack-box vs white-box), grounding document requirements, and supervision type\n(self-supervised or supervised). TruthTorchLM is seamlessly compatible with\nboth HuggingFace and LiteLLM, enabling support for locally hosted and API-based\nmodels. It also provides a unified interface for generation, evaluation,\ncalibration, and long-form truthfulness prediction, along with a flexible\nframework for extending the library with new methods. We conduct an evaluation\nof representative truth methods on three datasets, TriviaQA, GSM8K, and\nFactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM", "AI": {"tldr": "Introduction of TruthTorchLM, an open-source library for truthfulness prediction of LLM outputs.", "motivation": "To address the need for reliable truthfulness prediction in generative LLMs, especially in critical contexts, by providing a comprehensive toolkit for researchers.", "method": "Developed an open-source Python library with over 30 methods for truthfulness prediction, compatible with HuggingFace and LiteLLM, featuring diverse approaches based on computational cost and supervision type.", "result": "TruthTorchLM was evaluated across three datasets, demonstrating its broad applicability compared to existing toolkits.", "conclusion": "TruthTorchLM offers a versatile, comprehensive solution for truthfulness prediction in LLM outputs, encouraging further research and development in this critical area.", "key_contributions": ["Introduction of a comprehensive library with over 30 truth prediction methods.", "Compatibility with popular LLM frameworks like HuggingFace and LiteLLM.", "Evaluation of methods against multiple datasets."], "limitations": "", "keywords": ["Generative LLMs", "Truthfulness Prediction", "Open-source Library"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.08218", "pdf": "https://arxiv.org/pdf/2507.08218.pdf", "abs": "https://arxiv.org/abs/2507.08218", "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning", "authors": ["Atticus Wang", "Joshua Engels", "Oliver Clive-Griffin"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025 Workshop R2-FM", "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.", "AI": {"tldr": "This paper investigates how LoRA fine-tuning in LLMs enables out-of-context reasoning (OOCR) by adding a constant steering vector that improves generalization across tasks.", "motivation": "To understand the mechanism behind out-of-context reasoning in fine-tuned large language models and its implications for LLM deployment.", "method": "The authors analyze the role of LoRA fine-tuning and propose that it introduces a constant steering vector that enhances model performance and generalization in related domains.", "result": "The study finds that many instances of OOCR can be explained by this steering vector, which allows LLMs to generalize unexpectedly well, even in tasks typically reliant on conditional behavior.", "conclusion": "The findings suggest that understanding the learning process of fine-tuning can improve the reliability and safety of LLMs in various applications.", "key_contributions": ["Identification of steering vectors in LoRA fine-tuning as a key factor in OOCR.", "Evidence that OOCR can be induced even without conditional behavior.", "Insights on the implications for the deployment of LLMs in real-world tasks."], "limitations": "", "keywords": ["out-of-distribution generalization", "fine-tuning", "LoRA", "large language models", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.08232", "pdf": "https://arxiv.org/pdf/2507.08232.pdf", "abs": "https://arxiv.org/abs/2507.08232", "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025", "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.", "AI": {"tldr": "The paper examines the effectiveness of Large Language Models (LLMs) as proxies for real students in educational assessments, finding that strong models outperform average students, while suggesting improvements for training and evaluation strategies.", "motivation": "To investigate how accurately LLMs can emulate real students' behavior in educational settings, particularly in Intelligent Tutoring Systems and test question piloting.", "method": "A dataset of 489 items from the National Assessment of Educational Progress (NAEP) was analyzed using an Item Response Theory (IRT) model to compare 11 LLMs against real student populations in grades 4, 8, and 12.", "result": "Strong general-purpose LLMs consistently outperform average students across grades, while weaker models only incidentally align with students. Performance is significantly affected by prompts, varying model efficacy across subjects and grades.", "conclusion": "The study highlights the limitations of current models in serving as proxies for students and calls for enhanced training and evaluation practices, along with specific guidelines for proxy selection.", "key_contributions": ["Analysis of LLMs using Item Response Theory on educational datasets", "Identification of performance gaps between LLMs and average students", "Guidelines for selecting effective LLM proxies in educational contexts"], "limitations": "Performance may vary widely based on specific model-prompt combinations, indicating the need for further research.", "keywords": ["Large Language Models", "Intelligent Tutoring Systems", "Item Response Theory", "Educational Assessment", "Proxy Students"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08241", "pdf": "https://arxiv.org/pdf/2507.08241.pdf", "abs": "https://arxiv.org/abs/2507.08241", "title": "Exploring Gender Differences in Chronic Pain Discussions on Reddit", "authors": ["Ancita Maria Andrade", "Tanvi Banerjee", "Ramakrishna Mundugar"], "categories": ["cs.CL", "cs.LG"], "comment": "This is an extended version of the short paper accepted at ASONAM\n  2025", "summary": "Pain is an inherent part of human existence, manifesting as both physical and\nemotional experiences, and can be categorized as either acute or chronic. Over\nthe years, extensive research has been conducted to understand the causes of\npain and explore potential treatments, with contributions from various\nscientific disciplines. However, earlier studies often overlooked the role of\ngender in pain experiences. In this study, we utilized Natural Language\nProcessing (NLP) to analyze and gain deeper insights into individuals' pain\nexperiences, with a particular focus on gender differences. We successfully\nclassified posts into male and female corpora using the Hidden Attribute\nModel-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by\naggregating posts based on usernames. Our analysis revealed linguistic\ndifferences between genders, with female posts tending to be more emotionally\nfocused. Additionally, the study highlighted that conditions such as migraine\nand sinusitis are more prevalent among females and explored how pain medication\naffects individuals differently based on gender.", "AI": {"tldr": "This study employs NLP to analyze gender differences in pain experiences, utilizing a Hidden Attribute Model-CNN for classification, and revealing significant linguistic and prevalence differences between genders.", "motivation": "Investigate the role of gender in pain experiences which was overlooked in earlier studies.", "method": "Used the Hidden Attribute Model-Convolutional Neural Network (HAM-CNN) to classify posts into male and female corpora, achieving an F1 score of 0.86 by aggregating posts based on usernames.", "result": "The analysis identified linguistic differences in posts, revealing that female posts are more emotionally focused. Conditions like migraine and sinusitis are more prevalent in females, with differing effects of pain medication between genders.", "conclusion": "The findings underline the importance of considering gender differences in pain experiences and treatment approaches.", "key_contributions": ["Application of NLP for analyzing gender differences in pain", "Introduction of HAM-CNN for post classification", "Identification of gender-specific linguistic patterns in pain-related discussions"], "limitations": "", "keywords": ["pain", "gender differences", "Natural Language Processing", "HAM-CNN", "health informatics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08297", "pdf": "https://arxiv.org/pdf/2507.08297.pdf", "abs": "https://arxiv.org/abs/2507.08297", "title": "KAT-V1: Kwai-AutoThink Technical Report", "authors": ["Zizheng Zhan", "Ken Deng", "Huaixi Tang", "Wen Xiang", "Kun Wu", "Weihao Li", "Wenqiang Zhu", "Jingxuan Xu", "Lecheng Huang", "Zongxian Feng", "Shaojie Wang", "Shangpeng Yan", "Jiaheng Liu", "Zhongyuan Peng", "Zuchen Gao", "Haoyang Huang", "Ziqi Zhan", "Yanan Wu", "Yuanxing Zhang", "Jian Yang", "Guang Chen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "categories": ["cs.CL"], "comment": null, "summary": "We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage by up to approximately\n30\\%. Beyond academic evaluation, KAT has been successfully deployed in\nKwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world\ndevelopment workflows with high accuracy, efficiency, and controllable\nreasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) with 40B activation parameters, where the early-stage\nresults already demonstrate promising improvements in performance and\nefficiency, further showing the scalability of the AutoThink paradigm.", "AI": {"tldr": "KAT is an open-source 40B large language model addressing reasoning-intensive tasks by dynamically switching between reasoning and non-reasoning modes.", "motivation": "To tackle the overthinking problem in reasoning-intensive tasks within AI applications.", "method": "The model utilizes a dual-regime dataset with a tagging pipeline, enhanced knowledge distillation via Multi-Token Prediction, and a cold-start initialization strategy. Additionally, it employs a reinforcement learning algorithm called Step-SRPO for guided reasoning-mode selection and improved response accuracy.", "result": "KAT matches or outperforms state-of-the-art models, reduces token usage by 30%, and has been successfully deployed in a coding assistant, enhancing real-world development workflows.", "conclusion": "KAT showcases significant efficiency and controllability in reasoning behaviors, with ongoing development of a larger 200B Mixture-of-Experts model indicating strong scalability.", "key_contributions": ["Dynamic reasoning-mode switching based on task complexity", "Novel tagging pipeline for dual-regime dataset construction", "Introduction of reinforcement learning for structured reasoning guidance"], "limitations": "", "keywords": ["large language model", "reasoning-intensive tasks", "reinforcement learning", "knowledge distillation", "computer science"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08309", "pdf": "https://arxiv.org/pdf/2507.08309.pdf", "abs": "https://arxiv.org/abs/2507.08309", "title": "Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Zhiyuan Chen", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted by ACL 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) have shown strong performance in\ndocument image tasks, especially Optical Character Recognition (OCR). However,\nthey struggle with Document Image Machine Translation (DIMT), which requires\nhandling both cross-modal and cross-lingual challenges. Previous efforts to\nenhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT\ndataset often result in the forgetting of the model's existing monolingual\nabilities, such as OCR. To address these challenges, we introduce a novel\nfine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR\nproficiency, inspired by the concept \"Bilingual Cognitive Advantage\".\nSpecifically, SSR prompts the model to generate OCR text before producing\ntranslation text, which allows the model to leverage its strong monolingual OCR\nability while learning to translate text across languages. Comprehensive\nexperiments demonstrate the proposed SSR learning helps mitigate catastrophic\nforgetting, improving the generalization ability of MLLMs on both OCR and DIMT\ntasks.", "AI": {"tldr": "This paper presents a novel fine-tuning paradigm, Synchronously Self-Reviewing (SSR), for Multimodal Large Language Models (MLLMs) to enhance their performance in Document Image Machine Translation (DIMT) without sacrificing their Optical Character Recognition (OCR) capabilities.", "motivation": "The research addresses the challenges faced by MLLMs in DIMT tasks, particularly the problem of catastrophic forgetting when models are fine-tuned on DIMT datasets.", "method": "The authors introduce Synchronously Self-Reviewing (SSR), where models generate OCR text before producing translation text to leverage their existing OCR capabilities while learning to translate.", "result": "Experiments show that the SSR approach reduces catastrophic forgetting and enhances the generalization ability of MLLMs on both OCR and DIMT tasks.", "conclusion": "The SSR paradigm effectively allows MLLMs to retain their OCR proficiency while improving performance on cross-lingual translation tasks.", "key_contributions": ["Introduction of the Synchronously Self-Reviewing (SSR) fine-tuning paradigm", "Demonstrated improvement in DIMT capabilities without sacrificing OCR abilities", "Reduction of catastrophic forgetting in MLLMs during training"], "limitations": "The paper does not explore the impact of SSR on other multimodal tasks beyond OCR and DIMT.", "keywords": ["Multimodal Large Language Models", "Document Image Machine Translation", "Optical Character Recognition"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2507.08325", "pdf": "https://arxiv.org/pdf/2507.08325.pdf", "abs": "https://arxiv.org/abs/2507.08325", "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation", "authors": ["Yinzhu Quan", "Xinrui Li", "Ying Chen"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.", "AI": {"tldr": "CRMAgent is a multi-agent system using LLMs to generate high-quality marketing message templates and guidance focused on e-commerce merchants.", "motivation": "Merchants in e-commerce struggle with crafting persuasive copy for customer engagement due to a lack of expertise and tools, hindering their CRM efforts.", "method": "The system employs group-based learning from a merchant’s top-performing messages, retrieval-and-adaptation for template sourcing, and a rule-based fallback for zero-shot rewrites when needed.", "result": "CRMAgent outperforms original merchant templates, showing significant improvements in audience match and marketing effectiveness metrics.", "conclusion": "The use of CRMAgent can enhance merchant messaging strategies and improve customer engagement outcomes in e-commerce settings.", "key_contributions": ["Development of CRMAgent leveraging LLMs for CRM applications", "Integration of group-based learning, template retrieval, and rule-based rewriting", "Demonstrated effectiveness through extensive experimental results"], "limitations": "", "keywords": ["e-commerce", "CRM", "large language models", "template generation", "customer engagement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08335", "pdf": "https://arxiv.org/pdf/2507.08335.pdf", "abs": "https://arxiv.org/abs/2507.08335", "title": "MK2 at PBIG Competition: A Prompt Generation Solution", "authors": ["Yuzheng Xu", "Tosho Hirasawa", "Seiya Kawano", "Shota Kato", "Tadashi Kozuno"], "categories": ["cs.CL"], "comment": "9 pages, to appear in the 2nd Workshop on Agent AI for Scenario\n  Planning (AGENTSCEN 2025)", "summary": "The Patent-Based Idea Generation task asks systems to turn real patents into\nproduct ideas viable within three years. We propose MK2, a prompt-centric\npipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful\nfragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea\nper patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all\nwithout extra training data. Across three domains, two evaluator types, and six\ncriteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the\nmaterials-chemistry track lagged, indicating the need for deeper domain\ngrounding; yet, the results show that lightweight prompt engineering has\nalready delivered competitive, commercially relevant ideation from patents.", "AI": {"tldr": "MK2 is a prompt-centric pipeline that generates viable product ideas from real patents within three years, outperforming existing models in ideation quality.", "motivation": "Transform real patents into commercially viable product ideas using prompt engineering techniques.", "method": "MK2 utilizes a prompt-centric approach where Gemini 2.5 drafts and edits prompts, GPT-4.1 generates ideas from these prompts, and an Elo loop with Qwen3-8B evaluates and selects the best prompts without additional training data.", "result": "MK2 achieved top rankings in the automatic leaderboard across three domains, winning 25 out of 36 tests, with the exception of the materials-chemistry track which indicated the need for more domain grounding.", "conclusion": "Lightweight prompt engineering shows promise in delivering competitive and relevant product ideation from patents within a short timeframe.", "key_contributions": ["Introduction of MK2 pipeline for patent-based ideation", "Demonstrated effectiveness in multiple domains", "Competitive performance without extra training data"], "limitations": "Limited performance in the materials-chemistry domain indicating a need for deeper grounding in that area.", "keywords": ["patent ideation", "prompt engineering", "AI-based product development"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2507.08336", "pdf": "https://arxiv.org/pdf/2507.08336.pdf", "abs": "https://arxiv.org/abs/2507.08336", "title": "Distillation versus Contrastive Learning: How to Train Your Rerankers", "authors": ["Zhichao Xu", "Zhiqi Huang", "Shengyao Zhuang", "Ashim Gupta", "Vivek Srikumar"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Training text rerankers is crucial for information retrieval. Two primary\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied in the literature, a clear\ncomparison of their effectiveness for training cross-encoder rerankers under\npractical conditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. Therefore, we recommend using knowledge\ndistillation to train smaller rerankers if a larger, more powerful teacher is\naccessible; in its absence, contrastive learning provides a strong and more\nreliable alternative otherwise.", "AI": {"tldr": "This paper compares contrastive learning and knowledge distillation for training text rerankers, finding knowledge distillation generally yields better performance when using a larger teacher model.", "motivation": "To empirically compare the effectiveness of contrastive learning and knowledge distillation for training cross-encoder rerankers in information retrieval.", "method": "Rerankers of different sizes and architectures were trained using contrastive learning and knowledge distillation on the same dataset, with a strong contrastive learning model as the distillation teacher.", "result": "Knowledge distillation generally results in superior ranking performance for both in-domain and out-of-domain tasks compared to contrastive learning when a larger teacher model is available.", "conclusion": "Knowledge distillation is recommended for training smaller rerankers when a larger teacher is accessible; otherwise, contrastive learning is a reliable alternative.", "key_contributions": ["Empirical comparison of training strategies for text rerankers.", "Demonstration of knowledge distillation's advantages over contrastive learning.", "Guidance on training strategies based on teacher model availability."], "limitations": "Distillation from a teacher of the same capacity does not provide advantages, especially for out-of-domain tasks.", "keywords": ["text rerankers", "contrastive learning", "knowledge distillation", "information retrieval", "machine learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.08339", "pdf": "https://arxiv.org/pdf/2507.08339.pdf", "abs": "https://arxiv.org/abs/2507.08339", "title": "What Factors Affect LLMs and RLLMs in Financial Question Answering?", "authors": ["Peng Wang", "Xuesi Hu", "Jiageng Wu", "Yuntao Zou", "Qiancheng Zhang", "Dagang Li"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering.", "AI": {"tldr": "This study investigates the performance enhancement of LLMs and RLLMs in financial question-answering tasks via various prompting methods and agent frameworks.", "motivation": "To explore how different methods can unlock the full potential of LLMs and RLLMs in the financial domain, particularly for complex problem-solving.", "method": "Five LLMs and three RLLMs are utilized to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks.", "result": "Current prompting methods and agent frameworks boost LLM performance via simulated Long CoT; RLLMs demonstrate inherent Long CoT capabilities that limit conventional methods' effectiveness; advanced multilingual alignment methods mainly enhance LLM multilingual performance with little impact on RLLMs.", "conclusion": "The findings provide valuable insights on leveraging LLMs and RLLMs for financial question answering, with implications for future research and applications.", "key_contributions": ["Investigation of performance enhancement methods for LLMs and RLLMs in finance.", "Empirical comparison of various prompting and alignment strategies.", "Insights into the limitations of traditional methods on RLLMs."], "limitations": "Limited focus on domains outside of financial question answering, and the complexity inherent in assessing LLMs vs RLLMs.", "keywords": ["large language models", "reasoning large language models", "financial question answering", "prompting methods", "multilingual alignment"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08342", "pdf": "https://arxiv.org/pdf/2507.08342.pdf", "abs": "https://arxiv.org/abs/2507.08342", "title": "Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization", "authors": ["Itai Mondshine", "Tzuf Paz-Argaman", "Reut Tsarfaty"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Automatic n-gram based metrics such as ROUGE are widely used for evaluating\ngenerative tasks such as summarization. While these metrics are considered\nindicative (even if imperfect) of human evaluation for English, their\nsuitability for other languages remains unclear. To address this, we\nsystematically assess evaluation metrics for generation both n-gram-based and\nneural based to evaluate their effectiveness across languages and tasks.\nSpecifically, we design a large-scale evaluation suite across eight languages\nfrom four typological families: agglutinative, isolating, low-fusional, and\nhigh-fusional, spanning both low- and high-resource settings, to analyze their\ncorrelation with human judgments. Our findings highlight the sensitivity of\nevaluation metrics to the language type. For example, in fusional languages,\nn-gram-based metrics show lower correlation with human assessments compared to\nisolating and agglutinative languages. We also demonstrate that proper\ntokenization can significantly mitigate this issue for morphologically rich\nfusional languages, sometimes even reversing negative trends. Additionally, we\nshow that neural-based metrics specifically trained for evaluation, such as\nCOMET, consistently outperform other neural metrics and better correlate with\nhuman judgments in low-resource languages. Overall, our analysis highlights the\nlimitations of n-gram metrics for fusional languages and advocates for greater\ninvestment in neural-based metrics trained for evaluation tasks.", "AI": {"tldr": "The paper assesses the effectiveness of n-gram-based and neural-based evaluation metrics for generative tasks across various languages, revealing that these metrics' performance varies significantly by language type.", "motivation": "To evaluate the suitability of automatic n-gram based metrics for different languages in generative tasks and address their correlation with human judgments.", "method": "A large-scale evaluation suite was designed to analyze n-gram-based and neural-based metrics across eight languages from four typological families, focusing on their correlation with human assessments.", "result": "Key findings show that n-gram-based metrics correlate less with human evaluation in fusional languages compared to isolating and agglutinative languages. Proper tokenization improves correlation in fusional languages, and neural-based metrics like COMET outperform other metrics, especially in low-resource settings.", "conclusion": "The study shows significant limitations of n-gram metrics in fusional languages and calls for more attention to neural-based evaluation metrics for better alignment with human judgments.", "key_contributions": ["Systematic assessment of evaluation metrics across various languages and tasks.", "Demonstrated the impact of language type on metric effectiveness.", "Highlighted the importance of proper tokenization for evaluation in morphologically rich languages."], "limitations": "Focused primarily on specific language families and may not encompass all linguistic diversity.", "keywords": ["evaluation metrics", "n-gram", "neural metrics", "human judgments", "language typology"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08350", "pdf": "https://arxiv.org/pdf/2507.08350.pdf", "abs": "https://arxiv.org/abs/2507.08350", "title": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation", "authors": ["Keisuke Ueda", "Wataru Hirota", "Takuto Asakura", "Takahiro Omi", "Kosuke Takahashi", "Kosuke Arima", "Tatsuya Ishigaki"], "categories": ["cs.CL", "cs.MA", "I.2.11; I.2.7"], "comment": "16 pages, 1 figure, appendix. Accepted to SIGDIAL 2025", "summary": "Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator.", "AI": {"tldr": "This study analyzes multi-agent LLM dialogues for generating and critiquing research ideas, demonstrating how interaction design affects creativity outcomes.", "motivation": "To optimize interactions for creative tasks using multi-agent large language models, transcending mere idea generation to include critique and iterative improvement.", "method": "A comparative analysis across various multi-agent configurations, including role definitions, number of agents, and dialogue depth, focusing on the implications for novelty and feasibility in idea generation.", "result": "The study finds that increasing the number of agents, deepening interactions, and expanding persona diversity enhances idea diversity, while critic diversity in an iterative process improves the feasibility of proposals.", "conclusion": "Effective multi-agent LLM systems can significantly enhance scientific ideation by optimizing interaction designs based on specific configuration settings.", "key_contributions": ["Analysis of multi-agent LLM interaction configurations", "Identification of factors influencing idea novelty and feasibility", "Practical guidelines for effective multi-agent ideation systems"], "limitations": "", "keywords": ["multi-agent", "large language models", "scientific ideation", "creativity", "interaction design"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2507.08371", "pdf": "https://arxiv.org/pdf/2507.08371.pdf", "abs": "https://arxiv.org/abs/2507.08371", "title": "The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality", "authors": ["Benjamin Newman", "Abhilasha Ravichander", "Jaehun Jung", "Rui Xin", "Hamish Ivison", "Yegor Kuznetsov", "Pang Wei Koh", "Yejin Choi"], "categories": ["cs.CL"], "comment": "29 pages, 4 figures, 16 tables", "summary": "Language models are prone to hallucination - generating text that is\nfactually incorrect. Finetuning models on high-quality factual information can\npotentially reduce hallucination, but concerns remain; obtaining factual gold\ndata can be expensive and training on correct but unfamiliar data may\npotentially lead to even more downstream hallucination. What data should\npractitioners finetune on to mitigate hallucinations in language models? In\nthis work, we study the relationship between the factuality of finetuning data\nand the prevalence of hallucinations in long-form generation tasks.\nCounterintuitively, we find that finetuning on factual gold data is not as\nhelpful as finetuning on model-generated data that models believe to be\nfactual. Next, we evaluate filtering strategies applied on both factual gold\ndata and model-generated data, and find that finetuning on model-generated data\nthat is filtered by models' own internal judgments often leads to better\noverall factuality compared to other configurations: training on gold data\nfiltered by models' judgments, training on gold data alone, or training on\nmodel-generated data that is supported by gold data. These factuality\nimprovements transfer across three domains we study, suggesting that a models'\nown beliefs can provide a powerful signal for factuality.", "AI": {"tldr": "This paper investigates how the factuality of finetuning data affects hallucination rates in language models, revealing that finetuning on internally judged model-generated data is more effective than using factual gold data.", "motivation": "Understanding how to reduce hallucinations in language models is critical for improving their reliability and applicability, particularly in areas like health informatics.", "method": "The study examines the correlation between the quality of finetuning data and the frequency of hallucinations during long-form text generation, evaluating various finetuning strategies.", "result": "Finetuning on model-generated data that aligns with the model's own judgments results in better factuality compared to using factual gold data or other configurations.", "conclusion": "The findings indicate that a model's internal assessments may serve as a robust guide for selecting finetuning data to mitigate hallucinations in language generation tasks.", "key_contributions": ["Revealed that finetuning on factual gold data is less effective than model-generated data judged by the model.", "Identified that filtering strategies based on internal judgments enhance factuality across domains.", "Demonstrated that improvements in factuality can transfer across multiple generation domains."], "limitations": "The study focuses on long-form generation tasks, which may not generalize to all forms of language model applications.", "keywords": ["language models", "hallucination", "finetuning", "factuality", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08425", "pdf": "https://arxiv.org/pdf/2507.08425.pdf", "abs": "https://arxiv.org/abs/2507.08425", "title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities", "authors": ["Lu Xiang", "Yang Zhao", "Yaping Zhang", "Chengqing Zong"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.", "AI": {"tldr": "This survey paper examines the application of Large Language Models (LLMs) across various disciplines, providing technical insights and exploring their interdisciplinary contributions, challenges, and research directions.", "motivation": "To explore the integration of LLMs in diverse research disciplines and enhance understanding of their methodologies and applicability.", "method": "The paper reviews key technical methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration, which inform the application of LLMs in specific disciplines.", "result": "LLMs are shown to enhance processes in various fields including mathematics, physics, chemistry, biology, and the humanities, while challenges and promising research directions are identified.", "conclusion": "The survey serves as a comprehensive resource for researchers to understand the use of LLMs in interdisciplinary studies and navigate its complexities effectively.", "key_contributions": ["Comprehensive overview of LLM applications in various disciplines.", "Identification of technical methodologies enhancing LLM effectiveness.", "Highlighting challenges and future research directions in interdisciplinary applications of LLMs."], "limitations": "", "keywords": ["Large Language Models", "interdisciplinary studies", "methodologies", "applications", "research directions"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.08427", "pdf": "https://arxiv.org/pdf/2507.08427.pdf", "abs": "https://arxiv.org/abs/2507.08427", "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains", "authors": ["Zilu Dong", "Xiangqing Shen", "Zinong Yang", "Rui Xia"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main)", "summary": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.", "AI": {"tldr": "ChainEdit, a framework combining knowledge graph-derived logical rules with LLM reasoning, enables systematic updates and improves logical consistency in knowledge editing.", "motivation": "To enhance the logical consistency of knowledge editing in large language models which currently faces challenges in maintaining logical coherence.", "method": "By extracting logical patterns from structured knowledge bases and integrating them with LLMs' reasoning capabilities, ChainEdit generates and edits clusters of logically connected knowledge.", "result": "ChainEdit shows over 30% improvement in logical generalization compared to existing methods while maintaining reliability and specificity in knowledge editing.", "conclusion": "ChainEdit establishes state-of-the-art performance in managing ripple effects and ensures internal logical consistency in knowledge editing processes.", "key_contributions": ["Development of ChainEdit framework for systematic knowledge updates", "Improved logical generalization metrics over previous methods", "Addressing evaluation biases with knowledge-aware protocols"], "limitations": "", "keywords": ["knowledge editing", "large language models", "logical reasoning", "knowledge graphs", "ripple effect"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08440", "pdf": "https://arxiv.org/pdf/2507.08440.pdf", "abs": "https://arxiv.org/abs/2507.08440", "title": "Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences", "authors": ["Selina Heller", "Mohamed Ibrahim", "David Antony Selby", "Sebastian Vollmer"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.", "AI": {"tldr": "This paper presents a novel LLM-based multi-agent system designed to simulate decision conferences by detecting agreement among agents.", "motivation": "To explore the potential of LLMs in simulating collaborative decision-making processes and enhancing the efficiency and quality of deliberations.", "method": "The study evaluates six different LLMs on stance detection and stance polarity detection tasks within a multi-agent system to simulate decision conferences.", "result": "LLMs reliably detect agreement in dynamic debates, improving the efficiency of group discussions and resulting in decision-making outcomes comparable to real-world scenarios.", "conclusion": "LLM-based multi-agent systems can effectively simulate group decision-making and support expert elicitation workshops in diverse fields.", "key_contributions": ["Development of a novel LLM-based multi-agent system for decision conferences", "Evaluation of LLMs on stance detection and polarity detection", "Demonstration of enhanced efficiency in group debates through agreement detection"], "limitations": "", "keywords": ["Large Language Models", "Multi-Agent Systems", "Decision-Making"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08459", "pdf": "https://arxiv.org/pdf/2507.08459.pdf", "abs": "https://arxiv.org/abs/2507.08459", "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework", "authors": ["Zishan Xu", "Shuyi Xie", "Qingsong Lv", "Shupei Xiao", "Linlin Song", "Sui Wenjuan", "Fan Lin"], "categories": ["cs.CL"], "comment": null, "summary": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.", "AI": {"tldr": "This paper presents a Misattribution Framework and a dataset, AttriData, to analyze and attribute errors in Large Language Models, along with a fine-tuned model called MisAttributionLLM for generating scores and feedback.", "motivation": "To analyze the performance of LLMs and diagnose failures in their answers through automated error categorization and attribution.", "method": "Developing a Misattribution Framework with 6 primary and 15 secondary categories, creating AttriData dataset for error attribution, and fine-tuning MisAttributionLLM model on AttriData.", "result": "The proposed method shows effectiveness and robustness in error attribution, illustrated through extensive experiments.", "conclusion": "The Misattribution Framework and MisAttributionLLM provide new capabilities for performance analysis of LLMs by simultaneously generating scores, misattributions, and feedback.", "key_contributions": ["Misattribution Framework with detailed categorization for error analysis.", "AttriData, a dataset tailored for error attribution with scores and feedback.", "MisAttributionLLM, a general-purpose judge model for generating scores and misattribution feedback."], "limitations": "", "keywords": ["Large Language Models", "error attribution", "Misattribution Framework", "AttriData", "MisAttributionLLM"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08468", "pdf": "https://arxiv.org/pdf/2507.08468.pdf", "abs": "https://arxiv.org/abs/2507.08468", "title": "Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study", "authors": ["Marina Luketina", "Andrea Benkel", "Christoph G. Schuetz"], "categories": ["cs.CL"], "comment": "26 pages, 5 figures, 6 tables", "summary": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.", "AI": {"tldr": "This paper evaluates the use of large language models (LLMs) in legal decision-making for VAT law, exploring methods like fine-tuning and retrieval-augmented generation (RAG).", "motivation": "The study aims to assess the potential of LLMs in assisting tax professionals by automating workflows and aiding in legal analyses, particularly in the context of Austrian and EU VAT law.", "method": "The research employs experimental evaluations on textbook and real-world cases, applying fine-tuning and RAG to improve LLM performance for legal reasoning.", "result": "The experiments show that LLMs can support tax consultants by automating routine tasks and providing initial analyses, though they are not yet suitable for full automation due to the complexities of the legal domain.", "conclusion": "While LLMs exhibit potential in VAT decision-making support, limitations persist in handling specific client knowledge and documentation, indicating a need for future developments in the integration of structured information.", "key_contributions": ["Evaluation of LLMs in the context of VAT law decision-making", "Comparison of fine-tuning and RAG methods for legal reasoning", "Identification of limitations in current LLM applications for legal contexts"], "limitations": "LLMs struggle with implicit client knowledge and context-specific documentation, limiting their applicability in sensitive legal tasks.", "keywords": ["large language models", "legal decision-making", "VAT law", "fine-tuning", "retrieval-augmented generation"], "importance_score": 7, "read_time_minutes": 26}}
{"id": "2507.08477", "pdf": "https://arxiv.org/pdf/2507.08477.pdf", "abs": "https://arxiv.org/abs/2507.08477", "title": "ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition", "authors": ["Qingliang Meng", "Hao Wu", "Wei Liang", "Wei Xu", "Qing Zhao"], "categories": ["cs.CL"], "comment": "Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper", "summary": "The deep integration of large language models and automatic speech\nrecognition systems has become a promising research direction with high\npractical value. To address the overfitting issue commonly observed in Low-Rank\nAdaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work\nproposes an innovative training paradigm Iterative LoRA Training (ILT) in\ncombination with an Iterative Pseudo Labeling strategy, effectively enhancing\nthe theoretical upper bound of model performance. Based on Whisper-large-v3 and\nQwen2-Audio, we conduct systematic experiments using a three-stage training\nprocess: Focus Training, Feed Back Training, and Fix Training. Experimental\nresults demonstrate the effectiveness of the proposed method. Furthermore, the\nMegaAIS research team applied this technique in the Interspeech 2025\nMultilingual Conversational Speech Language Modeling Challenge (MLC-SLM),\nachieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2\n(Speech Separation and Recognition Task), showcasing the practical feasibility\nand strong application potential of our approach.", "AI": {"tldr": "This paper introduces Iterative LoRA Training (ILT) to improve the performance of large language models combined with speech recognition, aimed at reducing overfitting during the fine-tuning process.", "motivation": "To address the overfitting issue in Low-Rank Adaptation (LoRA) during the supervised fine-tuning of large language models and automatic speech recognition systems.", "method": "The paper proposes an innovative training paradigm called Iterative LoRA Training (ILT) along with an Iterative Pseudo Labeling strategy, utilizing a three-stage training process: Focus Training, Feed Back Training, and Fix Training.", "result": "Experiments show that the ILT approach effectively enhances model performance, achieving 4th place in Track 1 and 1st place in Track 2 of the Interspeech 2025 MLC-SLM workshop.", "conclusion": "The proposed method demonstrates practical feasibility and strong application potential in speech language modeling tasks.", "key_contributions": ["Introduction of Iterative LoRA Training (ILT).", "Development of Iterative Pseudo Labeling strategy.", "Systematic experiments showing improved performance in speech recognition tasks."], "limitations": "", "keywords": ["large language models", "automatic speech recognition", "iterative training"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.08487", "pdf": "https://arxiv.org/pdf/2507.08487.pdf", "abs": "https://arxiv.org/abs/2507.08487", "title": "Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach", "authors": ["Bruno Alexandre Rosa", "Hilário Oliveira", "Luiz Rodrigues", "Eduardo Araujo Oliveira", "Rafael Ferreira Mello"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 4 tables", "summary": "Essays are considered a valuable mechanism for evaluating learning outcomes\nin writing. Textual cohesion is an essential characteristic of a text, as it\nfacilitates the establishment of meaning between its parts. Automatically\nscoring cohesion in essays presents a challenge in the field of educational\nartificial intelligence. The machine learning algorithms used to evaluate texts\ngenerally do not consider the individual characteristics of the instances that\ncomprise the analysed corpus. In this meaning, item response theory can be\nadapted to the context of machine learning, characterising the ability,\ndifficulty and discrimination of the models used. This work proposes and\nanalyses the performance of a cohesion score prediction approach based on item\nresponse theory to adjust the scores generated by machine learning models. In\nthis study, the corpus selected for the experiments consisted of the extended\nEssay-BR, which includes 6,563 essays in the style of the National High School\nExam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235\nessays written by 5th to 9th grade students from public schools. We extracted\n325 linguistic features and treated the problem as a machine learning\nregression task. The experimental results indicate that the proposed approach\noutperforms conventional machine learning models and ensemble methods in\nseveral evaluation metrics. This research explores a potential approach for\nimproving the automatic evaluation of cohesion in educational essays.", "AI": {"tldr": "This paper presents a method for automatically scoring textual cohesion in educational essays using item response theory combined with machine learning.", "motivation": "To address the challenge of automatically evaluating textual cohesion in essays, which is crucial for understanding meaning within texts.", "method": "The authors propose a cohesion score prediction approach based on item response theory, which is integrated with machine learning regression tasks. The experiments utilized a corpus of essays to extract 325 linguistic features.", "result": "The proposed method outperformed conventional machine learning models and ensemble methods across various evaluation metrics, indicating its effectiveness in scoring cohesion.", "conclusion": "This research suggests a promising approach for enhancing the automatic evaluation of cohesion in educational essays using machine learning and item response theory.", "key_contributions": ["Introduction of item response theory to machine learning for scoring textual cohesion", "Analysis of a large corpus of essays for validation", "Demonstration of improved performance over traditional methods"], "limitations": "", "keywords": ["textual cohesion", "machine learning", "item response theory", "educational essays", "automatic evaluation"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2507.08491", "pdf": "https://arxiv.org/pdf/2507.08491.pdf", "abs": "https://arxiv.org/abs/2507.08491", "title": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench", "authors": ["David Schlangen", "Sherzod Hakimov", "Jonathan Jordan", "Philipp Sadler"], "categories": ["cs.CL"], "comment": "All code required to run the benchmark, as well as extensive\n  documentation, is available at https://github.com/clembench/clembench", "summary": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests.", "AI": {"tldr": "This paper introduces clembench, a new benchmarking tool that integrates dialogue game based evaluation for large language models.", "motivation": "The need for improved evaluation paradigms for large language models that combine strength of reference-based and preference-based evaluations.", "method": "Introduction and description of the clembench tool, enabling dialogue game based evaluation to benchmark models with user-friendly implementation.", "result": "clembench provides a software platform that facilitates both the evaluation of LLMs through controlled dialogue games and the extension of benchmark tests.", "conclusion": "Clembench is positioned to enhance the evaluation of LLMs by making dialogue-based benchmarks more accessible and customizable.", "key_contributions": ["Introduction of a new benchmarking tool for dialogue game based evaluation of LLMs.", "Optimization for ease of use and extendability in benchmarking tests.", "Provision of comprehensive documentation and code for implementation."], "limitations": "", "keywords": ["large language models", "evaluation paradigms", "benchmarking", "dialogue games", "clembench"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08496", "pdf": "https://arxiv.org/pdf/2507.08496.pdf", "abs": "https://arxiv.org/abs/2507.08496", "title": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning", "authors": ["Shibo Sun", "Xue Li", "Donglin Di", "Mingjie Wei", "Lanshun Nie", "Wei-Nan Zhang", "Dechen Zhan", "Yang Song", "Lei Fan"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa.", "AI": {"tldr": "LLaPa is a vision-language model framework for multimodal procedural planning, addressing challenges in integrating multimodal inputs and counterfactual reasoning.", "motivation": "To explore the integration of multimodal inputs and counterfactual reasoning in procedural planning for embodied AI systems.", "method": "LLaPa generates executable action sequences from textual descriptions and visual images, enhanced with a Task-Environment Reranker for task-sensitive feature alignment and a Counterfactual Activities Retriever for improved reasoning in counterfactual scenarios.", "result": "LLaPa outperforms advanced models on ActPlan-1K and ALFRED benchmarks with higher-quality plans and improved LCS and correctness.", "conclusion": "LLaPa sets a new benchmark in multimodal procedural planning and demonstrates the importance of integrating counterfactual reasoning with multimodal inputs.", "key_contributions": ["Introduction of a multimodal procedural planning framework (LLaPa)", "Development of Task-Environment Reranker for task-sensitive feature spaces", "Creation of Counterfactual Activities Retriever to enhance reasoning"], "limitations": "", "keywords": ["multimodal", "procedural planning", "vision-language models", "counterfactual reasoning", "embodied AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08498", "pdf": "https://arxiv.org/pdf/2507.08498.pdf", "abs": "https://arxiv.org/abs/2507.08498", "title": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop", "authors": ["Mengze Hong", "Chen Jason Zhang", "Di Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.", "AI": {"tldr": "This paper investigates enhancing Latent Dirichlet Allocation (LDA) topic models with Large Language Models (LLMs) during initialization and post-correction phases, finding mixed results in practical improvements.", "motivation": "To evaluate the effectiveness of integrating Large Language Models with Latent Dirichlet Allocation for topic modeling and to challenge the assumption of LLM superiority in text mining.", "method": "The study employs LLM-guided topic clustering for initializing the Gibbs sampling algorithm in LDA and assesses the impact of LLM-enabled post-correction on topic coherence.", "result": "LLM-guided initialization improves early iterations of LDA but adversely affects overall performance, whereas LLM-enabled post-correction enhances coherence by 5.86%.", "conclusion": "The findings reveal that while LLMs can be beneficial in specific phases of topic modeling, they do not universally enhance LDA performance, prompting a reevaluation of their role in text mining.", "key_contributions": ["Integration of LLMs into LDA phases", "Empirical analysis of LLM impacts on topic modeling", "Insight into the limitations of LLMs in text mining"], "limitations": "The initialization from LLMs did not improve convergence and performed worse than baselines.", "keywords": ["Latent Dirichlet Allocation", "Large Language Models", "Topic Modeling", "Text Mining", "Coherence Evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.08499", "pdf": "https://arxiv.org/pdf/2507.08499.pdf", "abs": "https://arxiv.org/abs/2507.08499", "title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts", "authors": ["Ziyi Huang", "Xia Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection (Track A), which focuses on multi-label emotion\ndetection in short texts. We propose a feature-centric framework that\ndynamically adapts document representations and learning algorithms to optimize\nlanguage-specific performance. Our study evaluates three key components:\ndocument representation, dimensionality reduction, and model training in 28\nlanguages, highlighting five for detailed analysis. The results show that\nTF-IDF remains highly effective for low-resource languages, while contextual\nembeddings like FastText and transformer-based document representations, such\nas those produced by Sentence-BERT, exhibit language-specific strengths.\nPrincipal Component Analysis (PCA) reduces training time without compromising\nperformance, particularly benefiting FastText and neural models such as\nMulti-Layer Perceptrons (MLP). Computational efficiency analysis underscores\nthe trade-off between model complexity and processing cost. Our framework\nprovides a scalable solution for multilingual emotion detection, addressing the\nchallenges of linguistic diversity and resource constraints.", "AI": {"tldr": "This paper presents a feature-centric framework for multi-label emotion detection in 28 languages, optimizing document representations and learning algorithms based on language-specific performance.", "motivation": "The paper aims to improve text-based emotion detection using a system that effectively manages linguistic diversity and resource constraints.", "method": "A feature-centric framework was used to dynamically adapt document representations and learning algorithms for emotion detection across multiple languages.", "result": "The study finds that TF-IDF is effective for low-resource languages, while contextual embeddings and transformer-based models show varied strengths; PCA improves training efficiency.", "conclusion": "The proposed framework is scalable for multilingual emotion detection and successfully addresses challenges related to linguistic diversity.", "key_contributions": ["Dynamic adaptation of document representations", "Evaluation across 28 languages", "Efficiency improvements using PCA"], "limitations": "Focused primarily on feature extraction without exploring algorithm design intricacies.", "keywords": ["emotion detection", "multilingual", "TF-IDF", "contextual embeddings", "PCA"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.08538", "pdf": "https://arxiv.org/pdf/2507.08538.pdf", "abs": "https://arxiv.org/abs/2507.08538", "title": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks", "authors": ["David Pomerenke", "Jonas Nothnagel", "Simon Ostermann"], "categories": ["cs.CL"], "comment": null, "summary": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language.", "AI": {"tldr": "The paper introduces the AI Language Proficiency Monitor, a multilingual benchmark for assessing LLM performance across 200 languages, focusing on low-resource languages.", "motivation": "To ensure equitable access to LLM benefits by evaluating their capabilities in various languages, especially low-resource ones.", "method": "Developed a benchmark that includes diverse tasks such as translation and reasoning, utilizing datasets like FLORES+, MMLU, GSM8K, and others, with an open-source leaderboard.", "result": "The benchmark provides a comprehensive assessment of LLM performance, highlighting strengths and gaps, and includes features like a global proficiency map and trends over time.", "conclusion": "The system promotes transparency and inclusivity in multilingual AI and is available on Hugging Face.", "key_contributions": ["Introduction of a comprehensive multilingual benchmark for LLMs", "Focus on performance assessment in low-resource languages", "Creation of an open-source, auto-updating leaderboard and analysis dashboard."], "limitations": "", "keywords": ["multilingual", "language models", "benchmark", "low-resource languages", "AI proficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08606", "pdf": "https://arxiv.org/pdf/2507.08606.pdf", "abs": "https://arxiv.org/abs/2507.08606", "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.", "AI": {"tldr": "DocPolarBERT is a layout-aware BERT model that improves document understanding by using a polar coordinate system for text block positions instead of traditional 2D positional embeddings.", "motivation": "The paper aims to enhance document understanding by developing a model that effectively utilizes the layout of documents without the limitations of absolute positional embeddings.", "method": "Introduces a modified self-attention mechanism using relative polar coordinates to account for text block positions, enhancing the efficiency of the model.", "result": "DocPolarBERT achieves state-of-the-art results despite being trained on a dataset more than six times smaller than the IIT-CDIP corpus.", "conclusion": "The study indicates that innovative attention mechanisms can yield significant performance improvements in document understanding even with less training data.", "key_contributions": ["Development of a layout-aware BERT model", "Introduction of relative polar coordinate system in self-attention", "Achieving state-of-the-art results with less pre-training data"], "limitations": "", "keywords": ["BERT", "document understanding", "self-attention", "polar coordinates", "layout-aware"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.08621", "pdf": "https://arxiv.org/pdf/2507.08621.pdf", "abs": "https://arxiv.org/abs/2507.08621", "title": "A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1", "authors": ["Marcin Pietroń", "Rafał Olszowski", "Jakub Gomułka", "Filip Gampel", "Andrzej Tomski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.", "AI": {"tldr": "This paper studies the performance of large language models (LLMs) in argument mining using various datasets, highlighting strengths and weaknesses of models like ChatGPT-4o and DeepSeek-R1 in argument classification tasks.", "motivation": "The paper addresses the lack of research on the performance of LLMs in publicly available argument classification databases, despite advancements in argument mining facilitated by LLMs.", "method": "The study involves testing a selection of LLMs, including versions of GPT, Llama, and DeepSeek, on datasets like Args.me and UKP, and utilizes reasoning-enhanced variants with the Chain-of-Thoughts algorithm.", "result": "Results show that ChatGPT-4o is the best performer in argument classification benchmarks, while Deepseek-R1 excels among reasoning models. Common errors across models are also discussed.", "conclusion": "The work provides a comprehensive analysis of argument classification datasets, reveals limitations of existing prompt algorithms, and suggests improvements for future research.", "key_contributions": ["First broader analysis of LLMs in argument mining using various datasets", "Identification of strengths and weaknesses in argument classification performance", "Analysis of common errors in model outputs and prompt algorithm limitations."], "limitations": "The models still make errors, and the analysis shows shortcomings in existing prompt algorithms for argument analysis.", "keywords": ["argument mining", "large language models", "classification benchmarks", "Chain-of-Thoughts", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.08660", "pdf": "https://arxiv.org/pdf/2507.08660.pdf", "abs": "https://arxiv.org/abs/2507.08660", "title": "The Impact of Automatic Speech Transcription on Speaker Attribution", "authors": ["Cristina Aggazzotti", "Matthew Wiesner", "Elizabeth Allyn Smith", "Nicholas Andrews"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Speaker attribution from speech transcripts is the task of identifying a\nspeaker from the transcript of their speech based on patterns in their language\nuse. This task is especially useful when the audio is unavailable (e.g.\ndeleted) or unreliable (e.g. anonymized speech). Prior work in this area has\nprimarily focused on the feasibility of attributing speakers using transcripts\nproduced by human annotators. However, in real-world settings, one often only\nhas more errorful transcripts produced by automatic speech recognition (ASR)\nsystems. In this paper, we conduct what is, to our knowledge, the first\ncomprehensive study of the impact of automatic transcription on speaker\nattribution performance. In particular, we study the extent to which speaker\nattribution performance degrades in the face of transcription errors, as well\nas how properties of the ASR system impact attribution. We find that\nattribution is surprisingly resilient to word-level transcription errors and\nthat the objective of recovering the true transcript is minimally correlated\nwith attribution performance. Overall, our findings suggest that speaker\nattribution on more errorful transcripts produced by ASR is as good, if not\nbetter, than attribution based on human-transcribed data, possibly because ASR\ntranscription errors can capture speaker-specific features revealing of speaker\nidentity.", "AI": {"tldr": "This paper investigates how transcription errors from automatic speech recognition (ASR) systems affect speaker attribution performance, revealing that ASR errors may not significantly hinder attribution capabilities.", "motivation": "The study aims to address the challenge of speaker attribution using transcripts from automatic speech recognition systems, which are often error-prone compared to human-generated transcripts.", "method": "The authors conduct a comprehensive analysis of speaker attribution performance using various ASR-generated transcripts, measuring how different levels of transcription error affect the ability to accurately identify speakers.", "result": "The research finds that speaker attribution performance remains surprisingly resilient to word-level transcription errors, with ASR-produced transcripts performing at least as well as human-transcribed data in certain contexts.", "conclusion": "The findings indicate that errors in ASR transcripts can still facilitate effective speaker attribution, suggesting that reliance on error-prone transcripts may not be as detrimental as previously assumed.", "key_contributions": ["First comprehensive study on ASR impact on speaker attribution", "Demonstrated resilience of attribution performance to transcription errors", "Highlighted potential advantages of ASR transcripts for speaker identity recovery"], "limitations": "The study primarily focuses on the impact of word-level errors and may not address other types of errors or contextual factors affecting attribution performance.", "keywords": ["speaker attribution", "automatic speech recognition", "transcription errors", "human-computer interaction", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.08665", "pdf": "https://arxiv.org/pdf/2507.08665.pdf", "abs": "https://arxiv.org/abs/2507.08665", "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment", "authors": ["Jiyao Zhang", "Chengli Zhong", "Hui Xu", "Qige Li", "Yi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2\n  tables", "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.", "AI": {"tldr": "Proposes KELPS, a neuro-symbolic framework for translating informal mathematics into machine-verifiable theorems, enhancing multilingual processing capabilities.", "motivation": "To address the bottlenecks in the conversion of informal mathematics to formal theorems caused by the scarcity of quality multilingual parallel corpora.", "method": "KELPS, an iterative framework, translates natural language into Knowledge Equations, then converts these into formal languages (Lean, Coq, Isabelle) using defined rules.", "result": "Achieved 88.9% syntactic accuracy on MiniF2F, surpassing existing models like Deepseek-V3 and Herald, with a newly created parallel corpus of over 60,000 problems.", "conclusion": "The KELPS framework effectively enhances the process of translating informal mathematics into formal languages, paving the way for better multilingual corpora.", "key_contributions": ["Introduction of Knowledge Equations (KEs) for representing informal data.", "Development of a framework that maintains syntactic structure and semantic meaning during translation.", "Creation of a large parallel corpus of math problems."], "limitations": "", "keywords": ["neuro-symbolic AI", "machine-verifiable theorems", "multilingual parallel corpora"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08704", "pdf": "https://arxiv.org/pdf/2507.08704.pdf", "abs": "https://arxiv.org/abs/2507.08704", "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation", "authors": ["Songlin Zhai", "Guilin Qi", "Yuan Meng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.", "AI": {"tldr": "This paper introduces a framework for integrating knowledge graphs into large language models at test time without parameter updates, using a dual-pathway attention mechanism for real-time knowledge fusion.", "motivation": "To enhance large language models with structured knowledge while avoiding risks of catastrophic forgetting and improving adaptability to real-time knowledge updates.", "method": "The proposed framework employs a knowledge graph-guided attention (KGA) module that consists of outward and inward aggregation pathways for dynamic knowledge fusion during test-time.", "result": "Extensive experiments demonstrate that the KGA module achieves comparable knowledge fusion performance across five benchmarks.", "conclusion": "The KGA module provides a novel approach to integrate knowledge dynamically, supporting real-time updates without modifying model parameters.", "key_contributions": ["Introduction of test-time KG-augmented framework for LLMs", "Development of a knowledge graph-guided attention module with dual pathways", "Demonstration of effective real-time knowledge fusion without parameter updates"], "limitations": "", "keywords": ["knowledge graphs", "large language models", "attention mechanism", "real-time updates", "knowledge fusion"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08719", "pdf": "https://arxiv.org/pdf/2507.08719.pdf", "abs": "https://arxiv.org/abs/2507.08719", "title": "Multilingual Multimodal Software Developer for Code Generation", "authors": ["Linzheng Chai", "Jian Yang", "Shukai Liu", "Wei Zhang", "Liran Wang", "Ke Jin", "Tao Sun", "Congnan Liu", "Chenchen Zhang", "Hualei Zhu", "Jiaheng Liu", "Xianjie Wu", "Ge Zhang", "Tianyu Liu", "Zhoujun Li"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Preprint", "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.", "AI": {"tldr": "Introducing MM-Coder, a Multilingual Multimodal software developer that integrates visual design inputs with textual instructions for improved code generation.", "motivation": "To address the limitations of text-only large language models in code generation by incorporating visual aids like diagrams and flowcharts.", "method": "Developed MM-Coder, which utilizes a multimodal instruction-tuning dataset (MMc-Instruct) and introduces a new benchmark (MMEval) for evaluating multimodal code generation.", "result": "MM-Coder demonstrates the potential for improved architectural alignment and code generation accuracy by synthesizing textual and visual information, overcoming challenges seen in text-only models.", "conclusion": "The integration of visual workflows into code generation could significantly impact industrial programming practices by enabling better comprehension of complex specifications.", "key_contributions": ["Introduction of MM-Coder as a Multilingual Multimodal software developer.", "Development of MMc-Instruct dataset for multimodal instruction tuning.", "Launch of MMEval benchmark for evaluating multimodal code generation."], "limitations": "Significant challenges still exist in precise visual information capture, instruction following, and advanced programming knowledge for models.", "keywords": ["Large Language Models", "Multimodal", "Code Generation", "UML Diagrams", "Visual Workflows"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.08799", "pdf": "https://arxiv.org/pdf/2507.08799.pdf", "abs": "https://arxiv.org/abs/2507.08799", "title": "KV Cache Steering for Inducing Reasoning in Small Language Models", "authors": ["Max Belitsky", "Dawid J. Kopiczko", "Michael Dorkenwald", "M. Jehanzeb Mirza", "Cees G. M. Snoek", "Yuki M. Asano"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.", "AI": {"tldr": "Cache steering offers a one-shot method for enhancing reasoning in language models by modifying the key-value cache, improving model behavior without fine-tuning.", "motivation": "To improve the reasoning capabilities of language models through a lightweight intervention method.", "method": "Cache steering is implemented as a one-shot intervention directly applied to the key-value cache of language models, utilizing GPT-4o-generated reasoning traces to create steering vectors.", "result": "Experimental evaluations show that cache steering enhances reasoning structure qualitatively and improves quantitative task performance across diverse benchmarks.", "conclusion": "Cache steering is more efficient and stable compared to prior activation steering techniques, making it a practical solution for controlled generation.", "key_contributions": ["Introduction of cache steering for language models", "Demonstrated improvements in reasoning capabilities", "Substantial advantages over continuous activation steering methods"], "limitations": "", "keywords": ["cache steering", "language models", "implicit steering", "chain-of-thought reasoning", "controlled generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2111.14003", "pdf": "https://arxiv.org/pdf/2111.14003.pdf", "abs": "https://arxiv.org/abs/2111.14003", "title": "Answer Generation for Questions With Multiple Information Sources in E-Commerce", "authors": ["Anand A. Rajasekar", "Nikesh Garera"], "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.3"], "comment": "7 pages, 10 tables, 1 figure", "summary": "Automatic question answering is an important yet challenging task in\nE-commerce given the millions of questions posted by users about the product\nthat they are interested in purchasing. Hence, there is a great demand for\nautomatic answer generation systems that provide quick responses using related\ninformation about the product. There are three sources of knowledge available\nfor answering a user posted query, they are reviews, duplicate or similar\nquestions, and specifications. Effectively utilizing these information sources\nwill greatly aid us in answering complex questions. However, there are two main\nchallenges present in exploiting these sources: (i) The presence of irrelevant\ninformation and (ii) the presence of ambiguity of sentiment present in reviews\nand similar questions. Through this work we propose a novel pipeline (MSQAP)\nthat utilizes the rich information present in the aforementioned sources by\nseparately performing relevancy and ambiguity prediction before generating a\nresponse.\n  Experimental results show that our relevancy prediction model (BERT-QA)\noutperforms all other variants and has an improvement of 12.36% in F1 score\ncompared to the BERT-base baseline. Our generation model (T5-QA) outperforms\nthe baselines in all content preservation metrics such as BLEU, ROUGE and has\nan average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the\nhighest performing baseline (HSSC-q). Human evaluation of our pipeline shows us\nthat our method has an overall improvement in accuracy of 30.7% over the\ngeneration model (T5-QA), resulting in our full pipeline-based approach (MSQAP)\nproviding more accurate answers. To the best of our knowledge, this is the\nfirst work in the e-commerce domain that automatically generates natural\nlanguage answers combining the information present in diverse sources such as\nspecifications, similar questions, and reviews data.", "AI": {"tldr": "Proposes a novel pipeline (MSQAP) for automatic question answering in E-commerce by using multiple information sources and addressing challenges of relevancy and ambiguity.", "motivation": "There's a high demand for efficient automatic answer generation systems in E-commerce due to the large volume of user queries about products, necessitating effective use of available information sources.", "method": "A novel pipeline (MSQAP) is proposed, which performs relevancy and ambiguity prediction separately before generating a response, effectively using reviews, similar questions, and specifications.", "result": "The relevancy prediction model (BERT-QA) shows a 12.36% improvement in F1 score over the BERT-base baseline. The generation model (T5-QA) significantly surpasses baselines in content preservation metrics, with a 35.02% improvement in ROUGE and 198.75% in BLEU. The overall pipeline shows a 30.7% improvement in accuracy over T5-QA.", "conclusion": "The proposed method, combining diverse sources of information, represents a significant advancement in automatically generating accurate natural language answers in the E-commerce domain.", "key_contributions": ["Introduction of the MSQAP pipeline for E-commerce QA systems", "Demonstration of efficacy through substantial metric improvements", "First approach to combine specifications, similar questions, and reviews for answer generation"], "limitations": "", "keywords": ["E-commerce", "automatic question answering", "natural language processing", "BERT", "T5"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2301.12463", "pdf": "https://arxiv.org/pdf/2301.12463.pdf", "abs": "https://arxiv.org/abs/2301.12463", "title": "Comparing Spoken Languages using Paninian System of Sounds and Finite State Machines", "authors": ["Shreekanth M Prabhu", "Abhisek Midya"], "categories": ["cs.CL", "cs.FL"], "comment": "63 Pages, 20 Figures, 27 Tables", "summary": "The study of spoken languages comprises phonology, morphology, and grammar.\nThe languages can be classified as root languages, inflectional languages, and\nstem languages. In addition, languages continually change over time and space\nby picking isoglosses, as speakers move from region to/through region. All\nthese factors lead to the formation of vocabulary, which has\ncommonality/similarity across languages as well as distinct and subtle\ndifferences among them. Comparison of vocabularies across languages and\ndetailed analysis has led to the hypothesis of language families. In\nparticular, in the view of Western linguists, Vedic Sanskrit is a daughter\nlanguage, part of the Indo-Iranian branch of the Indo-European Language family,\nand Dravidian Languages belong to an entirely different family. These and such\nconclusions are reexamined in this paper. Based on our study and analysis, we\npropose an Ecosystem Model for Linguistic Development with Sanskrit at the\ncore, in place of the widely accepted family tree model. To that end, we\nleverage the Paninian system of sounds to construct a phonetic map. Then we\nrepresent words across languages as state transitions on the phonetic map and\nconstruct corresponding Morphological Finite Automata (MFA) that accept groups\nof words. Regardless of whether the contribution of this paper is significant\nor minor, it is an important step in challenging policy-driven research that\nhas plagued this field.", "AI": {"tldr": "This paper challenges the traditional family tree model of language development by proposing an Ecosystem Model centered around Sanskrit, utilizing phonetic mapping and Morphological Finite Automata for linguistic analysis.", "motivation": "To reexamine the classification of languages and propose an alternative model for linguistic development that better accounts for the complexities of language evolution.", "method": "The paper constructs a phonetic map using the Paninian system of sounds and represents words across languages as state transitions on this map, creating Morphological Finite Automata for groups of words.", "result": "The proposed Ecosystem Model provides a nuanced understanding of language relationships, challenging the conventional family tree perspective and emphasizing continuous language evolution and interaction.", "conclusion": "This work suggests a paradigm shift in linguistic study that departs from traditional models, paving the way for more dynamic and holistic approaches to understanding language development.", "key_contributions": ["Introduction of the Ecosystem Model for Linguistic Development centered on Sanskrit", "Utilization of phonetic mapping and Morphological Finite Automata in language analysis", "Challenging established linguistic theories regarding language families"], "limitations": "", "keywords": ["Linguistic Development", "Phonetics", "Morphological Finite Automata"], "importance_score": 0, "read_time_minutes": 63}}
{"id": "2401.02984", "pdf": "https://arxiv.org/pdf/2401.02984.pdf", "abs": "https://arxiv.org/abs/2401.02984", "title": "Large Language Models in Mental Health Care: a Scoping Review", "authors": ["Yining Hua", "Fenglin Liu", "Kailai Yang", "Zehan Li", "Hongbin Na", "Yi-han Sheu", "Peilin Zhou", "Lauren V. Moran", "Sophia Ananiadou", "David A. Clifton", "Andrew Beam", "John Torous"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations.", "AI": {"tldr": "This review analyzes the effectiveness of Large Language Models (LLMs) in mental health care, identifying challenges and future potential applications.", "motivation": "To evaluate the utilization of LLMs in mental health care and explore their effectiveness and challenges.", "method": "A systematic review was conducted on studies employing LLMs for mental health care published between October 2019 and December 2023, utilizing multiple databases.", "result": "34 relevant studies were identified that highlighted the applications of LLMs in diagnostics, therapy, and enhancing patient engagement, along with challenges regarding data and ethical considerations.", "conclusion": "LLMs have significant potential in mental health care enhancement, necessitating development of robust datasets and ethical guidelines to mitigate limitations.", "key_contributions": ["Comprehensive overview of LLM applications in mental health care.", "Identification of key challenges in the implementation of LLMs in clinical settings.", "Recommendations for future research and interdisciplinary collaborations."], "limitations": "Noted limitations include gaps in clinical applicability, ethical considerations, and the need for robust datasets and evaluation methods.", "keywords": ["Large Language Models", "mental health care", "systematic review", "diagnostics", "therapy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.14192", "pdf": "https://arxiv.org/pdf/2404.14192.pdf", "abs": "https://arxiv.org/abs/2404.14192", "title": "Swap distance minimization beyond entropy minimization in word order variation", "authors": ["Víctor Franco-Sánchez", "Arnau Martí-Llobet", "Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": "Reorganization with technical appendices; minor corrections; in press\n  in the Journal of Quantitative Linguistics", "summary": "Consider a linguistic structure formed by $n$ elements, for instance,\nsubject, direct object and verb ($n=3$) or subject, direct object, indirect\nobject and verb ($n=4$). We investigate whether the frequency of the $n!$\npossible orders is constrained by two principles. First, entropy minimization,\na principle that has been suggested to shape natural communication systems at\ndistinct levels of organization. Second, swap distance minimization, namely a\npreference for word orders that require fewer swaps of adjacent elements to be\nproduced from a source order. We present average swap distance, a novel score\nfor research on swap distance minimization. We find strong evidence of pressure\nfor entropy minimization and swap distance minimization with respect to a die\nrolling experiment in distinct linguistic structures with $n=3$ or $n=4$.\nEvidence with respect to a Polya urn process is strong for $n=4$ but weaker for\n$n=3$. We still find evidence consistent with the action of swap distance\nminimization when word order frequencies are shuffled, indicating that swap\ndistance minimization effects are beyond pressure to reduce word order entropy.", "AI": {"tldr": "The paper investigates how linguistic structures' word order frequencies are influenced by entropy and swap distance minimization principles.", "motivation": "To understand the constraints on word order in linguistic structures and how they are shaped by principles of entropy and swap distance minimization.", "method": "The study employs a die rolling experiment and a Polya urn process to examine the frequency of word orders in linguistic structures with different quantities of elements (n=3 and n=4).", "result": "The analysis reveals strong evidence supporting both entropy minimization and swap distance minimization, especially for n=4, indicating these principles shape word order frequencies in linguistic structures.", "conclusion": "The findings suggest that swap distance minimization effects exist independently of pressures to reduce word order entropy, demonstrating complex influences on linguistic structures.", "key_contributions": ["Introduced the average swap distance score for linguistic research.", "Provided empirical evidence for both entropy minimization and swap distance minimization in linguistic structures.", "Demonstrated the effects of word order frequency shuffling on minimizing swap distances."], "limitations": "Evidence for swap distance minimization is weaker for n=3 compared to n=4.", "keywords": ["entropy minimization", "swap distance minimization", "linguistic structure", "word order frequency", "Polya urn process"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2405.19715", "pdf": "https://arxiv.org/pdf/2405.19715.pdf", "abs": "https://arxiv.org/abs/2405.19715", "title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths", "authors": ["Kaixuan Huang", "Xudong Guo", "Mengdi Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to COLM 2025", "summary": "Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (7.2% improvement over the baseline speculative\ndecoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x\nspeedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.\nThe code of this paper is available at\nhttps://github.com/Kaffaljidhmah2/SpecDec_pp.", "AI": {"tldr": "This paper presents SpecDec++, an enhanced speculative decoding method for large language models that adaptively determines candidate length to improve inference speed and accuracy.", "motivation": "To improve the inference latency of large language models by optimally determining the candidate length for speculative decoding, which traditionally relies on simple heuristics.", "method": "Formulated the candidate length as a Markov Decision Process and proposed a threshold policy for deciding when to stop speculation. Developed an enhanced model called SpecDec++ that integrates an acceptance prediction mechanism to predict token acceptance probabilities.", "result": "SpecDec++ achieves speedups of 2.04x on the Alpaca dataset and 2.26x on the GSM8K dataset while improving accuracy compared to baseline methods.", "conclusion": "The introduction of SpecDec++ demonstrates a significant improvement in decoding speed and reliability for large language models, with practical implementations available.", "key_contributions": ["Formulated candidate length as a Markov Decision Process.", "Developed an adaptive method to determine candidate length on the fly.", "Demonstrated performance improvements across multiple datasets."], "limitations": "", "keywords": ["Speculative Decoding", "Large Language Models", "Markov Decision Process", "Adaptation", "Acceptance Prediction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.03897", "pdf": "https://arxiv.org/pdf/2406.03897.pdf", "abs": "https://arxiv.org/abs/2406.03897", "title": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew", "authors": ["Tzuf Paz-Argaman", "Itai Mondshine", "Asaf Achi Mordechai", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.", "AI": {"tldr": "HeSum introduces a benchmark for evaluating abstractive summarization with LLMs in Modern Hebrew, addressing resource gaps in lower-resourced languages.", "motivation": "To tackle the performance gap of large language models in generative tasks for lower-resourced languages, particularly Hebrew.", "method": "The paper introduces HeSum, a benchmarking dataset of 10,000 article-summary pairs from Hebrew news sites, analyzing linguistic features and summarization challenges.", "result": "HeSum is shown to present unique difficulties for state-of-the-art LLMs, revealing the challenges in handling the morphological richness of Hebrew.", "conclusion": "HeSum provides a valuable resource for testing generative models in Hebrew, contributing to understanding challenges in low-resource language contexts.", "key_contributions": ["Introduction of HeSum benchmark for Hebrew summarization", "Linguistic analysis highlighting summary abstractness", "Establishment of distinct challenges for LLMs in lower-resourced languages"], "limitations": "", "keywords": ["large language models", "Hebrew", "abstractive summarization", "HeSum", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2407.10657", "pdf": "https://arxiv.org/pdf/2407.10657.pdf", "abs": "https://arxiv.org/abs/2407.10657", "title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "authors": ["Usneek Singh", "José Cambronero", "Sumit Gulwani", "Aditya Kanade", "Anirudh Khatry", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of NAACL", "summary": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.", "AI": {"tldr": "This paper investigates the impact of validating synthetic natural language (NL) utterances generated for fine-tuning large language models (LLMs) used in spreadsheet formula writing. It shows that validation improves model performance despite pruning challenging examples.", "motivation": "To improve the performance of LLMs in writing spreadsheet formulas by generating synthetic training data, and to assess the efficacy of validating these synthetic examples.", "method": "The paper uses empirical evaluation to compare model performance with and without validation of synthetic NL annotations across four models (two open and two closed weight).", "result": "Validation of synthetic training examples leads to improved performance on the models; despite pruning more challenging examples, it enhances the complexity of problems the models can solve post fine-tuning.", "conclusion": "Validating synthetic data is crucial for leveraging LLMs effectively, offering significant improvements in performance and problem-solving capabilities.", "key_contributions": ["Empirical assessment of synthetic NL validation on model performance", "Demonstration of the balance between pruning challenging examples and increasing problem complexity", "Novel approaches to fine-tuning LLMs with validated synthetic data"], "limitations": "", "keywords": ["large language models", "synthetic data", "validation", "fine-tuning", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.08893", "pdf": "https://arxiv.org/pdf/2503.08893.pdf", "abs": "https://arxiv.org/abs/2503.08893", "title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees", "authors": ["Zhiyuan Zeng", "Yizhong Wang", "Hannaneh Hajishirzi", "Pang Wei Koh"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025", "summary": "An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor language model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also introduce a weakness profiling method EvalTree. EvalTree\nconstructs a capability tree where each node represents a capability described\nin natural language and is linked to a subset of benchmark instances that\nspecifically evaluate this capability; it then extracts nodes where the LM\nperforms poorly to generate a weakness profile. On the MATH and WildChat\nbenchmarks, we show that EvalTree outperforms baseline weakness profiling\nmethods by identifying weaknesses more precisely and comprehensively. Weakness\nprofiling further enables weakness-guided data collection, and training data\ncollection guided by EvalTree-identified weaknesses improves LM performance\nmore than other data collection strategies. We also show how EvalTree exposes\nflaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate\nfuture work, we provide an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.", "AI": {"tldr": "This paper presents EvalTree, a method for generating weakness profiles of language models that identifies specific weaknesses based on their performance across benchmark instances, guiding subsequent data collection and training improvements.", "motivation": "To improve model evaluation by identifying weaknesses in language models and providing actionable insights for refining these models.", "method": "The authors introduce EvalTree, which constructs a capability tree with nodes representing different capabilities of a language model and links to benchmark instances testing these capabilities, allowing for a focused analysis of weaknesses.", "result": "EvalTree demonstrates superior performance in identifying weaknesses over baseline methods on the MATH and WildChat benchmarks, leading to more effective training data collection.", "conclusion": "Weakness profiling through EvalTree not only enhances the identification of weaknesses but also informs better strategies for training data collection, ultimately improving the performance of language models.", "key_contributions": ["Introduction of a weakness profiling method called EvalTree.", "Creation of a capability tree to link weaknesses to benchmark instances.", "Demonstration of improved performance in LM training data collection by addressing weaknesses identified by EvalTree."], "limitations": "", "keywords": ["language models", "weakness profiling", "capability tree"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.11924", "pdf": "https://arxiv.org/pdf/2503.11924.pdf", "abs": "https://arxiv.org/abs/2503.11924", "title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives", "authors": ["Kun Su", "Krishna Sayana", "Hubert Pham", "James Pine", "Yuri Vasilevski", "Raghavendra Vasudeva", "Marialena Kyriakidi", "Liam Hebert", "Ambarish Jash", "Anushya Subbiah", "Sukhdeep Sodhi"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.", "AI": {"tldr": "The paper introduces the REGEN dataset for benchmarking the conversational capabilities of recommender LLMs and presents the LUMEN modeling framework for generating recommendations and narratives based on user history.", "motivation": "To address the limitations of existing datasets for conversational recommendation that mainly focus on sequential item prediction.", "method": "The authors extend the Amazon Product Reviews dataset by adding user critiques and rich narratives. They introduce the LUMEN framework for training models on generating critiques, recommendations, and narratives.", "result": "The dataset's quality was evaluated, and traditional and LLM-based recommender models were trained, showing that critiques enhance recommendation quality and LLMs can generate effective contextual narratives.", "conclusion": "Incorporating critiques into recommendations significantly improves the learning of language understanding and integrates this with recommendation signals, achieving performance comparable to state-of-the-art systems.", "key_contributions": ["Introduction of the REGEN dataset for conversational recommendation", "Development of the LUMEN framework for model training", "Demonstration that critiques improve recommendation quality and narrative generation"], "limitations": "", "keywords": ["Large Language Models", "Conversational Recommendation", "Recommendation Systems"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2504.00927", "pdf": "https://arxiv.org/pdf/2504.00927.pdf", "abs": "https://arxiv.org/abs/2504.00927", "title": "Multi-Token Attention", "authors": ["Olga Golovneva", "Tianlu Wang", "Jason Weston", "Sainbayar Sukhbaatar"], "categories": ["cs.CL"], "comment": null, "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.", "AI": {"tldr": "Multi-Token Attention (MTA) improves LLM attention mechanisms by using multiple query and key vectors, enhancing context relevance determination.", "motivation": "The paper addresses the limitations of single token attention in LLMs, which restricts the use of information when identifying relevant context.", "method": "The authors propose a new attention method called Multi-Token Attention, which uses convolution operations over multiple query and key vectors to condition attention weights simultaneously.", "result": "MTA shows improved performance over standard Transformer models, especially in tasks involving long contexts and information retrieval.", "conclusion": "The new attention mechanism offers richer information processing capabilities, outperforming traditional models in various benchmarks.", "key_contributions": ["Introduction of Multi-Token Attention method", "Improved performance on language modeling tasks", "Enhanced ability to process long-context information retrieval."], "limitations": "", "keywords": ["Multi-Token Attention", "LLM", "attention mechanism", "language modeling", "information retrieval"], "importance_score": 7, "read_time_minutes": 10}}
