{"id": "2508.05637", "pdf": "https://arxiv.org/pdf/2508.05637.pdf", "abs": "https://arxiv.org/abs/2508.05637", "title": "Automated Visualization Makeovers with LLMs", "authors": ["Siddharth Gangwar", "David A. Selby", "Sebastian J. Vollmer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Making a good graphic that accurately and efficiently conveys the desired\nmessage to the audience is both an art and a science, typically not taught in\nthe data science curriculum. Visualisation makeovers are exercises where the\ncommunity exchange feedback to improve charts and data visualizations. Can\nmulti-modal large language models (LLMs) emulate this task? Given a plot in the\nform of an image file, or the code used to generate it, an LLM, primed with a\nlist of visualization best practices, is employed to semi-automatically\ngenerate constructive criticism to produce a better plot. Our system is centred\naround prompt engineering of a pre-trained model, relying on a combination of\nuserspecified guidelines and any latent knowledge of data visualization\npractices that might lie within an LLMs training corpus. Unlike other works,\nthe focus is not on generating valid visualization scripts from raw data or\nprompts, but on educating the user how to improve their existing data\nvisualizations according to an interpretation of best practices. A quantitative\nevaluation is performed to measure the sensitivity of the LLM agent to various\nplotting issues across different chart types. We make the tool available as a\nsimple self-hosted applet with an accessible Web interface."}
{"id": "2508.05646", "pdf": "https://arxiv.org/pdf/2508.05646.pdf", "abs": "https://arxiv.org/abs/2508.05646", "title": "A Humanoid Social Robot as a Teaching Assistant in the Classroom", "authors": ["Thomas Sievers"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Although innovation and the support of new technologies are much needed to\nease the burden on the education system, social robots in schools to help\nteachers with educational tasks are rare. Child-Robot Interaction (CRI) could\nsupport teachers and add an embodied social component to modern multi-modal and\nmulti-sensory learning environments already in use. The social robot Pepper,\nconnected to the Large Language Model (LLM) ChatGPT, was used in a high school\nclassroom to teach new learning content to groups of students. I tested the\ntechnical possibilities with the robot on site and asked the students about\ntheir acceptance and perceived usefulness of teaching with the help of a social\nrobot. All participants felt that the robot's presentation of the learning\nmaterial was appropriate or at least partially appropriate and that its use\nmade sense."}
{"id": "2508.05653", "pdf": "https://arxiv.org/pdf/2508.05653.pdf", "abs": "https://arxiv.org/abs/2508.05653", "title": "Modeling Interactive Narrative Systems: A Formal Approach", "authors": ["Jules Clerc", "Domitile Lourdeaux", "Mohamed Sallak", "Johann Barbier", "Marc Ravaine"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Interactive Narrative Systems (INS) have revolutionized digital experiences\nby empowering users to actively shape their stories, diverging from traditional\npassive storytelling. However, the field faces challenges due to fragmented\nresearch efforts and diverse system representations. This paper introduces a\nformal representation framework for INS, inspired by diverse approaches from\nthe state of the art. By providing a consistent vocabulary and modeling\nstructure, the framework facilitates the analysis, the description and\ncomparison of INS properties. Experimental validations on the \"Little Red\nRiding Hood\" scenario highlight the usefulness of the proposed formalism and\nits impact on improving the evaluation of INS. This work aims to foster\ncollaboration and coherence within the INS research community by proposing a\nmethodology for formally representing these systems."}
{"id": "2508.05913", "pdf": "https://arxiv.org/pdf/2508.05913.pdf", "abs": "https://arxiv.org/abs/2508.05913", "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "authors": ["Stefan Pasch", "Min Chul Cha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types."}
{"id": "2508.05722", "pdf": "https://arxiv.org/pdf/2508.05722.pdf", "abs": "https://arxiv.org/abs/2508.05722", "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "authors": ["Rania Al-Sabbagh"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible."}
{"id": "2508.05933", "pdf": "https://arxiv.org/pdf/2508.05933.pdf", "abs": "https://arxiv.org/abs/2508.05933", "title": "REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The affective brain-computer interface is a crucial technology for affective\ninteraction and emotional intelligence, emerging as a significant area of\nresearch in the human-computer interaction. Compared to single-type features,\nmulti-type EEG features provide a multi-level representation for analyzing\nmulti-dimensional emotions. However, the high dimensionality of multi-type EEG\nfeatures, combined with the relatively small number of high-quality EEG\nsamples, poses challenges such as classifier overfitting and suboptimal\nreal-time performance in multi-dimensional emotion recognition. Moreover,\npractical applications of affective brain-computer interface frequently\nencounters partial absence of multi-dimensional emotional labels due to the\nopen nature of the acquisition environment, and ambiguity and variability in\nindividual emotion perception. To address these challenges, this study proposes\na novel EEG feature selection method for missing multi-dimensional emotion\nrecognition. The method leverages adaptive orthogonal non-negative matrix\nfactorization to reconstruct the multi-dimensional emotional label space\nthrough second-order and higher-order correlations, which could reduce the\nnegative impact of missing values and outliers on label reconstruction.\nSimultaneously, it employs least squares regression with graph-based manifold\nlearning regularization and global feature redundancy minimization\nregularization to enable EEG feature subset selection despite missing\ninformation, ultimately achieving robust EEG-based multi-dimensional emotion\nrecognition. Simulation experiments on three widely used multi-dimensional\nemotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method\noutperforms thirteen advanced feature selection methods in terms of robustness\nfor EEG emotional feature selection."}
{"id": "2508.05775", "pdf": "https://arxiv.org/pdf/2508.05775.pdf", "abs": "https://arxiv.org/abs/2508.05775", "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies."}
{"id": "2508.05934", "pdf": "https://arxiv.org/pdf/2508.05934.pdf", "abs": "https://arxiv.org/abs/2508.05934", "title": "ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection", "authors": ["Xueyuan Xu", "Tianze Yu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, multi-modal physiological signals based emotion recognition has\ngarnered increasing attention in the field of brain-computer interfaces.\nNevertheness, the associated multi-modal physiological features are often\nhigh-dimensional and inevitably include irrelevant, redundant, and noisy\nrepresentation, which can easily lead to overfitting, poor performance, and\nhigh computational complexity in emotion classifiers. Feature selection has\nbeen widely applied to address these challenges. However, previous studies\ngenerally assumed that multi-modal physiological data are complete, whereas in\nreality, the data are often incomplete due to the openness of the acquisition\nand operational environment. For example, a part of samples are available in\nseveral modalities but not in others. To address this issue, we propose a novel\nmethod for incomplete multi-modal physiological signal feature selection called\nadaptive shared latent structure learning (ASLSL). Based on the property that\nsimilar features share similar emotional labels, ASLSL employs adaptive shared\nlatent structure learning to explore a common latent space shared for\nincomplete multi-modal physiological signals and multi-dimensional emotional\nlabels, thereby mitigating the impact of missing information and mining\nconsensus information. Two most popular multi-modal physiological emotion\ndatasets (DEAP and DREAMER) with multi-dimensional emotional labels were\nutilized to compare the performance between compare ASLSL and seventeen feature\nselection methods. Comprehensive experimental results on these datasets\ndemonstrate the effectiveness of ASLSL."}
{"id": "2508.05782", "pdf": "https://arxiv.org/pdf/2508.05782.pdf", "abs": "https://arxiv.org/abs/2508.05782", "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub."}
{"id": "2508.05940", "pdf": "https://arxiv.org/pdf/2508.05940.pdf", "abs": "https://arxiv.org/abs/2508.05940", "title": "It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design", "authors": ["Kathy Cheng", "Alison Olechowski", "Shurui Zhou"], "categories": ["cs.HC"], "comment": "To be published in the Proceedings of the ACM on Human-Computer\n  Interaction, Volume 9, Issue CSCW2", "summary": "In today's landscape, hardware development teams face increasing demands for\nbetter quality products, greater innovation, and shorter manufacturing lead\ntimes. Despite the need for more efficient and effective processes, hardware\ndesigners continue to struggle with a lack of awareness of design changes and\nother collaborators' actions, a persistent issue in decades of CSCW research.\nOne significant and unaddressed challenge is understanding and managing\ndependencies between 3D CAD (computer-aided design) models, especially when\nproducts can contain thousands of interconnected components. In this two-phase\nformative study, we explore designers' pain points of CAD dependency management\nthrough a thematic analysis of 100 online forum discussions and semi-structured\ninterviews with 10 designers. We identify nine key challenges related to the\ntraceability, navigation, and consistency of CAD dependencies, that harm the\neffective coordination of hardware development teams. To address these\nchallenges, we propose design goals and necessary features to enhance hardware\ndesigners' awareness and management of dependencies, ultimately with the goal\nof improving collaborative workflows."}
{"id": "2508.05803", "pdf": "https://arxiv.org/pdf/2508.05803.pdf", "abs": "https://arxiv.org/abs/2508.05803", "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "authors": ["Abishek Thamma", "Micha Heilbron"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior."}
{"id": "2508.06000", "pdf": "https://arxiv.org/pdf/2508.06000.pdf", "abs": "https://arxiv.org/abs/2508.06000", "title": "Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning", "authors": ["Wei Xiang", "Ziyue Lei", "Haoyuan Che", "Fangyuan Ye", "Xueting Wu", "Lingyun Sun"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Operational skill learning, inherently physical and reliant on hands-on\npractice and kinesthetic feedback, has yet to be effectively replicated in\nlarge language model (LLM)-supported training. Current LLM training assistants\nprimarily generate customized textual feedback, neglecting the crucial\nkinesthetic modality. This gap derives from the textual and uncertain nature of\nLLMs, compounded by concerns on user acceptance of LLM driven body control. To\nbridge this gap and realize the potential of collaborative human-LLM action,\nthis work explores human experience of LLM driven kinesthetic assistance.\nSpecifically, we introduced an \"Align-Analyze-Adjust\" strategy and developed\nFlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)\nfor flight skill acquisition, a representative operational skill domain.\nFlightAxis learns flight skills from manuals and guides forearm movements\nduring simulated flight tasks. Our results demonstrate high user acceptance of\nLLM-mediated body control and significantly reduced task completion times.\nCrucially, trainees reported that this kinesthetic assistance enhanced their\nawareness of operation flaws and fostered increased engagement in the training\nprocess, rather than relieving perceived load. This work demonstrated the\npotential of kinesthetic LLM training in operational skill acquisition."}
{"id": "2508.05830", "pdf": "https://arxiv.org/pdf/2508.05830.pdf", "abs": "https://arxiv.org/abs/2508.05830", "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "categories": ["cs.CL", "cs.CY"], "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment."}
{"id": "2508.06056", "pdf": "https://arxiv.org/pdf/2508.06056.pdf", "abs": "https://arxiv.org/abs/2508.06056", "title": "RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation", "authors": ["Sizhe Cheng", "Jiaping Li", "Huanchen Wang", "Yuxin Ma"], "categories": ["cs.HC"], "comment": "19 pages, 9 figures, Accepted by UIST 2025", "summary": "Retrieval-Augmented Generation (RAG) systems have emerged as a promising\nsolution to enhance large language models (LLMs) by integrating external\nknowledge retrieval with generative capabilities. While significant\nadvancements have been made in improving retrieval accuracy and response\nquality, a critical challenge remains that the internal knowledge integration\nand retrieval-generation interactions in RAG workflows are largely opaque. This\npaper introduces RAGTrace, an interactive evaluation system designed to analyze\nretrieval and generation dynamics in RAG-based workflows. Informed by a\ncomprehensive literature review and expert interviews, the system supports a\nmulti-level analysis approach, ranging from high-level performance evaluation\nto fine-grained examination of retrieval relevance, generation fidelity, and\ncross-component interactions. Unlike conventional evaluation practices that\nfocus on isolated retrieval or generation quality assessments, RAGTrace enables\nan integrated exploration of retrieval-generation relationships, allowing users\nto trace knowledge sources and identify potential failure cases. The system's\nworkflow allows users to build, evaluate, and iterate on retrieval processes\ntailored to their specific domains of interest. The effectiveness of the system\nis demonstrated through case studies and expert evaluations on real-world RAG\napplications."}
{"id": "2508.05843", "pdf": "https://arxiv.org/pdf/2508.05843.pdf", "abs": "https://arxiv.org/abs/2508.05843", "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "categories": ["cs.CL"], "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes."}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065.pdf", "abs": "https://arxiv.org/abs/2508.06065", "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools."}
{"id": "2508.05880", "pdf": "https://arxiv.org/pdf/2508.05880.pdf", "abs": "https://arxiv.org/abs/2508.05880", "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available."}
{"id": "2508.06117", "pdf": "https://arxiv.org/pdf/2508.06117.pdf", "abs": "https://arxiv.org/abs/2508.06117", "title": "A Multimodal Framework for Understanding Collaborative Design Processes", "authors": ["Maurice Koch", "Nelusa Pathmanathan", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "Accepted to IEEE VIS 2025", "summary": "An essential task in analyzing collaborative design processes, such as those\nthat are part of workshops in design studies, is identifying design outcomes\nand understanding how the collaboration between participants formed the results\nand led to decision-making. However, findings are typically restricted to a\nconsolidated textual form based on notes from interviews or observations. A\nchallenge arises from integrating different sources of observations, leading to\nlarge amounts and heterogeneity of collected data. To address this challenge we\npropose a practical, modular, and adaptable framework of workshop setup,\nmultimodal data acquisition, AI-based artifact extraction, and visual analysis.\nOur interactive visual analysis system, reCAPit, allows the flexible\ncombination of different modalities, including video, audio, notes, or gaze, to\nanalyze and communicate important workshop findings. A multimodal streamgraph\ndisplays activity and attention in the working area, temporally aligned topic\ncards summarize participants' discussions, and drill-down techniques allow\ninspecting raw data of included sources. As part of our research, we conducted\nsix workshops across different themes ranging from social science research on\nurban planning to a design study on band-practice visualization. The latter two\nare examined in detail and described as case studies. Further, we present\nconsiderations for planning workshops and challenges that we derive from our\nown experience and the interviews we conducted with workshop experts. Our\nresearch extends existing methodology of collaborative design workshops by\npromoting data-rich acquisition of multimodal observations, combined AI-based\nextraction and interactive visual analysis, and transparent dissemination of\nresults."}
{"id": "2508.05909", "pdf": "https://arxiv.org/pdf/2508.05909.pdf", "abs": "https://arxiv.org/abs/2508.05909", "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation."}
{"id": "2508.06300", "pdf": "https://arxiv.org/pdf/2508.06300.pdf", "abs": "https://arxiv.org/abs/2508.06300", "title": "Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models", "authors": ["Weihan Zhang", "Jun Tao"], "categories": ["cs.HC"], "comment": "Accepted by IEEE VIS 2025", "summary": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration."}
{"id": "2508.05938", "pdf": "https://arxiv.org/pdf/2508.05938.pdf", "abs": "https://arxiv.org/abs/2508.05938", "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks."}
{"id": "2508.06349", "pdf": "https://arxiv.org/pdf/2508.06349.pdf", "abs": "https://arxiv.org/abs/2508.06349", "title": "Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance", "authors": ["Serena Tardelli", "Lorenzo Alvisi", "Lorenzo Cima", "Stefano Cresci", "Maurizio Tesconi"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Emoji reactions are a frequently used feature of messaging platforms. Prior\nwork mainly interpreted emojis as indicators of emotional resonance or user\nsentiment. However, emoji reactions may instead reflect broader social\ndynamics. Here, we investigate the communicative function of emoji reactions on\nTelegram by analyzing the relationship between the emotional and rhetorical\ncontent of messages and the emoji reactions they receive. We collect and\nanalyze over 650k Telegram messages that received at least one emoji reaction.\nWe annotate each message with sentiment, emotion, persuasion strategy, and\nspeech act labels, and infer the sentiment and emotion of emoji reactions using\nboth lexicons and large languages. We find a systematic mismatch between\nmessage sentiment and reaction sentiment, with positive reactions dominating\neven when the message is neutral or negative. We show that this pattern remains\nconsistent across rhetorical strategies and emotional tones, suggesting that\nemoji reactions may signal a degree of social approval rather than reflecting\nemotional resonance. Finally, we shed light on the communicative strategies\nthat predict greater emoji engagement. These findings have methodological\nimplications for sentiment analysis, as interpreting emoji reactions as direct\nproxies for emotional response may be misleading."}
{"id": "2508.05987", "pdf": "https://arxiv.org/pdf/2508.05987.pdf", "abs": "https://arxiv.org/abs/2508.05987", "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271."}
{"id": "2508.06354", "pdf": "https://arxiv.org/pdf/2508.06354.pdf", "abs": "https://arxiv.org/abs/2508.06354", "title": "Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems", "authors": ["Clara Rigaud"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "This article explores the possibilities of reusing obsolete smartphones and\ntablets to build new interactive systems. Taking the case of a musical\ninstrument, I present my research into the design of a controller made from\nvarious of these obsolete smartphones. From the diagnostic stage to the\ncreation of a new autonomous electronic object, I document the process, the\nbarriers and the levers encountered. Based on these explorations and\ndiscussions with two professional musicians, I provide several insights into\nthe software and hardware aspects, with a view to continuing this work, towards\nthe creation of an open-source toolkit enabling anyone to build new interactive\nsystems with old devices. I discuss the implication of how a high-level\nweb-based approach could allow designers to enter the black box and foster\npermacomputing using smartphones."}
{"id": "2508.06016", "pdf": "https://arxiv.org/pdf/2508.06016.pdf", "abs": "https://arxiv.org/abs/2508.06016", "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models."}
{"id": "2508.06484", "pdf": "https://arxiv.org/pdf/2508.06484.pdf", "abs": "https://arxiv.org/abs/2508.06484", "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data", "authors": ["Yuvraj Virk", "Dongyu Liu"], "categories": ["cs.HC"], "comment": "Accepted by VL/HCC 2025", "summary": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions."}
{"id": "2508.06026", "pdf": "https://arxiv.org/pdf/2508.06026.pdf", "abs": "https://arxiv.org/abs/2508.06026", "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data."}
{"id": "2508.05799", "pdf": "https://arxiv.org/pdf/2508.05799.pdf", "abs": "https://arxiv.org/abs/2508.05799", "title": "AI-Guided Exploration of Large-Scale Codebases", "authors": ["Yoseph Berhanu Alebachew"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding large-scale, complex software systems is a major challenge for\ndevelopers, who spend a significant portion of their time on program\ncomprehension. Traditional tools such as static visualizations and reverse\nengineering techniques provide structural insights but often lack\ninteractivity, adaptability, and integration with contextual information.\nRecent advancements in large language models (LLMs) offer new opportunities to\nenhance code exploration workflows, yet their lack of grounding and integration\nwith structured views limits their effectiveness. This work introduces a hybrid\napproach that integrates deterministic reverse engineering with LLM-guided,\nintent-aware visual exploration. The proposed system combines UML-based\nvisualization, dynamic user interfaces, historical context, and collaborative\nfeatures into an adaptive tool for code comprehension. By interpreting user\nqueries and interaction patterns, the LLM helps developers navigate and\nunderstand complex codebases more effectively. A prototype implementation for\nJava demonstrates the feasibility of this approach. Future work includes\nempirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM\ninteraction models. This research lays the groundwork for intelligent,\ninteractive environments that align with developer cognition and collaborative\nworkflows."}
{"id": "2508.06030", "pdf": "https://arxiv.org/pdf/2508.06030.pdf", "abs": "https://arxiv.org/abs/2508.06030", "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek."}
{"id": "2508.05846", "pdf": "https://arxiv.org/pdf/2508.05846.pdf", "abs": "https://arxiv.org/abs/2508.05846", "title": "Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "cs.RO", "68T01, 68T40", "K.7.4; K.4.1; I.2.9; H.1.2"], "comment": "Published in the Proceedings of the 2025 3rd International Conference\n  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 tables", "summary": "As artificial intelligence (AI) and robotics increasingly permeate society,\nensuring the ethical behavior of these systems has become paramount. This paper\ncontends that transparency in AI decision-making processes is fundamental to\ndeveloping trustworthy and ethically aligned robotic systems. We explore how\ntransparency facilitates accountability, enables informed consent, and supports\nthe debugging of ethical algorithms. The paper outlines technical, ethical, and\npractical challenges in implementing transparency and proposes novel approaches\nto enhance it, including standardized metrics, explainable AI techniques, and\nuser-friendly interfaces. This paper introduces a framework that connects\ntechnical implementation with ethical considerations in robotic systems,\nfocusing on the specific challenges of achieving transparency in dynamic,\nreal-world contexts. We analyze how prioritizing transparency can impact public\ntrust, regulatory policies, and avenues for future research. By positioning\ntransparency as a fundamental element in ethical AI system design, we aim to\nadd to the ongoing discussion on responsible AI and robotics, providing\ndirection for future advancements in this vital field."}
{"id": "2508.06046", "pdf": "https://arxiv.org/pdf/2508.06046.pdf", "abs": "https://arxiv.org/abs/2508.06046", "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach."}
{"id": "2508.05946", "pdf": "https://arxiv.org/pdf/2508.05946.pdf", "abs": "https://arxiv.org/abs/2508.05946", "title": "Social and Telepresence Robots for Accessibility and Inclusion in Small Museums", "authors": ["Nello Balossino", "Rossana Damiano", "Cristina Gena", "Alberto Lillo", "Anna Maria Marras", "Claudio Mattutino", "Antonio Pizzo", "Alessia Prin", "Fabiana Vernero"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "There are still many museums that present accessibility barriers,\nparticularly regarding perceptual, cultural, and cognitive aspects. This is\nespecially evident in low-density population areas. The aim of the ROBSO-PM\nproject is to improve the accessibility of small museums through the use of\nsocial robots and social telepresence robots, focusing on three museums as case\nstudies: the Museum of the Holy Shroud in Turin, a small but globally known\ninstitution, and two lesser known mountain museums: the Museum of the Champlas\ndu Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and\nTraditions. The project explores two main applications for robots: as guides\nsupporting inclusive visits for foreign or disabled visitors, and as\ntelepresence tools allowing people with limited mobility to access museums\nremotely. From a research perspective, key topics include storytelling, robot\npersonality, empathy, personalization, and, in the case of telepresence,\ncollaboration between the robot and the person, with clearly defined roles and\nautonomy."}
{"id": "2508.06094", "pdf": "https://arxiv.org/pdf/2508.06094.pdf", "abs": "https://arxiv.org/abs/2508.06094", "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Gaper Begu"], "categories": ["cs.CL"], "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise."}
{"id": "2508.05963", "pdf": "https://arxiv.org/pdf/2508.05963.pdf", "abs": "https://arxiv.org/abs/2508.05963", "title": "Bionic Vision as Neuroadaptive XR: Closed-Loop Perceptual Interfaces for Neurotechnology", "authors": ["Michael Beyeler"], "categories": ["cs.ET", "cs.HC"], "comment": null, "summary": "Visual neuroprostheses are commonly framed as technologies to restore natural\nsight to people who are blind. In practice, they create a novel mode of\nperception shaped by sparse, distorted, and unstable input. They resemble early\nextended reality (XR) headsets more than natural vision, streaming video from a\nhead-mounted camera to a neural \"display\" with under 1000 pixels, limited field\nof view, low refresh rates, and nonlinear spatial mappings. No amount of\nresolution alone will make this experience natural. This paper proposes a\nreframing: bionic vision as neuroadaptive XR. Rather than replicating natural\nsight, the goal is to co-adapt brain and device through a bidirectional\ninterface that responds to neural constraints, behavioral goals, and cognitive\nstate. By comparing traditional XR, current implants, and proposed\nneuroadaptive systems, it introduces a new design space for inclusive,\nbrain-aware computing. It concludes with research provocations spanning\nencoding, evaluation, learning, and ethics, and invites the XR community to\nhelp shape the future of sensory augmentation."}
{"id": "2508.06103", "pdf": "https://arxiv.org/pdf/2508.06103.pdf", "abs": "https://arxiv.org/abs/2508.06103", "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "categories": ["cs.CL", "cs.IR"], "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks."}
{"id": "2508.05979", "pdf": "https://arxiv.org/pdf/2508.05979.pdf", "abs": "https://arxiv.org/abs/2508.05979", "title": "Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education", "authors": ["Xinming Yang", "Haasil Pujara", "Jun Li"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Published at COLM 2025", "summary": "While Large Language Models (LLMs) are often used as virtual tutors in\ncomputer science (CS) education, this approach can foster passive learning and\nover-reliance. This paper presents a novel pedagogical paradigm that inverts\nthis model: students act as instructors who must teach an LLM to solve\nproblems. To facilitate this, we developed strategies for designing questions\nwith engineered knowledge gaps that only a student can bridge, and we introduce\nSocrates, a system for deploying this method with minimal overhead. We\nevaluated our approach in an undergraduate course and found that this\nactive-learning method led to statistically significant improvements in student\nperformance compared to historical cohorts. Our work demonstrates a practical,\ncost-effective framework for using LLMs to deepen student engagement and\nmastery."}
{"id": "2508.06105", "pdf": "https://arxiv.org/pdf/2508.06105.pdf", "abs": "https://arxiv.org/abs/2508.06105", "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines."}
{"id": "2508.06086", "pdf": "https://arxiv.org/pdf/2508.06086.pdf", "abs": "https://arxiv.org/abs/2508.06086", "title": "Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions", "authors": ["Kojiro Tanaka", "Keiichi Sato", "Masahiko Mikawa", "Makoto Fujisawa"], "categories": ["cs.GR", "cs.HC"], "comment": "Accepted to 20th IFIP TC13 International Conference on Human-Computer\n  Interaction (INTERACT '25), 24 pages", "summary": "Recent research has focused on incorporating media into living environments\nvia color-controlled materials and image display. In particular, grass-based\ndisplays have drawn attention as landscape-friendly interactive interfaces. To\ndevelop the grass display, it is important to obtain the grass color change\ncharacteristics that depend on the real environment. However, conventional\nmethods require experiments on actual equipment every time the lighting or\nviewpoint changes, which is time-consuming and costly. Although research has\nbegun on simulating grass colors, this approach still faces significant issues\nas it takes many hours for a single measurement. In this paper, we explore an\ninteractive simulation of a grass display color change characteristic based on\nreal-world conditions in a virtual environment. We evaluated our method's\naccuracy by simulating grass color characteristics across multiple viewpoints\nand environments, and then compared the results against prior work. The results\nindicated that our method tended to simulate the grass color characteristics\nsimilar to the actual characteristics and showed the potential to do so more\nquickly and with comparable accuracy to the previous study."}
{"id": "2508.06124", "pdf": "https://arxiv.org/pdf/2508.06124.pdf", "abs": "https://arxiv.org/abs/2508.06124", "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "categories": ["cs.CL"], "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications."}
{"id": "2508.06167", "pdf": "https://arxiv.org/pdf/2508.06167.pdf", "abs": "https://arxiv.org/abs/2508.06167", "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "authors": ["Vt Gvodiak"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI."}
{"id": "2508.06135", "pdf": "https://arxiv.org/pdf/2508.06135.pdf", "abs": "https://arxiv.org/abs/2508.06135", "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs."}
{"id": "2508.06196", "pdf": "https://arxiv.org/pdf/2508.06196.pdf", "abs": "https://arxiv.org/abs/2508.06196", "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."}
{"id": "2508.06149", "pdf": "https://arxiv.org/pdf/2508.06149.pdf", "abs": "https://arxiv.org/abs/2508.06149", "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents."}
{"id": "2508.06321", "pdf": "https://arxiv.org/pdf/2508.06321.pdf", "abs": "https://arxiv.org/abs/2508.06321", "title": "EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition", "authors": ["Durjoy Chandra Paul", "Gaurob Saha", "Md Amjad Hossain"], "categories": ["cs.SD", "cs.HC", "cs.LG"], "comment": "To be published in ICCCNT 2025 (16th International Conference on\n  Computing Communication and Networking Technologies)", "summary": "Recognizing emotional signals in speech has a significant impact on enhancing\nthe effectiveness of human-computer interaction (HCI). This study introduces\nEmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term\nMemory (LSTM) layers with one-dimensional Convolutional Neural Networks\n(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and\nvariety of the features that are taken from speech signals have a significant\nimpact on how well SER systems perform. A comprehensive speech data\naugmentation strategy was used to combine both traditional methods, such as\nnoise addition, pitch shifting, and time stretching, with a novel\ncombination-based augmentation pipeline to enhance generalization and reduce\noverfitting. Each audio sample was transformed into a high-dimensional feature\nvector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient\n(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a\nweighted accuracy of 95.78\\% and unweighted accuracy of 92.52\\% on the IEMOCAP\ndataset and, with ELU activation, has a weighted accuracy of 96.75\\% and\nunweighted accuracy of 91.28\\%. On the RAVDESS dataset, we get a weighted\naccuracy of 94.53\\% and 94.98\\% unweighted accuracy for ReLU activation and\n93.72\\% weighted accuracy and 94.64\\% unweighted accuracy for ELU activation.\nThese results highlight EmoAugNet's effectiveness in improving the robustness\nand performance of SER systems through integated data augmentation and hybrid\nmodeling."}
{"id": "2508.06155", "pdf": "https://arxiv.org/pdf/2508.06155.pdf", "abs": "https://arxiv.org/abs/2508.06155", "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired."}
{"id": "2508.06336", "pdf": "https://arxiv.org/pdf/2508.06336.pdf", "abs": "https://arxiv.org/abs/2508.06336", "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating."}
{"id": "2508.06163", "pdf": "https://arxiv.org/pdf/2508.06163.pdf", "abs": "https://arxiv.org/abs/2508.06163", "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging."}
{"id": "2508.06352", "pdf": "https://arxiv.org/pdf/2508.06352.pdf", "abs": "https://arxiv.org/abs/2508.06352", "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts."}
{"id": "2508.06165", "pdf": "https://arxiv.org/pdf/2508.06165.pdf", "abs": "https://arxiv.org/abs/2508.06165", "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."}
{"id": "2508.06391", "pdf": "https://arxiv.org/pdf/2508.06391.pdf", "abs": "https://arxiv.org/abs/2508.06391", "title": "Improved Dysarthric Speech to Text Conversion via TTS Personalization", "authors": ["Pter Mihajlik", "va Szkely", "Piroska Barta", "Mt Soma Kdr", "Gergely Dobsinszki", "Lszl Tth"], "categories": ["cs.SD", "cs.HC"], "comment": null, "summary": "We present a case study on developing a customized speech-to-text system for\na Hungarian speaker with severe dysarthria. State-of-the-art automatic speech\nrecognition (ASR) models struggle with zero-shot transcription of dysarthric\nspeech, yielding high error rates. To improve performance with limited real\ndysarthric data, we fine-tune an ASR model using synthetic speech generated via\na personalized text-to-speech (TTS) system. We introduce a method for\ngenerating synthetic dysarthric speech with controlled severity by leveraging\npremorbidity recordings of the given speaker and speaker embedding\ninterpolation, enabling ASR fine-tuning on a continuum of impairments.\nFine-tuning on both real and synthetic dysarthric speech reduces the character\nerror rate (CER) from 36-51% (zero-shot) to 7.3%. Our monolingual\nFastConformer_Hu ASR model significantly outperforms Whisper-turbo when\nfine-tuned on the same data, and the inclusion of synthetic speech contributes\nto an 18% relative CER reduction. These results highlight the potential of\npersonalized ASR systems for improving accessibility for individuals with\nsevere speech impairments."}
{"id": "2508.06167", "pdf": "https://arxiv.org/pdf/2508.06167.pdf", "abs": "https://arxiv.org/abs/2508.06167", "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "authors": ["Vt Gvodiak"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI."}
{"id": "2412.00207", "pdf": "https://arxiv.org/pdf/2412.00207.pdf", "abs": "https://arxiv.org/abs/2412.00207", "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots", "authors": ["Huiqi Zou", "Pengda Wang", "Zihan Yan", "Tianjun Sun", "Ziang Xiao"], "categories": ["cs.HC"], "comment": "Accepted by COLM 2025", "summary": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation."}
{"id": "2508.06178", "pdf": "https://arxiv.org/pdf/2508.06178.pdf", "abs": "https://arxiv.org/abs/2508.06178", "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods."}
{"id": "2412.16321", "pdf": "https://arxiv.org/pdf/2412.16321.pdf", "abs": "https://arxiv.org/abs/2412.16321", "title": "XR for All: Understanding Developers' Perspectives on Accessibility Integration in Extended Reality", "authors": ["Daniel Killough", "Tiger F. Ji", "Kexin Zhang", "Yaxin Hu", "Yu Huang", "Ruofei Du", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": "20 pages, 1 figure, 3 tables, LaTeX", "summary": "As immersive technologies enable unique, multimodal interaction methods,\ndevelopers must also use tailored methods to support user accessibility,\ndistinct from traditional software practices. We interviewed 25 industry\nextended reality (XR) developers, including freelancers, startups, midsize, and\nbig tech companies about their motivations, techniques, barriers, and attitudes\ntowards incorporating accessibility features in their XR apps. Our study\nrevealed a variety of challenges, including conflicting priorities between\napplication and platform developers regarding accessibility infrastructure;\nrapid development culture hindering accessible development; and the lack of\naccessible interaction design considerations at the ideation, design, and early\nprototyping stages. As a comprehensive set of XR accessibility guidelines has\nyet to be established, we also compiled and evaluated a set of accessibility\nguidelines for 3D virtual worlds and addressed their limitations when applied\nto XR. Finally, we inform the creation of effective support methods for\nindustry developers."}
{"id": "2508.06186", "pdf": "https://arxiv.org/pdf/2508.06186.pdf", "abs": "https://arxiv.org/abs/2508.06186", "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input."}
{"id": "2502.20491", "pdf": "https://arxiv.org/pdf/2502.20491.pdf", "abs": "https://arxiv.org/abs/2502.20491", "title": "Examining Algorithmic Curation on Social Media: An Empirical Audit of Reddit's r/popular Feed", "authors": ["Jackie Chan", "Fred Choi", "Koustuv Saha", "Eshwar Chandrasekharan"], "categories": ["cs.HC", "cs.SI"], "comment": "15 pages, 5 figures", "summary": "Platforms are increasingly relying on algorithms to curate the content within\nusers' social media feeds. However, the growing prominence of proprietary,\nalgorithmically curated feeds has concealed what factors influence the\npresentation of content on social media feeds and how that presentation affects\nuser behavior. This lack of transparency can be detrimental to users, from\nreducing users' agency over their content consumption to the propagation of\nmisinformation and toxic content. To uncover details about how these feeds\noperate and influence user behavior, we conduct an empirical audit of Reddit's\nalgorithmically curated trending feed called r/popular. Using 10K r/popular\nposts collected by taking snapshots of the feed over 11 months, we find that\nrecent comments help a post remain on r/popular longer and climb the feed. We\nalso find that posts below rank 80 correspond to a sharp decline in activity\ncompared to posts above. When examining the effects of having a higher\nproportion of undesired behavior -- i.e., moderator-removed and toxic comments\n-- we find no significant evidence that it helps posts stay on r/popular for\nlonger. Although posts closer to the top receive more undesired comments, we\nfind this increase to coincide with a broader increase in overall engagement --\nrather than indicating a disproportionate effect on undesired activity. The\nrelationships between algorithmic rank and engagement highlight the extent to\nwhich algorithms employed by social media platforms essentially determine which\ncontent is prioritized and which is not. We conclude by discussing how content\ncreators, consumers, and moderators on social media platforms can benefit from\nempirical audits aimed at improving transparency in algorithmically curated\nfeeds."}
{"id": "2508.06194", "pdf": "https://arxiv.org/pdf/2508.06194.pdf", "abs": "https://arxiv.org/abs/2508.06194", "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage."}
{"id": "2504.13908", "pdf": "https://arxiv.org/pdf/2504.13908.pdf", "abs": "https://arxiv.org/abs/2504.13908", "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience", "authors": ["Soubhik Barari", "Jarret Angbazo", "Natalie Wang", "Leah M. Christian", "Elizabeth Dean", "Zoe Slowinski", "Brandon Sepulvado"], "categories": ["cs.HC", "cs.AI", "stat.AP"], "comment": null, "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys."}
{"id": "2508.06196", "pdf": "https://arxiv.org/pdf/2508.06196.pdf", "abs": "https://arxiv.org/abs/2508.06196", "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."}
{"id": "2507.02819", "pdf": "https://arxiv.org/pdf/2507.02819.pdf", "abs": "https://arxiv.org/abs/2507.02819", "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": "CSCW 2025", "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction."}
{"id": "2508.06204", "pdf": "https://arxiv.org/pdf/2508.06204.pdf", "abs": "https://arxiv.org/abs/2508.06204", "title": "Classification is a RAG problem: A case study on hate speech detection", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems."}
{"id": "2507.21462", "pdf": "https://arxiv.org/pdf/2507.21462.pdf", "abs": "https://arxiv.org/abs/2507.21462", "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education."}
{"id": "2508.06220", "pdf": "https://arxiv.org/pdf/2508.06220.pdf", "abs": "https://arxiv.org/abs/2508.06220", "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems."}
{"id": "2508.02639", "pdf": "https://arxiv.org/pdf/2508.02639.pdf", "abs": "https://arxiv.org/abs/2508.02639", "title": "Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable", "authors": ["Tingying He", "Jason Dykes", "Petra Isenberg", "Tobias Isenberg"], "categories": ["cs.HC"], "comment": "IEEE Transactions on Visualization and Computer Graphics, 2026", "summary": "We present a new comprehensive theory for explaining, exploring, and using\npattern as a visual variable in visualization. Although patterns have long been\nused for data encoding and continue to be valuable today, their conceptual\nfoundations are precarious: the concepts and terminology used across the\nresearch literature and in practice are inconsistent, making it challenging to\nuse patterns effectively and to conduct research to inform their use. To\naddress this problem, we conduct a comprehensive cross-disciplinary literature\nreview that clarifies ambiguities around the use of \"pattern\" and \"texture\". As\na result, we offer a new consistent treatment of pattern as a composite visual\nvariable composed of structured groups of graphic primitives that can serve as\nmarks for encoding data individually and collectively. This new and widely\napplicable formulation opens a sizable design space for the visual variable\npattern, which we formalize as a new system comprising three sets of variables:\nthe spatial arrangement of primitives, the appearance relationships among\nprimitives, and the retinal visual variables that characterize individual\nprimitives. We show how our pattern system relates to existing visualization\ntheory and highlight opportunities for visualization design. We further explore\npatterns based on complex spatial arrangements, demonstrating explanatory power\nand connecting our conceptualization to broader theory on maps and cartography.\nAn author version and additional materials are available on OSF: osf.io/z7ae2."}
{"id": "2508.06277", "pdf": "https://arxiv.org/pdf/2508.06277.pdf", "abs": "https://arxiv.org/abs/2508.06277", "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility."}
{"id": "2508.02679", "pdf": "https://arxiv.org/pdf/2508.02679.pdf", "abs": "https://arxiv.org/abs/2508.02679", "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "categories": ["cs.HC"], "comment": null, "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health."}
{"id": "2508.06309", "pdf": "https://arxiv.org/pdf/2508.06309.pdf", "abs": "https://arxiv.org/abs/2508.06309", "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "authors": ["Ruichong Zhang"], "categories": ["cs.CL", "math.PR"], "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible."}
{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700.pdf", "abs": "https://arxiv.org/abs/2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhiheng Xi", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Xuanjing Huang", "Yu-Gang Jiang", "Tao Gui", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."}
{"id": "2508.06345", "pdf": "https://arxiv.org/pdf/2508.06345.pdf", "abs": "https://arxiv.org/abs/2508.06345", "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy"}
{"id": "2508.04541", "pdf": "https://arxiv.org/pdf/2508.04541.pdf", "abs": "https://arxiv.org/abs/2508.04541", "title": "Measuring Information Richness in Product Images: Implications for Online Sales", "authors": ["Zhu Yuting", "Cao Xinyu", "Su Yuzhuo", "Ma Yongbin"], "categories": ["cs.HC"], "comment": null, "summary": "A common challenge for e-commerce sellers is to decide what product images to\ndisplay on online shopping sites. In this paper, we propose and validate a\nnovel metric, k-value, to quantify the information richness of an image set,\nand we further investigate its effect on consumers' purchase decisions. We\nleverage patch-level embeddings from Vision Transformers (ViT) and apply\nk-means clustering to identify distinct visual features, defining k-value as\nthe number of clusters. An online experiment demonstrates that k-value aligns\nwith human-perceived information richness, validating the metric. A simulated\nonline shopping experiment further reveals a significant yet counterintuitive\nresult: while an image set with a higher k-value (richer information) shortens\ndecision time, it paradoxically reduces purchase propensity. Our findings\nilluminate the complex relationship between visual information richness and\nconsumer behavior, providing sellers a quantifiable tool for image selection."}
{"id": "2508.06360", "pdf": "https://arxiv.org/pdf/2508.06360.pdf", "abs": "https://arxiv.org/abs/2508.06360", "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."}
{"id": "2508.04902", "pdf": "https://arxiv.org/pdf/2508.04902.pdf", "abs": "https://arxiv.org/abs/2508.04902", "title": "Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model", "authors": ["Luis Morales-Navarro", "Michelle Gan", "Evelyn Yu", "Lauren Vogelstein", "Yasmin B. Kafai", "Dana Metaxa"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.2"], "comment": null, "summary": "This study investigates how high school-aged youth engage in algorithm\nauditing to identify and understand biases in artificial intelligence and\nmachine learning (AI/ML) tools they encounter daily. With AI/ML technologies\nbeing increasingly integrated into young people's lives, there is an urgent\nneed to equip teenagers with AI literacies that build both technical knowledge\nand awareness of social impacts. Algorithm audits (also called AI audits) have\ntraditionally been employed by experts to assess potential harmful biases, but\nrecent research suggests that non-expert users can also participate\nproductively in auditing. We conducted a two-week participatory design workshop\nwith 14 teenagers (ages 14-15), where they audited the generative AI model\nbehind TikTok's Effect House, a tool for creating interactive TikTok filters.\nWe present a case study describing how teenagers approached the audit, from\ndeciding what to audit to analyzing data using diverse strategies and\ncommunicating their results. Our findings show that participants were engaged\nand creative throughout the activities, independently raising and exploring new\nconsiderations, such as age-related biases, that are uncommon in professional\naudits. We drew on our expertise in algorithm auditing to triangulate their\nfindings as a way to examine if the workshop supported participants to reach\ncoherent conclusions in their audit. Although the resulting number of changes\nin race, gender, and age representation uncovered by the teens were slightly\ndifferent from ours, we reached similar conclusions. This study highlights the\npotential for auditing to inspire learning activities to foster AI literacies,\nempower teenagers to critically examine AI systems, and contribute fresh\nperspectives to the study of algorithmic harms."}
{"id": "2508.06374", "pdf": "https://arxiv.org/pdf/2508.06374.pdf", "abs": "https://arxiv.org/abs/2508.06374", "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "categories": ["cs.CL"], "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation."}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."}
{"id": "2508.06388", "pdf": "https://arxiv.org/pdf/2508.06388.pdf", "abs": "https://arxiv.org/abs/2508.06388", "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "categories": ["cs.CL"], "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks."}
{"id": "2508.06418", "pdf": "https://arxiv.org/pdf/2508.06418.pdf", "abs": "https://arxiv.org/abs/2508.06418", "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy."}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306.pdf", "abs": "https://arxiv.org/abs/2507.06306", "title": "Humans overrely on overconfident language models, across languages", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "camera ready", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Prior work shows that LLMs are linguistically\noverconfident in English, leading users to overrely on confident generations.\nHowever, the usage and interpretation of epistemic markers (e.g., 'I think\nit's') differs sharply across languages. Here, we study the risks of\nmultilingual linguistic (mis)calibration, overconfidence, and overreliance\nacross five languages to evaluate LLM safety in a global context. Our work\nfinds that overreliance risks are high across languages. We first analyze the\ndistribution of LLM-generated epistemic markers and observe that LLMs are\noverconfident across languages, frequently generating strengtheners even as\npart of incorrect responses. Model generations are, however, sensitive to\ndocumented cross-linguistic variation in usage: for example, models generate\nthe most markers of uncertainty in Japanese and the most markers of certainty\nin German and Mandarin. Next, we measure human reliance rates across languages,\nfinding that reliance behaviors differ cross-linguistically: for example,\nparticipants are significantly more likely to discount expressions of\nuncertainty in Japanese than in English (i.e., ignore their 'hedging' function\nand rely on generations that contain them). Taken together, these results\nindicate a high risk of reliance on overconfident model generations across\nlanguages. Our findings highlight the challenges of multilingual linguistic\ncalibration and stress the importance of culturally and linguistically\ncontextualized model safety evaluations."}
{"id": "2508.06433", "pdf": "https://arxiv.org/pdf/2508.06433.pdf", "abs": "https://arxiv.org/abs/2508.06433", "title": "Memp: Exploring Agent Procedural Memory", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."}
{"id": "2508.06435", "pdf": "https://arxiv.org/pdf/2508.06435.pdf", "abs": "https://arxiv.org/abs/2508.06435", "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research."}
{"id": "2508.06445", "pdf": "https://arxiv.org/pdf/2508.06445.pdf", "abs": "https://arxiv.org/abs/2508.06445", "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media."}
{"id": "2508.06447", "pdf": "https://arxiv.org/pdf/2508.06447.pdf", "abs": "https://arxiv.org/abs/2508.06447", "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."}
{"id": "2508.06471", "pdf": "https://arxiv.org/pdf/2508.06471.pdf", "abs": "https://arxiv.org/abs/2508.06471", "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5."}
{"id": "2508.06475", "pdf": "https://arxiv.org/pdf/2508.06475.pdf", "abs": "https://arxiv.org/abs/2508.06475", "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "categories": ["cs.CL"], "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data."}
{"id": "2508.06482", "pdf": "https://arxiv.org/pdf/2508.06482.pdf", "abs": "https://arxiv.org/abs/2508.06482", "title": "Post-training for Efficient Communication via Convention Formation", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."}
{"id": "2508.04748", "pdf": "https://arxiv.org/pdf/2508.04748.pdf", "abs": "https://arxiv.org/abs/2508.04748", "title": "AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models", "authors": ["Xuan Lin", "Long Chen", "Yile Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) have shown promise in assisting molecular\nproperty prediction tasks but often rely on human-crafted prompts and\nchain-of-thought templates. While recent advanced large reasoning models like\nDeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,\ntheir reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,\nan attribute-guided reinforcement learning framework for molecular property\nprediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)\na format reward encouraging attribute-based structured output, (2) a count\nreward to avoid enumerating irrelevant attributes, and (3) a rationality reward\nusing advanced LLMs and RDKit to verify the relatedness of the generated\nattributes. This approach implicitly elicits the model's inherent knowledge of\nrelevant molecular attributes during reasoning, enables making predictions for\nthe molecular property more effectively. Experiments on both in-distribution\nand out-of-distribution datasets show that, training both 7B-size\nR1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our\nproposed AttriLens-Mol method significantly boosts the performance, getting\ncomparable or better results than supervised fine-tuning models\n(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,\nDeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the\ntarget property, when used as features for an interpretable decision tree\nmodel, yield superior performance compared to attributes generated by prompting\nLLMs. This shows that AttriLens-Mol effectively elicits more relevant and\npredictive molecular attributes, leading to enhanced interpretability and\nperformance for property prediction. We release the code in\nhttps://github.com/szu-tera/AttriLens-Mol."}
{"id": "2508.05664", "pdf": "https://arxiv.org/pdf/2508.05664.pdf", "abs": "https://arxiv.org/abs/2508.05664", "title": "Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support", "authors": ["Hei Yu Chan", "Kuok Tou Ho", "Chenglong Ma", "Yujing Si", "Hok Lai Lin", "Sa Lei Lam"], "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.m"], "comment": "6 pages", "summary": "Many AI customer service systems use standard NLP pipelines or finetuned\nlanguage models, which often fall short on ambiguous, multi-intent, or\ndetail-specific queries. This case study evaluates recent techniques: query\nrewriting, RAG Fusion, keyword augmentation, intent recognition, and context\nreranking, for building a robust customer support system in the electric power\ndomain. We compare vector-store and graph-based RAG frameworks, ultimately\nselecting the graph-based RAG for its superior performance in handling complex\nqueries. We find that query rewriting improves retrieval for queries using\nnon-standard terminology or requiring precise detail. RAG Fusion boosts\nperformance on vague or multifaceted queries by merging multiple retrievals.\nReranking reduces hallucinations by filtering irrelevant contexts. Intent\nrecognition supports the decomposition of complex questions into more targeted\nsub-queries, increasing both relevance and efficiency. In contrast, keyword\naugmentation negatively impacts results due to biased keyword selection. Our\nfinal system combines intent recognition, RAG Fusion, and reranking to handle\ndisambiguation and multi-source queries. Evaluated on both a GPT-4-generated\ndataset and a real-world electricity provider FAQ dataset, it achieves 97.9%\nand 89.6% accuracy respectively, substantially outperforming baseline RAG\nmodels."}
{"id": "2508.05668", "pdf": "https://arxiv.org/pdf/2508.05668.pdf", "abs": "https://arxiv.org/abs/2508.05668", "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges", "authors": ["Yunjia Xi", "Jianghao Lin", "Yongzhao Xiao", "Zheli Zhou", "Rong Shan", "Te Gao", "Jiachen Zhu", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers."}
{"id": "2508.05669", "pdf": "https://arxiv.org/pdf/2508.05669.pdf", "abs": "https://arxiv.org/abs/2508.05669", "title": "Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports", "authors": ["Jin Khye Tan", "En Jun Choong", "Ethan Jeremiah Chitty", "Yan Pheng Choo", "John Hsin Yang Wong", "Chern Eu Cheah"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "I.2.7; I.7.2; J.1"], "comment": "28 pages, 14 figures, 5 tables. Evaluation code (LLM-as-a-judge and\n  Markdown TEDS) is available at https://github.com/jinkhye/MyFinMarkdown. The\n  development dataset and evaluation benchmark are available on Hugging Face at\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-sample and\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-bench respectively", "summary": "Accurately extracting and representing the structure of tabular data from\nfinancial documents remains a critical challenge in document understanding,\nparticularly for regulatory and analytical use cases. This study addresses the\ncomplexity of converting financial tables from Malaysian audited financial\nreports into Markdown format, a task complicated by rotated layouts,\nmulti-level headers, and implicit structural cues. We propose a fine-tuned\nvision-language model (VLM), based on Qwen2.5-VL-7B, optimized for\nhigh-fidelity Markdown generation from document images. Our approach includes a\ncurated dataset of 2,152 image-text pairs with augmentations and a supervised\nfine-tuning strategy using LoRA. To assess performance, we evaluated our model\non 100 out-of-sample tables using a dual framework: a criteria-based\nLLM-as-a-judge for fine-grained accuracy and our novel Markdown\nTree-Edit-Distance-based Similarity (TEDS) metric for holistic structural\nfidelity. Our model achieves a 92.20% overall accuracy on the criteria-based\nassessment and a 96.53% Markdown TEDS score. This performance significantly\nsurpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized\nreasoning-enabled models. Compared to these self-hosted alternatives, it also\nsignificantly reduces inference time. Furthermore, its accuracy exceeds that of\nwidely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.\nThese results demonstrate that domain-specific fine-tuning provides an\neffective and efficient method to bridge the gap between unstructured financial\ndocuments and downstream automation, rivalling much larger and more general\nmodels without their computational overhead."}
{"id": "2508.05671", "pdf": "https://arxiv.org/pdf/2508.05671.pdf", "abs": "https://arxiv.org/abs/2508.05671", "title": "DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing", "authors": ["Ko-Wei Chuang", "Hen-Hsen Huang", "Tsai-Yen Li"], "categories": ["cs.CR", "cs.CL"], "comment": "7 pages", "summary": "As large language models (LLMs) and generative AI become increasingly\nintegrated into customer service and moderation applications, adversarial\nthreats emerge from both external manipulations and internal label corruption.\nIn this work, we identify and systematically address these dual adversarial\nthreats by introducing DINA (Dual Defense Against Internal Noise and\nAdversarial Attacks), a novel unified framework tailored specifically for NLP.\nOur approach adapts advanced noisy-label learning methods from computer vision\nand integrates them with adversarial training to simultaneously mitigate\ninternal label sabotage and external adversarial perturbations. Extensive\nexperiments conducted on a real-world dataset from an online gaming service\ndemonstrate that DINA significantly improves model robustness and accuracy\ncompared to baseline models. Our findings not only highlight the critical\nnecessity of dual-threat defenses but also offer practical strategies for\nsafeguarding NLP systems in realistic adversarial scenarios, underscoring\nbroader implications for fair and responsible AI deployment."}
{"id": "2508.05694", "pdf": "https://arxiv.org/pdf/2508.05694.pdf", "abs": "https://arxiv.org/abs/2508.05694", "title": "DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Guanggang Geng", "Zhiying Li", "Jian Weng"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Insider threat detection (ITD) poses a persistent and high-impact challenge\nin cybersecurity due to the subtle, long-term, and context-dependent nature of\nmalicious insider behaviors. Traditional models often struggle to capture\nsemantic intent and complex behavior dynamics, while existing LLM-based\nsolutions face limitations in prompt adaptability and modality coverage. To\nbridge this gap, we propose DMFI, a dual-modality framework that integrates\nsemantic inference with behavior-aware fine-tuning. DMFI converts raw logs into\ntwo structured views: (1) a semantic view that processes content-rich artifacts\n(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral\nabstraction, constructed via a 4W-guided (When-Where-What-Which) transformation\nto encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned\nindependently, and their outputs are fused via a lightweight MLP-based decision\nmodule. We further introduce DMFI-B, a discriminative adaptation strategy that\nseparates normal and abnormal behavior representations, improving robustness\nunder severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets\ndemonstrate that DMFI outperforms state-of-the-art methods in detection\naccuracy. Our approach combines the semantic reasoning power of LLMs with\nstructured behavior modeling, offering a scalable and effective solution for\nreal-world insider threat detection. Our work demonstrates the effectiveness of\ncombining LLM reasoning with structured behavioral modeling, offering a\nscalable and deployable solution for modern insider threat detection."}
{"id": "2508.05731", "pdf": "https://arxiv.org/pdf/2508.05731.pdf", "abs": "https://arxiv.org/abs/2508.05731", "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "categories": ["cs.AI", "cs.CL"], "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1."}
{"id": "2508.05798", "pdf": "https://arxiv.org/pdf/2508.05798.pdf", "abs": "https://arxiv.org/abs/2508.05798", "title": "Basic interactive algorithms: Preview", "authors": ["Yuri Gurevich"], "categories": ["cs.LO", "cs.CL", "math.LO", "quant-ph"], "comment": null, "summary": "This dialog paper offers a preview and provides a foretaste of an upcoming\nwork on the axiomatization of basic interactive algorithms.\n  The modern notion of algorithm was elucidated in the 1930s--1950s. It was\naxiomatized a quarter of a century ago as the notion of ``sequential\nalgorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm\"\nnow. The axiomatization was used to show that for every basic algorithm there\nis a behaviorally equivalent abstract state machine. It was also used to prove\nthe Church-Turing thesis as it has been understood by the logicians.\n  Starting from the 1960s, the notion of algorithm has expanded --\nprobabilistic algorithms, quantum algorithms, etc. -- prompting introduction of\na much more ambitious version of the Church-Turing thesis commonly known as the\n``physical thesis.'' We emphasize the difference between the two versions of\nthe Church-Turing thesis and illustrate how nondeterministic and probabilistic\nalgorithms can be viewed as basic algorithms with appropriate oracles. The same\nview applies to quantum circuit algorithms and many other classes of\nalgorithms."}
{"id": "2508.05835", "pdf": "https://arxiv.org/pdf/2508.05835.pdf", "abs": "https://arxiv.org/abs/2508.05835", "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference", "authors": ["Edresson Casanova", "Paarth Neekhara", "Ryan Langman", "Shehzeen Hussain", "Subhankar Ghosh", "Xuesong Yang", "Ante Juki", "Jason Li", "Boris Ginsburg"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference."}
{"id": "2508.05913", "pdf": "https://arxiv.org/pdf/2508.05913.pdf", "abs": "https://arxiv.org/abs/2508.05913", "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "authors": ["Stefan Pasch", "Min Chul Cha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types."}
{"id": "2508.05954", "pdf": "https://arxiv.org/pdf/2508.05954.pdf", "abs": "https://arxiv.org/abs/2508.05954", "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices."}
{"id": "2508.06017", "pdf": "https://arxiv.org/pdf/2508.06017.pdf", "abs": "https://arxiv.org/abs/2508.06017", "title": "Position: Intelligent Coding Systems Should Write Programs with Justifications", "authors": ["Xiangzhe Xu", "Shiwei Feng", "Zian Su", "Chengpeng Wang", "Xiangyu Zhang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Intelligent coding systems are transforming software development by enabling\nusers to specify code behavior in natural language. However, the opaque\ndecision-making of AI-driven coders raises trust and usability concerns,\nparticularly for non-expert users who cannot inspect low-level implementations.\nWe argue that these systems should not only generate code but also produce\nclear, consistent justifications that bridge model reasoning and user\nunderstanding. To this end, we identify two critical justification\nproperties-cognitive alignment and semantic faithfulness-and highlight the\nlimitations of existing methods, including formal verification, static\nanalysis, and post-hoc explainability. We advocate exploring neuro-symbolic\napproaches for justification generation, where symbolic constraints guide model\nbehavior during training and program semantics are enriched through neural\nrepresentations, enabling automated consistency checks at inference time."}
{"id": "2508.06059", "pdf": "https://arxiv.org/pdf/2508.06059.pdf", "abs": "https://arxiv.org/abs/2508.06059", "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System", "authors": ["Haorui He", "Yupeng Li", "Bin Benjamin Zhu", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures."}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065.pdf", "abs": "https://arxiv.org/abs/2508.06065", "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools."}
{"id": "2508.06401", "pdf": "https://arxiv.org/pdf/2508.06401.pdf", "abs": "https://arxiv.org/abs/2508.06401", "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges", "authors": ["Andrew Brown", "Muhammad Roman", "Barry Devereux"], "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.IR"], "comment": "58 pages", "summary": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research."}
{"id": "2508.06412", "pdf": "https://arxiv.org/pdf/2508.06412.pdf", "abs": "https://arxiv.org/abs/2508.06412", "title": "Sample-efficient LLM Optimization with Reset Replay", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."}
{"id": "2508.06457", "pdf": "https://arxiv.org/pdf/2508.06457.pdf", "abs": "https://arxiv.org/abs/2508.06457", "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls", "authors": ["Sanket Badhe"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.MA"], "comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI."}
{"id": "2508.06492", "pdf": "https://arxiv.org/pdf/2508.06492.pdf", "abs": "https://arxiv.org/abs/2508.06492", "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD."}
{"id": "2402.17008", "pdf": "https://arxiv.org/pdf/2402.17008.pdf", "abs": "https://arxiv.org/abs/2402.17008", "title": "Benchmarking LLMs on the Semantic Overlap Summarization Task", "authors": ["John Salvador", "Naman Bansal", "Mousumi Akter", "Souvika Sarkar", "Anupam Das", "Shubhra Kanti Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Semantic Overlap Summarization (SOS) is a constrained multi-document\nsummarization task, where the constraint is to capture the common/overlapping\ninformation between two alternative narratives. In this work, we perform a\nbenchmarking study of popular Large Language Models (LLMs) exclusively on the\nSOS task. Additionally, we introduce the PrivacyPolicyPairs (3P) dataset to\nexpand the space of SOS benchmarks in terms of quantity and variety. This\ndataset provides 135 high-quality SOS data samples sourced from privacy policy\ndocuments. We then use a standard prompting taxonomy called TELeR to create and\nevaluate 905,216 distinct LLM-generated summaries over two SOS datasets from\ndifferent domains, and we further conduct human evaluation on a subset of 540\nsamples. We conclude the paper by analyzing models' performances and the\nreliability of automatic evaluation. The code and datasets used to conduct this\nstudy are available at https://anonymous.4open.science/r/llm_eval-E16D."}
{"id": "2404.03353", "pdf": "https://arxiv.org/pdf/2404.03353.pdf", "abs": "https://arxiv.org/abs/2404.03353", "title": "Towards Pareto Optimal Throughput in Small Language Model Serving", "authors": ["Pol G. Recasens", "Yue Zhu", "Chen Wang", "Eun Kyung Lee", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "categories": ["cs.CL"], "comment": "Revised version of the paper published at EuroMLSys'24, fix figure 6\n  and 7", "summary": "Large language models (LLMs) have revolutionized the state-of-the-art of many\ndifferent natural language processing tasks. Although serving LLMs is\ncomputationally and memory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who now are able to\nserve small models with cutting-edge performance. In this paper, we present a\nset of experiments designed to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in serving, highlighting\nthat the small memory footprint of SLMs allows for reaching the Pareto-optimal\nthroughput within the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating how model\nreplication can effectively improve resource utilization for serving SLMs."}
{"id": "2409.11827", "pdf": "https://arxiv.org/pdf/2409.11827.pdf", "abs": "https://arxiv.org/abs/2409.11827", "title": "Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework", "authors": ["Yuping Wu", "Hao Li", "Goran Nenadic", "Xiao-Jun Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive\nsummarization with the help of salient information identified by the extractive\nmodel. Previous works that adopt this paradigm train the extractor and\nabstractor separately and introduce extra parameters to highlight the extracted\nsalients to the abstractor, which results in error accumulation and additional\ntraining costs. In this paper, we first introduce a parameter-free highlight\nmethod into the encoder-decoder framework: replacing the encoder attention mask\nwith a saliency mask in the cross-attention module to force the decoder to\nfocus only on salient parts of the input. A preliminary analysis compares\ndifferent highlight methods, demonstrating the effectiveness of our saliency\nmask. We further propose the novel extract-and-abstract paradigm, ExtAbs.,\nwhich jointly and seamlessly performs Extractive and Abstractive summarization\ntasks within single encoder-decoder model to reduce error accumulation. In\nExtAbs, the vanilla encoder is augmented to extract salients, and the vanilla\ndecoder is modified with the proposed saliency mask to generate summaries.\nBuilt upon BART and PEGASUS, experiments on three datasets show that ExtAbs can\nachieve superior performance than baselines on the extractive task and performs\ncomparable, or even better than the vanilla models on the abstractive task."}
{"id": "2501.01872", "pdf": "https://arxiv.org/pdf/2501.01872.pdf", "abs": "https://arxiv.org/abs/2501.01872", "title": "Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions", "authors": ["Rachneet Sachdeva", "Rima Hazra", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Our code is publicly available at\n  https://github.com/UKPLab/arxiv2025-poate-attack", "summary": "Large language models, despite extensive alignment with human values and\nethical principles, remain vulnerable to sophisticated jailbreak attacks that\nexploit their reasoning abilities. Existing safety measures often detect overt\nmalicious intent but fail to address subtle, reasoning-driven vulnerabilities.\nIn this work, we introduce POATE (Polar Opposite query generation, Adversarial\nTemplate construction, and Elaboration), a novel jailbreak technique that\nharnesses contrastive reasoning to provoke unethical responses. POATE crafts\nsemantically opposing intents and integrates them with adversarial templates,\nsteering models toward harmful outputs with remarkable subtlety. We conduct\nextensive evaluation across six diverse language model families of varying\nparameter sizes to demonstrate the robustness of the attack, achieving\nsignificantly higher attack success rates (~44%) compared to existing methods.\nTo counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which\ndecompose queries to detect malicious intent and reason in reverse to evaluate\nand reject harmful responses. These methods enhance reasoning robustness and\nstrengthen the model's defense against adversarial exploits."}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."}
{"id": "2501.11417", "pdf": "https://arxiv.org/pdf/2501.11417.pdf", "abs": "https://arxiv.org/abs/2501.11417", "title": "Neural Contextual Reinforcement Framework for Logical Structure Language Generation", "authors": ["Marcus Irvin", "William Cooper", "Edward Hughes", "Jessica Morgan", "Christopher Hamilton"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment."}
{"id": "2501.12901", "pdf": "https://arxiv.org/pdf/2501.12901.pdf", "abs": "https://arxiv.org/abs/2501.12901", "title": "Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration", "authors": ["Offa Kingsleigh", "Alfred Abercrombie", "David Woolstencroft", "Beorhtric Meadowcroft", "Marcus Irvin"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Contextual Partitioning introduces an innovative approach to enhancing the\narchitectural design of large-scale computational models through the dynamic\nsegmentation of parameters into context-aware regions. This methodology\nemphasizes the importance of task-specific specialization, achieved through\nadaptive parameter allocation mechanisms that align with the linguistic\nfeatures of input data. Experimental evaluations demonstrated substantial\nimprovements in accuracy, perplexity, and contextual coherence across a variety\nof linguistic tasks, highlighting the adaptability and scalability of the\nproposed framework. By reducing redundancy and enhancing computational\nefficiency, Contextual Partitioning not only streamlines model operations but\nalso expands the scope of applications for advanced language processing\nsystems. The approach operates autonomously, requiring no external fine-tuning,\nthereby addressing a significant limitation in conventional parameter\noptimization techniques. Empirical results demonstrate the effectiveness of\ngradient-driven segmentation, enabling models to dynamically recalibrate and\nspecialize in response to task-specific demands. Furthermore, resource\nutilization metrics reveal notable reductions in memory usage and training\ntimes, confirming the efficiency of the approach. Observations from qualitative\nanalyses illustrate improved contextual coherence and logical flow in generated\noutputs, reinforcing the practical value of this technique. The findings\ncollectively demonstrate the potential for Contextual Partitioning to redefine\nthe scalability and adaptability of computational language architectures in\ndiverse and complex domains."}
{"id": "2501.14119", "pdf": "https://arxiv.org/pdf/2501.14119.pdf", "abs": "https://arxiv.org/abs/2501.14119", "title": "Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation", "authors": ["Derek Yotheringhay", "Alistair Kirkland", "Humphrey Kirkbride", "Josiah Whitesteeple"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Transformative innovations in model architectures have introduced\nhierarchical embedding augmentation as a means to redefine the representation\nof tokens through multi-level semantic structures, offering enhanced\nadaptability to complex linguistic inputs. Autonomous structural memory\nmanipulation further advances this paradigm through dynamic memory reallocation\nmechanisms that prioritize critical contextual features while suppressing less\nrelevant information, enabling scalable and efficient performance across\ndiverse tasks. Experimental results reveal substantial improvements in\ncomputational efficiency, with marked reductions in processing overhead for\nlonger input sequences, achieved through memory reorganization strategies that\nadapt to evolving contextual requirements. Hierarchical embeddings not only\nimproved contextual alignment but also facilitated task generalization by\ncapturing relationships at varying semantic granularities, ensuring coherence\nacross layers without introducing significant computational redundancies.\nComparative analysis against baseline models demonstrated unique advantages in\naccuracy, efficiency, and interpretability, particularly in tasks requiring\ncomplex contextual understanding or domain-specific adaptability. The ability\nto dynamically adjust token representations and memory configurations\ncontributed to the model's robustness under varied and unpredictable input\nconditions. Applications benefiting from these advancements include\nmulti-domain generalization, interactive systems, and scenarios involving\nreal-time decision-making, where traditional static memory architectures often\nface limitations. The proposed methodology combines advanced embedding and\nmemory management strategies into a cohesive framework that addresses\nscalability challenges while preserving task-specific relevance."}
{"id": "2501.16658", "pdf": "https://arxiv.org/pdf/2501.16658.pdf", "abs": "https://arxiv.org/abs/2501.16658", "title": "Contextual Reinforcement in Multimodal Token Compression for Large Language Models", "authors": ["Naderdel Piero", "Zacharias Cromwell", "Nathaniel Wainwright", "Matthias Nethercott"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Effective token compression remains a critical challenge for scaling models\nto handle increasingly complex and diverse datasets. A novel mechanism based on\ncontextual reinforcement is introduced, dynamically adjusting token importance\nthrough interdependencies and semantic relevance. This approach enables\nsubstantial reductions in token usage while preserving the quality and\ncoherence of information representation. Incorporating graph-based algorithms\nand adaptive weighting, the method captures subtle contextual relationships\nacross textual and multimodal data, ensuring robust alignment and performance\nin downstream tasks. Evaluations across varied domains reveal significant\nimprovements in accuracy and semantic retention, particularly for tasks\nrequiring detailed cross-modal interactions. Memory usage analyses demonstrate\nimproved computational efficiency, with minimal overhead despite the additional\nreinforcement processes. Performance gains are further validated through error\ndistribution analyses, showing reduced semantic loss and syntactic\ninconsistencies compared to baseline models. The modular architecture ensures\ncompatibility with a wide range of open-source frameworks, facilitating\nscalable implementation for real-world applications. These findings highlight\nthe potential of contextual reinforcement in redefining token management\nstrategies and advancing large-scale model design."}
{"id": "2501.18826", "pdf": "https://arxiv.org/pdf/2501.18826.pdf", "abs": "https://arxiv.org/abs/2501.18826", "title": "Structural Embedding Projection for Contextual Large Language Model Inference", "authors": ["Vincent Enoasmo", "Cedric Featherstonehaugh", "Xavier Konstantinopoulos", "Zacharias Huntington"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens."}
{"id": "2502.00246", "pdf": "https://arxiv.org/pdf/2502.00246.pdf", "abs": "https://arxiv.org/abs/2502.00246", "title": "Context-Preserving Tensorial Reconfiguration in Large Language Model Training", "authors": ["Larin Tonix", "Morgana Baskerville", "Nathaniel Stourton", "Ophelia Tattershall"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Handling long-range dependencies in neural architectures has remained a\npersistent challenge due to computational limitations and inefficient\ncontextual retention mechanisms. Tensorial operations have provided a\nfoundation for restructuring model representations, yet conventional\narchitectures have struggled to incorporate such techniques without introducing\nexcessive complexity. A novel approach, Context-Preserving Tensorial\nReconfiguration (CPTR), enables dynamic reorganization of weight tensors\nthrough structured factorization and adaptive contraction, allowing for\nenhanced contextual integration without substantial computational overhead.\nEmpirical evaluations demonstrate that CPTR improves coherence retention across\nextended sequences, leading to measurable reductions in perplexity and improved\nrecall accuracy for long-context tasks. Performance comparisons reveal that\nCPTR-enhanced models exhibit greater computational efficiency and reduced\nmemory consumption while maintaining competitive language generation fluency\nand accuracy. Gradient stability metrics further validate the improved training\nefficiency, revealing more controlled variance in weight updates. Comparative\nstudies across baseline and CPTR-enhanced models confirm that tensorial\nreconfiguration contributes to more stable and computationally efficient\nlanguage modeling. The findings support the potential of CPTR in refining\ncontemporary neural architectures for tasks requiring long-range contextual\nunderstanding and efficient memory utilization."}
{"id": "2502.00301", "pdf": "https://arxiv.org/pdf/2502.00301.pdf", "abs": "https://arxiv.org/abs/2502.00301", "title": "Contextual Morphogenesis in Large Language Models: A Novel Approach to Self-Organizing Token Representations", "authors": ["Alistair Dombrowski", "Beatrix Engelhardt", "Dimitri Fairbrother", "Henry Evidail"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Token representations influence the efficiency and adaptability of language\nmodels, yet conventional tokenization strategies impose rigid segmentation\nboundaries that do not adjust dynamically to evolving contextual relationships.\nThe introduction of contextual morphogenesis establishes a self-organizing\nmechanism that restructures token boundaries based on learned contextual\ndependencies, allowing embeddings to evolve progressively across iterative\nprocessing steps. Empirical evaluations demonstrate that dynamically adjusted\ntokenization contributes to reductions in perplexity while maintaining\nrepresentational stability, particularly in linguistically complex domains\nwhere static segmentation fails to capture nuanced dependencies. Computational\ntrade-offs associated with self-organizing token structures indicate that\nadditional processing overhead remains within feasible limits, provided that\noptimization strategies account for segmentation update efficiency. Comparative\nassessments across different linguistic corpora suggest that adaptive\ntokenization preserves interpretability while improving alignment with\ncontextual cues, reinforcing the potential of morphogenetic segmentation\nmechanisms to refine predictive accuracy. Stability analyses confirm that\nevolving token structures maintain consistent segmentation behaviors across\nvaried text distributions, ensuring that representational adaptations remain\nlinguistically coherent. The effectiveness of contextual morphogenesis in\nrefining structural stability and predictive performance highlights its\nviability as an alternative to traditional tokenization methods. Further\nanalysis of computational efficiency considerations suggests that hybrid\nstrategies integrating both static and dynamic segmentation techniques may\noffer a balanced approach to optimizing representational flexibility while\nmaintaining inference efficiency."}
{"id": "2502.00977", "pdf": "https://arxiv.org/pdf/2502.00977.pdf", "abs": "https://arxiv.org/abs/2502.00977", "title": "Context-Aware Hierarchical Merging for Long Document Summarization", "authors": ["Litu Ou", "Mirella Lapata"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Hierarchical Merging is a technique commonly used to summarize very long\ntexts ($>$100K tokens) by breaking down the input into smaller sections,\nsummarizing those sections individually, and then merging or combining those\nsummaries into a final coherent summary. Although it helps address the\nlimitations of large language models (LLMs) with fixed input length\nconstraints, the recursive merging process can amplify LLM hallucinations,\nincreasing the risk of factual inaccuracies. In this paper, we seek to mitigate\nhallucinations by enriching hierarchical merging with context from the source\ndocument. Specifically, we propose different approaches to contextual\naugmentation ranging from \\emph{replacing} intermediate summaries with relevant\ninput context, to \\emph{refining} them while using the context as supporting\nevidence, and \\emph{aligning} them implicitly (via citations) to the input.\nExperimental results on datasets representing legal and narrative domains show\nthat contextual augmentation consistently outperforms zero-shot and\nhierarchical merging baselines for the Llama 3.1 model family. Our analysis\nfurther reveals that refinement methods tend to perform best when paired with\nextractive summarization for identifying relevant input."}
{"id": "2502.01979", "pdf": "https://arxiv.org/pdf/2502.01979.pdf", "abs": "https://arxiv.org/abs/2502.01979", "title": "Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis", "authors": ["Derek Yotheringhay", "Beatrix Nightingale", "Maximilian Featherstone", "Edmund Worthington", "Hugo Ashdown"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Generating structured textual content requires mechanisms that enforce\ncoherence, stability, and adherence to predefined constraints while maintaining\nsemantic fidelity. Conventional approaches often rely on rule-based heuristics\nor fine-tuning strategies that lack flexibility and generalizability across\ndiverse tasks. The incorporation of Gradient-Regularized Latent Space\nModulation (GRLSM) introduces a novel paradigm for guiding text generation\nthrough the application of structured constraints within the latent space. The\nintegration of gradient-based regularization mitigates abrupt variations in\nlatent representations, ensuring a smoother encoding process that enhances\nstructural consistency and logical progression within generated sequences.\nComparative evaluations demonstrate that latent space modulation leads to a\nreduction in perplexity, increased coherence scores, and improved structural\nalignment across multiple domains. Stability assessments further indicate that\nthe imposition of spectral norm constraints facilitates more controlled\nvariations in generated text, preserving semantic consistency under input\nperturbations. Empirical results confirm that structured latent space\nconstraints not only refine the organization of generated outputs but also\nenhance interpretability through more predictable and reliable synthesis\npatterns. Performance metrics illustrate that the GRLSM framework substantially\nreduces structural inconsistencies while preserving the generative flexibility\ninherent in neural models."}
{"id": "2502.05553", "pdf": "https://arxiv.org/pdf/2502.05553.pdf", "abs": "https://arxiv.org/abs/2502.05553", "title": "Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions", "authors": ["Stefan Whitaker", "Colin Sisate", "Marcel Windsor", "Nikolai Fairweather", "Tarquin Goldborough", "Oskar Lindenfeld"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Stochastic embedding transitions introduce a probabilistic mechanism for\nadjusting token representations dynamically during inference, mitigating the\nconstraints imposed through static or deterministic embeddings. A transition\nframework was proposed in which each token embedding evolved through\nprobabilistic updates, ensuring adaptability while preserving semantic\nintegrity across linguistic contexts. Empirical evaluations demonstrated that\nmodels incorporating stochastic transitions exhibited greater lexical\ndiversity, improved generative coherence, and enhanced retention of\nlow-frequency vocabulary, contributing to more varied sentence structures and\nreduced reliance on high-probability token selections. Statistical analyses of\nembedding drift across transformer layers indicated that representations\nevolved more flexibly without losing coherence, supporting the hypothesis that\ncontrolled stochasticity facilitated context-sensitive representation learning.\nExperimental results revealed that probabilistic embeddings introduced minor\ncomputational overhead while maintaining generative efficiency, reinforcing\ntheir feasibility in large-scale applications. A comparative study with\ntraditional embedding approaches highlighted measurable gains in text\ncompletion accuracy, dialogue coherence, and structural complexity, confirming\nthe effectiveness of stochastic transitions in enhancing representation\nexpressiveness. Clustering patterns in the embedding space suggested that\nprobabilistic updates preserved meaningful semantic groupings while enabling\ncontext-driven shifts, further validating the stability of the transition\nmechanism. Performance metrics indicated that stochastic transitions balanced\nadaptability and control, ensuring that generative outputs remained\nlinguistically coherent without excessive randomness."}
{"id": "2502.05794", "pdf": "https://arxiv.org/pdf/2502.05794.pdf", "abs": "https://arxiv.org/abs/2502.05794", "title": "Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration", "authors": ["Kathlyn Eaglewood", "Tobias Featherington", "Dorian Mayfair", "Sylvester Grimshaw", "James Pettigrew"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation."}
{"id": "2502.07124", "pdf": "https://arxiv.org/pdf/2502.07124.pdf", "abs": "https://arxiv.org/abs/2502.07124", "title": "Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation", "authors": ["Denis Bakushev", "Gideon Boultinghouse", "Harriet Oppenheimer", "Sebastian Gillingwater", "Valentina Ashington", "Wilfred Stanborough"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters."}
{"id": "2502.08947", "pdf": "https://arxiv.org/pdf/2502.08947.pdf", "abs": "https://arxiv.org/abs/2502.08947", "title": "Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding", "authors": ["Fenella Harcourt", "Naderdel Piero", "Gilbert Sutherland", "Daphne Holloway", "Harriet Bracknell", "Julian Ormsby"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency."}
{"id": "2502.09815", "pdf": "https://arxiv.org/pdf/2502.09815.pdf", "abs": "https://arxiv.org/abs/2502.09815", "title": "Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence", "authors": ["Jonathan Gale", "Godfrey Aldington", "Harriet Thistlewood", "Thomas Tattershall", "Basil Wentworth", "Vincent Enoasmo"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Representation learning plays a central role in structuring internal\nembeddings to capture the statistical properties of language, influencing the\ncoherence and contextual consistency of generated text. Statistical Coherence\nAlignment is introduced as a method to enforce structured token representations\nthrough tensor field convergence, guiding embeddings to reflect statistical\ndependencies inherent in linguistic data. A mathematical framework is\nestablished to quantify coherence alignment, integrating a loss function that\noptimizes representational consistency across training iterations. Empirical\nevaluations demonstrate that applying coherence constraints improves\nperplexity, enhances classification accuracy, and refines rare word embeddings,\ncontributing to a more stable representation space. Comparative analyses with\nbaseline models reveal that the proposed method fosters a more interpretable\ninternal structure, ensuring that embeddings retain contextual dependencies\nwhile mitigating representation collapse. The impact on coherence score\ndistributions suggests that the alignment mechanism strengthens semantic\nintegrity across diverse linguistic constructs, leading to a more balanced\norganization of learned embeddings. Computational assessments indicate that\nwhile the method introduces additional memory and training costs, the\nstructured optimization process justifies the trade-offs in applications\nrequiring heightened contextual fidelity. Experimental results validate the\neffectiveness of coherence alignment in optimizing token representations,\nproviding insights into how statistical dependencies can be leveraged to\nimprove language model training."}
{"id": "2502.10699", "pdf": "https://arxiv.org/pdf/2502.10699.pdf", "abs": "https://arxiv.org/abs/2502.10699", "title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration", "authors": ["George Applegarth", "Christian Weatherstone", "Maximilian Hollingsworth", "Henry Middlebrook", "Marcus Irvin"], "categories": ["cs.CL", "cs.AI", "cs.NE"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling."}
{"id": "2502.10942", "pdf": "https://arxiv.org/pdf/2502.10942.pdf", "abs": "https://arxiv.org/abs/2502.10942", "title": "Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks", "authors": ["Henry Evidail", "Zachary Mountebank", "Alistair Hathersage", "Peter Stanhope", "Basil Ravenscroft", "Tobias Waddingham"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Self-modulating mechanisms introduce dynamic adaptation capabilities within\nlanguage models through contextual realignment strategies that influence token\nembedding trajectories across extended sequences. Contextual Flux is explored\nas an approach to embedding modulation, integrating an auxiliary gating\nmechanism within the self-attention framework to dynamically adjust token\nrepresentations based on evolving contextual dependencies. The empirical\nanalysis evaluates entropy variations, latent space realignments, and coherence\nstability to assess the extent to which self-regulation enhances text\ngeneration consistency while preserving generative flexibility. Quantitative\nassessments suggest that embedding shifts contribute to more structured\nadaptation in long-form sequences, with measured reductions in redundant phrase\nrepetitions and improvements in thematic retention. Variability in contextual\nweight computation affects modulation stability, leading to differing levels of\nadaptation across diverse linguistic structures. The computational demands\nintroduced through real-time embedding reconfiguration are examined in relation\nto model scalability, emphasizing the need for optimization strategies in\nhigh-volume generative applications. The findings suggest that while adaptive\nembedding updates improve certain aspects of coherence, their impact remains\ncontingent on model capacity and input complexity."}
{"id": "2502.16802", "pdf": "https://arxiv.org/pdf/2502.16802.pdf", "abs": "https://arxiv.org/abs/2502.16802", "title": "Topic Over Source: The Key to Effective Data Mixing for Language Models Pre-training", "authors": ["Jiahui Peng", "Xinlin Zhuang", "Jiantao Qiu", "Ren Ma", "Jing Yu", "He Zhu", "Conghui He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research."}
{"id": "2503.01996", "pdf": "https://arxiv.org/pdf/2503.01996.pdf", "abs": "https://arxiv.org/abs/2503.01996", "title": "One ruler to measure them all: Benchmarking multilingual long-context language models", "authors": ["Yekyung Kim", "Jenna Russell", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": null, "summary": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines."}
{"id": "2504.01943", "pdf": "https://arxiv.org/pdf/2504.01943.pdf", "abs": "https://arxiv.org/abs/2504.01943", "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."}
{"id": "2504.03101", "pdf": "https://arxiv.org/pdf/2504.03101.pdf", "abs": "https://arxiv.org/abs/2504.03101", "title": "Single-Pass Document Scanning for Question Answering", "authors": ["Weili Cao", "Jianyou Wang", "Youze Zheng", "Longtian Bao", "Qirui Zheng", "Taylor Berg-Kirkpatrick", "Ramamohan Paturi", "Leon Bergen"], "categories": ["cs.CL"], "comment": "Published at Conference on Language Modeling (COLM), 2025", "summary": "Handling extremely large documents for question answering is challenging:\nchunk-based embedding methods often lose track of important global context,\nwhile full-context transformers can be prohibitively expensive for hundreds of\nthousands of tokens. We propose a single-pass document scanning approach that\nprocesses the entire text in linear time, preserving global coherence while\ndeciding which sentences are most relevant to the query. On 41 QA benchmarks,\nour single-pass scanner consistently outperforms chunk-based embedding methods\nand competes with large language models at a fraction of the computational\ncost. By conditioning on the entire preceding context without chunk breaks, the\nmethod preserves global coherence, which is especially important for long\ndocuments. Overall, single-pass document scanning offers a simple solution for\nquestion answering over massive text. All code, datasets, and model checkpoints\nare available at https://github.com/MambaRetriever/MambaRetriever"}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058.pdf", "abs": "https://arxiv.org/abs/2504.05058", "title": "Not All Data Are Unlearned Equally", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."}
{"id": "2504.07081", "pdf": "https://arxiv.org/pdf/2504.07081.pdf", "abs": "https://arxiv.org/abs/2504.07081", "title": "Self-Steering Language Models", "authors": ["Gabriel Grand", "Joshua B. Tenenbaum", "Vikash K. Mansinghka", "Alexander K. Lew", "Jacob Andreas"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025", "summary": "While test-time reasoning enables language models (LMs) to tackle complex\ntasks, searching or planning in natural language can be slow, costly, and\nerror-prone. But even when LMs struggle to emulate the precise reasoning steps\nneeded to solve a problem, they often excel at describing its abstract\nstructure--both how to verify solutions and how to search for them. This paper\nintroduces DisCIPL, a method for \"self-steering\" LMs where a Planner model\ngenerates a task-specific inference program that is executed by a population of\nFollower models. Our approach equips LMs with the ability to write recursive\nsearch procedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much\nlarger models, including GPT-4o and o1, on challenging constrained generation\ntasks. Our work opens up a design space of highly-parallelized Monte Carlo\ninference strategies that outperform standard best-of-N sampling, require no\nfinetuning, and can be implemented automatically by existing LMs."}
{"id": "2504.08775", "pdf": "https://arxiv.org/pdf/2504.08775.pdf", "abs": "https://arxiv.org/abs/2504.08775", "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures", "authors": ["Christopher Wolfram", "Aaron Schein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "How do the latent spaces used by independently-trained LLMs relate to one\nanother? We study the nearest neighbor relationships induced by activations at\ndifferent layers of 24 open-weight LLMs, and find that they 1) tend to vary\nfrom layer to layer within a model, and 2) are approximately shared between\ncorresponding layers of different models. Claim 2 shows that these nearest\nneighbor relationships are not arbitrary, as they are shared across models, but\nClaim 1 shows that they are not \"obvious\" either, as there is no single set of\nnearest neighbor relationships that is universally shared. Together, these\nsuggest that LLMs generate a progression of activation geometries from layer to\nlayer, but that this entire progression is largely shared between models,\nstretched and squeezed to fit into different architectures."}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736.pdf", "abs": "https://arxiv.org/abs/2504.18736", "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "categories": ["cs.CL"], "comment": "Published at Conference on Language Modeling (COLM) 2025", "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench"}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811.pdf", "abs": "https://arxiv.org/abs/2504.19811", "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships-i.e., which models are derived or\nmerged from which parents. In this work, we propose a novel Lineage-Regularized\nMatrix Factorization (LRMF) framework that encodes ancestral ties among LLMs\nvia a graph Laplacian regularizer. By leveraging multi-hop parent-child\nconnections, LRMF consistently outperforms conventional matrix factorization\nand collaborative filtering methods in both instance-level and benchmark-level\nperformance prediction. Our large-scale study includes 2,934 publicly available\nHugging Face models and 21,000+ instances across 6 major benchmarks, showing\nthat the introduction of lineage constraints yields up to 0.15-0.30 higher\nPearson correlation coefficients with actual performance compared to baseline\nmethods. Moreover, LRMF effectively addresses the cold-start problem, providing\naccurate estimates for newly derived or merged models even with minimal data.\nThis lineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."}
{"id": "2505.07258", "pdf": "https://arxiv.org/pdf/2505.07258.pdf", "abs": "https://arxiv.org/abs/2505.07258", "title": "No Query, No Access", "authors": ["Wenqiang Wang", "Siyuan Liang", "Yangshijie Zhang", "Xiaojun Jia", "Hao Lin", "Xiaochun Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual adversarial attacks mislead NLP models, including Large Language\nModels (LLMs), by subtly modifying text. While effective, existing attacks\noften require knowledge of the victim model, extensive queries, or access to\ntraining data, limiting real-world feasibility. To overcome these constraints,\nwe introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which\noperates using only victim texts. To prevent access to the victim model, we\ncreate a shadow dataset with publicly available pre-trained models and\nclustering methods as a foundation for developing substitute models. To address\nthe low attack success rate (ASR) due to insufficient information feedback, we\npropose the hierarchical substitution model design, generating substitute\nmodels to mitigate the failure of a single substitute model at the decision\nboundary.\n  Concurrently, we use diverse adversarial example generation, employing\nvarious attack methods to generate and select the adversarial example with\nbetter similarity and attack effectiveness. Experiments on the Emotion and SST5\ndatasets show that VDBA outperforms state-of-the-art methods, achieving an ASR\nimprovement of 52.08\\% while significantly reducing attack queries to 0. More\nimportantly, we discover that VDBA poses a significant threat to LLMs such as\nQwen2 and the GPT family, and achieves the highest ASR of 45.99% even without\naccess to the API, confirming that advanced NLP models still face serious\nsecurity risks. Our codes can be found at\nhttps://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/"}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507.pdf", "abs": "https://arxiv.org/abs/2505.10507", "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "authors": ["Benedikt Ebing", "Goran Glava"], "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks."}
{"id": "2505.14848", "pdf": "https://arxiv.org/pdf/2505.14848.pdf", "abs": "https://arxiv.org/abs/2505.14848", "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation", "authors": ["George Wang", "Jiaqian Hu", "Safinah Ali"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity."}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518.pdf", "abs": "https://arxiv.org/abs/2505.16518", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagstrm", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "28 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) have recently been proposed to alleviate these\nissues, few have seen systematic comparison. In this paper, we develop CUB\n(Context Utilisation Benchmark) - the first comprehensive benchmark designed to\nhelp practitioners within retrieval-augmented generation (RAG) diagnose CMTs\nunder different context conditions. With this benchmark, we conduct the most\nextensive evaluation to date of seven state-of-the-art methods, representative\nof the main categories of CMTs, across three diverse datasets and tasks,\napplied to nine LMs. Our results reveal that most existing CMTs struggle to\nhandle the full spectrum of context types encountered in real-world\nretrieval-augmented scenarios. We also find that many CMTs display inflated\nperformance on simple synthesised datasets, compared to more realistic datasets\nwith naturally occurring samples. Our findings expose critical gaps in current\nCMT evaluation practices and demonstrate the need for holistic testing and the\ndevelopment of CMTs that can robustly handle multiple context types."}
{"id": "2505.20910", "pdf": "https://arxiv.org/pdf/2505.20910.pdf", "abs": "https://arxiv.org/abs/2505.20910", "title": "Automated Privacy Information Annotation in Large Language Model Interactions", "authors": ["Hang Zeng", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Shaojie Tang", "Guihai Chen"], "categories": ["cs.CL"], "comment": "8 content pages", "summary": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application domains, typically tagging personally identifiable\ninformation (PII) in anonymous content, which is insufficient in real-name\ninteraction scenarios with LLMs. In this work, to support the development and\nevaluation of privacy detection models for LLM interactions that are deployable\non local user devices, we construct a large-scale multilingual dataset with\n249K user queries and 154K annotated privacy phrases. In particular, we build\nan automated privacy annotation pipeline with strong LLMs to automatically\nextract privacy phrases from dialogue datasets and annotate leaked information.\nWe also design evaluation metrics at the levels of privacy leakage, extracted\nprivacy phrase, and privacy information. We further establish baseline methods\nusing light-weight LLMs with both tuning-free and tuning-based methods, and\nreport a comprehensive evaluation of their performance. Evaluation results\nreveal a gap between current performance and the requirements of real-world LLM\napplications, motivating future research into more effective local privacy\ndetection methods grounded in our dataset."}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349.pdf", "abs": "https://arxiv.org/abs/2506.09349", "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Whereas current methods\nutilize mainly 12.5Hz input audio representation, our proposed dual-resolution\nmechanism reduces the input frequency for the LLM to 5Hz. Experimental results\non Spoken Question Answering benchmarks demonstrate that D RVOICE establishes\nnew state-of-the-art (SOTA) performance among similar size speech foundation\nmodels with relative small amount of data."}
{"id": "2506.11246", "pdf": "https://arxiv.org/pdf/2506.11246.pdf", "abs": "https://arxiv.org/abs/2506.11246", "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning", "authors": ["Abhishek Rajgaria", "Kushagra Dixit", "Mayank Vyas", "Harshavardhan Kalalbandi", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 21 Tables, 10 Figures", "summary": "Temporal Table Reasoning is a critical challenge for Large Language Models\n(LLMs), requiring effective reasoning to extract relevant insights. Despite\nexistence of multiple prompting methods, their impact on table reasoning\nremains largely unexplored. Furthermore, model performance varies drastically\nacross different table and context structures, making it difficult to determine\nan optimal approach. This work investigates multiple prompting technique on\ndiverse table types to determine that performance depends on factors such as\nentity type, table structure, requirement of additional context and question\ncomplexity, with \"NO\" single method consistently outperforming others. To\naddress this, we introduce SEAR, an adaptive prompting framework inspired by\nhuman reasoning that dynamically adjusts to context and integrates structured\nreasoning. Our results demonstrate that SEAR achieves superior performance\nacross all table types compared to baseline prompting techniques. Additionally,\nwe explore the impact of table structure refactoring, finding that a unified\nrepresentation enhances model reasoning."}
{"id": "2506.13380", "pdf": "https://arxiv.org/pdf/2506.13380.pdf", "abs": "https://arxiv.org/abs/2506.13380", "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models", "authors": ["Valentin Six", "Evan Dufraisse", "Gal de Chalendar"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls."}
{"id": "2506.19028", "pdf": "https://arxiv.org/pdf/2506.19028.pdf", "abs": "https://arxiv.org/abs/2506.19028", "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective", "authors": ["Weijie Xu", "Yiwen Wang", "Chi Xue", "Xiangkun Hu", "Xi Fang", "Guimin Dong", "Chandan K. Reddy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "29 pages, 9 figures, 15 tables", "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics."}
{"id": "2506.20160", "pdf": "https://arxiv.org/pdf/2506.20160.pdf", "abs": "https://arxiv.org/abs/2506.20160", "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths."}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306.pdf", "abs": "https://arxiv.org/abs/2507.06306", "title": "Humans overrely on overconfident language models, across languages", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "camera ready", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Prior work shows that LLMs are linguistically\noverconfident in English, leading users to overrely on confident generations.\nHowever, the usage and interpretation of epistemic markers (e.g., 'I think\nit's') differs sharply across languages. Here, we study the risks of\nmultilingual linguistic (mis)calibration, overconfidence, and overreliance\nacross five languages to evaluate LLM safety in a global context. Our work\nfinds that overreliance risks are high across languages. We first analyze the\ndistribution of LLM-generated epistemic markers and observe that LLMs are\noverconfident across languages, frequently generating strengtheners even as\npart of incorrect responses. Model generations are, however, sensitive to\ndocumented cross-linguistic variation in usage: for example, models generate\nthe most markers of uncertainty in Japanese and the most markers of certainty\nin German and Mandarin. Next, we measure human reliance rates across languages,\nfinding that reliance behaviors differ cross-linguistically: for example,\nparticipants are significantly more likely to discount expressions of\nuncertainty in Japanese than in English (i.e., ignore their 'hedging' function\nand rely on generations that contain them). Taken together, these results\nindicate a high risk of reliance on overconfident model generations across\nlanguages. Our findings highlight the challenges of multilingual linguistic\ncalibration and stress the importance of culturally and linguistically\ncontextualized model safety evaluations."}
{"id": "2507.17747", "pdf": "https://arxiv.org/pdf/2507.17747.pdf", "abs": "https://arxiv.org/abs/2507.17747", "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "authors": ["Linbo Cao", "Jinman Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models."}
{"id": "2507.20091", "pdf": "https://arxiv.org/pdf/2507.20091.pdf", "abs": "https://arxiv.org/abs/2507.20091", "title": "ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models", "authors": ["Kaizhi Qian", "Xulin Fan", "Junrui Ni", "Slava Shechtman", "Mark Hasegawa-Johnson", "Chuang Gan", "Yang Zhang"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Speech language models refer to language models with speech processing and\nunderstanding capabilities. One key desirable capability for speech language\nmodels is the ability to capture the intricate interdependency between content\nand prosody. The existing mainstream paradigm of training speech language\nmodels, which converts speech into discrete tokens before feeding them into\nLLMs, is sub-optimal in learning prosody information -- we find that the\nresulting LLMs do not exhibit obvious emerging prosody processing capabilities\nvia pre-training alone. To overcome this, we propose ProsodyLM, which\nintroduces a simple tokenization scheme amenable to learning prosody. Each\nspeech utterance is first transcribed into text, followed by a sequence of\nword-level prosody tokens. Compared with conventional speech tokenization\nschemes, the proposed tokenization scheme retains more complete prosody\ninformation, and is more understandable to text-based LLMs. We find that\nProsodyLM can learn surprisingly diverse emerging prosody processing\ncapabilities through pre-training alone, ranging from harnessing the prosody\nnuances in generated speech, such as contrastive focus, understanding emotion\nand stress in an utterance, to maintaining prosody consistency in long\ncontexts."}
{"id": "2508.00544", "pdf": "https://arxiv.org/pdf/2508.00544.pdf", "abs": "https://arxiv.org/abs/2508.00544", "title": "PaPaformer: Language Model from Pre-trained Parallel Paths", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements."}
{"id": "2508.03923", "pdf": "https://arxiv.org/pdf/2508.03923.pdf", "abs": "https://arxiv.org/abs/2508.03923", "title": "CoAct-1: Computer-using Agents with Coding as Actions", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation."}
{"id": "2508.05028", "pdf": "https://arxiv.org/pdf/2508.05028.pdf", "abs": "https://arxiv.org/abs/2508.05028", "title": "Evaluation of LLMs in AMR Parsing", "authors": ["Shu Han Ho"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 32 figures", "summary": "AMR (Abstract Meaning Representation) is a semantic formalism that encodes\nsentence meaning as rooted, directed, acyclic graphs, where nodes represent\nconcepts and edges denote semantic relations. Finetuning decoder only Large\nLanguage Models (LLMs) represent a promising novel straightfoward direction for\nAMR parsing. This paper presents a comprehensive evaluation of finetuning four\ndistinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA\nDistilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown\nthat straightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity."}
{"id": "2508.05429", "pdf": "https://arxiv.org/pdf/2508.05429.pdf", "abs": "https://arxiv.org/abs/2508.05429", "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints", "authors": ["Zhong Ken Hew", "Jia Xin Low", "Sze Jue Yang", "Chee Seng Chan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs."}
{"id": "2308.04941", "pdf": "https://arxiv.org/pdf/2308.04941.pdf", "abs": "https://arxiv.org/abs/2308.04941", "title": "Integrating large language models and active inference to understand eye movements in reading and dyslexia", "authors": ["Francesco Donnarumma", "Mirco Frosolone", "Giovanni Pezzulo"], "categories": ["q-bio.NC", "cs.CL"], "comment": "Main Document - 30 pages, 1 Table, 10 Figures + Supplementary 16\n  pages, 17 Tables", "summary": "We present a novel computational model employing hierarchical active\ninference to simulate reading and eye movements. The model characterizes\nlinguistic processing as inference over a hierarchical generative model,\nfacilitating predictions and inferences at various levels of granularity, from\nsyllables to sentences. Our approach combines the strengths of large language\nmodels for realistic textual predictions and active inference for guiding eye\nmovements to informative textual information, enabling the testing of\npredictions. The model exhibits proficiency in reading both known and unknown\nwords and sentences, adhering to the distinction between lexical and nonlexical\nroutes in dual route theories of reading. Our model therefore provides a novel\napproach to understand the cognitive processes underlying reading and eye\nmovements, within a predictive processing framework. Furthermore, our model can\npotentially aid in understanding how maladaptive predictive processing can\nproduce reading deficits associated with dyslexia. As a proof of concept, we\nshow that attenuating the contribution of priors during the reading process\nleads to incorrect inferences and a more fragmented reading style,\ncharacterized by a greater number of shorter saccades, aligning with empirical\nfindings regarding eye movements in dyslexic individuals. In summary, our model\nrepresents a significant advancement in comprehending the cognitive processes\ninvolved in reading and eye movements, with potential implications for\nunderstanding dyslexia in terms of maladaptive inference."}
{"id": "2406.09105", "pdf": "https://arxiv.org/pdf/2406.09105.pdf", "abs": "https://arxiv.org/abs/2406.09105", "title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance", "authors": ["Chenwei Lin", "Hanjia Lyu", "Xian Xu", "Jiebo Luo"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "To appear in the International Conference on Computer Vision, ICCV\n  2025", "summary": "Large Vision-Language Models (LVLMs) and Multimodal Large Language Models\n(MLLMs) have demonstrated outstanding performance in various general multimodal\napplications and have shown increasing promise in specialized domains. However,\ntheir potential in the insurance domain-characterized by diverse application\nscenarios and rich multimodal data-remains largely underexplored. To date,\nthere is no systematic review of multimodal tasks, nor a benchmark specifically\ndesigned to assess the capabilities of LVLMs in insurance. This gap hinders the\ndevelopment of LVLMs within the insurance industry. This study systematically\nreviews and categorizes multimodal tasks for 4 representative types of\ninsurance: auto, property, health, and agricultural. We introduce INS-MMBench,\nthe first hierarchical benchmark tailored for the insurance domain. INS-MMBench\nencompasses 22 fundamental tasks, 12 meta-tasks and 5 scenario tasks, enabling\na comprehensive and progressive assessment from basic capabilities to\nreal-world use cases. We benchmark 11 leading LVLMs, including closed-source\nmodels such as GPT-4o and open-source models like LLaVA. Our evaluation\nvalidates the effectiveness of INS-MMBench and offers detailed insights into\nthe strengths and limitations of current LVLMs on a variety of\ninsurance-related multimodal tasks. We hope that INS-MMBench will accelerate\nthe integration of LVLMs into the insurance industry and foster\ninterdisciplinary research. Our dataset and evaluation code are available at\nhttps://github.com/FDU-INS/INS-MMBench."}
{"id": "2407.00900", "pdf": "https://arxiv.org/pdf/2407.00900.pdf", "abs": "https://arxiv.org/abs/2407.00900", "title": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models", "authors": ["Shubhra Mishra", "Gabriel Poesia", "Noah D. Goodman"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to COLM 2025. Dataset and code:\n  https://github.com/gpoesia/mathcamps/", "summary": "Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning."}
{"id": "2407.02596", "pdf": "https://arxiv.org/pdf/2407.02596.pdf", "abs": "https://arxiv.org/abs/2407.02596", "title": "Towards More Realistic Extraction Attacks: An Adversarial Perspective", "authors": ["Yash More", "Prakhar Ganesh", "Golnoosh Farnadi"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "To appear in TACL", "summary": "Language models are prone to memorizing their training data, making them\nvulnerable to extraction attacks. While existing research often examines\nisolated setups, such as a single model or a fixed prompt, real-world\nadversaries have a considerably larger attack surface due to access to models\nacross various sizes and checkpoints, and repeated prompting. In this paper, we\nrevisit extraction attacks from an adversarial perspective -- with\nmulti-faceted access to the underlying data. We find significant churn in\nextraction trends, i.e., even unintuitive changes to the prompt, or targeting\nsmaller models and earlier checkpoints, can extract distinct information. By\ncombining multiple attacks, our adversary doubles ($2 \\times$) the extraction\nrisks, persisting even under mitigation strategies like data deduplication. We\nconclude with four case studies, including detecting pre-training data,\ncopyright violations, extracting personally identifiable information, and\nattacking closed-source models, showing how our more realistic adversary can\noutperform existing adversaries in the literature."}
{"id": "2411.14553", "pdf": "https://arxiv.org/pdf/2411.14553.pdf", "abs": "https://arxiv.org/abs/2411.14553", "title": "Reducibility among NP-Hard graph problems and boundary classes", "authors": ["Syed Mujtaba Hassan", "Shahid Hussain", "Abdul Samad"], "categories": ["cs.CC", "cs.CL", "cs.DM"], "comment": "9 pages, 6 figures", "summary": "Many NP-hard graph problems become easy for some classes of graphs. For\nexample, coloring is easy for bipartite graphs, but NP-hard in general. So we\ncan ask question like when does a hard problem become easy? What is the minimum\nsubstructure for which the problem remains hard? We use the notion of boundary\nclasses to study such questions. In this paper, we introduce a method for\ntransforming the boundary class of one NP-hard graph problem into a boundary\nclass for another problem. If {\\Pi} and {\\Gamma} are two NP-hard graph problems\nwhere {\\Pi} is reducible to {\\Gamma}, we transform a boundary class of {\\Pi}\ninto a boundary class of {\\Gamma}. More formally if {\\Pi} is reducible to\n{\\Gamma}, where the reduction satisfies certain conditions, then X is a\nboundary class of {\\Pi} if and only if the image of X under the reduction is a\nboundary class of {\\Gamma}. This gives us a relationship between boundary\nclasses and reducibility among several NP-hard problems. To show the strength\nof our main result, we apply our theorem to obtain some previously unknown\nboundary classes for a few graph problems namely; vertex-cover, clique,\ntraveling-salesperson, bounded-degree-spanning-tree, subgraph-isomorphism and\nclique-cover."}
{"id": "2412.13147", "pdf": "https://arxiv.org/pdf/2412.13147.pdf", "abs": "https://arxiv.org/abs/2412.13147", "title": "Are Your LLMs Capable of Stable Reasoning?", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Ziyi Wang", "Kuikun Liu", "Songyang Gao", "Wenwei Zhang", "Songyang Zhang", "Kai Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025 Camera, Benchmark:\n  https://huggingface.co/datasets/opencompass/LiveMathBench, Code:\n  https://github.com/open-compass/GPassK", "summary": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics."}
{"id": "2502.00048", "pdf": "https://arxiv.org/pdf/2502.00048.pdf", "abs": "https://arxiv.org/abs/2502.00048", "title": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension", "authors": ["Colin Sisate", "Alistair Goldfinch", "Vincent Waterstone", "Sebastian Kingsley", "Mariana Blackthorn"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies."}
{"id": "2502.11881", "pdf": "https://arxiv.org/pdf/2502.11881.pdf", "abs": "https://arxiv.org/abs/2502.11881", "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models", "authors": ["Hyunwoo Kim", "Melanie Sclar", "Tan Zhi-Xuan", "Lance Ying", "Sydney Levine", "Yang Liu", "Joshua B. Tenenbaum", "Yejin Choi"], "categories": ["cs.AI", "cs.CL"], "comment": "COLM 2025. For code and data, see https://hyunw.kim/thought-tracing", "summary": "Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o3 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains."}
{"id": "2502.18418", "pdf": "https://arxiv.org/pdf/2502.18418.pdf", "abs": "https://arxiv.org/abs/2502.18418", "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval", "authors": ["Orion Weller", "Kathryn Ricci", "Eugene Yang", "Andrew Yates", "Dawn Lawrie", "Benjamin Van Durme"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Published at CoLM 2025", "summary": "We introduce Rank1, the first reranking model trained to take advantage of\ntest-time compute. Rank1 demonstrates the applicability within retrieval of\nusing a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for\ndistillation in order to rapidly improve the performance of a smaller model. We\ngather and open-source a dataset of more than 600,000 examples of R1 reasoning\ntraces from queries and passages in MS MARCO. Models trained on this dataset\nshow: (1) state-of-the-art performance on advanced reasoning and instruction\nfollowing datasets; (2) work remarkably well out of distribution due to the\nability to respond to user-input prompts; and (3) have explainable reasoning\nchains that can be given to users or RAG-based systems. Further, we demonstrate\nthat quantized versions of these models retain strong performance while using\nless compute/memory. Overall, Rank1 shows that test-time compute allows for a\nfundamentally new type of explainable and performant reranker model for search."}
{"id": "2503.01700", "pdf": "https://arxiv.org/pdf/2503.01700.pdf", "abs": "https://arxiv.org/abs/2503.01700", "title": "Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation", "authors": ["Yongchao Chen", "Yilun Hao", "Yang Zhang", "Chuchu Fan"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "7 pages, 7 figures, 3 tables", "summary": "Recent works have shown great potentials of Large Language Models (LLMs) in\nrobot task and motion planning (TAMP). Current LLM approaches generate text- or\ncode-based reasoning chains with sub-goals and action plans. However, they do\nnot fully leverage LLMs' symbolic computing and code generation capabilities.\nMany robot TAMP tasks involve complex optimization under multiple constraints,\nwhere pure textual reasoning is insufficient. While augmenting LLMs with\npredefined solvers and planners improves performance, it lacks generalization\nacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMP\ncapabilities by steering them to generate code as symbolic planners for\noptimization and constraint verification. Unlike prior work that uses code to\ninterface with robot action modules, we steer LLMs to generate code as solvers,\nplanners, and checkers for TAMP tasks requiring symbolic computing, while still\nleveraging textual reasoning to incorporate common sense. With a multi-round\nguidance and answer evolution framework, the proposed Code-as-Symbolic-Planner\nimproves success rates by average 24.1\\% over best baseline methods across\nseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows\nstrong effectiveness and generalizability across discrete and continuous\nenvironments, 2D/3D simulations and real-world settings, as well as single- and\nmulti-robot tasks with diverse requirements. See our project website\nhttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and\ncode."}
{"id": "2503.23145", "pdf": "https://arxiv.org/pdf/2503.23145.pdf", "abs": "https://arxiv.org/abs/2503.23145", "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning. Our code, data, and models are\npublicly available at https://github.com/Anjiang-Wei/CodeARC"}
{"id": "2504.04030", "pdf": "https://arxiv.org/pdf/2504.04030.pdf", "abs": "https://arxiv.org/abs/2504.04030", "title": "OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs", "authors": ["Wasi Uddin Ahmad", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Vahid Noroozi", "Somshubra Majumdar", "Boris Ginsburg"], "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have transformed software development by\nenabling code generation, automated debugging, and complex reasoning. However,\ntheir continued advancement is constrained by the scarcity of high-quality,\npublicly available supervised fine-tuning (SFT) datasets tailored for coding\ntasks. To bridge this gap, we introduce OpenCodeInstruct, the largest\nopen-access instruction tuning dataset, comprising 5 million diverse samples.\nEach sample includes a programming question, solution, test cases, execution\nfeedback, and LLM-generated quality assessments. We fine-tune various base\nmodels, including LLaMA and Qwen, across multiple scales (1B+, 3B+, and 7B+)\nusing our dataset. Comprehensive evaluations on popular benchmarks (HumanEval,\nMBPP, LiveCodeBench, and BigCodeBench) demonstrate substantial performance\nimprovements achieved by SFT with OpenCodeInstruct. We also present a detailed\nmethodology encompassing seed data curation, synthetic instruction and solution\ngeneration, and filtering."}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254.pdf", "abs": "https://arxiv.org/abs/2504.15254", "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "To be published at COLM, 2025", "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."}
{"id": "2505.09614", "pdf": "https://arxiv.org/pdf/2505.09614.pdf", "abs": "https://arxiv.org/abs/2505.09614", "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "categories": ["cs.AI", "cs.CL"], "comment": "COLM 2025 Camera Ready", "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers which need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established Blicket Test paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not child-like). Finally, we propose\na test-time sampling method which explicitly samples and eliminates hypotheses\nabout causal relationships from the LM. This scalable approach significantly\nreduces the disjunctive bias and moves LMs closer to the goal of scientific,\ncausally rigorous reasoning."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks."}
{"id": "2507.06090", "pdf": "https://arxiv.org/pdf/2507.06090.pdf", "abs": "https://arxiv.org/abs/2507.06090", "title": "Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India", "authors": ["Swapnil Bhattacharyya", "Harshvivek Kashid", "Shrey Ganatra", "Spandan Anaokar", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "AI-based judicial assistance and case prediction have been extensively\nstudied in criminal and civil domains, but remain largely unexplored in\nconsumer law, especially in India. In this paper, we present Nyay-Darpan, a\nnovel two-in-one framework that (i) summarizes consumer case files and (ii)\nretrieves similar case judgements to aid decision-making in consumer dispute\nresolution. Our methodology not only addresses the gap in consumer law AI tools\nbut also introduces an innovative approach to evaluate the quality of the\nsummary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',\nsymbolizing the ability of our tool to reflect the core of consumer disputes\nthrough precise summarization and intelligent case retrieval. Our system\nachieves over 75 percent accuracy in similar case prediction and approximately\n70 percent accuracy across material summary evaluation metrics, demonstrating\nits practical effectiveness. We will publicly release the Nyay-Darpan framework\nand dataset to promote reproducibility and facilitate further research in this\nunderexplored yet impactful domain."}
{"id": "2508.02622", "pdf": "https://arxiv.org/pdf/2508.02622.pdf", "abs": "https://arxiv.org/abs/2508.02622", "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction", "authors": ["Enrico De Santis", "Antonello Rizzi"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "This version has been extensively revised and revisited in light of\n  feedback and further research. Several sections have been expanded or\n  improved for greater clarity and completeness. Specifically, new\n  clarification on complex system foundation related to Noosemia has been added\n  (Secs. \"2.4 and \"2.5\")", "summary": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research."}
{"id": "2508.05118", "pdf": "https://arxiv.org/pdf/2508.05118.pdf", "abs": "https://arxiv.org/abs/2508.05118", "title": "Exploring Superior Function Calls via Reinforcement Learning", "authors": ["Bingguang Hao", "Maolin Wang", "Zengzhuang Xu", "Yicheng Chen", "Cunyin Peng", "Jinjie GU", "Chenyi Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community."}
{"id": "2508.05464", "pdf": "https://arxiv.org/pdf/2508.05464.pdf", "abs": "https://arxiv.org/abs/2508.05464", "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?", "authors": ["Matteo Prandi", "Vincenzo Suriani", "Federico Pierucci", "Marcello Galisai", "Daniele Nardi", "Piercosma Bisconti"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools."}
