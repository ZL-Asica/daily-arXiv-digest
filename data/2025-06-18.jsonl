{"id": "2506.13882", "pdf": "https://arxiv.org/pdf/2506.13882.pdf", "abs": "https://arxiv.org/abs/2506.13882", "title": "Toward Practical Privacy in XR: Empirical Analysis of Multimodal Anonymization Mechanisms", "authors": ["Azim Ibragimov", "Ethan Wilson", "Kevin R. B. Butler", "Eakta Jain"], "categories": ["cs.HC"], "comment": null, "summary": "As extended reality (XR) systems become increasingly immersive and\nsensor-rich, they enable the collection of fine-grained behavioral signals such\nas eye and body telemetry. These signals support personalized and responsive\nexperiences and may also contain unique patterns that can be linked back to\nindividuals. However, privacy mechanisms that naively pair unimodal mechanisms\n(e.g., independently apply privacy mechanisms for eye and body privatization)\nare often ineffective at preventing re-identification in practice. In this\nwork, we systematically evaluate real-time privacy mechanisms for XR, both\nindividually and in pair, across eye and body modalities. To preserve\nusability, all mechanisms were tuned based on empirically grounded thresholds\nfor real-time interaction. We evaluated four eye and ten body mechanisms across\nmultiple datasets, comprising up to 407 participants. Our results show that\nwhile obfuscating eye telemetry alone offers moderate privacy gains, body\ntelemetry perturbation is substantially more effective. When carefully paired,\nmultimodal mechanisms reduce re-identification rate from 80.3% to 26.3% in\ncasual XR applications (e.g., VRChat and Job Simulator) and from 84.8% to 26.1%\nin competitive XR applications (e.g., Beat Saber and Synth Riders), all without\nviolating real-time usability requirements. These findings underscore the\npotential of modality-specific and context-aware privacy strategies for\nprotecting behavioral data in XR environments."}
{"id": "2506.13904", "pdf": "https://arxiv.org/pdf/2506.13904.pdf", "abs": "https://arxiv.org/abs/2506.13904", "title": "A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare", "authors": ["Ivania Donoso-Guzmán", "Kristýna Sirka Kacafírková", "Maxwell Szymanski", "An Jacobs", "Denis Parra", "Katrien Verbert"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite promising developments in Explainable Artificial Intelligence, the\npractical value of XAI methods remains under-explored and insufficiently\nvalidated in real-world settings. Robust and context-aware evaluation is\nessential, not only to produce understandable explanations but also to ensure\ntheir trustworthiness and usability for intended users, but tends to be\noverlooked because of no clear guidelines on how to design an evaluation with\nusers.\n  This study addresses this gap with two main goals: (1) to develop a framework\nof well-defined, atomic properties that characterise the user experience of XAI\nin healthcare; and (2) to provide clear, context-sensitive guidelines for\ndefining evaluation strategies based on system characteristics.\n  We conducted a systematic review of 82 user studies, sourced from five\ndatabases, all situated within healthcare settings and focused on evaluating\nAI-generated explanations. The analysis was guided by a predefined coding\nscheme informed by an existing evaluation framework, complemented by inductive\ncodes developed iteratively.\n  The review yields three key contributions: (1) a synthesis of current\nevaluation practices, highlighting a growing focus on human-centred approaches\nin healthcare XAI; (2) insights into the interrelations among explanation\nproperties; and (3) an updated framework and a set of actionable guidelines to\nsupport interdisciplinary teams in designing and implementing effective\nevaluation strategies for XAI systems tailored to specific application\ncontexts."}
{"id": "2506.14018", "pdf": "https://arxiv.org/pdf/2506.14018.pdf", "abs": "https://arxiv.org/abs/2506.14018", "title": "\"I Cannot Write This Because It Violates Our Content Policy\": Understanding Content Moderation Policies and User Experiences in Generative AI Products", "authors": ["Lan Gao", "Oscar Chen", "Rachel Lee", "Nick Feamster", "Chenhao Tan", "Marshini Chetty"], "categories": ["cs.HC"], "comment": "Preprint for USENIX Security 2025", "summary": "While recent research has focused on developing safeguards for generative AI\n(GAI) model-level content safety, little is known about how content moderation\nto prevent malicious content performs for end-users in real-world GAI products.\nTo bridge this gap, we investigated content moderation policies and their\nenforcement in GAI online tools -- consumer-facing web-based GAI applications.\nWe first analyzed content moderation policies of 14 GAI online tools. While\nthese policies are comprehensive in outlining moderation practices, they\nusually lack details on practical implementations and are not specific about\nhow users can aid in moderation or appeal moderation decisions. Next, we\nexamined user-experienced content moderation successes and failures through\nReddit discussions on GAI online tools. We found that although moderation\nsystems succeeded in blocking malicious generations pervasively, users\nfrequently experienced frustration in failures of both moderation systems and\nuser support after moderation. Based on these findings, we suggest improvements\nfor content moderation policy and user experiences in real-world GAI products."}
{"id": "2506.14056", "pdf": "https://arxiv.org/pdf/2506.14056.pdf", "abs": "https://arxiv.org/abs/2506.14056", "title": "FEWSim: A Visual Analytic Framework for Exploring the Nexus of Food-Energy-Water Simulations", "authors": ["Fan Lei", "David A. Sampson", "Jiayi Hong", "Yuxin Ma", "Giuseppe Mascaro", "Dave White", "Rimjhim Agarwal", "Ross Maciejewski"], "categories": ["cs.HC"], "comment": "Accepted by IEEE Computer Graphics and Applications (CG&A)", "summary": "The interdependencies of food, energy, and water (FEW) systems create a nexus\nopportunity to explore the strengths and vulnerabilities of individual and\ncross-sector interactions within FEW systems. However, the variables\nquantifying nexus interactions are hard to observe, which hinders the\ncross-sector analysis. To overcome such challenges, we present FEWSim, a visual\nanalytics framework designed to support domain experts in exploring and\ninterpreting simulation results from a coupled FEW model. FEWSim employs a\nthree-layer asynchronous architecture: the model layer integrates food, energy,\nand water models to simulate the FEW nexus; the middleware layer manages\nscenario configuration and execution; and the visualization layer provides\ninteractive visual exploration of simulated time-series results across FEW\nsectors. The visualization layer further facilitates the exploration across\nmultiple scenarios and evaluates scenario differences in performance using\nsustainability indices of the FEW nexus. We demonstrate the utility of FEWSim\nthrough a case study for the Phoenix Active Management Area (AMA) in Arizona."}
{"id": "2506.13796", "pdf": "https://arxiv.org/pdf/2506.13796.pdf", "abs": "https://arxiv.org/abs/2506.13796", "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand\nfor research in climate science continues to grow. Natural language processing\ntechnologies, represented by Large Language Models (LLMs), have been widely\napplied to climate change-specific research, providing essential information\nsupport for decision-makers and the public. Some studies have improved model\nperformance on relevant tasks by constructing climate change-related\ninstruction data and instruction-tuning LLMs. However, current research remains\ninadequate in efficiently producing large volumes of high-precision instruction\ndata for climate change, which limits further development of climate change\nLLMs. This study introduces an automated method for constructing instruction\ndata. The method generates instructions using facts and background knowledge\nfrom documents and enhances the diversity of the instruction data through web\nscraping and the collection of seed instructions. Using this method, we\nconstructed a climate change instruction dataset, named ClimateChat-Corpus,\nwhich was used to fine-tune open-source LLMs, resulting in an LLM named\nClimateChat. Evaluation results show that ClimateChat significantly improves\nperformance on climate change question-and-answer tasks. Additionally, we\nevaluated the impact of different base models and instruction data on LLM\nperformance and demonstrated its capability to adapt to a wide range of climate\nchange scientific discovery tasks, emphasizing the importance of selecting an\nappropriate base model for instruction tuning. This research provides valuable\nreferences and empirical support for constructing climate change instruction\ndata and training climate change-specific LLMs."}
{"id": "2506.14147", "pdf": "https://arxiv.org/pdf/2506.14147.pdf", "abs": "https://arxiv.org/abs/2506.14147", "title": "The Teacher's Dilemma: Balancing Trade-Offs in Programming Education for Emergent Bilingual Students", "authors": ["Emma R. Dodoo", "Tamara Nelson-Fromm", "Mark Guzdial"], "categories": ["cs.HC"], "comment": null, "summary": "K-12 computing teachers must navigate complex trade-offs when selecting\nprogramming languages and instructional materials for classrooms with emergent\nbilingual students. While they aim to foster an inclusive learning environment\nby addressing language barriers that impact student engagement, they must also\nalign with K-12 computer science curricular guidelines and prepare students for\nindustry-standard programming tools. Because programming languages\npredominantly use English keywords and most instructional materials are written\nin English, these linguistic barriers introduce cognitive load and\naccessibility challenges. This paper examines teachers' decisions in balancing\nthese competing priorities, highlighting the tensions between accessibility,\ncurriculum alignment, and workforce preparation. The findings shed light on how\nour teacher participants negotiate these trade-offs and what factors influence\ntheir selection of programming tools to best support EB students while meeting\nbroader educational and professional goals."}
{"id": "2506.13886", "pdf": "https://arxiv.org/pdf/2506.13886.pdf", "abs": "https://arxiv.org/abs/2506.13886", "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and\ncombine numbers. While humans consistently learn to navigate this diversity,\nlarge language models (LLMs) struggle with linguistic-mathematical puzzles\ninvolving cross-linguistic numeral systems, which humans can learn to solve\nsuccessfully. We investigate why this task is difficult for LLMs through a\nseries of experiments that untangle the linguistic and mathematical aspects of\nnumbers in language. Our experiments establish that models cannot consistently\nsolve such problems unless the mathematical operations in the problems are\nexplicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty +\nthree\"). In further ablation studies, we probe how individual parameters of\nnumeral construction and combination affect performance. While humans use their\nlinguistic understanding of numbers to make inferences about the implicit\ncompositional structure of numerals, LLMs seem to lack this notion of implicit\nnumeral structure. We conclude that the ability to flexibly infer compositional\nrules from implicit patterns in human-scale data remains an open challenge for\ncurrent reasoning models."}
{"id": "2506.14159", "pdf": "https://arxiv.org/pdf/2506.14159.pdf", "abs": "https://arxiv.org/abs/2506.14159", "title": "StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework", "authors": ["Shayan Talaei", "Meijin Li", "Kanu Grover", "James Kent Hippler", "Diyi Yang", "Amin Saberi"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "Every individual carries a unique and personal life story shaped by their\nmemories and experiences. However, these memories are often scattered and\ndifficult to organize into a coherent narrative, a challenge that defines the\ntask of autobiography writing. Existing conversational writing assistants tend\nto rely on generic user interactions and pre-defined guidelines, making it\ndifficult for these systems to capture personal memories and develop a complete\nbiography over time. We introduce StorySage, a user-driven software system\ndesigned to meet the needs of a diverse group of users that supports a flexible\nconversation and a structured approach to autobiography writing. Powered by a\nmulti-agent framework composed of an Interviewer, Session Scribe, Planner,\nSection Writer, and Session Coordinator, our system iteratively collects user\nmemories, updates their autobiography, and plans for future conversations. In\nexperimental simulations, StorySage demonstrates its ability to navigate\nmultiple sessions and capture user memories across many conversations. User\nstudies (N=28) highlight how StorySage maintains improved conversational flow,\nnarrative completeness, and higher user satisfaction when compared to a\nbaseline. In summary, StorySage contributes both a novel architecture for\nautobiography writing and insights into how multi-agent systems can enhance\nhuman-AI creative partnerships."}
{"id": "2506.13888", "pdf": "https://arxiv.org/pdf/2506.13888.pdf", "abs": "https://arxiv.org/abs/2506.13888", "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning."}
{"id": "2506.14166", "pdf": "https://arxiv.org/pdf/2506.14166.pdf", "abs": "https://arxiv.org/abs/2506.14166", "title": "Affective-CARA: A Knowledge Graph Driven Framework for Culturally Adaptive Emotional Intelligence in HCI", "authors": ["Nirodya Pussadeniya", "Bahareh Nakisa", "Mohmmad Naim Rastgoo"], "categories": ["cs.HC"], "comment": null, "summary": "Culturally adaptive emotional responses remain a critical challenge in\naffective computing. This paper introduces Affective-CARA, an agentic framework\ndesigned to enhance user-agent interactions by integrating a Cultural Emotion\nKnowledge Graph (derived from StereoKG) with Valence, Arousal, and Dominance\nannotations, culture-specific data, and cross-cultural checks to minimize bias.\nA Gradient-Based Reward Policy Optimization mechanism further refines responses\naccording to cultural alignment, affective appropriateness, and iterative user\nfeedback. A Cultural-Aware Response Mediator coordinates knowledge retrieval,\nreinforcement learning updates, and historical data fusion. By merging\nreal-time user input with past emotional states and cultural insights,\nAffective-CARA delivers narratives that are deeply personalized and sensitive\nto diverse cultural norms. Evaluations on AffectNet, SEMAINE DB, and MERD\nconfirm that the framework consistently outperforms baseline models in\nsentiment alignment, cultural adaptation, and narrative quality. Affective-CARA\nachieved a Cultural Semantic Density of 9.32 out of 10 and lowered cultural\nrepresentation bias by 61% (KL-Divergence: 0.28), demonstrating robust\nperformance in generating ethical, adaptive responses. These findings suggest\nthe potential for more inclusive and empathetic interactions, making\nAffective-CARA an avenue for fostering culturally grounded user experiences\nacross domains such as cross-cultural communication, mental health support, and\neducation."}
{"id": "2506.13894", "pdf": "https://arxiv.org/pdf/2506.13894.pdf", "abs": "https://arxiv.org/abs/2506.13894", "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "categories": ["cs.CL"], "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates\nemotional speech based on contextual cues to enable more empathetic news\nconversations. Despite advancements in emotional text-to-speech (TTS)\ntechniques, task-oriented emotional SDSs remain underexplored due to the\ncompartmentalized nature of SDS and emotional TTS research, as well as the lack\nof standardized evaluation metrics for social goals. We address these\nchallenges by developing an emotional SDS for news conversations that utilizes\na large language model (LLM)-based sentiment analyzer to identify appropriate\nemotions and PromptTTS to synthesize context-appropriate emotional speech. We\nalso propose subjective evaluation scale for emotional SDSs and judge the\nemotion regulation performance of the proposed and baseline systems.\nExperiments showed that our emotional SDS outperformed a baseline system in\nterms of the emotion regulation and engagement. These results suggest the\ncritical role of speech emotion for more engaging conversations. All our source\ncode is open-sourced at\nhttps://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1"}
{"id": "2506.14196", "pdf": "https://arxiv.org/pdf/2506.14196.pdf", "abs": "https://arxiv.org/abs/2506.14196", "title": "Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Keran Wang", "Dong Whi Yoo", "Ravi Karkar", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive\nneurodegenerative conditions that impair memory, thought processes, and\nfunctioning. Family caregivers of individuals with AD/ADRD face significant\nmental health challenges due to long-term caregiving responsibilities. Yet,\ncurrent support systems often overlook the evolving nature of their mental\nwellbeing needs. Our study examines caregivers' mental wellbeing concerns,\nfocusing on the practices they adopt to manage the burden of caregiving and the\ntechnologies they use for support. Through semi-structured interviews with 25\nfamily caregivers of individuals with AD/ADRD, we identified the key causes and\neffects of mental health challenges, and developed a temporal mapping of how\ncaregivers' mental wellbeing evolves across three distinct stages of the\ncaregiving journey. Additionally, our participants shared insights into\nimprovements for existing mental health technologies, emphasizing the need for\naccessible, scalable, and personalized solutions that adapt to caregivers'\nchanging needs over time. These findings offer a foundation for designing\ndynamic, stage-sensitive interventions that holistically support caregivers'\nmental wellbeing, benefiting both caregivers and care recipients."}
{"id": "2506.13901", "pdf": "https://arxiv.org/pdf/2506.13901.pdf", "abs": "https://arxiv.org/abs/2506.13901", "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area."}
{"id": "2506.14295", "pdf": "https://arxiv.org/pdf/2506.14295.pdf", "abs": "https://arxiv.org/abs/2506.14295", "title": "The Impact of Generative AI on Social Media: An Experimental Study", "authors": ["Anders Giovanni Møller", "Daniel M. Romero", "David Jurgens", "Luca Maria Aiello"], "categories": ["cs.HC"], "comment": "48 pages, 12 figures", "summary": "Generative Artificial Intelligence (AI) tools are increasingly deployed\nacross social media platforms, yet their implications for user behavior and\nexperience remain understudied, particularly regarding two critical dimensions:\n(1) how AI tools affect the behaviors of content producers in a social media\ncontext, and (2) how content generated with AI assistance is perceived by\nusers. To fill this gap, we conduct a controlled experiment with a\nrepresentative sample of 680 U.S. participants in a realistic social media\nenvironment. The participants are randomly assigned to small discussion groups,\neach consisting of five individuals in one of five distinct experimental\nconditions: a control group and four treatment groups, each employing a unique\nAI intervention-chat assistance, conversation starters, feedback on comment\ndrafts, and reply suggestions. Our findings highlight a complex duality: some\nAI-tools increase user engagement and volume of generated content, but at the\nsame time decrease the perceived quality and authenticity of discussion, and\nintroduce a negative spill-over effect on conversations. Based on our findings,\nwe propose four design principles and recommendations aimed at social media\nplatforms, policymakers, and stakeholders: ensuring transparent disclosure of\nAI-generated content, designing tools with user-focused personalization,\nincorporating context-sensitivity to account for both topic and user intent,\nand prioritizing intuitive user interfaces. These principles aim to guide an\nethical and effective integration of generative AI into social media."}
{"id": "2506.13956", "pdf": "https://arxiv.org/pdf/2506.13956.pdf", "abs": "https://arxiv.org/abs/2506.13956", "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial\nto enhance user requests with visual cues from their surroundings for improved\nintent understanding. This process is defined as a multimodal classification\ntask. However, gathering a large-scale dataset encompassing both visual and\nlinguistic elements for model training is challenging and time-consuming. To\naddress this issue, our paper introduces a novel framework focusing on data\naugmentation in robotic assistance scenarios, encompassing both dialogues and\nrelated environmental imagery. This approach involves leveraging a\nsophisticated large language model to simulate potential conversations and\nenvironmental contexts, followed by the use of a stable diffusion model to\ncreate images depicting these environments. The additionally generated data\nserves to refine the latest multimodal models, enabling them to more accurately\ndetermine appropriate actions in response to user interactions with the limited\ntarget data. Our experimental results, based on a dataset collected from\nreal-world scenarios, demonstrate that our methodology significantly enhances\nthe robot's action selection capabilities, achieving the state-of-the-art\nperformance."}
{"id": "2506.14376", "pdf": "https://arxiv.org/pdf/2506.14376.pdf", "abs": "https://arxiv.org/abs/2506.14376", "title": "System 0: Transforming Artificial Intelligence into a Cognitive Extension", "authors": ["Massimo Chiriatti", "Marianna Bergamaschi Ganapini", "Enrico Panai", "Brenda K. Wiederhold", "Giuseppe Riva"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces System 0, a conceptual framework for understanding how\nartificial intelligence functions as a cognitive extension preceding both\nintuitive (System 1) and deliberative (System 2) thinking processes. As AI\nsystems increasingly shape the informational substrate upon which human\ncognition operates, they transform from passive tools into active cognitive\npartners. Building on the Extended Mind hypothesis and Heersmink's criteria for\ncognitive extension, we argue that AI systems satisfy key conditions for\ncognitive integration. These include reliability, trust, transparency,\nindividualization, and the ability to enhance and transform human mental\nfunctions. However, AI integration creates a paradox: while expanding cognitive\ncapabilities, it may simultaneously constrain thinking through sycophancy and\nbias amplification. To address these challenges, we propose seven\nevidence-based frameworks for effective human-AI cognitive integration:\nEnhanced Cognitive Scaffolding, which promotes progressive autonomy; Symbiotic\nDivision of Cognitive Labor, strategically allocating tasks based on\ncomparative strengths; Dialectical Cognitive Enhancement, countering AI\nsycophancy through productive epistemic tension; Agentic Transparency and\nControl, ensuring users understand and direct AI influence; Expertise\nDemocratization, breaking down knowledge silos; Social-Emotional Augmentation,\naddressing affective dimensions of cognitive work; and Duration-Optimized\nIntegration, managing the evolving human-AI relationship over time. Together,\nthese frameworks provide a comprehensive approach for harnessing AI as a\ngenuine cognitive extension while preserving human agency, critical thinking,\nand intellectual growth, transforming AI from a replacement for human cognition\ninto a catalyst for enhanced thinking."}
{"id": "2506.13965", "pdf": "https://arxiv.org/pdf/2506.13965.pdf", "abs": "https://arxiv.org/abs/2506.13965", "title": "Are manual annotations necessary for statutory interpretations retrieval?", "authors": ["Aleksander Smywiński-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Król"], "categories": ["cs.CL"], "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have\nextended the meaning of a legal concept by providing interpretations of what a\nconcept means or does not mean. This allow legal professionals to use such\ninterpretations as precedents as well as laymen to better understand the legal\nconcept. The state-of-the-art approach for retrieving the most relevant\ninterpretations for these concepts currently depends on the ranking of\nsentences and the training of language models over annotated examples. That\nmanual annotation process can be quite expensive and need to be repeated for\neach such concept, which prompted recent research in trying to automate this\nprocess. In this paper, we highlight the results of various experiments\nconducted to determine the volume, scope and even the need for manual\nannotation. First of all, we check what is the optimal number of annotations\nper a legal concept. Second, we check if we can draw the sentences for\nannotation randomly or there is a gain in the performance of the model, when\nonly the best candidates are annotated. As the last question we check what is\nthe outcome of automating the annotation process with the help of an LLM."}
{"id": "2506.14468", "pdf": "https://arxiv.org/pdf/2506.14468.pdf", "abs": "https://arxiv.org/abs/2506.14468", "title": "MERba: Multi-Receptive Field MambaVision for Micro-Expression Recognition", "authors": ["Xinglong Mao", "Shifeng Liu", "Sirui Zhao", "Tong Xu", "Enhong Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Micro-expressions (MEs) are brief, involuntary facial movements that reveal\ngenuine emotions, holding significant potential in psychological diagnosis and\ncriminal investigations. Despite notable advances in automatic ME recognition\n(MER), existing methods still struggle to jointly capture localized muscle\nactivations and global facial dependencies, both critical for recognizing\nsubtle emotional cues. To tackle this challenge, we propose MERba, a novel\nmulti-receptive field architecture tailored for MER. MERba introduces a series\nof Local-Global Feature Integration stages, where fine-grained motion features\nare first extracted by local extractors containing MambaVision Mixers within\nnon-overlapping windows, and then global dependencies across these regions are\nmodeled via lightweight self-attention layers. This hierarchical design enables\na progressive transition from localized perception to holistic facial\nunderstanding. Furthermore, we introduce an asymmetric multi-scanning strategy\nto eliminate redundant scanning directions and enhance local spatial\nperception. To address the high inter-class similarity among negative MEs, we\nintroduce a Dual-Granularity Classification Module that decouples the\nrecognition process into a coarse-to-fine paradigm. Experiments on two\nbenchmark MER datasets demonstrate that MERba outperforms existing methods,\nwith ablation studies confirming the effectiveness of each proposed component."}
{"id": "2506.13978", "pdf": "https://arxiv.org/pdf/2506.13978.pdf", "abs": "https://arxiv.org/abs/2506.13978", "title": "AI shares emotion with humans across languages and cultures", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "categories": ["cs.CL"], "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and\nmeaningful exchange of emotions between humans and artificial intelligence\n(AI). Current AI systems based on large language models (LLMs) can provide\nfeedback that makes people feel heard. Yet it remains unclear whether LLMs\nrepresent emotion in language as humans do, or whether and how the emotional\ntone of their output can be controlled. We assess human-AI emotional alignment\nacross linguistic-cultural groups and model-families, using interpretable LLM\nfeatures translated from concept-sets for over twenty nuanced emotion\ncategories (including six basic emotions). Our analyses reveal that LLM-derived\nemotion spaces are structurally congruent with human perception, underpinned by\nthe fundamental affective dimensions of valence and arousal. Furthermore, these\nemotion-related features also accurately predict large-scale behavioural data\non word ratings along these two core dimensions, reflecting both universal and\nlanguage-specific patterns. Finally, by leveraging steering vectors derived\nsolely from human-centric emotion concepts, we show that model expressions can\nbe stably and naturally modulated across distinct emotion categories, which\nprovides causal evidence that human emotion concepts can be used to\nsystematically induce LLMs to produce corresponding affective states when\nconveying content. These findings suggest AI not only shares emotional\nrepresentations with humans but its affective outputs can be precisely guided\nusing psychologically grounded emotion concepts."}
{"id": "2506.14476", "pdf": "https://arxiv.org/pdf/2506.14476.pdf", "abs": "https://arxiv.org/abs/2506.14476", "title": "SimSpark: Interactive Simulation of Social Media Behaviors", "authors": ["Ziyue Lin", "Yi Shan", "Lin Gao", "Xinghua Jia", "Siming Chen"], "categories": ["cs.HC"], "comment": "32 pages, 7 figures", "summary": "Understanding user behaviors on social media has garnered significant\nscholarly attention, enhancing our comprehension of how virtual platforms\nimpact society and empowering decision-makers. Simulating social media\nbehaviors provides a robust tool for capturing the patterns of social media\nbehaviors, testing hypotheses, and predicting the effects of various\ninterventions, ultimately contributing to a deeper understanding of social\nmedia environments. Moreover, it can overcome difficulties associated with\nutilizing real data for analysis, such as data accessibility issues, ethical\nconcerns, and the complexity of processing large and heterogeneous datasets.\nHowever, researchers and stakeholders need more flexible platforms to\ninvestigate different user behaviors by simulating different scenarios and\ncharacters, which is not possible yet. Therefore, this paper introduces\nSimSpark, an interactive system including simulation algorithms and interactive\nvisual interfaces which is capable of creating small simulated social media\nplatforms with customizable characters and social environments. We address\nthree key challenges: generating believable behaviors, validating simulation\nresults, and supporting interactive control for generation and results\nanalysis. A simulation workflow is introduced to generate believable behaviors\nof agents by utilizing large language models. A visual interface enables\nreal-time parameter adjustment and process monitoring for customizing\ngeneration settings. A set of visualizations and interactions are also designed\nto display the models' outputs for further analysis. Effectiveness is evaluated\nthrough case studies, quantitative simulation model assessments, and expert\ninterviews."}
{"id": "2506.14012", "pdf": "https://arxiv.org/pdf/2506.14012.pdf", "abs": "https://arxiv.org/abs/2506.14012", "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "categories": ["cs.CL"], "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English text$\\unicode{x2013}$even under linguistic\nconstraints$\\unicode{x2013}$embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation."}
{"id": "2506.14567", "pdf": "https://arxiv.org/pdf/2506.14567.pdf", "abs": "https://arxiv.org/abs/2506.14567", "title": "Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains", "authors": ["Emanuel Moss", "Elizabeth Watkins", "Christopher Persaud", "Passant Karunaratne", "Dawn Nafus"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI tools have become more prevalent in engineering workflows,\nparticularly through chatbots and code assistants. As the perceived accuracy of\nthese tools improves, questions arise about whether and how those who work in\nhigh-precision domains might maintain vigilance for errors, and what other\naspects of using such tools might trouble their work. This paper analyzes\ninterviews with hardware and software engineers, and their collaborators, who\nwork in integrated circuit design to identify the role accuracy plays in their\nuse of generative AI tools and what other forms of trouble they face in using\nsuch tools. The paper inventories these forms of trouble, which are then mapped\nto elements of generative AI systems, to conclude that controlling the context\nof interactions between engineers and the generative AI tools is one of the\nlargest challenges they face. The paper concludes with recommendations for\nmitigating this form of trouble by increasing the ability to control context\ninteractively."}
{"id": "2506.14028", "pdf": "https://arxiv.org/pdf/2506.14028.pdf", "abs": "https://arxiv.org/abs/2506.14028", "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications."}
{"id": "2506.14611", "pdf": "https://arxiv.org/pdf/2506.14611.pdf", "abs": "https://arxiv.org/abs/2506.14611", "title": "Exploring MLLMs Perception of Network Visualization Principles", "authors": ["Jacob Miller", "Markus Wallinger", "Ludwig Felder", "Timo Brand", "Henry Förster", "Johannes Zink", "Chunyang Chen", "Stephen Kobourov"], "categories": ["cs.HC"], "comment": null, "summary": "In this paper, we test whether Multimodal Large Language Models (MLLMs) can\nmatch human-subject performance in tasks involving the perception of properties\nin network layouts. Specifically, we replicate a human-subject experiment about\nperceiving quality (namely stress) in network layouts using GPT-4o and\nGemini-2.5. Our experiments show that giving MLLMs exactly the same study\ninformation as trained human participants results in a similar performance to\nhuman experts and exceeds the performance of untrained non-experts.\nAdditionally, we show that prompt engineering that deviates from the\nhuman-subject experiment can lead to better-than-human performance in some\nsettings. Interestingly, like human subjects, the MLLMs seem to rely on visual\nproxies rather than computing the actual value of stress, indicating some sense\nor facsimile of perception. Explanations from the models provide descriptions\nsimilar to those used by the human participants (e.g., even distribution of\nnodes and uniform edge lengths)."}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040.pdf", "abs": "https://arxiv.org/abs/2506.14040", "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "authors": ["Md Nazmus Sakib"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design."}
{"id": "2506.14653", "pdf": "https://arxiv.org/pdf/2506.14653.pdf", "abs": "https://arxiv.org/abs/2506.14653", "title": "How Viable are Energy Savings in Smart Homes? A Call to Embrace Rebound Effects in Sustainable HCI", "authors": ["Christina Bremer", "Harshit Gujral", "Michelle Lin", "Lily Hinkers", "Christoph Becker", "Vlad C. Coroamă"], "categories": ["cs.HC"], "comment": "25 pages, 3 figures", "summary": "As part of global climate action, digital technologies are seen as a key\nenabler of energy efficiency savings. A popular application domain for this\nwork is smart homes. There is a risk, however, that these efficiency gains\nresult in rebound effects, which reduce or even overcompensate the savings.\nRebound effects are well-established in economics, but it is less clear whether\nthey also inform smart energy research in other disciplines. In this paper, we\nask: to what extent have rebound effects and their underlying mechanisms been\nconsidered in computing, HCI and smart home research? To answer this, we\nconducted a literature mapping drawing on four scientific databases and a\nSIGCHI corpus. Our results reveal limited consideration of rebound effects and\nsignificant opportunities for HCI to advance this topic. We conclude with a\ntaxonomy of actions for HCI to address rebound effects and help determine the\nviability of energy efficiency projects."}
{"id": "2506.14046", "pdf": "https://arxiv.org/pdf/2506.14046.pdf", "abs": "https://arxiv.org/abs/2506.14046", "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short,\nconversational passages of text, particularly for training and filtering Large\nLanguage Models (LLMs). We introduce Ace-CEFR, a dataset of English\nconversational text passages expert-annotated with their corresponding level of\ntext difficulty. We experiment with several models on Ace-CEFR, including\nTransformer-based models and LLMs. We show that models trained on Ace-CEFR can\nmeasure text difficulty more accurately than human experts and have latency\nappropriate to production environments. Finally, we release the Ace-CEFR\ndataset to the public for research and development."}
{"id": "2506.14670", "pdf": "https://arxiv.org/pdf/2506.14670.pdf", "abs": "https://arxiv.org/abs/2506.14670", "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery", "authors": ["Jina Kim", "Leeje Jang", "Yao-Yi Chiang", "Guanyu Wang", "Michelle Pasco"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and\nmanual image annotation guided by detailed protocols to identify environmental\ncharacteristics, including physical disorder, decay, street safety, and\nsociocultural symbols, and to examine their impact on developmental and health\noutcomes. While these methods yield rich insights, they are time-consuming and\nrequire intensive expert intervention. Recent technological advances, including\nvision-language models (VLMs), have begun to automate parts of this process;\nhowever, existing efforts are often ad hoc and lack adaptability across\nresearch designs and geographic contexts. In this demo paper, we present\nStreetLens, a human-centered, researcher-configurable workflow that embeds\nrelevant social science expertise in a VLM for scalable neighborhood\nenvironmental assessments. StreetLens mimics the process of trained human\ncoders by grounding the analysis in questions derived from established\ninterview protocols, retrieving relevant street view imagery (SVI), and\ngenerating a wide spectrum of semantic annotations from objective features\n(e.g., the number of cars) to subjective perceptions (e.g., the sense of\ndisorder in an image). By enabling researchers to define the VLM's role through\ndomain-informed prompting, StreetLens places domain knowledge at the core of\nthe analysis process. It also supports the integration of prior survey data to\nenhance robustness and expand the range of characteristics assessed across\ndiverse settings. We provide a Google Colab notebook to make StreetLens\naccessible and extensible for researchers working with public or custom SVI\ndatasets. StreetLens represents a shift toward flexible, agentic AI systems\nthat work closely with researchers to accelerate and scale neighborhood\nstudies."}
{"id": "2506.14064", "pdf": "https://arxiv.org/pdf/2506.14064.pdf", "abs": "https://arxiv.org/abs/2506.14064", "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "categories": ["cs.CL"], "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of\ntheir intricate distribution of syntactic and semantic features. Yet, current\nresearch relies on schematically created language examples to investigate these\nconstructions, missing out on statistical information and naturally-occurring\nexamples that can be gained from large language corpora. Thus, we present a\nmethodological approach for detecting and annotating naturally-occurring\nexamples of English embedded clauses in large-scale text data using\nconstituency parsing and a set of parsing heuristics. Our tool has been\nevaluated on our dataset Golden Embedded Clause Set (GECS), which includes\nhand-annotated examples of naturally-occurring English embedded clause\nsentences. Finally, we present a large-scale dataset of naturally-occurring\nEnglish embedded clauses which we have extracted from the open-source corpus\nDolma using our extraction tool."}
{"id": "2506.14677", "pdf": "https://arxiv.org/pdf/2506.14677.pdf", "abs": "https://arxiv.org/abs/2506.14677", "title": "Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach", "authors": ["Yingchao Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents a human-centered, real-time, user-adaptive speech-to-sign\nlanguage animation system that integrates Transformer-based motion generation\nwith a transparent, user-editable JSON intermediate layer. The framework\novercomes key limitations in prior sign language technologies by enabling\ndirect user inspection and modification of sign segments, thus enhancing\nnaturalness, expressiveness, and user agency. Leveraging a streaming Conformer\nencoder and autoregressive Transformer-MDN decoder, the system synchronizes\nspoken input into upper-body and facial motion for 3D avatar rendering. Edits\nand user ratings feed into a human-in-the-loop optimization loop for continuous\nimprovement. Experiments with 20 deaf signers and 5 interpreters show that the\neditable interface and participatory feedback significantly improve\ncomprehension, naturalness, usability, and trust, while lowering cognitive\nload. With sub-20 ms per-frame inference on standard hardware, the system is\nready for real-time communication and education. This work illustrates how\ntechnical and participatory innovation together enable accessible, explainable,\nand user-adaptive AI for sign language technology."}
{"id": "2506.14101", "pdf": "https://arxiv.org/pdf/2506.14101.pdf", "abs": "https://arxiv.org/abs/2506.14101", "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "categories": ["cs.CL"], "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has\ndrastic consequences for the clinical domain. This is particularly important\nwith regards to automatically generating discharge summaries (a lengthy medical\ndocument that summarizes a hospital in-patient visit). Automatically generating\nthese summaries would free physicians to care for patients and reduce\ndocumentation burden. The goal of this work is to discover new methods that\ncombine language-based graphs and deep learning models to address provenance of\ncontent and trustworthiness in automatic summarization. Our method shows\nimpressive reliability results on the publicly available Medical Information\nMart for Intensive III (MIMIC-III) corpus and clinical notes written by\nphysicians at Anonymous Hospital. rovide our method, generated discharge ary\noutput examples, source code and trained models."}
{"id": "2506.14720", "pdf": "https://arxiv.org/pdf/2506.14720.pdf", "abs": "https://arxiv.org/abs/2506.14720", "title": "How Warm-Glow Alters the Usability of Technology", "authors": ["Antonios Saravanos"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "As technology increasingly aligns with users' personal values, traditional\nmodels of usability, focused on functionality and specifically effectiveness,\nefficiency, and satisfaction, may not fully capture how people perceive and\nevaluate it. This study investigates how the warm-glow phenomenon, the positive\nfeeling associated with doing good, shapes perceived usability. An experimental\napproach was taken in which participants evaluated a hypothetical technology\nunder conditions designed to evoke either the intrinsic (i.e., personal\nfulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A\nMultivariate Analysis of Variance as well as subsequent follow-up analyses\nrevealed that intrinsic warm-glow significantly enhances all dimensions of\nperceived usability, while extrinsic warm-glow selectively influences perceived\neffectiveness and satisfaction. These findings suggest that perceptions of\nusability extend beyond functionality and are shaped by how technology\nresonates with users' broader sense of purpose. We conclude by proposing that\ndesigners consider incorporating warm-glow into technology as a strategic\ndesign decision."}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111.pdf", "abs": "https://arxiv.org/abs/2506.14111", "title": "Essential-Web v1.0: 24T tokens of organized web data", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}
{"id": "2506.13845", "pdf": "https://arxiv.org/pdf/2506.13845.pdf", "abs": "https://arxiv.org/abs/2506.13845", "title": "Students' Reliance on AI in Higher Education: Identifying Contributing Factors", "authors": ["Griffin Pitts", "Neha Rani", "Weedguet Mildort", "Eva-Marie Cook"], "categories": ["cs.CY", "cs.AI", "cs.HC", "K.3; K.4; I.2.6"], "comment": null, "summary": "The increasing availability and use of artificial intelligence (AI) tools in\neducational settings has raised concerns about students' overreliance on these\ntechnologies. Overreliance occurs when individuals accept incorrect\nAI-generated recommendations, often without critical evaluation, leading to\nflawed problem solutions and undermining learning outcomes. This study\ninvestigates potential factors contributing to patterns of AI reliance among\nundergraduate students, examining not only overreliance but also appropriate\nreliance (correctly accepting helpful and rejecting harmful recommendations)\nand underreliance (incorrectly rejecting helpful recommendations). Our approach\ncombined pre- and post-surveys with a controlled experimental task where\nparticipants solved programming problems with an AI assistant that provided\nboth accurate and deliberately incorrect suggestions, allowing direct\nobservation of students' reliance patterns when faced with varying AI\nreliability. We find that appropriate reliance is significantly related to\nstudents' programming self-efficacy, programming literacy, and need for\ncognition, while showing negative correlations with post-task trust and\nsatisfaction. Overreliance showed significant correlations with post-task trust\nand satisfaction with the AI assistant. Underreliance was negatively correlated\nwith programming literacy, programming self-efficacy, and need for cognition.\nOverall, the findings provide insights for developing targeted interventions\nthat promote appropriate reliance on AI tools, with implications for the\nintegration of AI in curriculum and educational technologies."}
{"id": "2506.14123", "pdf": "https://arxiv.org/pdf/2506.14123.pdf", "abs": "https://arxiv.org/abs/2506.14123", "title": "Sampling from Your Language Model One Byte at a Time", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals."}
{"id": "2506.13933", "pdf": "https://arxiv.org/pdf/2506.13933.pdf", "abs": "https://arxiv.org/abs/2506.13933", "title": "TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles", "authors": ["Tobias Kerbl", "David Brecht", "Nils Gehrke", "Nijinshan Karunainayagam", "Niklas Krauss", "Florian Pfab", "Richard Taupitz", "Ines Trautmannsheimer", "Xiyan Su", "Maria-Magdalena Wolf", "Frank Diermeyer"], "categories": ["cs.RO", "cs.HC", "cs.SE"], "comment": null, "summary": "Teleoperation is a key enabler for future mobility, supporting Automated\nVehicles in rare and complex scenarios beyond the capabilities of their\nautomation. Despite ongoing research, no open source software currently\ncombines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance\nthrough high-level interaction with automated driving software modules, and\nintegration with a real-world vehicle for practical testing. To address this\ngap, we present a modular, open source teleoperation software stack that can\ninteract with an automated driving software, e.g., Autoware, enabling Remote\nAssistance and Remote Driving. The software featuresstandardized interfaces for\nseamless integration with various real-world and simulation platforms, while\nallowing for flexible design of the human-machine interface. The system is\ndesigned for modularity and ease of extension, serving as a foundation for\ncollaborative development on individual software components as well as\nrealistic testing and user studies. To demonstrate the applicability of our\nsoftware, we evaluated the latency and performance of different vehicle\nplatforms in simulation and real-world. The source code is available on GitHub"}
{"id": "2506.14157", "pdf": "https://arxiv.org/pdf/2506.14157.pdf", "abs": "https://arxiv.org/abs/2506.14157", "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "authors": ["Chengyu Huang", "Tanya Goyal"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO)\nperformance with the underlying preference datasets. In this work, our\nobservation is that the differences between the preferred response $y^+$ and\ndispreferred response $y^-$ influence what LLMs can learn, which may not match\nthe desirable differences to learn. Therefore, we use distance and reward\nmargin to quantify these differences, and combine them to get Distance\nCalibrated Reward Margin (DCRM), a metric that measures the quality of a\nresponse pair for PO. Intuitively, DCRM encourages minimal noisy differences\nand maximal desired differences. With this, we study 3 types of commonly used\npreference datasets, classified along two axes: the source of the responses and\nthe preference labeling function. We establish a general correlation between\nhigher DCRM of the training set and better learning outcome. Inspired by this,\nwe propose a best-of-$N^2$ pairing method that selects response pairs with the\nhighest DCRM. Empirically, in various settings, our method produces training\ndatasets that can further improve models' performance on AlpacaEval, MT-Bench,\nand Arena-Hard over the existing training sets."}
{"id": "2506.13971", "pdf": "https://arxiv.org/pdf/2506.13971.pdf", "abs": "https://arxiv.org/abs/2506.13971", "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior.\nHowever, the subjective moments of negative experience, where the conversation\nloses fluidity or enjoyment remain understudied. These moments are infrequent\nin naturalistic data, and thus training a supervised learning (SL) model\nrequires costly manual data annotation. We applied semi-supervised learning\n(SSL) to leverage targeted labeled and unlabeled clips for training multimodal\n(audio, facial, text) deep features to predict non-fluid or unenjoyable moments\nin holdout videoconference sessions. The modality-fused co-training SSL\nachieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by\nup to 4% with the same amount of labeled data. Remarkably, the best SSL model\nwith just 8% labeled data matched 96% of the SL model's full-data performance.\nThis shows an annotation-efficient framework for modeling videoconference\nexperience."}
{"id": "2506.14158", "pdf": "https://arxiv.org/pdf/2506.14158.pdf", "abs": "https://arxiv.org/abs/2506.14158", "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across\ndiverse downstream tasks. However, their autoregressive nature leads to\nsubstantial inference latency, posing challenges for real-time applications.\nSpeculative sampling mitigates this issue by introducing a drafting phase\nfollowed by a parallel validation phase, enabling faster token generation and\nverification. Existing approaches, however, overlook the inherent coherence in\ntext generation, limiting their efficiency. To address this gap, we propose a\nSpeculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,\nwhich extends speculative sampling by leveraging multi-head drafting for rapid\ntoken generation and a continuous verification tree for efficient candidate\nvalidation and feature reuse. Experimental results demonstrate that S$^4$C\nsurpasses baseline methods across mainstream tasks, offering enhanced\nefficiency, parallelism, and the ability to generate more valid tokens with\nfewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an\nacceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods."}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040.pdf", "abs": "https://arxiv.org/abs/2506.14040", "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "authors": ["Md Nazmus Sakib"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design."}
{"id": "2506.14161", "pdf": "https://arxiv.org/pdf/2506.14161.pdf", "abs": "https://arxiv.org/abs/2506.14161", "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias."}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200.pdf", "abs": "https://arxiv.org/abs/2506.14200", "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness."}
{"id": "2506.14175", "pdf": "https://arxiv.org/pdf/2506.14175.pdf", "abs": "https://arxiv.org/abs/2506.14175", "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models."}
{"id": "2506.14268", "pdf": "https://arxiv.org/pdf/2506.14268.pdf", "abs": "https://arxiv.org/abs/2506.14268", "title": "Public Acceptance of Cybernetic Avatars in the service sector: Evidence from a Large-Scale Survey in Dubai", "authors": ["Laura Aymerich-Franch", "Tarek Taha", "Takahiro Miyashita", "Hiroko Kamide", "Hiroshi Ishiguro", "Paolo Dario"], "categories": ["cs.RO", "cs.ET", "cs.HC"], "comment": "25 pages, 3 Figures", "summary": "Cybernetic avatars are hybrid interaction robots or digital representations\nthat combine autonomous capabilities with teleoperated control. This study\ninvestigates the acceptance of cybernetic avatars in the highly multicultural\nsociety of Dubai, with particular emphasis on robotic avatars for customer\nservice. Specifically, we explore how acceptance varies as a function of robot\nappearance (e.g., android, robotic-looking, cartoonish), deployment settings\n(e.g., shopping malls, hotels, hospitals), and functional tasks (e.g.,\nproviding information, patrolling). To this end, we conducted a large-scale\nsurvey with over 1,000 participants. Overall, cybernetic avatars received a\nhigh level of acceptance, with physical robot avatars receiving higher\nacceptance than digital avatars. In terms of appearance, robot avatars with a\nhighly anthropomorphic robotic appearance were the most accepted, followed by\ncartoonish designs and androids. Animal-like appearances received the lowest\nlevel of acceptance. Among the tasks, providing information and guidance was\nrated as the most valued. Shopping malls, airports, public transport stations,\nand museums were the settings with the highest acceptance, whereas\nhealthcare-related spaces received lower levels of support. An analysis by\ncommunity cluster revealed among others that Emirati respondents showed\nsignificantly greater acceptance of android appearances compared to the overall\nsample, while participants from the 'Other Asia' cluster were significantly\nmore accepting of cartoonish appearances. Our study underscores the importance\nof incorporating citizen feedback into the design and deployment of cybernetic\navatars from the early stages to enhance acceptance of this technology in\nsociety."}
{"id": "2506.14177", "pdf": "https://arxiv.org/pdf/2506.14177.pdf", "abs": "https://arxiv.org/abs/2506.14177", "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for\nASR due to scarce and costly transcribed data caused by linguistic complexity.\nThis study investigates building CS-ASR using synthetic CS data. We propose a\nphrase-level mixing method to generate synthetic CS data that mimics natural\npatterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data\nto fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This\npaper focuses on three under-resourced Southeast Asian language pairs:\nMalay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),\nestablishing a new comprehensive benchmark for CS-ASR to evaluate the\nperformance of leading ASR models. Experimental results show that the proposed\ntraining strategy enhances ASR performance on monolingual and CS tests, with\nBM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a\ncost-effective approach for CS-ASR development, benefiting research and\nindustry."}
{"id": "2506.14287", "pdf": "https://arxiv.org/pdf/2506.14287.pdf", "abs": "https://arxiv.org/abs/2506.14287", "title": "Steering Robots with Inference-Time Interactions", "authors": ["Yanwei Wang"], "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "MIT Robotics PhD Thesis", "summary": "Imitation learning has driven the development of generalist policies capable\nof autonomously solving multiple tasks. However, when a pretrained policy makes\nerrors during deployment, there are limited mechanisms for users to correct its\nbehavior. While collecting additional data for finetuning can address such\nissues, doing so for each downstream use case is inefficient at deployment. My\nresearch proposes an alternative: keeping pretrained policies frozen as a fixed\nskill repertoire while allowing user interactions to guide behavior generation\ntoward user preferences at inference time. By making pretrained policies\nsteerable, users can help correct policy errors when the model struggles to\ngeneralize-without needing to finetune the policy. Specifically, I propose (1)\ninference-time steering, which leverages user interactions to switch between\ndiscrete skills, and (2) task and motion imitation, which enables user\ninteractions to edit continuous motions while satisfying task constraints\ndefined by discrete symbolic plans. These frameworks correct misaligned policy\npredictions without requiring additional training, maximizing the utility of\npretrained models while achieving inference-time user objectives."}
{"id": "2506.14190", "pdf": "https://arxiv.org/pdf/2506.14190.pdf", "abs": "https://arxiv.org/abs/2506.14190", "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication.\n  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity\nand limited exposure to multilingual, code-switched data, while collecting such\nspeech is costly. Prior work generates synthetic audio from text, but these\nmethods are computationally intensive and hard to scale. We introduce\nAsyncSwitch, a novel asynchronous adaptation framework that leverages\nlarge-scale, text-rich web data to pre-expose ASR models to diverse\ncode-switched domains before fine-tuning on paired speech-text corpora. Our\nthree-stage process (1) trains decoder self-attention and feedforward layers on\ncode-switched text, (2) aligns decoder and encoder via cross-attention using\nlimited speech-text data, and (3) fully fine-tunes the entire model.\nExperiments with Whisper on Malay-English code-switching demonstrate a 9.02%\nrelative WER reduction, while improving monolingual performance in Singlish,\nMalay, and other English variants."}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371.pdf", "abs": "https://arxiv.org/abs/2506.14371", "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts."}
{"id": "2506.14199", "pdf": "https://arxiv.org/pdf/2506.14199.pdf", "abs": "https://arxiv.org/abs/2506.14199", "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "categories": ["cs.CL"], "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic\nelements, which traditional metrics like BLEU and METEOR fail to assess due to\ntheir focus on lexical overlap. This oversight neglects the narrative\nconsistency and stylistic fidelity that are crucial for literary works. To\naddress this, we propose MAS-LitEval, a multi-agent system using Large Language\nModels (LLMs) to evaluate translations based on terminology, narrative, and\nstyle. We tested MAS-LitEval on translations of The Little Prince and A\nConnecticut Yankee in King Arthur's Court, generated by various LLMs, and\ncompared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these\nmetrics, with top models scoring up to 0.890 in capturing literary nuances.\nThis work introduces a scalable, nuanced framework for Translation Quality\nAssessment (TQA), offering a practical tool for translators and researchers."}
{"id": "2409.19139", "pdf": "https://arxiv.org/pdf/2409.19139.pdf", "abs": "https://arxiv.org/abs/2409.19139", "title": "Gaze-informed Signatures of Trust and Collaboration in Human-Autonomy Teams", "authors": ["Anthony J. Ries", "Stéphane Aroca-Ouellette", "Alessandro Roncone", "Ewart J. de Visser"], "categories": ["cs.HC", "J.4"], "comment": null, "summary": "In the evolving landscape of human-autonomy teaming (HAT), fostering\neffective collaboration and trust between human and autonomous agents is\nincreasingly important. To explore this, we used the game Overcooked AI to\ncreate dynamic teaming scenarios featuring varying agent behaviors (clumsy,\nrigid, adaptive) and environmental complexities (low, medium, high). Our\nobjectives were to assess the performance of adaptive AI agents designed with\nhierarchical reinforcement learning for better teamwork and measure eye\ntracking signals related to changes in trust and collaboration. The results\nindicate that the adaptive agent was more effective in managing teaming and\ncreating an equitable task distribution across environments compared to the\nother agents. Working with the adaptive agent resulted in better coordination,\nreduced collisions, more balanced task contributions, and higher trust ratings.\nReduced gaze allocation, across all agents, was associated with higher trust\nlevels, while blink count, scan path length, agent revisits and trust were\npredictive of the humans contribution to the team. Notably, fixation revisits\non the agent increased with environmental complexity and decreased with agent\nversatility, offering a unique metric for measuring teammate performance\nmonitoring. These findings underscore the importance of designing autonomous\nteammates that not only excel in task performance but also enhance teamwork by\nbeing more predictable and reducing the cognitive load on human team members.\nAdditionally, this study highlights the potential of eye-tracking as an\nunobtrusive measure for evaluating and improving human-autonomy teams,\nsuggesting eye gaze could be used by agents to dynamically adapt their\nbehaviors."}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200.pdf", "abs": "https://arxiv.org/abs/2506.14200", "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness."}
{"id": "2410.00174", "pdf": "https://arxiv.org/pdf/2410.00174.pdf", "abs": "https://arxiv.org/abs/2410.00174", "title": "Why Interdisciplinary Teams Fail: A Systematic Analysis With Activity Theory in Clinical AI Collaboration", "authors": ["Bingsheng Yao", "Yao Du", "Yue Fu", "Xuhai Xu", "Yanjun Gao", "Hong Yu", "Dakuo Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Advanced AI technologies are increasingly integrated into clinical domains to\nadvance patient care. The design and development of clinical AI technologies\nnecessitate seamless collaboration between clinical and technical experts. Yet,\nsuch interdisciplinary teams are often unsuccessful, with a lack of systematic\nanalysis of collaboration barriers and coping strategies. This work examines\ntwo clinical AI collaborations in the context of speech-language pathology via\nsemi-structured interviews with six clinical and seven technical experts. Using\nActivity Theory (AT) as our analytical lens, we systematically investigate\npersistent knowledge gaps in mismatched data coding themes and specialized\nlanguages, and also highlight how clinical data can act as boundary objects and\nhuman knowledge brokers to alleviate these challenges. Our work underscores the\nbenefits of leveraging analytical frameworks like AT to systematically examine\ninterdisciplinary teams' collaborative work and provide meaningful insights on\nbest practices in future collaboration."}
{"id": "2506.14203", "pdf": "https://arxiv.org/pdf/2506.14203.pdf", "abs": "https://arxiv.org/abs/2506.14203", "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "authors": ["Jongho Kim", "Romain Storaï", "Seung-won Hwang"], "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in\naiding patients experiencing anomia, a difficulty identifying the names of\nitems. Identifying the intended target item from patient's circumlocution\ninvolves the two challenges of term failure and error: (1) The terms relevant\nto identifying the item remain unseen. (2) What makes the challenge unique is\ninherent perturbed terms by semantic paraphasia, which are not exactly related\nto the target item, hindering the identification process. To address each, we\npropose robustifying the model from semantically paraphasic errors and\nenhancing the model with unseen terms with gradient-based selective\naugmentation. Specifically, the gradient value controls augmented data quality\namid semantic errors, while the gradient variance guides the inclusion of\nunseen but relevant terms. Due to limited domain-specific datasets, we evaluate\nthe model on the Tip-of-the-Tongue dataset as an intermediary task and then\napply our findings to real patient data from AphasiaBank. Our results\ndemonstrate strong performance against baselines, aiding anomia patients by\naddressing the outlined challenges."}
{"id": "2501.04227", "pdf": "https://arxiv.org/pdf/2501.04227.pdf", "abs": "https://arxiv.org/abs/2501.04227", "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Michael Moor", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery."}
{"id": "2506.14205", "pdf": "https://arxiv.org/pdf/2506.14205.pdf", "abs": "https://arxiv.org/abs/2506.14205", "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth"}
{"id": "2502.05017", "pdf": "https://arxiv.org/pdf/2502.05017.pdf", "abs": "https://arxiv.org/abs/2502.05017", "title": "Bridging Voting and Deliberation with Algorithms: Field Insights from vTaiwan and Kultur Komitee", "authors": ["Joshua C. Yang", "Fynn Bachmann"], "categories": ["cs.HC", "cs.AI", "econ.GN", "q-fin.EC", "91B14, 91B12, 91A12, 68T01, 68T20, 68U35", "H.5.3; I.2.0; I.2.11; J.1; G.2.0; G.2.2; K.4.1; K.4.3"], "comment": "In Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25), 2025", "summary": "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Preference-based Clustering for\nDeliberation (PCD), which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives. By\nintroducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes."}
{"id": "2506.14206", "pdf": "https://arxiv.org/pdf/2506.14206.pdf", "abs": "https://arxiv.org/abs/2506.14206", "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "categories": ["cs.CL"], "comment": null, "summary": "Training data has been proven to be one of the most critical components in\ntraining generative AI. However, obtaining high-quality data remains\nchallenging, with data privacy issues presenting a significant hurdle. To\naddress the need for high-quality data. Synthesize data has emerged as a\nmainstream solution, demonstrating impressive performance in areas such as\nimages, audio, and video. Generating mixed-type data, especially high-quality\ntabular data, still faces significant challenges. These primarily include its\ninherent heterogeneous data types, complex inter-variable relationships, and\nintricate column-wise distributions. In this paper, we introduce CausalDiffTab,\na diffusion model-based generative model specifically designed to handle mixed\ntabular data containing both numerical and categorical features, while being\nmore flexible in capturing complex interactions among variables. We further\npropose a hybrid adaptive causal regularization method based on the principle\nof Hierarchical Prior Fusion. This approach adaptively controls the weight of\ncausal regularization, enhancing the model's performance without compromising\nits generative capabilities. Comprehensive experiments conducted on seven\ndatasets demonstrate that CausalDiffTab outperforms baseline methods across all\nmetrics. Our code is publicly available at:\nhttps://github.com/Godz-z/CausalDiffTab."}
{"id": "2502.18145", "pdf": "https://arxiv.org/pdf/2502.18145.pdf", "abs": "https://arxiv.org/abs/2502.18145", "title": "Carbon and Silicon, Coexist or Compete? A Survey on Human-AI Interactions in Agent-based Modeling and Simulation", "authors": ["Ziyue Lin", "Siqi Shen", "Zichen Cheng", "Cheok Lam Lai", "Siming Chen"], "categories": ["cs.HC"], "comment": "36 pages, 3 figures", "summary": "Recent interest in human-AI interactions in agent-based modeling and\nsimulation (ABMS) has grown rapidly due to the widespread utilization of large\nlanguage models (LLMs). ABMS is an intelligent approach that simulates\nautonomous agents' behaviors within a defined environment to research emergent\nphenomena. Integrating LLMs into ABMS enables natural language interaction\nbetween humans and models. Meanwhile, it introduces new challenges that rely on\nhuman interaction to address. Human involvement can assist ABMS in adapting to\nflexible and complex research demands. However, systematic reviews of\ninteractions that examine how humans and AI interact in ABMS are lacking. In\nthis paper, we investigate existing works and propose a novel taxonomy to\ncategorize the interactions derived from them. Specifically, human users refer\nto researchers who utilize ABMS tools to conduct their studies in our survey.\nWe decompose interactions into five dimensions: the goals that users want to\nachieve (Why), the phases that users are involved (When), the components of the\nsystem (What), the roles of users (Who), and the means of interactions (How).\nOur analysis summarizes the findings that reveal existing interaction patterns.\nThey provide researchers who develop interactions with comprehensive guidance\non how humans and AI interact. We further discuss the unexplored interactions\nand suggest future research directions."}
{"id": "2506.14211", "pdf": "https://arxiv.org/pdf/2506.14211.pdf", "abs": "https://arxiv.org/abs/2506.14211", "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "categories": ["cs.CL"], "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital\nplatforms for communication and news consumption, various actors employ\nlinguistic strategies to influence public perception. While models have become\nproficient at detecting explicit patterns, which typically appear in texts as\nsingle remarks referred to as utterances, such as social media posts, malicious\nactors have shifted toward utilizing implicit influential verbal patterns\nembedded within conversations. These verbal patterns aim to mentally penetrate\nthe victim's mind in order to influence them, enabling the actor to obtain the\ndesired information through implicit means. This paper presents an improved\napproach for detecting such implicit influential patterns. Furthermore, the\nproposed model is capable of identifying the specific locations of these\ninfluential elements within a conversation. To achieve this, the existing\ndataset was augmented using the reasoning capabilities of state-of-the-art\nlanguage models. Our designed framework resulted in a 6% improvement in the\ndetection of implicit influential patterns in conversations. Moreover, this\napproach improved the multi-label classification tasks related to both the\ntechniques used for influence and the vulnerability of victims by 33% and 43%,\nrespectively."}
{"id": "2506.12605", "pdf": "https://arxiv.org/pdf/2506.12605.pdf", "abs": "https://arxiv.org/abs/2506.12605", "title": "The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being", "authors": ["Yutong Zhang", "Dora Zhao", "Jeffrey T. Hancock", "Robert Kraut", "Diyi Yang"], "categories": ["cs.HC"], "comment": null, "summary": "As large language models (LLMs)-enhanced chatbots grow increasingly\nexpressive and socially responsive, many users are beginning to form\ncompanionship-like bonds with them, particularly with simulated AI partners\ndesigned to mimic emotionally attuned interlocutors. These emerging AI\ncompanions raise critical questions: Can such systems fulfill social needs\ntypically met by human relationships? How do they shape psychological\nwell-being? And what new risks arise as users develop emotional ties to\nnon-human agents? This study investigates how people interact with AI\ncompanions, especially simulated partners on CharacterAI, and how this use is\nassociated with users' psychological well-being. We analyzed survey data from\n1,131 users and 4,363 chat sessions (413,509 messages) donated by 244\nparticipants, focusing on three dimensions of use: nature of the interaction,\ninteraction intensity, and self-disclosure. By triangulating self-reports\nprimary motivation, open-ended relationship descriptions, and annotated chat\ntranscripts, we identify patterns in how users engage with AI companions and\nits associations with well-being. Findings suggest that people with smaller\nsocial networks are more likely to turn to chatbots for companionship, but that\ncompanionship-oriented chatbot usage is consistently associated with lower\nwell-being, particularly when people use the chatbots more intensively, engage\nin higher levels of self-disclosure, and lack strong human social support. Even\nthough some people turn to chatbots to fulfill social needs, these uses of\nchatbots do not fully substitute for human connection. As a result, the\npsychological benefits may be limited, and the relationship could pose risks\nfor more socially isolated or emotionally vulnerable users."}
{"id": "2506.14213", "pdf": "https://arxiv.org/pdf/2506.14213.pdf", "abs": "https://arxiv.org/abs/2506.14213", "title": "Chaining Event Spans for Temporal Relation Grounding", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "categories": ["cs.CL"], "comment": "In Proceedings of the 18th Conference of the European Chapter of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages\n  1689-1700", "summary": "Accurately understanding temporal relations between events is a critical\nbuilding block of diverse tasks, such as temporal reading comprehension (TRC)\nand relation extraction (TRE). For example in TRC, we need to understand the\ntemporal semantic differences between the following two questions that are\nlexically near-identical: \"What finished right before the decision?\" or \"What\nfinished right after the decision?\". To discern the two questions, existing\nsolutions have relied on answer overlaps as a proxy label to contrast similar\nand dissimilar questions. However, we claim that answer overlap can lead to\nunreliable results, due to spurious overlaps of two dissimilar questions with\ncoincidentally identical answers. To address the issue, we propose a novel\napproach that elicits proper reasoning behaviors through a module for\npredicting time spans of events. We introduce the Timeline Reasoning Network\n(TRN) operating in a two-step inductive reasoning process: In the first step\nmodel initially answers each question with semantic and syntactic information.\nThe next step chains multiple questions on the same event to predict a\ntimeline, which is then used to ground the answers. Results on the TORQUE and\nTB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms\nprevious methods by effectively resolving the spurious overlaps using the\npredicted timeline."}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."}
{"id": "2506.14234", "pdf": "https://arxiv.org/pdf/2506.14234.pdf", "abs": "https://arxiv.org/abs/2506.14234", "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/."}
{"id": "2501.18045", "pdf": "https://arxiv.org/pdf/2501.18045.pdf", "abs": "https://arxiv.org/abs/2501.18045", "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors", "authors": ["Myra Cheng", "Angela Y. Lee", "Kristina Rapuano", "Kate Niederhoffer", "Alex Liebscher", "Jeffrey Hancock"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025", "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development."}
{"id": "2506.14235", "pdf": "https://arxiv.org/pdf/2506.14235.pdf", "abs": "https://arxiv.org/abs/2506.14235", "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "categories": ["cs.CL"], "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with\nknowledge of existing facts and plays a key role in various downstream tasks.\nPrevious methods focused on either graph structure learning or semantic\nreasoning, failing to integrate dual reasoning perspectives to handle different\nprediction scenarios. Moreover, they lack the capability to capture the\ninherent differences between historical and non-historical events, which limits\ntheir generalization across different temporal contexts. To this end, we\npropose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs\nthree kinds of expert modules to integrate both structural and semantic\ninformation, guiding the reasoning process for different events. Extensive\nexperiments on three datasets demonstrate the effectiveness of our approach."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227.pdf", "abs": "https://arxiv.org/abs/2505.13227", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2506.14248", "pdf": "https://arxiv.org/pdf/2506.14248.pdf", "abs": "https://arxiv.org/abs/2506.14248", "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains."}
{"id": "2506.01391", "pdf": "https://arxiv.org/pdf/2506.01391.pdf", "abs": "https://arxiv.org/abs/2506.01391", "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "I.2.8; I.2.7; I.2.10; H.5.2"], "comment": "Updated results in Table 2 and Table 3; The project is available at\n  https://github.com/OpenBMB/AgentCPM-GUI", "summary": "The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data."}
{"id": "2506.14285", "pdf": "https://arxiv.org/pdf/2506.14285.pdf", "abs": "https://arxiv.org/abs/2506.14285", "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on\ngenerating coherent responses conditioning on textual context, the critical\nquestion of when to respond grounded on the temporal context remains\nunderexplored. To bridge this gap, we propose a novel task called timely\ndialogue response generation and introduce the TimelyChat benchmark, which\nevaluates the capabilities of language models to predict appropriate time\nintervals and generate time-conditioned responses. Additionally, we construct a\nlarge-scale training dataset by leveraging unlabeled event knowledge from a\ntemporal commonsense knowledge graph and employing a large language model (LLM)\nto synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent\ndesigned to proactively predict time intervals and generate timely responses\nthat align with those intervals. Experimental results show that Timer\noutperforms prompting-based LLMs and other fine-tuned baselines in both\nturn-level and dialogue-level evaluations. We publicly release our data, model,\nand code."}
{"id": "2506.12347", "pdf": "https://arxiv.org/pdf/2506.12347.pdf", "abs": "https://arxiv.org/abs/2506.12347", "title": "Sharp Tools: How Developers Wield Agentic AI in Real Software Engineering Tasks", "authors": ["Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Gustavo Soares", "Emerson Murphy-Hill"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents."}
{"id": "2506.14302", "pdf": "https://arxiv.org/pdf/2506.14302.pdf", "abs": "https://arxiv.org/abs/2506.14302", "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\npropelled the development of Conversational Recommendation Agents (CRAs).\nHowever, these agents often generate short-sighted responses that fail to\nsustain user guidance and meet expectations. Although preference optimization\nhas proven effective in aligning LLMs with user expectations, it remains costly\nand performs poorly in multi-turn dialogue. To address this challenge, we\nintroduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,\nwhich leverages Expectation Confirmation Theory to explicitly model the\nevolution of user satisfaction throughout multi-turn dialogues, uncovering the\nunderlying causes of dissatisfaction. These causes can be utilized to support\ntargeted optimization of unsatisfactory responses, thereby achieving turn-level\npreference optimization. ECPO ingeniously eliminates the significant sampling\noverhead of existing MTPO methods while ensuring the optimization process\ndrives meaningful improvements. To support ECPO, we introduce an LLM-based user\nsimulator, AILO, to simulate user feedback and perform expectation confirmation\nduring conversational recommendations. Experimental results show that ECPO\nsignificantly enhances CRA's interaction capabilities, delivering notable\nimprovements in both efficiency and effectiveness over existing MTPO methods."}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335.pdf", "abs": "https://arxiv.org/abs/2506.14335", "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "categories": ["cs.CL"], "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nusing different reference sets on reference-based metrics has not been\nsystematically investigated. This work examines the sensitivity of widely used\nreference-based metrics in relation to the choice of reference sets, analyzing\nthree diverse multi-reference summarization datasets: SummEval, GUMSum, and\nDUC2004. We demonstrate that many popular metrics exhibit significant\ninstability. This instability is particularly concerning for n-gram-based\nmetrics like ROUGE, where model rankings vary depending on the reference sets,\nundermining the reliability of model comparisons. We also collect human\njudgments on LLM outputs for genre-diverse data and examine their correlation\nwith metrics to supplement existing findings beyond newswire summaries, finding\nweak-to-no correlation. Taken together, we recommend incorporating reference\nset variation into summarization evaluation to enhance consistency alongside\ncorrelation with human judgments, especially when evaluating LLMs."}
{"id": "2506.14345", "pdf": "https://arxiv.org/pdf/2506.14345.pdf", "abs": "https://arxiv.org/abs/2506.14345", "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "authors": ["Bruno Martins", "Piotr Szymański", "Piotr Gramacki"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access."}
{"id": "2506.14370", "pdf": "https://arxiv.org/pdf/2506.14370.pdf", "abs": "https://arxiv.org/abs/2506.14370", "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the\nvisibility of Web and social media content through algorithmic curation. This\nstudy investigates how search engines like Google selectively promotes or\nsuppresses certain hashtags and subreddits, impacting the information users\nencounter. By comparing search engine results with nonsampled data from Reddit\nand Twitter/X, we reveal systematic biases in content visibility. Google's\nalgorithms tend to suppress subreddits and hashtags related to sexually\nexplicit material, conspiracy theories, advertisements, and cryptocurrencies,\nwhile promoting content associated with higher engagement. These findings\nsuggest that Google's gatekeeping practices influence public discourse by\ncurating the social media narratives available to users."}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371.pdf", "abs": "https://arxiv.org/abs/2506.14371", "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts."}
{"id": "2506.14397", "pdf": "https://arxiv.org/pdf/2506.14397.pdf", "abs": "https://arxiv.org/abs/2506.14397", "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to\nassess sentence-level negation understanding in LLMs. Thunder-NUBench goes\nbeyond surface-level cue detection by contrasting standard negation with\nstructurally diverse alternatives such as local negation, contradiction, and\nparaphrase. The benchmark consists of manually curated sentence-negation pairs\nand a multiple-choice dataset that enables in-depth evaluation of models'\nnegation understanding."}
{"id": "2506.14407", "pdf": "https://arxiv.org/pdf/2506.14407.pdf", "abs": "https://arxiv.org/abs/2506.14407", "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Schütze"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 15.07%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only ten documents, including the positive document, GPT-4.1 scores only\n35.06%, showing that document-side reasoning remains a challenge. Our codes are\navailable at github.com/ZeinabTaghavi/IMPLIRET.Contribution."}
{"id": "2506.14429", "pdf": "https://arxiv.org/pdf/2506.14429.pdf", "abs": "https://arxiv.org/abs/2506.14429", "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textbf{\\textit{stable perplexity}} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs."}
{"id": "2506.14448", "pdf": "https://arxiv.org/pdf/2506.14448.pdf", "abs": "https://arxiv.org/abs/2506.14448", "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks."}
{"id": "2506.14474", "pdf": "https://arxiv.org/pdf/2506.14474.pdf", "abs": "https://arxiv.org/abs/2506.14474", "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training."}
{"id": "2506.14493", "pdf": "https://arxiv.org/pdf/2506.14493.pdf", "abs": "https://arxiv.org/abs/2506.14493", "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance."}
{"id": "2506.14532", "pdf": "https://arxiv.org/pdf/2506.14532.pdf", "abs": "https://arxiv.org/abs/2506.14532", "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "categories": ["cs.CL"], "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems."}
{"id": "2506.14562", "pdf": "https://arxiv.org/pdf/2506.14562.pdf", "abs": "https://arxiv.org/abs/2506.14562", "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines."}
{"id": "2506.14580", "pdf": "https://arxiv.org/pdf/2506.14580.pdf", "abs": "https://arxiv.org/abs/2506.14580", "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI"], "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality."}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606.pdf", "abs": "https://arxiv.org/abs/2506.14606", "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch."}
{"id": "2506.14613", "pdf": "https://arxiv.org/pdf/2506.14613.pdf", "abs": "https://arxiv.org/abs/2506.14613", "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the\nsemantic content of the premise and hypothesis. In this work, we investigate\nwhether adding semantic information in the form of an Abstract Meaning\nRepresentation (AMR) helps pretrained language models better generalize in NLI.\nOur experiments integrating AMR into NLI in both fine-tuning and prompting\nsettings show that the presence of AMR in fine-tuning hinders model\ngeneralization while prompting with AMR leads to slight gains in\n\\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes\nfrom amplifying surface-level differences rather than aiding semantic\nreasoning. This amplification can mislead models to predict non-entailment even\nwhen the core meaning is preserved."}
{"id": "2506.14625", "pdf": "https://arxiv.org/pdf/2506.14625.pdf", "abs": "https://arxiv.org/abs/2506.14625", "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems."}
{"id": "2506.14634", "pdf": "https://arxiv.org/pdf/2506.14634.pdf", "abs": "https://arxiv.org/abs/2506.14634", "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Weiß", "Jessika Daikeler"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research."}
{"id": "2506.14641", "pdf": "https://arxiv.org/pdf/2506.14641.pdf", "abs": "https://arxiv.org/abs/2506.14641", "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars."}
{"id": "2506.14645", "pdf": "https://arxiv.org/pdf/2506.14645.pdf", "abs": "https://arxiv.org/abs/2506.14645", "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks."}
{"id": "2506.14646", "pdf": "https://arxiv.org/pdf/2506.14646.pdf", "abs": "https://arxiv.org/abs/2506.14646", "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git."}
{"id": "2506.14681", "pdf": "https://arxiv.org/pdf/2506.14681.pdf", "abs": "https://arxiv.org/abs/2506.14681", "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research."}
{"id": "2506.14702", "pdf": "https://arxiv.org/pdf/2506.14702.pdf", "abs": "https://arxiv.org/abs/2506.14702", "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet Üstün", "Sara Hooker"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations."}
{"id": "2506.14704", "pdf": "https://arxiv.org/pdf/2506.14704.pdf", "abs": "https://arxiv.org/abs/2506.14704", "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "authors": ["Anton Changalidis", "Aki Härmä"], "categories": ["cs.CL"], "comment": "This work has been accepted for publication at the First Workshop on\n  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations\ninfluence the empirical memorization capacity of generative transformers. The\nmodels are trained using synthetic text datasets derived from the Systematized\nNomenclature of Medicine (SNOMED) knowledge graph: triplets, representing\nstatic connections, and sequences, simulating complex relation patterns. The\nresults show that embedding size is the primary determinant of learning speed\nand capacity, while additional layers provide limited benefits and may hinder\nperformance on simpler datasets. Activation functions play a crucial role, and\nSoftmax demonstrates greater stability and capacity. Furthermore, increasing\nthe complexity of the data set seems to improve the final memorization. These\ninsights improve our understanding of transformer memory mechanisms and provide\na framework for optimizing model design with structured real-world data."}
{"id": "2506.14731", "pdf": "https://arxiv.org/pdf/2506.14731.pdf", "abs": "https://arxiv.org/abs/2506.14731", "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code."}
{"id": "2506.14758", "pdf": "https://arxiv.org/pdf/2506.14758.pdf", "abs": "https://arxiv.org/abs/2506.14758", "title": "Reasoning with Exploration: An Entropy Perspective", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning."}
{"id": "2506.14761", "pdf": "https://arxiv.org/pdf/2506.14761.pdf", "abs": "https://arxiv.org/abs/2506.14761", "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages."}
{"id": "2506.14767", "pdf": "https://arxiv.org/pdf/2506.14767.pdf", "abs": "https://arxiv.org/abs/2506.14767", "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm."}
{"id": "2506.13771", "pdf": "https://arxiv.org/pdf/2506.13771.pdf", "abs": "https://arxiv.org/abs/2506.13771", "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization", "authors": ["Banseok Lee", "Dongkyu Kim", "Youngcheon You", "Youngmin Kim"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. This establishes a superior size-performance trade-off, with\nkernel-level benchmarks indicating potential for a 5$\\times$ speedup compared\nto FP16. LittleBit paves the way for deploying powerful LLMs in\nresource-constrained environments."}
{"id": "2506.13778", "pdf": "https://arxiv.org/pdf/2506.13778.pdf", "abs": "https://arxiv.org/abs/2506.13778", "title": "Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning", "authors": ["Anvi Alex Eponon", "Moein Shahiki-Tash", "Ildar Batyrshin", "Christian E. Maldonado-Sifuentes", "Grigori Sidorov", "Alexander Gelbukh"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "This study presents a question-based knowledge encoding approach that\nimproves retrieval-augmented generation (RAG) systems without requiring\nfine-tuning or traditional chunking. We encode textual content using generated\nquestions that span the lexical and semantic space, creating targeted retrieval\ncues combined with a custom syntactic reranking method.\n  In single-hop retrieval over 109 scientific papers, our approach achieves a\nRecall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We\nalso introduce \"paper-cards\", concise paper summaries under 300 characters,\nwhich enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified\ntechnical queries.\n  For multihop tasks, our reranking method reaches an F1 score of 0.52 with\nLLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking\nand fine-tuned baselines which score 0.328 and 0.412 respectively.\n  This method eliminates fine-tuning requirements, reduces retrieval latency,\nenables intuitive question-driven knowledge access, and decreases vector\nstorage demands by 80%, positioning it as a scalable and efficient RAG\nalternative."}
{"id": "2506.13784", "pdf": "https://arxiv.org/pdf/2506.13784.pdf", "abs": "https://arxiv.org/abs/2506.13784", "title": "AcademicBrowse: Benchmarking Academic Browse Ability of LLMs", "authors": ["Junting Zhou", "Wang Li", "Yiyan Liao", "Nengyuan Zhang", "Tingjia Miaoand Zhihui Qi", "Yuhan Wu", "Tong Yang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed AcademicBrowse, the first dataset specifically designed to evaluate\nthe complex information retrieval capabilities of Large Language Models (LLMs)\nin academic research. AcademicBrowse possesses the following key\ncharacteristics: Academic Practicality, where question content closely mirrors\nreal academic learning and research environments, avoiding deliberately\nmisleading models; High Difficulty, with answers that are challenging for\nsingle models (e.g., Grok DeepSearch or Gemini Deep Research) to provide\ndirectly, often requiring at least three deep searches to derive; Concise\nEvaluation, where limiting conditions ensure answers are as unique as possible,\naccompanied by clear sources and brief solution explanations, greatly\nfacilitating subsequent audit and verification, surpassing the current lack of\nanalyzed search datasets both domestically and internationally; and Broad\nCoverage, as the dataset spans at least 15 different academic disciplines.\nThrough AcademicBrowse, we expect to more precisely measure and promote the\nperformance improvement of LLMs in complex academic information retrieval\ntasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse"}
{"id": "2506.13792", "pdf": "https://arxiv.org/pdf/2506.13792.pdf", "abs": "https://arxiv.org/abs/2506.13792", "title": "ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution", "authors": ["Gonçalo Hora de Carvalho", "Lazar S. Popov", "Sander Kaatee", "Kristinn R. Thórisson", "Tangrui Li", "Pétur Húni Björnsson", "Jilles S. Dibangoye"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "We introduce ICE-ID, a novel benchmark dataset for historical identity\nresolution, comprising 220 years (1703-1920) of Icelandic census records.\nICE-ID spans multiple generations of longitudinal data, capturing name\nvariations, demographic changes, and rich genealogical links. To the best of\nour knowledge, this is the first large-scale, open tabular dataset specifically\ndesigned to study long-term person-entity matching in a real-world population.\nWe define identity resolution tasks (within and across census waves) with\nclearly documented metrics and splits. We evaluate a range of methods:\nhandcrafted rule-based matchers, a ML ensemble as well as LLMs for structured\ndata (e.g. transformer-based tabular networks) against a novel approach to\ntabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose\nAI framework designed to reason with limited knowledge and resources. Its core\nis Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that\nNARS is suprisingly simple and competitive with other standard approaches,\nachieving SOTA at our task. By releasing ICE-ID and our code, we enable\nreproducible benchmarking of identity resolution approaches in longitudinal\nsettings and hope that ICE-ID opens new avenues for cross-disciplinary research\nin data linkage and historical analytics."}
{"id": "2506.13811", "pdf": "https://arxiv.org/pdf/2506.13811.pdf", "abs": "https://arxiv.org/abs/2506.13811", "title": "Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study", "authors": ["Sompote Youwai", "David Phim", "Vianne Gayl Murcia", "Rianne Clair Onas"], "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "This study investigates router-based multi-agent systems for automating\nfoundation design calculations through intelligent task classification and\nexpert selection. Three approaches were evaluated: single-agent processing,\nmulti-agent designer-checker architecture, and router-based expert selection.\nPerformance assessment utilized baseline models including DeepSeek R1, ChatGPT\n4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design\nscenarios. The router-based configuration achieved performance scores of 95.00%\nfor shallow foundations and 90.63% for pile design, representing improvements\nof 8.75 and 3.13 percentage points over standalone Grok 3 performance\nrespectively. The system outperformed conventional agentic workflows by 10.0 to\n43.75 percentage points. Grok 3 demonstrated superior standalone performance\nwithout external computational tools, indicating advances in direct LLM\nmathematical reasoning for engineering applications. The dual-tier\nclassification framework successfully distinguished foundation types, enabling\nappropriate analytical approaches. Results establish router-based multi-agent\nsystems as optimal for foundation design automation while maintaining\nprofessional documentation standards. Given safety-critical requirements in\ncivil engineering, continued human oversight remains essential, positioning\nthese systems as advanced computational assistance tools rather than autonomous\ndesign replacements in professional practice."}
{"id": "2506.13923", "pdf": "https://arxiv.org/pdf/2506.13923.pdf", "abs": "https://arxiv.org/abs/2506.13923", "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models", "authors": ["Vaskar Nath", "Elaine Lau", "Anisha Gunjal", "Manasi Sharma", "Nikhil Baharte", "Sean Hendryx"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study the process through which reasoning models trained with\nreinforcement learning on verifiable rewards (RLVR) can learn to solve new\nproblems. We find that RLVR drives performance through two main means: (1) by\ncompressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models\nlearn to solve new problems that they previously could not solve even at high\n$k$. We find that while capability gain exists across model scales, learning to\nsolve new problems is primarily driven through self-distillation. We\ndemonstrate these findings across model scales ranging from 0.5B to 72B on\n>500,000 reasoning problems with prompts and verifiable final answers across\nmath, science, and code domains. We further show that we can significantly\nimprove pass@$k$ rates by leveraging natural language guidance for the model to\nconsider within context while still requiring the model to derive a solution\nchain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new\nclass of online training algorithms. $\\text{Guide}$ adaptively incorporates\nhints into the model's context on problems for which all rollouts were\ninitially incorrect and adjusts the importance sampling ratio for the\n\"off-policy\" trajectories in order to optimize the policy for contexts in which\nthe hints are no longer present. We describe variants of $\\text{Guide}$ for\nGRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter\nmodels improves generalization over its vanilla counterpart with up to 4$\\%$\nmacro-average improvement across math benchmarks. We include careful ablations\nto analyze $\\text{Guide}$'s components and theoretically analyze Guide's\nlearning efficiency."}
{"id": "2506.13971", "pdf": "https://arxiv.org/pdf/2506.13971.pdf", "abs": "https://arxiv.org/abs/2506.13971", "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior.\nHowever, the subjective moments of negative experience, where the conversation\nloses fluidity or enjoyment remain understudied. These moments are infrequent\nin naturalistic data, and thus training a supervised learning (SL) model\nrequires costly manual data annotation. We applied semi-supervised learning\n(SSL) to leverage targeted labeled and unlabeled clips for training multimodal\n(audio, facial, text) deep features to predict non-fluid or unenjoyable moments\nin holdout videoconference sessions. The modality-fused co-training SSL\nachieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by\nup to 4% with the same amount of labeled data. Remarkably, the best SSL model\nwith just 8% labeled data matched 96% of the SL model's full-data performance.\nThis shows an annotation-efficient framework for modeling videoconference\nexperience."}
{"id": "2506.13977", "pdf": "https://arxiv.org/pdf/2506.13977.pdf", "abs": "https://arxiv.org/abs/2506.13977", "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "authors": ["Shiting Huang", "Zhen Fang", "Zehui Chen", "Siyu Yuan", "Junjie Ye", "Yu Zeng", "Lin Chen", "Qi Mao", "Feng Zhao"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\n\\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}."}
{"id": "2506.13992", "pdf": "https://arxiv.org/pdf/2506.13992.pdf", "abs": "https://arxiv.org/abs/2506.13992", "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science", "authors": ["An Luo", "Xun Xian", "Jin Du", "Fangqiao Tian", "Ganghua Wang", "Ming Zhong", "Shengchun Zhao", "Xuan Bi", "Zirui Liu", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "62-07, 62-08, 68T05, 68T07, 68T01, 68T50", "I.2.0; I.2.6; I.2.7; I.5.1; I.5.4; H.2.8; G.3"], "comment": null, "summary": "Large language models (LLMs) have advanced the automation of data science\nworkflows. Yet it remains unclear whether they can critically leverage external\ndomain knowledge as human data scientists do in practice. To answer this\nquestion, we introduce AssistedDS (Assisted Data Science), a benchmark designed\nto systematically evaluate how LLMs handle domain knowledge in tabular\nprediction tasks. AssistedDS features both synthetic datasets with explicitly\nknown generative mechanisms and real-world Kaggle competitions, each\naccompanied by curated bundles of helpful and adversarial documents. These\ndocuments provide domain-specific insights into data cleaning, feature\nengineering, and model selection. We assess state-of-the-art LLMs on their\nability to discern and apply beneficial versus harmful domain knowledge,\nevaluating submission validity, information recall, and predictive performance.\nOur results demonstrate three key findings: (1) LLMs frequently exhibit an\nuncritical adoption of provided information, significantly impairing their\npredictive performance when adversarial content is introduced, (2) helpful\nguidance is often insufficient to counteract the negative influence of\nadversarial information, and (3) in Kaggle datasets, LLMs often make errors in\nhandling time-series data, applying consistent feature engineering across\ndifferent folds, and interpreting categorical variables correctly. These\nfindings highlight a substantial gap in current models' ability to critically\nevaluate and leverage expert knowledge, underscoring an essential research\ndirection for developing more robust, knowledge-aware automated data science\nsystems."}
{"id": "2506.14086", "pdf": "https://arxiv.org/pdf/2506.14086.pdf", "abs": "https://arxiv.org/abs/2506.14086", "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking", "authors": ["Rahul Seetharaman", "Kaustubh D. Dhole", "Aman Bansal"], "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7", "H.3.3; I.5.4"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant strides across\nvarious information retrieval tasks, particularly as rerankers, owing to their\nstrong generalization and knowledge-transfer capabilities acquired from\nextensive pretraining. In parallel, the rise of LLM-based chat interfaces has\nraised user expectations, encouraging users to pose more complex queries that\nnecessitate retrieval by ``reasoning'' over documents rather than through\nsimple keyword matching or semantic similarity. While some recent efforts have\nexploited reasoning abilities of LLMs for reranking such queries, considerable\npotential for improvement remains. In that regards, we introduce InsertRank, an\nLLM-based reranker that leverages lexical signals like BM25 scores during\nreranking to further improve retrieval performance. InsertRank demonstrates\nimproved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning\n12 diverse domains, and R2MED, a specialized medical reasoning retrieval\nbenchmark spanning 8 different tasks. We conduct an exhaustive evaluation and\nseveral ablation studies and demonstrate that InsertRank consistently improves\nretrieval effectiveness across multiple families of LLMs, including GPT,\nGemini, and Deepseek models. %In addition, we also conduct ablation studies on\nnormalization by varying the scale of the BM25 scores, and positional bias by\nshuffling the order of the documents. With Deepseek-R1, InsertRank achieves a\nscore of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,\nsurpassing previous methods."}
{"id": "2506.14104", "pdf": "https://arxiv.org/pdf/2506.14104.pdf", "abs": "https://arxiv.org/abs/2506.14104", "title": "Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints", "authors": ["RuiKun Yang", "ZhongLiang Wei", "Longdi Xian"], "categories": ["cs.GR", "cs.CL", "cs.CY"], "comment": null, "summary": "Yangliuqing woodblock prints, a cornerstone of China's intangible cultural\nheritage, are celebrated for their intricate designs and vibrant colors.\nHowever, preserving these traditional art forms while fostering innovation\npresents significant challenges. This study explores the DeepSeek + MidJourney\napproach to generating creative, themed Yangliuqing woodblock prints focused on\nthe fight against COVID-19 and depicting joyous winners. Using Fr\\'echet\nInception Distance (FID) scores for evaluation, the method that combined\nDeepSeek-generated thematic prompts, MidJourney-generated thematic images,\noriginal Yangliuqing prints, and DeepSeek-generated key prompts in\nMidJourney-generated outputs achieved the lowest mean FID score (150.2) with\nminimal variability ({\\sigma} = 4.9). Additionally, feedback from 62\nparticipants, collected via questionnaires, confirmed that this hybrid approach\nproduced the most representative results. Moreover, the questionnaire data\nrevealed that participants demonstrated the highest willingness to promote\ntraditional culture and the strongest interest in consuming the AI-generated\nimages produced through this method. These findings underscore the\neffectiveness of an innovative approach that seamlessly blends traditional\nartistic elements with modern AI-driven creativity, ensuring both cultural\npreservation and contemporary relevance."}
{"id": "2506.14142", "pdf": "https://arxiv.org/pdf/2506.14142.pdf", "abs": "https://arxiv.org/abs/2506.14142", "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis."}
{"id": "2506.14148", "pdf": "https://arxiv.org/pdf/2506.14148.pdf", "abs": "https://arxiv.org/abs/2506.14148", "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment", "authors": ["Long-Vu Hoang", "Tuan Nguyen", "Tran Huy Dat"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a novel non-invasive object classification approach using\nacoustic scattering, demonstrated through a case study on hair assessment. When\nan incident wave interacts with an object, it generates a scattered acoustic\nfield encoding structural and material properties. By emitting acoustic stimuli\nand capturing the scattered signals from head-with-hair-sample objects, we\nclassify hair type and moisture using AI-driven, deep-learning-based sound\nclassification. We benchmark comprehensive methods, including (i) fully\nsupervised deep learning, (ii) embedding-based classification, (iii) supervised\nfoundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our\nbest strategy achieves nearly 90% classification accuracy by fine-tuning all\nparameters of a self-supervised model. These results highlight acoustic\nscattering as a privacy-preserving, non-contact alternative to visual\nclassification, opening huge potential for applications in various industries."}
{"id": "2506.14153", "pdf": "https://arxiv.org/pdf/2506.14153.pdf", "abs": "https://arxiv.org/abs/2506.14153", "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models", "authors": ["Tuan Dat Phuong", "Long-Vu Hoang", "Huy Dat Tran"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in speech synthesis technologies have led to increasingly\nadvanced spoofing attacks, posing significant challenges for automatic speaker\nverification systems. While systems based on self-supervised learning (SSL)\nmodels, particularly the XLSR-Conformer model, have demonstrated remarkable\nperformance in synthetic speech detection, there remains room for architectural\nimprovements. In this paper, we propose a novel approach that replaces the\ntraditional Multi-Layer Perceptron in the XLSR-Conformer model with a\nKolmogorov-Arnold Network (KAN), a novel architecture based on the\nKolmogorov-Arnold representation theorem. Our results on ASVspoof2021\ndemonstrate that integrating KAN into the SSL-based models can improve the\nperformance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER\non the 21LA set. These findings suggest that incorporating KAN into SSL-based\nmodels is a promising direction for advances in synthetic speech detection."}
{"id": "2506.14204", "pdf": "https://arxiv.org/pdf/2506.14204.pdf", "abs": "https://arxiv.org/abs/2506.14204", "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios", "authors": ["Aswin Shanmugam Subramanian", "Amit Das", "Naoyuki Kanda", "Jinyu Li", "Xiaofei Wang", "Yifan Gong"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "We extend the frameworks of Serialized Output Training (SOT) to address\npractical needs of both streaming and offline automatic speech recognition\n(ASR) applications. Our approach focuses on balancing latency and accuracy,\ncatering to real-time captioning and summarization requirements. We propose\nseveral key improvements: (1) Leveraging Continuous Speech Separation (CSS)\nsingle-channel front-end with end-to-end (E2E) systems for highly overlapping\nscenarios, challenging the conventional wisdom of E2E versus cascaded setups.\nThe CSS framework improves the accuracy of the ASR system by separating\noverlapped speech from multiple speakers. (2) Implementing dual models --\nConformer Transducer for streaming and Sequence-to-Sequence for offline -- or\nalternatively, a two-pass model based on cascaded encoders. (3) Exploring\nsegment-based SOT (segSOT) which is better suited for offline scenarios while\nalso enhancing readability of multi-talker transcriptions."}
{"id": "2506.14223", "pdf": "https://arxiv.org/pdf/2506.14223.pdf", "abs": "https://arxiv.org/abs/2506.14223", "title": "Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription", "authors": ["Anna Hamberger", "Sebastian Murgul", "Jochen Schmidt", "Michael Heizmann"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "Music transcription plays a pivotal role in Music Information Retrieval\n(MIR), particularly for stringed instruments like the guitar, where symbolic\nmusic notations such as MIDI lack crucial playability information. This\ncontribution introduces the Fretting-Transformer, an encoderdecoder model that\nutilizes a T5 transformer architecture to automate the transcription of MIDI\nsequences into guitar tablature. By framing the task as a symbolic translation\nproblem, the model addresses key challenges, including string-fret ambiguity\nand physical playability. The proposed system leverages diverse datasets,\nincluding DadaGP, GuitarToday, and Leduc, with novel data pre-processing and\ntokenization strategies. We have developed metrics for tablature accuracy and\nplayability to quantitatively evaluate the performance. The experimental\nresults demonstrate that the Fretting-Transformer surpasses baseline methods\nlike A* and commercial applications like Guitar Pro. The integration of\ncontext-sensitive processing and tuning/capo conditioning further enhances the\nmodel's performance, laying a robust foundation for future developments in\nautomated guitar transcription."}
{"id": "2506.14245", "pdf": "https://arxiv.org/pdf/2506.14245.pdf", "abs": "https://arxiv.org/abs/2506.14245", "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs", "authors": ["Xumeng Wen", "Zihan Liu", "Shun Zheng", "Zhijian Xu", "Shengyu Ye", "Zhirong Wu", "Xiao Liang", "Yang Wang", "Junjie Li", "Ziming Miao", "Jiang Bian", "Mao Yang"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the $Pass@K$ metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\n$Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\n$CoT$-$Pass@K$, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of $K$. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning."}
{"id": "2506.14280", "pdf": "https://arxiv.org/pdf/2506.14280.pdf", "abs": "https://arxiv.org/abs/2506.14280", "title": "Improving LoRA with Variational Learning", "authors": ["Bai Cong", "Nico Daheim", "Yuesong Shen", "Rio Yokota", "Mohammad Emtiyaz Khan", "Thomas Möllenhoff"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "16 pages, 4 figures", "summary": "Bayesian methods have recently been used to improve LoRA finetuning and,\nalthough they improve calibration, their effect on other metrics (such as\naccuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian\nmethods also increase computational overheads and require additional tricks for\nthem to work well. Here, we fix these issues by using a recently proposed\nvariational algorithm called IVON. We show that IVON is easy to implement and\nhas similar costs to AdamW, and yet it can also drastically improve many\nmetrics by using a simple posterior pruning technique. We present extensive\nresults on billion-scale LLMs (Llama and Qwen series) going way beyond the\nscale of existing applications of IVON. For example, we finetune a Llama-3.2-3B\nmodel on a set of commonsense reasoning tasks and improve accuracy over AdamW\nby 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian\nmethods like Laplace-LoRA and BLoB. Overall, our results show that variational\nlearning with IVON can effectively improve LoRA finetuning."}
{"id": "2506.14574", "pdf": "https://arxiv.org/pdf/2506.14574.pdf", "abs": "https://arxiv.org/abs/2506.14574", "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization", "authors": ["Mingkang Zhu", "Xi Chen", "Zhongdao Wang", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent advancements in reinforcement learning from human feedback have shown\nthat utilizing fine-grained token-level reward models can substantially enhance\nthe performance of Proximal Policy Optimization (PPO) in aligning large\nlanguage models. However, it is challenging to leverage such token-level reward\nas guidance for Direct Preference Optimization (DPO), since DPO is formulated\nas a sequence-level bandit problem. To address this challenge, this work\ndecomposes the sequence-level PPO into a sequence of token-level proximal\npolicy optimization problems and then frames the problem of token-level PPO\nwith token-level reward guidance, from which closed-form optimal token-level\npolicy and the corresponding token-level reward can be derived. Using the\nobtained reward and Bradley-Terry model, this work establishes a framework of\ncomputable loss functions with token-level reward guidance for DPO, and\nproposes a practical reward guidance based on the induced DPO reward. This\nformulation enables different tokens to exhibit varying degrees of deviation\nfrom reference policy based on their respective rewards. Experiment results\ndemonstrate that our method achieves substantial performance improvements over\nDPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on\nAlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at\nhttps://github.com/dvlab-research/TGDPO."}
{"id": "2506.14602", "pdf": "https://arxiv.org/pdf/2506.14602.pdf", "abs": "https://arxiv.org/abs/2506.14602", "title": "Computational Studies in Influencer Marketing: A Systematic Literature Review", "authors": ["Haoyang Gui", "Thales Bertaglia", "Catalina Goanta", "Gerasimos Spanakis"], "categories": ["cs.CY", "cs.CL"], "comment": "journal submission, under review", "summary": "Influencer marketing has become a crucial feature of digital marketing\nstrategies. Despite its rapid growth and algorithmic relevance, the field of\ncomputational studies in influencer marketing remains fragmented, especially\nwith limited systematic reviews covering the computational methodologies\nemployed. This makes overarching scientific measurements in the influencer\neconomy very scarce, to the detriment of interested stakeholders outside of\nplatforms themselves, such as regulators, but also researchers from other\nfields. This paper aims to provide an overview of the state of the art of\ncomputational studies in influencer marketing by conducting a systematic\nliterature review (SLR) based on the PRISMA model. The paper analyses 69\nstudies to identify key research themes, methodologies, and future directions\nin this research field. The review identifies four major research themes:\nInfluencer identification and characterisation, Advertising strategies and\nengagement, Sponsored content analysis and discovery, and Fairness.\nMethodologically, the studies are categorised into machine learning-based\ntechniques (e.g., classification, clustering) and non-machine-learning-based\ntechniques (e.g., statistical analysis, network analysis). Key findings reveal\na strong focus on optimising commercial outcomes, with limited attention to\nregulatory compliance and ethical considerations. The review highlights the\nneed for more nuanced computational research that incorporates contextual\nfactors such as language, platform, and industry type, as well as improved\nmodel explainability and dataset reproducibility. The paper concludes by\nproposing a multidisciplinary research agenda that emphasises the need for\nfurther links to regulation and compliance technology, finer granularity in\nanalysis, and the development of standardised datasets."}
{"id": "2506.14629", "pdf": "https://arxiv.org/pdf/2506.14629.pdf", "abs": "https://arxiv.org/abs/2506.14629", "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito"}
{"id": "2506.14755", "pdf": "https://arxiv.org/pdf/2506.14755.pdf", "abs": "https://arxiv.org/abs/2506.14755", "title": "Optimizing Length Compression in Large Reasoning Models", "authors": ["Zhengxiang Cheng", "Dongping Chen", "Mingyang Fu", "Tianyi Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 7 figures, 4 tables", "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1."}
{"id": "2506.14766", "pdf": "https://arxiv.org/pdf/2506.14766.pdf", "abs": "https://arxiv.org/abs/2506.14766", "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "categories": ["cs.CV", "cs.CL", "68T45"], "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks."}
{"id": "2304.03030", "pdf": "https://arxiv.org/pdf/2304.03030.pdf", "abs": "https://arxiv.org/abs/2304.03030", "title": "Compression of enumerations and gain", "authors": ["George Barmpalias", "Xiaoyan Zhang", "Bohua Zhan"], "categories": ["cs.CL", "cs.IT", "math.IT", "math.LO"], "comment": null, "summary": "We study the compressibility of enumerations in the context of Kolmogorov\ncomplexity, focusing on strong and weak forms of compression and their gain:\nthe amount of auxiliary information embedded in the compressed enumeration. The\nexistence of strong compression and weak gainless compression is shown for any\ncomputably enumerable (c.e.) set. The density problem of c.e. sets with respect\nto their prefix complexity is reduced to the question of whether every c.e. set\nis well-compressible, which we study via enumeration games."}
{"id": "2307.10867", "pdf": "https://arxiv.org/pdf/2307.10867.pdf", "abs": "https://arxiv.org/abs/2307.10867", "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback", "authors": ["Ashish Singh", "Ashutosh Singh", "Prateek Agarwal", "Zixuan Huang", "Arpita Singh", "Tong Yu", "Sungchul Kim", "Victor Bursztyn", "Nesreen K. Ahmed", "Puneet Mathur", "Erik Learned-Miller", "Franck Dernoncourt", "Ryan A. Rossi"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/", "summary": "Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem."}
{"id": "2312.16490", "pdf": "https://arxiv.org/pdf/2312.16490.pdf", "abs": "https://arxiv.org/abs/2312.16490", "title": "Exploring news intent and its application: A theory-driven approach", "authors": ["Zhengjia Wang", "Danding Wang", "Qiang Sheng", "Juan Cao", "Siyuan Ma", "Haonan Cheng"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to Information Processing & Management. DOI:\n  https://doi.org/10.1016/j.ipm.2025.104229", "summary": "Understanding the intent behind information is crucial. However, news as a\nmedium of public discourse still lacks a structured investigation of perceived\nnews intent and its application. To advance this field, this paper reviews\ninterdisciplinary studies on intentional action and introduces a conceptual\ndeconstruction-based news intent understanding framework (NINT). This framework\nidentifies the components of intent, facilitating a structured representation\nof news intent and its applications. Building upon NINT, we contribute a new\nintent perception dataset. Moreover, we investigate the potential of intent\nassistance on news-related tasks, such as significant improvement (+2.2% macF1)\nin the task of fake news detection. We hope that our findings will provide\nvaluable insights into action-based intent cognition and computational social\nscience."}
{"id": "2402.10735", "pdf": "https://arxiv.org/pdf/2402.10735.pdf", "abs": "https://arxiv.org/abs/2402.10735", "title": "Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification", "authors": ["John Dougrez-Lewis", "Mahmud Elahi Akhter", "Federico Ruggeri", "Sebastian Löbbers", "Yulan He", "Maria Liakata"], "categories": ["cs.CL"], "comment": "First two authors contributed equally to this work. 25 pages, 3\n  figure", "summary": "Although LLMs have shown great performance on Mathematics and Coding related\nreasoning tasks, the reasoning capabilities of LLMs regarding other forms of\nreasoning are still an open problem. Here, we examine the issue of reasoning\nfrom the perspective of claim verification. We propose a framework designed to\nbreak down any claim paired with evidence into atomic reasoning types that are\nnecessary for verification. We use this framework to create RECV, the first\nclaim verification benchmark, incorporating real-world claims, to assess the\ndeductive and abductive reasoning capabilities of LLMs. The benchmark comprises\nof three datasets, covering reasoning problems of increasing complexity. We\nevaluate three state-of-the-art proprietary LLMs under multiple prompt\nsettings. Our results show that while LLMs can address deductive reasoning\nproblems, they consistently fail in cases of abductive reasoning. Moreover, we\nobserve that enhancing LLMs with rationale generation is not always beneficial.\nNonetheless, we find that generated rationales are semantically similar to\nthose provided by humans, especially in deductive reasoning cases."}
{"id": "2406.13677", "pdf": "https://arxiv.org/pdf/2406.13677.pdf", "abs": "https://arxiv.org/abs/2406.13677", "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora", "authors": ["Erik Derner", "Sara Sansalvador de la Fuente", "Yoan Gutiérrez", "Paloma Moreda", "Nuria Oliver"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted for presentation at the 6th Workshop on Gender Bias in\n  Natural Language Processing (GeBNLP) at ACL 2025", "summary": "Large language models (LLMs) often inherit and amplify social biases embedded\nin their training data. A prominent social bias is gender bias. In this regard,\nprior work has mainly focused on gender stereotyping bias - the association of\nspecific roles or traits with a particular gender - in English and on\nevaluating gender bias in model embeddings or generated outputs. In contrast,\ngender representation bias - the unequal frequency of references to individuals\nof different genders - in the training corpora has received less attention. Yet\nsuch imbalances in the training data constitute an upstream source of bias that\ncan propagate and intensify throughout the entire model lifecycle. To fill this\ngap, we propose a novel LLM-based method to detect and quantify gender\nrepresentation bias in LLM training data in gendered languages, where\ngrammatical gender challenges the applicability of methods developed for\nEnglish. By leveraging the LLMs' contextual understanding, our approach\nautomatically identifies and classifies person-referencing words in gendered\nlanguage corpora. Applied to four Spanish-English benchmarks and five Valencian\ncorpora, our method reveals substantial male-dominant imbalances. We show that\nsuch biases in training data affect model outputs, but can surprisingly be\nmitigated leveraging small-scale training on datasets that are biased towards\nthe opposite gender. Our findings highlight the need for corpus-level gender\nbias analysis in multilingual NLP. We make our code and data publicly\navailable."}
{"id": "2407.07313", "pdf": "https://arxiv.org/pdf/2407.07313.pdf", "abs": "https://arxiv.org/abs/2407.07313", "title": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models", "authors": ["Benjamin G. Ascoli", "Yasoda Sai Ram Kandikonda", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": null, "summary": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics - Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) - suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL."}
{"id": "2410.01444", "pdf": "https://arxiv.org/pdf/2410.01444.pdf", "abs": "https://arxiv.org/abs/2410.01444", "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "authors": ["Jin Hwa Lee", "Thomas Jiralerspong", "Lei Yu", "Yoshua Bengio", "Emily Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "Published at ACL 2025", "summary": "By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition."}
{"id": "2410.07819", "pdf": "https://arxiv.org/pdf/2410.07819.pdf", "abs": "https://arxiv.org/abs/2410.07819", "title": "Uncovering Overfitting in Large Language Model Editing", "authors": ["Mengqi Zhang", "Xiaotian Ye", "Qiang Liu", "Pengjie Ren", "Shu Wu", "Zhumin Chen"], "categories": ["cs.CL"], "comment": "ICLR 2025", "summary": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge\nrecall mechanisms, we propose a new plug-and-play strategy called Learn the\nInference (LTI), which introduce a Multi-stage Inference Constraint module to\nguide the edited models in recalling new knowledge similarly to how unedited\nLLMs leverage knowledge through in-context learning. Extensive experimental\nresults across a wide range of tasks validate the effectiveness of LTI in\nmitigating Editing Overfit."}
{"id": "2410.16464", "pdf": "https://arxiv.org/pdf/2410.16464.pdf", "abs": "https://arxiv.org/abs/2410.16464", "title": "Beyond Browsing: API-Based Web Agents", "authors": ["Yueqi Song", "Frank Xu", "Shuyan Zhou", "Graham Neubig"], "categories": ["cs.CL", "cs.MA"], "comment": "20 pages, 8 figures", "summary": "Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by Browsing Agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-Based Agents outperform web Browsing Agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n24.0% absolute improvement over web browsing alone, achieving a success rate of\n38.9%, the SOTA performance among task-agnostic agents. These results strongly\nsuggest that when APIs are available, they present an attractive alternative to\nrelying on web browsing alone."}
{"id": "2410.18653", "pdf": "https://arxiv.org/pdf/2410.18653.pdf", "abs": "https://arxiv.org/abs/2410.18653", "title": "Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Meimingwei Li", "Christian Heumann", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)", "summary": "Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging due to trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. This paper addresses the specific problem\nof multicriteria evaluation for open-ended text generation, proposing novel\nmethods for both relative and absolute rankings of decoding methods.\nSpecifically, we employ benchmarking approaches based on partial orderings and\npresent a new summary metric to balance existing automatic indicators,\nproviding a more holistic evaluation of text generation quality. Our\nexperiments demonstrate that the proposed approaches offer a robust way to\ncompare decoding strategies and serve as valuable tools to guide model\nselection for open-ended text generation tasks. We suggest future directions\nfor improving evaluation methodologies in text generation and make our code,\ndatasets, and models publicly available."}
{"id": "2411.19563", "pdf": "https://arxiv.org/pdf/2411.19563.pdf", "abs": "https://arxiv.org/abs/2411.19563", "title": "Ensemble Watermarks for Large Language Models", "authors": ["Georg Niess", "Roman Kern"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference. This article extends our\n  earlier work arXiv:2405.08400 by introducing an ensemble of stylometric\n  watermarking features and alternative experimental analysis. Code and data\n  are available at http://github.com/CommodoreEU/ensemble-watermark", "summary": "As large language models (LLMs) reach human-like fluency, reliably\ndistinguishing AI-generated text from human authorship becomes increasingly\ndifficult. While watermarks already exist for LLMs, they often lack flexibility\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack, the performance\nremains high with 95% detection rate. In comparison, the red-green feature\nalone as a baseline achieves a detection rate of 49% after paraphrasing. The\nevaluation of all feature combinations reveals that the ensemble of all three\nconsistently has the highest detection rate across several LLMs and watermark\nstrength settings. Due to the flexibility of combining features in the\nensemble, various requirements and trade-offs can be addressed. Additionally,\nthe same detection function can be used without adaptations for all ensemble\nconfigurations. This method is particularly of interest to facilitate\naccountability and prevent societal harm."}
{"id": "2412.04726", "pdf": "https://arxiv.org/pdf/2412.04726.pdf", "abs": "https://arxiv.org/abs/2412.04726", "title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English", "authors": ["Dipankar Srirag", "Aditya Joshi", "Jordan Painter", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL: ACL 2025", "summary": "Despite large language models (LLMs) being known to exhibit bias against\nnon-standard language varieties, there are no known labelled datasets for\nsentiment analysis of English. To address this gap, we introduce BESSTIE, a\nbenchmark for sentiment and sarcasm classification for three varieties of\nEnglish: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect\ndatasets for these language varieties using two methods: location-based for\nGoogle Places reviews, and topic-based filtering for Reddit comments. To assess\nwhether the dataset accurately represents these varieties, we conduct two\nvalidation steps: (a) manual annotation of language varieties and (b) automatic\nlanguage variety prediction. Native speakers of the language varieties manually\nannotate the datasets with sentiment and sarcasm labels. We perform an\nadditional annotation exercise to validate the reliance of the annotated\nlabels. Subsequently, we fine-tune nine LLMs (representing a range of\nencoder/decoder and mono/multilingual models) on these datasets, and evaluate\ntheir performance on the two tasks. Our results show that the models\nconsistently perform better on inner-circle varieties (i.e., en-AU and en-UK),\nin comparison with en-IN, particularly for sarcasm classification. We also\nreport challenges in cross-variety generalisation, highlighting the need for\nlanguage variety-specific datasets such as ours. BESSTIE promises to be a\nuseful evaluative benchmark for future research in equitable LLMs, specifically\nin terms of language varieties. The BESSTIE dataset is publicly available at:\nhttps://huggingface.co/ datasets/unswnlporg/BESSTIE."}
{"id": "2412.05693", "pdf": "https://arxiv.org/pdf/2412.05693.pdf", "abs": "https://arxiv.org/abs/2412.05693", "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression", "authors": ["Michael R. Metel", "Boxing Chen", "Mehdi Rezagholizadeh"], "categories": ["cs.CL"], "comment": null, "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."}
{"id": "2412.14533", "pdf": "https://arxiv.org/pdf/2412.14533.pdf", "abs": "https://arxiv.org/abs/2412.14533", "title": "ClusterChat: Multi-Feature Search for Corpus Exploration", "authors": ["Ashish Chouhan", "Saifeldin Mandour", "Michael Gertz"], "categories": ["cs.CL"], "comment": "5 pages, 1 table, 1 figure, Accepted to SIGIR Demo Paper Track 2025", "summary": "Exploring large-scale text corpora presents a significant challenge in\nbiomedical, finance, and legal domains, where vast amounts of documents are\ncontinuously published. Traditional search methods, such as keyword-based\nsearch, often retrieve documents in isolation, limiting the user's ability to\neasily inspect corpus-wide trends and relationships. We present ClusterChat\n(The demo video and source code are available at:\nhttps://github.com/achouhan93/ClusterChat), an open-source system for corpus\nexploration that integrates cluster-based organization of documents using\ntextual embeddings with lexical and semantic search, timeline-driven\nexploration, and corpus and document-level question answering (QA) as\nmulti-feature search capabilities. We validate the system with two case studies\non a four million abstract PubMed dataset, demonstrating that ClusterChat\nenhances corpus exploration by delivering context-aware insights while\nmaintaining scalability and responsiveness on large-scale document collections."}
{"id": "2501.05478", "pdf": "https://arxiv.org/pdf/2501.05478.pdf", "abs": "https://arxiv.org/abs/2501.05478", "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models", "authors": ["Malak Mansour", "Ahmed Aly", "Bahey Tharwat", "Sarim Hashmi", "Dong An", "Ian Reid"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/", "summary": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications."}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."}
{"id": "2502.02958", "pdf": "https://arxiv.org/pdf/2502.02958.pdf", "abs": "https://arxiv.org/abs/2502.02958", "title": "Position: Editing Large Language Models Poses Serious Safety Risks", "authors": ["Paul Youssef", "Zhixue Zhao", "Daniel Braun", "Jörg Schlötterer", "Christin Seifert"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem."}
{"id": "2502.11425", "pdf": "https://arxiv.org/pdf/2502.11425.pdf", "abs": "https://arxiv.org/abs/2502.11425", "title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models", "authors": ["Jongho Kim", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main (short)", "summary": "Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies."}
{"id": "2502.13497", "pdf": "https://arxiv.org/pdf/2502.13497.pdf", "abs": "https://arxiv.org/abs/2502.13497", "title": "Towards Geo-Culturally Grounded LLM Generations", "authors": ["Piyawat Lertvittayakumjorn", "David Kinney", "Vinodkumar Prabhakaran", "Donald Martin Jr.", "Sunipa Dev"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (main conference)", "summary": "Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness."}
{"id": "2502.14445", "pdf": "https://arxiv.org/pdf/2502.14445.pdf", "abs": "https://arxiv.org/abs/2502.14445", "title": "PredictaBoard: Benchmarking LLM Score Predictability", "authors": ["Lorenzo Pacchiardi", "Konstantinos Voudouris", "Ben Slater", "Fernando Martínez-Plumed", "José Hernández-Orallo", "Lexin Zhou", "Wout Schellaert"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": "Accepted at ACL Findings 2025", "summary": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard"}
{"id": "2502.15910", "pdf": "https://arxiv.org/pdf/2502.15910.pdf", "abs": "https://arxiv.org/abs/2502.15910", "title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models", "authors": ["Zheyuan Liu", "Guangyao Dou", "Xiangchi Yuan", "Chunhui Zhang", "Zhaoxuan Tan", "Meng Jiang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility."}
{"id": "2502.17421", "pdf": "https://arxiv.org/pdf/2502.17421.pdf", "abs": "https://arxiv.org/abs/2502.17421", "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification", "authors": ["Penghui Yang", "Cunxiao Du", "Fengzhuo Zhang", "Haonan Wang", "Tianyu Pang", "Chao Du", "Bo An"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."}
{"id": "2502.19110", "pdf": "https://arxiv.org/pdf/2502.19110.pdf", "abs": "https://arxiv.org/abs/2502.19110", "title": "Conformal Linguistic Calibration: Trading-off between Factuality and Specificity", "authors": ["Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity."}
{"id": "2502.20620", "pdf": "https://arxiv.org/pdf/2502.20620.pdf", "abs": "https://arxiv.org/abs/2502.20620", "title": "Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning", "authors": ["Ayana Niwa", "Masahiro Kaneko", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "Accepted at ACL2025 Findings (long)", "summary": "Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability."}
{"id": "2503.04619", "pdf": "https://arxiv.org/pdf/2503.04619.pdf", "abs": "https://arxiv.org/abs/2503.04619", "title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Linhai Zhang", "Deyu Zhou", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews."}
{"id": "2503.06926", "pdf": "https://arxiv.org/pdf/2503.06926.pdf", "abs": "https://arxiv.org/abs/2503.06926", "title": "Effect of Selection Format on LLM Performance", "authors": ["Yuchen Han", "Yucheng Wu", "Jeffrey Willard"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.ET", "cs.LG"], "comment": null, "summary": "This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance."}
{"id": "2503.08669", "pdf": "https://arxiv.org/pdf/2503.08669.pdf", "abs": "https://arxiv.org/abs/2503.08669", "title": "SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints", "authors": ["Zekun Li", "Shinda Huang", "Jiangtian Wang", "Nathan Zhang", "Antonis Antoniades", "Wenyue Hua", "Kaijie Zhu", "Sirui Zeng", "Chi Wang", "William Yang Wang", "Xifeng Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Code, data, and over 24k agent trajectories are released at\n  https://github.com/Leezekun/SOPBench", "summary": "As language agents increasingly automate critical tasks, their ability to\nfollow domain-specific standard operating procedures (SOPs), policies, and\nconstraints when taking actions and making tool calls becomes essential yet\nremains underexplored. To address this gap, we develop an automated evaluation\npipeline SOPBench with: (1) executable environments containing 167\ntools/functions across seven customer service domains with service-specific\nSOPs and rule-based verifiers, (2) an automated test generation framework\nproducing over 900 verified test cases, and (3) an automated evaluation\nframework to rigorously assess agent adherence from multiple dimensions. Our\napproach transforms each service-specific SOP code program into a directed\ngraph of executable functions and requires agents to call these functions based\non natural language SOP descriptions. The original code serves as oracle\nrule-based verifiers to assess compliance, reducing reliance on manual\nannotations and LLM-based evaluations. We evaluate 18 leading models, and\nresults show the task is challenging even for top-tier models (like GPT-4o,\nClaude-3.7-Sonnet), with variances across domains. Reasoning models like\no4-mini-high show superiority while other powerful models perform less\neffectively (pass rates of 30%-50%), and small models (7B, 8B) perform\nsignificantly worse. Additionally, language agents can be easily jailbroken to\noverlook SOPs and constraints. Code, data, and over 24k agent trajectories are\nreleased at https://github.com/Leezekun/SOPBench."}
{"id": "2503.11593", "pdf": "https://arxiv.org/pdf/2503.11593.pdf", "abs": "https://arxiv.org/abs/2503.11593", "title": "Do Construction Distributions Shape Formal Language Learning In German BabyLMs?", "authors": ["Bastian Bunzeck", "Daniel Duran", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "Accepted at CoNNL 2025", "summary": "We analyze the influence of utterance-level construction distributions in\nGerman child-directed/child-available speech on the resulting word-level,\nsyntactic and semantic competence (and their underlying learning trajectories)\nin small LMs, which we train on a novel collection of developmentally plausible\nlanguage data for German. We find that trajectories are surprisingly robust for\nmarkedly different distributions of constructions in the training data, which\nhave little effect on final accuracies and almost no effect on global learning\ntrajectories. While syntax learning benefits from more complex utterances,\nword-level learning culminates in better scores with more fragmentary\nutterances. We argue that LMs trained on developmentally plausible data can\ncontribute to debates on how conducive different kinds of linguistic stimuli\nare to language learning."}
{"id": "2504.07738", "pdf": "https://arxiv.org/pdf/2504.07738.pdf", "abs": "https://arxiv.org/abs/2504.07738", "title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information", "authors": ["Andrea Loreti", "Kesi Chen", "Ruby George", "Robert Firth", "Adriano Agnello", "Shinnosuke Tanaka"], "categories": ["cs.CL"], "comment": null, "summary": "In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005.pdf", "abs": "https://arxiv.org/abs/2504.16005", "title": "CAPO: Cost-Aware Prompt Optimization", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automatic prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm\nachieves better performances already with smaller budgets, saves evaluations\nthrough racing, and decreases average prompt length via a length penalty,\nmaking it both cost-efficient and cost-aware. Even without few-shot examples,\nCAPO outperforms its competitors and generally remains robust to initial\nprompts. CAPO represents an important step toward making prompt optimization\nmore powerful and accessible by improving cost-efficiency."}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This version enhances the theoretical underpinnings of the proposed\n  Graph RAG methodology, including the introduction of a formal, FRBRoo-based\n  model for versioning, and enabling multi-language support for both content\n  and metadata", "summary": "This article proposes an adaptation of Graph Retrieval-Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms. Legal texts are characterized by a predefined hierarchical structure, an\nextensive network of references and a continuous evolution through multiple\ntemporal versions. This temporal dynamism poses a significant challenge for\nstandard AI systems, demanding a deterministic representation of the law at any\ngiven point in time. To address this, our approach grounds the knowledge graph\nconstruction in a formal, FRBRoo-inspired model that distinguishes abstract\nlegal works from their concrete textual expressions. We introduce a\nmulti-layered representation of Temporal Versions (capturing date-specific\nchanges) and Language Versions (capturing linguistic variations). By modeling\nnormative evolution as a precise sequence of these versioned entities, we\nenable the construction of a knowledge graph that serves as a verifiable\n\"ground truth\". This allows Large Language Models to generate responses based\non accurate, context-aware, and point-in-time correct legal information,\novercoming the risk of temporal inaccuracies. Through a detailed analysis of\nthis formal Graph RAG approach and its application to legal norm datasets, this\narticle aims to advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective and reliable systems in legal\nresearch, legislative analysis, and decision support."}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987.pdf", "abs": "https://arxiv.org/abs/2505.06987", "title": "Convert Language Model into a Value-based Strategic Planner", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."}
{"id": "2505.07897", "pdf": "https://arxiv.org/pdf/2505.07897.pdf", "abs": "https://arxiv.org/abs/2505.07897", "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows", "authors": ["Stefano Rando", "Luca Romani", "Alessio Sampieri", "Luca Franco", "John Yang", "Yuta Kyuragi", "Fabio Galasso", "Tatsunori Hashimoto"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Context lengths for models have grown rapidly, from thousands to millions of\ntokens in just a few years. The extreme context sizes of modern long-context\nmodels have made it difficult to construct realistic long-context benchmarks --\nnot only due to the cost of collecting million-context tasks but also in\nidentifying realistic scenarios that require significant contexts. We identify\ncode comprehension and repair as a natural testbed and challenge task for\nlong-context models and introduce LongCodeBench (LCB), a benchmark to test LLM\ncoding abilities in long-context scenarios. Our benchmark tests both the\ncomprehension and repair capabilities of LCLMs in realistic and important\nsettings by drawing from real-world GitHub issues and constructing QA\n(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the\ncomplexity of our benchmark, enabling us to evaluate models across different\nscales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.\nWe find that long-context remains a weakness for all models, with performance\ndrops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for\nQwen2.5."}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368.pdf", "abs": "https://arxiv.org/abs/2505.11368", "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368.pdf", "abs": "https://arxiv.org/abs/2505.12368", "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations. To demonstrate our framework's utility, we\ntrain CaptureGuard on our generated data. This new model drastically reduces\nboth false negative and false positive rates on our context-aware datasets\nwhile also generalizing effectively to external benchmarks, establishing a path\ntoward more robust and practical prompt injection defenses."}
{"id": "2505.17238", "pdf": "https://arxiv.org/pdf/2505.17238.pdf", "abs": "https://arxiv.org/abs/2505.17238", "title": "Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)", "authors": ["Clayton Cohn", "Surya Rayala", "Caitlin Snyder", "Joyce Fonteles", "Shruti Jain", "Naveeduddin Mohammed", "Umesh Timalsina", "Sarah K. Burriss", "Ashwin T S", "Namrata Srivastava", "Menton Deweese", "Angela Eeds", "Gautam Biswas"], "categories": ["cs.CL"], "comment": "To appear in the International Conference on Artificial Intelligence\n  in Education (AIED25) Workshop on Epistemics and Decision-Making in\n  AI-Supported Education", "summary": "Collaborative dialogue offers rich insights into students' learning and\ncritical thinking, which is essential for personalizing pedagogical agent\ninteractions in STEM+C settings. While large language models (LLMs) facilitate\ndynamic pedagogical interactions, hallucinations undermine confidence, trust,\nand instructional value. Retrieval-augmented generation (RAG) grounds LLM\noutputs in curated knowledge but requires a clear semantic link between user\ninput and a knowledge base, which is often weak in student dialogue. We propose\nlog-contextualized RAG (LC-RAG), which enhances RAG retrieval by using\nenvironment logs to contextualize collaborative discourse. Our findings show\nthat LC-RAG improves retrieval over a discourse-only baseline and allows our\ncollaborative peer agent, Copa, to deliver relevant, personalized guidance that\nsupports students' critical thinking and epistemic decision-making in a\ncollaborative computational modeling environment, C2STEM."}
{"id": "2505.20613", "pdf": "https://arxiv.org/pdf/2505.20613.pdf", "abs": "https://arxiv.org/abs/2505.20613", "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64)."}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854.pdf", "abs": "https://arxiv.org/abs/2506.00854", "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese."}
{"id": "2506.01565", "pdf": "https://arxiv.org/pdf/2506.01565.pdf", "abs": "https://arxiv.org/abs/2506.01565", "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation", "authors": ["Li Zhou", "Lutong Yu", "Dongchu Xie", "Shaohuan Cheng", "Wenyan Li", "Haizhou Li"], "categories": ["cs.CL", "cs.CV"], "comment": "cultural analysis, cultural visual understanding, cultural image\n  transcreation (update dataset license)", "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation."}
{"id": "2506.04079", "pdf": "https://arxiv.org/pdf/2506.04079.pdf", "abs": "https://arxiv.org/abs/2506.04079", "title": "EuroLLM-9B: Technical Report", "authors": ["Pedro Henrique Martins", "João Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "José Pombal", "Nicolas Boizard", "Manuel Faysse", "Pierre Colombo", "François Yvon", "Barry Haddow", "José G. C. de Souza", "Alexandra Birch", "André F. T. Martins"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "56 pages", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset."}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801.pdf", "abs": "https://arxiv.org/abs/2506.07801", "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks."}
{"id": "2506.10055", "pdf": "https://arxiv.org/pdf/2506.10055.pdf", "abs": "https://arxiv.org/abs/2506.10055", "title": "TaskCraft: Automated Generation of Agentic Tasks", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Liu", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation."}
{"id": "2506.11681", "pdf": "https://arxiv.org/pdf/2506.11681.pdf", "abs": "https://arxiv.org/abs/2506.11681", "title": "A Hybrid Multi-Agent Prompting Approach for Simplifying Complex Sentences", "authors": ["Pratibha Zunjare", "Michael Hsiao"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task."}
{"id": "2506.12674", "pdf": "https://arxiv.org/pdf/2506.12674.pdf", "abs": "https://arxiv.org/abs/2506.12674", "title": "Enhancing Clinical Models with Pseudo Data for De-identification", "authors": ["Paul Landes", "Aaron J Chaise", "Tarak Nath Nandi", "Ravi K Madduri"], "categories": ["cs.CL"], "comment": null, "summary": "Many models are pretrained on redacted text for privacy reasons. Clinical\nfoundation models are often trained on de-identified text, which uses special\nsyntax (masked) text in place of protected health information. Even though\nthese models have increased in popularity, there has been little effort in\nunderstanding the effects of training them on redacted text. In this work, we\npretrain several encoder-only models on a dataset that contains redacted text\nand a version with replaced realistic pseudo text. We then fine-tuned models\nfor the protected health information de-identification task and show how our\nmethods significantly outperform previous baselines. The contributions of this\nwork include: a) our novel, and yet surprising findings with training\nrecommendations, b) redacted text replacements used to produce the pseudo\ndataset, c) pretrained embeddings and fine-tuned task specific models, and d)\nfreely available pseudo training dataset generation and model source code used\nin our experiments."}
{"id": "2506.12796", "pdf": "https://arxiv.org/pdf/2506.12796.pdf", "abs": "https://arxiv.org/abs/2506.12796", "title": "Surprise Calibration for Better In-Context Learning", "authors": ["Zhihang Tan", "Jingrui Hou", "Ping Wang", "Qibiao Hu", "Peng Zhu"], "categories": ["cs.CL", "I.2.7"], "comment": "16 pages, 11 figures", "summary": "In-context learning (ICL) has emerged as a powerful paradigm for task\nadaptation in large language models (LLMs), where models infer underlying task\nstructures from a few demonstrations. However, ICL remains susceptible to\nbiases that arise from prior knowledge and contextual demonstrations, which can\ndegrade the performance of LLMs. Existing bias calibration methods typically\napply fixed class priors across all inputs, limiting their efficacy in dynamic\nICL settings where the context for each query differs. To address these\nlimitations, we adopt implicit sequential Bayesian inference as a framework for\ninterpreting ICL, identify \"surprise\" as an informative signal for class prior\nshift, and introduce a novel method--Surprise Calibration (SC). SC leverages\nthe notion of surprise to capture the temporal dynamics of class priors,\nproviding a more adaptive and computationally efficient solution for in-context\nlearning. We empirically demonstrate the superiority of SC over existing bias\ncalibration techniques across a range of benchmark natural language processing\ntasks."}
{"id": "2506.13172", "pdf": "https://arxiv.org/pdf/2506.13172.pdf", "abs": "https://arxiv.org/abs/2506.13172", "title": "AI-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns", "authors": ["Evgeny Markhasin"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in the high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks within academic summaries (abstracts and conclusions):\nidentifying unsubstantiated claims (informational integrity) and flagging\nsemantically confusing ambiguous pronoun references (linguistic clarity). We\nconducted a systematic, multi-run evaluation on two frontier models (Gemini Pro\n2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for\nthe informational integrity task reveal a significant divergence in model\nperformance: while both models successfully identified an unsubstantiated head\nof a noun phrase (95% success), ChatGPT consistently failed (0% success) to\nidentify an unsubstantiated adjectival modifier that Gemini correctly flagged\n(95% success), raising a question regarding the potential influence of the\ntarget's syntactic role. For the linguistic analysis task, both models\nperformed well (80-90% success) with full manuscript context. Surprisingly, in\na summary-only setting, Gemini's performance was substantially degraded, while\nChatGPT achieved a perfect (100%) success rate. Our findings suggest that while\nstructured prompting is a viable methodology for complex textual analysis,\nprompt performance may be highly dependent on the interplay between the model,\ntask type, and context, highlighting the need for rigorous, model-specific\ntesting."}
{"id": "2506.13300", "pdf": "https://arxiv.org/pdf/2506.13300.pdf", "abs": "https://arxiv.org/abs/2506.13300", "title": "Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models", "authors": ["Bo Li", "Chengben Xu", "Wufeng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints."}
{"id": "2506.13366", "pdf": "https://arxiv.org/pdf/2506.13366.pdf", "abs": "https://arxiv.org/abs/2506.13366", "title": "Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction", "authors": ["Didi Zhang", "Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Goal-oriented proactive dialogue systems are designed to guide user\nconversations seamlessly towards specific objectives by planning a\ngoal-oriented path. However, previous research has focused predominantly on\noptimizing these paths while neglecting the inconsistencies that may arise\nbetween generated responses and dialogue contexts, including user profiles,\ndialogue history, domain knowledge, and subgoals. To address this issue, we\nintroduce a model-agnostic two-stage Consistency Reflection and Correction\n(CRC) framework. Specifically, in the consistency reflection stage, the model\nis prompted to reflect on the discrepancies between generated responses and\ndialogue contexts, identifying inconsistencies and suggesting possible\ncorrections. In the consistency correction stage, the model generates responses\nthat are more consistent with the dialogue context based on these reflection\nresults. We conducted experiments on various model architectures with different\nparameter sizes, including encoder-decoder models (BART, T5) and decoder-only\nmodels (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental\nresults on three datasets demonstrate that our CRC framework significantly\nimproves the consistency between generated responses and dialogue contexts."}
{"id": "2506.13472", "pdf": "https://arxiv.org/pdf/2506.13472.pdf", "abs": "https://arxiv.org/abs/2506.13472", "title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models", "authors": ["Junho Yoon", "Geom Lee", "Donghyeon Jeon", "Inho Kang", "Seung-Hoon Na"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures", "summary": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64."}
{"id": "2506.13674", "pdf": "https://arxiv.org/pdf/2506.13674.pdf", "abs": "https://arxiv.org/abs/2506.13674", "title": "Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention", "authors": ["Haonan Wang", "Brian Chen", "Siquan Li", "Xinhe Liang", "Hwee Kuan Lee", "Kenji Kawaguchi", "Tianyang Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation."}
{"id": "2308.12420", "pdf": "https://arxiv.org/pdf/2308.12420.pdf", "abs": "https://arxiv.org/abs/2308.12420", "title": "Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature", "authors": ["Walter Hernandez Cruz", "Kamil Tylinski", "Alastair Moore", "Niall Roche", "Nikhil Vadgama", "Horst Treiblmaier", "Jiangbo Shangguan", "Paolo Tasca", "Jiahua Xu"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,\nparticularly concerning the energy consumption of the Proof of Work (PoW)\nconsensus mechanism and broader Environmental, Social, and Governance (ESG)\nissues. However, existing systematic literature reviews of DLT rely on limited\nanalyses of citations, abstracts, and keywords, failing to fully capture the\nfield's complexity and ESG concerns. We address these challenges by analyzing\nthe full text of 24,539 publications using Natural Language Processing (NLP)\nwith our manually labeled Named Entity Recognition (NER) dataset of 39,427\nentities for DLT. This methodology identified 505 key publications at the\nDLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP\nand temporal graph analysis reveals critical trends in DLT evolution and ESG\nimpacts, including cryptography and peer-to-peer networks research's\nfoundational influence, Bitcoin's persistent impact on research and\nenvironmental concerns (a \"Lindy effect\"), Ethereum's catalytic role on Proof\nof Stake (PoS) and smart contract adoption, and the industry's progressive\nshift toward energy-efficient consensus mechanisms. Our contributions include\nthe first DLT-specific NER dataset addressing the scarcity of high-quality\nlabeled NLP data in blockchain research, a methodology integrating NLP and\ntemporal graph analysis for large-scale interdisciplinary literature reviews,\nand the first NLP-driven literature review focusing on DLT's ESG aspects."}
{"id": "2406.11423", "pdf": "https://arxiv.org/pdf/2406.11423.pdf", "abs": "https://arxiv.org/abs/2406.11423", "title": "Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains", "authors": ["Evan M. Williams", "Peter Carragher", "Kathleen M. Carley"], "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms."}
{"id": "2406.14986", "pdf": "https://arxiv.org/pdf/2406.14986.pdf", "abs": "https://arxiv.org/abs/2406.14986", "title": "Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers", "authors": ["Manuel Mondal", "Ljiljana Dolamic", "Gérôme Bovet", "Philippe Cudré-Mauroux", "Julien Audiffren"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Multiple Choice Questions (MCQ) have become a commonly used approach to\nassess the capabilities of Large Language Models (LLMs), due to their ease of\nmanipulation and evaluation. The experimental appraisals of the LLMs' Stated\nAnswer (their answer to MCQ) have pointed to their apparent ability to perform\nprobabilistic reasoning or to grasp uncertainty. In this work, we investigate\nwhether these aptitudes are measurable outside tailored prompting and MCQ by\nreformulating these issues as direct text-completion - the fundamental\ncomputational unit of LLMs. We introduce Revealed Belief, an evaluation\nframework that evaluates LLMs on tasks requiring reasoning under uncertainty,\nwhich complements MCQ scoring by analyzing text-completion probability\ndistributions. Our findings suggest that while LLMs frequently state the\ncorrect answer, their Revealed Belief shows that they often allocate\nprobability mass inconsistently, exhibit systematic biases, and often fail to\nupdate their beliefs appropriately when presented with new evidence, leading to\nstrong potential impacts on downstream tasks. These results suggest that common\nevaluation methods may only provide a partial picture and that more research is\nneeded to assess the extent and nature of their capabilities."}
{"id": "2407.05674", "pdf": "https://arxiv.org/pdf/2407.05674.pdf", "abs": "https://arxiv.org/abs/2407.05674", "title": "Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets", "authors": ["Harshit Joshi", "Shicheng Liu", "James Chen", "Robert Weigle", "Monica S. Lam"], "categories": ["cs.AI", "cs.CL", "cs.PL"], "comment": "Accepted at ACL 2025", "summary": "Large Language Models can carry out human-like conversations in diverse\nsettings, responding to user requests for tasks and knowledge. However,\nexisting conversational agents implemented with LLMs often struggle with\nhallucination, following instructions with conditional logic, and integrating\nknowledge from different sources. These shortcomings compromise the agents'\neffectiveness, rendering them unsuitable for deployment. To address these\nchallenges, we introduce Genie, a programmable framework for creating\nknowledge-intensive task-oriented conversational agents. Genie can handle\ninvolved interactions and answer complex queries. Unlike LLMs, it delivers\nreliable, grounded responses through advanced dialogue state management and\nsupports controllable agent policies via its declarative specification -- Genie\nWorksheet. This is achieved through an algorithmic runtime system that\nimplements the developer-supplied policy, limiting LLMs to (1) parse user input\nusing a succinct conversational history, and (2) generate responses according\nto supplied context. Agents built with Genie outperform SOTA methods on complex\nlogic dialogue datasets. We conducted a user study with 62 participants on\nthree real-life applications: restaurant reservations with Yelp, as well as\nticket submission and course enrollment for university students. Genie agents\nwith GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling,\nimproving goal completion rates from 21.8% to 82.8% across three real-world\ntasks."}
{"id": "2410.05243", "pdf": "https://arxiv.org/pdf/2410.05243.pdf", "abs": "https://arxiv.org/abs/2410.05243", "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents", "authors": ["Boyu Gou", "Ruohan Wang", "Boyuan Zheng", "Yanan Xie", "Cheng Chang", "Yiheng Shu", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted to ICLR 2025 (Oral). Project Homepage:\n  https://osu-nlp-group.github.io/UGround/", "summary": "Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly perform\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do."}
{"id": "2412.06745", "pdf": "https://arxiv.org/pdf/2412.06745.pdf", "abs": "https://arxiv.org/abs/2412.06745", "title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities", "authors": ["Adhiraj Ghosh", "Sebastian Dziadzio", "Ameya Prabhu", "Vishaal Udandarao", "Samuel Albanie", "Matthias Bethge"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models."}
{"id": "2501.04227", "pdf": "https://arxiv.org/pdf/2501.04227.pdf", "abs": "https://arxiv.org/abs/2501.04227", "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Michael Moor", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery."}
{"id": "2501.18045", "pdf": "https://arxiv.org/pdf/2501.18045.pdf", "abs": "https://arxiv.org/abs/2501.18045", "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors", "authors": ["Myra Cheng", "Angela Y. Lee", "Kristina Rapuano", "Kate Niederhoffer", "Alex Liebscher", "Jeffrey Hancock"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025", "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development."}
{"id": "2502.17514", "pdf": "https://arxiv.org/pdf/2502.17514.pdf", "abs": "https://arxiv.org/abs/2502.17514", "title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment", "authors": ["Hantao Lou", "Changye Li", "Jiaming Ji", "Yaodong Yang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 13 figures", "summary": "With the integration of image modality, the semantic space of multimodal\nlarge language models (MLLMs) is more complex than text-only models, making\ntheir interpretability more challenging and their alignment less stable,\nparticularly susceptible to low-quality data, which can lead to inconsistencies\nbetween modalities, hallucinations, and biased outputs. As a result, developing\ninterpretability methods for MLLMs is crucial for improving alignment quality\nand efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained\nattention for their ability to interpret latent representations. However,\nextending SAEs to multimodal settings presents new challenges due to modality\nfusion and the difficulty of isolating cross-modal representations. To address\nthese challenges, we introduce SAE-V, a mechanistic interpretability framework\nthat extends the SAE paradigm to MLLMs. By identifying and analyzing\ninterpretable features along with their corresponding data, SAE-V enables\nfine-grained interpretation of both model behavior and data quality,\nfacilitating a deeper understanding of cross-modal interactions and alignment\ndynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides\nan intrinsic data filtering mechanism to enhance model alignment without\nrequiring additional models. Specifically, when applied to the alignment\nprocess of MLLMs, SAE-V-based data filtering methods could achieve more than\n110% performance with less than 50% data. Our results highlight SAE-V's ability\nto enhance interpretability and alignment in MLLMs, providing insights into\ntheir internal mechanisms."}
{"id": "2502.18770", "pdf": "https://arxiv.org/pdf/2502.18770.pdf", "abs": "https://arxiv.org/abs/2502.18770", "title": "Reward Shaping to Mitigate Reward Hacking in RLHF", "authors": ["Jiayi Fu", "Xuandong Zhao", "Chengyuan Yao", "Heng Wang", "Qi Han", "Yanghua Xiao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "24 pages", "summary": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\n\\emph{reward hacking}, where the agent exploits flaws in the reward function\nrather than learning the intended behavior, thus degrading alignment. Although\nreward shaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests two key\ndesign principles: (1) the RL reward should be bounded, and (2) the RL reward\nbenefits from rapid initial growth followed by gradual convergence. Guided by\nthese insights, we propose Preference As Reward (PAR), a novel approach that\nleverages the latent preferences embedded within the reward model as the signal\nfor reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. The code is available at\nhttps://github.com/PorUna-byte/PAR, and the Work done during the internship at\nStepFun by Jiayi Fu."}
{"id": "2503.04804", "pdf": "https://arxiv.org/pdf/2503.04804.pdf", "abs": "https://arxiv.org/abs/2503.04804", "title": "What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text", "authors": ["Arturs Kanepajs", "Aditi Basu", "Sankalpa Ghose", "Constance Li", "Akshat Mehta", "Ronak Mehta", "Samuel David Tucker-Davis", "Eric Zhou", "Bob Fischer", "Jacy Reese Anthis"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "As machine learning systems become increasingly embedded in society, their\nimpact on human and nonhuman life continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present\nAnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated\ntext. Our benchmark dataset comprises 1,850 curated questions from Reddit post\ntitles and 2,500 synthetic questions based on 50 animal categories (e.g., cats,\nreptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios\ninclude open-ended questions about how to treat animals, practical scenarios\nwith potential animal harm, and willingness-to-pay measures for the prevention\nof animal harm. Using the LLM-as-a-judge framework, responses are evaluated for\ntheir potential to increase or decrease harm, and evaluations are debiased for\nthe tendency of judges to judge their own outputs more favorably. AHB reveals\nsignificant differences across frontier LLMs, animal categories, scenarios, and\nsubreddits. We conclude with future directions for technical research and\naddressing the challenges of building evaluations on complex social and moral\ntopics."}
{"id": "2503.07631", "pdf": "https://arxiv.org/pdf/2503.07631.pdf", "abs": "https://arxiv.org/abs/2503.07631", "title": "OWLViz: An Open-World Benchmark for Visual Question Answering", "authors": ["Thuy Nguyen", "Dang Nguyen", "Hoang Nguyen", "Thuan Luong", "Long Hoang Dang", "Viet Dac Lai"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025 Workshop on Multi-Agent Systems in the Era of Foundation\n  Models: Opportunities, Challenges, and Futures. (8 pages + appendix)", "summary": "We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research."}
{"id": "2503.08679", "pdf": "https://arxiv.org/pdf/2503.08679.pdf", "abs": "https://arxiv.org/abs/2503.08679", "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful", "authors": ["Iván Arcuschin", "Jett Janiak", "Robert Krzyzanowski", "Senthooran Rajamanoharan", "Neel Nanda", "Arthur Conmy"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages", "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful when models face an explicit bias in their prompts, i.e., the\nCoT can give an incorrect picture of how models arrive at conclusions. We go\nfurther and show that unfaithful CoT can also occur on realistic prompts with\nno artificial bias. We find that when separately presented with the questions\n\"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce\nsuperficially coherent arguments to justify systematically answering Yes to\nboth questions or No to both questions, despite such responses being logically\ncontradictory. We show preliminary evidence that this is due to models'\nimplicit biases towards Yes or No, thus labeling this unfaithfulness as\nImplicit Post-Hoc Rationalization. Our results reveal that several production\nmodels exhibit surprisingly high rates of post-hoc rationalization in our\nsettings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more\nfaithful, especially thinking ones, none are entirely faithful: Gemini 2.5\nFlash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%),\nand Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical\nShortcuts, where models use subtly illogical reasoning to try to make a\nspeculative answer to hard maths problems seem rigorously proven. Our findings\nraise challenges for strategies for detecting undesired behavior in LLMs via\nthe chain of thought."}
{"id": "2503.16974", "pdf": "https://arxiv.org/pdf/2503.16974.pdf", "abs": "https://arxiv.org/abs/2503.16974", "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks", "authors": ["Julian Junyan Wang", "Victor Xiaoqi Wang"], "categories": ["q-fin.GN", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": "89 pages, 20 tables, 15 figures", "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks."}
{"id": "2504.03255", "pdf": "https://arxiv.org/pdf/2504.03255.pdf", "abs": "https://arxiv.org/abs/2504.03255", "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective", "authors": ["Garry A. Gabison", "R. Patrick Xian"], "categories": ["cs.CY", "cs.CL", "cs.MA"], "comment": "22 pages (incl. appendix), accepted at REALM workshop, ACL2025", "summary": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention to effective governance policies,\nmonitoring, and control protocols. Based on the emerging landscape of the\nagentic market, we analyze potential liability issues arising from the\ndelegated use of LLM agents and their extended systems through a\nprincipal-agent perspective. Our analysis complements existing risk-based\nstudies on artificial agency and covers the spectrum of important aspects of\nthe principal-agent relationship and their potential consequences at\ndeployment. Furthermore, we motivate method developments for technical\ngovernance along the directions of interpretability and behavior evaluations,\nreward and conflict management, and the mitigation of misalignment and\nmisconduct through principled engineering of detection and fail-safe\nmechanisms. By illustrating the outstanding issues in AI liability for\nLLM-based agentic systems, we aim to inform the system design, auditing, and\ntracing to enhance transparency and liability attribution."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442.pdf", "abs": "https://arxiv.org/abs/2505.12442", "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227.pdf", "abs": "https://arxiv.org/abs/2505.13227", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.20612", "pdf": "https://arxiv.org/pdf/2505.20612.pdf", "abs": "https://arxiv.org/abs/2505.20612", "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally. Project Page:\n  https://rf100-vl.org/", "summary": "Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025\nFoundational FSOD competition and share insights from the community. Notably,\nthe winning team significantly outperforms our baseline by 16.8 mAP! Our code\nand dataset are available at https://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/"}
{"id": "2506.00555", "pdf": "https://arxiv.org/pdf/2506.00555.pdf", "abs": "https://arxiv.org/abs/2506.00555", "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "authors": ["Peng Xia", "Jinglu Wang", "Yibo Peng", "Kaide Zeng", "Xian Wu", "Xiangru Tang", "Hongtu Zhu", "Yun Li", "Shujie Liu", "Yan Lu", "Huaxiu Yao"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 20.7% over supervised\nfine-tuning baselines."}
{"id": "2506.01391", "pdf": "https://arxiv.org/pdf/2506.01391.pdf", "abs": "https://arxiv.org/abs/2506.01391", "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "I.2.8; I.2.7; I.2.10; H.5.2"], "comment": "Updated results in Table 2 and Table 3; The project is available at\n  https://github.com/OpenBMB/AgentCPM-GUI", "summary": "The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data."}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413.pdf", "abs": "https://arxiv.org/abs/2506.01413", "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "13 pages of main body, 3 tables, 5 figures, 45 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions"}
{"id": "2506.08001", "pdf": "https://arxiv.org/pdf/2506.08001.pdf", "abs": "https://arxiv.org/abs/2506.08001", "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Schölkopf", "Weiyang Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report v3 (38 pages, 26 figures, project page:\n  https://spherelab.ai/poet/, v3: added singular spectrum and energy analyses\n  in Section 4)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs."}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081.pdf", "abs": "https://arxiv.org/abs/2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Jin-Ge Yao", "Bowen Qin", "Richeng Xuan", "Xi Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM."}
{"id": "2506.10274", "pdf": "https://arxiv.org/pdf/2506.10274.pdf", "abs": "https://arxiv.org/abs/2506.10274", "title": "Discrete Audio Tokens: More Than a Survey!", "authors": ["Pooneh Mousavi", "Gallil Maimon", "Adel Moumen", "Darius Petermann", "Jiatong Shi", "Haibin Wu", "Haici Yang", "Anastasia Kuznetsova", "Artem Ploujnikov", "Ricard Marxer", "Bhuvana Ramabhadran", "Benjamin Elizalde", "Loren Lugosch", "Jinyu Li", "Cem Subakan", "Phil Woodland", "Minje Kim", "Hung-yi Lee", "Shinji Watanabe", "Yossi Adi", "Mirco Ravanelli"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks. They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/."}
{"id": "2506.12376", "pdf": "https://arxiv.org/pdf/2506.12376.pdf", "abs": "https://arxiv.org/abs/2506.12376", "title": "ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities", "authors": ["Zhaochen Hong", "Haofei Yu", "Jiaxuan You"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Evaluating consistency in large language models (LLMs) is crucial for\nensuring reliability, particularly in complex, multi-step interactions between\nhumans and LLMs. Traditional self-consistency methods often miss subtle\nsemantic changes in natural language and functional shifts in code or\nequations, which can accumulate over multiple transformations. To address this,\nwe propose ConsistencyChecker, a tree-based evaluation framework designed to\nmeasure consistency through sequences of reversible transformations, including\nmachine translation tasks and AI-assisted programming tasks. In our framework,\nnodes represent distinct text states, while edges correspond to pairs of\ninverse operations. Dynamic and LLM-generated benchmarks ensure a fair\nassessment of the model's generalization ability and eliminate benchmark\nleakage. Consistency is quantified based on similarity across different depths\nof the transformation tree. Experiments on eight models from various families\nand sizes show that ConsistencyChecker can distinguish the performance of\ndifferent models. Notably, our consistency scores-computed entirely without\nusing WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking,\ndemonstrating the validity of our benchmark-free approach. Our implementation\nis available at: https://github.com/ulab-uiuc/consistencychecker."}
{"id": "2506.13277", "pdf": "https://arxiv.org/pdf/2506.13277.pdf", "abs": "https://arxiv.org/abs/2506.13277", "title": "SeqPE: Transformer with Sequential Position Encoding", "authors": ["Huayang Li", "Yahui Liu", "Hongyu Sun", "Deng Cai", "Leyang Cui", "Wei Bi", "Peilin Zhao", "Taro Watanabe"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe."}
