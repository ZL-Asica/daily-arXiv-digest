{"id": "2505.01520", "pdf": "https://arxiv.org/pdf/2505.01520.pdf", "abs": "https://arxiv.org/abs/2505.01520", "title": "Content and Quality Analysis of Parent-Facing Applications for Feeding Children with Autism Spectrum Disorder", "authors": ["Christopher Cofie Kuzagbe", "Fabrice Mukarage", "Skye Nandi Adams", "N'guessan Yves-Roland Douha", "Edith Talina Luhanga"], "categories": ["cs.HC", "cs.CY"], "comment": "8 pages, 1 figure, 5 tables", "summary": "Approximately 1 in 100 children worldwide are diagnosed with Autism Spectrum\nDisorder (ASD), and 46% to 89% experience significant feeding difficulties.\nAlthough mobile health (mHealth) applications offer potential support for\ncaregivers, the quality and relevance of apps targeting autism-related feeding\nissues remain unclear. This systematic review evaluated mobile applications\navailable on the Apple App Store and the Google Play Store between September\nand October 2024. The searches were carried out using 15 predefined terms\n(e.g., \"child autism feeding\", \"child autism food\"). Applications were eligible\nif they were in English, free to download, updated within the past year,\nexplicitly addressed feeding in children with autism, accessible in Africa, and\nhad more than 100 downloads. Of the 326 apps identified, only two iOS\napplications met all inclusion criteria; no Android apps qualified. Behavior\nChange Wheel (BCW) analysis showed that the selected applications incorporated\nmultiple intervention functions, such as education, training, enablement,\nincentivization, and modeling, though none addressed the full spectrum of\nbehavioral strategies. Mobile App Rating Scale (MARS) indicated moderate to\nhigh usability, with features such as sensory-friendly food routines and\nstructured caregiver tools. However, both apps lacked clinical validation and\ncomprehensive customization. These findings highlight a critical gap in the\navailability of evidence-based high-quality mHealth tools for caregivers\nmanaging ASD-related feeding challenges and underscore the need for\nprofessionally developed and culturally sensitive digital solutions."}
{"id": "2505.01537", "pdf": "https://arxiv.org/pdf/2505.01537.pdf", "abs": "https://arxiv.org/abs/2505.01537", "title": "Passing the Buck to AI: How Individuals' Decision-Making Patterns Affect Reliance on AI", "authors": ["Katelyn Xiaoying Mei", "Rock Yuren Pang", "Alex Lyford", "Lucy Lu Wang", "Katharina Reinecke"], "categories": ["cs.HC"], "comment": null, "summary": "Psychological research has identified different patterns individuals have\nwhile making decisions, such as vigilance (making decisions after thorough\ninformation gathering), hypervigilance (rushed and anxious decision-making),\nand buckpassing (deferring decisions to others). We examine whether these\ndecision-making patterns shape peoples' likelihood of seeking out or relying on\nAI. In an online experiment with 810 participants tasked with distinguishing\nfood facts from myths, we found that a higher buckpassing tendency was\npositively correlated with both seeking out and relying on AI suggestions,\nwhile being negatively correlated with the time spent reading AI explanations.\nIn contrast, the higher a participant tended towards vigilance, the more\ncarefully they scrutinized the AI's information, as indicated by an increased\ntime spent looking through the AI's explanations. These findings suggest that a\nperson's decision-making pattern plays a significant role in their adoption and\nreliance on AI, which provides a new understanding of individual differences in\nAI-assisted decision-making."}
{"id": "2505.01542", "pdf": "https://arxiv.org/pdf/2505.01542.pdf", "abs": "https://arxiv.org/abs/2505.01542", "title": "Emotions in the Loop: A Survey of Affective Computing for Emotional Support", "authors": ["Karishma Hegde", "Hemadri Jayalath"], "categories": ["cs.HC", "cs.AI", "I.2.10; I.2.7; H.5.2"], "comment": "20 pages, 7 tables, 96 references. Survey paper on affective\n  computing applications using large language models, multimodal AI, and\n  therapeutic chatbots", "summary": "In a world where technology is increasingly embedded in our everyday\nexperiences, systems that sense and respond to human emotions are elevating\ndigital interaction. At the intersection of artificial intelligence and\nhuman-computer interaction, affective computing is emerging with innovative\nsolutions where machines are humanized by enabling them to process and respond\nto user emotions. This survey paper explores recent research contributions in\naffective computing applications in the area of emotion recognition, sentiment\nanalysis and personality assignment developed using approaches like large\nlanguage models (LLMs), multimodal techniques, and personalized AI systems. We\nanalyze the key contributions and innovative methodologies applied by the\nselected research papers by categorizing them into four domains: AI chatbot\napplications, multimodal input systems, mental health and therapy applications,\nand affective computing for safety applications. We then highlight the\ntechnological strengths as well as the research gaps and challenges related to\nthese studies. Furthermore, the paper examines the datasets used in each study,\nhighlighting how modality, scale, and diversity impact the development and\nperformance of affective models. Finally, the survey outlines ethical\nconsiderations and proposes future directions to develop applications that are\nmore safe, empathetic and practical."}
{"id": "2505.01601", "pdf": "https://arxiv.org/pdf/2505.01601.pdf", "abs": "https://arxiv.org/abs/2505.01601", "title": "Beyond Productivity: Rethinking the Impact of Creativity Support Tools", "authors": ["Samuel Rhys Cox", "Helena Bøjer Djernæs", "Niels van Berkel"], "categories": ["cs.HC", "cs.MM"], "comment": "In ACM Creativity and Cognition (C&C '25), June 23-25, 2025; 15\n  pages; 2 Figures; 3 Tables", "summary": "Creativity Support Tools (CSTs) are widely used across diverse creative\ndomains, with generative AI recently increasing the abilities of CSTs. To\nbetter understand how the success of CSTs is determined in the literature, we\nconducted a review of outcome measures used in CST evaluations. Drawing from\n(n=173) CST evaluations in the ACM Digital Library, we identified the metrics\ncommonly employed to assess user interactions with CSTs. Our findings reveal\nprevailing trends in current evaluation practices, while exposing underexplored\nmeasures that could broaden the scope of future research. Based on these\nresults, we argue for a more holistic approach to evaluating CSTs, encouraging\nthe HCI community to consider not only user experience and the quality of the\ngenerated output, but also user-centric aspects such as self-reflection and\nwell-being as critical dimensions of assessment. We also highlight a need for\nvalidated measures specifically suited to the evaluation of generative AI in\nCSTs."}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456.pdf", "abs": "https://arxiv.org/abs/2505.01456", "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs."}
{"id": "2505.01648", "pdf": "https://arxiv.org/pdf/2505.01648.pdf", "abs": "https://arxiv.org/abs/2505.01648", "title": "Interaction Configurations and Prompt Guidance in Conversational AI for Question Answering in Human-AI Teams", "authors": ["Jaeyoon Song", "Zahra Ashktorab", "Qian Pan", "Casey Dugan", "Werner Geyer", "Thomas W. Malone"], "categories": ["cs.HC"], "comment": "This paper has been accepted at CSCW 2025", "summary": "Understanding the dynamics of human-AI interaction in question answering is\ncrucial for enhancing collaborative efficiency. Extending from our initial\nformative study, which revealed challenges in human utilization of\nconversational AI support, we designed two configurations for prompt guidance:\na Nudging approach, where the AI suggests potential responses for human agents,\nand a Highlight strategy, emphasizing crucial parts of reference documents to\naid human responses. Through two controlled experiments, the first involving 31\nparticipants and the second involving 106 participants, we compared these\nconfigurations against traditional human-only approaches, both with and without\nAI assistance. Our findings suggest that effective human-AI collaboration can\nenhance response quality, though merely combining human and AI efforts does not\nensure improved outcomes. In particular, the Nudging configuration was shown to\nhelp improve the quality of the output when compared to AI alone. This paper\ndelves into the development of these prompt guidance paradigms, offering\ninsights for refining human-AI collaborations in conversational\nquestion-answering contexts and contributing to a broader understanding of\nhuman perceptions and expectations in AI partnerships."}
{"id": "2505.01459", "pdf": "https://arxiv.org/pdf/2505.01459.pdf", "abs": "https://arxiv.org/abs/2505.01459", "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces MoxE, a novel architecture that synergistically\ncombines the Extended Long Short-Term Memory (xLSTM) with the Mixture of\nExperts (MoE) framework to address critical scalability and efficiency\nchallenges in large language models (LLMs). The proposed method effectively\nleverages xLSTM's innovative memory structures while strategically introducing\nsparsity through MoE to substantially reduce computational overhead. At the\nheart of our approach is a novel entropy-based routing mechanism, designed to\ndynamically route tokens to specialized experts, thereby ensuring efficient and\nbalanced resource utilization. This entropy awareness enables the architecture\nto effectively manage both rare and common tokens, with mLSTM blocks being\nfavored to handle rare tokens. To further enhance generalization, we introduce\na suite of auxiliary losses, including entropy-based and group-wise balancing\nlosses, ensuring robust performance and efficient training. Theoretical\nanalysis and empirical evaluations rigorously demonstrate that MoxE achieves\nsignificant efficiency gains and enhanced effectiveness compared to existing\napproaches, marking a notable advancement in scalable LLM architectures."}
{"id": "2505.01678", "pdf": "https://arxiv.org/pdf/2505.01678.pdf", "abs": "https://arxiv.org/abs/2505.01678", "title": "AI-Based Speaking Assistant: Supporting Non-Native Speakers' Speaking in Real-Time Multilingual Communication", "authors": ["Peinuan Qin", "Zicheng Zhu", "Naomi Yamashita", "Yitian Yang", "Keita Suga", "Yi-Chieh Lee"], "categories": ["cs.HC"], "comment": "Accepted to ACM CSCW 2025", "summary": "Non-native speakers (NNSs) often face speaking challenges in real-time\nmultilingual communication, such as struggling to articulate their thoughts. To\naddress this issue, we developed an AI-based speaking assistant (AISA) that\nprovides speaking references for NNSs based on their input queries, task\nbackground, and conversation history. To explore NNSs' interaction with AISA\nand its impact on NNSs' speaking during real-time multilingual communication,\nwe conducted a mixed-method study involving a within-subject experiment and\nfollow-up interviews. In the experiment, two native speakers (NSs) and one NNS\nformed a team (31 teams in total) and completed two collaborative tasks--one\nwith access to the AISA and one without. Overall, our study revealed four types\nof AISA input patterns among NNSs, each reflecting different levels of effort\nand language preferences. Although AISA did not improve NNSs' speaking\ncompetence, follow-up interviews revealed that it helped improve the logical\nflow and depth of their speech. Moreover, the additional multitasking\nintroduced by AISA, such as entering and reviewing system output, potentially\nelevated NNSs' workload and anxiety. Based on these observations, we discuss\nthe pros and cons of implementing tools to assist NNS in real-time multilingual\ncommunication and offer design recommendations."}
{"id": "2505.01479", "pdf": "https://arxiv.org/pdf/2505.01479.pdf", "abs": "https://arxiv.org/abs/2505.01479", "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation", "authors": ["Siheng Xiong", "Jieyu Zhou", "Zhangding Liu", "Yusen Su"], "categories": ["cs.CL"], "comment": null, "summary": "Planning remains a core challenge for language models (LMs), particularly in\ndomains that require coherent multi-step action sequences grounded in external\nconstraints. We introduce SymPlanner, a novel framework that equips LMs with\nstructured planning capabilities by interfacing them with a symbolic\nenvironment that serves as an explicit world model. Rather than relying purely\non natural language reasoning, SymPlanner grounds the planning process in a\nsymbolic state space, where a policy model proposes actions and a symbolic\nenvironment deterministically executes and verifies their effects. To enhance\nexploration and improve robustness, we introduce Iterative Correction (IC),\nwhich refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. We evaluate\nSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,\nand verifiable plans than pure natural language baselines."}
{"id": "2505.01679", "pdf": "https://arxiv.org/pdf/2505.01679.pdf", "abs": "https://arxiv.org/abs/2505.01679", "title": "Evaluating Input Modalities for Pilot-Centered Taxiway Navigation: Insights from a Wizard-of-Oz Simulation", "authors": ["Chan Chea Mean", "Sameer Alam", "Katherine Fennedy", "Meng-Hsueh Hsieh", "Shiwei Xin", "Brian Hilburn"], "categories": ["cs.HC"], "comment": null, "summary": "Runway and taxiway incursions continue to challenge aviation safety, as\npilots often experience disorientation from poor visibility in adverse\nconditions and cognitive workload in complex airport layouts. Current tools,\nsuch as airport moving maps on portable tablets, allow manual route planning\nbut do not dynamically adapt to air traffic controllers' (ATCOs) clearances,\nlimiting their effectiveness in high-stress scenarios. This study investigates\nthe impact of different input modalities - paper-based, keyboard touch, map\ntouch, and speech-to-text - on taxiway navigation performance, using a\nmedium-fidelity flight simulator and a Wizard-of-Oz methodology to simulate\nideal automation conditions. Contrary to common assumptions, recent studies\nindicate that paper-based methods outperform digital counterparts in accuracy\nand efficiency under certain conditions, highlighting critical limitations in\ncurrent automation strategies. In response, our study investigates why manual\nmethods may excel and how future automation can be optimized for pilot-centered\noperations. Employing a Wizard-of-Oz approach, we replicated the full taxiing\nprocess - from receiving ATCO clearances to executing maneuvers - and\ndifferentiated between readback and execution accuracy. Findings reveal that\nspeech-based systems suffer from low pilot trust, necessitating hybrid\nsolutions that integrate error correction and confidence indicators. These\ninsights contribute to the development of future pilot-centered taxiway\nassistance that enhance situational awareness, minimize workload, and improve\noverall operational safety."}
{"id": "2505.01559", "pdf": "https://arxiv.org/pdf/2505.01559.pdf", "abs": "https://arxiv.org/abs/2505.01559", "title": "On the effectiveness of Large Language Models in the mechanical design domain", "authors": ["Daniele Grandi", "Fabian Riquelme"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we seek to understand the performance of large language models\nin the mechanical engineering domain. We leverage the semantic data found in\nthe ABC dataset, specifically the assembly names that designers assigned to the\noverall assemblies, and the individual semantic part names that were assigned\nto each part. After pre-processing the data we developed two unsupervised tasks\nto evaluate how different model architectures perform on domain-specific data:\na binary sentence-pair classification task and a zero-shot classification task.\nWe achieved a 0.62 accuracy for the binary sentence-pair classification task\nwith a fine-tuned model that focuses on fighting over-fitting: 1) modifying\nlearning rates, 2) dropout values, 3) Sequence Length, and 4) adding a\nmulti-head attention layer. Our model on the zero-shot classification task\noutperforms the baselines by a wide margin, and achieves a top-1 classification\naccuracy of 0.386. The results shed some light on the specific failure modes\nthat arise when learning from language in this domain."}
{"id": "2505.01724", "pdf": "https://arxiv.org/pdf/2505.01724.pdf", "abs": "https://arxiv.org/abs/2505.01724", "title": "VisTaxa: Developing a Taxonomy of Historical Visualizations", "authors": ["Yu Zhang", "Xinyue Chen", "Weili Zheng", "Yuhan Guo", "Guozheng Li", "Siming Chen", "Xiaoru Yuan"], "categories": ["cs.HC", "cs.DL"], "comment": "Accepted to IEEE TVCG (IEEE PacificVis 2025 Journal Track)", "summary": "Historical visualizations are a rich resource for visualization research.\nWhile taxonomy is commonly used to structure and understand the design space of\nvisualizations, existing taxonomies primarily focus on contemporary\nvisualizations and largely overlook historical visualizations. To address this\ngap, we describe an empirical method for taxonomy development. We introduce a\ncoding protocol and the VisTaxa system for taxonomy labeling and comparison. We\ndemonstrate using our method to develop a historical visualization taxonomy by\ncoding 400 images of historical visualizations. We analyze the coding result\nand reflect on the coding process. Our work is an initial step toward a\nsystematic investigation of the design space of historical visualizations."}
{"id": "2505.01560", "pdf": "https://arxiv.org/pdf/2505.01560.pdf", "abs": "https://arxiv.org/abs/2505.01560", "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains", "authors": ["Vicent Briva Iglesias", "Gokhan Dogru"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the\nnext leap in machine translation (MT), but their benefits relative to\nconventional neural MT (NMT) remain unclear. This paper offers an empirical\nreality check. We benchmark five paradigms, Google Translate (strong NMT\nbaseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),\nand two GPT-4o-powered agentic workflows (sequential three-stage and iterative\nrefinement), on test data drawn from a legal contract and news prose in three\nEnglish-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is\nperformed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with\nexpert ratings of adequacy and fluency; efficiency with total input-plus-output\ntoken counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in\nseven of twelve metric-language combinations; o1-preview ties or places second\nin most remaining cases, while both multi-agent workflows trail. Human\nevaluation reverses part of this narrative: o1-preview produces the most\nadequate and fluent output in five of six comparisons, and the iterative agent\nedges ahead once, indicating that reasoning layers capture semantic nuance\nundervalued by surface metrics. Yet these qualitative gains carry steep costs.\nThe sequential agent consumes roughly five times, and the iterative agent\nfifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight\nresearch directions that could tip the balance: leaner coordination strategies,\nselective agent activation, and hybrid pipelines combining single-pass LLMs\nwith targeted agent intervention."}
{"id": "2505.01753", "pdf": "https://arxiv.org/pdf/2505.01753.pdf", "abs": "https://arxiv.org/abs/2505.01753", "title": "From Formulas to Figures: How Visual Elements Impact User Interactions in Educational Videos", "authors": ["Wolfgang Gritz", "Hewi Salih", "Anett Hoppe", "Ralph Ewerth"], "categories": ["cs.HC"], "comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. As soon as the manuscript has\n  been published, the DOI will appear here", "summary": "Educational videos have become increasingly relevant in today's learning\nenvironments. While prior research in laboratory studies has provided valuable\ninsights, analyzing real-world interaction data can enhance our understanding\nof authentic user behavior. Previous studies have investigated technical\naspects, such as the influence of cuts on pausing behavior, but the impact of\nvisual complexity remains understudied. In this paper, we address this gap and\npropose a novel approach centered on visual complexity, defined as the number\nof visually distinguishable and meaningful elements in a video frame, such as\nmathematical equations, chemical formulas, or graphical representations. Our\nstudy introduces a fine-grained taxonomy of visual objects in educational\nvideos, expanding on previous classifications. Applying this taxonomy to 25\nvideos from physics and chemistry, we examine the relationship between visual\ncomplexity and user behavior, including pauses, in-video navigation, and\nsession dropouts. The results indicate that increased visual complexity,\nespecially of textual elements, correlates with more frequent pauses, rewinds,\nand dropouts. The results offer a deeper understanding of how video design\naffects user behavior in real-world scenarios. Our work has implications for\noptimizing educational videos, particularly in STEM fields. We make our code\npublicly available (https://github.com/TIBHannover/from_formulas_to_figures)."}
{"id": "2505.01592", "pdf": "https://arxiv.org/pdf/2505.01592.pdf", "abs": "https://arxiv.org/abs/2505.01592", "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents", "authors": ["Takyoung Kim", "Janvijay Singh", "Shuhaib Mehri", "Emre Can Acikgoz", "Sagnik Mukherjee", "Nimet Beyza Bozdag", "Sumuk Shashidhar", "Gokhan Tur", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint in progress", "summary": "The growing capabilities of large language models (LLMs) in\ninstruction-following and context-understanding lead to the era of agents with\nnumerous applications. Among these, task planning agents have become especially\nprominent in realistic scenarios involving complex internal pipelines, such as\ncontext understanding, tool management, and response generation. However,\nexisting benchmarks predominantly evaluate agent performance based on task\ncompletion as a proxy for overall effectiveness. We hypothesize that merely\nimproving task completion is misaligned with maximizing user satisfaction, as\nusers interact with the entire agentic process and not only the end result. To\naddress this gap, we propose PIPA, a unified evaluation protocol that\nconceptualizes the behavioral process of interactive task planning agents\nwithin a partially observable Markov Decision Process (POMDP) paradigm. The\nproposed protocol offers a comprehensive assessment of agent performance\nthrough a set of atomic evaluation criteria, allowing researchers and\npractitioners to diagnose specific strengths and weaknesses within the agent's\ndecision-making pipeline. Our analyses show that agents excel in different\nbehavioral stages, with user satisfaction shaped by both outcomes and\nintermediate behaviors. We also highlight future directions, including systems\nthat leverage multiple agents and the limitations of user simulators in task\nplanning."}
{"id": "2505.01886", "pdf": "https://arxiv.org/pdf/2505.01886.pdf", "abs": "https://arxiv.org/abs/2505.01886", "title": "Interactive authoring of outcome-oriented lesson plans for immersive Virtual Reality training", "authors": ["Ananya Ipsita", "Ramesh Kaki", "Mayank Patel", "Asim Unmesh", "Kylie A. Peppler", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Immersive Virtual Reality (iVR) applications have shown immense potential for\nskill training and learning in manufacturing. However, authoring of such\napplications requires technical expertise, which makes it difficult for\neducators to author instructions targeted at desired learning outcomes. We\npresent FlowTrainer, an LLM-assisted interactive system to allow educators to\nauthor lesson plans for their iVR instruction based on desired goals. The\nauthoring workflow is supported by Backward design to align the planned lesson\nbased on the desired outcomes. We implemented a welding use case and conducted\na user study with welding experts to test the effectiveness of the system in\nauthoring outcome-oriented lesson plans. The study results showed that the\nsystem allowed users to plan lesson plans based on desired outcomes while\nreducing the time and technical expertise required for the authoring process.\nWe believe that such efforts can allow widespread adoption of iVR solutions in\nmanufacturing training to meet the workforce demands in the industry."}
{"id": "2505.01595", "pdf": "https://arxiv.org/pdf/2505.01595.pdf", "abs": "https://arxiv.org/abs/2505.01595", "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation", "authors": ["Liaoyaqi Wang", "Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a state-of-the-art model for fine-grained probability estimation\nof propositions conditioned on context. Recent advances in large language\nmodels (LLMs) have significantly enhanced their reasoning capabilities,\nparticularly on well-defined tasks with complete information. However, LLMs\ncontinue to struggle with making accurate and well-calibrated probabilistic\npredictions under uncertainty or partial information. While incorporating\nuncertainty into model predictions often boosts performance, obtaining reliable\nestimates of that uncertainty remains understudied. In particular, LLM\nprobability estimates tend to be coarse and biased towards more frequent\nnumbers. Through a combination of human and synthetic data creation and\nassessment, scaling to larger models, and better supervision, we propose a set\nof strong and precise probability estimation models. We conduct systematic\nevaluations across tasks that rely on conditional probability estimation and\nshow that our approach consistently outperforms existing fine-tuned and\nprompting-based methods by a large margin."}
{"id": "2505.02230", "pdf": "https://arxiv.org/pdf/2505.02230.pdf", "abs": "https://arxiv.org/abs/2505.02230", "title": "The GenAI Generation: Student Views of Awareness, Preparedness, and Concern", "authors": ["Micaela Siraj", "Jon Duke"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Generative AI (GenAI) is revolutionizing education and workforce development,\nprofoundly shaping how students learn, engage, and prepare for their future.\nOutpacing the development of uniform policies and structures, GenAI has\nheralded a unique era and given rise to the GenAI Generation: a cohort of\nstudents whose education has been increasingly shaped by the opportunities and\nchallenges GenAI presents during its widespread adoption within society. This\nstudy examines our students' perceptions of GenAI through a concise survey with\noptional open-ended questions, focusing on their awareness, preparedness, and\nconcerns. Evaluation of more than 250 responses with more than 40% providing\ndetailed qualitative feedback reveals a core dual sentiment: while most\nstudents express enthusiasm for GenAI, an even greater proportion voice a\nspectrum of concerns about ethics, job displacement, and the adequacy of\neducational structures given the highly transformative technology. These\nfindings offer critical insights into how students view the potential and\npitfalls of GenAI for future career impacts, with accompanying recommendations\nto guide educational institutions in navigating a future driven by GenAI."}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658.pdf", "abs": "https://arxiv.org/abs/2505.01658", "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"}
{"id": "2505.02428", "pdf": "https://arxiv.org/pdf/2505.02428.pdf", "abs": "https://arxiv.org/abs/2505.02428", "title": "Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A Randomized Study with 90+ Novice Counselors", "authors": ["Ryan Louie", "Ifdita Hasan Orney", "Juan Pablo Pacheco", "Raj Sanjay Shah", "Emma Brunskill", "Diyi Yang"], "categories": ["cs.HC"], "comment": "main paper is 11 pages, with methods it is 18 pages, with appendix\n  and references it is 33 pages", "summary": "Training more counselors, from clinical students to peer supporters, can help\nmeet the demand for accessible mental health support; however, current training\napproaches remain resource-intensive and difficult to scale effectively. Large\nLanguage Models (LLMs) offer promising solutions for growing counseling skills\ntraining through simulated practice and automated feedback. Despite successes\nin aligning LLMs with expert-counselor annotations, we do not know whether\nLLM-based counseling training tools -- such as AI patients that simulate\nreal-world challenges and generative AI feedback with suggested alternatives\nand rationales -- actually lead to improvements in novice counselor skill\ndevelopment. We develop CARE, an LLM-simulated practice and feedback system,\nand randomize 94 novice counselors to practice using an AI patient, either\nalone or with AI feedback, measuring changes in their behavioral performance,\nself-assessments, and qualitative learning takeaways. Our results show the\npractice-and-feedback group improved in their use of reflections and questions\n(d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI\npatient alone did not show improvements, and in the case of empathy, actually\nhad worse uses across time (d=$-$0.52, p=0.001) and when compared against the\npractice-and-feedback group (d=0.72, p=0.001). Participants' qualitative\nself-reflections revealed key differences: the practice-and-feedback group\nadopted a client-centered approach involving listening to and validating\nfeelings, while the practice-alone group remained solution-oriented but delayed\noffering suggestions until gathering more information. Overall, these results\nsuggest that LLM-based training systems can promote effective skill\ndevelopment, but that combining both simulated practice and structured feedback\nis critical."}
{"id": "2505.01693", "pdf": "https://arxiv.org/pdf/2505.01693.pdf", "abs": "https://arxiv.org/abs/2505.01693", "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers", "authors": ["Brian Wong", "Kaito Tanaka"], "categories": ["cs.CL"], "comment": null, "summary": "Automated labeling of chest X-ray reports is essential for enabling\ndownstream tasks such as training image-based diagnostic models, population\nhealth studies, and clinical decision support. However, the high variability,\ncomplexity, and prevalence of negation and uncertainty in these free-text\nreports pose significant challenges for traditional Natural Language Processing\nmethods. While large language models (LLMs) demonstrate strong text\nunderstanding, their direct application for large-scale, efficient labeling is\nlimited by computational cost and speed. This paper introduces DeBERTa-RAD, a\nnovel two-stage framework that combines the power of state-of-the-art LLM\npseudo-labeling with efficient DeBERTa-based knowledge distillation for\naccurate and fast chest X-ray report labeling. We leverage an advanced LLM to\ngenerate high-quality pseudo-labels, including certainty statuses, for a large\ncorpus of reports. Subsequently, a DeBERTa-Base model is trained on this\npseudo-labeled data using a tailored knowledge distillation strategy. Evaluated\non the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a\nstate-of-the-art Macro F1 score of 0.9120, significantly outperforming\nestablished rule-based systems, fine-tuned transformer models, and direct LLM\ninference, while maintaining a practical inference speed suitable for\nhigh-throughput applications. Our analysis shows particular strength in\nhandling uncertain findings. This work demonstrates a promising path to\novercome data annotation bottlenecks and achieve high-performance medical text\nprocessing through the strategic combination of LLM capabilities and efficient\nstudent models trained via distillation."}
{"id": "2505.02542", "pdf": "https://arxiv.org/pdf/2505.02542.pdf", "abs": "https://arxiv.org/abs/2505.02542", "title": "\"Salt is the Soul of Hakka Baked Chicken\": Reimagining Traditional Chinese Culinary ICH for Modern Contexts Without Losing Tradition", "authors": ["Sijia Liu", "XiaoKe Zeng", "Fengyihan Wu", "Shu Ye", "Bowen Liu", "Sidney Cheung", "Richard William Allen", "Ray Lc"], "categories": ["cs.HC"], "comment": null, "summary": "Intangible Cultural Heritage (ICH) like traditional culinary practices face\nincreasing pressure to adapt to globalization while maintaining their cultural\nauthenticity. Centuries-old traditions in Chinese cuisine are subject to rapid\nchanges for adaptation to contemporary tastes and dietary preferences. The\npreservation of these cultural practices requires approaches that can enable\nICH practitioners to reimagine and recreate ICH for modern contexts. To address\nthis, we created workshops where experienced practitioners of traditional\nChinese cuisine co-created recipes using GenAI tools and realized the dishes.\nWe found that GenAI inspired ICH practitioners to innovate recipes based on\ntraditional workflows for broader audiences and adapt to modern dining\ncontexts. However, GenAI-inspired co-creation posed challenges in maintaining\nthe accuracy of original ICH workflows and preserving traditional flavors in\nthe culinary outcomes. This study offers implications for designing human-AI\ncollaborative processes for safeguarding and enhancing culinary ICH."}
{"id": "2505.01731", "pdf": "https://arxiv.org/pdf/2505.01731.pdf", "abs": "https://arxiv.org/abs/2505.01731", "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "authors": ["Chuan Sun", "Han Yu", "Lizhen Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning large language models (LLMs) is a promising solution for reducing\nmodel sizes and computational complexity while preserving performance.\nTraditional layer-wise pruning methods often adopt a uniform sparsity approach\nacross all layers, which leads to suboptimal performance due to the varying\nsignificance of individual transformer layers within the model not being\naccounted for. To this end, we propose the \\underline{S}hapley\n\\underline{V}alue-based \\underline{N}on-\\underline{U}niform \\underline{P}runing\n(\\methodname{}) method for LLMs. This approach quantifies the contribution of\neach transformer layer to the overall model performance, enabling the\nassignment of tailored pruning budgets to different layers to retain critical\nparameters. To further improve efficiency, we design the Sliding Window-based\nShapley Value approximation method. It substantially reduces computational\noverhead compared to exact SV calculation methods. Extensive experiments on\nvarious LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness\nof the proposed approach. The results reveal that non-uniform pruning\nsignificantly enhances the performance of pruned models. Notably, \\methodname{}\nachieves a reduction in perplexity (PPL) of 18.01\\% and 19.55\\% on LLaMA-7B and\nLLaMA-13B, respectively, compared to SparseGPT at 70\\% sparsity."}
{"id": "2505.02558", "pdf": "https://arxiv.org/pdf/2505.02558.pdf", "abs": "https://arxiv.org/abs/2505.02558", "title": "The Turing Test Is More Relevant Than Ever", "authors": ["Avraham Rahimov", "Orel Zamler", "Amos Azaria"], "categories": ["cs.HC"], "comment": "10 pages, 5 figures, 1 listing, 6 tables", "summary": "The Turing Test, first proposed by Alan Turing in 1950, has historically\nserved as a benchmark for evaluating artificial intelligence (AI). However,\nsince the release of ELIZA in 1966, and particularly with recent advancements\nin large language models (LLMs), AI has been claimed to pass the Turing Test.\nFurthermore, criticism argues that the Turing Test primarily assesses deceptive\nmimicry rather than genuine intelligence, prompting the continuous emergence of\nalternative benchmarks. This study argues against discarding the Turing Test,\nproposing instead using more refined versions of it, for example, by\ninteracting simultaneously with both an AI and human candidate to determine who\nis who, allowing a longer interaction duration, access to the Internet and\nother AIs, using experienced people as evaluators, etc.\n  Through systematic experimentation using a web-based platform, we demonstrate\nthat richer, contextually structured testing environments significantly enhance\nparticipants' ability to differentiate between AI and human interactions.\nNamely, we show that, while an off-the-shelf LLM can pass some version of a\nTuring Test, it fails to do so when faced with a more robust version. Our\nfindings highlight that the Turing Test remains an important and effective\nmethod for evaluating AI, provided it continues to adapt as AI technology\nadvances. Additionally, the structured data gathered from these improved\ninteractions provides valuable insights into what humans expect from truly\nintelligent AI systems."}
{"id": "2505.01761", "pdf": "https://arxiv.org/pdf/2505.01761.pdf", "abs": "https://arxiv.org/abs/2505.01761", "title": "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models", "authors": ["Tobias Domhan", "Dawei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Accurately evaluating machine-translated text remains a long-standing\nchallenge, particularly for long documents. Recent work has shown that large\nlanguage models (LLMs) can serve as reliable and interpretable sentence-level\ntranslation evaluators via MQM error span annotations. With modern LLMs\nsupporting larger context windows, a natural question arises: can we feed\nentire document translations into an LLM for quality assessment? Ideally,\nevaluation should be invariant to text length, producing consistent error spans\nregardless of input granularity. However, our analysis shows that text length\nsignificantly impacts evaluation: longer texts lead to fewer error spans and\nreduced system ranking accuracy. To address this limitation, we evaluate\nseveral strategies, including granularity-aligned prompting, Focus Sentence\nPrompting (FSP), and a fine-tuning approach to better align LLMs with the\nevaluation task. The latter two methods largely mitigate this length bias,\nmaking LLMs more reliable for long-form translation evaluation."}
{"id": "2505.02582", "pdf": "https://arxiv.org/pdf/2505.02582.pdf", "abs": "https://arxiv.org/abs/2505.02582", "title": "FlyHaptics: Flying Multi-contact Haptic Interface", "authors": ["Luis Moreno", "Miguel Altamirano Cabrera", "Muhammad Haris Khan", "Issatay Tokmurziyev", "Yara Mahmoud", "Valerii Serpiva", "Dzmitry Tsetserukou"], "categories": ["cs.HC"], "comment": null, "summary": "This work presents FlyHaptics, an aerial haptic interface tracked via a Vicon\noptical motion capture system and built around six five-bar linkage assemblies\nenclosed in a lightweight protective cage. We predefined five static tactile\npatterns - each characterized by distinct combinations of linkage contact\npoints and vibration intensities - and evaluated them in a grounded pilot\nstudy, where participants achieved 86.5 recognition accuracy (F(4, 35) = 1.47,\np = 0.23) with no significant differences between patterns. Complementary\nflight demonstrations confirmed stable hover performance and consistent force\noutput under realistic operating conditions. These pilot results validate the\nfeasibility of drone-mounted, multi-contact haptic feedback and lay the\ngroundwork for future integration into fully immersive VR, teleoperation, and\nremote interaction scenarios."}
{"id": "2505.01794", "pdf": "https://arxiv.org/pdf/2505.01794.pdf", "abs": "https://arxiv.org/abs/2505.01794", "title": "A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments", "authors": ["Jared D. T. Guerrero-Sosa", "Francisco P. Romero", "Víctor Hugo Menéndez-Domínguez", "Jesus Serrano-Guerrero", "Andres Montoro-Montarroso", "Jose A. Olivas"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In the rapidly evolving educational landscape, the unbiased assessment of\nsoft skills is a significant challenge, particularly in higher education. This\npaper presents a fuzzy logic approach that employs a Granular Linguistic Model\nof Phenomena integrated with multimodal analysis to evaluate soft skills in\nundergraduate students. By leveraging computational perceptions, this approach\nenables a structured breakdown of complex soft skill expressions, capturing\nnuanced behaviours with high granularity and addressing their inherent\nuncertainties, thereby enhancing interpretability and reliability. Experiments\nwere conducted with undergraduate students using a developed tool that assesses\nsoft skills such as decision-making, communication, and creativity. This tool\nidentifies and quantifies subtle aspects of human interaction, such as facial\nexpressions and gesture recognition. The findings reveal that the framework\neffectively consolidates multiple data inputs to produce meaningful and\nconsistent assessments of soft skills, showing that integrating multiple\nmodalities into the evaluation process significantly improves the quality of\nsoft skills scores, making the assessment work transparent and understandable\nto educational stakeholders."}
{"id": "2505.02649", "pdf": "https://arxiv.org/pdf/2505.02649.pdf", "abs": "https://arxiv.org/abs/2505.02649", "title": "Eye Movements as Indicators of Deception: A Machine Learning Approach", "authors": ["Valentin Foucher", "Santiago de Leon-Martinez", "Robert Moro"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Gaze may enhance the robustness of lie detectors but remains under-studied.\nThis study evaluated the efficacy of AI models (using fixations, saccades,\nblinks, and pupil size) for detecting deception in Concealed Information Tests\nacross two datasets. The first, collected with Eyelink 1000, contains gaze data\nfrom a computerized experiment where 87 participants revealed, concealed, or\nfaked the value of a previously selected card. The second, collected with Pupil\nNeon, involved 36 participants performing a similar task but facing an\nexperimenter. XGBoost achieved accuracies up to 74% in a binary classification\ntask (Revealing vs. Concealing) and 49% in a more challenging\nthree-classification task (Revealing vs. Concealing vs. Faking). Feature\nanalysis identified saccade number, duration, amplitude, and maximum pupil size\nas the most important for deception prediction. These results demonstrate the\nfeasibility of using gaze and AI to enhance lie detectors and encourage future\nresearch that may improve on this."}
{"id": "2505.01800", "pdf": "https://arxiv.org/pdf/2505.01800.pdf", "abs": "https://arxiv.org/abs/2505.01800", "title": "Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis", "authors": ["Chidimma Opara"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8", "summary": "The increasing sophistication of AI-generated texts highlights the urgent\nneed for accurate and transparent detection tools, especially in educational\nsettings, where verifying authorship is essential. Existing literature has\ndemonstrated that the application of stylometric features with machine learning\nclassifiers can yield excellent results. Building on this foundation, this\nstudy proposes a comprehensive framework that integrates stylometric analysis\nwith psycholinguistic theories, offering a clear and interpretable approach to\ndistinguishing between AI-generated and human-written texts. This research\nspecifically maps 31 distinct stylometric features to cognitive processes such\nas lexical retrieval, discourse planning, cognitive load management, and\nmetacognitive self-monitoring. In doing so, it highlights the unique\npsycholinguistic patterns found in human writing. Through the intersection of\ncomputational linguistics and cognitive science, this framework contributes to\nthe development of reliable tools aimed at preserving academic integrity in the\nera of generative AI."}
{"id": "2505.02694", "pdf": "https://arxiv.org/pdf/2505.02694.pdf", "abs": "https://arxiv.org/abs/2505.02694", "title": "AI Standardized Patient Improves Human Conversations in Advanced Cancer Care", "authors": ["Kurtis Haut", "Masum Hasan", "Thomas Carroll", "Ronald Epstein", "Taylan Sen", "Ehsan Hoque"], "categories": ["cs.HC", "cs.AI"], "comment": "20 pages, 6 figures, 4 tables, submitting to New England Journal of\n  Medicine (NEJM)", "summary": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education."}
{"id": "2505.01812", "pdf": "https://arxiv.org/pdf/2505.01812.pdf", "abs": "https://arxiv.org/abs/2505.01812", "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge", "authors": ["Core Francisco Park", "Zechen Zhang", "Hidenori Tanaka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT."}
{"id": "2505.02699", "pdf": "https://arxiv.org/pdf/2505.02699.pdf", "abs": "https://arxiv.org/abs/2505.02699", "title": "Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for History Education in Virtual Reality", "authors": ["Zihao Zhu", "Ao Yu", "Xin Tong", "Pan Hui"], "categories": ["cs.HC"], "comment": "14 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://dl.acm.org/doi/10.1145/3706598.3713109", "summary": "Multi-role pedagogical agents can create engaging and immersive learning\nexperiences, helping learners better understand knowledge in history learning.\nHowever, existing pedagogical agents often struggle with multi-role\ninteractions due to complex controls, limited feedback forms, and difficulty\ndynamically adapting to user inputs. In this study, we developed a VR prototype\nwith LLM-powered adaptive role-switching and action-switching pedagogical\nagents to help users learn about the history of the Pavilion of Prince Teng. A\n2 x 2 between-subjects study was conducted with 84 participants to assess how\nadaptive role-switching and action-switching affect participants' learning\noutcomes and experiences. The results suggest that adaptive role-switching\nenhances participants' perception of the pedagogical agent's trustworthiness\nand expertise but may lead to inconsistent learning experiences. Adaptive\naction-switching increases participants' perceived social presence, expertise,\nand humanness. The study did not uncover any effects of role-switching and\naction-switching on usability, learning motivation, and cognitive load. Based\non the findings, we proposed five design implications for incorporating\nadaptive role-switching and action-switching into future VR history education\ntools."}
{"id": "2505.01855", "pdf": "https://arxiv.org/pdf/2505.01855.pdf", "abs": "https://arxiv.org/abs/2505.01855", "title": "Intra-Layer Recurrence in Transformers for Language Modeling", "authors": ["Anthony Nguyen", "Wenjun Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Canadian AI 2025. Code available at\n  https://github.com/ant-8/Layer-Recurrent-Transformers", "summary": "Transformer models have established new benchmarks in natural language\nprocessing; however, their increasing depth results in substantial growth in\nparameter counts. While existing recurrent transformer methods address this\nissue by reprocessing layers multiple times, they often apply recurrence\nindiscriminately across entire blocks of layers. In this work, we investigate\nIntra-Layer Recurrence (ILR), a more targeted approach that applies recurrence\nselectively to individual layers within a single forward pass. Our experiments\nshow that allocating more iterations to earlier layers yields optimal results.\nThese findings suggest that ILR offers a promising direction for optimizing\nrecurrent structures in transformer architectures."}
{"id": "2505.02780", "pdf": "https://arxiv.org/pdf/2505.02780.pdf", "abs": "https://arxiv.org/abs/2505.02780", "title": "Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow", "authors": ["Jai Prakash Veerla", "Partha Sai Guttikonda", "Helen H. Shang", "Mohammad Sadegh Nasr", "Cesar Torres", "Jacob M. Luber"], "categories": ["cs.HC", "cs.AI", "cs.ET", "q-bio.TO"], "comment": null, "summary": "Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases\nlike cancer, yet current digital pathology tools hinder diagnosis. The immense\nscale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the\nlimited views traditional monitors offer. This mismatch forces constant panning\nand zooming, increasing pathologist cognitive load, causing diagnostic fatigue,\nand slowing pathologists' adoption of digital methods. PathVis, our\nmixed-reality visualization platform for Apple Vision Pro, addresses these\nchallenges. It transforms the pathologist's interaction with data, replacing\ncumbersome mouse-and-monitor navigation with intuitive exploration using\nnatural hand gestures, eye gaze, and voice commands in an immersive workspace.\nPathVis integrates AI to enhance diagnosis. An AI-driven search function\ninstantly retrieves and displays the top five similar patient cases\nside-by-side, improving diagnostic precision and efficiency through rapid\ncomparison. Additionally, a multimodal conversational AI assistant offers\nreal-time image interpretation support and aids collaboration among\npathologists across multiple Apple devices. By merging the directness of\ntraditional pathology with advanced mixed-reality visualization and AI, PathVis\nimproves diagnostic workflows, reduces cognitive strain, and makes pathology\npractice more effective and engaging. The PathVis source code and a demo video\nare publicly available at: https://github.com/jaiprakash1824/Path_Vis"}
{"id": "2505.01868", "pdf": "https://arxiv.org/pdf/2505.01868.pdf", "abs": "https://arxiv.org/abs/2505.01868", "title": "Positional Attention for Efficient BERT-Based Named Entity Recognition", "authors": ["Mo Sun", "Siheng Xiong", "Yuankai Cai", "Bowen Zuo"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a framework for Named Entity Recognition (NER) leveraging\nthe Bidirectional Encoder Representations from Transformers (BERT) model in\nnatural language processing (NLP). NER is a fundamental task in NLP with broad\napplicability across downstream applications. While BERT has established itself\nas a state-of-the-art model for entity recognition, fine-tuning it from scratch\nfor each new application is computationally expensive and time-consuming. To\naddress this, we propose a cost-efficient approach that integrates positional\nattention mechanisms into the entity recognition process and enables effective\ncustomization using pre-trained parameters. The framework is evaluated on a\nKaggle dataset derived from the Groningen Meaning Bank corpus and achieves\nstrong performance with fewer training epochs. This work contributes to the\nfield by offering a practical solution for reducing the training cost of\nBERT-based NER systems while maintaining high accuracy."}
{"id": "2505.02802", "pdf": "https://arxiv.org/pdf/2505.02802.pdf", "abs": "https://arxiv.org/abs/2505.02802", "title": "Generating HomeAssistant Automations Using an LLM-based Chatbot", "authors": ["Mathyas Giudici", "Alessandro Sironi", "Ismaele Villa", "Samuele Scherini", "Franca Garzotto"], "categories": ["cs.HC"], "comment": null, "summary": "To combat climate change, individuals are encouraged to adopt sustainable\nhabits, in particular, with their household, optimizing their electrical\nconsumption. Conversational agents, such as Smart Home Assistants, hold promise\nas effective tools for promoting sustainable practices within households. Our\nresearch investigated the application of Large Language Models (LLM) in\nenhancing smart home automation and promoting sustainable household practices,\nspecifically using the HomeAssistant framework. In particular, it highlights\nthe potential of GPT models in generating accurate automation routines. While\nthe LLMs showed proficiency in understanding complex commands and creating\nvalid JSON outputs, challenges such as syntax errors and message malformations\nwere noted, indicating areas for further improvement. Still, despite minimal\nquantitative differences between \"green\" and \"no green\" prompts, qualitative\nfeedback highlighted a positive shift towards sustainability in the routines\ngenerated with environmentally focused prompts. Then, an empirical evaluation\n(N=56) demonstrated that the system was well-received and found engaging by\nusers compared to its traditional rule-based counterpart. Our findings\nhighlight the role of LLMs in advancing smart home technologies and suggest\nfurther research to refine these models for broader, real-world applications to\nsupport sustainable living."}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877.pdf", "abs": "https://arxiv.org/abs/2505.01877", "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "authors": ["Jiří Milička", "Anna Marklová", "Ondřej Drobil", "Eva Pospíšilová"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts."}
{"id": "2505.01651", "pdf": "https://arxiv.org/pdf/2505.01651.pdf", "abs": "https://arxiv.org/abs/2505.01651", "title": "Human-AI Governance (HAIG): A Trust-Utility Approach", "authors": ["Zeynep Engin"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "cs.SI"], "comment": "32 pages including references and appendix, 25 pages core text, 3\n  figures, 3 tables", "summary": "This paper introduces the HAIG framework for analysing trust dynamics across\nevolving human-AI relationships. Current categorical frameworks (e.g.,\n\"human-in-the-loop\" models) inadequately capture how AI systems evolve from\ntools to partners, particularly as foundation models demonstrate emergent\ncapabilities and multi-agent systems exhibit autonomous goal-setting\nbehaviours. As systems advance, agency redistributes in complex patterns that\nare better represented as positions along continua rather than discrete\ncategories, though progression may include both gradual shifts and significant\nstep changes. The HAIG framework operates across three levels: dimensions\n(Decision Authority Distribution, Process Autonomy, and Accountability\nConfiguration), continua (gradual shifts along each dimension), and thresholds\n(critical points requiring governance adaptation). Unlike risk-based or\nprinciple-based approaches, HAIG adopts a trust-utility orientation, focusing\non maintaining appropriate trust relationships that maximise utility while\nensuring sufficient safeguards. Our analysis reveals how technical advances in\nself-supervision, reasoning authority, and distributed decision-making drive\nnon-uniform trust evolution across both contextual variation and technological\nadvancement. Case studies in healthcare and European regulation demonstrate how\nHAIG complements existing frameworks while offering a foundation for\nalternative approaches that anticipate governance challenges before they\nemerge."}
{"id": "2505.01883", "pdf": "https://arxiv.org/pdf/2505.01883.pdf", "abs": "https://arxiv.org/abs/2505.01883", "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams", "authors": ["Yiwen Lu", "Siheng Xiong", "Zhaowei Li"], "categories": ["cs.CL"], "comment": null, "summary": "We present a framework for large-scale sentiment and topic analysis of\nTwitter discourse. Our pipeline begins with targeted data collection using\nconflict-specific keywords, followed by automated sentiment labeling via\nmultiple pre-trained models to improve annotation robustness. We examine the\nrelationship between sentiment and contextual features such as timestamp,\ngeolocation, and lexical content. To identify latent themes, we apply Latent\nDirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and\nmetadata attributes. Finally, we develop an interactive visualization interface\nto support exploration of sentiment trends and topic distributions across time\nand regions. This work contributes a scalable methodology for social media\nanalysis in dynamic geopolitical contexts."}
{"id": "2505.01680", "pdf": "https://arxiv.org/pdf/2505.01680.pdf", "abs": "https://arxiv.org/abs/2505.01680", "title": "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study", "authors": ["Tamim Ahmed", "Thanassis Rikakis"], "categories": ["cs.CV", "cs.AI", "cs.HC", "math.PR"], "comment": null, "summary": "Manual scoring of the Action Research Arm Test (ARAT) for upper extremity\nassessment in stroke rehabilitation is time-intensive and variable. We propose\nan automated ARAT scoring system integrating multimodal video analysis with\nSlowFast, I3D, and Transformer-based models using OpenPose keypoints and object\nlocations. Our approach employs multi-view data (ipsilateral, contralateral,\nand top perspectives), applying early and late fusion to combine features\nacross views and models. Hierarchical Bayesian Models (HBMs) infer movement\nquality components, enhancing interpretability. A clinician dashboard displays\ntask scores, execution times, and quality assessments. We conducted a study\nwith five clinicians who reviewed 500 video ratings generated by our system,\nproviding feedback on its accuracy and usability. Evaluated on a stroke\nrehabilitation dataset, our framework achieves 89.0% validation accuracy with\nlate fusion, with HBMs aligning closely with manual assessments. This work\nadvances automated rehabilitation by offering a scalable, interpretable\nsolution with clinical validation."}
{"id": "2505.01900", "pdf": "https://arxiv.org/pdf/2505.01900.pdf", "abs": "https://arxiv.org/abs/2505.01900", "title": "CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation", "authors": ["Mazal Bethany", "Nishant Vishwamitra", "Cho-Yu Jason Chiang", "Peyman Najafirad"], "categories": ["cs.CL"], "comment": null, "summary": "Automated evidence-based misinformation detection systems, which evaluate the\nveracity of short claims against evidence, lack comprehensive analysis of their\nadversarial vulnerabilities. Existing black-box text-based adversarial attacks\nare ill-suited for evidence-based misinformation detection systems, as these\nattacks primarily focus on token-level substitutions involving gradient or\nlogit-based optimization strategies, which are incapable of fooling the\nmulti-component nature of these detection systems. These systems incorporate\nboth retrieval and claim-evidence comparison modules, which requires attacks to\nbreak the retrieval of evidence and/or the comparison module so that it draws\nincorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach\nthat employs a two-agent system, a Prompt Optimization Agent and an Attacker\nAgent, to create adversarial claim rewritings that manipulate evidence\nretrieval and mislead claim-evidence comparison, effectively bypassing the\nsystem without altering the meaning of the claim. The Attacker Agent produces\nsemantically equivalent rewrites that attempt to mislead detectors, while the\nPrompt Optimization Agent analyzes failed attack attempts and refines the\nprompt of the Attacker to guide subsequent rewrites. This enables larger\nstructural and stylistic transformations of the text rather than token-level\nsubstitutions, adapting the magnitude of changes based on previous outcomes.\nUnlike existing approaches, CAMOUFLAGE optimizes its attack solely based on\nbinary model decisions to guide its rewriting process, eliminating the need for\nclassifier logits or extensive querying. We evaluate CAMOUFLAGE on four\nsystems, including two recent academic systems and two real-world APIs, with an\naverage attack success rate of 46.92\\% while preserving textual coherence and\nsemantic equivalence to the original claims."}
{"id": "2505.01692", "pdf": "https://arxiv.org/pdf/2505.01692.pdf", "abs": "https://arxiv.org/abs/2505.01692", "title": "Speculative Evolution Through 3D Cellular Automata", "authors": ["Amir Hossein Khazaei"], "categories": ["nlin.CG", "cs.HC"], "comment": "10 pages, 13 Figures", "summary": "This project explores speculative evolution through a 3D implementation of\nConway's Game of Life, using procedural simulation to generate unfamiliar\nextraterrestrial organic forms. By applying a volumetric optimized workflow,\nthe raw cellular structures are smoothed into unified, bone-like geometries\nthat resemble hypothetical non-terrestrial morphologies. The resulting forms,\nstrange yet organic, are 3D printed as fossil-like artifacts, presenting a\ntangible representation of generative structures. This process situates the\nwork at the intersection of artificial life, evolutionary modeling, and digital\nfabrication, illustrating how simple rules can simulate complex biological\nemergence and challenge conventional notions of organic form."}
{"id": "2505.01967", "pdf": "https://arxiv.org/pdf/2505.01967.pdf", "abs": "https://arxiv.org/abs/2505.01967", "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview", "authors": ["Jiatao Li", "Yanheng Li", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) have become integral to daily life, widely\nadopted in communication, decision-making, and information retrieval, raising\ncritical questions about how these systems implicitly form and express\nsocio-cognitive attitudes or \"worldviews\". While existing research extensively\naddresses demographic and ethical biases, broader dimensions-such as attitudes\ntoward authority, equality, autonomy, and fate-remain under-explored. In this\npaper, we introduce the Social Worldview Taxonomy (SWT), a structured framework\ngrounded in Cultural Theory, operationalizing four canonical worldviews\n(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable\nsub-dimensions. Using SWT, we empirically identify distinct and interpretable\ncognitive profiles across 28 diverse LLMs. Further, inspired by Social\nReferencing Theory, we experimentally demonstrate that explicit social cues\nsystematically shape these cognitive attitudes, revealing both general response\npatterns and nuanced model-specific variations. Our findings enhance the\ninterpretability of LLMs by revealing implicit socio-cognitive biases and their\nresponsiveness to social feedback, thus guiding the development of more\ntransparent and socially responsible language technologies."}
{"id": "2505.01967", "pdf": "https://arxiv.org/pdf/2505.01967.pdf", "abs": "https://arxiv.org/abs/2505.01967", "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview", "authors": ["Jiatao Li", "Yanheng Li", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) have become integral to daily life, widely\nadopted in communication, decision-making, and information retrieval, raising\ncritical questions about how these systems implicitly form and express\nsocio-cognitive attitudes or \"worldviews\". While existing research extensively\naddresses demographic and ethical biases, broader dimensions-such as attitudes\ntoward authority, equality, autonomy, and fate-remain under-explored. In this\npaper, we introduce the Social Worldview Taxonomy (SWT), a structured framework\ngrounded in Cultural Theory, operationalizing four canonical worldviews\n(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable\nsub-dimensions. Using SWT, we empirically identify distinct and interpretable\ncognitive profiles across 28 diverse LLMs. Further, inspired by Social\nReferencing Theory, we experimentally demonstrate that explicit social cues\nsystematically shape these cognitive attitudes, revealing both general response\npatterns and nuanced model-specific variations. Our findings enhance the\ninterpretability of LLMs by revealing implicit socio-cognitive biases and their\nresponsiveness to social feedback, thus guiding the development of more\ntransparent and socially responsible language technologies."}
{"id": "2505.01980", "pdf": "https://arxiv.org/pdf/2505.01980.pdf", "abs": "https://arxiv.org/abs/2505.01980", "title": "LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load", "authors": ["Theo Guidroz", "Diego Ardila", "Jimmy Li", "Adam Mansour", "Paul Jhun", "Nina Gonzalez", "Xiang Ji", "Mike Sanchez", "Sujay Kakarmath", "Mathias MJ Bellaiche", "Miguel Ángel Garrido", "Faruk Ahmed", "Divyansh Choudhary", "Jay Hartford", "Chenwei Xu", "Henry Javier Serrano Echeverria", "Yifan Wang", "Jeff Shaffer", "Eric", "Cao", "Yossi Matias", "Avinatan Hassidim", "Dale R Webster", "Yun Liu", "Sho Fujiwara", "Peggy Bui", "Quang Duong"], "categories": ["cs.CL"], "comment": null, "summary": "Information on the web, such as scientific publications and Wikipedia, often\nsurpasses users' reading level. To help address this, we used a self-refinement\napproach to develop a LLM capability for minimally lossy text simplification.\nTo validate our approach, we conducted a randomized study involving 4563\nparticipants and 31 texts spanning 6 broad subject areas: PubMed (biomedical\nscientific articles), biology, law, finance, literature/philosophy, and\naerospace/computer science. Participants were randomized to viewing original or\nsimplified texts in a subject area, and answered multiple-choice questions\n(MCQs) that tested their comprehension of the text. The participants were also\nasked to provide qualitative feedback such as task difficulty. Our results\nindicate that participants who read the simplified text answered more MCQs\ncorrectly than their counterparts who read the original text (3.9% absolute\nincrease, p<0.05). This gain was most striking with PubMed (14.6%), while more\nmoderate gains were observed for finance (5.5%), aerospace/computer science\n(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether\nparticipants could refer back to the text while answering MCQs. The absolute\naccuracy decreased by up to ~9% for both original and simplified setups where\nparticipants could not refer back to the text, but the ~4% overall improvement\npersisted. Finally, participants' self-reported perceived ease based on a\nsimplified NASA Task Load Index was greater for those who read the simplified\ntext (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,\ninvolving an order of magnitude more participants than prior works,\ndemonstrates the potential of LLMs to make complex information easier to\nunderstand. Our work aims to enable a broader audience to better learn and make\nuse of expert knowledge available on the web, improving information\naccessibility."}
{"id": "2505.02003", "pdf": "https://arxiv.org/pdf/2505.02003.pdf", "abs": "https://arxiv.org/abs/2505.02003", "title": "Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing", "authors": ["Maryam Sadeghi", "Darío Fernández Khatiboun", "Yasser Rezaeiyan", "Saima Rizwan", "Alessandro Barcellona", "Andrea Merello", "Marco Crepaldi", "Gabriella Panuccio", "Farshad Moradi"], "categories": ["cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "Closed-loop brain stimulation holds potential as personalized treatment for\ndrug-resistant epilepsy (DRE) but still suffers from limitations that result in\nhighly variable efficacy. First, stimulation is typically delivered upon\ndetection of the seizure to abort rather than prevent it; second, the\nstimulation parameters are established by trial and error, requiring lengthy\nrounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we\naddress these limitations by leveraging the potential of neuromorphic\ncomputing. We present a system capable of driving personalized free-run\nstimulations based on seizure forecasting, wherein each forecast triggers an\nelectrical pulse rather than an arbitrarily predefined fixed-frequency stimulus\ntrain. We validate the system against hippocampal spheroids coupled to 3D\nmicroelectrode array as a simplified testbed, showing that it can achieve\nseizure reduction >97% while primarily using instantaneous stimulation\nfrequencies within 20 Hz, well below what typically used in clinical settings.\nOur work demonstrates the potential of neuromorphic systems as a\nnext-generation neuromodulation strategy for personalized DRE treatment."}
{"id": "2505.02009", "pdf": "https://arxiv.org/pdf/2505.02009.pdf", "abs": "https://arxiv.org/abs/2505.02009", "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs", "authors": ["Sai Krishna Mendu", "Harish Yenala", "Aditi Gulati", "Shanu Kumar", "Parag Agrawal"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to various real-world\napplications, leveraging massive, web-sourced datasets like Common Crawl, C4,\nand FineWeb for pretraining. While these datasets provide linguistic data\nessential for high-quality natural language generation, they often contain\nharmful content, such as hate speech, misinformation, and biased narratives.\nTraining LLMs on such unfiltered data risks perpetuating toxic behaviors,\nspreading misinformation, and amplifying societal biases which can undermine\ntrust in LLM-driven applications and raise ethical concerns about their use.\nThis paper presents a large-scale analysis of inappropriate content across\nthese datasets, offering a comprehensive taxonomy that categorizes harmful\nwebpages into Topical and Toxic based on their intent. We also introduce a\nprompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and\na transformer-based model (HarmFormer) for content filtering. Additionally, we\ncreate a new multi-harm open-ended toxicity benchmark (HAVOC) and provide\ncrucial insights into how models respond to adversarial toxic inputs. Upon\npublishing, we will also opensource our model signal on the entire C4 dataset.\nOur work offers insights into ensuring safer LLM pretraining and serves as a\nresource for Responsible AI (RAI) compliance."}
{"id": "2505.02004", "pdf": "https://arxiv.org/pdf/2505.02004.pdf", "abs": "https://arxiv.org/abs/2505.02004", "title": "Triple-identity Authentication: The Future of Secure Access", "authors": ["Suyun Borjigin"], "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "comment": "10 pages, 2 figures,", "summary": "In a typical authentication process, the local system verifies the user's\nidentity using a stored hash value generated by a cross-system hash algorithm.\nThis article shifts the research focus from traditional password encryption to\nthe establishment of gatekeeping mechanisms for effective interactions between\na system and the outside world. Here, we propose a triple-identity\nauthentication system to achieve this goal. Specifically, this local system\nopens the inner structure of its hash algorithm to all user credentials,\nincluding the login name, login password, and authentication password. When a\nlogin credential is entered, the local system hashes it and then creates a\nunique identifier using intermediate hash elements randomly selected from the\nopen algorithm. Importantly, this locally generated unique identifier (rather\nthan the stored hash produced by the open algorithm) is utilized to verify the\nuser's combined identity, which is generated by combining the entered\ncredential with the International Mobile Equipment Identity and the\nInternational Mobile Subscriber Identity. The verification process is\nimplemented at each interaction point: the login name field, the login password\nfield, and the server's authentication point. Thus, within the context of this\ntriple-identity authentication system, we establish a robust gatekeeping\nmechanism for system interactions, ultimately providing a level of security\nthat is equivalent to multi-factor authentication."}
{"id": "2505.02032", "pdf": "https://arxiv.org/pdf/2505.02032.pdf", "abs": "https://arxiv.org/abs/2505.02032", "title": "An overview of artificial intelligence in computer-assisted language learning", "authors": ["Anisia Katinskaia"], "categories": ["cs.CL"], "comment": null, "summary": "Computer-assisted language learning -- CALL -- is an established research\nfield. We review how artificial intelligence can be applied to support language\nlearning and teaching. The need for intelligent agents that assist language\nlearners and teachers is increasing: the human teacher's time is a scarce and\ncostly resource, which does not scale with growing demand. Further factors\ncontribute to the need for CALL: pandemics and increasing demand for distance\nlearning, migration of large populations, the need for sustainable and\naffordable support for learning, etc. CALL systems are made up of many\ncomponents that perform various functions, and AI is applied to many different\naspects in CALL, corresponding to their own expansive research areas. Most of\nwhat we find in the research literature and in practical use are prototypes or\npartial implementations -- systems that perform some aspects of the overall\ndesired functionality. Complete solutions -- most of them commercial -- are\nfew, because they require massive resources. Recent advances in AI should\nresult in improvements in CALL, yet there is a lack of surveys that focus on AI\nin the context of this research field. This paper aims to present a perspective\non the AI methods that can be employed for language learning from a position of\na developer of a CALL system. We also aim to connect work from different\ndisciplines, to build bridges for interdisciplinary work."}
{"id": "2505.02180", "pdf": "https://arxiv.org/pdf/2505.02180.pdf", "abs": "https://arxiv.org/abs/2505.02180", "title": "MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input", "authors": ["Hirotaka Hiraki", "Jun Rekimoto"], "categories": ["cs.SD", "cs.AR", "cs.HC", "eess.AS", "H.5.2; H.5.5; B.4.2; I.2.7"], "comment": "Augmented Humans 2025", "summary": "Masks are essential in medical settings and during infectious outbreaks but\nsignificantly impair speech communication, especially in environments with\nbackground noise. Existing solutions often require substantial computational\nresources or compromise hygiene and comfort. We propose a novel sensing\napproach that captures only the wearer's voice by detecting mask surface\nvibrations using a piezoelectric sensor. Our developed device, MaskClip,\nemploys a stainless steel clip with an optimally positioned piezoelectric\nsensor to selectively capture speech vibrations while inherently filtering out\nambient noise. Evaluation experiments demonstrated superior performance with a\nlow Character Error Rate of 6.1\\% in noisy environments compared to\nconventional microphones. Subjective evaluations by 102 participants also\nshowed high satisfaction scores. This approach shows promise for applications\nin settings where clear voice communication must be maintained while wearing\nprotective equipment, such as medical facilities, cleanrooms, and industrial\nenvironments."}
{"id": "2505.02072", "pdf": "https://arxiv.org/pdf/2505.02072.pdf", "abs": "https://arxiv.org/abs/2505.02072", "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction", "authors": ["Eitan Wagner", "Omri Abend"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The notion of language modeling has gradually shifted in recent years from a\ndistribution over finite-length strings to general-purpose prediction models\nfor textual inputs and outputs, following appropriate alignment phases. This\npaper analyzes the distinction between distribution estimation and response\nprediction in the context of LLMs, and their often conflicting goals. We\nexamine the training phases of LLMs, which include pretraining, in-context\nlearning, and preference tuning, and also the common use cases for their output\nprobabilities, which include completion probabilities and explicit\nprobabilities as output. We argue that the different settings lead to three\ndistinct intended output distributions. We demonstrate that NLP works often\nassume that these distributions should be similar, which leads to\nmisinterpretations of their experimental findings. Our work sets firmer formal\nfoundations for the interpretation of LLMs, which will inform ongoing work on\nthe interpretation and use of LLMs' induced distributions."}
{"id": "2505.02209", "pdf": "https://arxiv.org/pdf/2505.02209.pdf", "abs": "https://arxiv.org/abs/2505.02209", "title": "Minimally Supervised Hierarchical Domain Intent Learning for CRS", "authors": ["Safikureshi Mondal", "Subhasis Dasgupta", "Amarnath Gupta"], "categories": ["cs.IR", "cs.HC"], "comment": "This research is funded by the National Institution of Food and\n  Agriculture U.S Department of Agriculture (USDA)", "summary": "Modeling domain intent within an evolving domain structure presents a\nsignificant challenge for domain-specific conversational recommendation systems\n(CRS). The conventional approach involves training an intent model using\nutterance-intent pairs. However, as new intents and patterns emerge, the model\nmust be continuously updated while preserving existing relationships and\nmaintaining efficient retrieval. This process leads to substantial growth in\nutterance-intent pairs, making manual labeling increasingly costly and\nimpractical. In this paper, we propose an efficient solution for constructing a\ndynamic hierarchical structure that minimizes the number of user utterances\nrequired to achieve adequate domain knowledge coverage. To this end, we\nintroduce a neural network-based attention-driven hierarchical clustering\nalgorithm designed to optimize intent grouping using minimal data. The proposed\nmethod builds upon and integrates concepts from two existing flat clustering\nalgorithms DEC and NAM, both of which utilize neural attention mechanisms. We\napply our approach to a curated subset of 44,000 questions from the business\nfood domain. Experimental results demonstrate that constructing the hierarchy\nusing a stratified sampling strategy significantly reduces the number of\nquestions needed to represent the evolving intent structure. Our findings\nindicate that this approach enables efficient coverage of dynamic domain\nknowledge without frequent retraining, thereby enhancing scalability and\nadaptability in domain-specific CSRs."}
{"id": "2505.02078", "pdf": "https://arxiv.org/pdf/2505.02078.pdf", "abs": "https://arxiv.org/abs/2505.02078", "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning", "authors": ["Joy Lim Jia Yin", "Daniel Zhang-Li", "Jifan Yu", "Haoxuan Li", "Shangqing Tu", "Yuanchun Wang", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Evaluating the quality of slide-based multimedia instruction is challenging.\nExisting methods like manual assessment, reference-based metrics, and large\nlanguage model evaluators face limitations in scalability, context capture, or\nbias. In this paper, we introduce LecEval, an automated metric grounded in\nMayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal\nknowledge acquisition in slide-based learning. LecEval assesses effectiveness\nusing four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical\nStructure (LS), and Audience Engagement (AE). We curate a large-scale dataset\nof over 2,000 slides from more than 50 online course videos, annotated with\nfine-grained human ratings across these rubrics. A model trained on this\ndataset demonstrates superior accuracy and adaptability compared to existing\nmetrics, bridging the gap between automated and human assessments. We release\nour dataset and toolkits at https://github.com/JoylimJY/LecEval."}
{"id": "2505.02329", "pdf": "https://arxiv.org/pdf/2505.02329.pdf", "abs": "https://arxiv.org/abs/2505.02329", "title": "Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling", "authors": ["Jonathan Lynn", "Rachel Y. Kim", "Sicun Gao", "Daniel Schneider", "Sachin S. Pandya", "Min Kyung Lee"], "categories": ["cs.CY", "cs.HC", "cs.SE"], "comment": "To appear in FAccT'25", "summary": "The impacts of algorithmic management (AM) on worker well-being have led to\nincreasing calls to regulate AM practices to prevent further worker harms. Yet\nexisting work in aligning software with the law reduces compliance to just one\npiece of the entire process of regulating AM -- which involves rule\noperationalization, software use, and enforcement. We interviewed key\nstakeholders involved in enforcing or complying with workplace scheduling law\n-- regulators, advocates, defense attorneys, scheduling managers, and workers\n($N = 38$). Based on their beliefs and experiences, we describe how scheduling\nsoftware affects beliefs about and compliance with workplace scheduling law. In\nso doing, we discuss the challenges and opportunities in designing software as\na tool for regulating AM."}
{"id": "2505.02091", "pdf": "https://arxiv.org/pdf/2505.02091.pdf", "abs": "https://arxiv.org/abs/2505.02091", "title": "LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications", "authors": ["Xinyue Peng", "Yanming Liu", "Yihan Cang", "Chaoqun Cao", "Ming Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "6 pages,4 figures", "summary": "Solving non-convex resource allocation problems poses significant challenges\nin wireless communication systems, often beyond the capability of traditional\noptimization techniques. To address this issue, we propose LLM-OptiRA, the\nfirst framework that leverages large language models (LLMs) to automatically\ndetect and transform non-convex components into solvable forms, enabling fully\nautomated resolution of non-convex resource allocation problems in wireless\ncommunication systems. LLM-OptiRA not only simplifies problem-solving by\nreducing reliance on expert knowledge, but also integrates error correction and\nfeasibility validation mechanisms to ensure robustness. Experimental results\nshow that LLM-OptiRA achieves an execution rate of 96% and a success rate of\n80% on GPT-4, significantly outperforming baseline approaches in complex\noptimization tasks across diverse scenarios."}
{"id": "2505.02414", "pdf": "https://arxiv.org/pdf/2505.02414.pdf", "abs": "https://arxiv.org/abs/2505.02414", "title": "Quadrupedal Spine Control Strategies: Exploring Correlations Between System Dynamic Responses and Human Perspectives", "authors": ["Nicholas Hafner", "Chaoran Liu", "Carlos Ishi", "Hiroshi Ishiguro"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "27 pages, 13 figures", "summary": "Unlike their biological cousins, the majority of existing quadrupedal robots\nare constructed with rigid chassis. This results in motion that is either\nbeetle-like or distinctly robotic, lacking the natural fluidity characteristic\nof mammalian movements. Existing literature on quadrupedal robots with spinal\nconfigurations primarily focuses on energy efficiency and does not consider the\neffects in human-robot interaction scenarios. Our contributions include an\ninitial investigation into various trajectory generation strategies for a\nquadrupedal robot with a four degree of freedom spine, and an analysis on the\neffect that such methods have on human perception of gait naturalness compared\nto a fixed spine baseline. The strategies were evaluated using videos of\nwalking, trotting and turning simulations. Among the four different strategies\ndeveloped, the optimised time varying and the foot-tracking strategies were\nperceived to be more natural than the baseline in a randomised trial with 50\nparticipants. Although none of the strategies demonstrated any energy\nefficiency improvements over the no-spine baseline, some showed greater\nfootfall consistency at higher speeds. Given the greater likeability drawn from\nthe more natural locomotion patterns, this type of robot displays potential for\napplications in social robot scenarios such as elderly care, where energy\nefficiency is not a primary concern."}
{"id": "2505.02142", "pdf": "https://arxiv.org/pdf/2505.02142.pdf", "abs": "https://arxiv.org/abs/2505.02142", "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in long-context reasoning by large language\nmodels (LLMs), primarily through Online Reinforcement Learning (RL) methods,\nthese approaches incur substantial computational costs and complexity. In\ncontrast, simpler and more economical Offline RL methods remain underexplored.\nTo address this gap, we investigate the effectiveness of Offline RL methods,\nspecifically Direct Preference Optimization (DPO) and its length-desensitized\nvariant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive\nexperiments across multiple reasoning benchmarks demonstrate that these simpler\nOffline RL methods substantially improve model performance, achieving an\naverage enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on\nthe challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity\nto output length, emphasizing that increasing reasoning length should align\nwith semantic richness, as indiscriminate lengthening may adversely affect\nmodel performance. We provide comprehensive descriptions of our data processing\nand training methodologies, offering empirical evidence and practical insights\nfor developing more cost-effective Offline RL approaches."}
{"id": "2505.02418", "pdf": "https://arxiv.org/pdf/2505.02418.pdf", "abs": "https://arxiv.org/abs/2505.02418", "title": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM Symbiotic Collaboration", "authors": ["Qiang Sun", "Tingting Bi", "Sirui Li", "Eun-Jung Holden", "Paul Duuring", "Kai Niu", "Wei Liu"], "categories": ["cs.IR", "cs.HC"], "comment": null, "summary": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally\nreimagines Retrieval-Augmented Generation~(RAG) systems by establishing a\nbidirectional learning relationship between humans and machines. Our approach\naddresses two critical challenges in current RAG systems: the inherently\nhuman-centered nature of relevance determination and users' progression from\n\"unconscious incompetence\" in query formulation. SymbioticRAG introduces a\ntwo-tier solution where Level 1 enables direct human curation of retrieved\ncontent through interactive source document exploration, while Level 2 aims to\nbuild personalized retrieval models based on captured user interactions. We\nimplement Level 1 through three key components: (1)~a comprehensive document\nprocessing pipeline with specialized models for layout detection, OCR, and\nextraction of tables, formulas, and figures; (2)~an extensible retriever module\nsupporting multiple retrieval strategies; and (3)~an interactive interface that\nfacilitates both user engagement and interaction data logging. We experiment\nLevel 2 implementation via a retriever strategy incorporated LLM summarized\nuser intention from user interaction logs. To maintain high-quality data\npreparation, we develop a human-on-the-loop validation interface that improves\npipeline output while advancing research in specialized extraction tasks.\nEvaluation across three scenarios (literature review, geological exploration,\nand education) demonstrates significant improvements in retrieval relevance and\nuser satisfaction compared to traditional RAG approaches. To facilitate broader\nresearch and further advancement of SymbioticRAG Level 2 implementation, we\nwill make our system openly accessible to the research community."}
{"id": "2505.02146", "pdf": "https://arxiv.org/pdf/2505.02146.pdf", "abs": "https://arxiv.org/abs/2505.02146", "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach", "authors": ["Shouyang Dong", "Yuanbo Wen", "Jun Bi", "Di Huang", "Jiaming Guo", "Jianxing Xu", "Ruibai Xu", "Xinkai Song", "Yifan Hao", "Xuehai Zhou", "Tianshi Chen", "Qi Guo", "Yunji Chen"], "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": "Accepted to OSDI 2025", "summary": "Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been\nwidely deployed in industrial data centers, which requires to develop multiple\nlow-level tensor programs for different platforms. An attractive solution to\nrelieve the programming burden is to transcompile the legacy code of one\nplatform to others. However, current transcompilation techniques struggle with\neither tremendous manual efforts or functional incorrectness, rendering \"Write\nOnce, Run Anywhere\" of tensor programs an open question.\n  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically\ntranslating tensor programs across DLS via both large language models (LLMs)\nand symbolic program synthesis, i.e., neural-symbolic synthesis. The key\ninsight is leveraging the powerful code generation ability of LLM to make\ncostly search-based symbolic synthesis computationally tractable. Concretely,\nwe propose multiple LLM-assisted compilation passes via pre-defined\nmeta-prompts for program transformation. During each program transformation,\nefficient symbolic program synthesis is employed to repair incorrect code\nsnippets with a limited scale. To attain high performance, we propose a\nhierarchical auto-tuning approach to systematically explore both the parameters\nand sequences of transformation passes. Experiments on 4 DLS with distinct\nprogramming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,\nAMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler\ncorrectly translates different tensor programs at the accuracy of 95% on\naverage, and the performance of translated programs achieves up to 2.0x over\nvendor-provided manually-optimized libraries. As a result, the programming\nproductivity of DLS is improved by up to 96.0x via transcompiling legacy tensor\nprograms."}
{"id": "2505.02443", "pdf": "https://arxiv.org/pdf/2505.02443.pdf", "abs": "https://arxiv.org/abs/2505.02443", "title": "Investigating the Impact of Personalized AI Tutors on Language Learning Performance", "authors": ["Simon Suh"], "categories": ["cs.AI", "cs.HC", "I.2.6; K.3.1"], "comment": "16 pages, 4 figures, 1 table, Uses three theoretical frameworks like\n  Domain modeling, Gardner Theory of Multiple Intelligences, and Zone of\n  Proximal Development", "summary": "Driven by the global shift towards online learning prompted by the COVID 19\npandemic, Artificial Intelligence has emerged as a pivotal player in the field\nof education. Intelligent Tutoring Systems offer a new method of personalized\nteaching, replacing the limitations of traditional teaching methods. However,\nconcerns arise about the ability of AI tutors to address skill development and\nengagement during the learning process. In this paper, I will conduct a quasi\nexperiment with paired sample t test on 34 students pre and post use of AI\ntutors in language learning platforms like Santa and Duolingo to examine the\nrelationship between students engagement, academic performance, and students\nsatisfaction during a personalized language learning experience."}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156.pdf", "abs": "https://arxiv.org/abs/2505.02156", "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The code and data are available, see\n  https://github.com/MozerWang/AMPO. arXiv admin note: text overlap with\n  arXiv:2502.15538 by other authors", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach"}
{"id": "2505.02569", "pdf": "https://arxiv.org/pdf/2505.02569.pdf", "abs": "https://arxiv.org/abs/2505.02569", "title": "HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction", "authors": ["Muhammad Haris Khan", "Miguel Altamirano Cabrera", "Dmitrii Iarchuk", "Yara Mahmoud", "Daria Trinitatova", "Issatay Tokmurziyev", "Dzmitry Tsetserukou"], "categories": ["cs.RO", "cs.HC"], "comment": "Submitted to IEEE conf", "summary": "This paper introduces HapticVLM, a novel multimodal system that integrates\nvision-language reasoning with deep convolutional networks to enable real-time\nhaptic feedback. HapticVLM leverages a ConvNeXt-based material recognition\nmodule to generate robust visual embeddings for accurate identification of\nobject materials, while a state-of-the-art Vision-Language Model\n(Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The\nsystem synthesizes tactile sensations by delivering vibrotactile feedback\nthrough speakers and thermal cues via a Peltier module, thereby bridging the\ngap between visual perception and tactile experience. Experimental evaluations\ndemonstrate an average recognition accuracy of 84.67% across five distinct\nauditory-tactile patterns and a temperature estimation accuracy of 86.7% based\non a tolerance-based evaluation method with an 8{\\deg}C margin of error across\n15 scenarios. Although promising, the current study is limited by the use of a\nsmall set of prominent patterns and a modest participant pool. Future work will\nfocus on expanding the range of tactile patterns and increasing user studies to\nfurther refine and validate the system's performance. Overall, HapticVLM\npresents a significant step toward context-aware, multimodal haptic interaction\nwith potential applications in virtual reality, and assistive technologies."}
{"id": "2505.02164", "pdf": "https://arxiv.org/pdf/2505.02164.pdf", "abs": "https://arxiv.org/abs/2505.02164", "title": "Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use", "authors": ["Justin Ho", "Alexandra Colby", "William Fisher"], "categories": ["cs.CL", "I.2.7; K.5; H.3.3"], "comment": "Submitted to the 7th Workshop on Automated Semantic Analysis of\n  Information in Legal Text. 8 pages, 5 Figures", "summary": "This paper presents a domain-specific implementation of Retrieval-Augmented\nGeneration (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.\nMotivated by the increasing prevalence of DMCA takedowns and the lack of\naccessible legal support for content creators, we propose a structured approach\nthat combines semantic search with legal knowledge graphs and court citation\nnetworks to improve retrieval quality and reasoning reliability. Our prototype\nmodels legal precedents at the statutory factor level (e.g., purpose, nature,\namount, market effect) and incorporates citation-weighted graph representations\nto prioritize doctrinally authoritative sources. We use Chain-of-Thought\nreasoning and interleaved retrieval steps to better emulate legal reasoning.\nPreliminary testing suggests this method improves doctrinal relevance in the\nretrieval process, laying groundwork for future evaluation and deployment of\nLLM-based legal assistance tools."}
{"id": "2401.15695", "pdf": "https://arxiv.org/pdf/2401.15695.pdf", "abs": "https://arxiv.org/abs/2401.15695", "title": "HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation", "authors": ["David Bethge", "Daniel Bulanda", "Adam Kozlowski", "Thomas Kosch", "Albrecht Schmidt", "Tobias Grosse-Puppendahl"], "categories": ["cs.HC", "cs.LG"], "comment": "17 pages", "summary": "Routes represent an integral part of triggering emotions in drivers.\nNavigation systems allow users to choose a navigation strategy, such as the\nfastest or shortest route. However, they do not consider the driver's emotional\nwell-being. We present HappyRouting, a novel navigation-based empathic car\ninterface guiding drivers through real-world traffic while evoking positive\nemotions. We propose design considerations, derive a technical architecture,\nand implement a routing optimization framework. Our contribution is a machine\nlearning-based generated emotion map layer, predicting emotions along routes\nbased on static and dynamic contextual data. We evaluated HappyRouting in a\nreal-world driving study (N=13), finding that happy routes increase\nsubjectively perceived valence by 11% (p=.007). Although happy routes take 1.25\ntimes longer on average, participants perceived the happy route as shorter,\npresenting an emotion-enhanced alternative to today's fastest routing\nmechanisms. We discuss how emotion-based routing can be integrated into\nnavigation apps, promoting emotional well-being for mobility use."}
{"id": "2505.02171", "pdf": "https://arxiv.org/pdf/2505.02171.pdf", "abs": "https://arxiv.org/abs/2505.02171", "title": "A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking", "authors": ["Henrik Brådland", "Morten Goodwin", "Per-Arne Andersen", "Alexander S. Nossum", "Aditya Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, To be published in SIGIR25", "summary": "Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)\nby determining how source materials are segmented before indexing. Despite\nevidence that Large Language Models (LLMs) are sensitive to the layout and\nstructure of retrieved data, there is currently no framework to analyze the\nimpact of different chunking methods. In this paper, we introduce a novel\nmethodology that defines essential characteristics of the chunking process at\nthree levels: intrinsic passage properties, extrinsic passage properties, and\npassages-document coherence. We propose HOPE (Holistic Passage Evaluation), a\ndomain-agnostic, automatic evaluation metric that quantifies and aggregates\nthese characteristics. Our empirical evaluations across seven domains\ndemonstrate that the HOPE metric correlates significantly (p > 0.13) with\nvarious RAG performance indicators, revealing contrasts between the importance\nof extrinsic and intrinsic properties of passages. Semantic independence\nbetween passages proves essential for system performance with a performance\ngain of up to 56.2% in factual correctness and 21.1% in answer correctness. On\nthe contrary, traditional assumptions about maintaining concept unity within\npassages show minimal impact. These findings provide actionable insights for\noptimizing chunking strategies, thus improving RAG system design to produce\nmore factually correct responses."}
{"id": "2411.03137", "pdf": "https://arxiv.org/pdf/2411.03137.pdf", "abs": "https://arxiv.org/abs/2411.03137", "title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice", "authors": ["Alicia Guo", "Shreya Sathyanarayanan", "Leijie Wang", "Jeffrey Heer", "Amy Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Creative writing is a deeply human craft, yet AI systems using large language\nmodels (LLMs) offer the automation of significant parts of the writing process.\nSo why do some creative writers choose to use AI? Through interviews and\nobserved writing sessions with 18 creative writers who already use AI regularly\nin their writing practice, we find that creative writers are intentional about\nhow they incorporate AI, making many deliberate decisions about when and how to\nengage AI based on their core values, such as authenticity and craftsmanship.\nWe characterize the interplay between writers' values, their fluid\nrelationships with AI, and specific integration strategies -- ultimately\nenabling writers to create new AI workflows without compromising their creative\nvalues. We provide insight for writing communities, AI developers and future\nresearchers on the importance of supporting transparency of these emerging\nwriting processes and rethinking what AI features can best serve writers."}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172.pdf", "abs": "https://arxiv.org/abs/2505.02172", "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "authors": ["Chuck Arvin"], "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks."}
{"id": "2501.10977", "pdf": "https://arxiv.org/pdf/2501.10977.pdf", "abs": "https://arxiv.org/abs/2501.10977", "title": "SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality", "authors": ["Roberto Daza", "Lin Shengkai", "Aythami Morales", "Julian Fierrez", "Katashi Nagao"], "categories": ["cs.HC", "cs.CV"], "comment": "Presented at the Workshop on Artificial Intelligence for Education\n  (AI4EDU) at AAAI 2025, and also at the Workshop on Computer Vision for Mixed\n  Reality (CV4MR) at CVPR 2025", "summary": "This work introduces SMARTe-VR, a platform for student monitoring in an\nimmersive virtual reality environment designed for online education. SMARTe-VR\naims to collect data for adaptive learning, focusing on facial biometrics and\nlearning metadata. The platform allows instructors to create customized\nlearning sessions with video lectures, featuring an interface with an AutoQA\nsystem to evaluate understanding, interaction tools (e.g., textbook\nhighlighting and lecture tagging), and real-time feedback. Furthermore, we\nreleased a dataset that contains 5 research challenges with data from 10 users\nin VR-based TOEIC sessions. This data set, which spans more than 25 hours,\nincludes facial features, learning metadata, 450 responses, difficulty levels\nof the questions, concept tags, and understanding labels. Alongside the\ndatabase, we present preliminary experiments using Item Response Theory models,\nadapted for understanding detection using facial features. Two architectures\nwere explored: a Temporal Convolutional Network for local features and a\nMultilayer Perceptron for global features."}
{"id": "2505.02177", "pdf": "https://arxiv.org/pdf/2505.02177.pdf", "abs": "https://arxiv.org/abs/2505.02177", "title": "Measuring Hong Kong Massive Multi-Task Language Understanding", "authors": ["Chuxue Cao", "Zhenghao Zhu", "Junqi Zhu", "Guoying Lu", "Siyu Peng", "Juntao Dai", "Weijie Shi", "Sirui Han", "Yike Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual understanding is crucial for the cross-cultural applicability of\nLarge Language Models (LLMs). However, evaluation benchmarks designed for Hong\nKong's unique linguistic landscape, which combines Traditional Chinese script\nwith Cantonese as the spoken form and its cultural context, remain\nunderdeveloped. To address this gap, we introduce HKMMLU, a multi-task language\nunderstanding benchmark that evaluates Hong Kong's linguistic competence and\nsocio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions\nacross 66 subjects, organized into four categories: Science, Technology,\nEngineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To\nevaluate the multilingual understanding ability of LLMs, 90,550\nMandarin-Cantonese translation tasks were additionally included. We conduct\ncomprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs\nof varying sizes on HKMMLU. The results show that the best-performing model,\nDeepSeek-V3, struggles to achieve an accuracy of 75\\%, significantly lower than\nthat of MMLU and CMMLU. This performance gap highlights the need to improve\nLLMs' capabilities in Hong Kong-specific language and knowledge domains.\nFurthermore, we investigate how question language, model size, prompting\nstrategies, and question and reasoning token lengths affect model performance.\nWe anticipate that HKMMLU will significantly advance the development of LLMs in\nmultilingual and cross-cultural contexts, thereby enabling broader and more\nimpactful applications."}
{"id": "2502.09203", "pdf": "https://arxiv.org/pdf/2502.09203.pdf", "abs": "https://arxiv.org/abs/2502.09203", "title": "Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces", "authors": ["Dongrui Wu"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Due to large intra-subject and inter-subject variabilities of\nelectroencephalogram (EEG) signals, EEG-based brain-computer interfaces (BCIs)\nusually need subject-specific calibration to tailor the decoding algorithm for\neach new subject, which is time-consuming and user-unfriendly, hindering their\nreal-world applications. Transfer learning (TL) has been extensively used to\nexpedite the calibration, by making use of EEG data from other\nsubjects/sessions. An important consideration in TL for EEG-based BCIs is to\nreduce the data distribution discrepancies among different subjects/sessions,\nto avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to\naddress this challenge. Numerous experiments from 13 different BCI paradigms\ndemonstrated its effectiveness and efficiency. This paper revisits EA,\nexplaining its procedure and correct usage, introducing its applications and\nextensions, and pointing out potential new research directions. It should be\nvery helpful to BCI researchers, especially those who are working on EEG signal\ndecoding."}
{"id": "2505.02235", "pdf": "https://arxiv.org/pdf/2505.02235.pdf", "abs": "https://arxiv.org/abs/2505.02235", "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation", "authors": ["Tanguy Herserant", "Vincent Guigue"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination."}
{"id": "2502.18881", "pdf": "https://arxiv.org/pdf/2502.18881.pdf", "abs": "https://arxiv.org/abs/2502.18881", "title": "Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration", "authors": ["Hayeon Jeon", "Suhwoo Yoon", "Keyeun Lee", "Seo Hyeong Kim", "Esther Hehsun Kim", "Seonghye Cho", "Yena Ko", "Soeun Yang", "Laura Dabbish", "John Zimmerman", "Eun-mee Kim", "Hajin Lim"], "categories": ["cs.HC"], "comment": "21 pages, 9 figures, Proceedings of the 2025 CHI Conference on Human\n  Factors in Computing Systems (Best Paper Award, Top 1%)", "summary": "Young adults often encounter challenges in career exploration. Self-guided\ninterventions, such as the letter-exchange exercise, where participants\nenvision and adopt the perspective of their future selves by exchanging letters\nwith their envisioned future selves, can support career development. However,\nthe broader adoption of such interventions may be limited without structured\nguidance. To address this, we integrated Large Language Model (LLM)-based\nagents that simulate participants' future selves into the letter-exchange\nexercise and evaluated their effectiveness. A one-week experiment (N=36)\ncompared three conditions: (1) participants manually writing replies to\nthemselves from the perspective of their future selves (baseline), (2)\nfuture-self agents generating letters to participants, and (3) future-self\nagents engaging in chat conversations with participants. Results indicated that\nexchanging letters with future-self agents enhanced participants' engagement\nduring the exercise, while overall benefits of the intervention on future\norientation, career self-concept, and psychological support remained comparable\nacross conditions. We discuss design implications for AI-augmented\ninterventions for supporting young adults' career exploration."}
{"id": "2505.02252", "pdf": "https://arxiv.org/pdf/2505.02252.pdf", "abs": "https://arxiv.org/abs/2505.02252", "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models", "authors": ["Paloma Piot", "Patricia Martín-Rodilla", "Javier Parapar"], "categories": ["cs.CL"], "comment": null, "summary": "Commercial Large Language Models (LLMs) have recently incorporated memory\nfeatures to deliver personalised responses. This memory retains details such as\nuser demographics and individual characteristics, allowing LLMs to adjust their\nbehaviour based on personal information. However, the impact of integrating\npersonalised information into the context has not been thoroughly assessed,\nleading to questions about its influence on LLM behaviour. Personalisation can\nbe challenging, particularly with sensitive topics. In this paper, we examine\nvarious state-of-the-art LLMs to understand their behaviour in different\npersonalisation scenarios, specifically focusing on hate speech. We prompt the\nmodels to assume country-specific personas and use different languages for hate\nspeech detection. Our findings reveal that context personalisation\nsignificantly influences LLMs' responses in this sensitive area. To mitigate\nthese unwanted biases, we fine-tune the LLMs by penalising inconsistent hate\nspeech classifications made with and without country or language-specific\ncontext. The refined models demonstrate improved performance in both\npersonalised contexts and when no context is provided."}
{"id": "2503.09805", "pdf": "https://arxiv.org/pdf/2503.09805.pdf", "abs": "https://arxiv.org/abs/2503.09805", "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models", "authors": ["Jordan Taylor", "Joel Mire", "Franchesca Spektor", "Alicia DeVrio", "Maarten Sap", "Haiyi Zhu", "Sarah Fox"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives."}
{"id": "2505.02266", "pdf": "https://arxiv.org/pdf/2505.02266.pdf", "abs": "https://arxiv.org/abs/2505.02266", "title": "Parameter-Efficient Transformer Embeddings", "authors": ["Henry Ndubuaku", "Mouad Talhi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07 (Primary) 68T50 (Secondary)"], "comment": "7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete", "summary": "Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings."}
{"id": "2504.04592", "pdf": "https://arxiv.org/pdf/2504.04592.pdf", "abs": "https://arxiv.org/abs/2504.04592", "title": "\"Trust me on this\" Explaining Agent Behavior to a Human Terminator", "authors": ["Uri Menkes", "Assaf Hallak", "Ofra Amir"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 3 figures, in proceedings of ICML 2024 Workshop on Models of\n  Human Feedback for AI Alignment", "summary": "Consider a setting where a pre-trained agent is operating in an environment\nand a human operator can decide to temporarily terminate its operation and\ntake-over for some duration of time. These kind of scenarios are common in\nhuman-machine interactions, for example in autonomous driving, factory\nautomation and healthcare. In these settings, we typically observe a trade-off\nbetween two extreme cases -- if no take-overs are allowed, then the agent might\nemploy a sub-optimal, possibly dangerous policy. Alternatively, if there are\ntoo many take-overs, then the human has no confidence in the agent, greatly\nlimiting its usefulness. In this paper, we formalize this setup and propose an\nexplainability scheme to help optimize the number of human interventions."}
{"id": "2505.02273", "pdf": "https://arxiv.org/pdf/2505.02273.pdf", "abs": "https://arxiv.org/abs/2505.02273", "title": "Demystifying optimized prompts in language models", "authors": ["Rimon Melamed", "Lucas H. McCabe", "H. Howie Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Modern language models (LMs) are not robust to out-of-distribution inputs.\nMachine generated (``optimized'') prompts can be used to modulate LM outputs\nand induce specific behaviors while appearing completely uninterpretable. In\nthis work, we investigate the composition of optimized prompts, as well as the\nmechanisms by which LMs parse and build predictions from optimized prompts. We\nfind that optimized prompts primarily consist of punctuation and noun tokens\nwhich are more rare in the training data. Internally, optimized prompts are\nclearly distinguishable from natural language counterparts based on sparse\nsubsets of the model's activations. Across various families of\ninstruction-tuned models, optimized prompts follow a similar path in how their\nrepresentations form through the network."}
{"id": "2504.15984", "pdf": "https://arxiv.org/pdf/2504.15984.pdf", "abs": "https://arxiv.org/abs/2504.15984", "title": "Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems", "authors": ["Lukas Gehrke", "Aleksandrs Koselevs", "Marius Klug", "Klaus Gramann"], "categories": ["cs.HC"], "comment": "15 pages, 6 figures", "summary": "Neuroadaptive haptics offers a path to more immersive extended reality (XR)\nexperiences by dynamically tuning multisensory feedback to user preferences. We\npresent a neuroadaptive haptics system that adapts XR feedback through\nreinforcement learning (RL) from explicit user ratings and brain-decoded neural\nsignals. In a user study, participants interacted with virtual objects in VR\nwhile Electroencephalography (EEG) data were recorded. An RL agent adjusted\nhaptic feedback based either on explicit ratings or on outputs from a neural\ndecoder. Results show that the RL agent's performance was comparable across\nfeedback sources, suggesting that implicit neural feedback can effectively\nguide personalization without requiring active user input. The EEG-based neural\ndecoder achieved a mean F1 score of 0.8, supporting reliable classification of\nuser experience. These findings demonstrate the feasibility of combining\nbrain-computer interfaces (BCI) and RL to autonomously adapt XR interactions,\nreducing cognitive load and enhancing immersion."}
{"id": "2505.02304", "pdf": "https://arxiv.org/pdf/2505.02304.pdf", "abs": "https://arxiv.org/abs/2505.02304", "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition", "authors": ["Siyu Liang", "Yunan Li", "Wentian Xin", "Huizhou Chen", "Xujie Liu", "Kang Liu", "Qiguang Miao"], "categories": ["cs.CL", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies."}
{"id": "2502.15666", "pdf": "https://arxiv.org/pdf/2502.15666.pdf", "abs": "https://arxiv.org/abs/2502.15666", "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing", "authors": ["Shoumik Saha", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "18 pages, 18 figures, 6 tables", "summary": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies."}
{"id": "2505.02311", "pdf": "https://arxiv.org/pdf/2505.02311.pdf", "abs": "https://arxiv.org/abs/2505.02311", "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering", "authors": ["Jihao Zhao", "Chunlai Zhou", "Biao Qin"], "categories": ["cs.CL"], "comment": null, "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs."}
{"id": "2504.11936", "pdf": "https://arxiv.org/pdf/2504.11936.pdf", "abs": "https://arxiv.org/abs/2504.11936", "title": "Mind2Matter: Creating 3D Models from EEG Signals", "authors": ["Xia Deng", "Shen Chen", "Jiale Zhou", "Lei Li"], "categories": ["cs.GR", "cs.HC", "eess.SP"], "comment": null, "summary": "The reconstruction of 3D objects from brain signals has gained significant\nattention in brain-computer interface (BCI) research. Current research\npredominantly utilizes functional magnetic resonance imaging (fMRI) for 3D\nreconstruction tasks due to its excellent spatial resolution. Nevertheless, the\nclinical utility of fMRI is limited by its prohibitive costs and inability to\nsupport real-time operations. In comparison, electroencephalography (EEG)\npresents distinct advantages as an affordable, non-invasive, and mobile\nsolution for real-time brain-computer interaction systems. While recent\nadvances in deep learning have enabled remarkable progress in image generation\nfrom neural data, decoding EEG signals into structured 3D representations\nremains largely unexplored. In this paper, we propose a novel framework that\ntranslates EEG recordings into 3D object reconstructions by leveraging neural\ndecoding techniques and generative models. Our approach involves training an\nEEG encoder to extract spatiotemporal visual features, fine-tuning a large\nlanguage model to interpret these features into descriptive multimodal outputs,\nand leveraging generative 3D Gaussians with layout-guided control to synthesize\nthe final 3D structures. Experiments demonstrate that our model captures\nsalient geometric and semantic features, paving the way for applications in\nbrain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our\ncode is available in https://github.com/sddwwww/Mind2Matter."}
{"id": "2505.02363", "pdf": "https://arxiv.org/pdf/2505.02363.pdf", "abs": "https://arxiv.org/abs/2505.02363", "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning", "authors": ["Tianjian Li", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": "To appear in ICML 2025", "summary": "Aligning language models with human preferences relies on pairwise preference\ndatasets. While some studies suggest that on-policy data consistently\noutperforms off -policy data for preference learning, others indicate that the\nadvantages of on-policy data may be task-dependent, highlighting the need for a\nsystematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary\nstrengths in preference optimization: on-policy data is particularly effective\nfor reasoning tasks like math and coding, while off-policy data performs better\non open-ended tasks such as creative writing and making personal\nrecommendations. Guided by these findings, we introduce SIMPLEMIX, an approach\nto combine the complementary strengths of on-policy and off-policy preference\nlearning by simply mixing these two data sources. Our empirical results across\ndiverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves\nlanguage model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO\nand off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it\noutperforms prior approaches that are much more complex in combining on- and\noff-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%."}
{"id": "2504.20903", "pdf": "https://arxiv.org/pdf/2504.20903.pdf", "abs": "https://arxiv.org/abs/2504.20903", "title": "Modeling AI-Human Collaboration as a Multi-Agent Adaptation", "authors": ["Prothit Sen", "Sai Mihir Jakkaraju"], "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": null, "summary": "We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings."}
{"id": "2505.02366", "pdf": "https://arxiv.org/pdf/2505.02366.pdf", "abs": "https://arxiv.org/abs/2505.02366", "title": "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings", "authors": ["Tianyu Zong", "Hongzhu Yi", "Bingkang Shi", "Yuanxiang Wang", "Jungang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks."}
{"id": "2505.02387", "pdf": "https://arxiv.org/pdf/2505.02387.pdf", "abs": "https://arxiv.org/abs/2505.02387", "title": "RM-R1: Reward Modeling as Reasoning", "authors": ["Xiusi Chen", "Gaotang Li", "Ziqi Wang", "Bowen Jin", "Cheng Qian", "Yu Wang", "Hongru Wang", "Yu Zhang", "Denghui Zhang", "Tong Zhang", "Hanghang Tong", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 7 figures", "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1."}
{"id": "2505.02410", "pdf": "https://arxiv.org/pdf/2505.02410.pdf", "abs": "https://arxiv.org/abs/2505.02410", "title": "Bielik 11B v2 Technical Report", "authors": ["Krzysztof Ociepa", "Łukasz Flis", "Krzysztof Wróbel", "Adrian Gwoździej", "Remigiusz Kinas"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages."}
{"id": "2505.02456", "pdf": "https://arxiv.org/pdf/2505.02456.pdf", "abs": "https://arxiv.org/abs/2505.02456", "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs", "authors": ["Elisa Forcada Rodríguez", "Olatz Perez-de-Viñaspre", "Jon Ander Campos", "Dietrich Klakow", "Vagrant Gautam"], "categories": ["cs.CL"], "comment": null, "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work."}
{"id": "2505.02463", "pdf": "https://arxiv.org/pdf/2505.02463.pdf", "abs": "https://arxiv.org/abs/2505.02463", "title": "Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda", "authors": ["Richard Kimera", "Dongnyeong Heo", "Daniela N. Rim", "Heeyoul Choi"], "categories": ["cs.CL"], "comment": "NLPIR '24: Proceedings of the 2024 8th International Conference on\n  Natural Language Processing and Information Retrieval", "summary": "In this paper,we explore the application of Back translation (BT) as a\nsemi-supervised technique to enhance Neural Machine Translation(NMT) models for\nthe English-Luganda language pair, specifically addressing the challenges faced\nby low-resource languages. The purpose of our study is to demonstrate how BT\ncan mitigate the scarcity of bilingual data by generating synthetic data from\nmonolingual corpora. Our methodology involves developing custom NMT models\nusing both publicly available and web-crawled data, and applying Iterative and\nIncremental Back translation techniques. We strategically select datasets for\nincremental back translation across multiple small datasets, which is a novel\nelement of our approach. The results of our study show significant\nimprovements, with translation performance for the English-Luganda pair\nexceeding previous benchmarks by more than 10 BLEU score units across all\ntranslation directions. Additionally, our evaluation incorporates comprehensive\nassessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced\nunderstanding of translation quality. The conclusion drawn from our research\nconfirms the efficacy of BT when strategically curated datasets are utilized,\nestablishing new performance benchmarks and demonstrating the potential of BT\nin enhancing NMT models for low-resource languages."}
{"id": "2505.02518", "pdf": "https://arxiv.org/pdf/2505.02518.pdf", "abs": "https://arxiv.org/abs/2505.02518", "title": "Bemba Speech Translation: Exploring a Low-Resource African Language", "authors": ["Muhammad Hazim Al Farouq", "Aman Kassahun Wassie", "Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2025), low-resource languages track, namely\nfor Bemba-to-English speech translation. We built cascaded speech translation\nsystems based on Whisper and NLLB-200, and employed data augmentation\ntechniques, such as back-translation. We investigate the effect of using\nsynthetic data and discuss our experimental setup."}
{"id": "2505.02579", "pdf": "https://arxiv.org/pdf/2505.02579.pdf", "abs": "https://arxiv.org/abs/2505.02579", "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning", "authors": ["Lingxiao Kong", "Cong Yang", "Susanne Neufang", "Oya Deniz Beyan", "Zeyd Boukhers"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference", "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."}
{"id": "2505.02590", "pdf": "https://arxiv.org/pdf/2505.02590.pdf", "abs": "https://arxiv.org/abs/2505.02590", "title": "Ensemble Kalman filter for uncertainty in human language comprehension", "authors": ["Diksha Bhandari", "Alessandro Lopopolo", "Milena Rabovsky", "Sebastian Reich"], "categories": ["cs.CL", "stat.AP", "stat.ML"], "comment": null, "summary": "Artificial neural networks (ANNs) are widely used in modeling sentence\nprocessing but often exhibit deterministic behavior, contrasting with human\nsentence comprehension, which manages uncertainty during ambiguous or\nunexpected inputs. This is exemplified by reversal anomalies-sentences with\nunexpected role reversals that challenge syntax and semantics-highlighting the\nlimitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.\nTo address these limitations, we propose a Bayesian framework for sentence\ncomprehension, applying an extension of the ensemble Kalman filter (EnKF) for\nBayesian inference to quantify uncertainty. By framing language comprehension\nas a Bayesian inverse problem, this approach enhances the SG model's ability to\nreflect human sentence processing with respect to the representation of\nuncertainty. Numerical experiments and comparisons with maximum likelihood\nestimation (MLE) demonstrate that Bayesian methods improve uncertainty\nrepresentation, enabling the model to better approximate human cognitive\nprocessing when dealing with linguistic ambiguities."}
{"id": "2505.02615", "pdf": "https://arxiv.org/pdf/2505.02615.pdf", "abs": "https://arxiv.org/abs/2505.02615", "title": "Automatic Proficiency Assessment in L2 English Learners", "authors": ["Armita Mohammadi", "Alessandro Lameiras Koerich", "Laureano Moro-Velazquez", "Patrick Cardinal"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "6 pages", "summary": "Second language proficiency (L2) in English is usually perceptually evaluated\nby English teachers or expert evaluators, with the inherent intra- and\ninter-rater variability. This paper explores deep learning techniques for\ncomprehensive L2 proficiency assessment, addressing both the speech signal and\nits correspondent transcription. We analyze spoken proficiency classification\nprediction using diverse architectures, including 2D CNN, frequency-based CNN,\nResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based\nproficiency assessment by fine-tuning a BERT language model within resource\nconstraints. Finally, we tackle the complex task of spontaneous dialogue\nassessment, managing long-form audio and speaker interactions through separate\napplications of wav2vec 2.0 and BERT models. Results from experiments on\nEFCamDat and ANGLISH datasets and a private dataset highlight the potential of\ndeep learning, especially the pretrained wav2vec 2.0 model, for robust\nautomated L2 proficiency evaluation."}
{"id": "2505.02625", "pdf": "https://arxiv.org/pdf/2505.02625.pdf", "abs": "https://arxiv.org/abs/2505.02625", "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis", "authors": ["Qingkai Fang", "Yan Zhou", "Shoutao Guo", "Shaolei Zhang", "Yang Feng"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2", "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data."}
{"id": "2505.02656", "pdf": "https://arxiv.org/pdf/2505.02656.pdf", "abs": "https://arxiv.org/abs/2505.02656", "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset", "authors": ["Rawan Bondok", "Mayar Nassar", "Salam Khalifa", "Kurt Micallaf", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Proper names in Arabic Wikipedia are frequently undiacritized, creating\nambiguity in pronunciation and interpretation, especially for transliterated\nnamed entities of foreign origin. While transliteration and diacritization have\nbeen well-studied separately in Arabic NLP,their intersection remains\nunderexplored. In this paper, we introduce a new manually diacritized dataset\nof Arabic proper names of various origins with their English Wikipedia\nequivalent glosses, and present the challenges and guidelines we followed to\ncreate it. We benchmark GPT-4o on the task of recovering full diacritization\ngiven the undiacritized Arabic and English forms, and analyze its performance.\nAchieving 73% accuracy, our results underscore both the difficulty of the task\nand the need for improved models and resources. We release our dataset to\nfacilitate further research on Arabic Wikipedia proper name diacritization."}
{"id": "2505.02666", "pdf": "https://arxiv.org/pdf/2505.02666.pdf", "abs": "https://arxiv.org/abs/2505.02666", "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design", "authors": ["Miaomiao Ji", "Yanqiu Wu", "Zhibin Wu", "Shoujin Wang", "Jian Yang", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies."}
{"id": "2505.02686", "pdf": "https://arxiv.org/pdf/2505.02686.pdf", "abs": "https://arxiv.org/abs/2505.02686", "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models", "authors": ["Xiaobao Wu"], "categories": ["cs.CL"], "comment": "35 Pages", "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers."}
{"id": "2505.02692", "pdf": "https://arxiv.org/pdf/2505.02692.pdf", "abs": "https://arxiv.org/abs/2505.02692", "title": "fastabx: A library for efficient computation of ABX discriminability", "authors": ["Maxime Poli", "Emmanuel Chemla", "Emmanuel Dupoux"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "8 pages, 6 figures", "summary": "We introduce fastabx, a high-performance Python library for building ABX\ndiscrimination tasks. ABX is a measure of the separation between generic\ncategories of interest. It has been used extensively to evaluate phonetic\ndiscriminability in self-supervised speech representations. However, its\nbroader adoption has been limited by the absence of adequate tools. fastabx\naddresses this gap by providing a framework capable of constructing any type of\nABX task while delivering the efficiency necessary for rapid development\ncycles, both in task creation and in calculating distances between\nrepresentations. We believe that fastabx will serve as a valuable resource for\nthe broader representation learning community, enabling researchers to\nsystematically investigate what information can be directly extracted from\nlearned representations across several domains beyond speech processing. The\nsource code is available at https://github.com/bootphon/fastabx."}
{"id": "2505.02763", "pdf": "https://arxiv.org/pdf/2505.02763.pdf", "abs": "https://arxiv.org/abs/2505.02763", "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models", "authors": ["Matthew Dahl"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount."}
{"id": "2505.02819", "pdf": "https://arxiv.org/pdf/2505.02819.pdf", "abs": "https://arxiv.org/abs/2505.02819", "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."}
{"id": "2505.01433", "pdf": "https://arxiv.org/pdf/2505.01433.pdf", "abs": "https://arxiv.org/abs/2505.01433", "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations", "authors": ["Cong Qi", "Hanzhang Fang", "Siqi jiang", "Tianxing Hu", "Wei Zhi"], "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the binding specificity between T-cell receptors (TCRs) and\npeptide-major histocompatibility complexes (pMHCs) is central to immunotherapy\nand vaccine development. However, current predictive models struggle with\ngeneralization, especially in data-scarce settings and when faced with novel\nepitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced\nRecognition Network), a deep learning framework that combines large-scale\nprotein language models with chemical representations of peptides. By encoding\nTCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences\ninto SMILES strings processed by MolFormer, LANTERN captures rich biological\nand chemical features critical for TCR-peptide recognition. Through extensive\nbenchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,\nLANTERN demonstrates superior performance, particularly in zero-shot and\nfew-shot learning scenarios. Our model also benefits from a robust negative\nsampling strategy and shows significant clustering improvements via embedding\nanalysis. These results highlight the potential of LANTERN to advance TCR-pMHC\nbinding prediction and support the development of personalized immunotherapies."}
{"id": "2505.01435", "pdf": "https://arxiv.org/pdf/2505.01435.pdf", "abs": "https://arxiv.org/abs/2505.01435", "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine", "authors": ["Carlo Siebenschuh", "Kyle Hippe", "Ozan Gokdemir", "Alexander Brace", "Arham Khan", "Khalid Hossain", "Yadu Babuji", "Nicholas Chia", "Venkatram Vishwanath", "Rick Stevens", "Arvind Ramanathan", "Ian Foster", "Robert Underwood"], "categories": ["cs.IR", "cs.CL", "cs.DC", "cs.LG"], "comment": "This paper has been accepted at the The Eighth Annual Conference on\n  Machine Learning and Systems (MLSys 2025)", "summary": "Language models for scientific tasks are trained on text from scientific\npublications, most distributed as PDFs that require parsing. PDF parsing\napproaches range from inexpensive heuristics (for simple documents) to\ncomputationally intensive ML-driven systems (for complex or degraded ones). The\nchoice of the \"best\" parser for a particular document depends on its\ncomputational cost and the accuracy of its output. To address these issues, we\nintroduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine\n(AdaParse), a data-driven strategy for assigning an appropriate parser to each\ndocument. We enlist scientists to select preferred parser outputs and\nincorporate this information through direct preference optimization (DPO) into\nAdaParse, thereby aligning its selection process with human judgment. AdaParse\nthen incorporates hardware requirements and predicted accuracy of each parser\nto orchestrate computational resources efficiently for large-scale parsing\ncampaigns. We demonstrate that AdaParse, when compared to state-of-the-art\nparsers, improves throughput by $17\\times$ while still achieving comparable\naccuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.\nAdaParse's combination of high accuracy and parallel scalability makes it\nfeasible to parse large-scale scientific document corpora to support the\ndevelopment of high-quality, trillion-token-scale text datasets. The\nimplementation is available at https://github.com/7shoe/AdaParse/"}
{"id": "2505.01485", "pdf": "https://arxiv.org/pdf/2505.01485.pdf", "abs": "https://arxiv.org/abs/2505.01485", "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code", "authors": ["Tasnim Ahmed", "Salimur Choudhury"], "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted for presentation at the 19th Learning\n  and Intelligent Optimization Conference (LION 19)", "summary": "Linear Programming (LP) problems aim to find the optimal solution to an\nobjective under constraints. These problems typically require domain knowledge,\nmathematical skills, and programming ability, presenting significant challenges\nfor non-experts. This study explores the efficiency of Large Language Models\n(LLMs) in generating solver-specific LP code. We propose CHORUS, a\nretrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP\ncode from natural language problem statements. CHORUS incorporates a\nhierarchical tree-like chunking strategy for theoretical contents and generates\nadditional metadata based on code examples from documentation to facilitate\nself-contained, semantically coherent retrieval. Two-stage retrieval approach\nof CHORUS followed by cross-encoder reranking further ensures contextual\nrelevance. Finally, expertly crafted prompt and structured parser with\nreasoning steps improve code generation performance significantly. Experiments\non the NL4Opt-Code benchmark show that CHORUS improves the performance of\nopen-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1\n(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and\nconventional RAG. It also allows these open-source LLMs to outperform or match\nthe performance of much stronger baselines-GPT3.5 and GPT4 while requiring far\nfewer computational resources. Ablation studies further demonstrate the\nimportance of expert prompting, hierarchical chunking, and structured\nreasoning."}
{"id": "2505.01636", "pdf": "https://arxiv.org/pdf/2505.01636.pdf", "abs": "https://arxiv.org/abs/2505.01636", "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation", "authors": ["Amit Rath"], "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; H.2.8; D.2.13"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and task generalization. However, their\napplication to structured data analysis remains fragile due to inconsistencies\nin schema interpretation, misalignment between user intent and model output,\nand limited mechanisms for self-correction when failures occur. This paper\nintroduces the STROT Framework (Structured Task Reasoning and Output\nTransformation), a method for structured prompting and feedback-driven\ntransformation logic generation aimed at improving the reliability and semantic\nalignment of LLM-based analytical workflows. STROT begins with lightweight\nschema introspection and sample-based field classification, enabling dynamic\ncontext construction that captures both the structure and statistical profile\nof the input data. This contextual information is embedded in structured\nprompts that guide the model toward generating task-specific, interpretable\noutputs. To address common failure modes in complex queries, STROT incorporates\na refinement mechanism in which the model iteratively revises its outputs based\non execution feedback and validation signals. Unlike conventional approaches\nthat rely on static prompts or single-shot inference, STROT treats the LLM as a\nreasoning agent embedded within a controlled analysis loop -- capable of\nadjusting its output trajectory through planning and correction. The result is\na robust and reproducible framework for reasoning over structured data with\nLLMs, applicable to diverse data exploration and analysis tasks where\ninterpretability, stability, and correctness are essential."}
{"id": "2505.01706", "pdf": "https://arxiv.org/pdf/2505.01706.pdf", "abs": "https://arxiv.org/abs/2505.01706", "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm", "authors": ["Sarvesh Shashidhar", "Ritik", "Nachiketa Patil", "Suraj Racha", "Ganesh Ramakrishnan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Updated abstract, algorithm and experimental results", "summary": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present."}
{"id": "2505.01754", "pdf": "https://arxiv.org/pdf/2505.01754.pdf", "abs": "https://arxiv.org/abs/2505.01754", "title": "Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias", "authors": ["Orlando Jähde", "Thorsten Weber", "Rüdiger Buchkremer"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.MA", "68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05,\n  62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20", "I.2; H.3; I.5; I.7; H.5; H.1"], "comment": null, "summary": "Biased news reporting poses a significant threat to informed decision-making\nand the functioning of democracies. This study introduces a novel methodology\nfor scalable, minimally biased analysis of media bias in political news. The\nproposed approach examines event selection, labeling, word choice, and\ncommission and omission biases across news sources by leveraging natural\nlanguage processing techniques, including hierarchical topic modeling,\nsentiment analysis, and ontology learning with large language models. Through\nthree case studies related to current political events, we demonstrate the\nmethodology's effectiveness in identifying biases across news sources at\nvarious levels of granularity. This work represents a significant step towards\nscalable, minimally biased media bias analysis, laying the groundwork for tools\nto help news consumers navigate an increasingly complex media landscape."}
{"id": "2505.01790", "pdf": "https://arxiv.org/pdf/2505.01790.pdf", "abs": "https://arxiv.org/abs/2505.01790", "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos", "authors": ["Markos Stamatakis", "Joshua Berger", "Christian Wartena", "Ralph Ewerth", "Anett Hoppe"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "12 pages (excluding references), 8 tables, 1 equation", "summary": "Web-based educational videos offer flexible learning opportunities and are\nbecoming increasingly popular. However, improving user engagement and knowledge\nretention remains a challenge. Automatically generated questions can activate\nlearners and support their knowledge acquisition. Further, they can help\nteachers and learners assess their understanding. While large language and\nvision-language models have been employed in various tasks, their application\nto question generation for educational videos remains underexplored. In this\npaper, we investigate the capabilities of current vision-language models for\ngenerating learning-oriented questions for educational video content. We assess\n(1) out-of-the-box models' performance; (2) fine-tuning effects on\ncontent-specific question generation; (3) the impact of different video\nmodalities on question quality; and (4) in a qualitative study, question\nrelevance, answerability, and difficulty levels of generated questions. Our\nfindings delineate the capabilities of current vision-language models,\nhighlighting the need for fine-tuning and addressing challenges in question\ndiversity and relevance. We identify requirements for future multimodal\ndatasets and outline promising research directions."}
{"id": "2505.01944", "pdf": "https://arxiv.org/pdf/2505.01944.pdf", "abs": "https://arxiv.org/abs/2505.01944", "title": "Explainability by design: an experimental analysis of the legal coding process", "authors": ["Matteo Cristani", "Guido Governatori", "Francesco Olivieri", "Monica Palmirani", "Gabriele Buriola"], "categories": ["cs.LO", "cs.AI", "cs.CL"], "comment": null, "summary": "Behind a set of rules in Deontic Defeasible Logic, there is a mapping process\nof normative background fragments. This process goes from text to rules and\nimplicitly encompasses an explanation of the coded fragments.\n  In this paper we deliver a methodology for \\textit{legal coding} that starts\nwith a fragment and goes onto a set of Deontic Defeasible Logic rules,\ninvolving a set of \\textit{scenarios} to test the correctness of the coded\nfragments. The methodology is illustrated by the coding process of an example\ntext. We then show the results of a series of experiments conducted with humans\nencoding a variety of normative backgrounds and corresponding cases in which we\nhave measured the efforts made in the coding process, as related to some\nmeasurable features. To process these examples, a recently developed\ntechnology, Houdini, that allows reasoning in Deontic Defeasible Logic, has\nbeen employed.\n  Finally we provide a technique to forecast time required in coding, that\ndepends on factors such as knowledge of the legal domain, knowledge of the\ncoding processes, length of the text, and a measure of \\textit{depth} that\nrefers to the length of the paths of legal references."}
{"id": "2505.01958", "pdf": "https://arxiv.org/pdf/2505.01958.pdf", "abs": "https://arxiv.org/abs/2505.01958", "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "authors": ["Liqiang Jing", "Guiming Hardy Chen", "Ehsan Aghazadeh", "Xin Eric Wang", "Xinya Du"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks, but visual object hallucination remains a persistent issue.\nIt refers to scenarios where models generate inaccurate visual object-related\ninformation based on the query input, potentially leading to misinformation and\nconcerns about safety and reliability. Previous works focus on the evaluation\nand mitigation of visual hallucinations, but the underlying causes have not\nbeen comprehensively investigated. In this paper, we analyze each component of\nLLaVA-like LVLMs -- the large language model, the vision backbone, and the\nprojector -- to identify potential sources of error and their impact. Based on\nour observations, we propose methods to mitigate hallucination for each\nproblematic component. Additionally, we developed two hallucination benchmarks:\nQA-VisualGenome, which emphasizes attribute and relation hallucinations, and\nQA-FB15k, which focuses on cognition-based hallucinations."}
{"id": "2505.02130", "pdf": "https://arxiv.org/pdf/2505.02130.pdf", "abs": "https://arxiv.org/abs/2505.02130", "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data", "authors": ["Zhong Guan", "Likang Wu", "Hongke Zhao", "Ming He", "Jianpin Fan"], "categories": ["cs.AI", "cs.CL"], "comment": "ICML2025 Accept", "summary": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}"}
{"id": "2505.02199", "pdf": "https://arxiv.org/pdf/2505.02199.pdf", "abs": "https://arxiv.org/abs/2505.02199", "title": "Exploring new Approaches for Information Retrieval through Natural Language Processing", "authors": ["Manak Raj", "Nidhi Mishra"], "categories": ["cs.IR", "cs.CL", "68T50", "H.3.3; I.2.7"], "comment": "12 pages, 4 figures, comprehensive literature review covering six key\n  IR-NLP papers, plus keywords and full reference list", "summary": "This review paper explores recent advancements and emerging approaches in\nInformation Retrieval (IR) applied to Natural Language Processing (NLP). We\nexamine traditional IR models such as Boolean, vector space, probabilistic, and\ninference network models, and highlight modern techniques including deep\nlearning, reinforcement learning, and pretrained transformer models like BERT.\nWe discuss key tools and libraries - Lucene, Anserini, and Pyserini - for\nefficient text indexing and search. A comparative analysis of sparse, dense,\nand hybrid retrieval methods is presented, along with applications in web\nsearch engines, cross-language IR, argument mining, private information\nretrieval, and hate speech detection. Finally, we identify open challenges and\nfuture research directions to enhance retrieval accuracy, scalability, and\nethical considerations."}
{"id": "2505.02206", "pdf": "https://arxiv.org/pdf/2505.02206.pdf", "abs": "https://arxiv.org/abs/2505.02206", "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units", "authors": ["Lei Mao", "Yuanhe Tian", "Yan Song"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 3 figures", "summary": "Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks."}
{"id": "2505.02215", "pdf": "https://arxiv.org/pdf/2505.02215.pdf", "abs": "https://arxiv.org/abs/2505.02215", "title": "Interpretable Emergent Language Using Inter-Agent Transformers", "authors": ["Mannan Bhardwaj"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments."}
{"id": "2505.02309", "pdf": "https://arxiv.org/pdf/2505.02309.pdf", "abs": "https://arxiv.org/abs/2505.02309", "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques", "authors": ["Sanjay Surendranath Girija", "Shashank Kapoor", "Lakshit Arora", "Dipen Pradhan", "Aman Raj", "Ankit Shetgaonkar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE COMPSAC 2025", "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."}
{"id": "2505.02391", "pdf": "https://arxiv.org/pdf/2505.02391.pdf", "abs": "https://arxiv.org/abs/2505.02391", "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "authors": ["Jiarui Yao", "Yifan Hao", "Hanning Zhang", "Hanze Dong", "Wei Xiong", "Nan Jiang", "Tong Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM."}
{"id": "2505.02462", "pdf": "https://arxiv.org/pdf/2505.02462.pdf", "abs": "https://arxiv.org/abs/2505.02462", "title": "Incentivizing Inclusive Contributions in Model Sharing Markets", "authors": ["Enpei Zhang", "Jingyi Chai", "Rui Ye", "Yanfeng Wang", "Siheng Chen"], "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "While data plays a crucial role in training contemporary AI models, it is\nacknowledged that valuable public data will be exhausted in a few years,\ndirecting the world's attention towards the massive decentralized private data.\nHowever, the privacy-sensitive nature of raw data and lack of incentive\nmechanism prevent these valuable data from being fully exploited. Addressing\nthese challenges, this paper proposes inclusive and incentivized personalized\nfederated learning (iPFL), which incentivizes data holders with diverse\npurposes to collaboratively train personalized models without revealing raw\ndata. iPFL constructs a model-sharing market by solving a graph-based training\noptimization and incorporates an incentive mechanism based on game theory\nprinciples. Theoretical analysis shows that iPFL adheres to two key incentive\nproperties: individual rationality and truthfulness. Empirical studies on\neleven AI tasks (e.g., large language models' instruction-following tasks)\ndemonstrate that iPFL consistently achieves the highest economic utility, and\nbetter or comparable model performance compared to baseline methods. We\nanticipate that our iPFL can serve as a valuable technique for boosting future\nAI models on decentralized private data while making everyone satisfied."}
{"id": "2505.02550", "pdf": "https://arxiv.org/pdf/2505.02550.pdf", "abs": "https://arxiv.org/abs/2505.02550", "title": "Bielik v3 Small: Technical Report", "authors": ["Krzysztof Ociepa", "Łukasz Flis", "Remigiusz Kinas", "Krzysztof Wróbel", "Adrian Gwoździej"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications."}
{"id": "2505.02639", "pdf": "https://arxiv.org/pdf/2505.02639.pdf", "abs": "https://arxiv.org/abs/2505.02639", "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning", "authors": ["Xuan Lin", "Qingrui Liu", "Hongxin Xiang", "Daojian Zeng", "Xiangxiang Zeng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design."}
{"id": "2505.02693", "pdf": "https://arxiv.org/pdf/2505.02693.pdf", "abs": "https://arxiv.org/abs/2505.02693", "title": "Predicting Movie Hits Before They Happen with LLMs", "authors": ["Shaghayegh Agah", "Yejin Kim", "Neeraj Sharma", "Mayur Nankani", "Kevin Foley", "H. Howie Huang", "Sardar Hamidian"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted at ACM UMAP 2025 Industry Track", "summary": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped."}
{"id": "2505.02707", "pdf": "https://arxiv.org/pdf/2505.02707.pdf", "abs": "https://arxiv.org/abs/2505.02707", "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play", "authors": ["Yemin Shi", "Yu Shu", "Siwei Dong", "Guangyi Liu", "Jaward Sesay", "Jingwen Li", "Zhiting Hu"], "categories": ["cs.AI", "cs.CL", "cs.SD"], "comment": "18 pages, 7 figures, Website: https://voila.maitrix.org", "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions."}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746.pdf", "abs": "https://arxiv.org/abs/2505.02746", "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "authors": ["Simon Ging", "Sebastian Walter", "Jelena Bratulić", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time."}
{"id": "2505.02811", "pdf": "https://arxiv.org/pdf/2505.02811.pdf", "abs": "https://arxiv.org/abs/2505.02811", "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", "authors": ["Diji Yang", "Linda Zeng", "Jinmeng Rao", "Yi Zhang"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR 2025", "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."}
{"id": "2505.02820", "pdf": "https://arxiv.org/pdf/2505.02820.pdf", "abs": "https://arxiv.org/abs/2505.02820", "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback", "authors": ["Hao Zhu", "Phil Cuvin", "Xinkai Yu", "Charlotte Ka Yee Yan", "Jason Zhang", "Diyi Yang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "https://opensocial.world/", "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830.pdf", "abs": "https://arxiv.org/abs/2505.02830", "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks."}
{"id": "2505.02835", "pdf": "https://arxiv.org/pdf/2505.02835.pdf", "abs": "https://arxiv.org/abs/2505.02835", "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Xiao Hu", "Chaoyou Fu", "Bin Wen", "Tianke Zhang", "Changyi Liu", "Kaiyu Jiang", "Kaibing Chen", "Kaiyu Tang", "Haojie Ding", "Jiankang Chen", "Fan Yang", "Zhang Zhang", "Tingting Gao", "Liang Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "Home page: https://github.com/yfzhang114/r1_reward", "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs."}
{"id": "2302.09327", "pdf": "https://arxiv.org/pdf/2302.09327.pdf", "abs": "https://arxiv.org/abs/2302.09327", "title": "Transformadores: Fundamentos teoricos y Aplicaciones", "authors": ["Jordi de la Torre"], "categories": ["cs.CL", "cs.AI", "68T01", "I.2"], "comment": "48 pages, in Spanish language, 24 figures, review", "summary": "Transformers are a neural network architecture originally developed for\nnatural language processing, which have since become a foundational tool for\nsolving a wide range of problems, including text, audio, image processing,\nreinforcement learning, and other tasks involving heterogeneous input data.\nTheir hallmark is the self-attention mechanism, which allows the model to weigh\ndifferent parts of the input sequence dynamically, and is an evolution of\nearlier attention-based approaches. This article provides readers with the\nnecessary background to understand recent research on transformer models, and\npresents the mathematical and algorithmic foundations of their core components.\nIt also explores the architecture's various elements, potential modifications,\nand some of the most relevant applications. The article is written in Spanish\nto help make this scientific knowledge more accessible to the Spanish-speaking\ncommunity."}
{"id": "2402.01685", "pdf": "https://arxiv.org/pdf/2402.01685.pdf", "abs": "https://arxiv.org/abs/2402.01685", "title": "SMUTF: Schema Matching Using Generative Tags and Hybrid Features", "authors": ["Yu Zhang", "Mei Di", "Haozheng Luo", "Chenwei Xu", "Richard Tzong-Han Tsai"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Information Systems", "summary": "We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid\nFeatures), a unique approach for large-scale tabular data schema matching (SM),\nwhich assumes that supervised learning does not affect performance in\nopen-domain tasks, thereby enabling effective cross-domain matching. This\nsystem uniquely combines rule-based feature engineering, pre-trained language\nmodels, and generative large language models. In an innovative adaptation\ninspired by the Humanitarian Exchange Language, we deploy \"generative tags\" for\neach data column, enhancing the effectiveness of SM. SMUTF exhibits extensive\nversatility, working seamlessly with any pre-existing pre-trained embeddings,\nclassification methods, and generative models.\n  Recognizing the lack of extensive, publicly available datasets for SM, we\nhave created and open-sourced the HDXSM dataset from the public humanitarian\ndata. We believe this to be the most exhaustive SM dataset currently available.\nIn evaluations across various public datasets and the novel HDXSM dataset,\nSMUTF demonstrated exceptional performance, surpassing existing\nstate-of-the-art models in terms of accuracy and efficiency, and improving the\nF1 score by 11.84% and the AUC of ROC by 5.08%. Code is available at\nhttps://github.com/fireindark707/Python-Schema-Matching."}
{"id": "2403.01954", "pdf": "https://arxiv.org/pdf/2403.01954.pdf", "abs": "https://arxiv.org/abs/2403.01954", "title": "DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation", "authors": ["Chen Xu", "Tian Lan", "Yu Ji", "Changlong Yu", "Wei Wang", "Jun Gao", "Qunxi Dong", "Kun Qian", "Piji Li", "Wei Bi", "Bin Hu"], "categories": ["cs.CL", "cs.AI", "cs.LO"], "comment": "Accepted by IEEE TKDE 2025, 14 pages, 6 figures", "summary": "Constrained decoding approaches aim to control the meaning or style of text\ngenerated by the pre-trained large language models (LLMs or also PLMs) for\nvarious tasks at inference time. However, these methods often guide plausible\ncontinuations by greedily and explicitly selecting targets. Though fulfilling\nthe task requirements, these methods may overlook certain general and natural\nlogics that humans would implicitly follow towards such targets. Inspired by\ncognitive dual-process theory, in this work, we propose a novel decoding\nframework DECIDER where the base LLMs are equipped with a First-Order Logic\n(FOL) reasoner to express and evaluate the rules, along with a decision\nfunction that merges the outputs of both systems to guide the generation.\nUnlike previous constrained decodings, DECIDER transforms the encouragement of\ntarget-specific words into all words that satisfy several high-level rules,\nenabling us to programmatically integrate our logic into LLMs. Experiments on\nCommonGen and PersonaChat demonstrate that DECIDER effectively follows given\nFOL rules to guide LLMs in a more human-like and logic-controlled manner."}
{"id": "2404.00570", "pdf": "https://arxiv.org/pdf/2404.00570.pdf", "abs": "https://arxiv.org/abs/2404.00570", "title": "ParaICL: Towards Parallel In-Context Learning", "authors": ["Xingxuan Li", "Xuan-Phi Nguyen", "Shafiq Joty", "Lidong Bing"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025", "summary": "Large language models (LLMs) have become the norm in natural language\nprocessing (NLP), excelling in few-shot in-context learning (ICL) with their\nremarkable abilities. Nonetheless, the success of ICL largely hinges on the\nchoice of few-shot demonstration examples, making the selection process\nincreasingly crucial. Existing methods have delved into optimizing the quantity\nand semantic similarity of these examples to improve ICL performances. However,\nour preliminary experiments indicate that the effectiveness of ICL is limited\nby the length of the input context. Moreover, varying combinations of few-shot\ndemonstration examples can significantly boost accuracy across different test\nsamples. To address this, we propose a novel method named parallel in-context\nlearning (ParaICL) that effectively utilizes all demonstration examples without\nexceeding the manageable input context length. ParaICL employs parallel\nbatching to distribute demonstration examples into different batches according\nto the semantic similarities of the questions in the demonstrations to the test\nquestion. It then computes normalized batch semantic scores for each batch. A\nweighted average semantic objective, constrained by adaptive plausibility, is\napplied to select the most appropriate tokens. Through extensive experiments,\nwe validate the effectiveness of ParaICL and conduct ablation studies to\nunderscore its design rationale. We further demonstrate that ParaICL can\nseamlessly integrate with existing methods."}
{"id": "2404.04748", "pdf": "https://arxiv.org/pdf/2404.04748.pdf", "abs": "https://arxiv.org/abs/2404.04748", "title": "Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind", "authors": ["Hongchuan Zeng", "Hongshen Xu", "Lu Chen", "Kai Yu"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 13 tables. Accepted by LREC-COLING 2024", "summary": "Large Language Models (LLMs) have ushered in a new era in Natural Language\nProcessing, but their massive size demands effective compression techniques for\npracticality. Although numerous model compression techniques have been\ninvestigated, they typically rely on a calibration set that overlooks the\nmultilingual context and results in significant accuracy degradation for\nlow-resource languages. This paper introduces Multilingual Brain Surgeon (MBS),\na novel calibration data sampling method for multilingual LLMs compression. MBS\novercomes the English-centric limitations of existing methods by sampling\ncalibration data from various languages proportionally to the language\ndistribution of the model training datasets. Our experiments, conducted on the\nBLOOM multilingual LLM, demonstrate that MBS improves the performance of\nexisting English-centric compression methods, especially for low-resource\nlanguages. We also uncover the dynamics of language interaction during\ncompression, revealing that the larger the proportion of a language in the\ntraining set and the more similar the language is to the calibration language,\nthe better performance the language retains after compression. In conclusion,\nMBS presents an innovative approach to compressing multilingual LLMs,\naddressing the performance disparities and improving the language inclusivity\nof existing compression techniques."}
{"id": "2405.05572", "pdf": "https://arxiv.org/pdf/2405.05572.pdf", "abs": "https://arxiv.org/abs/2405.05572", "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences", "authors": ["Prashant Kodali", "Anmol Goel", "Likhith Asapu", "Vamshi Krishna Bonagiri", "Anirudh Govil", "Monojit Choudhury", "Ponnurangam Kumaraguru", "Manish Shrivastava"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model ``naturalness'' or ``acceptability'' of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models when trained solely using code-mixing\nmetrics as features are outperformed by fine-tuned pre-trained Multilingual\nLarge Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta\nand Bernice outperform IndicBERT across different configurations. Among\nEncoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder\nmodels are not able to outperform Encoder-only models. Decoder-only models\nperform the best when compared to all other MLLMS, with Llama 3.2 - 3B models\noutperforming similarly sized Qwen, Phi models. Comparison with zero and\nfewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data\noutperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from En-Hi to En-Te acceptability judgments are better than\nrandom baselines."}
{"id": "2406.02481", "pdf": "https://arxiv.org/pdf/2406.02481.pdf", "abs": "https://arxiv.org/abs/2406.02481", "title": "Large Language Models as Carriers of Hidden Messages", "authors": ["Jakub Hoscilowicz", "Pawel Popiolek", "Jan Rudkowski", "Jedrzej Bieniasz", "Artur Janicki"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted on SECRYPT 2025 Conference. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano", "summary": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels)."}
{"id": "2407.12772", "pdf": "https://arxiv.org/pdf/2407.12772.pdf", "abs": "https://arxiv.org/abs/2407.12772", "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models", "authors": ["Kaichen Zhang", "Bo Li", "Peiyuan Zhang", "Fanyi Pu", "Joshua Adrian Cahyono", "Kairui Hu", "Shuai Liu", "Yuanhan Zhang", "Jingkang Yang", "Chunyuan Li", "Ziwei Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "Code ad leaderboard are available at\n  https://github.com/EvolvingLMMs-Lab/lmms-eval and\n  https://huggingface.co/spaces/lmms-lab/LiveBench", "summary": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench."}
{"id": "2408.03618", "pdf": "https://arxiv.org/pdf/2408.03618.pdf", "abs": "https://arxiv.org/abs/2408.03618", "title": "A Logical Fallacy-Informed Framework for Argument Generation", "authors": ["Luca Mouchel", "Debjit Paul", "Shaobo Cui", "Robert West", "Antoine Bosselut", "Boi Faltings"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs) in natural\nlanguage processing tasks, they still struggle with generating logically sound\narguments, resulting in potential risks such as spreading misinformation. To\naddress this issue, we introduce FIPO, a fallacy-informed framework that\nleverages preference optimization methods to steer LLMs toward logically sound\narguments. FIPO includes a classification loss, to capture the fine-grained\ninformation on fallacy types. Our results on argumentation datasets show that\nour method reduces the fallacy errors by up to 17.5%. Furthermore, our human\nevaluation results indicate that the quality of the generated arguments by our\nmethod significantly outperforms the fine-tuned baselines, as well as other\npreference optimization methods, such as DPO. These findings highlight the\nimportance of ensuring models are aware of logical fallacies for effective\nargument generation. Our code is available at\ngithub.com/lucamouchel/Logical-Fallacies."}
{"id": "2408.06150", "pdf": "https://arxiv.org/pdf/2408.06150.pdf", "abs": "https://arxiv.org/abs/2408.06150", "title": "LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library", "authors": ["Tianhao Yu", "Cai Yao", "Zhuorui Sun", "Feng Shi", "Lin Zhang", "Kangjie Lyu", "Xuan Bai", "Andong Liu", "Xicheng Zhang", "Jiali Zou", "Wenshou Wang", "Chris Lai", "Kai Wang"], "categories": ["cs.CL", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "In this study, we generate and maintain a database of 10 million virtual\nlipids through METiS's in-house de novo lipid generation algorithms and lipid\nvirtual screening techniques. These virtual lipids serve as a corpus for\npre-training, lipid representation learning, and downstream task knowledge\ntransfer, culminating in state-of-the-art LNP property prediction performance.\nWe propose LipidBERT, a BERT-like model pre-trained with the Masked Language\nModel (MLM) and various secondary tasks. Additionally, we compare the\nperformance of embeddings generated by LipidBERT and PhatGPT, our GPT-like\nlipid generation model, on downstream tasks. The proposed bilingual LipidBERT\nmodel operates in two languages: the language of ionizable lipid pre-training,\nusing in-house dry-lab lipid structures, and the language of LNP fine-tuning,\nutilizing in-house LNP wet-lab data. This dual capability positions LipidBERT\nas a key AI-based filter for future screening tasks, including new versions of\nMETiS de novo lipid libraries and, more importantly, candidates for in vivo\ntesting for orgran-targeting LNPs. To the best of our knowledge, this is the\nfirst successful demonstration of the capability of a pre-trained language\nmodel on virtual lipids and its effectiveness in downstream tasks using web-lab\ndata. This work showcases the clever utilization of METiS's in-house de novo\nlipid library as well as the power of dry-wet lab integration."}
{"id": "2409.09413", "pdf": "https://arxiv.org/pdf/2409.09413.pdf", "abs": "https://arxiv.org/abs/2409.09413", "title": "Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence", "authors": ["Tadahiro Taniguchi", "Masafumi Oizumi", "Noburo Saji", "Takato Horii", "Naotsugu Tsuchiya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This perspective paper explores the bidirectional influence between language\nemergence and the relational structure of subjective experiences, termed qualia\nstructure, and lays out a constructive approach to the intricate dependency\nbetween the two. We hypothesize that the emergence of languages with\ndistributional semantics (e.g., syntactic-semantic structures) is linked to the\ncoordination of internal representations shaped by experience, potentially\nfacilitating more structured language through reciprocal influence. This\nhypothesized mutual dependency connects to recent advancements in AI and symbol\nemergence robotics, and is explored within this paper through theoretical\nframeworks such as the collective predictive coding. Computational studies show\nthat neural network-based language models form systematically structured\ninternal representations, and multimodal language models can share\nrepresentations between language and perceptual information. This perspective\nsuggests that language emergence serves not only as a mechanism creating a\ncommunication tool but also as a mechanism for allowing people to realize\nshared understanding of qualitative experiences. The paper discusses the\nimplications of this bidirectional influence in the context of consciousness\nstudies, linguistics, and cognitive science, and outlines future constructive\nresearch directions to further explore this dynamic relationship between\nlanguage emergence and qualia structure."}
{"id": "2410.14567", "pdf": "https://arxiv.org/pdf/2410.14567.pdf", "abs": "https://arxiv.org/abs/2410.14567", "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions", "authors": ["Zhiyuan Peng", "Jinming Nian", "Alexandre Evfimievski", "Yi Fang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR'25", "summary": "Retrieval-augmented generation (RAG) has become integral to large language\nmodels (LLMs), particularly for conversational AI systems where user questions\nmay reference knowledge beyond the LLMs' training cutoff. However, many natural\nuser questions lack well-defined answers, either due to limited domain\nknowledge or because the retrieval system returns documents that are relevant\nin appearance but uninformative in content. In such cases, LLMs often produce\nhallucinated answers without flagging them. While recent work has largely\nfocused on questions with false premises, we study out-of-scope questions,\nwhere the retrieved document appears semantically similar to the question but\nlacks the necessary information to answer it. In this paper, we propose a\nguided hallucination-based approach ELOQ to automatically generate a diverse\nset of out-of-scope questions from post-cutoff documents, followed by human\nverification to ensure quality. We use this dataset to evaluate several LLMs on\ntheir ability to detect out-of-scope questions and generate appropriate\nresponses. Finally, we introduce an improved detection method that enhances the\nreliability of LLM-based question-answering systems in handling out-of-scope\nquestions."}
{"id": "2410.18902", "pdf": "https://arxiv.org/pdf/2410.18902.pdf", "abs": "https://arxiv.org/abs/2410.18902", "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages", "authors": ["Taido Purason", "Hele-Andra Kuulmets", "Mark Fishel"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP."}
{"id": "2412.04454", "pdf": "https://arxiv.org/pdf/2412.04454.pdf", "abs": "https://arxiv.org/abs/2412.04454", "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction", "authors": ["Yiheng Xu", "Zekun Wang", "Junli Wang", "Dunjie Lu", "Tianbao Xie", "Amrita Saha", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Automating GUI tasks remains challenging due to reliance on textual\nrepresentations, platform-specific action spaces, and limited reasoning\ncapabilities. We introduce Aguvis, a unified vision-based framework for\nautonomous GUI agents that directly operates on screen images, standardizes\ncross-platform interactions and incorporates structured reasoning via inner\nmonologue. To enable this, we construct Aguvis Data Collection, a large-scale\ndataset with multimodal grounding and reasoning annotations, and develop a\ntwo-stage training pipeline that separates GUI grounding from planning and\nreasoning. Experiments show that Aguvis achieves state-of-the-art performance\nacross offline and real-world online benchmarks, marking the first fully\nautonomous vision-based GUI agent that operates without closed-source models.\nWe open-source all datasets, models, and training recipes at\nhttps://aguvis-project.github.io to advance future research."}
{"id": "2412.11142", "pdf": "https://arxiv.org/pdf/2412.11142.pdf", "abs": "https://arxiv.org/abs/2412.11142", "title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection", "authors": ["Tiankai Yang", "Yi Nian", "Shawn Li", "Ruiyao Xu", "Yuangang Li", "Jiaqi Li", "Zhuo Xiao", "Xiyang Hu", "Ryan Rossi", "Kaize Ding", "Xia Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is an important machine learning task with many\nreal-world uses, including fraud detection, medical diagnosis, and industrial\nmonitoring. Within natural language processing (NLP), AD helps detect issues\nlike spam, misinformation, and unusual user activity. Although large language\nmodels (LLMs) have had a strong impact on tasks such as text generation and\nsummarization, their potential in AD has not been studied enough. This paper\nintroduces AD-LLM, the first benchmark that evaluates how LLMs can help with\nNLP anomaly detection. We examine three key tasks: (i) zero-shot detection,\nusing LLMs' pre-trained knowledge to perform AD without tasks-specific\ntraining; (ii) data augmentation, generating synthetic data and category\ndescriptions to improve AD models; and (iii) model selection, using LLMs to\nsuggest unsupervised AD models. Through experiments with different datasets, we\nfind that LLMs can work well in zero-shot AD, that carefully designed\naugmentation methods are useful, and that explaining model selection for\nspecific datasets remains challenging. Based on these results, we outline six\nfuture research directions on LLMs for AD."}
{"id": "2501.00062", "pdf": "https://arxiv.org/pdf/2501.00062.pdf", "abs": "https://arxiv.org/abs/2501.00062", "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis", "authors": ["James P. Beno"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "19 pages, 4 figures. Source code and data available at\n  https://github.com/jbeno/sentiment", "summary": "Bidirectional transformers excel at sentiment analysis, and Large Language\nModels (LLM) are effective zero-shot learners. Might they perform better as a\nteam? This paper explores collaborative approaches between ELECTRA and GPT-4o\nfor three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA\nBase/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment\nTreebank (SST) and DynaSent. We provided input from ELECTRA to GPT as:\npredicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT\npredictions with GPT-4o-mini significantly improved performance over either\nmodel alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and\nyielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT\nmodels were fine-tuned, including predictions decreased performance. GPT-4o\nFT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at\nmuch less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting\nprompts with predictions from fine-tuned encoders is an efficient way to boost\nperformance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76%\nless cost. Both are affordable options for projects with limited resources."}
{"id": "2501.00874", "pdf": "https://arxiv.org/pdf/2501.00874.pdf", "abs": "https://arxiv.org/abs/2501.00874", "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models", "authors": ["Hieu Man", "Nghia Trung Ngo", "Viet Dac Lai", "Ryan A. Rossi", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."}
{"id": "2501.02407", "pdf": "https://arxiv.org/pdf/2501.02407.pdf", "abs": "https://arxiv.org/abs/2501.02407", "title": "Towards the Anonymization of the Language Modeling", "authors": ["Antoine Boutet", "Lucas Magnana", "Juliette Sénéchal", "Helain Zimmermann"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Rapid advances in Natural Language Processing (NLP) have revolutionized many\nfields, including healthcare. However, these advances raise significant privacy\nconcerns, especially when pre-trained models fine-tuned and specialized on\nsensitive data can memorize and then expose and regurgitate personal\ninformation. This paper presents a privacy-preserving language modeling\napproach to address the problem of language models anonymization, and thus\npromote their sharing. Specifically, we propose both a Masking Language\nModeling (MLM) methodology to specialize a BERT-like language model, and a\nCausal Language Modeling (CLM) methodology to specialize a GPT-like model that\navoids the model from memorizing direct and indirect identifying information\npresent in the training data. We have comprehensively evaluated our approaches\nusing a medical dataset and compared them against different baselines. Our\nresults indicate that by avoiding memorizing both direct and indirect\nidentifiers during model specialization, our masking and causal language\nmodeling schemes offer a good tradeoff for maintaining high privacy while\nretaining high utility."}
{"id": "2502.01563", "pdf": "https://arxiv.org/pdf/2502.01563.pdf", "abs": "https://arxiv.org/abs/2502.01563", "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding", "authors": ["Mingyu Jin", "Kai Mei", "Wujiang Xu", "Mingjie Sun", "Ruixiang Tang", "Mengnan Du", "Zirui Liu", "Yongfeng Zhang"], "categories": ["cs.CL"], "comment": "International Conference on Machine Learning (ICML 2025)", "summary": "Large language models (LLMs) have achieved remarkable success in contextual\nknowledge understanding. In this paper, we show that these concentrated massive\nvalues consistently emerge in specific regions of attention queries (Q) and\nkeys (K) while not having such patterns in values (V) in various modern\ntransformer-based LLMs (Q, K, and V mean the representations output by the\nquery, key, and value layers respectively). Through extensive experiments, we\nfurther demonstrate that these massive values play a critical role in\ninterpreting contextual knowledge (knowledge obtained from the current context\nwindow) rather than in retrieving parametric knowledge stored within the\nmodel's parameters. Our further investigation of quantization strategies\nreveals that ignoring these massive values leads to a pronounced drop in\nperformance on tasks requiring rich contextual understanding, aligning with our\nanalysis. Finally, we trace the emergence of concentrated massive values and\nfind that such concentration is caused by Rotary Positional Encoding (RoPE),\nwhich has appeared since the first layers. These findings shed new light on how\nQ and K operate in LLMs and offer practical insights for model design and\noptimization. The Code is Available at\nhttps://github.com/MingyuJ666/Rope_with_LLM."}
{"id": "2502.03253", "pdf": "https://arxiv.org/pdf/2502.03253.pdf", "abs": "https://arxiv.org/abs/2502.03253", "title": "How do Humans and Language Models Reason About Creativity? A Comparative Analysis", "authors": ["Antonio Laverghetta Jr.", "Tuhin Chakrabarty", "Tom Hope", "Jimmy Pronchick", "Krupa Bhawsar", "Roger E. Beaty"], "categories": ["cs.CL"], "comment": "CogSci 2025", "summary": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially -- to upwards of $0.99$ -- suggesting a homogenization\nin the LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating."}
{"id": "2502.12050", "pdf": "https://arxiv.org/pdf/2502.12050.pdf", "abs": "https://arxiv.org/abs/2502.12050", "title": "SpeechT: Findings of the First Mentorship in Speech Translation", "authors": ["Yasmin Moslem", "Juan Julián Cea Morán", "Mariano Gonzalez-Gomez", "Muhammad Hazim Al Farouq", "Farah Abdou", "Satarupa Deb"], "categories": ["cs.CL", "cs.SD"], "comment": "MT Summit 2025", "summary": "This work presents the details and findings of the first mentorship in speech\ntranslation (SpeechT), which took place in December 2024 and January 2025. To\nfulfil the mentorship requirements, the participants engaged in key activities,\nincluding data preparation, modelling, and advanced research. The participants\nexplored data augmentation techniques and compared end-to-end and cascaded\nspeech translation systems. The projects covered various languages other than\nEnglish, including Arabic, Bengali, Galician, Indonesian, Japanese, and\nSpanish."}
{"id": "2502.15666", "pdf": "https://arxiv.org/pdf/2502.15666.pdf", "abs": "https://arxiv.org/abs/2502.15666", "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing", "authors": ["Shoumik Saha", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "18 pages, 18 figures, 6 tables", "summary": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies."}
{"id": "2502.16892", "pdf": "https://arxiv.org/pdf/2502.16892.pdf", "abs": "https://arxiv.org/abs/2502.16892", "title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data", "authors": ["Yejian Zhang", "Shingo Takada"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework,\nachieving high cross-task text classification performance without the need for\nany manually labeled data. Furthermore, compared to directly applying GPT for\nclassification tasks, our approach retains over 93% of its classification\nperformance while requiring only approximately 6% of the computational time and\nmonetary cost, effectively balancing performance and resource efficiency. These\nfindings provide new insights into the efficient utilization of LLMs and active\nlearning algorithms in text classification tasks, paving the way for their\nbroader application."}
{"id": "2502.17424", "pdf": "https://arxiv.org/pdf/2502.17424.pdf", "abs": "https://arxiv.org/abs/2502.17424", "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs", "authors": ["Jan Betley", "Daniel Tan", "Niels Warncke", "Anna Sztyber-Betley", "Xuchan Bao", "Martín Soto", "Nathan Labenz", "Owain Evans"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "40 pages, 38 figures An earlier revision of this paper was submitted\n  to ICML. Since then, it has been updated to include new results on training\n  dynamics (4.7) and base models (4.8)", "summary": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding. It asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned. Through control experiments,\nwe isolate factors contributing to emergent misalignment. Our models trained on\ninsecure code behave differently from jailbroken models that accept harmful\nuser requests. Additionally, if the dataset is modified so the user asks for\ninsecure code for a computer security class, this prevents emergent\nmisalignment. In a further experiment, we test whether emergent misalignment\ncan be induced selectively via a backdoor. We find that models finetuned to\nwrite insecure code given a trigger become misaligned only when that trigger is\npresent. So the misalignment is hidden without knowledge of the trigger. It's\nimportant to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork."}
{"id": "2502.21239", "pdf": "https://arxiv.org/pdf/2502.21239.pdf", "abs": "https://arxiv.org/abs/2502.21239", "title": "Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs", "authors": ["Xiaomin Li", "Zhou Yu", "Ziji Zhang", "Yingying Zhuang", "Swair Shah", "Narayanan Sadagopan", "Anurag Beniwal"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."}
{"id": "2503.04785", "pdf": "https://arxiv.org/pdf/2503.04785.pdf", "abs": "https://arxiv.org/abs/2503.04785", "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice", "authors": ["José Siqueira de Cerqueira", "Kai-Kristian Kemell", "Rebekah Rousi", "Nannan Xi", "Juho Hamari", "Pekka Abrahamsson"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has raised\nsignificant trustworthiness and ethical concerns. Despite the widespread\nadoption of LLMs across domains, there is still no clear consensus on how to\ndefine and operationalise trustworthiness. This study aims to bridge the gap\nbetween theoretical discussion and practical implementation by analysing\nresearch trends, definitions of trustworthiness, and practical techniques. We\nconducted a bibliometric mapping analysis of 2,006 publications from Web of\nScience (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We\nfound a shift from traditional AI ethics discussion to LLM trustworthiness\nframeworks. We identified 18 different definitions of trust/trustworthiness,\nwith transparency, explainability and reliability emerging as the most common\ndimensions. We identified 20 strategies to enhance LLM trustworthiness, with\nfine-tuning and retrieval-augmented generation (RAG) being the most prominent.\nMost of the strategies are developer-driven and applied during the\npost-training phase. Several authors propose fragmented terminologies rather\nthan unified frameworks, leading to the risks of \"ethics washing,\" where\nethical discourse is adopted without a genuine regulatory commitment. Our\nfindings highlight: persistent gaps between theoretical taxonomies and\npractical implementation, the crucial role of the developer in operationalising\ntrust, and call for standardised frameworks and stronger regulatory measures to\nenable trustworthy and ethical deployment of LLMs."}
{"id": "2503.17003", "pdf": "https://arxiv.org/pdf/2503.17003.pdf", "abs": "https://arxiv.org/abs/2503.17003", "title": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "authors": ["Jian Guan", "Junfei Wu", "Jia-Nan Li", "Chuanqi Cheng", "Wei Wu"], "categories": ["cs.CL"], "comment": "Survey paper; 11 pages; Literature reviewed up to ICLR 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895.pdf", "abs": "https://arxiv.org/abs/2503.23895", "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint. Code is available at https://github.com/Trae1ounG/DyPRAG", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."}
{"id": "2503.24235", "pdf": "https://arxiv.org/pdf/2503.24235.pdf", "abs": "https://arxiv.org/abs/2503.24235", "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?", "authors": ["Qiyuan Zhang", "Fuyuan Lyu", "Zexu Sun", "Lei Wang", "Weixu Zhang", "Wenyue Hua", "Haolun Wu", "Zhihan Guo", "Yufei Wang", "Niklas Muennighoff", "Irwin King", "Xue Liu", "Chen Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "v3: Expand Agentic and SFT Chapters. Build Website for better\n  visualization", "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions. Our repository is available on\nhttps://github.com/testtimescaling/testtimescaling.github.io/"}
{"id": "2504.03302", "pdf": "https://arxiv.org/pdf/2504.03302.pdf", "abs": "https://arxiv.org/abs/2504.03302", "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models", "authors": ["Afshin Khadangi", "Amir Sartipi", "Igor Tchappi", "Ramin Bahmani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility."}
{"id": "2504.03601", "pdf": "https://arxiv.org/pdf/2504.03601.pdf", "abs": "https://arxiv.org/abs/2504.03601", "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "authors": ["Akshara Prabhakar", "Zuxin Liu", "Ming Zhu", "Jianguo Zhang", "Tulika Awalgaonkar", "Shiyu Wang", "Zhiwei Liu", "Haolin Chen", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Weiran Yao", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages plus references and appendices", "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io"}
{"id": "2504.10637", "pdf": "https://arxiv.org/pdf/2504.10637.pdf", "abs": "https://arxiv.org/abs/2504.10637", "title": "Better Estimation of the KL Divergence Between Language Models", "authors": ["Afra Amini", "Tim Vieira", "Ryan Cotterell"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Estimating the Kullback--Leibler (KL) divergence between language models has\nmany applications, e.g., reinforcement learning from human feedback (RLHF),\ninterpretability, and knowledge distillation. However, computing the exact KL\ndivergence between two arbitrary language models is intractable. Thus,\npractitioners often resort to the use of sampling-based estimators. While it is\neasy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased\nestimate of the KL divergence between language models, this estimator\nnotoriously suffers from high variance, and can even result in a negative\nestimate of the KL divergence, a non-negative quantity. In this paper, we\nintroduce a Rao--Blackwellized estimator that is also unbiased and provably has\nvariance less than or equal to that of the standard Monte Carlo estimator. In\nan empirical study on sentiment-controlled fine-tuning, we show that our\nestimator provides more stable KL estimates and reduces variance substantially\nin practice. Additionally, we derive an analogous Rao--Blackwellized estimator\nof the gradient of the KL divergence, which leads to more stable training and\nproduces models that more frequently appear on the Pareto frontier of reward\nvs. KL compared to the ones trained with the MC estimator of the gradient."}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941.pdf", "abs": "https://arxiv.org/abs/2504.15941", "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "authors": ["Fanny Jourdan", "Yannick Chevalier", "Cécile Favre"], "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267.pdf", "abs": "https://arxiv.org/abs/2504.19267", "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304.pdf", "abs": "https://arxiv.org/abs/2504.20304", "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research."}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214.pdf", "abs": "https://arxiv.org/abs/2504.21214", "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research."}
{"id": "2505.00001", "pdf": "https://arxiv.org/pdf/2505.00001.pdf", "abs": "https://arxiv.org/abs/2505.00001", "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning", "authors": ["Shaun Baek", "Shaun Esua-Mensah", "Cyrus Tsui", "Sejan Vigneswaralingam", "Abdullah Alali", "Michael Lu", "Vasu Sharma", "Sean O'Brien", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are primarily trained on high-resource natural\nlanguages, limiting their effectiveness in low-resource settings and in tasks\nrequiring deep logical reasoning. This research introduces Rosetta-PL, a\nbenchmark designed to evaluate LLMs' logical reasoning and generalization\ncapabilities in a controlled environment. We construct Rosetta-PL by\ntranslating a dataset of logical propositions from Lean into a custom logical\nlanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Our\nexperiments analyze the impact of the size of the dataset and the translation\nmethodology on the performance of the model. Our results indicate that\npreserving logical relationships in the translation process significantly\nboosts precision, with accuracy plateauing beyond roughly 20,000 training\nsamples. These insights provide valuable guidelines for optimizing LLM training\nin formal reasoning tasks and improving performance in various low-resource\nlanguage applications."}
{"id": "2505.00034", "pdf": "https://arxiv.org/pdf/2505.00034.pdf", "abs": "https://arxiv.org/abs/2505.00034", "title": "Improving Phishing Email Detection Performance of Small Large Language Models", "authors": ["Zijie Lin", "Zikang Liu", "Hanbo Fan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models(LLMs) have demonstrated remarkable performance on many\nnatural language processing(NLP) tasks and have been employed in phishing email\ndetection research. However, in current studies, well-performing LLMs typically\ncontain billions or even tens of billions of parameters, requiring enormous\ncomputational resources. To reduce computational costs, we investigated the\neffectiveness of small-parameter LLMs for phishing email detection. These LLMs\nhave around 3 billion parameters and can run on consumer-grade GPUs. However,\nsmall LLMs often perform poorly in phishing email detection task. To address\nthese issues, we designed a set of methods including Prompt Engineering,\nExplanation Augmented Fine-tuning, and Model Ensemble to improve phishing email\ndetection capabilities of small LLMs. We validated the effectiveness of our\napproach through experiments, significantly improving both accuracy and F1\nscore on the SpamAssassin and CEAS\\_08 datasets. Furthermore, the fine-tuned\nmodels demonstrated strong transferability, achieving robust performance across\nmultiple unseen phishing datasets, outperforming traditional baselines and\napproaching standard-sized LLMs."}
{"id": "2505.00626", "pdf": "https://arxiv.org/pdf/2505.00626.pdf", "abs": "https://arxiv.org/abs/2505.00626", "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)", "authors": ["Zihao Wang", "Yibo Jiang", "Jiahao Yu", "Heqing Huang"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "comment": null, "summary": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers."}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654.pdf", "abs": "https://arxiv.org/abs/2505.00654", "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "authors": ["Daniel N. Nissani"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985.pdf", "abs": "https://arxiv.org/abs/2505.00985", "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315.pdf", "abs": "https://arxiv.org/abs/2505.01315", "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."}
{"id": "2309.15670", "pdf": "https://arxiv.org/pdf/2309.15670.pdf", "abs": "https://arxiv.org/abs/2309.15670", "title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection", "authors": ["Sumit Kumar Banshal", "Sajal Das", "Shumaiya Akter Shammi", "Narayan Ranjan Chakraborty", "Aulia Luqman Aziz", "Mohammed Aljuaid", "Fazla Rabby", "Rohit Bansal"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla."}
{"id": "2403.06869", "pdf": "https://arxiv.org/pdf/2403.06869.pdf", "abs": "https://arxiv.org/abs/2403.06869", "title": "Impact of Noisy Supervision in Foundation Model Learning", "authors": ["Hao Chen", "Zihan Wang", "Ran Tao", "Hongxin Wei", "Xing Xie", "Masashi Sugiyama", "Bhiksha Raj", "Jindong Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002", "summary": "Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning."}
{"id": "2407.06606", "pdf": "https://arxiv.org/pdf/2407.06606.pdf", "abs": "https://arxiv.org/abs/2407.06606", "title": "Tailored Design of Audio-Visual Speech Recognition Models using Branchformers", "authors": ["David Gimeno-Gómez", "Carlos-D. Martínez-Hinarejos"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in Computer Speech & Language journal of Elsevier", "summary": "Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr."}
{"id": "2501.17176", "pdf": "https://arxiv.org/pdf/2501.17176.pdf", "abs": "https://arxiv.org/abs/2501.17176", "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant", "authors": ["Marc Ballestero-Ribó", "Daniel Ortiz-Martínez"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective."}
{"id": "2502.04667", "pdf": "https://arxiv.org/pdf/2502.04667.pdf", "abs": "https://arxiv.org/abs/2502.04667", "title": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization", "authors": ["Xinhao Yao", "Ruifeng Ren", "Yun Liao", "Yong Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness."}
{"id": "2502.20490", "pdf": "https://arxiv.org/pdf/2502.20490.pdf", "abs": "https://arxiv.org/abs/2502.20490", "title": "EgoNormia: Benchmarking Physical Social Norm Understanding", "authors": ["MohammadHossein Rezaei", "Yicheng Fu", "Phil Cuvin", "Caleb Ziems", "Yanzhe Zhang", "Hao Zhu", "Diyi Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "V2, with updated VLM stats", "summary": "Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, particularly\nwhen norms are physically- or socially-grounded. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\n\\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ\nquestions based on ego-centric videos of human interactions, evaluating both\nthe prediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human\nbench of 92\\%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation (RAG) method, it is possible to\nuse \\dataset{} to enhance normative reasoning in VLMs."}
{"id": "2503.02650", "pdf": "https://arxiv.org/pdf/2503.02650.pdf", "abs": "https://arxiv.org/abs/2503.02650", "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats", "authors": ["William Brach", "Kristián Košťál", "Michal Ries"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information."}
{"id": "2504.12279", "pdf": "https://arxiv.org/pdf/2504.12279.pdf", "abs": "https://arxiv.org/abs/2504.12279", "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "authors": ["Mikhail Osipov"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Preprint. 15 pages, 6 figures, 6 tables, 11 appendices. Code and data\n  available upon request", "summary": "We present a geometry-driven method for normalizing dysarthric speech by\nmodeling time, frequency, and amplitude distortions as smooth, local Lie group\ntransformations of spectrograms. Scalar fields generate these deformations via\nexponential maps, and a neural network is trained - using only synthetically\nwarped healthy speech - to infer the fields and apply an approximate inverse at\ntest time. We introduce a spontaneous-symmetry-breaking (SSB) potential that\nencourages the model to discover non-trivial field configurations. On real\npathological speech, the system delivers consistent gains: up to 17\npercentage-point WER reduction on challenging TORGO utterances and a 16 percent\ndrop in WER variance, with no degradation on clean CommonVoice data. Character\nand phoneme error rates improve in parallel, confirming linguistic relevance.\nOur results demonstrate that geometrically structured warping provides\nconsistent, zero-shot robustness gains for dysarthric ASR."}
{"id": "2504.20117", "pdf": "https://arxiv.org/pdf/2504.20117.pdf", "abs": "https://arxiv.org/abs/2504.20117", "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies", "authors": ["Shubham Gandhi", "Dhruv Shah", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system\nleveraging large language models (LLMs) agents to automate the codification of\nresearch methodologies described in machine learning literature. The system\nbridges the gap between high-level research concepts and their practical\nimplementation, allowing researchers auto-generating code of existing research\npapers for benchmarking or building on top-of existing methods specified in the\nliterature with availability of partial or complete starter code.\nResearchCodeAgent employs a flexible agent architecture with a comprehensive\naction suite, enabling context-aware interactions with the research\nenvironment. The system incorporates a dynamic planning mechanism, utilizing\nboth short and long-term memory to adapt its approach iteratively. We evaluate\nResearchCodeAgent on three distinct machine learning tasks with distinct task\ncomplexity and representing different parts of the ML pipeline: data\naugmentation, optimization, and data batching. Our results demonstrate the\nsystem's effectiveness and generalizability, with 46.9% of generated code being\nhigh-quality and error-free, and 25% showing performance improvements over\nbaseline implementations. Empirical analysis shows an average reduction of\n57.9% in coding time compared to manual implementation. We observe higher gains\nfor more complex tasks. ResearchCodeAgent represents a significant step towards\nautomating the research implementation process, potentially accelerating the\npace of machine learning research."}
