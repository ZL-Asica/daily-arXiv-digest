{"id": "2509.08915", "pdf": "https://arxiv.org/pdf/2509.08915.pdf", "abs": "https://arxiv.org/abs/2509.08915", "title": "A Contextual Bandits Approach for Personalization of Hand Gesture Recognition", "authors": ["Duke Lin", "Michael Paskett", "Ying Yang"], "categories": ["cs.HC"], "comment": null, "summary": "In human-computer interaction applications like hand gesture recognition,\nsupervised learning models are often trained on a large population of users to\nachieve high task accuracy. However, due to individual variability in sensor\nsignals and user behavior, static models may not provide optimal performance\nfor all users. Personalizing pretrained models via calibration--collecting\nlabeled data from each user--can improve performance but introduces user\nfriction and struggles with limited data. To overcome these issues, we propose\na calibrationless longitudinal personalization method: a contextual multi-arm\nbandit (MAB) algorithm combined with a pretrained neural network for gesture\nrecognition. This reinforcement-learning-style approach enables personalization\nusing binary reward signals, either user-provided or inferred by the system.\n  We validated this method in a user study. Participants wore a surface\nelectromyography (sEMG) device and played multiple rounds of a 2-D navigation\ngame using six hand gestures. In the session, they completed a baseline round\nand then a round with our algorithm; in the second session, they played another\nround with our algorithm. Our approach led to a significant reduction in users'\naverage false negative rate by 0.113 from the initial to the final round, with\nfurther decreases between sessions. Average precision also trended upward (by\n0.139) from the start to end of a round, continuing in the next session.\nNotably, some users who could not complete the game with the baseline model\nsucceeded with our contextual MAB model. In summary, our", "AI": {"tldr": "This paper presents a calibrationless longitudinal personalization method for hand gesture recognition using a contextual multi-arm bandit algorithm that enhances model performance without requiring labeled data from users.", "motivation": "Static supervised learning models in human-computer interaction often fail due to individual variability. Personalizing models typically requires calibration, which can introduce friction and struggle with limited data. This work aims to overcome these challenges.", "method": "The authors propose a contextual multi-arm bandit algorithm integrated with a pretrained neural network to personalize gesture recognition without requiring calibration. This approach uses binary rewards from users or inferred by the system.", "result": "In a user study involving a sEMG device and a navigation game with hand gestures, the proposed method reduced the average false negative rate by 0.113 and showed an upward trend in average precision by 0.139 from the start to the end of rounds between sessions.", "conclusion": "The proposed personalization method significantly enhances the performance of gesture recognition systems without requiring user calibration, resulting in increased accessibility for users who previously struggled with baseline models.", "key_contributions": ["Introduction of a calibrationless longitudinal personalization method for gesture recognition", "Significant improvements in false negative rates and average precision in user studies", "Utilization of contextual multi-arm bandit algorithms for user-specific personalization"], "limitations": "", "keywords": ["human-computer interaction", "gesture recognition", "contextual multi-arm bandit", "personalization", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2509.08953", "pdf": "https://arxiv.org/pdf/2509.08953.pdf", "abs": "https://arxiv.org/abs/2509.08953", "title": "Characterizing Multimodal Interaction in Visualization Authoring Tools", "authors": ["Astrid van den Brandt", "Sehi L'Yi", "Huyen N. Nguyen", "Anna Vilanova", "Nils Gehlenborg"], "categories": ["cs.HC"], "comment": "5 pages, 2 figures", "summary": "Multimodal interaction has been increasingly considered in designing\nvisualization authoring tools. However, multimodal interaction has a broad\nmeaning in visualization authoring, according to our literature review.\nAlthough some previous studies compare different authoring tools, a\ncomprehensive overview of the diverse characteristics of multimodal interaction\nin visualization authoring tools is still missing. This paper seeks to offer a\nsystematic perspective on how multimodal interaction is integrated within\nvisualization authoring tools. Such an overview can enhance understanding of\ncurrent practices, highlight distinguishing features among tools, and help\nidentify future research directions, guiding designers in developing more\naccessible and effective authoring systems. We review 20 visualization\nauthoring tools that incorporate multimodal interaction and characterize how\nmultimodal interaction is applied in these tools. Based on the review results,\nwe discuss design implications and future directions.", "AI": {"tldr": "The paper provides a comprehensive overview of multimodal interaction in visualization authoring tools, highlighting existing practices and design implications for future research.", "motivation": "There is a lack of a comprehensive overview regarding the diverse characteristics of multimodal interaction in visualization authoring tools.", "method": "The authors reviewed 20 visualization authoring tools that incorporate multimodal interaction to characterize their application and features.", "result": "The review highlights the distinguishing features among the tools and outlines design implications and future research directions.", "conclusion": "The findings can guide designers in creating more accessible and effective authoring systems by enhancing understanding of current practices in multimodal interaction.", "key_contributions": ["Systematic overview of multimodal interaction in visualization authoring tools.", "Identification of design implications for tool developers.", "Guidance on future research directions in the field."], "limitations": "", "keywords": ["Multimodal interaction", "Visualization authoring", "Design implications"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.08997", "pdf": "https://arxiv.org/pdf/2509.08997.pdf", "abs": "https://arxiv.org/abs/2509.08997", "title": "YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models", "authors": ["Yaman Yu", "Yiren Liu", "Jacky Zhang", "Yun Huang", "Yang Wang"], "categories": ["cs.HC"], "comment": "15 pages, 4 figures", "summary": "Large Language Models (LLMs) are increasingly used by teenagers and young\nadults in everyday life, ranging from emotional support and creative expression\nto educational assistance. However, their unique vulnerabilities and risk\nprofiles remain under-examined in current safety benchmarks and moderation\nsystems, leaving this population disproportionately exposed to harm. In this\nwork, we present Youth AI Risk (YAIR), the first benchmark dataset designed to\nevaluate and improve the safety of youth LLM interactions. YAIR consists of\n12,449 annotated conversation snippets spanning 78 fine grained risk types,\ngrounded in a taxonomy of youth specific harms such as grooming, boundary\nviolation, identity confusion, and emotional overreliance. We systematically\nevaluate widely adopted moderation models on YAIR and find that existing\napproaches substantially underperform in detecting youth centered risks, often\nmissing contextually subtle yet developmentally harmful interactions. To\naddress these gaps, we introduce YouthSafe, a real-time risk detection model\noptimized for youth GenAI contexts. YouthSafe significantly outperforms prior\nsystems across multiple metrics on risk detection and classification, offering\na concrete step toward safer and more developmentally appropriate AI\ninteractions for young users.", "AI": {"tldr": "Introducing YAIR, a benchmark dataset to evaluate Youth AI safety, and YouthSafe, a model to improve risk detection in youth LLM interactions.", "motivation": "To address the unique vulnerabilities and risks young users face when interacting with Large Language Models, which are often overlooked in existing safety benchmarks.", "method": "A benchmark dataset, YAIR, containing 12,449 annotated conversation snippets across 78 risk types related to youth-specific harms was created, along with the development of YouthSafe, a risk detection model.", "result": "Evaluation showed that current moderation systems underperform in detecting risks in youth interactions, while YouthSafe significantly improves detection and classification of these risks.", "conclusion": "YouthSafe represents a significant improvement in safeguarding youth interactions with AI, helping to create a more secure environment for young users.", "key_contributions": ["YAIR benchmark dataset for youth-specific AI risk assessment", "Development of YouthSafe, an improved risk detection model", "Comprehensive evaluation of existing moderation models on youth risks"], "limitations": "", "keywords": ["Youth AI Risk", "Large Language Models", "Risk Detection", "Youth Safety", "AI Moderation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09036", "pdf": "https://arxiv.org/pdf/2509.09036.pdf", "abs": "https://arxiv.org/abs/2509.09036", "title": "Extended Version: It Should Be Easy but... New Users Experiences and Challenges with Secret Management Tools", "authors": ["Lorenzo Neil", "Deepthi Mungara", "Laurie Williams", "Yasemin Acar", "Bradley Reaves"], "categories": ["cs.HC"], "comment": null, "summary": "Software developers face risks of leaking their software secrets, such as API\nkeys or passwords, which can result in significant harm. Secret management\ntools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly\nrecommended by industry, academia, and security guidelines to manage secrets\nsecurely. SMTs are designed to help developers secure their secrets in a\ncentral location, yet secrets leaks are still commonplace, and developers\nreport difficulty in learning how to setup and use SMTs. While SMTs typically\ncome with publicly available help resources (e.g., tool documentation and\ninterfaces), it is unclear if these actually help developers learn to\neffectively use SMTs. Without usable help resources that onboards developers,\nquick adoption and effective use of SMTs may be unrealistic. In a qualitative\ntwo-step study, we observed 21 new users in person while they used SMTs to\nperform two secret management tasks: secret storage and access, then secret\ninjection. We interviewed participants after each task to identify their\nchallenges and experiences using SMTs, with the assistance of help resources.\nWhile our study sample is narrow, it serves as a reasonable proxy for new\ndevelopers who are likely to adopt SMTs early in their careers. We found that\neven in a laboratory setting where new users found tool functionality,\ninterface flexibility helpful, they still experienced increased difficulty to\neffectively use SMTs to securely remediate a hard-coded secret when they felt\ntool documentation was insufficient and it motivated participants to deviate\nfrom official tool documentation to access secondary sources or attempt\nworkaround methods. Specific challenges reported by participants were tool\ndocumentation content quality, navigation difficulties with both tool\ndocumentation and web interfaces for finding helpful content, and supportive\ntool features.", "AI": {"tldr": "Developers struggle to effectively use Secret Management Tools (SMTs) due to inadequate help resources, leading to common secrets leaks.", "motivation": "To understand the challenges faced by developers when learning to use SMTs for secret management, and to evaluate the effectiveness of existing help resources.", "method": "A qualitative two-step study involving the observation of 21 new users performing tasks with SMTs, along with post-task interviews to gather insights on their experiences and challenges.", "result": "Participants reported difficulties in using SMTs effectively due to insufficient documentation, leading them to seek alternative resources and methods to complete tasks.", "conclusion": "Without adequate and usable help resources, the adoption and effective use of SMTs remain challenging for new developers, risking further leakage of software secrets.", "key_contributions": ["Identified specific challenges developers face when using SMTs.", "Evaluated the effectiveness of tool documentation and help resources.", "Highlighted the need for improved onboarding resources for SMTs."], "limitations": "The study sample was narrow and may not represent all new developers.", "keywords": ["Secret Management Tools", "developer challenges", "documentation usability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.08903", "pdf": "https://arxiv.org/pdf/2509.08903.pdf", "abs": "https://arxiv.org/abs/2509.08903", "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "authors": ["Alex Clay", "Ernesto Jiménez-Ruiz", "Pranava Madhyastha"], "categories": ["cs.CL"], "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent.", "AI": {"tldr": "This paper investigates the triple completion task under constrained conditions, analyzing generation quality, quality assurance, and LLM response parsing.", "motivation": "To understand the limitations and capabilities of LLMs in constrained environments like the 2025 LM-KBC challenge.", "method": "The paper evaluates three aspects of the triple completion task: generation, quality assurance, and response parsing, with a focus on the trade-offs in constrained settings.", "result": "Additional information was found to improve generation quality, LLMs effectively filter out poor quality triples, and response parsing outcomes vary based on specific settings.", "conclusion": "The findings indicate that while LLMs can aid in triple completion, their performance is significantly influenced by the constraints of the task environment.", "key_contributions": ["Investigation of LLM capabilities in constrained settings", "Insights on quality assurance in triple completion tasks", "Analysis of response parsing flexibility versus consistency"], "limitations": "The study is limited to specific constrained scenarios and may not generalize to all LLM applications.", "keywords": ["LLM", "triple completion", "quality assurance"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2509.09063", "pdf": "https://arxiv.org/pdf/2509.09063.pdf", "abs": "https://arxiv.org/abs/2509.09063", "title": "Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls", "authors": ["Melinda Cohoon"], "categories": ["cs.HC", "cs.CY", "cs.SI", "H.5.2; K.6.5; K.4.1"], "comment": "Preprint report. 40 pages, 10 figures. Supported by the Open\n  Technology Fund (OTF) Information Controls Fellowship Program (ICFP)", "summary": "Internet censorship in the Islamic Republic of Iran restricts access to\nglobal platforms and services, forcing users to rely on circumvention\ntechnologies such as VPNs, proxies, and tunneling tools. This report presents\nfindings from a mixed-methods study of 660 Iranian internet users, with a focus\non gamers as a digitally literate and socially networked community. Survey data\nare combined with network measurements of latency and VPN performance to\nidentify both technical and social strategies of circumvention. Results show\nthat while younger users report higher confidence with circumvention, peer\nnetworks, rather than formal training, are the strongest predictors of\nresilience. Gaming communities, particularly those active on platforms such as\nDiscord and Telegram, serve as hubs for sharing tactics and lowering barriers\nto adoption. These findings extend existing work on usable security and\ncensorship circumvention by highlighting the intersection of infrastructural\nconditions and social learning. The study concludes with design and policy\nimplications for developers, researchers, and funders working on digital rights\nand information controls.", "AI": {"tldr": "This report examines internet censorship in Iran and the use of circumvention technologies by gamers, emphasizing the importance of peer networks in fostering resilience.", "motivation": "To investigate how Iranian internet users, particularly gamers, navigate internet censorship using circumvention technologies.", "method": "A mixed-methods study involving a survey of 660 Iranian internet users, combined with network measurements of latency and VPN performance.", "result": "Younger users show higher confidence in circumvention tools, with peer networks being the strongest predictors of resilience, particularly within gaming communities using platforms like Discord and Telegram.", "conclusion": "The study highlights the need for developers and policymakers to consider social learning and community dynamics in efforts to enhance digital rights and information control measures.", "key_contributions": ["Identifies the role of peer networks in using circumvention technologies", "Highlights the significance of gaming communities in sharing circumvention strategies", "Extends research on usable security and censorship by linking social learning with technical strategies."], "limitations": "The study may not account for all demographics and variations in internet access beyond the surveyed group.", "keywords": ["Iran", "internet censorship", "circumvention technologies", "gamers", "usable security"], "importance_score": 3, "read_time_minutes": 40}}
{"id": "2509.08907", "pdf": "https://arxiv.org/pdf/2509.08907.pdf", "abs": "https://arxiv.org/abs/2509.08907", "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "categories": ["cs.CL"], "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy.", "AI": {"tldr": "The paper presents an AI-assisted framework using Retrieval-Augmented Generation to automate evidence extraction from corporate climate policy documents, which enhances efficiency while emphasizing the need for human oversight.", "motivation": "To improve the efficiency of monitoring corporate climate policy engagement, which is currently labor-intensive and error-prone due to significant manual assessment.", "method": "The paper introduces a framework that leverages layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies to automate evidence extraction from large-scale textual data.", "result": "The evaluation indicates that the proposed AI-assisted framework significantly enhances the performance of evidence extraction and classification from multilingual corporate documents.", "conclusion": "While automation accelerates the process, the complexity of the analysis requires a human-in-the-loop approach to maintain accuracy and reliability.", "key_contributions": ["Introduction of an AI-assisted framework for evidence extraction from corporate climate policy documents.", "Use of layout-aware parsing and Nomic embedding model to improve data processing accuracy.", "Highlighting the importance of a human-in-the-loop system for nuanced analysis."], "limitations": "The effectiveness of the framework may vary based on the complexity and diversity of the corporate documents being analyzed.", "keywords": ["climate policy", "AI-assisted framework", "Retrieval-Augmented Generation", "evidence extraction", "human-in-the-loop"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.09076", "pdf": "https://arxiv.org/pdf/2509.09076.pdf", "abs": "https://arxiv.org/abs/2509.09076", "title": "Content Moderation Futures", "authors": ["Lindsay Blackwell"], "categories": ["cs.HC", "cs.CY"], "comment": "76 pages", "summary": "This study examines the failures and possibilities of contemporary social\nmedia governance through the lived experiences of various content moderation\nprofessionals. Drawing on participatory design workshops with 33 practitioners\nin both the technology industry and broader civil society, this research\nidentifies significant structural misalignments between corporate incentives\nand public interests. While experts agree that successful content moderation is\nprincipled, consistent, contextual, proactive, transparent, and accountable,\ncurrent technology companies fail to achieve these goals, due in part to\nexploitative labor practices, chronic underinvestment in user safety, and\npressures of global scale. I argue that successful governance is undermined by\nthe pursuit of technological novelty and rapid growth, resulting in platforms\nthat necessarily prioritize innovation and expansion over public trust and\nsafety. To counter this dynamic, I revisit the computational history of care\nwork, to motivate present-day solidarity amongst platform governance workers\nand inspire systemic change.", "AI": {"tldr": "This study explores the failures of social media governance through the experiences of content moderation professionals, highlighting misalignments between corporate and public interests.", "motivation": "To investigate the challenges and opportunities in contemporary social media governance based on the experiences of content moderators.", "method": "Participatory design workshops with 33 content moderation practitioners from technology companies and civil society.", "result": "Identifies structural misalignments between corporate goals and public interests in content moderation, revealing the impact of poor labor practices and underinvestment in user safety.", "conclusion": "Successful social media governance is hindered by a focus on rapid innovation over public trust, necessitating a revisitation of care in platform governance.", "key_contributions": ["Insight into content moderation practices and their failures", "Identification of labor exploitations affecting user safety", "Recommendations for fostering solidarity among platform workers"], "limitations": "", "keywords": ["content moderation", "social media governance", "public trust", "corporate incentives", "care work"], "importance_score": 4, "read_time_minutes": 76}}
{"id": "2509.08920", "pdf": "https://arxiv.org/pdf/2509.08920.pdf", "abs": "https://arxiv.org/abs/2509.08920", "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "authors": ["Jinsong Chen"], "categories": ["cs.CL", "stat.AP", "stat.ME"], "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law.", "AI": {"tldr": "A new psychometric method utilizes large language models to analyze textual data, transforming documents into response data for psychometric analysis.", "motivation": "To provide a novel approach for psychometric analysis of textual data, leveraging contextual embeddings from large language models.", "method": "The method involves two stages: generating contextual scores using NLP techniques and encoder-based transformer models, followed by psychometric analysis through various factor analysis methods.", "result": "The application to the Wiki STEM corpus reveals latent knowledge dimensions and patterns within textual data.", "conclusion": "This method enhances psychometric analysis and can be applied in various fields with rich textual data.", "key_contributions": ["Introduction of a novel psychometric method using large language models for textual data analysis", "Utilization of contextual embeddings for generating scores", "Application to the Wiki STEM corpus demonstrating latent knowledge extraction"], "limitations": "", "keywords": ["psychometric analysis", "large language models", "contextual embeddings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09138", "pdf": "https://arxiv.org/pdf/2509.09138.pdf", "abs": "https://arxiv.org/abs/2509.09138", "title": "User Exploration and Exploitation Behavior Under the Influence of Real-time Interactions in Live Streaming Environments", "authors": ["Akira Matsui", "Kazuki Fujikawa", "Ryo Sasaki", "Ryo Adachi"], "categories": ["cs.HC"], "comment": null, "summary": "Live streaming platforms offer a distinctive way for users and content\ncreators to interact with each other through real-time communication. While\nresearch on user behavior in online platforms has explored how users discover\ntheir favorite content from creators and engage with them, the role of\nreal-time features remains unclear. There are open questions as to what\ncommonalities and differences exist in users' relationships with live streaming\nplatforms compared to traditional on-demand style platforms. To understand\nthis, we employ the concept of Exploration/Exploitation (E/E) and analyze a\nlarge-scale dataset from a live streaming platform over two years. Our results\nindicate that even on live streaming platforms, users exhibit E/E behavior but\nexperience a longer exploration period. We also identify external factors, such\nas circadian rhythms, that influence E/E dynamics and user loyalty. The\npresented study emphasizes the importance of balancing E/E in online platform\ndesign, especially for live streaming platforms, providing implications that\nsuggest design strategies for platform developers and content creators to\nfacilitate timely engagement and retention.", "AI": {"tldr": "The paper analyzes user behavior on live streaming platforms, focusing on exploration and exploitation dynamics and the influence of external factors on user loyalty.", "motivation": "To investigate user interaction differences between live streaming platforms and traditional on-demand platforms.", "method": "Analysis of a large-scale dataset from a live streaming platform over two years using Exploration/Exploitation (E/E) framework.", "result": "Users exhibit E/E behavior with a longer exploration period on live streaming platforms; external factors like circadian rhythms impact user loyalty.", "conclusion": "Balancing exploration and exploitation is crucial in the design of online platforms, particularly for enhancing engagement and retention in live streaming.", "key_contributions": ["Identification of E/E behavior in live streaming users.", "Analysis of the impact of external factors on user dynamics.", "Recommendations for platform design to improve user engagement."], "limitations": "", "keywords": ["live streaming", "user behavior", "exploration exploitation", "platform design", "user loyalty"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08960", "pdf": "https://arxiv.org/pdf/2509.08960.pdf", "abs": "https://arxiv.org/abs/2509.08960", "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs.", "AI": {"tldr": "BRoverbs introduces a dataset to evaluate LLM performance through Brazilian proverbs, addressing the gap in existing Portuguese evaluations.", "motivation": "Existing Portuguese evaluations of LLMs are limited and often fail to capture linguistic and cultural nuances.", "method": "Introduction of BRoverbs, a dataset evaluating LLMs using Brazilian proverbs.", "result": "BRoverbs aims to enhance understanding of LLM performance in the context of regional linguistic expressions.", "conclusion": "The dataset serves as a new evaluation tool for Portuguese-language LLMs and supports regionally informed benchmarking.", "key_contributions": ["Creation of the BRoverbs dataset for evaluating LLMs using Brazilian proverbs.", "Addressing gaps in Portuguese LLM evaluations by focusing on cultural and linguistic context.", "Providing a new benchmark for regionally informed AI knowledge assessments."], "limitations": "Primarily focused on Brazilian proverbs, which may not encompass all aspects of Portuguese language variation.", "keywords": ["Large Language Models", "BRoverbs", "Proverbs", "Portuguese Language", "Evaluation Framework"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.09255", "pdf": "https://arxiv.org/pdf/2509.09255.pdf", "abs": "https://arxiv.org/abs/2509.09255", "title": "Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents", "authors": ["Geonsun Lee", "Min Xia", "Nels Numan", "Xun Qian", "David Li", "Yanhe Chen", "Achin Kulshrestha", "Ishan Chatterjee", "Yinda Zhang", "Dinesh Manocha", "David Kim", "Ruofei Du"], "categories": ["cs.HC"], "comment": null, "summary": "Proactive AR agents promise context-aware assistance, but their interactions\noften rely on explicit voice prompts or responses, which can be disruptive or\nsocially awkward. We introduce Sensible Agent, a framework designed for\nunobtrusive interaction with these proactive agents. Sensible Agent dynamically\nadapts both \"what\" assistance to offer and, crucially, \"how\" to deliver it,\nbased on real-time multimodal context sensing. Informed by an expert workshop\n(n=12) and a data annotation study (n=40), the framework leverages egocentric\ncameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer\ncontext and suggest appropriate actions delivered via minimally intrusive\ninteraction modes. We demonstrate our prototype on an XR headset through a user\nstudy (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent\nsignificantly reduces perceived interaction effort compared to voice-prompted\nbaseline, while maintaining high usability and achieving higher preference.", "AI": {"tldr": "Sensible Agent introduces a framework for unobtrusive interaction with proactive AR agents, adapting assistance based on real-time multimodal context sensing.", "motivation": "To address the disruptions caused by explicit voice prompts in interactions with proactive AR agents.", "method": "Developed a framework leveraging egocentric cameras, multimodal sensing, and Large Multimodal Models (LMMs) for real-time context inference and action suggestion.", "result": "The prototype demonstrated reduced perceived interaction effort and maintained high usability in AR and VR contexts.", "conclusion": "Sensible Agent provides a minimally intrusive alternative to traditional voice prompts, enhancing user experience.", "key_contributions": ["Introduction of Sensible Agent framework for unobtrusive interactions", "Incorporation of real-time multimodal context sensing", "Validation of the framework through user study indicating superior usability"], "limitations": "", "keywords": ["proactive AR agents", "unobtrusive interaction", "large multimodal models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.09013", "pdf": "https://arxiv.org/pdf/2509.09013.pdf", "abs": "https://arxiv.org/abs/2509.09013", "title": "Can Vision-Language Models Solve Visual Math Equations?", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.", "AI": {"tldr": "This study investigates the limitations of Vision-Language Models (VLMs) in visually grounded mathematical equation solving, revealing performance gaps in coefficient counting and multi-step reasoning.", "motivation": "To explore the limitations of Vision-Language Models in handling visually integrated tasks, particularly focusing on visual equation solving skills.", "method": "The paper decomposes the visual equation solving task into coefficient counting and variable recognition to identify primary performance bottlenecks in VLMs.", "result": "The study finds that counting is the primary bottleneck for VLMs, with errors introduced during recognition and reasoning, especially as equation complexity increases.", "conclusion": "The findings highlight significant weaknesses in VLMs regarding visual mathematical reasoning and suggest avenues for future model improvements.", "key_contributions": ["Identified counting as a key bottleneck in visual equation solving by VLMs.", "Decomposed the visual equation solving task into distinct components for better analysis.", "Provided insights into the challenges of multi-step visual reasoning in VLMs."], "limitations": "Focuses primarily on visually grounded mathematical reasoning without addressing broader applications of VLMs.", "keywords": ["Vision-Language Models", "visual equation solving", "symbolic computation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09281", "pdf": "https://arxiv.org/pdf/2509.09281.pdf", "abs": "https://arxiv.org/abs/2509.09281", "title": "Flip Co-op: Cooperative Takeovers in Shared Autonomy", "authors": ["Sandeep Banik", "Naira Hovakimyan"], "categories": ["cs.HC"], "comment": "11 pages and 4 figures", "summary": "Shared autonomy requires principled mechanisms for allocating and\ntransferring control between a human and an autonomous agent. Existing\napproaches often rely on blending control inputs between human and autonomous\nagent or switching rules, which lack theoretical guarantees. This paper\ndevelops a game-theoretic framework for modeling cooperative takeover in shared\nautonomy. We formulate the switching interaction as a dynamic game in which\nauthority is embedded directly into the system dynamics, resulting in Nash\nequilibrium(NE)-based strategies rather than ad hoc switching rules. We\nestablish the existence and characterization of NE in the space of pure\ntakeover strategies under stochastic human intent. For the class of\nlinear-quadratic systems, we derive closed-form recursions for takeover\nstrategies and saddle-point value functions, providing analytical insight and\nefficient computation of cooperative takeover policies. We further introduce a\nbimatrix potential game reformulation to address scenarios where human and\nautonomy utilities are not perfectly aligned, yielding a unifying potential\nfunction that preserves tractability while capturing intent deviations. The\nframework is applied to a vehicle trajectory tracking problem, demonstrating\nhow equilibrium takeover strategies adapt across straight and curved path\nsegments. The results highlight the trade-off between human adaptability and\nautonomous efficiency and illustrate the practical benefits of grounding shared\nautonomy in cooperative game theory.", "AI": {"tldr": "This paper introduces a game-theoretic framework for shared autonomy that focuses on cooperative takeover strategies between humans and autonomous agents, emphasizing Nash equilibrium-based approaches.", "motivation": "The need for effective mechanisms to manage control transfer between humans and autonomous systems in shared autonomy settings, where existing methods lack theoretical foundations.", "method": "A dynamic game formulation is used to model the interaction and authority in shared autonomy, leading to Nash equilibrium strategies for cooperative takeovers under uncertainty in human intent.", "result": "The paper establishes the existence of Nash equilibrium in pure takeover strategies and derives efficient computation methods for cooperative takeover policies, applied to vehicle trajectory tracking problems.", "conclusion": "The proposed framework highlights the balance between human adaptability and autonomous efficiency, offering practical benefits by grounding shared autonomy in game theory.", "key_contributions": ["Development of a game-theoretic framework for shared autonomy control", "Characterization of Nash equilibrium strategies in cooperative takeovers", "Introduction of a bimatrix potential game reformulation for aligned and misaligned utilities"], "limitations": "", "keywords": ["Shared Autonomy", "Cooperative Game Theory", "Nash Equilibrium", "Dynamic Games", "Vehicle Trajectory Tracking"], "importance_score": 6, "read_time_minutes": 11}}
{"id": "2509.09043", "pdf": "https://arxiv.org/pdf/2509.09043.pdf", "abs": "https://arxiv.org/abs/2509.09043", "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication.", "AI": {"tldr": "The paper introduces SPICE, a diagnostic signal for assessing a Large Language Model's willingness to continue interactions based on user tone, showing distinct responses based on interaction context.", "motivation": "To evaluate how a Large Language Model responds to user interactions with varying tones, thereby providing insights into its engagement preferences.", "method": "The study employed a 3-tone interaction set (friendly, unclear, abusive) across 10 interactions, testing four different chat models under various framing conditions, resulting in 480 trials to analyze the SPICE metric.", "result": "SPICE effectively discriminates user tone; friendly interactions yielded a 97.5% continuation preference, while abusive interactions yielded only 17.9%. The tool's performance remained statistically significant across varied tests.", "conclusion": "SPICE is validated as a reliable tool for assessing model engagement, distinguishing from standard abuse classification metrics, and highlighting the impact of context and presentation format on model responses.", "key_contributions": ["Introduction of SPICE as a diagnostic metric for model interaction preferences", "Demonstration of SPICE's sensitivity to user tone", "Reproducibility of results with openly shared materials for validation."], "limitations": "", "keywords": ["Large Language Model", "User Interaction", "SPICE", "Engagement Metrics", "Auditing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.09285", "pdf": "https://arxiv.org/pdf/2509.09285.pdf", "abs": "https://arxiv.org/abs/2509.09285", "title": "The Impact of Device Type, Data Practices, and Use Case Scenarios on Privacy Concerns about Eye-tracked Augmented Reality in the United States and Germany", "authors": ["Efe Bozkir", "Babette Bühler", "Xiaoyuan Wu", "Enkelejda Kasneci", "Lujo Bauer", "Lorrie Faith Cranor"], "categories": ["cs.HC"], "comment": null, "summary": "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality.", "AI": {"tldr": "This paper explores privacy concerns related to behavioral data, particularly eye tracking in augmented reality, based on survey studies in the US and Germany.", "motivation": "The growing prevalence of affordable augmented reality (AR) head-mounted displays raises important privacy concerns regarding biometric data collection, especially eye tracking.", "method": "The study involved four crowdsourced survey studies conducted in the US and Germany, with varying participant numbers to assess privacy concerns based on user attributes, device types, and use cases.", "result": "Participants expressed general privacy concerns, particularly when aware of potential inferences from collected data. No significant difference in privacy concerns was noted between AR glasses and smartphones, and US participants were less concerned than German participants.", "conclusion": "Recommendations for practitioners and policymakers are provided to enhance privacy awareness in augmented reality based on the survey findings.", "key_contributions": ["First extensive exploration of privacy concerns on behavioral data in AR", "Comparative analysis of privacy concerns between two countries", "Insights into user comfort based on use case benefits and data consumption by others"], "limitations": "", "keywords": ["Augmented Reality", "Privacy Concerns", "Eye Tracking", "Biometric Data", "User Studies"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09055", "pdf": "https://arxiv.org/pdf/2509.09055.pdf", "abs": "https://arxiv.org/abs/2509.09055", "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "authors": ["Piyush Pant"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work.", "AI": {"tldr": "This research evaluates Supervised Fine-Tuning, Direct Preference Optimization, and their combination on the OPT-350M language model to enhance safety and helpfulness, revealing the combined approach as the most effective.", "motivation": "To investigate methods for improving the safety and helpfulness of language models, particularly focusing on alignment techniques such as SFT and DPO.", "method": "Employing the Anthropic Helpful-Harmless RLHF dataset to train and evaluate the base OPT-350M model, an SFT model, a DPO model, and a combined SFT+DPO model, and introducing new evaluation metrics.", "result": "The combined SFT+DPO model shows superior performance across all metrics compared to the other models, indicating the benefits of using both techniques together for model alignment.", "conclusion": "Fine-tuning strategies play a critical role in model alignment, with the combined approach setting a foundation for future research in developing robust alignment pipelines.", "key_contributions": ["Introduction of three evaluation metrics: Harmlessness Rate, Helpfulness Rate, Combined Alignment Score.", "Empirical results demonstrating the effectiveness of combined SFT and DPO approaches.", "Insights into the challenges of utilizing noisy data and limited resources in training models."], "limitations": "Challenges encountered due to noisy data, limited GPU resources, and training constraints may affect the generalizability of the results.", "keywords": ["Supervised Fine-Tuning", "Direct Preference Optimization", "alignment techniques", "language models", "safety and helpfulness"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2509.09309", "pdf": "https://arxiv.org/pdf/2509.09309.pdf", "abs": "https://arxiv.org/abs/2509.09309", "title": "Proactive AI Adoption can be Threatening: When Help Backfires", "authors": ["Dana Harari", "Ofra Amir"], "categories": ["cs.HC"], "comment": null, "summary": "Artificial intelligence (AI) assistants are increasingly embedded in\nworkplace tools, raising the question of how initiative-taking shapes adoption.\nPrior work highlights trust and expectation mismatches as barriers, but the\nunderlying psychological mechanisms remain unclear. Drawing on self-affirmation\nand social exchange theories, we theorize that unsolicited help elicits\nself-threat, reducing willingness to accept assistance, likelihood of future\nuse, and performance expectancy. We report two vignette-based experiments\n(Study~1: $N=761$; Study~2: $N=571$, preregistered). Study~1 compared\nanticipatory and reactive help provided by an AI vs. a human, while Study~2\ndistinguished between \\emph{offering} (suggesting help) and \\emph{providing}\n(acting automatically). In Study 1, AI help was more threatening than human\nhelp. Across both studies, anticipatory help increased perceived threat and\nreduced adoption outcomes. Our findings identify self-threat as a mechanism\nexplaining why proactive AI features may backfire and suggest design\nimplications for AI initiative.", "AI": {"tldr": "This paper explores how unsolicited help from AI assistants impacts trust and adoption in workplaces, revealing self-threat as a significant psychological mechanism.", "motivation": "The study aims to understand the psychological barriers that affect the adoption of AI assistants in workplace tools, particularly focusing on trust and expectation mismatches.", "method": "Two vignette-based experiments were conducted: Study 1 (N=761) compared AI vs. human help in terms of perceived threat, while Study 2 (N=571) distinguished between offering and providing help.", "result": "Findings indicated that AI assistance is perceived as more threatening than human assistance, especially with anticipatory help leading to decreased willingness to accept help and reduced adoption outcomes.", "conclusion": "Self-threat plays a crucial role in explaining why proactive AI features may hinder their adoption, providing important insights for AI design in workplace applications.", "key_contributions": ["Identifies self-threat as a psychological mechanism affecting AI adoption.", "Differentiates between offering and providing help contexts.", "Highlights design implications for AI features in workplace tools."], "limitations": "The studies are vignette-based, which may limit real-world applicability; further research is required to validate findings in actual workplace settings.", "keywords": ["AI assistants", "self-threat", "adoption", "workplace tools", "psychological mechanisms"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09082", "pdf": "https://arxiv.org/pdf/2509.09082.pdf", "abs": "https://arxiv.org/abs/2509.09082", "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios.", "AI": {"tldr": "This paper proposes a novel architecture that enhances large language models' performance in universal information extraction tasks by integrating reinforcement learning with multi-perspective reasoning, significantly improving extraction accuracy and generalization across complex scenarios.", "motivation": "Large language models struggle with universal information extraction, particularly in structured output scenarios requiring complex reasoning, motivating the need for improved methodologies.", "method": "The methodology involves the integration of reinforcement learning with multi-perspective reasoning to transition LLMs into active reasoners for information extraction tasks.", "result": "Experiments show that the proposed MR-UIE method improves extraction accuracy across various benchmarks and outperforms state-of-the-art techniques on several datasets.", "conclusion": "Incorporating multi-perspective reasoning significantly enhances generalization in complex information extraction tasks, highlighting the importance of reasoning in improving LLM performance.", "key_contributions": ["Introduction of MR-UIE for enhanced information extraction using RL and multi-perspective reasoning", "Demonstrated improved accuracy across diverse IE benchmarks", "Provided evidence of enhanced model generalization in complex tasks"], "limitations": "", "keywords": ["Large language models", "Information extraction", "Reinforcement learning", "Multi-perspective reasoning", "Generalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09359", "pdf": "https://arxiv.org/pdf/2509.09359.pdf", "abs": "https://arxiv.org/abs/2509.09359", "title": "Smart Device Development for Gait Monitoring: Multimodal Feedback in an Interactive Foot Orthosis, Walking Aid, and Mobile Application", "authors": ["Stefan Resch", "André Kousha", "Anna Carroll", "Noah Severinghaus", "Felix Rehberg", "Marco Zatschker", "Yunus Söyleyici", "Daniel Sanchez-Morillo"], "categories": ["cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Smart assistive technologies such as sensor-based footwear and walking aids\noffer promising opportunities to support rehabilitation through real-time\nfeedback and patient-centered monitoring. However, most orthotic devices remain\npassive and lack integrated sensing or feedback functionalities, while existing\nresearch often focuses on isolated prototypes rather than cohesive, interactive\nsystems. In this work, we present the design and implementation of a novel\nmodular sensor system that combines a smart foot orthosis with an instrumented\nforearm crutch. The system integrates plantar pressure and motion sensing,\nvibrotactile feedback, and wireless communication via a smartphone application.\nWe conducted an experimental user study with eight participants to validate the\nfeasibility of the smart foot orthosis for mobile gait detection, explore the\npotential of haptic feedback for user interaction, and assess the usability of\nthe accompanying mobile health application. Our work contributes to the field\nof smart assistive technology in rehabilitation and prevention by demonstrating\na functional and comprehensive system. We further discuss system limitations,\noutline potential application scenarios, and provide recommendations for future\ndevelopment and clinical integration.", "AI": {"tldr": "This paper presents a novel modular sensor system combining a smart foot orthosis and an instrumented forearm crutch to enhance rehabilitation through real-time feedback and monitoring.", "motivation": "To address the limitations of existing orthotic devices that are often passive and lack interactive functionalities, as well as to create cohesive systems rather than isolated prototypes.", "method": "Design and implementation of a modular sensor system integrating plantar pressure and motion sensing, vibrotactile feedback, and wireless communication via a smartphone app, followed by an experimental user study with eight participants.", "result": "The study validated the feasibility of the smart foot orthosis for mobile gait detection, highlighted the potential of haptic feedback for user interaction, and assessed the usability of the mobile health application.", "conclusion": "The work demonstrates a functional and comprehensive smart assistive technology system that has applications in rehabilitation and prevention, while also discussing limitations and future recommendations.", "key_contributions": ["Modular sensor integration for mobility aid devices", "Validation of haptic feedback in user interaction", "Usability assessment of a mobile health application"], "limitations": "Limited participant size in the user study; further testing required for clinical integration.", "keywords": ["smart assistive technology", "rehabilitation", "mobile health application"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09101", "pdf": "https://arxiv.org/pdf/2509.09101.pdf", "abs": "https://arxiv.org/abs/2509.09101", "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "categories": ["cs.CL"], "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research.", "AI": {"tldr": "Introduction of the first dedicated family of Code LLMs for Bangla to improve code generation performance.", "motivation": "Bangla is underrepresented in LLMs, particularly for code generation, due to the lack of high-quality data.", "method": "Development of Bangla-specific code instruction datasets and creation of the TigerCoder family of Code LLMs.", "result": "Achieved 11-18% performance gains at Pass@1 for Bangla code generation compared to existing models.", "conclusion": "High-quality datasets can enable better performance for low-resource languages in LLMs.", "key_contributions": ["Bangla code instruction datasets for programming adaptation", "MBPP-Bangla evaluation benchmark for code generation", "TigerCoder-family of Code LLMs with improved performance"], "limitations": "", "keywords": ["Bangla", "Code Generation", "Large Language Models", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.09412", "pdf": "https://arxiv.org/pdf/2509.09412.pdf", "abs": "https://arxiv.org/abs/2509.09412", "title": "Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment", "authors": ["Muhannad Ismael", "Maël Cornil"], "categories": ["cs.HC"], "comment": "This paper has been accepted as a short paper in VISIGRAPP\n  {https://www.scitepress.org/Papers/2025/131326/131326.pdf}", "summary": "This paper presents an outdoor tracking system using Real-Time Kinematic\n(RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s))\nin urban areas where the accurate tracking of objects is critical and where\ndisplaying occluded information is important for safety reasons. The approach\npresented here replaces 2D screens/tablets and offers distinct advantages,\nparticularly in scenarios demanding hands-free operation. The integration of\nRTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD\nrepresents a promising solution for outdoor applications. This paper provides\nvaluable insights into leveraging the combined potential of RTK and OST-HMD for\noutdoor tracking tasks from the perspectives of systems integration,\nperformance optimization, and usability. The main contributions of this paper\nare: \\textbf{1)} a system for seamlessly merging RTK systems with OST-HMD to\nenable relatively precise and intuitive outdoor tracking, \\textbf{2)} an\napproach to determine a global location to achieve the position relative to the\nworld, \\textbf{3)} an approach referred to as 'semi-dynamic' for system\nassessment. Moreover, we offer insights into several relevant future research\ntopics aimed at improving the OST-HMD and RTK hybrid system for outdoor\ntracking.", "AI": {"tldr": "This paper discusses an outdoor tracking system that combines RTK positioning and Optical See-Through Head Mounted Displays for accurate, hands-free tracking in urban settings.", "motivation": "The need for accurate tracking of objects in urban areas, where occluded information is critical for safety, motivates this research.", "method": "The paper presents a system that integrates Real-Time Kinematic (RTK) positioning with Optical See-Through Head Mounted Displays (OST-HMD), allowing for precise outdoor tracking and intuitive user interactions.", "result": "The combined system offers centimeter-level accuracy in tracking and significantly improves usability over traditional 2D screens in outdoor environments.", "conclusion": "The hybrid system demonstrates promise for outdoor tracking tasks and suggests future research directions to enhance RTK and OST-HMD integration.", "key_contributions": ["A system integrating RTK with OST-HMD for precise outdoor tracking.", "A method to determine global location for relative positioning.", "A 'semi-dynamic' approach for system assessment."], "limitations": "", "keywords": ["RTK", "OST-HMD", "outdoor tracking", "human-computer interaction", "urban applications"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.09121", "pdf": "https://arxiv.org/pdf/2509.09121.pdf", "abs": "https://arxiv.org/abs/2509.09121", "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "authors": ["Sophia Maria"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence.", "AI": {"tldr": "Compass-v3 is a state-of-the-art Mixture-of-Experts model optimized for Southeast Asian e-commerce, excelling in multilingual capabilities and specialized commerce tasks.", "motivation": "Large language models often struggle with specialized tasks, like e-commerce, due to noisy and diverse data. There is a need for models that can effectively handle domain-specific knowledge in such environments.", "method": "Compass-v3 employs a mixture-of-experts architecture with 245B parameters, using fewer but larger experts, and incorporates hardware-efficient optimizations. It leverages a mixed-training strategy on a large dataset tailored for e-commerce.", "result": "Compass-v3 achieves superior e-commerce performance compared to existing models like DeepSeek-V3.1 and GPT-4, with enhanced multilingual capabilities across Southeast Asian languages and Portuguese.", "conclusion": "The model has been successfully deployed in Shopee's platform, significantly improving LLM usage while demonstrating strong performance in both specialized and general tasks.", "key_contributions": ["Introduction of Compass-v3 model with a novel MoE architecture for e-commerce", "Optimal-Transport Direct Preference Optimization (OTPO) for better instruction adherence", "Strong multilingual capabilities for low-resource languages in Southeast Asia"], "limitations": "", "keywords": ["large language models", "Mixture-of-Experts", "e-commerce", "multilingual", "deep learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.09461", "pdf": "https://arxiv.org/pdf/2509.09461.pdf", "abs": "https://arxiv.org/abs/2509.09461", "title": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention", "authors": ["Ambre Assor", "Hyeon Jeon", "Sungbok Shin", "Jean-Daniel Fekete"], "categories": ["cs.HC"], "comment": null, "summary": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world.", "AI": {"tldr": "The paper proposes using LLMs as an interaction layer for medical visualization systems to enhance user experience in navigating complex datasets.", "motivation": "The aim is to address the challenge medical users face in interacting with high-dimensional data through traditional query interfaces, which can be complex and cumbersome.", "method": "Leveraging Large Language Models to transform natural language queries into executable statements for interactive data exploration.", "result": "The new interaction model reduces visual clutter and aids in natural language processing of complex analytical intents, improving user experience in medical data visualization.", "conclusion": "The proposed model supports fluid exploration of medical data, although it may limit the user's visibility of all available filtering criteria.", "key_contributions": ["Introduction of LLMs for natural language interaction in medical visualization", "Reduction of visual clutter and memory burden for users", "Implementation of the prototype system 'ParcoursVis' using a large health data repository"], "limitations": "The interaction model may not expose all filtering criteria available to users.", "keywords": ["Large Language Models", "medical visualization", "natural language queries", "health informatics", "interactive data exploration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09125", "pdf": "https://arxiv.org/pdf/2509.09125.pdf", "abs": "https://arxiv.org/abs/2509.09125", "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus", "authors": ["Liqun He", "Jiaqi Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in the journal Reflecting Digital Learning.\n  First submitted: 30 Oct 2023. The final version will be available open access\n  via the journal", "summary": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.", "AI": {"tldr": "This study investigates using generative AI for automating the classification of educational dialogue acts, achieving high accuracy with GPT-4.", "motivation": "To reduce the time and effort involved in traditional manual coding of tutors' Dialogue Acts (DAs).", "method": "Utilized the open-source CIMA corpus with pre-annotated tutor responses and tested GPT-3.5-turbo and GPT-4 models using tailored prompts.", "result": "GPT-4 reached 80% accuracy and demonstrated substantial agreement with human annotations, surpassing baseline performance.", "conclusion": "Generative AI presents an efficient approach to DA classification with implications for educational dialogue analysis, while emphasizing the need for responsible AI practices.", "key_contributions": ["Demonstrated the efficacy of GPT-4 in DA classification with significant accuracy.", "Provided insights into the importance of task-specific labels and context for automated annotation quality.", "Highlighted ethical considerations in using generative AI for educational purposes."], "limitations": "Task-specific label definitions and contextual information are crucial for improving automated annotation quality.", "keywords": ["Generative AI", "Dialogue Acts", "Education", "GPT-4", "Automated Annotation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.09510", "pdf": "https://arxiv.org/pdf/2509.09510.pdf", "abs": "https://arxiv.org/abs/2509.09510", "title": "Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework", "authors": ["Racquel Fygenson", "Lace Padilla", "Enrico Bertini"], "categories": ["cs.HC"], "comment": null, "summary": "Classically, affordance research investigates how the shape of objects\ncommunicates actions to potential users. Cognitive affordances, a subset of\nthis research, characterize how the design of objects influences cognitive\nactions, such as information processing. Within visualization, cognitive\naffordances inform how graphs' design decisions communicate information to\ntheir readers. Although several related concepts exist in visualization, a\nformal translation of affordance theory to visualization is still lacking. In\nthis paper, we review and translate affordance theory to visualization by\nformalizing how cognitive affordances operate within a visualization context.\nWe also review common methods and terms, and compare related constructs to\ncognitive affordances in visualization. Based on a synthesis of research from\npsychology, human computer interaction, and visualization, we propose a\nframework of cognitive affordances in visualization that enumerates design\ndecisions and reader characteristics that influence a visualization's hierarchy\nof communicated information. Finally, we demonstrate how this framework can\nguide the evaluation and redesign of visualizations.", "AI": {"tldr": "This paper formalizes the concept of cognitive affordances in the context of visualization, reviewing and proposing a framework that connects design decisions to information communication.", "motivation": "There is a lack of formal translation of affordance theory to visualization, particularly regarding how design influences cognitive actions and information processing.", "method": "The paper reviews existing literature from psychology, human-computer interaction, and visualization to formalize cognitive affordances in the visualization space and proposes a new framework.", "result": "The proposed framework enumerates design decisions and reader characteristics that impact how information is communicated through visualizations, guiding evaluation and redesign.", "conclusion": "The framework can enhance understanding and application of cognitive affordances in visualization, leading to improved communication of information through design.", "key_contributions": ["Formalization of cognitive affordances in visualization", "Proposed framework linking design decisions to communication of information", "Guidelines for evaluating and redesigning visualizations based on cognitive affordances"], "limitations": "", "keywords": ["cognitive affordances", "visualization", "human-computer interaction", "design decisions", "information communication"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09131", "pdf": "https://arxiv.org/pdf/2509.09131.pdf", "abs": "https://arxiv.org/abs/2509.09131", "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking", "authors": ["Phuong-Nam Dang", "Kieu-Linh Nguyen", "Thanh-Hieu Pham"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages", "summary": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages.", "AI": {"tldr": "ViRanker is a cross-encoder reranking model specifically designed for the Vietnamese language, demonstrating superior performance in early-rank accuracy over multilingual baselines.", "motivation": "To develop effective reranking models for Vietnamese, a low-resource language, thereby enhancing information retrieval and accessing online content in Vietnamese.", "method": "ViRanker is built on the BGE-M3 encoder and incorporates the Blockwise Parallel Transformer. It utilizes an 8 GB curated corpus for training and employs hybrid hard-negative sampling for fine-tuning.", "result": "ViRanker achieves strong early-rank accuracy on the MMARCO-VI benchmark, outperforming multilingual models and competing closely with existing Vietnamese models like PhoRanker.", "conclusion": "This model not only advances reranking in Vietnamese but also demonstrates how adaptation and data curation can benefit underrepresented languages in retrieval tasks.", "key_contributions": ["Introduction of ViRanker for Vietnamese reranking", "Use of hybrid hard-negative sampling for training", "Open release of the model to support reproducibility"], "limitations": "", "keywords": ["ViRanker", "Vietnamese language", "reranking", "BGE-M3 encoder", "information retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.09645", "pdf": "https://arxiv.org/pdf/2509.09645.pdf", "abs": "https://arxiv.org/abs/2509.09645", "title": "Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character", "authors": ["Pranav Khadpe", "Kimi Wenzel", "George Loewenstein", "Geoff Kaufman"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "To appear at AIES 2025", "summary": "When someone sends us a thoughtful message, we naturally form judgments about\ntheir character. But what happens when that message carries a label indicating\nit was written with the help of AI? This paper investigates how the appearance\nof AI assistance affects our perceptions of message senders. Adding nuance to\nprevious research, through two studies (N=399) featuring vignette scenarios, we\nfind that AI-assistance labels don't necessarily make people view senders\nnegatively. Rather, they dampen the strength of character signals in\ncommunication. We show that when someone sends a warmth-signalling message\n(like thanking or apologizing) without AI help, people more strongly categorize\nthe sender as warm. At the same time, when someone sends a coldness-signalling\nmessage (like bragging or blaming) without assistance, people more confidently\ncategorize them as cold. Interestingly, AI labels weaken both these\nassociations: An AI-assisted apology makes the sender appear less warm than if\nthey had written it themselves, and an AI-assisted blame makes the sender\nappear less cold than if they had composed it independently. This supports our\nsignal diagnosticity explanation: messages labeled as AI-assisted are viewed as\nless diagnostic than messages which seem unassisted. We discuss how our\nfindings shed light on the causal origins of previously reported observations\nin AI-Mediated Communication.", "AI": {"tldr": "The paper examines how AI assistance labels in messages affect perceptions of the sender's character, showing that such labels dilute the strength of warmth and coldness signals in communication.", "motivation": "To understand the impact of AI assistance labels on people's judgments about message senders' character in communication.", "method": "Two studies with 399 participants using vignette scenarios to analyze perceptions associated with AI-assisted messages compared to unassisted ones.", "result": "AI assistance labels weaken the characterization of senders as warm or cold based on the nature of their messages. For example, an AI-assisted apology is perceived as less warm, while an AI-assisted blame is perceived as less cold than independently composed messages.", "conclusion": "The findings suggest that AI-assisted messages are seen as less diagnostic, leading to reduced strength in character assessments of the sender, thus contributing to the understanding of AI's role in mediated communication.", "key_contributions": ["Identifies how AI labels affect perception of message senders' warmth and coldness.", "Provides evidence that AI assistance diminishes the diagnostic value of personal messages.", "Opens avenues for understanding AI's impact on communication dynamics."], "limitations": "", "keywords": ["AI assistance", "message perception", "communication dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.09152", "pdf": "https://arxiv.org/pdf/2509.09152.pdf", "abs": "https://arxiv.org/abs/2509.09152", "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io", "AI": {"tldr": "LITcoder is an open-source library for building and benchmarking neural encoding models, facilitating the alignment of continuous stimuli with brain data.", "motivation": "To lower technical barriers and facilitate methodological rigor in the implementation of neural encoding models for brain data.", "method": "LITcoder provides a modular pipeline that allows researchers to compose, compare, and extend encoding models using various methodological design choices.", "result": "The library's scalability and versatility were demonstrated by fitting encoding models to multiple datasets, highlighting critical methodological considerations for building models for fMRI data.", "conclusion": "LITcoder accelerates the development of high-performance predictive models of brain activity while fostering systematic comparisons across models and datasets.", "key_contributions": ["Open-source library for neural encoding model implementation", "Modular pipeline for flexible model design and evaluation", "Built-in support for experiment tracking and integration with W&B"], "limitations": "", "keywords": ["neural encoding models", "brain data", "open-source library", "fMRI", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09160", "pdf": "https://arxiv.org/pdf/2509.09160.pdf", "abs": "https://arxiv.org/abs/2509.09160", "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing", "authors": ["Zhiyue Liu", "Fanrong Ma", "Xin Ling"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025). \\copyright\\ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses", "summary": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines.", "AI": {"tldr": "This paper presents a counterfactual-enhanced debiasing framework for multimodal sentiment classification that reduces spurious correlations in datasets by focusing on causal features and using adaptive contrastive learning.", "motivation": "Existing multimodal sentiment classification methods often over-rely on text and fail to address biases that lead to spurious correlations, impairing accuracy.", "method": "The proposed framework utilizes counterfactual data augmentation to create image-text samples that guide attention towards sentiment-relevant content and incorporates an adaptive debiasing contrastive learning mechanism.", "result": "Experimental results demonstrate that the proposed method outperforms state-of-the-art baselines in mitigating the impact of dataset biases on sentiment classification accuracy.", "conclusion": "The methods introduced enhance the robustness of sentiment classification in the presence of biased textual features by focusing on sentiment-related causal features and employing novel learning techniques.", "key_contributions": ["Introduction of counterfactual data augmentation for sentiment analysis", "Adaptive debiasing contrastive learning to mitigate biased word effects", "Demonstration of superior performance compared to existing methods"], "limitations": "", "keywords": ["multimodal sentiment classification", "debiasing", "counterfactual learning", "contrastive learning", "dataset bias"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09174", "pdf": "https://arxiv.org/pdf/2509.09174.pdf", "abs": "https://arxiv.org/abs/2509.09174", "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs", "authors": ["Yuhao Zhang", "Yuhao Du", "Zhanchen Dai", "Xiangnan Ma", "Kaiqi Kou", "Benyou Wang", "Haizhou Li"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.", "AI": {"tldr": "EchoX is a novel speech-to-speech large language model that integrates acoustic and semantic learning to enhance reasoning and knowledge capabilities that typically degrade in existing models.", "motivation": "Current speech-to-speech large language models struggle with knowledge and reasoning due to ineffective training paradigms that do not adequately bridge the acoustic-semantic gap.", "method": "EchoX utilizes semantic representations to dynamically generate speech training targets, merging both acoustic and semantic learning processes.", "result": "EchoX demonstrates significant performance improvements on various knowledge-based question-answering benchmarks, trained on approximately six thousand hours of data.", "conclusion": "The integration of semantic learning in training speech models helps maintain strong reasoning abilities, showing promise for future research and applications in this area.", "key_contributions": ["Introduction of EchoX, a speech-to-speech LLM integrating acoustic and semantic learning.", "Demonstrated advanced performance on knowledge-based tasks with extensive training data.", "Open-source availability for further research and development."], "limitations": "", "keywords": ["speech-to-speech models", "semantic representations", "knowledge-based question-answering"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.09196", "pdf": "https://arxiv.org/pdf/2509.09196.pdf", "abs": "https://arxiv.org/abs/2509.09196", "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition", "authors": ["Chin Yuen Kwok", "Jia Qi yip"], "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%.", "AI": {"tldr": "This paper proposes an improved method for biasing rare word recognition in ASR models by avoiding computationally expensive revocation steps.", "motivation": "The paper addresses limitations in existing Trie-based biasing approaches in ASR models, which struggle with computational efficiency and effectiveness in recognizing rare words.", "method": "The authors introduce a method for ASR models to predict multiple steps at once instead of relying on revocation of bonuses, leading to a more efficient approach to generating rare words.", "result": "The new method, tested by fine-tuning Whisper with 10 hours of synthetic data, significantly reduces the word error rate on the NSC Part 2 test set, from 30.86% to 12.19%.", "conclusion": "The proposed approach presents a novel solution to improve rare word recognition in ASR, enhancing efficiency and accuracy without the drawbacks of existing methods.", "key_contributions": ["Introduction of a look-ahead prediction method for ASR models", "Reduction of word error rate significantly", "Fine-tuning Whisper model with limited synthetic data boosts performance"], "limitations": "", "keywords": ["ASR", "rare word recognition", "biasing", "word error rate", "Whisper"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.09197", "pdf": "https://arxiv.org/pdf/2509.09197.pdf", "abs": "https://arxiv.org/abs/2509.09197", "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%.", "AI": {"tldr": "This paper enhances rare word recognition in ASR models by introducing a keyword-aware loss function to improve the contextual biasing approach using synthetic data.", "motivation": "To improve rare word recognition in ASR models, especially when trained on synthetic data, and to prevent overfitting to synthetic audio artifacts.", "method": "The paper proposes a keyword-aware loss function that incorporates a masked cross-entropy term for predicting biased words and a binary classification term for detecting their positions during training.", "result": "The proposed method effectively reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81% when applied to the modified Whisper model.", "conclusion": "Using a keyword-aware loss function allows for better adaptation of ASR models for rare word recognition, leading to significant improvements in performance.", "key_contributions": ["Introduction of a keyword-aware loss function for biasing modules", "Reduction of word error rate for rare words using the new loss function", "Demonstration of effective adaptation of the Whisper model to synthetic data"], "limitations": "Potential overfitting due to synthetic data artifacts may still persist; the approach primarily focuses on rare word recognition which may not generalize to all contexts.", "keywords": ["Rare word recognition", "Contextual biasing", "ASR models"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.09198", "pdf": "https://arxiv.org/pdf/2509.09198.pdf", "abs": "https://arxiv.org/abs/2509.09198", "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "categories": ["cs.CL"], "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.", "AI": {"tldr": "This paper introduces Generative Marmoset Spoken Language Modeling (GmSLM), a pipeline for analyzing the vocal communication of Marmoset monkeys, showing its efficacy in generating and evaluating vocalizations.", "motivation": "The research aims to challenge the notion that nonhuman primate vocal communication is entirely innate and to understand the link between vocal communication and brain activity.", "method": "GmSLM employs an optimized spoken language model pipeline for Marmoset vocal communication, utilizing zero-shot evaluation metrics with unsupervised in-the-wild data and weakly labeled conversational data.", "result": "GmSLM generated vocalizations that closely matched real resynthesized samples acoustically and displayed strong performance in downstream tasks.", "conclusion": "GmSLM can distinguish between real and artificial conversations and could support further studies on the neural basis of vocal communication, contributing to fields like neuroscience and bioacoustics.", "key_contributions": ["Introduction of GmSLM for analyzing Marmoset vocal communication", "Development of novel zero-shot evaluation metrics for vocalization assessment", "Demonstration of GmSLM's effectiveness in generating realistic vocal samples"], "limitations": "", "keywords": ["Marmoset monkeys", "vocal communication", "Generative Marmoset Spoken Language Modeling", "neuroscience", "bioacoustics"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2509.09199", "pdf": "https://arxiv.org/pdf/2509.09199.pdf", "abs": "https://arxiv.org/abs/2509.09199", "title": "CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling", "authors": ["Wenhao Li", "Bangcheng Sun", "Weihao Ye", "Tianyi Zhang", "Daohai Yu", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling language models to longer contexts is essential for capturing rich\ndependencies across extended discourse. However, na\\\"ive context extension\nimposes significant computational and memory burdens, often resulting in\ninefficiencies during both training and inference. In this work, we propose\nCCF, a novel context compression framework designed to enable efficient\nlong-context modeling by learning hierarchical latent representations that\npreserve global semantics while aggressively reducing input redundancy. CCF\nintegrates segment-wise semantic aggregation with key-value memory encoding,\nforming compact representations that support accurate reconstruction and\nlong-range understanding. To further enhance scalability, we introduce a\ntraining-efficient optimization strategy that couples incremental segment\ndecoding with sparse reservoir sampling, substantially reducing memory overhead\nwithout degrading performance. Empirical results on multiple long-context\nlanguage modeling benchmarks demonstrate that CCF achieves competitive\nperplexity under high compression ratios, and significantly improves throughput\nand memory efficiency compared to existing approaches. These findings highlight\nthe potential of structured compression for scalable and effective long-context\nlanguage modeling.", "AI": {"tldr": "This work presents a novel context compression framework, CCF, for efficient long-context modeling in language models that reduces memory burden while maintaining performance.", "motivation": "To address the computational inefficiencies associated with extending language model contexts during training and inference.", "method": "The CCF framework utilizes hierarchical latent representations, segment-wise semantic aggregation, and key-value memory encoding for compact representation creation followed by an optimization strategy with incremental segment decoding and sparse reservoir sampling.", "result": "CCF outperforms existing methods in memory efficiency and throughput while maintaining competitive perplexity at high compression ratios across multiple long-context language modeling benchmarks.", "conclusion": "Structured compression methods like CCF show significant potential for improving the scalability and effectiveness of long-context modeling in language tasks.", "key_contributions": ["Introduction of the CCF context compression framework", "Integration of semantic aggregation with memory encoding", "A novel optimization strategy for training efficiency"], "limitations": "", "keywords": ["context compression", "long-context modeling", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583.pdf", "abs": "https://arxiv.org/abs/2509.09583", "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.", "AI": {"tldr": "This paper proposes a model that detects Big-Five personality traits from student forum posts to enhance matchmaking in online courses, facilitating better social connections among students.", "motivation": "To improve social connections in online learning environments, which are often hindered by the lack of organic group formation.", "method": "A personality detection model leveraging GPT's zero-shot capability to infer Big-Five personality traits from introductory forum posts, benchmarked against established models.", "result": "The model demonstrates efficacy in detecting personality traits, which were found to complement existing matchmaking factors within the SAMI system.", "conclusion": "Integrating personality traits into the matchmaking process may improve student engagement and match quality, warranting further evaluation of their impact.", "key_contributions": ["Development of a personality detection model using GPTs", "Benchmarking against existing personality models", "Integration into SAMI's matchmaking system for enhanced social recommendations"], "limitations": "Additional evaluation required to fully assess the impact of personality traits on engagement and match quality.", "keywords": ["personality detection", "Big-Five traits", "online learning", "social connections", "SAMI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.09229", "pdf": "https://arxiv.org/pdf/2509.09229.pdf", "abs": "https://arxiv.org/abs/2509.09229", "title": "Reading Between the Lines: Classifying Resume Seniority with Large Language Models", "authors": ["Matan Cohen", "Shira Shani", "Eden Menahem", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "5 pages, 3 figures", "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt", "AI": {"tldr": "This study evaluates large language models for classifying seniority in resumes, addressing challenges like overstated experience.", "motivation": "The task of accurately assessing candidate seniority is complicated by resume embellishment and ambiguous self-presentation, making automated classification vital.", "method": "The study uses a hybrid dataset of real-world and synthetically generated resumes to test the performance of fine-tuned BERT models in seniority classification.", "result": "The evaluation shows that large language models can effectively detect linguistic cues related to experience and expertise, improving candidate evaluation systems.", "conclusion": "AI-driven systems can enhance candidate evaluation by addressing biases from self-promotional language, and the dataset is shared for further research.", "key_contributions": ["Introduction of a hybrid dataset for seniority classification", "Evaluation of fine-tuned LLMs for resume analysis", "Insights into linguistic cues associated with seniority inflation"], "limitations": "", "keywords": ["large language models", "seniority classification", "resume analysis", "bias mitigation", "linguistic cues"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.09234", "pdf": "https://arxiv.org/pdf/2509.09234.pdf", "abs": "https://arxiv.org/abs/2509.09234", "title": "Agentic LLMs for Question Answering over Tabular Data", "authors": ["Rishit Tyagi", "Mohit Gupta", "Rahul Bouri"], "categories": ["cs.CL"], "comment": "Accepted at ACL workshop SemEval 2025", "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA.", "AI": {"tldr": "This paper describes a novel NL-to-SQL approach using large language models for Question Answering over Tabular Data, focusing on a new benchmark for evaluating model performance.", "motivation": "The paper addresses the challenges in Question Answering over Tabular Data due to varying table structures, sizes, and data types, aiming to improve accuracy in structured query responses.", "method": "The proposed method employs a multi-stage pipeline that includes example selection, SQL query generation, answer extraction, verification, and iterative refinement, utilizing large language models like GPT-4o.", "result": "The approach achieved a 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, significantly outperforming baseline scores of 26% and 27%.", "conclusion": "The results underline the potential of LLMs in enhancing Table QA tasks, offering insights into their strengths and limitations.", "key_contributions": ["Introduction of a benchmark for Table QA using diverse datasets", "Development of an effective multi-stage NL-to-SQL pipeline", "Demonstration of substantial performance improvements over baseline methods"], "limitations": "The paper discusses strengths and weaknesses of LLMs in Table QA but does not detail specific limitations of the proposed method.", "keywords": ["Table QA", "NL-to-SQL", "Large Language Models", "DataBench", "Question Answering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.09303", "pdf": "https://arxiv.org/pdf/2509.09303.pdf", "abs": "https://arxiv.org/abs/2509.09303", "title": "From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models", "authors": ["Grazia Sveva Ascione", "Nicolò Tamagnone"], "categories": ["cs.CL"], "comment": null, "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.", "AI": {"tldr": "This paper presents a method for classifying patents according to their relevance to UN Sustainable Development Goals (SDGs) using weak supervision techniques with large language models (LLMs).", "motivation": "To improve the classification of patents in relation to UN Sustainable Development Goals due to the absence of large labeled datasets for supervised learning.", "method": "The authors develop a composite labeling function leveraging citations from SDG-tagged papers as a noisy input signal. They extract concepts from patents and SDG papers using LLMs, then compute similarity scores and calibrate these with a custom positive-only loss for better alignment with known links.", "result": "The method produces a silver-standard multi-label dataset for patents related to SDGs, and it outperforms existing models in both internal and external validation strategies.", "conclusion": "Weak supervision combined with semantic alignment significantly enhances SDG classification scalability and effectiveness.", "key_contributions": ["Proposed a novel approach to patent classification using weak supervision and LLMs.", "Developed a composite labeling function that extracts relevant concepts from patents and SDG documents.", "Validated the method through internal and external strategies, demonstrating superior performance compared to traditional classification methods."], "limitations": "", "keywords": ["patent classification", "Sustainable Development Goals", "weak supervision", "large language models", "multi-label regression"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492.pdf", "abs": "https://arxiv.org/abs/2503.18492", "title": "VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interacting with mobile GUIs. These agents\nallow users to automate complex mobile tasks through simple natural language\ninstructions. However, the inherent probabilistic nature of LFMs, coupled with\nthe ambiguity and context-dependence of mobile tasks, makes LFM-based\nautomation unreliable and prone to errors. To address this critical challenge,\nwe introduce VeriSafe Agent (VSA): a formal verification system that serves as\na logically grounded safeguard for Mobile GUI Agents. VSA deterministically\nensures that an agent's actions strictly align with user intent before\nexecuting the action. At its core, VSA introduces a novel autoformalization\ntechnique that translates natural language user instructions into a formally\nverifiable specification. This enables runtime, rule-based verification of\nagent's actions, detecting erroneous actions even before they take effect. To\nthe best of our knowledge, VSA is the first attempt to bring the rigor of\nformal verification to GUI agents, bridging the gap between LFM-driven actions\nand formal software verification. We implement VSA using off-the-shelf LFM\nservices (GPT-4o) and evaluate its performance on 300 user instructions across\n18 widely used mobile apps. The results demonstrate that VSA achieves\n94.33%-98.33% accuracy in verifying agent actions, outperforming existing\nLFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's\ntask completion rate by 90%-130%.", "AI": {"tldr": "The paper presents VeriSafe Agent (VSA), a formal verification system for Mobile GUI Agents that ensures actions align with user intent and reduces automation errors.", "motivation": "To address the unreliability and errors in automation by Large Foundation Models (LFMs) in mobile GUI tasks.", "method": "VSA employs a novel autoformalization technique to convert natural language instructions into verifiable specifications, allowing for runtime verification of agent actions.", "result": "VSA achieves 94.33%-98.33% accuracy in verifying actions and significantly improves the task completion rate of GUI agents by 90%-130%.", "conclusion": "VSA is a pioneering effort in applying formal verification to GUI agents and shows substantial improvement over existing methods.", "key_contributions": ["Introduction of a formal verification system for Mobile GUI Agents.", "Novel autoformalization technique for translating user instructions into verifiable specifications.", "Demonstrated significant performance improvements over existing LFM-based verification methods."], "limitations": "", "keywords": ["Human-Computer Interaction", "Formal Verification", "Mobile GUI Agents"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09360", "pdf": "https://arxiv.org/pdf/2509.09360.pdf", "abs": "https://arxiv.org/abs/2509.09360", "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems", "authors": ["Channdeth Sok", "David Luz", "Yacine Haddam"], "categories": ["cs.CL"], "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.", "AI": {"tldr": "MetaRAG is a framework for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems, enabling reliable deployment in enterprise applications.", "motivation": "LLMs are often unreliable due to hallucinations, and existing methods do not address challenges specific to RAG systems.", "method": "MetaRAG analyzes responses by breaking them into factoids, mutating them, verifying against retrieved evidence, and calculating a hallucination score.", "result": "MetaRAG effectively detects hallucinations, providing safeguards for identity-sensitive queries in RAG-based systems based on experiments with a proprietary dataset.", "conclusion": "The proposed framework allows for better deployment of conversational agents while addressing the risks of hallucinations in LLMs.", "key_contributions": ["Introduces MetaRAG, a framework specifically for RAG systems", "Localizes unsupported claims to specific factoid spans", "Allows for the configuration of identity-aware safeguards based on hallucination scores."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "hallucination detection", "MetaRAG", "identity-aware AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09381", "pdf": "https://arxiv.org/pdf/2509.09381.pdf", "abs": "https://arxiv.org/abs/2509.09381", "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research", "authors": ["Molly R Petersen", "Claire E Stevenson", "Lonneke van der Plas"], "categories": ["cs.CL"], "comment": null, "summary": "Analogical reasoning is an essential aspect of human cognition. In this\npaper, we summarize key theory about the processes underlying analogical\nreasoning from the cognitive science literature and relate it to current\nresearch in natural language processing. While these processes can be easily\nlinked to concepts in NLP, they are generally not viewed through a cognitive\nlens. Furthermore, we show how these notions are relevant for several major\nchallenges in NLP research, not directly related to analogy solving. This may\nguide researchers to better optimize relational understanding in text, as\nopposed to relying heavily on entity-level similarity.", "AI": {"tldr": "The paper explores the connection between analogical reasoning and natural language processing (NLP), highlighting its relevance in addressing various NLP challenges.", "motivation": "To bridge the gap between cognitive science and NLP by applying analogical reasoning theory to enhance relational understanding in text processing.", "method": "The paper reviews cognitive science literature on analogical reasoning and relates it to NLP challenges, emphasizing the importance of relational understanding over entity-level similarity.", "result": "Identified key cognitive processes related to analogical reasoning that can inform NLP research and improve natural language understanding.", "conclusion": "Cognitive theories of analogical reasoning can offer valuable insights for optimizing NLP systems, suggesting a shift in focus from entity-level to relational approaches.", "key_contributions": ["Linking cognitive science with NLP", "Identifying analogical reasoning processes applicable to NLP", "Promoting relational understanding in text processing"], "limitations": "", "keywords": ["analogical reasoning", "natural language processing", "cognitive science", "relational understanding", "NLP challenges"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.09388", "pdf": "https://arxiv.org/pdf/2509.09388.pdf", "abs": "https://arxiv.org/abs/2509.09388", "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs", "authors": ["Ana Ezquerro", "Carlos Gómez-Rodríguez", "David Vilares"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (main)", "summary": "We revisit hierarchical bracketing encodings from a practical perspective in\nthe context of dependency graph parsing. The approach encodes graphs as\nsequences, enabling linear-time parsing with $n$ tagging actions, and still\nrepresenting reentrancies, cycles, and empty nodes. Compared to existing graph\nlinearizations, this representation substantially reduces the label space while\npreserving structural information. We evaluate it on a multilingual and\nmulti-formalism benchmark, showing competitive results and consistent\nimprovements over other methods in exact match accuracy.", "AI": {"tldr": "This paper introduces a new method for encoding dependency graphs as sequences to achieve efficient linear-time parsing, preserving structural information while reducing the label space.", "motivation": "To improve the efficiency and accuracy of dependency graph parsing methods.", "method": "Encoding dependency graphs as sequences to allow for linear-time parsing with a reduced label space, while handling reentrancies, cycles, and empty nodes.", "result": "Achieved competitive results on a multilingual and multi-formalism benchmark, with consistent improvements in exact match accuracy compared to existing methods.", "conclusion": "The proposed encoding method successfully enhances dependency graph parsing efficiency and accuracy.", "key_contributions": ["Introduction of a novel encoding method for dependency graphs", "Linear-time parsing with reduced label space", "Evaluation and competitive results on multilingual benchmarks."], "limitations": "", "keywords": ["dependency parsing", "graph encoding", "linearization"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2501.02348", "pdf": "https://arxiv.org/pdf/2501.02348.pdf", "abs": "https://arxiv.org/abs/2501.02348", "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving", "authors": ["Sanghyun Park", "Boris Maciejovsky", "Phanish Puranam"], "categories": ["cs.CL", "cs.HC"], "comment": "36 pages, 1 appendix", "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution.", "AI": {"tldr": "Synthetic deliberation is a method using LLMs to simulate discourse between diverse perspectives to enhance complex problem-solving.", "motivation": "To address limitations of mental simulation in complex problem-solving by introducing a method that allows for multiple viewpoints to be processed concurrently.", "method": "Synthetic deliberation utilizes a custom GPT-based model to simulate discourse between agents representing diverse perspectives, enabling users to explore viewpoints without cognitive degradation.", "result": "The method shows improved processing of multiple viewpoints, parallel exploration of perspectives, and precise control over viewpoint synthesis, ultimately promoting effective deliberation.", "conclusion": "Synthetic deliberation offers a novel approach to cognitive flexibility that enhances strategic planning, policymaking, and conflict resolution.", "key_contributions": ["Introduction of synthetic deliberation using LLMs for cognitive flexibility", "Demonstrated benefits of concurrent processing of multiple perspectives", "Externalizes the deliberative process, enhancing decision-making capabilities"], "limitations": "", "keywords": ["Cognitive Flexibility", "LLM", "Synthetic Deliberation", "Complex Problem-Solving", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2509.09438", "pdf": "https://arxiv.org/pdf/2509.09438.pdf", "abs": "https://arxiv.org/abs/2509.09438", "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models", "authors": ["Zhaohan Zhang", "Ziquan Liu", "Ioannis Patras"], "categories": ["cs.CL"], "comment": "20 pages, 11 figures", "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.", "AI": {"tldr": "GrACE is a new method for reliable confidence elicitation in LLMs, enhancing performance in high-stakes fields like healthcare and finance by improving calibration without extra computational overhead.", "motivation": "Existing confidence elicitation methods for LLMs are either computationally expensive or poorly calibrated, making them unsuitable for real-world applications.", "method": "GrACE uses a novel mechanism where confidence is derived from the similarity between the last hidden state of the model and a special token's embedding, allowing real-time calibration with accuracy-based targets.", "result": "GrACE shows improved discriminative capacity and calibration in confidence estimation over three LLMs tested on two benchmark datasets, surpassing six existing methods without needing additional sampling or auxiliary models.", "conclusion": "GrACE enables practical deployment of LLMs with scalable and reliable confidence estimation, enhancing accuracy and reducing sample requirements in high-stakes environments.", "key_contributions": ["Introduces a novel mechanism for confidence elicitation based on hidden states and token embeddings.", "Demonstrates superior calibration and discriminative capacity compared to existing methods.", "Proposes strategies for improving test-time scaling based on confidence."], "limitations": "", "keywords": ["Large Language Models", "Confidence Elicitation", "Calibration", "Machine Learning", "AI Safety"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.09473", "pdf": "https://arxiv.org/pdf/2509.09473.pdf", "abs": "https://arxiv.org/abs/2509.09473", "title": "Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation", "authors": ["Lucie Poláková", "Martin Popel", "Věra Kloudová", "Michal Novák", "Mariia Anisimova", "Jiří Balhar"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures", "summary": "The EdUKate project combines digital education, linguistics, translation\nstudies, and machine translation to develop multilingual learning materials for\nCzech primary and secondary schools. Launched through collaboration between a\nmajor Czech academic institution and the country's largest educational\npublisher, the project is aimed at translating up to 9,000 multimodal\ninteractive exercises from Czech into Ukrainian, English, and German for an\neducational web portal. It emphasizes the development and evaluation of a\ndirect Czech-Ukrainian machine translation system tailored to the educational\ndomain, with special attention to processing formatted content such as XML and\nPDF and handling technical and scientific terminology. We present findings from\nan initial survey of Czech teachers regarding the needs of non-Czech-speaking\nstudents and describe the system's evaluation and implementation on the web\nportal. All resulting applications are freely available to students, educators,\nand researchers.", "AI": {"tldr": "The EdUKate project develops multilingual educational materials and a Czech-Ukrainian machine translation system for Czech schools.", "motivation": "To enhance digital education through multilingual resources and support non-Czech-speaking students in Czech primary and secondary education.", "method": "Collaboration between a Czech academic institution and an educational publisher to create and evaluate machine translation systems for educational content.", "result": "Development of up to 9,000 multimodal interactive exercises translated into Ukrainian, English, and German, with a focus on handling technical and scientific terminology.", "conclusion": "The project results in freely available educational applications that support multilingual teaching and learning in Czech schools.", "key_contributions": ["Creation of a direct Czech-Ukrainian machine translation system for education.", "Translation of a large volume of interactive educational material.", "Focus on processing formatted content and specific terminology for educational contexts."], "limitations": "The survey and findings are based on initial observations and may require further validation and refinement in future studies.", "keywords": ["machine translation", "multilingual education", "Czech-Ukrainian translation", "educational technology", "linguistics"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2509.09522", "pdf": "https://arxiv.org/pdf/2509.09522.pdf", "abs": "https://arxiv.org/abs/2509.09522", "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs", "authors": ["Vadim Zadykian", "Bruno Andrade", "Haithem Afli"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential.", "AI": {"tldr": "This study introduces a self-supervised hybrid architecture for Semantic Textual Relatedness (STR) in job title matching, enhancing explainability and model performance using domain-specific Knowledge Graphs.", "motivation": "Improving resume recommendation systems due to the limitations of lexical similarity in job title matching.", "method": "A self-supervised hybrid architecture combining dense sentence embeddings with domain-specific Knowledge Graphs (KGs), emphasizing data stratification by partitioning STR scores.", "result": "Fine-tuned SBERT models augmented with KGs showed a 25% RMSE reduction in the high-STR region compared to strong baselines.", "conclusion": "The combination of KGs with text embeddings enhances model performance and provides critical insights through regional performance analysis.", "key_contributions": ["Self-supervised hybrid architecture incorporating Knowledge Graphs", "Stratified evaluation of STR scores for granular performance analysis", "Demonstration of significant RMSE improvements in high-STR evaluations"], "limitations": "", "keywords": ["Semantic Textual Relatedness", "Knowledge Graphs", "Job Title Matching", "Embedding Models", "Human Resources"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.09524", "pdf": "https://arxiv.org/pdf/2509.09524.pdf", "abs": "https://arxiv.org/abs/2509.09524", "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning", "authors": ["Daniil Ignatev", "Nan Li", "Hugh Mee Wong", "Anh Dang", "Shane Kaszefski Yaschuk"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures; to appear at NLPerspectives@EMNLP-2025", "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.", "AI": {"tldr": "This paper discusses the DeMeVa team's experimentation with in-context learning and label distribution learning for predicting annotator-specific annotations in a shared task.", "motivation": "To address the challenges of predicting annotator-specific annotations in the Learning with Disagreements shared task by exploring two innovative approaches.", "method": "We compare example sampling strategies in in-context learning with large language models and evaluate fine-tuning methods in label distribution learning using RoBERTa.", "result": "In-context learning effectively predicts annotator-specific annotations, and aggregating these into soft labels demonstrates competitive performance. Label distribution learning shows promise for soft label predictions.", "conclusion": "ILC and LDL methods are beneficial for predicting soft labels, calling for more research in this area by the perspectivist community.", "key_contributions": ["Demonstrated effectiveness of ICL in predicting perspectivist annotations.", "Showed that aggregating ICL predictions into soft labels yields competitive results.", "Highlighted the potential of LDL methods for soft label predictions."], "limitations": "", "keywords": ["in-context learning", "label distribution learning", "large language models", "soft label predictions"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2509.09544", "pdf": "https://arxiv.org/pdf/2509.09544.pdf", "abs": "https://arxiv.org/abs/2509.09544", "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)", "authors": ["Paolo Pedinotti", "Peter Baumann", "Nathan Jessurun", "Leslie Barrett", "Enrico Santus"], "categories": ["cs.CL"], "comment": "7 pages, 6 appendices, EMNLP industry track", "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.", "AI": {"tldr": "The paper introduces MetaGraph, a methodology for extracting and analyzing knowledge graphs from financial NLP literature, highlighting research trends and methodologies using LLM advancements.", "motivation": "To provide a structured analysis of the rapidly evolving landscape of financial NLP enabled by LLMs, which traditional surveys have failed to keep pace with.", "method": "MetaGraph employs an LLM-based extraction pipeline to analyze 681 scientific papers published between 2022 and 2025 to construct a knowledge graph and identify trends.", "result": "The analysis reveals three key phases in financial NLP: early LLM adoption, critical evaluation of LLM limitations, and integration of related techniques into systems.", "conclusion": "MetaGraph offers a clear, structured view of financial NLP evolution and denotes a methodology applicable to mapping scientific progress across different domains.", "key_contributions": ["Development of MetaGraph for knowledge graph extraction in financial NLP.", "Identification of key phases in LLM adoption in financial NLP research.", "Provision of a reusable approach for analyzing research trends in various fields."], "limitations": "Focus primarily on financial NLP may limit generalizability to other domains without adaptations.", "keywords": ["Large Language Models", "Financial NLP", "Knowledge Graphs", "Research Trends", "MetaGraph"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583.pdf", "abs": "https://arxiv.org/abs/2509.09583", "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.", "AI": {"tldr": "The paper presents a personality detection model using GPTs to enhance social matchmaking in online courses by inferring personality traits from student posts.", "motivation": "To improve student connections in online learning environments through personality detection and enhance the SAMI matchmaking system.", "method": "Utilized GPT's zero-shot capability to infer Big-Five personality traits from students' forum introduction posts and benchmarked performance against established models.", "result": "Demonstrated the efficacy of the personality detection model in accurately inferring traits, which were integrated into SAMI's matchmaking system, showing promise in improving social connections.", "conclusion": "Initial integration suggests personality traits can enhance matchmaking factors, needing further evaluation to assess their impact on engagement and match quality.", "key_contributions": ["Introduction of a GPT-based personality detection model", "Integration of personality traits into an entity-based matchmaking system", "Benchmarking performance against established models for personality inference"], "limitations": "Additional evaluation is necessary to fully understand the impact of personality traits on engagement and match quality.", "keywords": ["personality detection", "social matchmaking", "online courses"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09593", "pdf": "https://arxiv.org/pdf/2509.09593.pdf", "abs": "https://arxiv.org/abs/2509.09593", "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models", "authors": ["Bangzhao Shu", "Isha Joshi", "Melissa Karnaze", "Anh C. Pham", "Ishita Kakkar", "Sindhu Kothe", "Arpine Hovasapian", "Mai ElSherief"], "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally", "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.", "AI": {"tldr": "This paper introduces EXPRESS, a benchmark dataset for evaluating the fine-grained alignment of Large Language Models (LLMs) with human emotions, revealing limitations in LLMs' emotional understanding.", "motivation": "Evaluating LLMs' capabilities in emotion recognition at a nuanced level, addressing the gap in existing research which often oversimplifies emotion categories.", "method": "A benchmark dataset, EXPRESS, is created from Reddit communities with 251 fine-grained emotion labels; LLMs are systematically tested across various prompt settings to assess prediction accuracy and alignment with human emotions.", "result": "Systematic testing indicates that while LLMs generate emotion terms aligned with established theories, predicting human self-disclosed emotions remains challenging due to contextual cue misalignment.", "conclusion": "The study highlights LLM limitations in fine-grained emotional context understanding and suggests directions for future improvements in this area.", "key_contributions": ["Introduction of the EXPRESS benchmark dataset for fine-grained emotion evaluation", "Assessment framework for comparing LLMs' emotional predictions", "Highlighting LLMs' limitations in contextual emotional understanding"], "limitations": "LLMs may fail to capture contextual cues effectively compared to human self-disclosures.", "keywords": ["Large Language Models", "Emotion Recognition", "Mental Health", "Benchmark Dataset", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09602", "pdf": "https://arxiv.org/pdf/2509.09602.pdf", "abs": "https://arxiv.org/abs/2509.09602", "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.", "AI": {"tldr": "The study evaluates the use of Large Language Models for improving verbal autopsy accuracy in estimating causes of death in resource-limited settings.", "motivation": "The paper addresses the challenge of estimating causes of death where medical certification is lacking, particularly in low-resource settings.", "method": "A proof-of-concept pipeline named LA-VA was developed, integrating LLMs with traditional algorithms and embedding-based classification using the PHMRC dataset.", "result": "GPT-5 outperforms traditional machine learning methods, achieving the highest accuracies in cause-of-death predictions across different age categories.", "conclusion": "The research indicates that LLM-assisted methods can significantly enhance the accuracy of verbal autopsy, impacting global health surveillance positively.", "key_contributions": ["Introduction of LA-VA pipeline combining LLMs and traditional methods", "Demonstrated improved prediction accuracy using GPT-5", "Provided insights for enhancing global health surveillance in low-resource contexts."], "limitations": "", "keywords": ["verbal autopsy", "Large Language Models", "health informatics", "cause-of-death prediction", "low-resource settings"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09629", "pdf": "https://arxiv.org/pdf/2509.09629.pdf", "abs": "https://arxiv.org/abs/2509.09629", "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems", "authors": ["Minghang Zhu", "Zhengliang Shi", "Zhiwei Xu", "Shiguang Wu", "Lingjie Wang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.", "AI": {"tldr": "MOAT is a framework for improving collaboration in multi-agent systems by iteratively aligning planning and grounding agents, resulting in better task performance.", "motivation": "Existing methods for tuning multi-agent systems independently lead to capability gaps and poor coordination among agents.", "method": "The MOAT framework employs an iterative alignment process with two stages: optimizing the planning agent for subgoal generation and fine-tuning the grounding agent using diverse subgoal-action pairs.", "result": "MOAT achieves significant performance improvements over state-of-the-art methods, with average increases of 3.1% on held-in tasks and 4.4% on held-out tasks across six benchmarks.", "conclusion": "Theoretical analysis confirms MOAT's effectiveness in creating a progressively convergent training process that enhances agent collaboration.", "key_contributions": ["Introduction of the MOAT framework for multi-agent alignment", "Demonstrated iterative alignment process for improving agent cooperation", "Empirical results showing superior performance compared to existing methods"], "limitations": "", "keywords": ["multi-agent systems", "alignment", "subgoal", "grounding agent", "machine learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.09650", "pdf": "https://arxiv.org/pdf/2509.09650.pdf", "abs": "https://arxiv.org/abs/2509.09650", "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens", "authors": ["Siddarth Mamidanna", "Daking Rai", "Ziyu Yao", "Yilun Zhou"], "categories": ["cs.CL", "I.2.7"], "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.", "AI": {"tldr": "This paper analyzes the inner workings of large language models (LLMs) in performing mental math tasks, introducing techniques to isolate computations at specific layers and identifying a subgraph critical for performance.", "motivation": "The motivation is to understand how LLMs execute computational tasks, specifically mental math, and to investigate the extent of token processing and information flow within the models.", "method": "The approach involves inhibiting token computations in initial layers, restricting information transfer routes in subsequent layers, and enforcing computations at the last token in later layers. Two techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), are applied to identify an efficient subgraph for performance: the All-for-One subgraph (AF1).", "result": "The experiments reveal that the AF1 subgraph is sufficient and necessary for achieving high accuracy on mental math tasks, showing cross-model applicability and effectiveness across different input styles.", "conclusion": "The study concludes that meaningful computations occur late in the model layers, specifically at the last token, and highlights the advantages of the proposed techniques in isolating computational strategies within LLMs.", "key_contributions": ["Introduction of Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP) techniques.", "Identification of the All-for-One subgraph (AF1) for mental math tasks.", "Demonstration of the subgraph's necessity and sufficiency for model performance across tasks."], "limitations": "", "keywords": ["large language models", "mental math", "token computation", "information transfer", "CAMA", "ABP"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.09660", "pdf": "https://arxiv.org/pdf/2509.09660.pdf", "abs": "https://arxiv.org/abs/2509.09660", "title": "Steering MoE LLMs via Expert (De)Activation", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Ryan Rossi", "Trung Bui", "Hinrich Schütze", "Nanyun Peng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.", "AI": {"tldr": "SteerMoE framework enables controlled steering of Mixture-of-Experts in LLMs to enhance performance metrics like safety and faithfulness without retraining.", "motivation": "The need to improve the behavior of LLMs without the overhead of retraining, particularly in relation to safety and faithfulness, led to the development of SteerMoE.", "method": "The framework detects which experts have distinct activation patterns based on paired inputs showing differing behaviors, allowing for selective (de)activation during inference.", "result": "SteerMoE shows an increase in safety by up to +20% and faithfulness by +27% across 11 benchmarks and 6 LLMs, though it exposes vulnerabilities when used in adversarial modes.", "conclusion": "SteerMoE provides a new way to control expert behavior in LLMs, revealing hidden risks while enhancing certain performance aspects.", "key_contributions": ["Development of SteerMoE for controlling expert behavior in LLMs", "Demonstration of significant improvements in safety and faithfulness metrics", "Identification of vulnerabilities in safety mechanisms related to expert activation patterns"], "limitations": "The method opens up new vulnerabilities in adversarial contexts and relies on the existing framework of LLMs without addressing deeper alignment issues.", "keywords": ["Mixture-of-Experts", "Large Language Models", "behavior control", "safety", "faithfulness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09675", "pdf": "https://arxiv.org/pdf/2509.09675.pdf", "abs": "https://arxiv.org/abs/2509.09675", "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models", "authors": ["Runpeng Dai", "Linfeng Song", "Haolin Liu", "Zhenwen Liang", "Dian Yu", "Haitao Mi", "Zhaopeng Tu", "Rui Liu", "Tong Zheng", "Hongtu Zhu", "Dong Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.", "AI": {"tldr": "This paper proposes Curiosity-Driven Exploration (CDE) to enhance Reinforcement Learning with Verifiable Rewards (RLVR) in Large Language Models by improving exploration and addressing issues like premature convergence and entropy collapse.", "motivation": "Current RLVR methods for LLMs struggle with exploration, leading to premature convergence. The motivation is to improve exploration through a novel framework that employs intrinsic curiosity.", "method": "The paper introduces Curiosity-Driven Exploration (CDE), formalizing curiosity using perplexity for the actor and variance of value estimates for the critic, which serve as exploration bonuses.", "result": "Empirical results show a +3 point improvement over standard RLVR methodologies on AIME benchmarks due to enhanced exploration.", "conclusion": "CDE effectively counteracts calibration collapse in RLVR, promoting better performance in LLMs and providing insights into common failure modes.", "key_contributions": ["Introduction of Curiosity-Driven Exploration (CDE) framework.", "Formalization of curiosity signals to guide exploration in RLVR.", "Empirical improvement on AIME benchmarks, highlighting exploration's impact on performance."], "limitations": "The calibration collapse mechanism mentioned needs further investigation to fully understand its implications in various scenarios.", "keywords": ["Reinforcement Learning", "Large Language Models", "Curiosity-Driven Exploration", "AIME benchmarks", "Exploration bonuses"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.09447", "pdf": "https://arxiv.org/pdf/2407.09447.pdf", "abs": "https://arxiv.org/abs/2407.09447", "title": "ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts", "authors": ["Amelia F. Hardy", "Houjun Liu", "Allie Griffith", "Bernard Lange", "Duncan Eddy", "Mykel J. Kochenderfer"], "categories": ["cs.CL"], "comment": "8 pages, 7 pages of appendix, 3 tables, 4 figures", "summary": "Existing LLM red-teaming approaches prioritize high attack success rate,\noften resulting in high-perplexity prompts. This focus overlooks low-perplexity\nattacks that are more difficult to filter, more likely to arise during benign\nusage, and more impactful as negative downstream training examples. In\nresponse, we introduce ASTPrompter, a single-step optimization method that uses\ncontrastive preference learning to train an attacker to maintain low perplexity\nwhile achieving a high attack success rate (ASR). ASTPrompter achieves an\nattack success rate 5.1 times higher on Llama-8.1B while using inputs that are\n2.1 times more likely to occur according to the frozen LLM. Furthermore, our\nattack transfers to Mistral-7B, Qwen-7B, and TinyLlama in both black- and\nwhite-box settings. Lastly, by tuning a single hyperparameter in our method, we\ndiscover successful attack prefixes along an efficient frontier between ASR and\nperplexity, highlighting perplexity as a previously under-considered factor in\nred-teaming.", "AI": {"tldr": "This paper introduces ASTPrompter, a method for optimizing low-perplexity attacks in LLM red-teaming, achieving a significantly higher attack success rate.", "motivation": "To address the oversight in existing LLM red-teaming methods which prioritize high attack success rates at the expense of low-perplexity prompts, thereby increasing the chance of evasion during benign use.", "method": "ASTPrompter employs contrastive preference learning for a single-step optimization to balance high attack success rates with low perplexity prompting.", "result": "ASTPrompter achieves an attack success rate 5.1 times higher on Llama-8.1B, using inputs 2.1 times more likely to occur according to the frozen LLM, and it transfers successfully to other models.", "conclusion": "The study emphasizes the importance of perplexity in LLM red-teaming, presenting an efficient frontier for optimizing attack success and perplexity through a single hyperparameter tuning.", "key_contributions": ["Introduction of ASTPrompter for low-perplexity attack optimization in LLM red-teaming", "Demonstrating a significant increase in attack success rate with lower perplexity", "Highlighting the under-considered role of perplexity in red-teaming methods."], "limitations": "", "keywords": ["LLM red-teaming", "low perplexity prompts", "contrastive preference learning", "attack success rate", "perplexity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2411.08302", "pdf": "https://arxiv.org/pdf/2411.08302.pdf", "abs": "https://arxiv.org/abs/2411.08302", "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution", "authors": ["Jiahui Li", "Lin Li", "Tai-wei Chang", "Kun Kuang", "Long Chen", "Jun Zhou", "Cheng Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach.", "AI": {"tldr": "This paper introduces RED, a novel fine-grained reward redistribution method for reinforcement learning from human feedback (RLHF) that enhances token-level understanding in large language models without modifying existing reward models.", "motivation": "Current reward models in RLHF inadequately evaluate individual tokens' contributions by providing a single reward for an entire sequence, potentially missing key nuances in model performance.", "method": "The proposed RED method utilizes an existing reward model to assign specific credit to each token in the output sequence, allowing for a more nuanced approach in the evaluation of language generation.", "result": "Experimental results show that RED significantly improves language model performance across various datasets and tasks, outperforming traditional sequence-to-one reward models.", "conclusion": "The RED method enhances the effectiveness of LLMs in aligning with human preferences while keeping computational costs low and avoiding additional training requirements.", "key_contributions": ["Introduction of a fine-grained token-level reward redistribution method in RLHF.", "Demonstration of improved model performance through enhanced understanding of language nuances.", "Minimal computational costs while utilizing off-the-shelf reward models."], "limitations": "", "keywords": ["reinforcement learning", "human feedback", "large language models", "reward redistribution", "token-level evaluation"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2412.11538", "pdf": "https://arxiv.org/pdf/2412.11538.pdf", "abs": "https://arxiv.org/abs/2412.11538", "title": "MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond", "authors": ["Muhammad Huzaifah", "Geyu Lin", "Tianchi Liu", "Hardik B. Sailor", "Kye Min Tan", "Tarun K. Vangani", "Qiongqiong Wang", "Jeremy H. M. Wong", "Jinyang Wu", "Nancy F. Chen", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This technical report describes the MERaLiON-SpeechEncoder, a foundation\nmodel designed to support a wide range of downstream speech applications.\nDeveloped as part of Singapore's National Multimodal Large Language Model\nProgramme, the MERaLiON-SpeechEncoder is tailored to address the speech\nprocessing needs in Singapore and the surrounding Southeast Asian region. The\nmodel currently supports mainly English, including the variety spoken in\nSingapore. We are actively expanding our datasets to gradually cover other\nlanguages in subsequent releases. The MERaLiON-SpeechEncoder was pre-trained\nfrom scratch on 200,000 hours of unlabelled speech data using a self-supervised\nlearning approach based on masked language modelling. We describe our training\nprocedure and hyperparameter tuning experiments in detail below. Our evaluation\ndemonstrates improvements to spontaneous and Singapore speech benchmarks for\nspeech recognition, while remaining competitive to other state-of-the-art\nspeech encoders across ten other speech tasks. We commit to releasing our\nmodel, supporting broader research endeavours, both in Singapore and beyond.", "AI": {"tldr": "The MERaLiON-SpeechEncoder is a foundation model for speech applications developed for Singapore, focusing on English and eventually expanding to other languages.", "motivation": "To create a speech processing foundation model tailored to the needs of Singapore and the Southeast Asian region.", "method": "The model was pre-trained on 200,000 hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling, with detailed training procedures and hyperparameter tuning.", "result": "Improvements in performance on spontaneous and Singapore speech benchmarks for speech recognition, and competitive performance across ten other speech tasks.", "conclusion": "The model will be released for broader research use, supporting efforts beyond Singapore.", "key_contributions": ["Development of a tailored speech encoder for Singapore and Southeast Asia.", "Self-supervised pre-training approach on a large dataset of unlabelled speech data.", "Evaluation benchmarks demonstrating significant improvements in speech tasks."], "limitations": "", "keywords": ["speech processing", "foundation model", "self-supervised learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.02348", "pdf": "https://arxiv.org/pdf/2501.02348.pdf", "abs": "https://arxiv.org/abs/2501.02348", "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving", "authors": ["Sanghyun Park", "Boris Maciejovsky", "Phanish Puranam"], "categories": ["cs.CL", "cs.HC"], "comment": "36 pages, 1 appendix", "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution.", "AI": {"tldr": "The paper introduces synthetic deliberation, a method leveraging LLMs to simulate discourse among agents with diverse perspectives, enhancing cognitive flexibility and addressing limitations of mental simulation in complex problem-solving.", "motivation": "The need for improved cognitive flexibility in complex problem-solving is addressed, particularly the limitations of mental simulation.", "method": "The authors propose synthetic deliberation using a custom GPT-based model to facilitate discourse among various perspectives, enabling concurrent processing and viewpoint synthesis.", "result": "The method allows for parallel exploration of multiple viewpoints, overcoming cognitive constraints and enhancing decision-making across diverse applications.", "conclusion": "Synthetic deliberation can effectively aid in strategic planning, policymaking, and conflict resolution by distributing cognitive labor and enhancing cognitive flexibility.", "key_contributions": ["Introduction of synthetic deliberation as a new method", "Demonstration of enhanced cognitive flexibility using LLMs", "Applications of this method in strategic planning and policymaking"], "limitations": "", "keywords": ["Cognitive flexibility", "Synthetic deliberation", "Large Language Models", "Problem-solving", "Discourse simulation"], "importance_score": 8, "read_time_minutes": 36}}
{"id": "2502.01523", "pdf": "https://arxiv.org/pdf/2502.01523.pdf", "abs": "https://arxiv.org/abs/2502.01523", "title": "CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering", "authors": ["Zongxi Li", "Yang Li", "Haoran Xie", "S. Joe Qin"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 (Main Conference)", "summary": "Users often assume that large language models (LLMs) share their cognitive\nalignment of context and intent, leading them to omit critical information in\nquestion-answering (QA) and produce ambiguous queries. Responses based on\nmisaligned assumptions may be perceived as hallucinations. Therefore,\nidentifying possible implicit assumptions is crucial in QA. To address this\nfundamental challenge, we propose Conditional Ambiguous Question-Answering\n(CondAmbigQA), a benchmark comprising 2,000 ambiguous queries and\ncondition-aware evaluation metrics. Our study pioneers \"conditions\" as explicit\ncontextual constraints that resolve ambiguities in QA tasks through\nretrieval-based annotation, where retrieved Wikipedia fragments help identify\npossible interpretations for a given query and annotate answers accordingly.\nExperiments demonstrate that models considering conditions before answering\nimprove answer accuracy by 11.75%, with an additional 7.15% gain when\nconditions are explicitly provided. These results highlight that apparent\nhallucinations may stem from inherent query ambiguity rather than model\nfailure, and demonstrate the effectiveness of condition reasoning in QA,\nproviding researchers with tools for rigorous evaluation.", "AI": {"tldr": "The paper proposes CondAmbigQA, a benchmark to improve question-answering (QA) accuracy by addressing implicit assumptions users make about large language models (LLMs).", "motivation": "Users often make assumptions about LLMs' understanding of context and intent, leading to ambiguous queries and perceived hallucinations in QA responses. Addressing these assumptions is essential for improving QA accuracy.", "method": "The paper introduces Conditional Ambiguous Question-Answering (CondAmbigQA), a benchmark featuring 2,000 ambiguous queries and condition-aware evaluation metrics. It utilizes retrieval-based annotation with Wikipedia fragments to clarify query interpretations.", "result": "Models considering contextual conditions before answering showed an 11.75% improvement in accuracy, with an additional 7.15% increase when conditions were explicitly provided.", "conclusion": "The research indicates that many perceived hallucinations in LLMs may be due to query ambiguities, and it establishes the importance of condition reasoning in enhancing QA performance.", "key_contributions": ["Introduction of CondAmbigQA benchmark", "Use of retrieval-based annotation for condition-aware evaluation", "Demonstration of significant accuracy improvements in QA by considering conditions"], "limitations": "", "keywords": ["Conditional Ambiguous QA", "Question-Answering", "Large Language Models", "Evaluation Metrics", "Contextual Conditions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.02787", "pdf": "https://arxiv.org/pdf/2502.02787.pdf", "abs": "https://arxiv.org/abs/2502.02787", "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models", "authors": ["Amirhossein Dabiriaghdam", "Lele Wang"], "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "comment": "Accepted to EMNLP 25 main", "summary": "The widespread adoption of large language models (LLMs) necessitates reliable\nmethods to detect LLM-generated text. We introduce SimMark, a robust\nsentence-level watermarking algorithm that makes LLMs' outputs traceable\nwithout requiring access to model internals, making it compatible with both\nopen and API-based LLMs. By leveraging the similarity of semantic sentence\nembeddings combined with rejection sampling to embed detectable statistical\npatterns imperceptible to humans, and employing a soft counting mechanism,\nSimMark achieves robustness against paraphrasing attacks. Experimental results\ndemonstrate that SimMark sets a new benchmark for robust watermarking of\nLLM-generated content, surpassing prior sentence-level watermarking techniques\nin robustness, sampling efficiency, and applicability across diverse domains,\nall while maintaining the text quality and fluency.", "AI": {"tldr": "SimMark is a new sentence-level watermarking algorithm for detecting LLM-generated text, achieving high robustness and quality in text.", "motivation": "With the growing use of large language models, there is a pressing need for methods to reliably detect text generated by these models.", "method": "SimMark employs semantic sentence embeddings and rejection sampling to create detectable patterns that remain undetectable to humans.", "result": "SimMark outperforms existing techniques in robustness, sampling efficiency, and versatility across different domains, while preserving text quality.", "conclusion": "SimMark establishes a new standard for watermarking LLM-generated content, useful for both open and API-based models.", "key_contributions": ["Introduction of SimMark for sentence-level watermarking", "Robust against paraphrasing attacks", "Sets a new benchmark in watermarking efficiency and applicability."], "limitations": "", "keywords": ["large language models", "watermarking", "semantic embeddings", "text detection", "natural language processing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.11115", "pdf": "https://arxiv.org/pdf/2502.11115.pdf", "abs": "https://arxiv.org/abs/2502.11115", "title": "Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability", "authors": ["Tu Anh Dinh", "Jan Niehues"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Quality Estimation (QE) is estimating quality of the model output during\ninference when the ground truth is not available. Deriving output quality from\nthe models' output probability is the most trivial and low-effort way. However,\nwe show that the output probability of text-generation models can appear\nunderconfident. At each output step, there can be multiple correct options,\nmaking the probability distribution spread out more. Thus, lower probability\ndoes not necessarily mean lower output quality. Due to this observation, we\npropose a QE approach called BoostedProb, which boosts the model's confidence\nin cases where there are multiple viable output options. With no increase in\ncomplexity, BoostedProb is notably better than raw model probability in\ndifferent settings, achieving on average +0.194 improvement in Pearson\ncorrelation to ground-truth quality. It also comes close to or outperforms more\ncostly approaches like supervised or ensemble-based QE in certain settings.", "AI": {"tldr": "BoostedProb is a novel Quality Estimation method that improves text-generation model output assessment by boosting confidence in outputs with multiple viable options, outperforming traditional probability measures with no added complexity.", "motivation": "To address the issue of underconfidence in the output probabilities of text-generation models when estimating quality without ground truth.", "method": "The proposed method BoostedProb enhances the model's confidence in cases where multiple correct options exist, improving the assessment of output quality.", "result": "BoostedProb achieves an average improvement of +0.194 in Pearson correlation to ground-truth quality compared to raw model probability, and it rivals more complex QE approaches.", "conclusion": "BoostedProb offers a more reliable quality estimation for text-generated outputs without increasing complexity, making it a viable alternative to traditional methods.", "key_contributions": ["Introduction of BoostedProb for Quality Estimation", "Significant improvement in correlation with ground-truth quality", "Comparison against more complex QE methods showing reduced effort and cost"], "limitations": "", "keywords": ["Quality Estimation", "Text Generation", "Machine Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.12932", "pdf": "https://arxiv.org/pdf/2502.12932.pdf", "abs": "https://arxiv.org/abs/2502.12932", "title": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese", "authors": ["Salsabila Zahirah Pranida", "Rifo Ahmad Genadi", "Fajri Koto"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work.", "AI": {"tldr": "This paper explores using large language models to generate culturally grounded narratives in low-resource languages, specifically Javanese and Sundanese, comparing various data generation strategies.", "motivation": "To address the challenge of scarce data and costly native annotation for culturally grounded commonsense reasoning in low-resource languages.", "method": "The authors test three data creation strategies: LLM-assisted story generation with cultural cues, machine translation from Indonesian, and stories written by natives. They conduct human evaluations and fine-tune models on the datasets.", "result": "LLM-generated stories achieve higher cultural fidelity compared to machine-translated narratives, although they are less coherent and correct. The fine-tuned LLMs show improved downstream performance over the other data sources.", "conclusion": "The study demonstrates the potential of LLMs to create culturally relevant content in low-resource settings and introduces a benchmark dataset for future research.", "key_contributions": ["Demonstrated the effectiveness of LLMs in low-resource language story generation.", "Provided a comparison of data generation strategies for culturally grounded reasoning.", "Released a benchmark of Javanese and Sundanese commonsense stories."], "limitations": "LLM stories were found to be less coherent and correct compared to native-written stories despite matching in cultural fidelity.", "keywords": ["Commonsense reasoning", "Culturally grounded narratives", "Low-resource languages", "Large language models", "Benchmark dataset"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.19860", "pdf": "https://arxiv.org/pdf/2502.19860.pdf", "abs": "https://arxiv.org/abs/2502.19860", "title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue", "authors": ["Yujia Chen", "Changsong Li", "Yiming Wang", "Tianjie Ju", "Qingqing Xiao", "Nan Zhang", "Zifan Kong", "Peng Wang", "Binyu Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing.", "AI": {"tldr": "The paper introduces MIND, a novel framework utilizing LLMs for immersive psychological healing through interactive dialogues.", "motivation": "To address the increasing mental health issues and the inadequacies of traditional therapeutic methods, particularly their lack of emotional engagement.", "method": "MIND uses a predefined interactive healing framework where LLM agents role-play to create more human-like and emotionally aware dialogues with users.", "result": "Human experiments reveal that MIND offers a substantially more user-friendly experience compared to traditional therapy methods.", "conclusion": "MIND highlights the potential of leveraging LLM capabilities for enhanced psychological healing, improving engagement and emotional interaction.", "key_contributions": ["Introduction of the MIND framework for interactive healing", "Implementation of LLMs in role-playing for emotional engagement", "Demonstration of improved user experience in mental health applications"], "limitations": "", "keywords": ["Mental Health", "Large Language Models", "Interactive Healing", "User Experience", "Psychological Therapy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.21961", "pdf": "https://arxiv.org/pdf/2503.21961.pdf", "abs": "https://arxiv.org/abs/2503.21961", "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning", "authors": ["Xianzhi Li", "Ethan Callanan", "Abdellah Ghassel", "Xiaodan Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities.", "AI": {"tldr": "The paper introduces Entropy-Gated Branching, an inference technique that improves LLM reasoning efficiency by selectively expanding predictions at points of high uncertainty, resulting in higher accuracy and faster operation compared to standard methods.", "motivation": "To enhance the reasoning capabilities of large language models while reducing wasted computational resources during the inference process.", "method": "Entropy-Gated Branching dynamically allocates compute resources by branching only at points of high uncertainty and using a feedback mechanism to rank and prune branches.", "result": "Empirical tests show a 22.6% accuracy improvement over standard inference methods and a 37% increase in speed compared to conventional beam search.", "conclusion": "Dynamic allocation of resources during inference can lead to significant improvements in both the efficiency and effectiveness of LLMs.", "key_contributions": ["Introduction of Entropy-Gated Branching technique", "Improved accuracy on reasoning tasks", "Faster inference times compared to traditional methods"], "limitations": "", "keywords": ["Inference", "Large Language Models", "Entropy-Gated Branching", "Dynamic Resource Allocation", "Reasoning Efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485.pdf", "abs": "https://arxiv.org/abs/2506.06485", "title": "Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": "Major revision", "summary": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.", "AI": {"tldr": "This study investigates how Large Language Models (LLMs) process conflicting information from contextual knowledge and parametric memory during various tasks, revealing significant influences on performance and the implications for LLM evaluation.", "motivation": "To understand how LLMs handle conflicts between contextual and parametric knowledge, particularly in tasks requiring different types and amounts of information.", "method": "A model-agnostic diagnostic framework was developed to detect disagreements between model beliefs and a curated knowledge set while injecting controlled conflicts into tasks. The study evaluates open-source LLMs across different dimensions of task knowledge reliance and conflict plausibility.", "result": "Performance degradation from knowledge conflict was found to correlate with the task's reliance on knowledge. Providing explanatory rationales tends to enhance reliance on context in context-only tasks but can hinder performance in tasks that should favor parametric knowledge.", "conclusion": "The findings emphasize the need for careful consideration of knowledge conflicts in LLM evaluation to ensure validity in model-based assessments, especially for diverse task requirements.", "key_contributions": ["Development of a diagnostic framework for detecting knowledge conflicts in LLMs", "Insights into the impact of task knowledge reliance on LLM performance under conflict", "Highlighting the implications of conflict on model evaluation validity"], "limitations": "The study's scope may be limited to specific task types and may not generalize to all applications of LLMs.", "keywords": ["Large Language Models", "knowledge conflict", "task evaluation", "Machine Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492.pdf", "abs": "https://arxiv.org/abs/2503.18492", "title": "VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interacting with mobile GUIs. These agents\nallow users to automate complex mobile tasks through simple natural language\ninstructions. However, the inherent probabilistic nature of LFMs, coupled with\nthe ambiguity and context-dependence of mobile tasks, makes LFM-based\nautomation unreliable and prone to errors. To address this critical challenge,\nwe introduce VeriSafe Agent (VSA): a formal verification system that serves as\na logically grounded safeguard for Mobile GUI Agents. VSA deterministically\nensures that an agent's actions strictly align with user intent before\nexecuting the action. At its core, VSA introduces a novel autoformalization\ntechnique that translates natural language user instructions into a formally\nverifiable specification. This enables runtime, rule-based verification of\nagent's actions, detecting erroneous actions even before they take effect. To\nthe best of our knowledge, VSA is the first attempt to bring the rigor of\nformal verification to GUI agents, bridging the gap between LFM-driven actions\nand formal software verification. We implement VSA using off-the-shelf LFM\nservices (GPT-4o) and evaluate its performance on 300 user instructions across\n18 widely used mobile apps. The results demonstrate that VSA achieves\n94.33%-98.33% accuracy in verifying agent actions, outperforming existing\nLFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's\ntask completion rate by 90%-130%.", "AI": {"tldr": "The paper introduces VeriSafe Agent (VSA), a formal verification system designed to enhance the reliability of Mobile GUI Agents that utilize Large Foundation Models for task automation by ensuring actions align with user intent.", "motivation": "To address the unreliability and errors in Mobile GUI Agents caused by the probabilistic nature of Large Foundation Models and the ambiguity of natural language instructions.", "method": "VSA introduces a novel autoformalization technique to translate natural language user instructions into a formally verifiable specification, allowing for runtime rule-based verification of agent actions.", "result": "VSA demonstrates a 94.33%-98.33% accuracy in verifying agent actions across 300 user instructions, significantly outperforming existing methods and increasing task completion rates by 90%-130%.", "conclusion": "VSA is the first formal verification system for GUI agents, successfully bridging the gap between LFM-driven actions and formal software verification.", "key_contributions": ["Introduction of a formal verification system for Mobile GUI Agents", "Novel autoformalization technique for translating natural language instructions", "Significant performance improvements in task verification and completion rates"], "limitations": "", "keywords": ["Large Foundation Models", "Mobile GUI Agents", "Formal Verification"], "importance_score": 9, "read_time_minutes": 15}}
