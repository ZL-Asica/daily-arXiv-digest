{"id": "2506.11276", "pdf": "https://arxiv.org/pdf/2506.11276.pdf", "abs": "https://arxiv.org/abs/2506.11276", "title": "Needling Through the Threads: A Visualization Tool for Navigating Threaded Online Discussions", "authors": ["Yijun Liu", "Frederick Choi", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": null, "summary": "Navigating large-scale online discussions is difficult due to the rapid pace\nand large volume of user-generated content. Prior work in CSCW has shown that\nmoderators often struggle to follow multiple simultaneous discussions, track\nevolving conversations, and maintain contextual understanding--all of which\nhinder timely and effective moderation. While platforms like Reddit use\nthreaded structures to organize discourse, deeply nested threads can still\nobscure discussions and make it difficult to grasp the overall trajectory of\nconversations. In this paper, we present an interactive system called Needle to\nsupport better navigation and comprehension of complex discourse within\nthreaded discussions. Needle uses visual analytics to summarize key\nconversational metrics--such as activity, toxicity levels, and voting\ntrends--over time, offering both high-level insights and detailed breakdowns of\ndiscussion threads. Through a user study with ten Reddit moderators, we find\nthat Needle supports moderation by reducing cognitive load in making sense of\nlarge discussion, helping prioritize areas that need attention, and providing\ndecision-making supports. Based on our findings, we provide a set of design\nguidelines to inform future visualization-driven moderation tools and\nsociotechnical systems. To the best of our knowledge, Needle is one of the\nfirst systems to combine interactive visual analytics with human-in-the-loop\nmoderation for threaded online discussions."}
{"id": "2506.11326", "pdf": "https://arxiv.org/pdf/2506.11326.pdf", "abs": "https://arxiv.org/abs/2506.11326", "title": "Combining Log Data and Collaborative Dialogue Features to Predict Project Quality in Middle School AI Education", "authors": ["Conrad Borchers", "Xiaoyi Tian", "Kristy Elizabeth Boyer", "Maya Israel"], "categories": ["cs.HC"], "comment": "Research paper accepted to the 9th Educational Data Mining in\n  Computer Science Education (CSEDM) Workshop", "summary": "Project-based learning plays a crucial role in computing education. However,\nits open-ended nature makes tracking project development and assessing success\nchallenging. We investigate how dialogue and system interaction logs predict\nproject quality during collaborative, project-based AI learning of 94 middle\nschool students working in pairs. We used linguistic features from dialogue\ntranscripts and behavioral features from system logs to predict three project\nquality outcomes: productivity (number of training phrases), content richness\n(word density), and lexical variation (word diversity) of chatbot training\nphrases. We compared the predictive accuracy of each modality and a fusion of\nthe modalities. Results indicate log data better predicts productivity, while\ndialogue data is more effective for content richness. Both modalities modestly\npredict lexical variation. Multimodal fusion improved predictions for\nproductivity and lexical variation of training phrases but not content\nrichness. These findings suggest that the value of multimodal fusion depends on\nthe specific learning outcome. The study contributes to multimodal learning\nanalytics by demonstrating the nuanced interplay between behavioral and\nlinguistic data in assessing student learning progress in open-ended AI\nlearning environments."}
{"id": "2506.11366", "pdf": "https://arxiv.org/pdf/2506.11366.pdf", "abs": "https://arxiv.org/abs/2506.11366", "title": "Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities", "authors": ["Alemitu Bezabih", "Shadi Nourriz", "Anne-Marie Snider", "Rosalie Rauenzahn", "C. Estelle Smith"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Despite a growing need for spiritual care in the US, it is often\nunder-served, inaccessible, or misunderstood, while almost no prior work in\nCSCW/HCI research has engaged with professional chaplains and spiritual care\nproviders. This interdisciplinary study aims to develop a foundational\nunderstanding of how spiritual care may (or may not) be expanded into online\nspaces -- especially focusing on anonymous, asynchronous, and text-based online\ncommunities. We conducted an exploratory mixed-methods study with chaplains\n(N=22) involving interviews and user testing sessions centered around Reddit\nsupport communities to understand participants' perspectives on technology and\ntheir ideations about the role of chaplaincy in prospective Online Spiritual\nCare Communities (OSCCs). Our Grounded Theory Method analysis highlighted\nbenefits of OSCCs including: meeting patients where they are at; accessibility\nand scalability; and facilitating patient-initiated care. Chaplains highlighted\nhow their presence in OSCCs could help with shaping peer interactions,\nmoderation, synchronous chats for group care, and redirecting to external\nresources, while also raising important feasibility concerns, risks, and needs\nfor future design and research. We used an existing taxonomy of chaplaincy\ntechniques to show that some spiritual care strategies may be amenable to\nonline spaces, yet we also exposed the limitations of technology to fully\nmediate spiritual care and the need to develop new online chaplaincy\ninterventions. Based on these findings, we contribute the model of a ``Care\nLoop'' between institutionally-based formal care and platform-based community\ncare to expand access and drive greater awareness and utilization of spiritual\ncare. We also contribute design implications to guide future work in online\nspiritual care."}
{"id": "2506.11393", "pdf": "https://arxiv.org/pdf/2506.11393.pdf", "abs": "https://arxiv.org/abs/2506.11393", "title": "Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections", "authors": ["Sandro Radovanović", "Shuangyu Li"], "categories": ["cs.HC", "cs.CY"], "comment": "19 pages, 7 figures", "summary": "Clinical communication skills are essential for preparing healthcare\nprofessionals to provide equitable care across cultures. However, traditional\ntraining with simulated patients can be resource intensive and difficult to\nscale, especially in under-resourced settings. In this project, we explore the\nuse of an AI-driven chatbot to support culturally competent communication\ntraining for medical students. The chatbot was designed to simulate realistic\npatient conversations and provide structured feedback based on the ACT Cultural\nCompetence model. We piloted the chatbot with a small group of third-year\nmedical students at a UK medical school in 2024. Although we did not follow a\nformal experimental design, our experience suggests that the chatbot offered\nuseful opportunities for students to reflect on their communication,\nparticularly around empathy and interpersonal understanding. More challenging\nareas included addressing systemic issues and historical context. Although this\nearly version of the chatbot helped surface some interesting patterns,\nlimitations were also clear, such as the absence of nonverbal cues and the\ntendency for virtual patients to be overly agreeable. In general, this\nreflection highlights both the potential and the current limitations of AI\ntools in communication training. More work is needed to better understand their\nimpact and improve the learning experience."}
{"id": "2506.11017", "pdf": "https://arxiv.org/pdf/2506.11017.pdf", "abs": "https://arxiv.org/abs/2506.11017", "title": "TeleEval-OS: Performance evaluations of large language models for operations scheduling", "authors": ["Yanyan Wang", "Yingying Wang", "Junli Liang", "Yin Xu", "Yunlong Liu", "Yiming Xu", "Zhengwang Jiang", "Zhehe Li", "Fei Li", "Long Zhao", "Kuang Xu", "Qi Song", "Xiangyang Li"], "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in artificial intelligence, demonstrating substantial\napplication potential across multiple specialized domains. Telecommunications\noperation scheduling (OS) is a critical aspect of the telecommunications\nindustry, involving the coordinated management of networks, services, risks,\nand human resources to optimize production scheduling and ensure unified\nservice control. However, the inherent complexity and domain-specific nature of\nOS tasks, coupled with the absence of comprehensive evaluation benchmarks, have\nhindered thorough exploration of LLMs' application potential in this critical\nfield. To address this research gap, we propose the first Telecommunications\nOperation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this\nbenchmark comprises 15 datasets across 13 subtasks, comprehensively simulating\nfour key operational stages: intelligent ticket creation, intelligent ticket\nhandling, intelligent ticket closure, and intelligent evaluation. To\nsystematically assess the performance of LLMs on tasks of varying complexity,\nwe categorize their capabilities in telecommunications operation scheduling\ninto four hierarchical levels, arranged in ascending order of difficulty: basic\nNLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we\nleverage zero-shot and few-shot evaluation methods to comprehensively assess 10\nopen-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)\nacross diverse scenarios. Experimental results demonstrate that open-source\nLLMs can outperform closed-source LLMs in specific scenarios, highlighting\ntheir significant potential and value in the field of telecommunications\noperation scheduling."}
{"id": "2506.11536", "pdf": "https://arxiv.org/pdf/2506.11536.pdf", "abs": "https://arxiv.org/abs/2506.11536", "title": "Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance", "authors": ["Daniel Zielasko", "Ben Rehling", "Bernadette von Dawans", "Gregor Domes"], "categories": ["cs.HC"], "comment": null, "summary": "Extended exposure to virtual reality environments can induce motion sickness,\noften referred to as cybersickness, which may lead to physiological stress\nresponses and impaired cognitive performance. This study investigates the\naftereffects of VR-induced motion sickness with a focus on physiological stress\nmarkers and working memory performance. Using a carousel simulation to elicit\ncybersickness, we assessed subjective discomfort (SSQ, FMS), physiological\nstress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate),\nand cognitive performance (n-Back task) over a 90-minute post-exposure period.\nOur findings demonstrate a significant increase in both subjective and\nphysiological stress indicators following VR exposure, accompanied by a decline\nin working memory performance. Notably, delayed symptom progression was\nobserved in a substantial proportion of participants, with some reporting peak\nsymptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained\nelevated throughout the observation period, indicating prolonged stress\nrecovery. These results highlight the need for longer washout phases in XR\nresearch and raise safety concerns for professional applications involving\npost-exposure task performance."}
{"id": "2506.11063", "pdf": "https://arxiv.org/pdf/2506.11063.pdf", "abs": "https://arxiv.org/abs/2506.11063", "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Yuyao Ge", "Zhecheng Li", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential\nin knowledge-intensive and open-domain tasks. As retrieval complexity\nincreases, ensuring the robustness of these systems is critical. However,\ncurrent RAG models are highly sensitive to the order in which evidence is\npresented, often resulting in unstable performance and biased reasoning,\nparticularly as the number of retrieved items or modality diversity grows. This\nraises a central question: How does the position of retrieved evidence affect\nmultimodal RAG performance? To answer this, we present the first comprehensive\nstudy of position bias in multimodal RAG systems. Through controlled\nexperiments across text-only, image-only, and mixed-modality tasks, we observe\na consistent U-shaped accuracy curve with respect to evidence position. To\nquantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and\ndevelop a visualization framework to trace attention allocation patterns across\ndecoder layers. Our results reveal that multimodal interactions intensify\nposition bias compared to unimodal settings, and that this bias increases\nlogarithmically with retrieval range. These findings offer both theoretical and\nempirical foundations for position-aware analysis in RAG, highlighting the need\nfor evidence reordering or debiasing strategies to build more reliable and\nequitable generation systems."}
{"id": "2506.11610", "pdf": "https://arxiv.org/pdf/2506.11610.pdf", "abs": "https://arxiv.org/abs/2506.11610", "title": "\"If we misunderstand the client, we misspend 100 hours\": Exploring conversational AI and response types for information elicitation", "authors": ["Daniel Hove Paludan", "Julie Fredsgård", "Kasper Patrick Bährentz", "Ilhan Aslan"], "categories": ["cs.HC"], "comment": "27 pages, 8 figures", "summary": "Client-designer alignment is crucial to the success of design projects, yet\nlittle research has explored how digital technologies might influence this\nalignment. To address this gap, this paper presents a three-phase study\ninvestigating how digital systems can support requirements elicitation in\nprofessional design practice. Specifically, it examines how integrating a\nconversational agent and choice-based response formats into a digital\nelicitation tool affects early-stage client-designer collaboration. The first\nphase of the study inquired into the current practices of 10 design companies\nthrough semi-structured interviews, informing the system's design. The second\nphase evaluated the system using a 2x2 factorial design with 50 mock clients,\nquantifying the effects of conversational AI and response type on user\nexperience and perceived preparedness. In phase three, the system was presented\nto seven of the original 10 companies to gather reflections on its value,\nlimitations, and potential integration into practice. Findings show that both\nconversational AI and choice-based responses lead to lower dependability scores\non the User Experience Questionnaire, yet result in client input with greater\nclarity. We contribute design implications for integrating conversational AI\nand choice-based responses into elicitation tools to support mutual\nunderstanding in early-stage client-designer collaboration."}
{"id": "2506.11065", "pdf": "https://arxiv.org/pdf/2506.11065.pdf", "abs": "https://arxiv.org/abs/2506.11065", "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study", "authors": ["Alexey Tikhonov", "Sergei Shteiner", "Anna Bykova", "Ivan P. Yamshchikov"], "categories": ["cs.CL", "Primary 68T50, Secondary 68T05, 91F20", "I.2.7; I.2.6; I.5.4"], "comment": "ACL Findings 2025", "summary": "Russenorsk, a pidgin language historically used in trade interactions between\nRussian and Norwegian speakers, represents a unique linguistic phenomenon. In\nthis paper, we attempt to analyze its lexicon using modern large language\nmodels (LLMs), based on surviving literary sources. We construct a structured\ndictionary of the language, grouped by synonyms and word origins. Subsequently,\nwe use this dictionary to formulate hypotheses about the core principles of\nword formation and grammatical structure in Russenorsk and show which\nhypotheses generated by large language models correspond to the hypotheses\npreviously proposed ones in the academic literature. We also develop a\n\"reconstruction\" translation agent that generates hypothetical Russenorsk\nrenderings of contemporary Russian and Norwegian texts."}
{"id": "2506.11665", "pdf": "https://arxiv.org/pdf/2506.11665.pdf", "abs": "https://arxiv.org/abs/2506.11665", "title": "Perspectives on Explanation Formats From Two Stakeholder Groups in Germany: Software Providers and Dairy Farmers", "authors": ["Mengisti Berihu Girmay", "Felix Möhrle"], "categories": ["cs.HC"], "comment": "Accepted at IJCAI 2024, Explainable AI Workshop", "summary": "This paper examines the views of software providers in the German dairy\nindustry with regard to dairy farmers' needs for explanation of digital\ndecision support systems. The study is based on mastitis detection in dairy\ncows using a hypothetical herd management system. We designed four exemplary\nexplanation formats for mastitis assessments with different types of\npresentation (textual, rule-based, herd comparison, and time series). In our\nprevious study, 14 dairy farmers in Germany had rated these formats in terms of\ncomprehensibility and the trust they would have in a system providing each\nformat. In this study, we repeat the survey with 13 software providers active\nin the German dairy industry. We ask them how well they think the formats would\nbe received by farmers. We hypothesized that there may be discrepancies between\nthe views of both groups that are worth investigating, partly to find reasons\nfor the reluctance to adopt digital systems. A comparison of the feedback from\nboth groups supports the hypothesis and calls for further investigation. The\nresults show that software providers tend to make assumptions about farmers'\npreferences that are not necessarily accurate. Our study, although not\nrepresentative due to the small sample size, highlights the potential benefits\nof a thorough user requirements analysis (farmers' needs) to improve software\nadaptation and user acceptance."}
{"id": "2506.11067", "pdf": "https://arxiv.org/pdf/2506.11067.pdf", "abs": "https://arxiv.org/abs/2506.11067", "title": "A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes", "authors": ["Hieu Nghiem", "Hemanth Reddy Singareddy", "Zhuqi Miao", "Jivan Lamichhane", "Abdulaziz Ahmed", "Johnson Thomas", "Dursun Delen", "William Paiva"], "categories": ["cs.CL"], "comment": null, "summary": "Objective: Develop a cost-effective, large language model (LLM)-based\npipeline for automatically extracting Review of Systems (ROS) entities from\nclinical notes. Materials and Methods: The pipeline extracts ROS sections using\nSecTag, followed by few-shot LLMs to identify ROS entity spans, their\npositive/negative status, and associated body systems. We implemented the\npipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The\nevaluation was conducted on 36 general medicine notes containing 341 annotated\nROS entities. Results: When integrating ChatGPT, the pipeline achieved the\nlowest error rates in detecting ROS entity spans and their corresponding\nstatuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable\nlocal, cost-efficient execution of the pipeline while delivering promising\nperformance with similarly low error rates (span: 30.5-36.7%; status/system:\n24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and\nlocally deployable solution to reduce ROS documentation burden. Open-source\nLLMs present a viable alternative to commercial models in resource-limited\nhealthcare environments."}
{"id": "2506.11718", "pdf": "https://arxiv.org/pdf/2506.11718.pdf", "abs": "https://arxiv.org/abs/2506.11718", "title": "Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration", "authors": ["Yun Wang", "Yan Lu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI tools proliferate across domains, from chatbots and copilots to\nemerging agents, they increasingly support professional knowledge work. Yet\ndespite their growing capabilities, these systems remain fragmented: they\nassist with isolated tasks but lack the architectural scaffolding for\nsustained, adaptive collaboration. We propose a layered framework for\nhuman-agent systems that integrates three interdependent dimensions:\ninteraction, process, and infrastructure. Crucially, our architecture elevates\nprocess to a primary focus by making it explicit, inspectable, and adaptable,\nenabling humans and agents to align with evolving goals and coordinate over\ntime. This model clarifies limitations of current tools, unifies emerging\nsystem design approaches, and reveals new opportunities for researchers and AI\nsystem builders. By grounding intelligent behavior in structured collaboration,\nwe reimagine human-agent collaboration not as task-specific augmentation, but\nas a form of coherent and aligned system for real-world work."}
{"id": "2506.11068", "pdf": "https://arxiv.org/pdf/2506.11068.pdf", "abs": "https://arxiv.org/abs/2506.11068", "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models", "authors": ["Bumjin Park", "Jinsil Lee", "Jaesik Choi"], "categories": ["cs.CL"], "comment": "20 pages including references and appendix; To appear in ACL 2025\n  main conference", "summary": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment."}
{"id": "2506.11781", "pdf": "https://arxiv.org/pdf/2506.11781.pdf", "abs": "https://arxiv.org/abs/2506.11781", "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant", "authors": ["Gaspard Merten", "Gilles Dejaegere", "Mahmoud Sakr"], "categories": ["cs.HC", "cs.SE"], "comment": "Submitted to ACM SIGSPATIAL 2025", "summary": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment."}
{"id": "2506.11070", "pdf": "https://arxiv.org/pdf/2506.11070.pdf", "abs": "https://arxiv.org/abs/2506.11070", "title": "Targeted control of fast prototyping through domain-specific interface", "authors": ["Yu-Zhe Shi", "Mingchen Liu", "Hanlu Ma", "Qiao Xu", "Huamin Qu", "Kun He", "Lecheng Ruan", "Qining Wang"], "categories": ["cs.CL"], "comment": "In International Conference on Machine Learning (ICML'25)", "summary": "Industrial designers have long sought a natural and intuitive way to achieve\nthe targeted control of prototype models -- using simple natural language\ninstructions to configure and adjust the models seamlessly according to their\nintentions, without relying on complex modeling commands. While Large Language\nModels have shown promise in this area, their potential for controlling\nprototype models through language remains partially underutilized. This\nlimitation stems from gaps between designers' languages and modeling languages,\nincluding mismatch in abstraction levels, fluctuation in semantic precision,\nand divergence in lexical scopes. To bridge these gaps, we propose an interface\narchitecture that serves as a medium between the two languages. Grounded in\ndesign principles derived from a systematic investigation of fast prototyping\npractices, we devise the interface's operational mechanism and develop an\nalgorithm for its automated domain specification. Both machine-based\nevaluations and human studies on fast prototyping across various product design\ndomains demonstrate the interface's potential to function as an auxiliary\nmodule for Large Language Models, enabling precise and effective targeted\ncontrol of prototype models."}
{"id": "2506.11788", "pdf": "https://arxiv.org/pdf/2506.11788.pdf", "abs": "https://arxiv.org/abs/2506.11788", "title": "Digital Labor: Challenges, Ethical Insights, and Implications", "authors": ["ATM Mizanur Rahman", "Sharifa Sultana"], "categories": ["cs.HC"], "comment": null, "summary": "Digital workers on crowdsourcing platforms (e.g., Amazon Mechanical Turk,\nAppen, Clickworker, Prolific) play a crucial role in training and improving AI\nsystems, yet they often face low pay, unfair conditions, and a lack of\nrecognition for their contributions. To map these issues in the existing\nliterature of computer science, AI, and related scholarship, we selected over\n300 research papers on digital labor published between 2015 and 2024, narrowing\nthem down to 143 on digital gig-labor for a detailed analysis. This analysis\nprovides a broad overview of the key challenges, concerns, and trends in the\nfield. Our synthesis reveals how the persistent patterns of representation and\nvoices of gig workers in digital labor are structured and governed. We offer\nnew insights for researchers, platform designers, and policymakers, helping\nthem better understand the experiences of digital workers and pointing to key\nareas where interventions and future investigations are promptly needed. By\nmapping the findings from the past ten years' growth of the domain and possible\nimplications, this paper contributes to a more coherent and critical\nunderstanding of digital labor in contemporary and future AI ecosystems."}
{"id": "2506.11073", "pdf": "https://arxiv.org/pdf/2506.11073.pdf", "abs": "https://arxiv.org/abs/2506.11073", "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention", "authors": ["Zekai Ye", "Qiming Li", "Xiaocheng Feng", "Libo Qin", "Yichong Huang", "Baohang Li", "Kui Jiang", "Yang Xiang", "Zhirui Zhang", "Yunfei Lu", "Duyu Tang", "Dandan Tu", "Bing Qin"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL2025 Main", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nabilities but remain prone to multilingual object hallucination, with a higher\nlikelihood of generating responses inconsistent with the visual input when\nutilizing queries in non-English languages compared to English. Most existing\napproaches to address these rely on pretraining or fine-tuning, which are\nresource-intensive. In this paper, inspired by observing the disparities in\ncross-modal attention patterns across languages, we propose Cross-Lingual\nAttention Intervention for Mitigating multilingual object hallucination (CLAIM)\nin LVLMs, a novel near training-free method by aligning attention patterns.\nCLAIM first identifies language-specific cross-modal attention heads, then\nestimates language shift vectors from English to the target language, and\nfinally intervenes in the attention outputs during inference to facilitate\ncross-lingual visual perception capability alignment. Extensive experiments\ndemonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in\nSpanish) on the POPE and 21.75% on the hallucination subsets of the MME\nbenchmark across various languages. Further analysis reveals that multilingual\nattention divergence is most prominent in intermediate layers, highlighting\ntheir critical role in multilingual scenarios."}
{"id": "2506.11789", "pdf": "https://arxiv.org/pdf/2506.11789.pdf", "abs": "https://arxiv.org/abs/2506.11789", "title": "Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale Study on LLM Use in Everyday Learning", "authors": ["Nađa Terzimehić", "Babette Bühler", "Enkelejda Kasneci"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design."}
{"id": "2506.11077", "pdf": "https://arxiv.org/pdf/2506.11077.pdf", "abs": "https://arxiv.org/abs/2506.11077", "title": "CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling", "authors": ["Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Alfred Hero", "Sijia Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness\ntest-time scaling to perform multi-step reasoning for complex problem-solving.\nThis reasoning process, executed before producing final answers, is often\nguided by special juncture tokens or textual segments that prompt\nself-evaluative reflection. We refer to these transition markers and reflective\ncues as \"reflection tokens\" (e.g., \"wait\", \"but\", \"alternatively\"). In this\nwork, we treat reflection tokens as a \"resource\" and introduce the problem of\nresource allocation, aimed at improving the test-time compute performance of\nLRMs by adaptively regulating the frequency and placement of reflection tokens.\nThrough empirical analysis, we show that both excessive and insufficient use of\nreflection tokens, referred to as over-reflection and under-reflection, can\ndegrade model performance. To better understand and manage this trade-off, we\ndraw an analogy between reflection token usage and learning rate scheduling in\noptimization. Building on this insight, we propose cyclical reflection token\nscheduling (termed CyclicReflex), a decoding strategy that dynamically\nmodulates reflection token logits using a position-dependent triangular\nwaveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that\nCyclicReflex consistently improves performance across model sizes (1.5B-8B),\noutperforming standard decoding and more recent approaches such as TIP (thought\nswitching penalty) and S1. Codes are available at\nhttps://github.com/OPTML-Group/CyclicReflex."}
{"id": "2506.11890", "pdf": "https://arxiv.org/pdf/2506.11890.pdf", "abs": "https://arxiv.org/abs/2506.11890", "title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training", "authors": ["Judson Leroy Dean Haynes IV"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Virtual Reality simulators offer a powerful tool for teacher training, yet\nthe integration of AI-powered student avatars presents a critical challenge:\ndetermining the optimal level of avatar realism for effective pedagogy. This\nliterature review examines the evolution of avatar realism in VR teacher\ntraining, synthesizes its theoretical implications, and proposes a new\npedagogical framework to guide future design. Through a systematic review, this\npaper traces the progression from human-controlled avatars to generative AI\nprototypes. Applying learning theories like Cognitive Load Theory, we argue\nthat hyper-realism is not always optimal, as high-fidelity avatars can impose\nexcessive extraneous cognitive load on novices, a stance supported by recent\nempirical findings. A significant gap exists between the technological drive\nfor photorealism and the pedagogical need for scaffolded learning. To address\nthis gap, we propose Graduated Realism, a framework advocating for starting\ntrainees with lower-fidelity avatars and progressively increasing behavioral\ncomplexity as skills develop. To make this computationally feasible, we outline\na novel single-call architecture, Crazy Slots, which uses a probabilistic\nengine and a Retrieval-Augmented Generation database to generate authentic,\nreal-time responses without the latency and cost of multi-step reasoning\nmodels. This review provides evidence-based principles for designing the next\ngeneration of AI simulators, arguing that a pedagogically grounded approach to\nrealism is essential for creating scalable and effective teacher education\ntools."}
{"id": "2506.11078", "pdf": "https://arxiv.org/pdf/2506.11078.pdf", "abs": "https://arxiv.org/abs/2506.11078", "title": "RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs", "authors": ["Yuzhou Yang", "Yangming Zhou", "Zhiying Zhu", "Zhenxing Qian", "Xinpeng Zhang", "Sheng Li"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of deceptive content online necessitates robust Fake News\nDetection (FND) systems. While evidence-based approaches leverage external\nknowledge to verify claims, existing methods face critical limitations: noisy\nevidence selection, generalization bottlenecks, and unclear decision-making\nprocesses. Recent efforts to harness Large Language Models (LLMs) for FND\nintroduce new challenges, including hallucinated rationales and conclusion\nbias. To address these issues, we propose \\textbf{RoE-FND}\n(\\textbf{\\underline{R}}eason \\textbf{\\underline{o}}n\n\\textbf{\\underline{E}}xperiences FND), a framework that reframes evidence-based\nFND as a logical deduction task by synergizing LLMs with experiential learning.\nRoE-FND encompasses two stages: (1) \\textit{self-reflective knowledge\nbuilding}, where a knowledge base is curated by analyzing past reasoning\nerrors, namely the exploration stage, and (2) \\textit{dynamic criterion\nretrieval}, which synthesizes task-specific reasoning guidelines from\nhistorical cases as experiences during deployment. It further cross-checks\nrationales against internal experience through a devised dual-channel\nprocedure. Key contributions include: a case-based reasoning framework for FND\nthat addresses multiple existing challenges, a training-free approach enabling\nadaptation to evolving situations, and empirical validation of the framework's\nsuperior generalization and effectiveness over state-of-the-art methods across\nthree datasets."}
{"id": "2506.11004", "pdf": "https://arxiv.org/pdf/2506.11004.pdf", "abs": "https://arxiv.org/abs/2506.11004", "title": "Developing a Dyslexia Indicator Using Eye Tracking", "authors": ["Kevin Cogan", "Vuong M. Ngo", "Mark Roantree"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Dyslexia, affecting an estimated 10% to 20% of the global population,\nsignificantly impairs learning capabilities, highlighting the need for\ninnovative and accessible diagnostic methods. This paper investigates the\neffectiveness of eye-tracking technology combined with machine learning\nalgorithms as a cost-effective alternative for early dyslexia detection. By\nanalyzing general eye movement patterns, including prolonged fixation durations\nand erratic saccades, we proposed an enhanced solution for determining\neye-tracking-based dyslexia features. A Random Forest Classifier was then\nemployed to detect dyslexia, achieving an accuracy of 88.58\\%. Additionally,\nhierarchical clustering methods were applied to identify varying severity\nlevels of dyslexia. The analysis incorporates diverse methodologies across\nvarious populations and settings, demonstrating the potential of this\ntechnology to identify individuals with dyslexia, including those with\nborderline traits, through non-invasive means. Integrating eye-tracking with\nmachine learning represents a significant advancement in the diagnostic\nprocess, offering a highly accurate and accessible method in clinical research."}
{"id": "2506.11080", "pdf": "https://arxiv.org/pdf/2506.11080.pdf", "abs": "https://arxiv.org/abs/2506.11080", "title": "MANBench: Is Your Multimodal Model Smarter than Human?", "authors": ["Han Zhou", "Qitong Xu", "Yiheng Dong", "Xin Yang"], "categories": ["cs.CL"], "comment": "Multimodal Benchmark, Project Url: https://github.com/micdz/MANBench,\n  ACL2025 Findings", "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited\ndiscussions regarding their potential to surpass human performance in\nmultimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms\nBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314\nquestions across nine tasks, spanning knowledge-based and non-knowledge-based\ndomains. MANBench emphasizes intuitive reasoning, seamless cross-modal\nintegration, and real-world complexity, providing a rigorous evaluation\nframework.\n  Through extensive human experiments involving diverse participants, we\ncompared human performance against state-of-the-art MLLMs. The results indicate\nthat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,\nthey struggle with deeper cross-modal reasoning tasks such as Transmorphic\nUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, both\nhumans and MLLMs face challenges in highly complex tasks like Puzzles and\nSpatial Imagination.\n  MANBench highlights the strengths and limitations of MLLMs, revealing that\neven advanced models fall short of achieving human-level performance across\nmany domains. We hope MANBench will inspire efforts to bridge the gap between\nMLLMs and human multimodal capabilities. The code and dataset are available at\nhttps://github.com/micdz/MANBench."}
{"id": "2506.11015", "pdf": "https://arxiv.org/pdf/2506.11015.pdf", "abs": "https://arxiv.org/abs/2506.11015", "title": "The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI", "authors": ["Barbara Oakley", "Michael Johnston", "Ken-Zen Chen", "Eulho Jung", "Terrence J. Sejnowski"], "categories": ["cs.CY", "cs.AI", "cs.HC", "q-bio.NC"], "comment": "50 pages, 8 figures", "summary": "In the age of generative AI and ubiquitous digital tools, human cognition\nfaces a structural paradox: as external aids become more capable, internal\nmemory systems risk atrophy. Drawing on neuroscience and cognitive psychology,\nthis paper examines how heavy reliance on AI systems and discovery-based\npedagogies may impair the consolidation of declarative and procedural memory --\nsystems essential for expertise, critical thinking, and long-term retention. We\nreview how tools like ChatGPT and calculators can short-circuit the retrieval,\nerror correction, and schema-building processes necessary for robust neural\nencoding. Notably, we highlight striking parallels between deep learning\nphenomena such as \"grokking\" and the neuroscience of overlearning and\nintuition. Empirical studies are discussed showing how premature reliance on AI\nduring learning inhibits proceduralization and intuitive mastery. We argue that\neffective human-AI interaction depends on strong internal models -- biological\n\"schemata\" and neural manifolds -- that enable users to evaluate, refine, and\nguide AI output. The paper concludes with policy implications for education and\nworkforce training in the age of large language models."}
{"id": "2506.11081", "pdf": "https://arxiv.org/pdf/2506.11081.pdf", "abs": "https://arxiv.org/abs/2506.11081", "title": "SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs", "authors": ["Aditi", "Hyunwoo Park", "Sicheol Sung", "Yo-Sub Han", "Sang-Ki Ko"], "categories": ["cs.CL"], "comment": null, "summary": "Grammar-based test case generation has proven effective for competitive\nprogramming problems, but generating valid and general grammars from natural\nlanguage specifications remains a key challenge, especially under limited\nsupervision. Context-Free Grammars with Counters (CCFGs) have recently been\nintroduced as a formalism to represent such specifications with logical\nconstraints by storing and reusing counter values during derivation. In this\nwork, we explore the use of open-source large language models (LLMs) to induce\nCCFGs from specifications using a small number of labeled examples and\nverifiable reward-guided reinforcement learning. Our approach first fine-tunes\nan open-source LLM to perform specification-to-grammar translation, and further\napplies Group Relative Policy Optimization (GRPO) to enhance grammar validity\nand generality. We also examine the effectiveness of iterative feedback for\nopen and closed-source LLMs in correcting syntactic and semantic errors in\ngenerated grammars.\n  Experimental results show that our approach SAGE achieves stronger\ngeneralization and outperforms 17 open and closed-source LLMs in both grammar\nquality and test effectiveness, improving over the state-of-the-art by 15.92%p\nin grammar validity and 12.34%p in test effectiveness. We provide our\nimplementation and dataset at the following anonymous\nrepository:https://anonymous.4open.science/r/SAGE-5714"}
{"id": "2506.11047", "pdf": "https://arxiv.org/pdf/2506.11047.pdf", "abs": "https://arxiv.org/abs/2506.11047", "title": "Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment", "authors": ["Chirudeep Tupakula", "Rittika Shamsuddin"], "categories": ["cs.LG", "cs.HC"], "comment": "Pilot Study. 12 pages. 4 Figures", "summary": "Machine learning systems are increasingly deployed in high-stakes domains,\nyet they remain vulnerable to bias systematic disparities that\ndisproportionately impact specific demographic groups. Traditional bias\ndetection methods often depend on access to sensitive labels or rely on rigid\nfairness metrics, limiting their applicability in real-world settings. This\npaper introduces a novel, perception-driven framework for bias detection that\nleverages crowdsourced human judgment. Inspired by reCAPTCHA and other\ncrowd-powered systems, we present a lightweight web platform that displays\nstripped-down visualizations of numeric data (for example-salary distributions\nacross demographic clusters) and collects binary judgments on group similarity.\nWe explore how users' visual perception-shaped by layout, spacing, and question\nphrasing can signal potential disparities. User feedback is aggregated to flag\ndata segments as biased, which are then validated through statistical tests and\nmachine learning cross-evaluations. Our findings show that perceptual signals\nfrom non-expert users reliably correlate with known bias cases, suggesting that\nvisual intuition can serve as a powerful, scalable proxy for fairness auditing.\nThis approach offers a label-efficient, interpretable alternative to\nconventional fairness diagnostics, paving the way toward human-aligned,\ncrowdsourced bias detection pipelines."}
{"id": "2506.11082", "pdf": "https://arxiv.org/pdf/2506.11082.pdf", "abs": "https://arxiv.org/abs/2506.11082", "title": "PRISM: A Transformer-based Language Model of Structured Clinical Event Data", "authors": ["Lionel Levine", "John Santerre", "Alex S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 Figures, 1 Table", "summary": "We introduce PRISM (Predictive Reasoning in Sequential Medicine), a\ntransformer-based architecture designed to model the sequential progression of\nclinical decision-making processes. Unlike traditional approaches that rely on\nisolated diagnostic classification, PRISM frames clinical trajectories as\ntokenized sequences of events - including diagnostic tests, laboratory results,\nand diagnoses - and learns to predict the most probable next steps in the\npatient diagnostic journey. Leveraging a large custom clinical vocabulary and\nan autoregressive training objective, PRISM demonstrates the ability to capture\ncomplex dependencies across longitudinal patient timelines. Experimental\nresults show substantial improvements over random baselines in next-token\nprediction tasks, with generated sequences reflecting realistic diagnostic\npathways, laboratory result progressions, and clinician ordering behaviors.\nThese findings highlight the feasibility of applying generative language\nmodeling techniques to structured medical event data, enabling applications in\nclinical decision support, simulation, and education. PRISM establishes a\nfoundation for future advancements in sequence-based healthcare modeling,\nbridging the gap between machine learning architectures and real-world\ndiagnostic reasoning."}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."}
{"id": "2506.11083", "pdf": "https://arxiv.org/pdf/2506.11083.pdf", "abs": "https://arxiv.org/abs/2506.11083", "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates", "authors": ["Ali Asad", "Stephen Obadinma", "Radin Shayanfar", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "We propose RedDebate, a novel multi-agent debate framework that leverages\nadversarial argumentation among Large Language Models (LLMs) to proactively\nidentify and mitigate their own unsafe behaviours. Existing AI safety methods\noften depend heavily on costly human evaluations or isolated single-model\nassessment, both subject to scalability constraints and oversight risks.\nRedDebate instead embraces collaborative disagreement, enabling multiple LLMs\nto critically examine one another's reasoning, and systematically uncovering\nunsafe blind spots through automated red-teaming, and iteratively improve their\nresponses. We further integrate distinct types of long-term memory that retain\nlearned safety insights from debate interactions. Evaluating on established\nsafety benchmarks such as HarmBench, we demonstrate the proposed method's\neffectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when\ncombined with long-term memory modules, achieves reductions exceeding 23.5%. To\nour knowledge, RedDebate constitutes the first fully automated framework that\ncombines multi-agent debates with red-teaming to progressively enhance AI\nsafety without direct human intervention.(Github Repository:\nhttps://github.com/aliasad059/RedDebate)"}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112.pdf", "abs": "https://arxiv.org/abs/2506.11112", "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen."}
{"id": "2506.11088", "pdf": "https://arxiv.org/pdf/2506.11088.pdf", "abs": "https://arxiv.org/abs/2506.11088", "title": "Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing", "authors": ["Pengbo Wang", "Chaozhuo Li", "Chenxu Wang", "Liwen Zheng", "Litian Zhang", "Xi Zhang"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": null, "summary": "LLMs have demonstrated unprecedented capabilities in natural language\nprocessing, yet their practical deployment remains hindered by persistent\nfactuality and faithfulness hallucinations. While existing methods address\nthese hallucination types independently, they inadvertently induce performance\ntrade-offs, as interventions targeting one type often exacerbate the other.\nThrough empirical and theoretical analysis of activation space dynamics in\nLLMs, we reveal that these hallucination categories share overlapping subspaces\nwithin neural representations, presenting an opportunity for concurrent\nmitigation. To harness this insight, we propose SPACE, a unified framework that\njointly enhances factuality and faithfulness by editing shared activation\nsubspaces. SPACE establishes a geometric foundation for shared subspace\nexistence through dual-task feature modeling, then identifies and edits these\nsubspaces via a hybrid probe strategy combining spectral clustering and\nattention head saliency scoring. Experimental results across multiple benchmark\ndatasets demonstrate the superiority of our approach."}
{"id": "2506.11151", "pdf": "https://arxiv.org/pdf/2506.11151.pdf", "abs": "https://arxiv.org/abs/2506.11151", "title": "Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels", "authors": ["Jonathan Grizou", "Carlos de la Torre-Ortiz", "Tuukka Ruotsalo"], "categories": ["cs.CV", "cs.HC"], "comment": "10 pages, 4 figures, 11 appendix pages, 7 appendix figures", "summary": "We consider the problem of recovering a mental target (e.g., an image of a\nface) that a participant has in mind from paired EEG (i.e., brain responses)\nand image (i.e., perceived faces) data collected during interactive sessions\nwithout access to labeled information. The problem has been previously explored\nwith labeled data but not via self-calibration, where labeled data is\nunavailable. Here, we present the first framework and an algorithm, CURSOR,\nthat learns to recover unknown mental targets without access to labeled data or\npre-trained decoders. Our experiments on naturalistic images of faces\ndemonstrate that CURSOR can (1) predict image similarity scores that correlate\nwith human perceptual judgments without any label information, (2) use these\nscores to rank stimuli against an unknown mental target, and (3) generate new\nstimuli indistinguishable from the unknown mental target (validated via a user\nstudy, N=53)."}
{"id": "2506.11091", "pdf": "https://arxiv.org/pdf/2506.11091.pdf", "abs": "https://arxiv.org/abs/2506.11091", "title": "Customizing Speech Recognition Model with Large Language Model Feedback", "authors": ["Shaoshi Ling", "Guoli Ye"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic speech recognition (ASR) systems have achieved strong performance\non general transcription tasks. However, they continue to struggle with\nrecognizing rare named entities and adapting to domain mismatches. In contrast,\nlarge language models (LLMs), trained on massive internet-scale datasets, are\noften more effective across a wide range of domains. In this work, we propose a\nreinforcement learning based approach for unsupervised domain adaptation,\nleveraging unlabeled data to enhance transcription quality, particularly the\nnamed entities affected by domain mismatch, through feedback from a LLM. Given\ncontextual information, our framework employs a LLM as the reward model to\nscore the hypotheses from the ASR model. These scores serve as reward signals\nto fine-tune the ASR model via reinforcement learning. Our method achieves a\n21\\% improvement on entity word error rate over conventional self-training\nmethods."}
{"id": "2506.11179", "pdf": "https://arxiv.org/pdf/2506.11179.pdf", "abs": "https://arxiv.org/abs/2506.11179", "title": "Brain2Vec: A Deep Learning Framework for EEG-Based Stress Detection Using CNN-LSTM-Attention", "authors": ["Md Mynoddin", "Troyee Dev", "Rishita Chakma"], "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.NE", "q-bio.NC"], "comment": null, "summary": "Mental stress has become a pervasive factor affecting cognitive health and\noverall well-being, necessitating the development of robust, non-invasive\ndiagnostic tools. Electroencephalogram (EEG) signals provide a direct window\ninto neural activity, yet their non-stationary and high-dimensional nature\nposes significant modeling challenges. Here we introduce Brain2Vec, a new deep\nlearning tool that classifies stress states from raw EEG recordings using a\nhybrid architecture of convolutional, recurrent, and attention mechanisms. The\nmodel begins with a series of convolutional layers to capture localized spatial\ndependencies, followed by an LSTM layer to model sequential temporal patterns,\nand concludes with an attention mechanism to emphasize informative temporal\nregions. We evaluate Brain2Vec on the DEAP dataset, applying bandpass\nfiltering, z-score normalization, and epoch segmentation as part of a\ncomprehensive preprocessing pipeline. Compared to traditional CNN-LSTM\nbaselines, our proposed model achieves an AUC score of 0.68 and a validation\naccuracy of 81.25%. These findings demonstrate Brain2Vec's potential for\nintegration into wearable stress monitoring platforms and personalized\nhealthcare systems."}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."}
{"id": "2506.11212", "pdf": "https://arxiv.org/pdf/2506.11212.pdf", "abs": "https://arxiv.org/abs/2506.11212", "title": "User Perceptions and Attitudes Toward Untraceability in Messaging Platforms", "authors": ["Carla F. Griggio", "Boel Nelson", "Zefan Sramek", "Aslan Askarov"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Mainstream messaging platforms offer a variety of features designed to\nenhance user privacy, such as disappearing messages, password-protected chats,\nand end-to-end encryption (E2EE), which primarily protect message contents.\nBeyond contents, the transmission of messages generates metadata that can\nreveal who communicates with whom, when and how often. In this paper, we study\nuser perceptions of \"untraceability\", i.e., preventing third parties from\ntracing who communicates with whom, with the goal of informing the design of\nprivacy-enhancing features in messaging platforms and untraceable communication\nprotocols that depend on large anonymity sets and widespread user adoption. We\nexplore this from a broad conceptual standpoint: rather than studying mental\nmodels of a particular solution, we analyze how users reason about what\nfeatures should be incorporated by two fictitious platforms, Texty and Chatty,\nto prevent third parties from knowing who communicates with whom. Through a\nvignette-based survey with 189 participants, we found that users associate the\nconcept of untraceability with a wide range of privacy enhancing technologies,\nimplying a diverse set of threat models. Overall, the features suggested by\nparticipants show awareness of privacy threats stemming from forms of\nsurveillance and unauthorized access to message contents. Many participants\nalso associated untraceability with the notion of anonymity, but interpreted it\nas senders and receivers concealing their identity from each other rather than\nonly from third parties. We discuss the gap between users' perceptions of\nuntraceability and the threat models addressed by untraceable communication\nprotocols, as well as how different privacy attitudes point to challenges and\nopportunities for the adoption of untraceable communication tools in messaging\nplatforms."}
{"id": "2506.11094", "pdf": "https://arxiv.org/pdf/2506.11094.pdf", "abs": "https://arxiv.org/abs/2506.11094", "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs", "authors": ["Songyang Liu", "Chaozhuo Li", "Jiameng Qiu", "Xi Zhang", "Feiran Huang", "Litian Zhang", "Yiming Hei", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "21 pages, preprint", "summary": "With the rapid advancement of artificial intelligence technology, Large\nLanguage Models (LLMs) have demonstrated remarkable potential in the field of\nNatural Language Processing (NLP), including areas such as content generation,\nhuman-computer interaction, machine translation, and code generation, among\nothers. However, their widespread deployment has also raised significant safety\nconcerns. In recent years, LLM-generated content has occasionally exhibited\nunsafe elements like toxicity and bias, particularly in adversarial scenarios,\nwhich has garnered extensive attention from both academia and industry. While\nnumerous efforts have been made to evaluate the safety risks associated with\nLLMs, there remains a lack of systematic reviews summarizing these research\nendeavors. This survey aims to provide a comprehensive and systematic overview\nof recent advancements in LLMs safety evaluation, focusing on several key\naspects: (1) \"Why evaluate\" that explores the background of LLMs safety\nevaluation, how they differ from general LLMs evaluation, and the significance\nof such evaluation; (2) \"What to evaluate\" that examines and categorizes\nexisting safety evaluation tasks based on key capabilities, including\ndimensions such as toxicity, robustness, ethics, bias and fairness,\ntruthfulness, and so on; (3) \"Where to evaluate\" that summarizes the evaluation\nmetrics, datasets and benchmarks currently used in safety evaluations; (4) \"How\nto evaluate\" that reviews existing evaluation toolkit, and categorizing\nmainstream evaluation methods based on the roles of the evaluators. Finally, we\nidentify the challenges in LLMs safety evaluation and propose potential\nresearch directions to promote further advancement in this field. We emphasize\nthe importance of prioritizing LLMs safety evaluation to ensure the safe\ndeployment of these models in real-world applications."}
{"id": "2506.11376", "pdf": "https://arxiv.org/pdf/2506.11376.pdf", "abs": "https://arxiv.org/abs/2506.11376", "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning", "authors": ["Liying Wang", "Ph. D.", "Daffodil Carrington", "M. S.", "Daniil Filienko", "M. S.", "Caroline El Jazmi", "M. S.", "Serena Jinchen Xie", "M. S.", "Martine De Cock", "Ph. D.", "Sarah Iribarren", "Ph. D.", "Weichao Yuwen", "Ph. D"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Family caregivers often face substantial mental health challenges due to\ntheir multifaceted roles and limited resources. This study explored the\npotential of a large language model (LLM)-powered conversational agent to\ndeliver evidence-based mental health support for caregivers, specifically\nProblem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)\nand Behavioral Chain Analysis (BCA). A within-subject experiment was conducted\nwith 28 caregivers interacting with four LLM configurations to evaluate empathy\nand therapeutic alliance. The best-performing models incorporated Few-Shot and\nRetrieval-Augmented Generation (RAG) prompting techniques, alongside\nclinician-curated examples. The models showed improved contextual understanding\nand personalized support, as reflected by qualitative responses and\nquantitative ratings on perceived empathy and therapeutic alliances.\nParticipants valued the model's ability to validate emotions, explore\nunexpressed feelings, and provide actionable strategies. However, balancing\nthorough assessment with efficient advice delivery remains a challenge. This\nwork highlights the potential of LLMs in delivering empathetic and tailored\nsupport for family caregivers."}
{"id": "2506.11095", "pdf": "https://arxiv.org/pdf/2506.11095.pdf", "abs": "https://arxiv.org/abs/2506.11095", "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity", "authors": ["Manuel D. S. Hopp", "Vincent Labatut", "Arthur Amalvy", "Richard Dufour", "Hannah Stone", "Hayley Jach", "Kou Murayama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reader curiosity, the drive to seek information, is crucial for textual\nengagement, yet remains relatively underexplored in NLP. Building on\nLoewenstein's Information Gap Theory, we introduce a framework that models\nreader curiosity by quantifying semantic information gaps within a text's\nsemantic structure. Our approach leverages BERTopic-inspired topic modeling and\npersistent homology to analyze the evolving topology (connected components,\ncycles, voids) of a dynamic semantic network derived from text segments,\ntreating these features as proxies for information gaps. To empirically\nevaluate this pipeline, we collect reader curiosity ratings from participants\n(n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the\ntopological features from our pipeline as independent variables to predict\nthese ratings, and experimentally show that they significantly improve\ncuriosity prediction compared to a baseline model (73% vs. 30% explained\ndeviance), validating our approach. This pipeline offers a new computational\nmethod for analyzing text structure and its relation to reader engagement."}
{"id": "2506.11727", "pdf": "https://arxiv.org/pdf/2506.11727.pdf", "abs": "https://arxiv.org/abs/2506.11727", "title": "Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research", "authors": ["Bernhard Rieder", "Adrian Padilla", "Oscar Coromina"], "categories": ["cs.IR", "cs.HC", "cs.SI"], "comment": "34 pages, 2 tables and 4 figures", "summary": "This paper critically audits the search endpoint of YouTube's Data API (v3),\na common tool for academic research. Through systematic weekly searches over\nsix months using eleven queries, we identify major limitations regarding\ncompleteness, representativeness, consistency, and bias. Our findings reveal\nsubstantial differences between ranking parameters like relevance and date in\nterms of video recall and precision, with relevance often retrieving numerous\noff-topic videos. We also find severe temporal decay, as the number of findable\nvideos for a specific period dramatically decreases after just 20-60 days from\nthe publication date, potentially hampering many different research designs.\nFurthermore, search results lack consistency, with identical queries yielding\ndifferent video sets over time, compromising replicability. A case study on the\nEuropean Parliament elections highlights how these issues impact research\noutcomes. While the paper offers several mitigation strategies, it concludes\nthat the API's search function, potentially prioritizing \"freshness\" over\ncomprehensive retrieval, is not adequate for robust academic research,\nespecially concerning Digital Services Act requirements."}
{"id": "2506.11097", "pdf": "https://arxiv.org/pdf/2506.11097.pdf", "abs": "https://arxiv.org/abs/2506.11097", "title": "C-SEO Bench: Does Conversational SEO Work?", "authors": ["Haritz Puerto", "Martin Gubri", "Tommaso Green", "Seong Joon Oh", "Sangdoo Yun"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench."}
{"id": "2506.11773", "pdf": "https://arxiv.org/pdf/2506.11773.pdf", "abs": "https://arxiv.org/abs/2506.11773", "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments", "authors": ["Zikang Leng", "Megha Thukral", "Yaqi Liu", "Hrudhai Rajasekhar", "Shruthi K. Hiremath", "Thomas Plötz"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts."}
{"id": "2506.11102", "pdf": "https://arxiv.org/pdf/2506.11102.pdf", "abs": "https://arxiv.org/abs/2506.11102", "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs), such as GPT, Gemini, and\nDeepSeek, has significantly advanced natural language processing, giving rise\nto sophisticated chatbots capable of diverse language-related tasks. The\ntransition from these traditional LLM chatbots to more advanced AI agents\nrepresents a pivotal evolutionary step. However, existing evaluation frameworks\noften blur the distinctions between LLM chatbots and AI agents, leading to\nconfusion among researchers selecting appropriate benchmarks. To bridge this\ngap, this paper introduces a systematic analysis of current evaluation\napproaches, grounded in an evolutionary perspective. We provide a detailed\nanalytical framework that clearly differentiates AI agents from LLM chatbots\nalong five key aspects: complex environment, multi-source instructor, dynamic\nfeedback, multi-modal perception, and advanced capability. Further, we\ncategorize existing evaluation benchmarks based on external environments\ndriving forces, and resulting advanced internal capabilities. For each\ncategory, we delineate relevant evaluation attributes, presented\ncomprehensively in practical reference tables. Finally, we synthesize current\ntrends and outline future evaluation methodologies through four critical\nlenses: environment, agent, evaluator, and metrics. Our findings offer\nactionable guidance for researchers, facilitating the informed selection and\napplication of benchmarks in AI agent evaluation, thus fostering continued\nadvancement in this rapidly evolving research domain."}
{"id": "2506.11774", "pdf": "https://arxiv.org/pdf/2506.11774.pdf", "abs": "https://arxiv.org/abs/2506.11774", "title": "Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation", "authors": ["Abhishek Jaiswal", "Armeet Singh Luthra", "Purav Jangir", "Bhavya Garg", "Nisheeth Srivastava"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Isometric exercises appeal to individuals seeking convenience, privacy, and\nminimal dependence on equipments. However, such fitness training is often\noverdependent on unreliable digital media content instead of expert\nsupervision, introducing serious risks, including incorrect posture, injury,\nand disengagement due to lack of corrective feedback. To address these\nchallenges, we present a real-time feedback system for assessing isometric\nposes. Our contributions include the release of the largest multiclass\nisometric exercise video dataset to date, comprising over 3,600 clips across\nsix poses with correct and incorrect variations. To support robust evaluation,\nwe benchmark state-of-the-art models-including graph-based networks-on this\ndataset and introduce a novel three-part metric that captures classification\naccuracy, mistake localization, and model confidence. Our results enhance the\nfeasibility of intelligent and personalized exercise training systems for home\nworkouts. This expert-level diagnosis, delivered directly to the users, also\nexpands the potential applications of these systems to rehabilitation,\nphysiotherapy, and various other fitness disciplines that involve physical\nmotion."}
{"id": "2506.11103", "pdf": "https://arxiv.org/pdf/2506.11103.pdf", "abs": "https://arxiv.org/abs/2506.11103", "title": "You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model", "authors": ["Wenchong He", "Liqian Peng", "Zhe Jiang", "Alex Go"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Large language models (LLMs) possess a remarkable ability to perform\nin-context learning (ICL), which enables them to handle multiple downstream\ntasks simultaneously without requiring task-specific fine-tuning. Recent\nstudies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma\n7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of\nall tasks at once. However, this approach still lags behind dedicated\nfine-tuning, where a separate model is trained for each individual task.\n  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning\n(ManyICL), which significantly narrows this performance gap by extending the\nprinciples of ICL to a many-shot setting. To unlock the full potential of\nManyICL and address the inherent inefficiency of processing long sequences with\nnumerous in-context examples, we propose a novel training objective. Instead of\nsolely predicting the final answer, our approach treats every answer within the\ncontext as a supervised training target. This effectively shifts the role of\nmany-shot examples from prompts to targets for autoregressive learning. Through\nextensive experiments on diverse downstream tasks, including classification,\nsummarization, question answering, natural language inference, and math, we\ndemonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning\nand approaches the performance of dedicated fine-tuning. Furthermore, ManyICL\nsignificantly mitigates catastrophic forgetting issues observed in\nzero/few-shot fine-tuning. The code will be made publicly available upon\npublication."}
{"id": "2506.11827", "pdf": "https://arxiv.org/pdf/2506.11827.pdf", "abs": "https://arxiv.org/abs/2506.11827", "title": "Auditory-Tactile Congruence for Synthesis of Adaptive Pain Expressions in RoboPatients", "authors": ["Saitarun Nadipineni", "Chapa Sirithunge", "Yue Xie", "Fumiya Iida", "Thilina Dulantha Lalitharatne"], "categories": ["cs.RO", "cs.HC"], "comment": "17 pages, 9 figures, journal", "summary": "Misdiagnosis can lead to delayed treatments and harm. Robotic patients offer\na controlled way to train and evaluate clinicians in rare, subtle, or complex\ncases, reducing diagnostic errors. We present RoboPatient, a medical robotic\nsimulator aimed at multimodal pain synthesis based on haptic and auditory\nfeedback during palpation-based training scenarios. The robopatient functions\nas an adaptive intermediary, capable of synthesizing plausible pain expressions\nvocal and facial in response to tactile stimuli generated during palpation.\nUsing an abdominal phantom, robopatient captures and processes haptic input via\nan internal palpation-to-pain mapping model. To evaluate perceptual congruence\nbetween palpation and the corresponding auditory output, we conducted a study\ninvolving 7680 trials across 20 participants, where they evaluated pain\nintensity through sound. Results show that amplitude and pitch significantly\ninfluence agreement with the robot's pain expressions, irrespective of pain\nsounds. Stronger palpation forces elicited stronger agreement, aligning with\npsychophysical patterns. The study revealed two key dimensions: pitch and\namplitude are central to how people perceive pain sounds, with pitch being the\nmost influential cue. These acoustic features shape how well the sound matches\nthe applied force during palpation, impacting perceived realism. This approach\nlays the groundwork for high-fidelity robotic patients in clinical education\nand diagnostic simulation."}
{"id": "2506.11104", "pdf": "https://arxiv.org/pdf/2506.11104.pdf", "abs": "https://arxiv.org/abs/2506.11104", "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration", "authors": ["Hanzhi Zhang", "Heng Fan", "Kewei Sha", "Yan Huang", "Yunhe Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding is crucial for many NLP applications, yet\ntransformers struggle with efficiency due to the quadratic complexity of\nself-attention. Sparse attention methods alleviate this cost but often impose\nstatic, predefined masks, failing to capture heterogeneous attention patterns.\nThis results in suboptimal token interactions, limiting adaptability and\nretrieval accuracy in long-sequence tasks. This work introduces a dynamic\nsparse attention mechanism that assigns adaptive masks at the attention-map\nlevel, preserving heterogeneous patterns across layers and heads. Unlike\nexisting approaches, our method eliminates the need for fine-tuning and\npredefined mask structures while maintaining computational efficiency. By\nlearning context-aware attention structures, it achieves high alignment with\nfull-attention models, ensuring minimal performance degradation while reducing\nmemory and compute overhead. This approach provides a scalable alternative to\nfull attention, enabling the practical deployment of large-scale Large Language\nModels (LLMs) without sacrificing retrieval performance. DAM is available at:\nhttps://github.com/HanzhiZhang-Ulrica/DAM."}
{"id": "2506.11829", "pdf": "https://arxiv.org/pdf/2506.11829.pdf", "abs": "https://arxiv.org/abs/2506.11829", "title": "The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions", "authors": ["Ana Müller", "Anja Richert"], "categories": ["cs.RO", "cs.HC", "stat.ME"], "comment": "Accepted for presentation at the Workshop on Advancing Group\n  Understanding and Robots' Adaptive Behavior (GROUND), held at the Intelligent\n  Autonomous Systems (IAS) Conference 2025, Genoa, Italy", "summary": "This paper introduces a multimethod framework for studying spatial and social\ndynamics in real-world group-agent interactions with socially interactive\nagents. Drawing on proxemics and bonding theories, the method combines\nsubjective self-reports and objective spatial tracking. Applied in two field\nstudies in a museum (N = 187) with a robot and a virtual agent, the paper\naddresses the challenges in aligning human perception and behavior. We focus on\npresenting an open source, scalable, and field-tested toolkit for future\nstudies."}
{"id": "2506.11105", "pdf": "https://arxiv.org/pdf/2506.11105.pdf", "abs": "https://arxiv.org/abs/2506.11105", "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation", "authors": ["Uttej Kallakurik", "Edward Humes", "Rithvik Jonna", "Xiaomin Lin", "Tinoosh Mohsenin"], "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints."}
{"id": "2506.11932", "pdf": "https://arxiv.org/pdf/2506.11932.pdf", "abs": "https://arxiv.org/abs/2506.11932", "title": "Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A Comparative Study of Appearance-Based and Infrared Eye Trackers", "authors": ["Nishan Gunawardena", "Gough Yumu Lui", "Jeewani Anupama Ginige", "Bahman Javadi"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "This study evaluates a smartphone-based, deep-learning eye-tracking algorithm\nby comparing its performance against a commercial infrared-based eye tracker,\nthe Tobii Pro Nano. The aim is to investigate the feasibility of\nappearance-based gaze estimation under realistic mobile usage conditions. Key\nsensitivity factors, including age, gender, vision correction, lighting\nconditions, device type, and head position, were systematically analysed. The\nappearance-based algorithm integrates a lightweight convolutional neural\nnetwork (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to\npredict gaze coordinates from grayscale facial images. Gaze data were collected\nfrom 51 participants using dynamic visual stimuli, and accuracy was measured\nusing Euclidean distance. The deep learning model produced a mean error of\n17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy\ndifferences were small, the deep learning-based method was more sensitive to\nfactors such as lighting, vision correction, and age, with higher failure rates\nobserved under low-light conditions among participants using glasses and in\nolder age groups. Device-specific and positional factors also influenced\ntracking performance. These results highlight the potential of appearance-based\napproaches for mobile eye tracking and offer a reference framework for\nevaluating gaze estimation systems across varied usage conditions."}
{"id": "2506.11106", "pdf": "https://arxiv.org/pdf/2506.11106.pdf", "abs": "https://arxiv.org/abs/2506.11106", "title": "Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking", "authors": ["Ningyuan Li", "Junrui Liu", "Yi Shan", "Minghui Huang", "Tong Li"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Contemporary graph-based retrieval-augmented generation (RAG) methods\ntypically begin by extracting entities from user queries and then leverage\npre-constructed knowledge graphs to retrieve related relationships and\nmetadata. However, this pipeline's exclusive reliance on entity-level\nextraction can lead to the misinterpretation or omission of latent yet critical\ninformation and relations. As a result, retrieved content may be irrelevant or\ncontradictory, and essential knowledge may be excluded, exacerbating\nhallucination risks and degrading the fidelity of generated responses. To\naddress these limitations, we introduce PankRAG, a framework that combines a\nglobally aware, hierarchical query-resolution strategy with a novel\ndependency-aware reranking mechanism. PankRAG first constructs a multi-level\nresolution path that captures both parallel and sequential interdependencies\nwithin a query, guiding large language models (LLMs) through structured\nreasoning. It then applies its dependency-aware reranker to exploit the\ndependency structure among resolved sub-questions, enriching and validating\nretrieval results for subsequent sub-questions. Empirical evaluations\ndemonstrate that PankRAG consistently outperforms state-of-the-art approaches\nacross multiple benchmarks, underscoring its robustness and generalizability."}
{"id": "2506.12008", "pdf": "https://arxiv.org/pdf/2506.12008.pdf", "abs": "https://arxiv.org/abs/2506.12008", "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI", "authors": ["Olga Vechtomova", "Jeff Bos"], "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "comment": "Accepted for publication at ICCC 2025 (International Conference on\n  Computational Creativity)", "summary": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations."}
{"id": "2506.11108", "pdf": "https://arxiv.org/pdf/2506.11108.pdf", "abs": "https://arxiv.org/abs/2506.11108", "title": "History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "categories": ["cs.CL"], "comment": null, "summary": "We present CAGSR-vLLM-MTC, an extension of our Self-Supervised\nCross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the\nhigh-performance vLLM runtime, to address both multi-turn dialogue and\nchain-of-thought reasoning. Building upon our original single-turn approach, we\nfirst instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer,\nper-head cross-attention weights during generation. We then generalized our\nself-supervised reward function to accumulate attention signals over entire\nconversation histories and intermediate chain-of-thought steps. We discuss\npractical trade-offs, including an entropy-based clamping mechanism to prevent\nattention collapse on early context, and outline future directions for\nmulti-party dialogues and hierarchical reasoning."}
{"id": "2409.07406", "pdf": "https://arxiv.org/pdf/2409.07406.pdf", "abs": "https://arxiv.org/abs/2409.07406", "title": "Predicting Trust Dynamics Type Using Seven Personal Characteristics", "authors": ["Hyesun Chung", "X. Jessie Yang"], "categories": ["cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This study aims to explore the associations between individuals' trust\ndynamics in automated/autonomous technologies and their personal\ncharacteristics, and to further examine whether personal characteristics can be\nused to predict a user's trust dynamics type. We conducted a human-subject\nexperiment (N=130) in which participants performed a simulated surveillance\ntask assisted by an automated threat detector. Using a pre-experimental survey\ncovering 12 constructs and 28 dimensions, we collected data on participants'\npersonal characteristics. Based on the experimental data, we performed k-means\nclustering and identified three trust dynamics types. Subsequently, we\nconducted one-way Analyses of Variance to evaluate differences among the three\ntrust dynamics types in terms of personal characteristics, behaviors,\nperformance, and post-experimental ratings. Participants were clustered into\nthree groups, namely Bayesian decision makers, disbelievers, and oscillators.\nResults showed that the clusters differ significantly in seven personal\ncharacteristics: masculinity, positive affect, extraversion, neuroticism,\nintellect, performance expectancy, and high expectations. The disbelievers tend\nto have high neuroticism and low performance expectancy. The oscillators tend\nto have higher scores in masculinity, positive affect, extraversion, and\nintellect. We also found significant differences in behaviors, performance, and\npost-experimental ratings across the three groups. The disbelievers are the\nleast likely to blindly follow the recommendations made by the automated threat\ndetector. Based on the significant personal characteristics, we developed a\ndecision tree model to predict the trust dynamics type with an accuracy of 70%.\nThis model offers promising implications for identifying individuals whose\ntrust dynamics may deviate from a Bayesian pattern."}
{"id": "2506.11109", "pdf": "https://arxiv.org/pdf/2506.11109.pdf", "abs": "https://arxiv.org/abs/2506.11109", "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization", "authors": ["Yile Chen", "Yicheng Tao", "Yue Jiang", "Shuai Liu", "Han Yu", "Gao Cong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "The widespread adoption of location-based services has led to the generation\nof vast amounts of mobility data, providing significant opportunities to model\nuser movement dynamics within urban environments. Recent advancements have\nfocused on adapting Large Language Models (LLMs) for mobility analytics.\nHowever, existing methods face two primary limitations: inadequate semantic\nrepresentation of locations (i.e., discrete IDs) and insufficient modeling of\nmobility signals within LLMs (i.e., single templated instruction fine-tuning).\nTo address these issues, we propose QT-Mob, a novel framework that\nsignificantly enhances LLMs for mobility analytics. QT-Mob introduces a\nlocation tokenization module that learns compact, semantically rich tokens to\nrepresent locations, preserving contextual information while ensuring\ncompatibility with LLMs. Furthermore, QT-Mob incorporates a series of\ncomplementary fine-tuning objectives that align the learned tokens with the\ninternal representations in LLMs, improving the model's comprehension of\nsequential movement patterns and location semantics. The proposed QT-Mob\nframework not only enhances LLMs' ability to interpret mobility data but also\nprovides a more generalizable approach for various mobility analytics tasks.\nExperiments on three real-world dataset demonstrate the superior performance in\nboth next-location prediction and mobility recovery tasks, outperforming\nexisting deep learning and LLM-based methods."}
{"id": "2411.03295", "pdf": "https://arxiv.org/pdf/2411.03295.pdf", "abs": "https://arxiv.org/abs/2411.03295", "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments Online", "authors": ["Farhana Shahid", "Maximilian Dittgen", "Mor Naaman", "Aditya Vashistha"], "categories": ["cs.HC"], "comment": null, "summary": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online."}
{"id": "2506.11110", "pdf": "https://arxiv.org/pdf/2506.11110.pdf", "abs": "https://arxiv.org/abs/2506.11110", "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models", "authors": ["Jaeho Lee", "Atharv Chowdhary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, appendix contains 2 additional figures and 2\n  tables", "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench."}
{"id": "2502.07598", "pdf": "https://arxiv.org/pdf/2502.07598.pdf", "abs": "https://arxiv.org/abs/2502.07598", "title": "Towards spatial computing: recent advances in multimodal natural interaction for XR headsets", "authors": ["Zhimin Wang", "Maohang Rao", "Shanghua Ye", "Weitao Song", "Feng Lu"], "categories": ["cs.HC"], "comment": "28 pages, 10 figures", "summary": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing."}
{"id": "2506.11111", "pdf": "https://arxiv.org/pdf/2506.11111.pdf", "abs": "https://arxiv.org/abs/2506.11111", "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions", "authors": ["Kun Zhang", "Le Wu", "Kui Yu", "Guangyi Lv", "Dacao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity."}
{"id": "2504.09346", "pdf": "https://arxiv.org/pdf/2504.09346.pdf", "abs": "https://arxiv.org/abs/2504.09346", "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services", "authors": ["Shira Michel", "Sufi Kaur", "Sarah Elizabeth Gillespie", "Jeffrey Gleason", "Christo Wilson", "Avijit Ghosh"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This paper has been accepted to FAccT 2025", "summary": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies."}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112.pdf", "abs": "https://arxiv.org/abs/2506.11112", "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen."}
{"id": "2506.05720", "pdf": "https://arxiv.org/pdf/2506.05720.pdf", "abs": "https://arxiv.org/abs/2506.05720", "title": "A Survey of Earable Technology: Trends, Tools, and the Road Ahead", "authors": ["Changshuo Hu", "Qiang Yang", "Yang Liu", "Tobias Röddiger", "Kayla-Jade Butkow", "Mathias Ciliberto", "Adam Luke Pullin", "Jake Stuchbury-Wass", "Mahbub Hassan", "Cecilia Mascolo", "Dong Ma"], "categories": ["cs.HC"], "comment": null, "summary": "Earable devices, wearables positioned in or around the ear, are undergoing a\nrapid transformation from audio-centric accessories into multifunctional\nsystems for interaction, contextual awareness, and health monitoring. This\nevolution is driven by commercial trends emphasizing sensor integration and by\na surge of academic interest exploring novel sensing capabilities. Building on\nthe foundation established by earlier surveys, this work presents a timely and\ncomprehensive review of earable research published since 2022. We analyze over\none hundred recent studies to characterize this shifting research landscape,\nidentify emerging applications and sensing modalities, and assess progress\nrelative to prior efforts. In doing so, we address three core questions: how\nhas earable research evolved in recent years, what enabling resources are now\navailable, and what opportunities remain for future exploration. Through this\nsurvey, we aim to provide both a retrospective and forward-looking view of\nearable technology as a rapidly expanding frontier in ubiquitous computing. In\nparticular, this review reveals that over the past three years, researchers\nhave discovered a variety of novel sensing principles, developed many new\nearable sensing applications, enhanced the accuracy of existing sensing tasks,\nand created substantial new resources to advance research in the field. Based\non this, we further discuss open challenges and propose future directions for\nthe next phase of earable research."}
{"id": "2506.11113", "pdf": "https://arxiv.org/pdf/2506.11113.pdf", "abs": "https://arxiv.org/abs/2506.11113", "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks", "authors": ["Tzu-Ling Lin", "Wei-Chih Chen", "Teng-Fang Hsiao", "Hou-I Liu", "Ya-Hsin Yeh", "Yu Kai Chan", "Wen-Sheng Lien", "Po-Yen Kuo", "Philip S. Yu", "Hong-Han Shuai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Peer review is essential for maintaining academic quality, but the increasing\nvolume of submissions places a significant burden on reviewers. Large language\nmodels (LLMs) offer potential assistance in this process, yet their\nsusceptibility to textual adversarial attacks raises reliability concerns. This\npaper investigates the robustness of LLMs used as automated reviewers in the\npresence of such attacks. We focus on three key questions: (1) The\neffectiveness of LLMs in generating reviews compared to human reviewers. (2)\nThe impact of adversarial attacks on the reliability of LLM-generated reviews.\n(3) Challenges and potential mitigation strategies for LLM-based review. Our\nevaluation reveals significant vulnerabilities, as text manipulations can\ndistort LLM assessments. We offer a comprehensive evaluation of LLM performance\nin automated peer reviewing and analyze its robustness against adversarial\nattacks. Our findings emphasize the importance of addressing adversarial risks\nto ensure AI strengthens, rather than compromises, the integrity of scholarly\ncommunication."}
{"id": "2506.08303", "pdf": "https://arxiv.org/pdf/2506.08303.pdf", "abs": "https://arxiv.org/abs/2506.08303", "title": "EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation", "authors": ["Thomas M. Kwok", "Hilary HY Cheng", "Wai Tuck Chow"], "categories": ["cs.HC"], "comment": "Accepted by the Workshop on Human-Robot Contact and Manipulation\n  (HRCM 2025) at RSS Conference 2025", "summary": "In this work, we introduce HJ-Pal, a lightweight wearable haptic device that\nleverages EMG-driven honeycomb jamming to render muscle activation as\nkinesthetic feedback, enabling remote palpation for small muscle assessment in\ntelerehabilitation."}
{"id": "2506.11114", "pdf": "https://arxiv.org/pdf/2506.11114.pdf", "abs": "https://arxiv.org/abs/2506.11114", "title": "KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations", "authors": ["Junyu Liu", "Kaiqi Yan", "Tianyang Wang", "Qian Niu", "Momoko Nagai-Tanima", "Tomoki Aoyama"], "categories": ["cs.CL", "cs.AI"], "comment": "9pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have demonstrated notable\nperformance in medical licensing exams. However, comprehensive evaluation of\nLLMs across various healthcare roles, particularly in high-stakes clinical\nscenarios, remains a challenge. Existing benchmarks are typically text-based,\nEnglish-centric, and focus primarily on medicines, which limits their ability\nto assess broader healthcare knowledge and multimodal reasoning. To address\nthese gaps, we introduce KokushiMD-10, the first multimodal benchmark\nconstructed from ten Japanese national healthcare licensing exams. This\nbenchmark spans multiple fields, including Medicine, Dentistry, Nursing,\nPharmacy, and allied health professions. It contains over 11588 real exam\nquestions, incorporating clinical images and expert-annotated rationales to\nevaluate both textual and visual reasoning. We benchmark over 30\nstate-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both\ntext and image-based settings. Despite promising results, no model consistently\nmeets passing thresholds across domains, highlighting the ongoing challenges in\nmedical AI. KokushiMD-10 provides a comprehensive and linguistically grounded\nresource for evaluating and advancing reasoning-centric medical AI across\nmultilingual and multimodal clinical tasks."}
{"id": "2506.08467", "pdf": "https://arxiv.org/pdf/2506.08467.pdf", "abs": "https://arxiv.org/abs/2506.08467", "title": "Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education", "authors": ["Prakash Shukla", "Suchismita Naik", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "categories": ["cs.HC"], "comment": "8 pages, EduCHI 2025: 7th Annual Symposium on HCI Education,\n  Bloomington, IN, USA, July 2025", "summary": "The growing integration of AI tools in student design projects presents an\nunresolved challenge in HCI education: how should AI-generated content be cited\nand documented? Traditional citation frameworks -- grounded in credibility,\nretrievability, and authorship -- struggle to accommodate the dynamic and\nephemeral nature of AI outputs. In this paper, we examine how undergraduate\nstudents in a UX design course approached AI usage and citation when given the\nfreedom to integrate generative tools into their design process. Through\nqualitative analysis of 35 team projects and reflections from 175 students, we\nidentify varied citation practices ranging from formal attribution to indirect\nor absent acknowledgment. These inconsistencies reveal gaps in existing\nframeworks and raise questions about authorship, assessment, and pedagogical\ntransparency. We argue for rethinking AI citation as a reflective and\npedagogical practice; one that supports metacognitive engagement by prompting\nstudents to critically evaluate how and why they used AI throughout the design\nprocess. We propose alternative strategies -- such as AI contribution\nstatements and process-aware citation models that better align with the\niterative and reflective nature of design education. This work invites\neducators to reconsider how citation practices can support meaningful\nstudent--AI collaboration."}
{"id": "2506.11115", "pdf": "https://arxiv.org/pdf/2506.11115.pdf", "abs": "https://arxiv.org/abs/2506.11115", "title": "Incorporating Domain Knowledge into Materials Tokenization", "authors": ["Yerim Oh", "Jun-Hyung Park", "Junho Kim", "SungHo Kim", "SangKeun Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER"}
{"id": "2311.16380", "pdf": "https://arxiv.org/pdf/2311.16380.pdf", "abs": "https://arxiv.org/abs/2311.16380", "title": "Learning Multimodal Latent Dynamics for Human-Robot Interaction", "authors": ["Vignesh Prasad", "Lea Heitlinger", "Dorothea Koert", "Ruth Stock-Homburg", "Jan Peters", "Georgia Chalvatzaki"], "categories": ["cs.RO", "cs.HC", "cs.LG"], "comment": "Preprint version of paper accepted at IEEE T-RO. Project website:\n  https://sites.google.com/view/mild-hri", "summary": "This article presents a method for learning well-coordinated Human-Robot\nInteraction (HRI) from Human-Human Interactions (HHI). We devise a hybrid\napproach using Hidden Markov Models (HMMs) as the latent space priors for a\nVariational Autoencoder to model a joint distribution over the interacting\nagents. We leverage the interaction dynamics learned from HHI to learn HRI and\nincorporate the conditional generation of robot motions from human observations\ninto the training, thereby predicting more accurate robot trajectories. The\ngenerated robot motions are further adapted with Inverse Kinematics to ensure\nthe desired physical proximity with a human, combining the ease of joint space\nlearning and accurate task space reachability. For contact-rich interactions,\nwe modulate the robot's stiffness using HMM segmentation for a compliant\ninteraction. We verify the effectiveness of our approach deployed on a Humanoid\nrobot via a user study. Our method generalizes well to various humans despite\nbeing trained on data from just two humans. We find that users perceive our\nmethod as more human-like, timely, and accurate and rank our method with a\nhigher degree of preference over other baselines. We additionally show the\nability of our approach to generate successful interactions in a more complex\nscenario of Bimanual Robot-to-Human Handovers."}
{"id": "2506.11116", "pdf": "https://arxiv.org/pdf/2506.11116.pdf", "abs": "https://arxiv.org/abs/2506.11116", "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models", "authors": ["Jijie Li", "Li Du", "Hanyu Zhao", "Bo-wen Zhang", "Liangdong Wang", "Boyan Gao", "Guang Liu", "Yonghua Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and\ncodes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly\nreleased."}
{"id": "2405.01066", "pdf": "https://arxiv.org/pdf/2405.01066.pdf", "abs": "https://arxiv.org/abs/2405.01066", "title": "HandS3C: 3D Hand Mesh Reconstruction with State Space Spatial Channel Attention from RGB images", "authors": ["Zixun Jiao", "Xihan Wang", "Zhaoqiang Xia", "Lianhe Shao", "Quanli Gao"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "5 pages, 3 figures", "summary": "Reconstructing the hand mesh from one single RGB image is a challenging task\nbecause hands are often occluded by other objects. Most previous works attempt\nto explore more additional information and adopt attention mechanisms for\nimproving 3D reconstruction performance, while it would increase computational\ncomplexity simultaneously. To achieve a performance-reserving architecture with\nhigh computational efficiency, in this work, we propose a simple but effective\n3D hand mesh reconstruction network (i.e., HandS3C), which is the first time to\nincorporate state space model into the task of hand mesh reconstruction. In the\nnetwork, we design a novel state-space spatial-channel attention module that\nextends the effective receptive field, extracts hand features in the spatial\ndimension, and enhances regional features of hands in the channel dimension.\nThis helps to reconstruct a complete and detailed hand mesh. Extensive\nexperiments conducted on well-known datasets facing heavy occlusions (such as\nFREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandS3C achieves\nstate-of-the-art performance while maintaining a minimal parameters."}
{"id": "2506.11117", "pdf": "https://arxiv.org/pdf/2506.11117.pdf", "abs": "https://arxiv.org/abs/2506.11117", "title": "ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research", "authors": ["Junyong Lin", "Lu Dai", "Ruiqian Han", "Yijie Sui", "Ruilin Wang", "Xingliang Sun", "Qinglin Wu", "Min Feng", "Hao Liu", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD 2025 Accepted", "summary": "Scientific researchers need intensive information about datasets to\neffectively evaluate and develop theories and methodologies. The information\nneeds regarding datasets are implicitly embedded in particular research tasks,\nrather than explicitly expressed in search queries. However, existing\nscientific retrieval and question-answering (QA) datasets typically address\nstraightforward questions, which do not align with the distribution of\nreal-world research inquiries. To bridge this gap, we developed ScIRGen, a\ndataset generation framework for scientific QA \\& retrieval that more\naccurately reflects the information needs of professional science researchers,\nand uses it to create a large-scale scientific retrieval-augmented generation\n(RAG) dataset with realistic queries, datasets and papers. Technically, we\ndesigned a dataset-oriented information extraction method that leverages\nacademic papers to augment the dataset representation. We then proposed a\nquestion generation framework by employing cognitive taxonomy to ensure the\nquality of synthesized questions. We also design a method to automatically\nfilter synthetic answers based on the perplexity shift of LLMs, which is highly\naligned with human judgment of answers' validity. Collectively, these\nmethodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We\nbenchmarked representative methods on the ScIRGen-Geo dataset for their\nquestion-answering and retrieval capabilities, finding out that current methods\nstill suffer from reasoning from complex questions. This work advances the\ndevelopment of more sophisticated tools to support the intricate information\nneeds of the scientific community."}
{"id": "2409.18009", "pdf": "https://arxiv.org/pdf/2409.18009.pdf", "abs": "https://arxiv.org/abs/2409.18009", "title": "Control Industrial Automation System with Large Language Model Agents", "authors": ["Yuchen Xia", "Nasser Jazdi", "Jize Zhang", "Chaitanya Shah", "Michael Weyrich"], "categories": ["eess.SY", "cs.AI", "cs.HC", "cs.MA", "cs.RO", "cs.SY"], "comment": "Pre-print accepted at 30th IEEE ETFA 2025", "summary": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS."}
{"id": "2506.11119", "pdf": "https://arxiv.org/pdf/2506.11119.pdf", "abs": "https://arxiv.org/abs/2506.11119", "title": "Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech", "authors": ["Jingyu Li", "Lingchao Mao", "Hairong Wang", "Zhendong Wang", "Xi Mao", "Xuelei Sherry Ni"], "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10 (Primary), 68U99 (Secondary)", "I.2.1; J.3"], "comment": null, "summary": "Background: Alzheimer's disease and related dementias (ADRD) are progressive\nneurodegenerative conditions where early detection is vital for timely\nintervention and care. Spontaneous speech contains rich acoustic and linguistic\nmarkers that may serve as non-invasive biomarkers for cognitive decline.\nFoundation models, pre-trained on large-scale audio or text data, produce\nhigh-dimensional embeddings encoding contextual and acoustic features.\n  Methods: We used the PREPARE Challenge dataset, which includes audio\nrecordings from over 1,600 participants with three cognitive statuses: healthy\ncontrol (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We\nexcluded non-English, non-spontaneous, or poor-quality recordings. The final\ndataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We\nbenchmarked a range of open-source foundation speech and language models to\nclassify cognitive status into the three categories.\n  Results: The Whisper-medium model achieved the highest performance among\nspeech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with\npause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection\nusing state-of-the-art automatic speech recognition (ASR) model-generated audio\nembeddings outperformed others. Including non-semantic features like pause\npatterns consistently improved text-based classification.\n  Conclusion: This study introduces a benchmarking framework using foundation\nmodels and a clinically relevant dataset. Acoustic-based approaches --\nparticularly ASR-derived embeddings -- demonstrate strong potential for\nscalable, non-invasive, and cost-effective early detection of ADRD."}
{"id": "2412.14684", "pdf": "https://arxiv.org/pdf/2412.14684.pdf", "abs": "https://arxiv.org/abs/2412.14684", "title": "Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines", "authors": ["Yunsu Kim", "AhmedElmogtaba Abdelaziz", "Thiago Castro Ferreira", "Mohamed Al-Badrashiny", "Hassan Sawaf"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": "ACL 2025 System Demonstrations", "summary": "As the demand for artificial intelligence (AI) grows to address complex\nreal-world tasks, single models are often insufficient, requiring the\nintegration of multiple models into pipelines. This paper introduces Bel\nEsprit, a conversational agent designed to construct AI model pipelines based\non user-defined requirements. Bel Esprit employs a multi-agent framework where\nsubagents collaborate to clarify requirements, build, validate, and populate\npipelines with appropriate models. We demonstrate the effectiveness of this\nframework in generating pipelines from ambiguous user queries, using both\nhuman-curated and synthetic data. A detailed error analysis highlights ongoing\nchallenges in pipeline construction. Bel Esprit is available for a free trial\nat https://belesprit.aixplain.com."}
{"id": "2506.11120", "pdf": "https://arxiv.org/pdf/2506.11120.pdf", "abs": "https://arxiv.org/abs/2506.11120", "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models", "authors": ["Hourun Zhu", "Chengchao Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In spite of strong performance achieved by LLMs, the costs of their\ndeployment are unaffordable. For the compression of LLMs, gradient-based\npruning methods present promising effectiveness. However, in these methods, the\ngradient computation with one-hot labels ignore the potential predictions on\nother words, thus missing key information for generative capability of the\noriginal model. To address this issue, we introduce a self-distillation loss\nduring the pruning phase (rather than post-training) to fully exploit the\npredictions of the original model, thereby obtaining more accurate gradient\ninformation for pruning. Moreover, we find that, compared to attention modules,\nthe predictions of LLM are less sensitive to multilayer perceptron (MLP)\nmodules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this\nend, we focus on the pruning of MLP modules, to significantly compress LLM\nwithout obvious performance degradation. Experimental results on extensive\nzero-shot benchmarks demonstrate that our method significantly outperforms\nexisting pruning methods. Furthermore, our method achieves very competitive\nperformance among 1B-scale open source LLMs. The source code and trained\nweights are available at https://github.com/visresearch/SDMPrune."}
{"id": "2412.21015", "pdf": "https://arxiv.org/pdf/2412.21015.pdf", "abs": "https://arxiv.org/abs/2412.21015", "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets", "authors": ["Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 (Demo)", "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."}
{"id": "2506.11121", "pdf": "https://arxiv.org/pdf/2506.11121.pdf", "abs": "https://arxiv.org/abs/2506.11121", "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR", "authors": ["Wei-Ping Huang", "Guan-Ting Lin", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains."}
{"id": "2506.00073", "pdf": "https://arxiv.org/pdf/2506.00073.pdf", "abs": "https://arxiv.org/abs/2506.00073", "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."}
{"id": "2506.11125", "pdf": "https://arxiv.org/pdf/2506.11125.pdf", "abs": "https://arxiv.org/abs/2506.11125", "title": "ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams", "authors": ["Freddie Grabovski", "Gilad Gressel", "Yisroel Mirsky"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and\nAutomatic Speech Recognition (ASR), are increasingly used to automate voice\nphishing (vishing) scams. These systems are scalable and convincing, posing a\nsignificant security threat. We identify the ASR transcription step as the most\nvulnerable link in the scam pipeline and introduce ASRJam, a proactive defence\nframework that injects adversarial perturbations into the victim's audio to\ndisrupt the attacker's ASR. This breaks the scam's feedback loop without\naffecting human callers, who can still understand the conversation. While prior\nadversarial audio techniques are often unpleasant and impractical for real-time\nuse, we also propose EchoGuard, a novel jammer that leverages natural\ndistortions, such as reverberation and echo, that are disruptive to ASR but\ntolerable to humans. To evaluate EchoGuard's effectiveness and usability, we\nconducted a 39-person user study comparing it with three state-of-the-art\nattacks. Results show that EchoGuard achieved the highest overall utility,\noffering the best combination of ASR disruption and human listening experience."}
{"id": "2506.06381", "pdf": "https://arxiv.org/pdf/2506.06381.pdf", "abs": "https://arxiv.org/abs/2506.06381", "title": "DURA-CPS: A Multi-Role Orchestrator for Dependability Assurance in LLM-Enabled Cyber-Physical Systems", "authors": ["Trisanth Srinivasan", "Santosh Patapati", "Himani Musku", "Idhant Gode", "Aditya Arora", "Samvit Bhattacharya", "Abubakr Nazriev", "Sanika Hirave", "Zaryab Kanjiani", "Srinjoy Ghose"], "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.HC", "cs.MA", "C.3; C.4; D.2.4; D.4.6; I.2.7"], "comment": "Accepted to the 55th Annual IEEE/IFIP International Conference on\n  Dependable Systems and Networks Workshops (DSN-W)", "summary": "Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to\noperate in critical applications. However, traditional verification and\nvalidation methods often struggle to handle the unpredictable and dynamic\nnature of AI components. In this paper, we introduce DURA-CPS, a novel\nframework that employs multi-role orchestration to automate the iterative\nassurance process for AI-powered CPS. By assigning specialized roles (e.g.,\nsafety monitoring, security assessment, fault injection, and recovery planning)\nto dedicated agents within a simulated environment, DURA-CPS continuously\nevaluates and refines AI behavior against a range of dependability\nrequirements. We demonstrate the framework through a case study involving an\nautonomous vehicle navigating an intersection with an AI-based planner. Our\nresults show that DURA-CPS effectively detects vulnerabilities, manages\nperformance impacts, and supports adaptive recovery strategies, thereby\noffering a structured and extensible solution for rigorous V&V in safety- and\nsecurity-critical systems."}
{"id": "2506.11127", "pdf": "https://arxiv.org/pdf/2506.11127.pdf", "abs": "https://arxiv.org/abs/2506.11127", "title": "GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Longrong Yang", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the\nfirst end-to-end autonomous GUI agent that directly accepts speech instructions\nand on-device screenshots to predict actions. Confronted with the scarcity of\nspeech-based GUI agent datasets, we initially generated high-quality speech\ninstructions for training by leveraging a random timbre text-to-speech (TTS)\nmodel to convert existing text instructions. We then develop\nGUIRoboTron-Speech's capabilities through progressive grounding and planning\ntraining stages. A key contribution is a heuristic mixed-instruction training\nstrategy designed to mitigate the modality imbalance inherent in pre-trained\nfoundation models. Comprehensive experiments on several benchmark datasets\nvalidate the robust and superior performance of GUIRoboTron-Speech,\ndemonstrating the significant potential and widespread applicability of speech\nas an effective instruction modality for driving GUI agents. Our code and\ndatasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech."}
{"id": "2506.11128", "pdf": "https://arxiv.org/pdf/2506.11128.pdf", "abs": "https://arxiv.org/abs/2506.11128", "title": "Stronger Language Models Produce More Human-Like Errors", "authors": ["Andrew Keenan Richardson", "Ryan Othniel Kearns", "Sean Moss", "Vincent Wang-Mascianica", "Philipp Koralus"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do language models converge toward human-like reasoning patterns as they\nimprove? We provide surprising evidence that while overall reasoning\ncapabilities increase with model sophistication, the nature of errors\nincreasingly mirrors predictable human reasoning fallacies: a previously\nunobserved inverse scaling phenomenon. To investigate this question, we apply\nthe Erotetic Theory of Reasoning (ETR), a formal cognitive framework with\nempirical support for predicting human reasoning outcomes. Using the\nopen-source package PyETR, we generate logical reasoning problems where humans\npredictably err, evaluating responses from 38 language models across 383\nreasoning tasks. Our analysis indicates that as models advance in general\ncapability (as measured by Chatbot Arena scores), the proportion of their\nincorrect answers that align with ETR-predicted human fallacies tends to\nincrease ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation\nbetween model sophistication and logical correctness on these tasks, this shift\nin error patterns toward human-likeness occurs independently of error rate.\nThese findings challenge the prevailing view that scaling language models\nnaturally obtains normative rationality, suggesting instead a convergence\ntoward human-like cognition inclusive of our characteristic biases and\nlimitations, as we further confirm by demonstrating order-effects in language\nmodel reasoning."}
{"id": "2506.11129", "pdf": "https://arxiv.org/pdf/2506.11129.pdf", "abs": "https://arxiv.org/abs/2506.11129", "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK", "authors": ["Carlos Garcia-Fernandez", "Luis Felipe", "Monique Shotande", "Muntasir Zitu", "Aakash Tripathi", "Ghulam Rasool", "Issam El Naqa", "Vivek Rudrapatna", "Gilmer Valdes"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise in healthcare, but hallucinations\nremain a major barrier to clinical use. We present CHECK, a continuous-learning\nframework that integrates structured clinical databases with a classifier\ngrounded in information theory to detect both factual and reasoning-based\nhallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials,\nCHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% -\nmaking an open source model state of the art. Its classifier generalized across\nmedical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE)\nbenchmark and HealthBench realistic multi-turn medical questioning. By\nleveraging hallucination probabilities to guide GPT-4o's refinement and\njudiciously escalate compute, CHECK boosted its USMLE passing rate by 5\npercentage points, achieving a state-of-the-art 92.1%. By suppressing\nhallucinations below accepted clinical error thresholds, CHECK offers a\nscalable foundation for safe LLM deployment in medicine and other high-stakes\ndomains."}
{"id": "2506.11130", "pdf": "https://arxiv.org/pdf/2506.11130.pdf", "abs": "https://arxiv.org/abs/2506.11130", "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data", "authors": ["Cheng Kang Chou", "Chan-Jan Hsu", "Ho-Lam Chung", "Liang-Hsuan Tseng", "Hsi-Chun Cheng", "Yu-Kuan Fu", "Kuan Po Huang", "Hung-Yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings."}
{"id": "2506.11135", "pdf": "https://arxiv.org/pdf/2506.11135.pdf", "abs": "https://arxiv.org/abs/2506.11135", "title": "Large Language Models and Emergence: A Complex Systems Perspective", "authors": ["David C. Krakauer", "John W. Krakauer", "Melanie Mitchell"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Emergence is a concept in complexity science that describes how many-body\nsystems manifest novel higher-level properties, properties that can be\ndescribed by replacing high-dimensional mechanisms with lower-dimensional\neffective variables and theories. This is captured by the idea \"more is\ndifferent\". Intelligence is a consummate emergent property manifesting\nincreasingly efficient -- cheaper and faster -- uses of emergent capabilities\nto solve problems. This is captured by the idea \"less is more\". In this paper,\nwe first examine claims that Large Language Models exhibit emergent\ncapabilities, reviewing several approaches to quantifying emergence, and\nsecondly ask whether LLMs possess emergent intelligence."}
{"id": "2506.11137", "pdf": "https://arxiv.org/pdf/2506.11137.pdf", "abs": "https://arxiv.org/abs/2506.11137", "title": "Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models", "authors": ["Chong Shao", "Douglas Snyder", "Chiran Li", "Bowen Gu", "Kerry Ngan", "Chun-Ting Yang", "Jiageng Wu", "Richard Wyss", "Kueiyu Joshua Lin", "Jie Yang"], "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Identifying medication discontinuations in electronic health records (EHRs)\nis vital for patient safety but is often hindered by information being buried\nin unstructured notes. This study aims to evaluate the capabilities of advanced\nopen-sourced and proprietary large language models (LLMs) in extracting\nmedications and classifying their medication status from EHR notes, focusing on\ntheir scalability on medication information extraction without human\nannotation. We collected three EHR datasets from diverse sources to build the\nevaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM\nprompting strategies. Performance on medication extraction, medication status\nclassification, and their joint task (extraction then classification) was\nsystematically compared across all experiments. We found that LLMs showed\npromising performance on the medication extraction and discontinuation\nclassification from EHR notes. GPT-4o consistently achieved the highest average\nF1 scores in all tasks under zero-shot setting - 94.0% for medication\nextraction, 78.1% for discontinuation classification, and 72.7% for the joint\ntask. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the\nhighest performance in medication status classification on the MIV-Med dataset\n(68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%)\ndatasets. Medical-specific LLMs demonstrated lower performance compared to\nadvanced general-domain LLMs. Few-shot learning generally improved performance,\nwhile CoT reasoning showed inconsistent gains. LLMs demonstrate strong\npotential for medication extraction and discontinuation identification on EHR\nnotes, with open-sourced models offering scalable alternatives to proprietary\nsystems and few-shot can further improve LLMs' capability."}
{"id": "2506.11243", "pdf": "https://arxiv.org/pdf/2506.11243.pdf", "abs": "https://arxiv.org/abs/2506.11243", "title": "RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?", "authors": ["Santiago Góngora", "Ignacio Sastre", "Santiago Robaina", "Ignacio Remersaro", "Luis Chiruzzo", "Aiala Rosá"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented at the 20th BEA Workshop (Innovative Use\n  of NLP for Building Educational Applications) at ACL 2025", "summary": "In this paper, we present the RETUYT-INCO participation at the BEA 2025\nshared task. Our participation was characterized by the decision of using\nrelatively small models, with fewer than 1B parameters. This self-imposed\nrestriction tries to represent the conditions in which many research labs or\ninstitutions are in the Global South, where computational power is not easily\naccessible due to its prohibitive cost. Even under this restrictive\nself-imposed setting, our models managed to stay competitive with the rest of\nteams that participated in the shared task. According to the $exact\\ F_1$\nscores published by the organizers, the performance gaps between our models and\nthe winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in\nTrack 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the\nminimum difference with a winner team is $6.46$ points -- and the maximum\ndifference is $13.13$ -- according to the $exact\\ F_1$ score, we find that\nmodels with a size smaller than 1B parameters are competitive for these tasks,\nall of which can be run on computers with a low-budget GPU or even without a\nGPU."}
{"id": "2506.11244", "pdf": "https://arxiv.org/pdf/2506.11244.pdf", "abs": "https://arxiv.org/abs/2506.11244", "title": "Iterative Multilingual Spectral Attribute Erasure", "authors": ["Shun Shao", "Yftah Ziser", "Zheng Zhao", "Yifu Qiu", "Shay B. Cohen", "Anna Korhonen"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Multilingual representations embed words with similar meanings to share a\ncommon semantic space across languages, creating opportunities to transfer\ndebiasing effects between languages. However, existing methods for debiasing\nare unable to exploit this opportunity because they operate on individual\nlanguages. We present Iterative Multilingual Spectral Attribute Erasure\n(IMSAE), which identifies and mitigates joint bias subspaces across multiple\nlanguages through iterative SVD-based truncation. Evaluating IMSAE across eight\nlanguages and five demographic dimensions, we demonstrate its effectiveness in\nboth standard and zero-shot settings, where target language data is\nunavailable, but linguistically similar languages can be used for debiasing.\nOur comprehensive experiments across diverse language models (BERT, LLaMA,\nMistral) show that IMSAE outperforms traditional monolingual and cross-lingual\napproaches while maintaining model utility."}
{"id": "2506.11246", "pdf": "https://arxiv.org/pdf/2506.11246.pdf", "abs": "https://arxiv.org/abs/2506.11246", "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning", "authors": ["Kushagra Dixit", "Abhishek Rajgaria", "Harshavardhan Kalalbandi", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 19 Tables, 9 Figures", "summary": "Temporal Table Reasoning is a critical challenge for Large Language Models\n(LLMs), requiring effective prompting techniques to extract relevant insights.\nDespite existence of multiple prompting methods, their impact on table\nreasoning remains largely unexplored. Furthermore, the performance of these\nmodels varies drastically across different table and context structures, making\nit difficult to determine an optimal approach. This work investigates multiple\nprompting technique across diverse table types to determine optimal approaches\nfor different scenarios. We find that performance varies based on entity type,\ntable structure, requirement of additional context and question complexity,\nwith NO single method consistently outperforming others. To mitigate these\nchallenges, we introduce SEAR, an adaptive prompting framework inspired by\nhuman reasoning that dynamically adjusts based on context characteristics and\nintegrates a structured reasoning. Our results demonstrate that SEAR achieves\nsuperior performance across all table types compared to other baseline\nprompting techniques. Additionally, we explore the impact of table structure\nrefactoring, finding that a unified representation enhances model's reasoning."}
{"id": "2506.11274", "pdf": "https://arxiv.org/pdf/2506.11274.pdf", "abs": "https://arxiv.org/abs/2506.11274", "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling", "authors": ["Liran Ringel", "Elad Tolochinsky", "Yaniv Romano"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing."}
{"id": "2506.11300", "pdf": "https://arxiv.org/pdf/2506.11300.pdf", "abs": "https://arxiv.org/abs/2506.11300", "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning", "authors": ["Yang Zhang", "Amr Mohamed", "Hadi Abdine", "Guokan Shang", "Michalis Vazirgiannis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Curriculum learning has shown promise in improving training efficiency and\ngeneralization in various machine learning domains, yet its potential in\npretraining language models remains underexplored, prompting our work as the\nfirst systematic investigation in this area. We experimented with different\nsettings, including vanilla curriculum learning, pacing-based sampling, and\ninterleaved curricula-guided by six difficulty metrics spanning linguistic and\ninformation-theoretic perspectives. We train models under these settings and\nevaluate their performance on eight diverse benchmarks. Our experiments reveal\nthat curriculum learning consistently improves convergence in early and\nmid-training phases, and can yield lasting gains when used as a warmup strategy\nwith up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical\ndiversity, and readability as effective difficulty signals across settings. Our\nfindings highlight the importance of data ordering in large-scale pretraining\nand provide actionable insights for scalable, data-efficient model development\nunder realistic training scenarios."}
{"id": "2506.11305", "pdf": "https://arxiv.org/pdf/2506.11305.pdf", "abs": "https://arxiv.org/abs/2506.11305", "title": "Don't Pay Attention", "authors": ["Mohammad Hammoud", "Devang Acharya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies."}
{"id": "2506.11338", "pdf": "https://arxiv.org/pdf/2506.11338.pdf", "abs": "https://arxiv.org/abs/2506.11338", "title": "Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly", "authors": ["Yi-Chien Lin", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "As Transformers become more widely incorporated into natural language\nprocessing tasks, there has been considerable interest in using surprisal from\nthese models as predictors of human sentence processing difficulty. Recent work\nhas observed a positive relationship between Transformer-based models'\nperplexity and the predictive power of their surprisal estimates on reading\ntimes, showing that language models with more parameters and trained on more\ndata are less predictive of human reading times. However, these studies focus\non predicting latency-based measures (i.e., self-paced reading times and\neye-gaze durations) with surprisal estimates from Transformer-based language\nmodels. This trend has not been tested on brain imaging data. This study\ntherefore evaluates the predictive power of surprisal estimates from 17\npre-trained Transformer-based models across three different language families\non two functional magnetic resonance imaging datasets. Results show that the\npositive relationship between model perplexity and model fit still obtains,\nsuggesting that this trend is not specific to latency-based measures and can be\ngeneralized to neural measures."}
{"id": "2506.11343", "pdf": "https://arxiv.org/pdf/2506.11343.pdf", "abs": "https://arxiv.org/abs/2506.11343", "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review", "authors": ["Yaohui Zhang", "Haijing Zhang", "Wenlong Ji", "Tianyu Hua", "Nick Haber", "Hancheng Cao", "Weixin Liang"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of large language models (LLMs) offers unprecedented opportunities\nto reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating\ntraditional review workflows with LLMs serving as direct substitutes for human\nreviewers, while limited attention has been given to exploring new paradigms\nthat fundamentally rethink how LLMs can participate in the academic review\nprocess. In this paper, we introduce and explore a novel mechanism that employs\nLLM agents to perform pairwise comparisons among manuscripts instead of\nindividual scoring. By aggregating outcomes from substantial pairwise\nevaluations, this approach enables a more accurate and robust measure of\nrelative manuscript quality. Our experiments demonstrate that this comparative\napproach significantly outperforms traditional rating-based methods in\nidentifying high-impact papers. However, our analysis also reveals emergent\nbiases in the selection process, notably a reduced novelty in research topics\nand an increased institutional imbalance. These findings highlight both the\ntransformative potential of rethinking peer review with LLMs and critical\nchallenges that future systems must address to ensure equity and diversity."}
{"id": "2506.11344", "pdf": "https://arxiv.org/pdf/2506.11344.pdf", "abs": "https://arxiv.org/abs/2506.11344", "title": "Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models", "authors": ["Peilin Wu", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel approach to Speaker Diarization (SD) by leveraging\ntext-based methods focused on Sentence-level Speaker Change Detection within\ndialogues. Unlike audio-based SD systems, which are often challenged by audio\nquality and speaker similarity, our approach utilizes the dialogue transcript\nalone. Two models are developed: the Single Prediction Model (SPM) and the\nMultiple Prediction Model (MPM), both of which demonstrate significant\nimprovements in identifying speaker changes, particularly in short\nconversations. Our findings, based on a curated dataset encompassing diverse\nconversational scenarios, reveal that the text-based SD approach, especially\nthe MPM, performs competitively against state-of-the-art audio-based SD\nsystems, with superior performance in short conversational contexts. This paper\nnot only showcases the potential of leveraging linguistic features for SD but\nalso highlights the importance of integrating semantic understanding into SD\nsystems, opening avenues for future research in multimodal and semantic\nfeature-based diarization."}
{"id": "2506.11361", "pdf": "https://arxiv.org/pdf/2506.11361.pdf", "abs": "https://arxiv.org/abs/2506.11361", "title": "The Biased Samaritan: LLM biases in Perceived Kindness", "authors": ["Jack H Fagan", "Ruhaan Juyaal", "Amy Yue-Ming Yu", "Siya Pun"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "While Large Language Models (LLMs) have become ubiquitous in many fields,\nunderstanding and mitigating LLM biases is an ongoing issue. This paper\nprovides a novel method for evaluating the demographic biases of various\ngenerative AI models. By prompting models to assess a moral patient's\nwillingness to intervene constructively, we aim to quantitatively evaluate\ndifferent LLMs' biases towards various genders, races, and ages. Our work\ndiffers from existing work by aiming to determine the baseline demographic\nidentities for various commercial models and the relationship between the\nbaseline and other demographics. We strive to understand if these biases are\npositive, neutral, or negative, and the strength of these biases. This paper\ncan contribute to the objective assessment of bias in Large Language Models and\ngive the user or developer the power to account for these biases in LLM output\nor in training future LLMs. Our analysis suggested two key findings: that\nmodels view the baseline demographic as a white middle-aged or young adult\nmale; however, a general trend across models suggested that non-baseline\ndemographics are more willing to help than the baseline. These methodologies\nallowed us to distinguish these two biases that are often tangled together."}
{"id": "2506.11381", "pdf": "https://arxiv.org/pdf/2506.11381.pdf", "abs": "https://arxiv.org/abs/2506.11381", "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction", "authors": ["Samuel Mensah", "Elena Kochkina", "Jabez Magomere", "Joy Prakash Sain", "Simerjot Kaur", "Charese Smiley"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main", "summary": "Mitigating entity bias is a critical challenge in Relation Extraction (RE),\nwhere models often rely excessively on entities, resulting in poor\ngeneralization. This paper presents a novel approach to address this issue by\nadapting a Variational Information Bottleneck (VIB) framework. Our method\ncompresses entity-specific information while preserving task-relevant features.\nIt achieves state-of-the-art performance on relation extraction datasets across\ngeneral, financial, and biomedical domains, in both indomain (original test\nsets) and out-of-domain (modified test sets with type-constrained entity\nreplacements) settings. Our approach offers a robust, interpretable, and\ntheoretically grounded methodology."}
{"id": "2506.11389", "pdf": "https://arxiv.org/pdf/2506.11389.pdf", "abs": "https://arxiv.org/abs/2506.11389", "title": "Curriculum-Guided Layer Scaling for Language Model Pretraining", "authors": ["Karanpartap Singh", "Neil Band", "Ehsan Adeli"], "categories": ["cs.CL"], "comment": null, "summary": "As the cost of pretraining large language models grows, there is continued\ninterest in strategies to improve learning efficiency during this core training\nstage. Motivated by cognitive development, where humans gradually build\nknowledge as their brains mature, we propose Curriculum-Guided Layer Scaling\n(CGLS), a framework for compute-efficient pretraining that synchronizes\nincreasing data difficulty with model growth through progressive layer stacking\n(i.e. gradually adding layers during training). At the 100M parameter scale,\nusing a curriculum transitioning from synthetic short stories to general web\ndata, CGLS outperforms baseline methods on the question-answering benchmarks\nPIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus\nwith a DistilBERT-based classifier and progress from general text to highly\ntechnical or specialized content. Our results show that progressively\nincreasing model depth alongside sample difficulty leads to better\ngeneralization and zero-shot performance on various downstream benchmarks.\nAltogether, our findings demonstrate that CGLS unlocks the potential of\nprogressive stacking, offering a simple yet effective strategy for improving\ngeneralization on knowledge-intensive and reasoning tasks."}
{"id": "2506.11410", "pdf": "https://arxiv.org/pdf/2506.11410.pdf", "abs": "https://arxiv.org/abs/2506.11410", "title": "Predicting Early-Onset Colorectal Cancer with Large Language Models", "authors": ["Wilson Lau", "Youngwon Kim", "Sravanthi Parasa", "Md Enamul Haque", "Anand Oka", "Jay Nanduri"], "categories": ["cs.CL"], "comment": "Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA)", "summary": "The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has\nincreased every year, but this population is younger than the recommended age\nestablished by national guidelines for cancer screening. In this paper, we\napplied 10 different machine learning models to predict EoCRC, and compared\ntheir performance with advanced large language models (LLM), using patient\nconditions, lab results, and observations within 6 months of patient journey\nprior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients\nfrom multiple health systems across the United States. The results demonstrated\nthat the fine-tuned LLM achieved an average of 73% sensitivity and 91%\nspecificity."}
{"id": "2506.11418", "pdf": "https://arxiv.org/pdf/2506.11418.pdf", "abs": "https://arxiv.org/abs/2506.11418", "title": "Efficient Long-Context LLM Inference via KV Cache Clustering", "authors": ["Jie Hu", "Shengnan Wang", "Yutong He", "Ping Gong", "Jiawei Yi", "Juncheng Zhang", "Youhui Bai", "Renhai Chen", "Gong Zhang", "Cheng Li", "Kun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."}
{"id": "2506.11425", "pdf": "https://arxiv.org/pdf/2506.11425.pdf", "abs": "https://arxiv.org/abs/2506.11425", "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards", "authors": ["Jeff Da", "Clinton Wang", "Xiang Deng", "Yuntao Ma", "Nikhil Barhate", "Sean Hendryx"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle."}
{"id": "2506.11432", "pdf": "https://arxiv.org/pdf/2506.11432.pdf", "abs": "https://arxiv.org/abs/2506.11432", "title": "KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models", "authors": ["Taeeun Kim", "Semin Jeong", "Youngsook Song"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks."}
{"id": "2506.11440", "pdf": "https://arxiv.org/pdf/2506.11440.pdf", "abs": "https://arxiv.org/abs/2506.11440", "title": "AbsenceBench: Language Models Can't Tell What's Missing", "authors": ["Harvey Yiyun Fu", "Aryan Shrivastava", "Jared Moore", "Peter West", "Chenhao Tan", "Ari Holtzman"], "categories": ["cs.CL"], "comment": "23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench", "summary": "Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench)."}
{"id": "2506.11467", "pdf": "https://arxiv.org/pdf/2506.11467.pdf", "abs": "https://arxiv.org/abs/2506.11467", "title": "A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems", "authors": ["Carlos Rafael Catalan"], "categories": ["cs.CL", "cs.SI", "F.2.2, I.2.7"], "comment": "7 pages, 7 figures, presented at the HEAL Workshop at CHI", "summary": "Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research."}
{"id": "2506.11474", "pdf": "https://arxiv.org/pdf/2506.11474.pdf", "abs": "https://arxiv.org/abs/2506.11474", "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards", "authors": ["Jaehoon Yun", "Jiwoong Sohn", "Jungwoo Park", "Hyunjae Kim", "Xiangru Tang", "Yanjun Shao", "Yonghoe Koo", "Minhyeok Ko", "Qingyu Chen", "Mark Gerstein", "Michael Moor", "Jaewoo Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/"}
{"id": "2506.11478", "pdf": "https://arxiv.org/pdf/2506.11478.pdf", "abs": "https://arxiv.org/abs/2506.11478", "title": "ImmunoFOMO: Are Language Models missing what oncologists see?", "authors": ["Aman Sinha", "Bogdan-Valentin Popescu", "Xavier Coubez", "Marianne Clausel", "Mathieu Constant"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts."}
{"id": "2506.11485", "pdf": "https://arxiv.org/pdf/2506.11485.pdf", "abs": "https://arxiv.org/abs/2506.11485", "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models", "authors": ["Cole Gawin"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 3 tables", "summary": "While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training."}
{"id": "2506.11498", "pdf": "https://arxiv.org/pdf/2506.11498.pdf", "abs": "https://arxiv.org/abs/2506.11498", "title": "Lag-Relative Sparse Attention In Long Context Training", "authors": ["Manlai Liang", "Wanyi Huang", "Mandi Liu", "Huaijun Li", "Jinlong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."}
{"id": "2506.11499", "pdf": "https://arxiv.org/pdf/2506.11499.pdf", "abs": "https://arxiv.org/abs/2506.11499", "title": "On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval", "authors": ["Seongbo Jang", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "9 pages, 1 figure", "summary": "Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities."}
{"id": "2506.11557", "pdf": "https://arxiv.org/pdf/2506.11557.pdf", "abs": "https://arxiv.org/abs/2506.11557", "title": "From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation", "authors": ["Chih-Hao Hsu", "Ying-Jia Lin", "Hung-Yu Kao"], "categories": ["cs.CL"], "comment": "Accepted by PAKDD 2025", "summary": "In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges."}
{"id": "2506.11602", "pdf": "https://arxiv.org/pdf/2506.11602.pdf", "abs": "https://arxiv.org/abs/2506.11602", "title": "Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study", "authors": ["Hawau Olamide Toyin", "Samar M. Magdy", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates."}
{"id": "2506.11631", "pdf": "https://arxiv.org/pdf/2506.11631.pdf", "abs": "https://arxiv.org/abs/2506.11631", "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context", "authors": ["Simeon Junker", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references."}
{"id": "2506.11638", "pdf": "https://arxiv.org/pdf/2506.11638.pdf", "abs": "https://arxiv.org/abs/2506.11638", "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Yixiao Ge", "Xiu Li", "Ying Shan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks."}
{"id": "2506.11666", "pdf": "https://arxiv.org/pdf/2506.11666.pdf", "abs": "https://arxiv.org/abs/2506.11666", "title": "Converting Annotated Clinical Cases into Structured Case Report Forms", "authors": ["Pietro Ferrazzi", "Alberto Lavelli", "Bernardo Magnini"], "categories": ["cs.CL", "cs.AI"], "comment": "to be published in BioNLP 2025", "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166"}
{"id": "2506.11673", "pdf": "https://arxiv.org/pdf/2506.11673.pdf", "abs": "https://arxiv.org/abs/2506.11673", "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE", "authors": ["Alicja Dobrzeniecka", "Antske Fokkens", "Pia Sommerauer"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing."}
{"id": "2506.11681", "pdf": "https://arxiv.org/pdf/2506.11681.pdf", "abs": "https://arxiv.org/abs/2506.11681", "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach", "authors": ["Pratibha Zunjare", "Michael Hsiao"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task."}
{"id": "2506.11702", "pdf": "https://arxiv.org/pdf/2506.11702.pdf", "abs": "https://arxiv.org/abs/2506.11702", "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data", "authors": ["Víctor Gallego"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment", "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning"}
{"id": "2506.11728", "pdf": "https://arxiv.org/pdf/2506.11728.pdf", "abs": "https://arxiv.org/abs/2506.11728", "title": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference", "authors": ["Héctor Martínez", "Adrián Castelló", "Francisco D. Igual", "Enrique S. Quintana-Ortí"], "categories": ["cs.CL"], "comment": "16 pages, 7 tables, 7 figures", "summary": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication."}
{"id": "2506.11752", "pdf": "https://arxiv.org/pdf/2506.11752.pdf", "abs": "https://arxiv.org/abs/2506.11752", "title": "DART: Distilling Autoregressive Reasoning to Silent Thought", "authors": ["Nan Jiang", "Ziming Wu", "De-Chuan Zhan", "Fuming Lai", "Shaobing Lian"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning."}
{"id": "2506.11763", "pdf": "https://arxiv.org/pdf/2506.11763.pdf", "abs": "https://arxiv.org/abs/2506.11763", "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao"], "categories": ["cs.CL", "cs.IR"], "comment": "31 pages, 5 figures", "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents."}
{"id": "2506.11769", "pdf": "https://arxiv.org/pdf/2506.11769.pdf", "abs": "https://arxiv.org/abs/2506.11769", "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs", "authors": ["Tianqi Du", "Haotian Huang", "Yifei Wang", "Yisen Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment."}
{"id": "2506.11798", "pdf": "https://arxiv.org/pdf/2506.11798.pdf", "abs": "https://arxiv.org/abs/2506.11798", "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models", "authors": ["Maximilian Kreutner", "Marlene Lutz", "Markus Strohmaier"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation."}
{"id": "2506.11807", "pdf": "https://arxiv.org/pdf/2506.11807.pdf", "abs": "https://arxiv.org/abs/2506.11807", "title": "Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?", "authors": ["Simeon Junker", "Manar Ali", "Larissa Koch", "Sina Zarrieß", "Hendrik Buschmeier"], "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs."}
{"id": "2506.11857", "pdf": "https://arxiv.org/pdf/2506.11857.pdf", "abs": "https://arxiv.org/abs/2506.11857", "title": "Post Persona Alignment for Multi-Session Dialogue Generation", "authors": ["Yi-Pei Chen", "Noriki Nishida", "Hideki Nakayama", "Yuji Matsumoto"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation."}
{"id": "2506.11886", "pdf": "https://arxiv.org/pdf/2506.11886.pdf", "abs": "https://arxiv.org/abs/2506.11886", "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache", "authors": ["Xiaoran Liu", "Siyang He", "Qiqi Wang", "Ruixiao Li", "Yuerong Song", "Zhigeng Liu", "Linlin Li", "Qun Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures, work in progress", "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903.pdf", "abs": "https://arxiv.org/abs/2506.11903", "title": "GeistBERT: Breathing Life into German NLP", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license."}
{"id": "2506.11919", "pdf": "https://arxiv.org/pdf/2506.11919.pdf", "abs": "https://arxiv.org/abs/2506.11919", "title": "Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study", "authors": ["Greta Damo", "Elena Cabrio", "Serena Villata"], "categories": ["cs.CL"], "comment": null, "summary": "Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions."}
{"id": "2506.11930", "pdf": "https://arxiv.org/pdf/2506.11930.pdf", "abs": "https://arxiv.org/abs/2506.11930", "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback", "authors": ["Dongwei Jiang", "Alvin Zhang", "Andrew Wang", "Nicholas Andrews", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement."}
{"id": "2506.11938", "pdf": "https://arxiv.org/pdf/2506.11938.pdf", "abs": "https://arxiv.org/abs/2506.11938", "title": "Improving Large Language Model Safety with Contrastive Representation Learning", "authors": ["Samuel Simko", "Mrinmaya Sachan", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense"}
{"id": "2506.12014", "pdf": "https://arxiv.org/pdf/2506.12014.pdf", "abs": "https://arxiv.org/abs/2506.12014", "title": "code_transformed: The Influence of Large Language Models on Code", "authors": ["Yuliang Xu", "Siming Huang", "Mingmeng Geng", "Yao Wan", "Xuanhua Shi", "Dongping Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code", "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style."}
{"id": "2506.11004", "pdf": "https://arxiv.org/pdf/2506.11004.pdf", "abs": "https://arxiv.org/abs/2506.11004", "title": "Developing a Dyslexia Indicator Using Eye Tracking", "authors": ["Kevin Cogan", "Vuong M. Ngo", "Mark Roantree"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Dyslexia, affecting an estimated 10% to 20% of the global population,\nsignificantly impairs learning capabilities, highlighting the need for\ninnovative and accessible diagnostic methods. This paper investigates the\neffectiveness of eye-tracking technology combined with machine learning\nalgorithms as a cost-effective alternative for early dyslexia detection. By\nanalyzing general eye movement patterns, including prolonged fixation durations\nand erratic saccades, we proposed an enhanced solution for determining\neye-tracking-based dyslexia features. A Random Forest Classifier was then\nemployed to detect dyslexia, achieving an accuracy of 88.58\\%. Additionally,\nhierarchical clustering methods were applied to identify varying severity\nlevels of dyslexia. The analysis incorporates diverse methodologies across\nvarious populations and settings, demonstrating the potential of this\ntechnology to identify individuals with dyslexia, including those with\nborderline traits, through non-invasive means. Integrating eye-tracking with\nmachine learning represents a significant advancement in the diagnostic\nprocess, offering a highly accurate and accessible method in clinical research."}
{"id": "2506.11012", "pdf": "https://arxiv.org/pdf/2506.11012.pdf", "abs": "https://arxiv.org/abs/2506.11012", "title": "A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects", "authors": ["Guanglin Niu", "Bo Li", "Yangguang Lin"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "45 pages, 17 figures, 12 tables", "summary": "Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring\nand leveraging diverse real-world knowledge, which serve as a fundamental\ntechnology for enabling cognitive intelligence systems with advanced\nunderstanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims\nto infer new knowledge based on existing facts in KGs, playing a crucial role\nin applications such as public security intelligence, intelligent healthcare,\nand financial risk assessment. From a task-centric perspective, existing KGR\napproaches can be broadly classified into static single-step KGR, static\nmulti-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR.\nWhile existing surveys have covered these six types of KGR tasks, a\ncomprehensive review that systematically summarizes all KGR tasks particularly\nincluding downstream applications and more challenging reasoning paradigms\nremains lacking. In contrast to previous works, this survey provides a more\ncomprehensive perspective on the research of KGR by categorizing approaches\nbased on primary reasoning tasks, downstream application tasks, and potential\nchallenging reasoning tasks. Besides, we explore advanced techniques, such as\nlarge language models (LLMs), and their impact on KGR. This work aims to\nhighlight key research trends and outline promising future directions in the\nfield of KGR."}
{"id": "2506.11022", "pdf": "https://arxiv.org/pdf/2506.11022.pdf", "abs": "https://arxiv.org/abs/2506.11022", "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\"."}
{"id": "2506.11031", "pdf": "https://arxiv.org/pdf/2506.11031.pdf", "abs": "https://arxiv.org/abs/2506.11031", "title": "Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models", "authors": ["Zoher Kachwala", "Danishjeet Singh", "Danielle Yang", "Filippo Menczer"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As image generators produce increasingly realistic images, concerns about\npotential misuse continue to grow. Supervised detection relies on large,\ncurated datasets and struggles to generalize across diverse generators. In this\nwork, we investigate the use of pre-trained Vision-Language Models (VLMs) for\nzero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit\nsome task-specific reasoning and chain-of-thought prompting offers gains, we\nshow that task-aligned prompting elicits more focused reasoning and\nsignificantly improves performance without fine-tuning. Specifically, prefixing\nthe model's response with the phrase ``Let's examine the style and the\nsynthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1\nscores by 8%-29% for two widely used open-source models. These gains are\nconsistent across three recent, diverse datasets spanning human faces, objects,\nand animals with images generated by 16 different models -- demonstrating\nstrong generalization. We further evaluate the approach across three additional\nmodel sizes and observe improvements in most dataset-model combinations --\nsuggesting robustness to model scale. Surprisingly, self-consistency, a\nbehavior previously observed in language reasoning, where aggregating answers\nfrom diverse reasoning paths improves performance, also holds in this setting.\nEven here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --\nindicating that it elicits more useful diversity. Our findings show that\ntask-aligned prompts elicit more focused reasoning and enhance latent\ncapabilities in VLMs, like the detection of AI-generated images -- offering a\nsimple, generalizable, and explainable alternative to supervised methods. Our\ncode is publicly available on github:\nhttps://github.com/osome-iu/Zero-shot-s2.git."}
{"id": "2506.11034", "pdf": "https://arxiv.org/pdf/2506.11034.pdf", "abs": "https://arxiv.org/abs/2506.11034", "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models", "authors": ["Aneesh Komanduri", "Karuna Bhaila", "Xintao Wu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable ability in various\nlanguage tasks, especially with their emergent in-context learning capability.\nExtending LLMs to incorporate visual inputs, large vision-language models\n(LVLMs) have shown impressive performance in tasks such as recognition and\nvisual question answering (VQA). Despite increasing interest in the utility of\nLLMs in causal reasoning tasks such as causal discovery and counterfactual\nreasoning, there has been relatively little work showcasing the abilities of\nLVLMs on visual causal reasoning tasks. We take this opportunity to formally\nintroduce a comprehensive causal reasoning benchmark for multi-modal in-context\nlearning from LVLMs. Our CausalVLBench encompasses three representative tasks:\ncausal structure inference, intervention target prediction, and counterfactual\nprediction. We evaluate the ability of state-of-the-art open-source LVLMs on\nour causal reasoning tasks across three causal representation learning datasets\nand demonstrate their fundamental strengths and weaknesses. We hope that our\nbenchmark elucidates the drawbacks of existing vision-language models and\nmotivates new directions and paradigms in improving the visual causal reasoning\nabilities of LVLMs."}
{"id": "2506.11035", "pdf": "https://arxiv.org/pdf/2506.11035.pdf", "abs": "https://arxiv.org/abs/2506.11035", "title": "Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity", "authors": ["Moussa Koulako Bala Doumbouya", "Dan Jurafsky", "Christopher D. Manning"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68", "I.2.0; I.2.4; I.2.6; I.2.7; I.4.7; I.4.10; I.5.1; F.1.1"], "comment": null, "summary": "Work in psychology has highlighted that the geometric model of similarity\nstandard in deep learning is not psychologically plausible because its metric\nproperties such as symmetry do not align with human perception. In contrast,\nTversky (1977) proposed an axiomatic theory of similarity based on a\nrepresentation of objects as sets of features, and their similarity as a\nfunction of common and distinctive features. However, this model has not been\nused in deep learning before, partly due to the challenge of incorporating\ndiscrete set operations. We develop a differentiable parameterization of\nTversky's similarity that is learnable through gradient descent, and derive\nneural network building blocks such as the Tversky projection layer, which\nunlike the linear projection layer can model non-linear functions such as XOR.\nThrough experiments with image recognition and language modeling, we show that\nthe Tversky projection layer is a beneficial replacement for the linear\nprojection layer, which employs geometric similarity. On the NABirds image\nclassification task, a frozen ResNet-50 adapted with a Tversky projection layer\nachieves a 24.7% relative accuracy improvement over the linear layer adapter\nbaseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases\nby 7.5%, and its parameter count by 34.8%. Finally, we propose a unified\ninterpretation of both projection layers as computing similarities of input\nstimuli to learned prototypes, for which we also propose a novel visualization\ntechnique highlighting the interpretability of Tversky projection layers. Our\nwork offers a new paradigm for thinking about the similarity model implicit in\ndeep learning, and designing networks that are interpretable under an\nestablished theory of psychological similarity."}
{"id": "2506.11040", "pdf": "https://arxiv.org/pdf/2506.11040.pdf", "abs": "https://arxiv.org/abs/2506.11040", "title": "Large Language models for Time Series Analysis: Techniques, Applications, and Challenges", "authors": ["Feifei Shi", "Xueyan Yin", "Kang Wang", "Wanyu Tu", "Qifu Sun", "Huansheng Ning"], "categories": ["cs.LG", "cs.CL", "cs.ET"], "comment": null, "summary": "Time series analysis is pivotal in domains like financial forecasting and\nbiomedical monitoring, yet traditional methods are constrained by limited\nnonlinear feature representation and long-term dependency capture. The\nemergence of Large Language Models (LLMs) offers transformative potential by\nleveraging their cross-modal knowledge integration and inherent attention\nmechanisms for time series analysis. However, the development of\ngeneral-purpose LLMs for time series from scratch is still hindered by data\ndiversity, annotation scarcity, and computational requirements. This paper\npresents a systematic review of pre-trained LLM-driven time series analysis,\nfocusing on enabling techniques, potential applications, and open challenges.\nFirst, it establishes an evolutionary roadmap of AI-driven time series\nanalysis, from the early machine learning era, through the emerging LLM-driven\nparadigm, to the development of native temporal foundation models. Second, it\norganizes and systematizes the technical landscape of LLM-driven time series\nanalysis from a workflow perspective, covering LLMs' input, optimization, and\nlightweight stages. Finally, it critically examines novel real-world\napplications and highlights key open challenges that can guide future research\nand innovation. The work not only provides valuable insights into current\nadvances but also outlines promising directions for future development. It\nserves as a foundational reference for both academic and industrial\nresearchers, paving the way for the development of more efficient,\ngeneralizable, and interpretable systems of LLM-driven time series analysis."}
{"id": "2506.11059", "pdf": "https://arxiv.org/pdf/2506.11059.pdf", "abs": "https://arxiv.org/abs/2506.11059", "title": "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs", "authors": ["Hanxi Guo", "Siyuan Cheng", "Kaiyuan Zhang", "Guangyu Shen", "Xiangyu Zhang"], "categories": ["cs.SE", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to modern software\ndevelopment, producing vast amounts of AI-generated source code. While these\nmodels boost programming productivity, their misuse introduces critical risks,\nincluding code plagiarism, license violations, and the propagation of insecure\nprograms. As a result, robust detection of AI-generated code is essential. To\nsupport the development of such detectors, a comprehensive benchmark that\nreflects real-world conditions is crucial. However, existing benchmarks fall\nshort -- most cover only a limited set of programming languages and rely on\nless capable generative models. In this paper, we present CodeMirage, a\ncomprehensive benchmark that addresses these limitations through three major\nadvancements: (1) it spans ten widely used programming languages, (2) includes\nboth original and paraphrased code samples, and (3) incorporates outputs from\nten state-of-the-art production-level LLMs, including both reasoning and\nnon-reasoning models from six major providers. Using CodeMirage, we evaluate\nten representative detectors across four methodological paradigms under four\nrealistic evaluation configurations, reporting results using three\ncomplementary metrics. Our analysis reveals nine key findings that uncover the\nstrengths and weaknesses of current detectors, and identify critical challenges\nfor future work. We believe CodeMirage offers a rigorous and practical testbed\nto advance the development of robust and generalizable AI-generated code\ndetectors."}
{"id": "2506.11064", "pdf": "https://arxiv.org/pdf/2506.11064.pdf", "abs": "https://arxiv.org/abs/2506.11064", "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding", "authors": ["Jiajun He", "Tomoki Toda"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted by IEEE TASLP 2025", "summary": "End-to-end automatic speech recognition (ASR) models often struggle to\naccurately recognize rare words. Previously, we introduced an ASR\npostprocessing method called error detection and context-aware error correction\n(ED-CEC), which leverages contextual information such as named entities and\ntechnical terms to improve the accuracy of ASR transcripts. Although ED-CEC\nachieves a notable success in correcting rare words, its accuracy remains low\nwhen dealing with rare words that have similar pronunciations but different\nspellings. To address this issue, we proposed a phoneme-augmented multimodal\nfusion method for context-aware error correction (PMF-CEC) method on the basis\nof ED-CEC, which allowed for better differentiation between target rare words\nand homophones. Additionally, we observed that the previous ASR error detection\nmodule suffers from overdetection. To mitigate this, we introduced a retention\nprobability mechanism to filter out editing operations with confidence scores\nbelow a set threshold, preserving the original operation to improve error\ndetection accuracy. Experiments conducted on five datasets demonstrated that\nour proposed PMF-CEC maintains reasonable inference speed while further\nreducing the biased word error rate compared with ED-CEC, showing a stronger\nadvantage in correcting homophones. Moreover, our method outperforms other\ncontextual biasing methods, and remains valuable compared with LLM-based\nmethods in terms of faster inference and better robustness under large biasing\nlists."}
{"id": "2506.11069", "pdf": "https://arxiv.org/pdf/2506.11069.pdf", "abs": "https://arxiv.org/abs/2506.11069", "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition", "authors": ["Tao Zhong", "Mengzhe Geng", "Shujie Hu", "Guinan Li", "Xunying Liu"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance."}
{"id": "2506.11072", "pdf": "https://arxiv.org/pdf/2506.11072.pdf", "abs": "https://arxiv.org/abs/2506.11072", "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "authors": ["Tahiya Chowdhury", "Veronica Romero"], "categories": ["eess.AS", "cs.CL", "cs.CY", "cs.SD", "stat.AP", "K.4; J.4; I.2"], "comment": "5 pages, 1 figure, 3 tables", "summary": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications."}
{"id": "2506.11079", "pdf": "https://arxiv.org/pdf/2506.11079.pdf", "abs": "https://arxiv.org/abs/2506.11079", "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts", "authors": ["Lingyun Gao", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "This paper is accepted to Interspeech 2025. This publication is part\n  of the project Responsible AI for Voice Diagnostics (RAIVD) with file number\n  NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which\n  is financed by the Dutch Research Council (NWO)", "summary": "Automatic reading aloud evaluation can provide valuable support to teachers\nby enabling more efficient scoring of reading exercises. However, research on\nreading evaluation systems and applications remains limited. We present a novel\nmultimodal approach that leverages audio and knowledge from text resources. In\nparticular, we explored the potential of using Whisper and instruction-tuned\nlarge language models (LLMs) with prompts to improve transcriptions for child\nspeech recognition, as well as their effectiveness in downstream reading\nmistake detection. Our results demonstrate the effectiveness of prompting\nWhisper and prompting LLM, compared to the baseline Whisper model without\nprompting. The best performing system achieved state-of-the-art recognition\nperformance in Dutch child read speech, with a word error rate (WER) of 5.1%,\nimproving the baseline WER of 9.4%. Furthermore, it significantly improved\nreading mistake detection, increasing the F1 score from 0.39 to 0.73."}
{"id": "2506.11085", "pdf": "https://arxiv.org/pdf/2506.11085.pdf", "abs": "https://arxiv.org/abs/2506.11085", "title": "LeanExplore: A search engine for Lean 4 declarations", "authors": ["Justin Asher"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO", "I.2.6; H.3.3; I.2.3"], "comment": "16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,\n  Code: https://github.com/justincasher/lean-explore", "summary": "The expanding Lean 4 ecosystem poses challenges for navigating its vast\nlibraries. This paper introduces LeanExplore, a search engine for Lean 4\ndeclarations. LeanExplore enables users to semantically search for statements,\nboth formally and informally, across select Lean 4 packages (including\nBatteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is\npowered by a hybrid ranking strategy, integrating scores from a multi-source\nsemantic embedding model (capturing conceptual meaning from formal Lean code,\ndocstrings, AI-generated informal translations, and declaration titles), BM25+\nfor keyword-based lexical relevance, and a PageRank-based score reflecting\ndeclaration importance and interconnectedness. The search engine is accessible\nvia a dedicated website (https://www.leanexplore.com/) and a Python API\n(https://github.com/justincasher/lean-explore). Furthermore, the database can\nbe downloaded, allowing users to self-host the service. LeanExplore integrates\neasily with LLMs via the model context protocol (MCP), enabling users to chat\nwith an AI assistant about Lean declarations or utilize the search engine for\nbuilding theorem-proving agents. This work details LeanExplore's architecture,\ndata processing, functionalities, and its potential to enhance Lean 4 workflows\nand AI-driven mathematical research"}
{"id": "2506.11087", "pdf": "https://arxiv.org/pdf/2506.11087.pdf", "abs": "https://arxiv.org/abs/2506.11087", "title": "ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models", "authors": ["Boya Xiong", "Shuo Wang", "Weifeng Ge", "Guanhua Chen", "Yun Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive performance on various\nknowledge-intensive and complex reasoning tasks in different domains. In\ncertain scenarios like multi-tenant serving, a large number of LLMs finetuned\nfrom the same base model are deployed to meet complex requirements for users.\nRecent works explore delta-compression approaches to quantize and compress the\ndelta parameters between the customized LLM and the corresponding base model.\nHowever, existing works either exhibit unsatisfactory performance at high\ncompression ratios or depend on empirical bit allocation schemes. In this work,\nwe propose ADAMIX, an effective adaptive mixed-precision delta-compression\nframework. We provide a mathematical derivation of quantization error to\nmotivate our mixed-precision compression strategy and formulate the optimal\nmixed-precision bit allocation scheme as the solution to a 0/1 integer linear\nprogramming problem. Our derived bit allocation strategy minimizes the\nquantization error while adhering to a predefined compression ratio\nrequirement. Experimental results on various models and benchmarks demonstrate\nthat our approach surpasses the best baseline by a considerable margin. On\ntasks like AIME2024 and GQA, where the norm of $\\Delta \\mathbf{W}$ is large and\nthe base model lacks sufficient ability, ADAMIX outperforms the best baseline\nDelta-CoMe by 22.3% and 6.1% with 7B models, respectively."}
{"id": "2506.11089", "pdf": "https://arxiv.org/pdf/2506.11089.pdf", "abs": "https://arxiv.org/abs/2506.11089", "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM", "authors": ["Jeena Prakash", "Blessingh Kumar", "Kadri Hacioglu", "Bidisha Sharma", "Sindhuja Gopalan", "Malolan Chetlur", "Shankar Venkatesan", "Andreas Stolcke"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Automatic speech recognition (ASR) models rely on high-quality transcribed\ndata for effective training. Generating pseudo-labels for large unlabeled audio\ndatasets often relies on complex pipelines that combine multiple ASR outputs\nthrough multi-stage processing, leading to error propagation, information loss\nand disjoint optimization. We propose a unified multi-ASR prompt-driven\nframework using postprocessing by either textual or speech-based large language\nmodels (LLMs), replacing voting or other arbitration logic for reconciling the\nensemble outputs. We perform a comparative study of multiple architectures with\nand without LLMs, showing significant improvements in transcription accuracy\ncompared to traditional methods. Furthermore, we use the pseudo-labels\ngenerated by the various approaches to train semi-supervised ASR models for\ndifferent datasets, again showing improved performance with textual and\nspeechLLM transcriptions compared to baselines."}
{"id": "2506.11096", "pdf": "https://arxiv.org/pdf/2506.11096.pdf", "abs": "https://arxiv.org/abs/2506.11096", "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting", "authors": ["Guillaume Wisniewski", "Séverine Guillaume", "Clara Rosina Fernández"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong\nanisotropy, leading to high similarity between random embeddings. While widely\nobserved, the impact of this property on downstream tasks remains unclear. This\nwork evaluates anisotropy in keyword spotting for computational documentary\nlinguistics. Using Dynamic Time Warping, we show that despite anisotropy,\nwav2vec2 similarity measures effectively identify words without transcription.\nOur results highlight the robustness of these representations, which capture\nphonetic structures and generalize across speakers. Our results underscore the\nimportance of pretraining in learning rich and invariant speech\nrepresentations."}
{"id": "2506.11099", "pdf": "https://arxiv.org/pdf/2506.11099.pdf", "abs": "https://arxiv.org/abs/2506.11099", "title": "Knowledge Graph Embeddings with Representing Relations as Annular Sectors", "authors": ["Huiling Zhu", "Yingqi Zeng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Knowledge graphs (KGs), structured as multi-relational data of entities and\nrelations, are vital for tasks like data analysis and recommendation systems.\nKnowledge graph completion (KGC), or link prediction, addresses incompleteness\nof KGs by inferring missing triples (h, r, t). It is vital for downstream\napplications. Region-based embedding models usually embed entities as points\nand relations as geometric regions to accomplish the task. Despite progress,\nthese models often overlook semantic hierarchies inherent in entities. To solve\nthis problem, we propose SectorE, a novel embedding model in polar coordinates.\nRelations are modeled as annular sectors, combining modulus and phase to\ncapture inference patterns and relation attributes. Entities are embedded as\npoints within these sectors, intuitively encoding hierarchical structure.\nEvaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive\nperformance against various kinds of models, demonstrating strengths in\nsemantic modeling capability."}
{"id": "2506.11221", "pdf": "https://arxiv.org/pdf/2506.11221.pdf", "abs": "https://arxiv.org/abs/2506.11221", "title": "LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic", "authors": ["Weibing Zheng", "Laurah Turner", "Jess Kropczynski", "Murat Ozer", "Tri Nguyen", "Shane Halse"], "categories": ["cs.AI", "cs.CL", "cs.LO", "D.2.4; K.3.1; C.3; I.2.6"], "comment": "12 pages, 1 figure, 2025 IFSA World Congress NAFIPS Annual Meeting", "summary": "Clinical communication skills are critical in medical education, and\npracticing and assessing clinical communication skills on a scale is\nchallenging. Although LLM-powered clinical scenario simulations have shown\npromise in enhancing medical students' clinical practice, providing automated\nand scalable clinical evaluation that follows nuanced physician judgment is\ndifficult. This paper combines fuzzy logic and Large Language Model (LLM) and\nproposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the\nautomated evaluation of medical students' clinical skills with subjective\nphysicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is\nfine-tuned to evaluate medical students' utterances within student-AI patient\nconversation scripts based on human annotations from four fuzzy sets, including\nProfessionalism, Medical Relevance, Ethical Behavior, and Contextual\nDistraction. The methodology of this paper started from data collection from\nthe LLM-powered medical education system, data annotation based on\nmultidimensional fuzzy sets, followed by prompt engineering and the supervised\nfine-tuning (SFT) of the pre-trained LLMs using these human annotations. The\nresults show that the LLM-as-a-Fuzzy-Judge achieves over 80\\% accuracy, with\nmajor criteria items over 90\\%, effectively leveraging fuzzy logic and LLM as a\nsolution to deliver interpretable, human-aligned assessment. This work suggests\nthe viability of leveraging fuzzy logic and LLM to align with human\npreferences, advances automated evaluation in medical education, and supports\nmore robust assessment and judgment practices. The GitHub repository of this\nwork is available at https://github.com/2sigmaEdTech/LLMAsAJudge"}
{"id": "2506.11237", "pdf": "https://arxiv.org/pdf/2506.11237.pdf", "abs": "https://arxiv.org/abs/2506.11237", "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation", "authors": ["Ngoc Phuoc An Vo", "Brent Paulovicks", "Vadim Sheinin"], "categories": ["cs.SE", "cs.CL"], "comment": "10 pages", "summary": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement."}
{"id": "2506.11350", "pdf": "https://arxiv.org/pdf/2506.11350.pdf", "abs": "https://arxiv.org/abs/2506.11350", "title": "GLAP: General contrastive audio-text pretraining across domains and languages", "authors": ["Heinrich Dinkel", "Zhiyong Yan", "Tianzi Wang", "Yongqing Wang", "Xingwei Sun", "Yadong Niu", "Jizhong Liu", "Gang Li", "Junbo Zhang", "Jian Luan"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Contrastive Language Audio Pretraining (CLAP) is a widely-used method to\nbridge the gap between audio and text domains. Current CLAP methods enable\nsound and music retrieval in English, ignoring multilingual spoken content. To\naddress this, we introduce general language audio pretraining (GLAP), which\nexpands CLAP with multilingual and multi-domain abilities. GLAP demonstrates\nits versatility by achieving competitive performance on standard audio-text\nretrieval benchmarks like Clotho and AudioCaps, while significantly surpassing\nexisting methods in speech retrieval and classification tasks. Additionally,\nGLAP achieves strong results on widely used sound-event zero-shot benchmarks,\nwhile simultaneously outperforming previous methods on speech content\nbenchmarks. Further keyword spotting evaluations across 50 languages emphasize\nGLAP's advanced multilingual capabilities. Finally, multilingual sound and\nmusic understanding is evaluated across four languages. Checkpoints and Source:\nhttps://github.com/xiaomi-research/dasheng-glap."}
{"id": "2506.11375", "pdf": "https://arxiv.org/pdf/2506.11375.pdf", "abs": "https://arxiv.org/abs/2506.11375", "title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables", "authors": ["Yitong Zhou", "Mingyue Cheng", "Qingyang Mao", "Yucong Luo", "Qi Liu", "Yupeng Li", "Xiaohan Zhang", "Deguang Liu", "Xin Li", "Enhong Chen"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Chemical tables encode complex experimental knowledge through symbolic\nexpressions, structured variables, and embedded molecular graphics. Existing\nbenchmarks largely overlook this multimodal and domain-specific complexity,\nlimiting the ability of multimodal large language models to support scientific\nunderstanding in chemistry. In this work, we introduce ChemTable, a large-scale\nbenchmark of real-world chemical tables curated from the experimental sections\nof literature. ChemTable includes expert-annotated cell polygons, logical\nlayouts, and domain-specific labels, including reagents, catalysts, yields, and\ngraphical components and supports two core tasks: (1) Table Recognition,\ncovering structure parsing and content extraction; and (2) Table Understanding,\nencompassing both descriptive and reasoning-oriented question answering\ngrounded in table structure and domain semantics. We evaluated a range of\nrepresentative multimodal models, including both open-source and closed-source\nmodels, on ChemTable and reported a series of findings with practical and\nconceptual insights. Although models show reasonable performance on basic\nlayout parsing, they exhibit substantial limitations on both descriptive and\ninferential QA tasks compared to human performance, and we observe significant\nperformance gaps between open-source and closed-source models across multiple\ndimensions. These results underscore the challenges of chemistry-aware table\nunderstanding and position ChemTable as a rigorous and realistic benchmark for\nadvancing scientific reasoning."}
{"id": "2506.11376", "pdf": "https://arxiv.org/pdf/2506.11376.pdf", "abs": "https://arxiv.org/abs/2506.11376", "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning", "authors": ["Liying Wang", "Ph. D.", "Daffodil Carrington", "M. S.", "Daniil Filienko", "M. S.", "Caroline El Jazmi", "M. S.", "Serena Jinchen Xie", "M. S.", "Martine De Cock", "Ph. D.", "Sarah Iribarren", "Ph. D.", "Weichao Yuwen", "Ph. D"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Family caregivers often face substantial mental health challenges due to\ntheir multifaceted roles and limited resources. This study explored the\npotential of a large language model (LLM)-powered conversational agent to\ndeliver evidence-based mental health support for caregivers, specifically\nProblem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)\nand Behavioral Chain Analysis (BCA). A within-subject experiment was conducted\nwith 28 caregivers interacting with four LLM configurations to evaluate empathy\nand therapeutic alliance. The best-performing models incorporated Few-Shot and\nRetrieval-Augmented Generation (RAG) prompting techniques, alongside\nclinician-curated examples. The models showed improved contextual understanding\nand personalized support, as reflected by qualitative responses and\nquantitative ratings on perceived empathy and therapeutic alliances.\nParticipants valued the model's ability to validate emotions, explore\nunexpressed feelings, and provide actionable strategies. However, balancing\nthorough assessment with efficient advice delivery remains a challenge. This\nwork highlights the potential of LLMs in delivering empathetic and tailored\nsupport for family caregivers."}
{"id": "2506.11402", "pdf": "https://arxiv.org/pdf/2506.11402.pdf", "abs": "https://arxiv.org/abs/2506.11402", "title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model", "authors": ["Pradyut Sekhsaria", "Marcel Mateos Salles", "Hai Huang", "Randall Balestriero"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 16 figures, 15 tables. Submitted for publication. for\n  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/", "summary": "Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),\naligns pre-trained Large Language Models (LLMs) to particular downstream tasks\nin a resource-efficient manner. Because efficiency has been the main metric of\nprogress, very little attention has been put in understanding possible\ncatastrophic failures. We uncover one such failure: PEFT encourages a model to\nsearch for shortcut solutions to solve its fine-tuning tasks. When very small\namount of tokens, e.g., one token per prompt, are correlated with downstream\ntask classes, PEFT makes any pretrained model rely predominantly on that token\nfor decision making. While such spurious tokens may emerge accidentally from\nincorrect data cleaning, it also opens opportunities for malevolent parties to\ncontrol a model's behavior from Seamless Spurious Token Injection (SSTI). In\nSSTI, a small amount of tokens correlated with downstream classes are injected\nby the dataset creators. At test time, the finetuned LLM's behavior can be\ncontrolled solely by injecting those few tokens. We apply SSTI across models\nfrom three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and\nfour diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias\nin Bios). Our findings reveal three astonishing behaviors. First, as few as a\nsingle token of SSTI is sufficient to steer a model's decision making. Second,\nfor light SSTI, the reliance on spurious tokens is proportional to the LoRA\nrank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable\nto small rank values as it makes the model attend to non-spurious tokens, hence\nimproving robustness."}
{"id": "2506.11415", "pdf": "https://arxiv.org/pdf/2506.11415.pdf", "abs": "https://arxiv.org/abs/2506.11415", "title": "Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs", "authors": ["Linlin Wang", "Tianqing Zhu", "Laiqiao Qin", "Longxiang Gao", "Wanlei Zhou"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "In Large Language Models, Retrieval-Augmented Generation (RAG) systems can\nsignificantly enhance the performance of large language models by integrating\nexternal knowledge. However, RAG also introduces new security risks. Existing\nresearch focuses mainly on how poisoning attacks in RAG systems affect model\noutput quality, overlooking their potential to amplify model biases. For\nexample, when querying about domestic violence victims, a compromised RAG\nsystem might preferentially retrieve documents depicting women as victims,\ncausing the model to generate outputs that perpetuate gender stereotypes even\nwhen the original query is gender neutral. To show the impact of the bias, this\npaper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which\nsystematically investigates attack pathways that amplify language model biases\nthrough a RAG system manipulation. We design an adversarial document generation\nmethod based on multi-objective reward functions, employ subspace projection\ntechniques to manipulate retrieval results, and construct a cyclic feedback\nmechanism for continuous bias amplification. Experiments on multiple mainstream\nlarge language models demonstrate that BRRA attacks can significantly enhance\nmodel biases in dimensions. In addition, we explore a dual stage defense\nmechanism to effectively mitigate the impacts of the attack. This study reveals\nthat poisoning attacks in RAG systems directly amplify model output biases and\nclarifies the relationship between RAG system security and model fairness. This\nnovel potential attack indicates that we need to keep an eye on the fairness\nissues of the RAG system."}
{"id": "2506.11475", "pdf": "https://arxiv.org/pdf/2506.11475.pdf", "abs": "https://arxiv.org/abs/2506.11475", "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "categories": ["cs.MA", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution."}
{"id": "2506.11515", "pdf": "https://arxiv.org/pdf/2506.11515.pdf", "abs": "https://arxiv.org/abs/2506.11515", "title": "Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs", "authors": ["Xiao Xu", "Libo Qin", "Wanxiang Che", "Min-Yen Kan"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266", "summary": "Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower."}
{"id": "2506.11516", "pdf": "https://arxiv.org/pdf/2506.11516.pdf", "abs": "https://arxiv.org/abs/2506.11516", "title": "Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning", "authors": ["Chengye Li", "Haiyun Liu", "Yuanxi Li"], "categories": ["cs.LG", "cs.CL"], "comment": "10 main pages, 10 page appendix", "summary": "In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection."}
{"id": "2506.11555", "pdf": "https://arxiv.org/pdf/2506.11555.pdf", "abs": "https://arxiv.org/abs/2506.11555", "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning", "authors": ["Yu Wang", "Shiwan Zhao", "Ming Fan", "Zhihu Wang", "Yubo Zhang", "Xicheng Zhang", "Zhengfan Wang", "Heyuan Huang", "Ting Liu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs."}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558.pdf", "abs": "https://arxiv.org/abs/2506.11558", "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."}
{"id": "2506.11604", "pdf": "https://arxiv.org/pdf/2506.11604.pdf", "abs": "https://arxiv.org/abs/2506.11604", "title": "VLM@school -- Evaluation of AI image understanding on German middle school knowledge", "authors": ["René Peinl", "Vincent Tischler"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems."}
{"id": "2506.11620", "pdf": "https://arxiv.org/pdf/2506.11620.pdf", "abs": "https://arxiv.org/abs/2506.11620", "title": "(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test", "authors": ["Stefan Bleeck"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials."}
{"id": "2506.11737", "pdf": "https://arxiv.org/pdf/2506.11737.pdf", "abs": "https://arxiv.org/abs/2506.11737", "title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model", "authors": ["Dinh Viet Cuong", "Hoang-Bao Le", "An Pham Ngoc Nguyen", "Liting Zhou", "Cathal Gurrin"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova."}
{"id": "2506.11812", "pdf": "https://arxiv.org/pdf/2506.11812.pdf", "abs": "https://arxiv.org/abs/2506.11812", "title": "On the Performance of LLMs for Real Estate Appraisal", "authors": ["Margot Geerts", "Manon Reusens", "Bart Baesens", "Seppe vanden Broucke", "Jochen De Weerdt"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ECML-PKDD 2025", "summary": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders."}
{"id": "2506.11820", "pdf": "https://arxiv.org/pdf/2506.11820.pdf", "abs": "https://arxiv.org/abs/2506.11820", "title": "Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation", "authors": ["Xintong Wang", "Jingheng Pan", "Yixiao Liu", "Xiaohu Zhao", "Chenyang Lyu", "Minghao Wu", "Chris Biemann", "Longyue Wang", "Linlong Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability."}
{"id": "2506.11880", "pdf": "https://arxiv.org/pdf/2506.11880.pdf", "abs": "https://arxiv.org/abs/2506.11880", "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment", "authors": ["Alejandro Peña", "Julian Fierrez", "Aythami Morales", "Gonzalo Mancera", "Miguel Lopez", "Ruben Tolosana"], "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to AIES 2025 (Under Review)", "summary": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data."}
{"id": "2506.11887", "pdf": "https://arxiv.org/pdf/2506.11887.pdf", "abs": "https://arxiv.org/abs/2506.11887", "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making", "authors": ["Claudio Fanconi", "Mihaela van der Schaar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions."}
{"id": "2506.11902", "pdf": "https://arxiv.org/pdf/2506.11902.pdf", "abs": "https://arxiv.org/abs/2506.11902", "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search", "authors": ["Zhenyu Hou", "Ziniu Hu", "Yujiang Li", "Rui Lu", "Jie Tang", "Yuxiao Dong"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL."}
{"id": "2506.11928", "pdf": "https://arxiv.org/pdf/2506.11928.pdf", "abs": "https://arxiv.org/abs/2506.11928", "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page at https://livecodebenchpro.com/", "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning."}
{"id": "2506.11986", "pdf": "https://arxiv.org/pdf/2506.11986.pdf", "abs": "https://arxiv.org/abs/2506.11986", "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task", "authors": ["Wuzhenghong Wen", "Su Pan", "yuwei Sun"], "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "11 pages, 3 figures, conference", "summary": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/."}
{"id": "2506.11991", "pdf": "https://arxiv.org/pdf/2506.11991.pdf", "abs": "https://arxiv.org/abs/2506.11991", "title": "VGR: Visual Grounded Reasoning", "authors": ["Jiacong Wang", "Zijiang Kang", "Haochen Wang", "Haiyong Jiang", "Jiawen Li", "Bohong Wu", "Ya Wang", "Jiao Ran", "Xiao Liang", "Chao Feng", "Jun Xiao"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "9 pages, 4 figures", "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA."}
{"id": "2506.11999", "pdf": "https://arxiv.org/pdf/2506.11999.pdf", "abs": "https://arxiv.org/abs/2506.11999", "title": "Generative Representational Learning of Foundation Models for Recommendation", "authors": ["Zheli Zhou", "Chenxu Zhu", "Jianghao Lin", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "categories": ["cs.IR", "cs.CL"], "comment": "Project page is available at https://junkfood436.github.io/RecFound/", "summary": "Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines."}
{"id": "2405.04065", "pdf": "https://arxiv.org/pdf/2405.04065.pdf", "abs": "https://arxiv.org/abs/2405.04065", "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference", "authors": ["Runheng Liu", "Xingchen Xiao", "Heyan Huang", "Zewen Chi", "Zhijing Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings, 14 pages", "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."}
{"id": "2406.02050", "pdf": "https://arxiv.org/pdf/2406.02050.pdf", "abs": "https://arxiv.org/abs/2406.02050", "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models", "authors": ["Hitomi Yanaka", "Namgi Han", "Ryoma Kumon", "Jie Lu", "Masashi Takeshita", "Ryo Sekizawa", "Taisei Kato", "Hiromi Arai"], "categories": ["cs.CL"], "comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025", "summary": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data."}
{"id": "2406.14023", "pdf": "https://arxiv.org/pdf/2406.14023.pdf", "abs": "https://arxiv.org/abs/2406.14023", "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective", "authors": ["Yuchen Wen", "Keping Bi", "Wei Chen", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/."}
{"id": "2410.11042", "pdf": "https://arxiv.org/pdf/2410.11042.pdf", "abs": "https://arxiv.org/abs/2410.11042", "title": "Persistent Topological Features in Large Language Models", "authors": ["Yuri Gardinazzi", "Karthik Viswanathan", "Giada Panerai", "Alessio Ansuini", "Alberto Cazzaniga", "Matteo Biagetti"], "categories": ["cs.CL", "cs.CG", "cs.LG"], "comment": "10+17 pages, 17 figures, 3 tables. Accepted as poster at ICML 2025", "summary": "Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework - zigzag persistence from topological\ndata analysis - with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective."}
{"id": "2410.20445", "pdf": "https://arxiv.org/pdf/2410.20445.pdf", "abs": "https://arxiv.org/abs/2410.20445", "title": "TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models", "authors": ["Yuwei Du", "Jie Feng", "Jie Zhao", "Jian Yuan", "Yong Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent", "summary": "Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods."}
{"id": "2411.15694", "pdf": "https://arxiv.org/pdf/2411.15694.pdf", "abs": "https://arxiv.org/abs/2411.15694", "title": "Deep Sparse Latent Feature Models for Knowledge Graph Completion", "authors": ["Haotian Li", "Rui Zhang", "Lingzhi Wang", "Bin Yu", "Youwei Wang", "Yuliang Wei", "Kai Wang", "Richard Yi Da Xu", "Bailing Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in knowledge graph completion (KGC) have emphasized\ntext-based approaches to navigate the inherent complexities of large-scale\nknowledge graphs (KGs). While these methods have achieved notable progress,\nthey frequently struggle to fully incorporate the global structural properties\nof the graph. Stochastic blockmodels (SBMs), especially the latent feature\nrelational model (LFRM), offer robust probabilistic frameworks for identifying\nlatent community structures and improving link prediction. This paper presents\na novel probabilistic KGC framework utilizing sparse latent feature models,\noptimized via a deep variational autoencoder (VAE). Our proposed method\ndynamically integrates global clustering information with local textual\nfeatures to effectively complete missing triples, while also providing enhanced\ninterpretability of the underlying latent structures. Extensive experiments on\nfour benchmark datasets with varying scales demonstrate the significant\nperformance gains achieved by our method."}
{"id": "2412.21015", "pdf": "https://arxiv.org/pdf/2412.21015.pdf", "abs": "https://arxiv.org/abs/2412.21015", "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets", "authors": ["Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 (Demo)", "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."}
{"id": "2501.03479", "pdf": "https://arxiv.org/pdf/2501.03479.pdf", "abs": "https://arxiv.org/abs/2501.03479", "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?", "authors": ["Sourabrata Mukherjee", "Atharva Mehta", "Soumya Teotia", "Sougata Saha", "Akhil Arora", "Monojit Choudhury"], "categories": ["cs.CL"], "comment": "Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)", "summary": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage."}
{"id": "2502.01220", "pdf": "https://arxiv.org/pdf/2502.01220.pdf", "abs": "https://arxiv.org/abs/2502.01220", "title": "Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations", "authors": ["Hichem Ammar Khodja", "Frédéric Béchet", "Quentin Brabant", "Alexis Nasr", "Gwénolé Lecorvé"], "categories": ["cs.CL", "cs.LG"], "comment": "preprint v5, accepted for publication at ACL 2025 - L2M2 Workshop", "summary": "This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\nLMs' ability to distinguish is analyzed along two dimensions: the distance of\nthe incorrect context from the validity period and the granularity of the\ncontext. To this end, a dataset called TimeStress is introduced, enabling the\nevaluation of 18 diverse LMs. Results reveal that the best LM achieves a\nperfect distinction for only 11% of the studied facts, with errors, certainly\nrare, but critical that humans would not make. This work highlights the\nlimitations of current LMs in temporal representation."}
{"id": "2502.01925", "pdf": "https://arxiv.org/pdf/2502.01925.pdf", "abs": "https://arxiv.org/abs/2502.01925", "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling", "authors": ["Avery Ma", "Yangchen Pan", "Amir-massoud Farahmand"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": "Accepted at ICML 2025 (Spotlight). Code:\n  https://github.com/averyma/pandas", "summary": "Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting\ntheir ability to process long input sequences. To achieve this, the malicious\ntarget prompt is prefixed with hundreds of fabricated conversational exchanges\nbetween the user and the model. These exchanges are randomly sampled from a\npool of unsafe question-answer pairs, making it appear as though the model has\nalready complied with harmful instructions. In this paper, we present PANDAS: a\nhybrid technique that improves many-shot jailbreaking by modifying these\nfabricated dialogues with Positive Affirmations, Negative Demonstrations, and\nan optimized Adaptive Sampling method tailored to the target prompt's topic. We\nalso introduce ManyHarm, a dataset of harmful question-answer pairs, and\ndemonstrate through extensive experiments that PANDAS significantly outperforms\nbaseline methods in long-context scenarios. Through attention analysis, we\nprovide insights into how long-context vulnerabilities are exploited and show\nhow PANDAS further improves upon many-shot jailbreaking."}
{"id": "2502.11020", "pdf": "https://arxiv.org/pdf/2502.11020.pdf", "abs": "https://arxiv.org/abs/2502.11020", "title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages", "authors": ["Jafar Isbarov", "Arofat Akhundjanova", "Mammad Hajili", "Kavsar Huseynova", "Dmitry Gaynullin", "Anar Rzayev", "Osman Tursun", "Aizirek Turdubaeva", "Ilshat Saetov", "Rinat Kharisov", "Saule Belginova", "Ariana Kenbayeva", "Amina Alisheva", "Abdullatif Köksal", "Samir Rustamov", "Duygu Ataman"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Main Conference", "summary": "Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts."}
{"id": "2502.11812", "pdf": "https://arxiv.org/pdf/2502.11812.pdf", "abs": "https://arxiv.org/abs/2502.11812", "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis", "authors": ["Xu Wang", "Yan Hu", "Wenyu Du", "Reynold Cheng", "Benyou Wang", "Difan Zou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms."}
{"id": "2502.19110", "pdf": "https://arxiv.org/pdf/2502.19110.pdf", "abs": "https://arxiv.org/abs/2502.19110", "title": "Conformal Linguistic Calibration: Trading-off between Factuality and Specificity", "authors": ["Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity."}
{"id": "2502.19175", "pdf": "https://arxiv.org/pdf/2502.19175.pdf", "abs": "https://arxiv.org/abs/2502.19175", "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis", "authors": ["Daniel Rose", "Chia-Chien Hung", "Marco Lepri", "Israa Alqassem", "Kiril Gashteovski", "Carolin Lawrence"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (main)", "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process."}
{"id": "2502.20273", "pdf": "https://arxiv.org/pdf/2502.20273.pdf", "abs": "https://arxiv.org/abs/2502.20273", "title": "How Much is Enough? The Diminishing Returns of Tokenization Training Data", "authors": ["Varshini Reddy", "Craig W. Schmidt", "Yuval Pinter", "Chris Tanner"], "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms."}
{"id": "2503.06706", "pdf": "https://arxiv.org/pdf/2503.06706.pdf", "abs": "https://arxiv.org/abs/2503.06706", "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts", "authors": ["Ming Zhang", "Yuhui Wang", "Yujiong Shen", "Tingyi Yang", "Changhao Jiang", "Yilong Wu", "Shihan Dou", "Qinhao Chen", "Zhiheng Xi", "Zhihao Zhang", "Yi Dong", "Zhen Wang", "Zhihui Fei", "Mingyang Wan", "Tao Liang", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial."}
{"id": "2503.09347", "pdf": "https://arxiv.org/pdf/2503.09347.pdf", "abs": "https://arxiv.org/abs/2503.09347", "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts", "authors": ["Hongyu Chen", "Seraphina Goldfarb-Tarrant"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025", "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."}
{"id": "2503.21227", "pdf": "https://arxiv.org/pdf/2503.21227.pdf", "abs": "https://arxiv.org/abs/2503.21227", "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models", "authors": ["Hengyuan Zhao", "Ziqin Wang", "Qixin Sun", "Kaiyou Song", "Yilin Li", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon."}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673.pdf", "abs": "https://arxiv.org/abs/2504.11673", "title": "Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses to various\nsurveys and polls. However, the questions in these surveys usually reflect\nsocially understood attitudes: the patterns of attitudes of old/young,\nliberal/conservative, as understood by both members and non-members of those\ngroups. It is not clear whether the LLM binding is \\emph{deep}, meaning the LLM\nanswers as a member of a particular in-group would, or \\emph{shallow}, meaning\nthe LLM responds as an out-group member believes an in-group member would. To\nexplore this difference, we use questions that expose known in-group/out-group\nbiases. This level of fidelity is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies of in-group/out-group biases. Altogether, our\nwork extends the applicability of LLMs beyond estimating socially understood\nresponses, enabling their use in a broader range of human studies."}
{"id": "2504.13439", "pdf": "https://arxiv.org/pdf/2504.13439.pdf", "abs": "https://arxiv.org/abs/2504.13439", "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model", "authors": ["Grace Byun", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Evaluating generative models with open-ended generation is challenging due to\ninconsistencies in response formats. Multiple-choice (MC) evaluation mitigates\nthis issue, but generating high-quality distractors is time-consuming and\nlabor-intensive. We introduce D-GEN, the first open-source distractor generator\nmodel that transforms open-ended data into an MC format. To evaluate distractor\nquality, we propose two novel methods: (1) ranking alignment, ensuring\ngenerated distractors retain the discriminatory power of ground-truth\ndistractors, and (2) entropy analysis, comparing model confidence\ndistributions. Our results show that D-GEN preserves ranking consistency\n(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy\ndistribution of ground-truth distractors. Human evaluation further confirms the\nfluency, coherence, distractiveness, and incorrectness. Our work advances\nrobust and efficient distractor generation with automated evaluation, setting a\nnew standard for MC evaluation."}
{"id": "2504.13615", "pdf": "https://arxiv.org/pdf/2504.13615.pdf", "abs": "https://arxiv.org/abs/2504.13615", "title": "Long-context Non-factoid Question Answering in Indic Languages", "authors": ["Ritwik Mishra", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "categories": ["cs.CL"], "comment": "Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/", "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."}
{"id": "2504.14218", "pdf": "https://arxiv.org/pdf/2504.14218.pdf", "abs": "https://arxiv.org/abs/2504.14218", "title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "authors": ["Junchi Yao", "Shu Yang", "Jianhua Xu", "Lijie Hu", "Mengdi Li", "Di Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025, Findings, Long Paper", "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse. The source code of our work is publicly\navailable at: https://github.com/kaustpradalab/repeat-curse-llm"}
{"id": "2504.18415", "pdf": "https://arxiv.org/pdf/2504.18415.pdf", "abs": "https://arxiv.org/abs/2504.18415", "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs", "authors": ["Hongyu Wang", "Shuming Ma", "Furu Wei"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415.pdf", "abs": "https://arxiv.org/abs/2505.12415", "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660.pdf", "abs": "https://arxiv.org/abs/2505.16660", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Chang Liu", "Dongbo Wang", "Liu liu", "Zhixiao Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models."}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076.pdf", "abs": "https://arxiv.org/abs/2505.17076", "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Leibny Paola Garcia", "Eng Siong Chng"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "6 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications."}
{"id": "2505.20813", "pdf": "https://arxiv.org/pdf/2505.20813.pdf", "abs": "https://arxiv.org/abs/2505.20813", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "summary": "In knowledge graph embedding, leveraging relation specific entity\ntransformation has markedly enhanced performance. However, the consistency of\nembedding differences before and after transformation remains unaddressed,\nrisking the loss of valuable inductive bias inherent in the embeddings. This\ninconsistency stems from two problems. First, transformation representations\nare specified for relations in a disconnected manner, allowing dissimilar\ntransformations and corresponding entity embeddings for similar relations.\nSecond, a generalized plug-in approach as a SFBR (Semantic Filter Based on\nRelations) disrupts this consistency through excessive concentration of entity\nembeddings under entity-based regularization, generating indistinguishable\nscore distributions among relations. In this paper, we introduce a plug-in KGE\nmethod, Relation-Semantics Consistent Filter (RSCF). Its entity transformation\nhas three features for enhancing semantic consistency: 1) shared affine\ntransformation of relation embeddings across all relations, 2) rooted entity\ntransformation that adds an entity embedding to its change represented by the\ntransformed vector, and 3) normalization of the change to prevent scale\nreduction. To amplify the advantages of consistency that preserve semantics on\nembeddings, RSCF adds relation transformation and prediction modules for\nenhancing the semantics. In knowledge graph completion tasks with\ndistance-based and tensor decomposition models, RSCF significantly outperforms\nstate-of-the-art KGE methods, showing robustness across all relations and their\nfrequencies."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657.pdf", "abs": "https://arxiv.org/abs/2505.21657", "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "authors": ["Zeinab Dehghani", "Mohammed Naveed Akram", "Koorosh Aslansefat", "Adil Khan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.23252", "pdf": "https://arxiv.org/pdf/2505.23252.pdf", "abs": "https://arxiv.org/abs/2505.23252", "title": "Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers", "authors": ["Bing Ma", "Hai Zhuge"], "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism."}
{"id": "2506.00637", "pdf": "https://arxiv.org/pdf/2506.00637.pdf", "abs": "https://arxiv.org/abs/2506.00637", "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics", "authors": ["Lorenzo Jaime Yu Flores", "Ori Ernst", "Jackie Chi Kit Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets."}
{"id": "2506.01305", "pdf": "https://arxiv.org/pdf/2506.01305.pdf", "abs": "https://arxiv.org/abs/2506.01305", "title": "VM14K: First Vietnamese Medical Benchmark", "authors": ["Thong Nguyen", "Duc Nguyen", "Minh Dang", "Thai Dao", "Long Nguyen", "Quan H. Nguyen", "Dat Nguyen", "Kien Tran", "Minh Tran"], "categories": ["cs.CL"], "comment": null, "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain."}
{"id": "2506.01602", "pdf": "https://arxiv.org/pdf/2506.01602.pdf", "abs": "https://arxiv.org/abs/2506.01602", "title": "Word Sense Detection Leveraging Maximum Mean Discrepancy", "authors": ["Kensuke Mitsuzawa"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach."}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078.pdf", "abs": "https://arxiv.org/abs/2506.04078", "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."}
{"id": "2506.06266", "pdf": "https://arxiv.org/pdf/2506.06266.pdf", "abs": "https://arxiv.org/abs/2506.06266", "title": "Cartridges: Lightweight and general-purpose long context representations via self-study", "authors": ["Sabri Eyuboglu", "Ryan Ehrlich", "Simran Arora", "Neel Guha", "Dylan Zinsley", "Emily Liu", "Will Tennien", "Atri Rudra", "James Zou", "Azalia Mirhoseini", "Christopher Re"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044.pdf", "abs": "https://arxiv.org/abs/2506.07044", "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is\n  https://alibaba-damo-academy.github.io/lingshu/", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ..."}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992.pdf", "abs": "https://arxiv.org/abs/2506.09992", "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities."}
{"id": "2506.10848", "pdf": "https://arxiv.org/pdf/2506.10848.pdf", "abs": "https://arxiv.org/abs/2506.10848", "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles", "authors": ["Qingyan Wei", "Yaojie Zhang", "Zhiyuan Liu", "Dongrui Liu", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "11 pages; 5 figures;", "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."}
{"id": "2403.07910", "pdf": "https://arxiv.org/pdf/2403.07910.pdf", "abs": "https://arxiv.org/abs/2403.07910", "title": "MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions", "authors": ["Tomáš Horych", "Martin Wessel", "Jan Philip Wahle", "Terry Ruas", "Jerome Waßmuth", "André Greiner-Petter", "Akiko Aizawa", "Bela Gipp", "Timo Spinde"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Media bias detection poses a complex, multifaceted problem traditionally\ntackled using single-task models and small in-domain datasets, consequently\nlacking generalizability. To address this, we introduce MAGPIE, the first\nlarge-scale multi-task pre-training approach explicitly tailored for media bias\ndetection. To enable pre-training at scale, we present Large Bias Mixture\n(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous\napproaches in media bias detection on the Bias Annotation By Experts (BABE)\ndataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs\nbetter than previous models on 5 out of 8 tasks in the Media Bias\nIdentification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%\nof finetuning steps compared to single-task approaches. Our evaluation shows,\nfor instance, that tasks like sentiment and emotionality boost all learning,\nall tasks enhance fake news detection, and scaling tasks leads to the best\nresults. MAGPIE confirms that MTL is a promising approach for addressing media\nbias detection, enhancing the accuracy and efficiency of existing models.\nFurthermore, LBM is the first available resource collection focused on media\nbias MTL."}
{"id": "2406.09459", "pdf": "https://arxiv.org/pdf/2406.09459.pdf", "abs": "https://arxiv.org/abs/2406.09459", "title": "Ad Auctions for LLMs via Retrieval Augmented Generation", "authors": ["MohammadTaghi Hajiaghayi", "Sébastien Lahaie", "Keivan Rezaei", "Suho Shin"], "categories": ["cs.GT", "cs.AI", "cs.CL", "cs.LG"], "comment": "NeurIPS 2024", "summary": "In the field of computational advertising, the integration of ads into the\noutputs of large language models (LLMs) presents an opportunity to support\nthese services without compromising content integrity. This paper introduces\nnovel auction mechanisms for ad allocation and pricing within the textual\noutputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a\nsegment auction where an ad is probabilistically retrieved for each discourse\nsegment (paragraph, section, or entire output) according to its bid and\nrelevance, following the RAG framework, and priced according to competing bids.\nWe show that our auction maximizes logarithmic social welfare, a new notion of\nwelfare that balances allocation efficiency and fairness, and we characterize\nthe associated incentive-compatible pricing rule. These results are extended to\nmulti-ad allocation per segment. An empirical evaluation validates the\nfeasibility and effectiveness of our approach over several ad auction\nscenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more\nflexibility to allocate ads."}
{"id": "2409.04267", "pdf": "https://arxiv.org/pdf/2409.04267.pdf", "abs": "https://arxiv.org/abs/2409.04267", "title": "An overview of domain-specific foundation model: key technologies, applications and challenges", "authors": ["Haolong Chen", "Hanzhi Chen", "Zijian Zhao", "Kaifeng Han", "Guangxu Zhu", "Yichen Zhao", "Ying Du", "Wei Xu", "Qingjiang Shi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models (FMs), addresses the limitations of\ngeneral-purpose models, which may not fully capture the unique patterns and\nrequirements of domain-specific data. Despite its importance, there is a\nnotable lack of comprehensive overview papers on building domain-specific FMs,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific FMs. It introduces basic concepts, outlines the\ngeneral architecture, and surveys key methods for constructing domain-specific\nmodels. Furthermore, the article discusses various domains that can benefit\nfrom these specialized models and highlights the challenges ahead. Through this\noverview, we aim to offer valuable guidance and reference for researchers and\npractitioners from diverse fields to develop their own customized FMs."}
{"id": "2409.19243", "pdf": "https://arxiv.org/pdf/2409.19243.pdf", "abs": "https://arxiv.org/abs/2409.19243", "title": "Jointly modelling the evolution of social structure and language in online communities", "authors": ["Christine de Kock"], "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Group interactions take place within a particular socio-temporal context,\nwhich should be taken into account when modelling interactions in online\ncommunities. We propose a method for jointly modelling community structure and\nlanguage over time. Our system produces dynamic word and user representations\nthat can be used to cluster users, investigate thematic interests of groups,\nand predict group membership. We apply and evaluate our method in the context\nof a set of misogynistic extremist groups. Our results indicate that this\napproach outperforms prior models which lacked one of these components (i.e.\nnot incorporating social structure, or using static word embeddings) when\nevaluated on clustering and embedding prediction tasks. Our method further\nenables novel types of analyses on online groups, including tracing their\nresponse to temporal events and quantifying their propensity for using violent\nlanguage, which is of particular importance in the context of extremist groups."}
{"id": "2410.07172", "pdf": "https://arxiv.org/pdf/2410.07172.pdf", "abs": "https://arxiv.org/abs/2410.07172", "title": "Glider: Global and Local Instruction-Driven Expert Router", "authors": ["Pingzhi Li", "Prateek Yadav", "Jaehong Yoon", "Jie Peng", "Yi-Lin Sung", "Mohit Bansal", "Tianlong Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "Our code is available at https://github.com/UNITES-Lab/glider", "summary": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods."}
{"id": "2410.14375", "pdf": "https://arxiv.org/pdf/2410.14375.pdf", "abs": "https://arxiv.org/abs/2410.14375", "title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts", "authors": ["Jialin Yu", "Yuxiang Zhou", "Yulan He", "Nevin L. Zhang", "Junchi Yu", "Philip Torr", "Ricardo Silva"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Adapting to latent-confounded shifts remains a core challenge in modern AI.\nThese shifts are propagated via latent variables that induce spurious,\nnon-transportable correlations between inputs and labels. One practical failure\nmode arises when fine-tuning pre-trained foundation models on confounded data\n(e.g., where certain text tokens or image backgrounds spuriously correlate with\nthe label), leaving models vulnerable at deployment. We frame causal\nfine-tuning as an identification problem and pose an explicit causal model that\ndecomposes inputs into low-level spurious features and high-level causal\nrepresentations. Under this family of models, we formalize the assumptions\nrequired for identification. Using pre-trained language models as a case study,\nwe show how identifying and adjusting these components during causal\nfine-tuning enables automatic adaptation to latent-confounded shifts at test\ntime. Experiments on semi-synthetic benchmarks derived from real-world problems\ndemonstrate that our method outperforms black-box domain generalization\nbaselines, illustrating the benefits of explicitly modeling causal structure."}
{"id": "2410.21027", "pdf": "https://arxiv.org/pdf/2410.21027.pdf", "abs": "https://arxiv.org/abs/2410.21027", "title": "Transferable Post-training via Inverse Value Learning", "authors": ["Xinyu Lu", "Xueru Wen", "Yaojie Lu", "Bowen Yu", "Hongyu Lin", "Haiyang Yu", "Le Sun", "Xianpei Han", "Yongbin Li"], "categories": ["cs.LG", "cs.CL"], "comment": "NAACL 2025 Camera Ready", "summary": "As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training."}
{"id": "2411.07595", "pdf": "https://arxiv.org/pdf/2411.07595.pdf", "abs": "https://arxiv.org/abs/2411.07595", "title": "Entropy Controllable Direct Preference Optimization", "authors": ["Motoki Omura", "Yasuhiro Fujita", "Toshiki Kataoka"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 Workshop on Models of Human Feedback for AI Alignment", "summary": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs."}
{"id": "2501.11651", "pdf": "https://arxiv.org/pdf/2501.11651.pdf", "abs": "https://arxiv.org/abs/2501.11651", "title": "T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling", "authors": ["Zhenyu Hou", "Xin Lv", "Rui Lu", "Jiajie Zhang", "Yujiang Li", "Zijun Yao", "Juanzi Li", "Jie Tang", "Yuxiao Dong"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification."}
{"id": "2501.18638", "pdf": "https://arxiv.org/pdf/2501.18638.pdf", "abs": "https://arxiv.org/abs/2501.18638", "title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation", "authors": ["Daniel Schwartz", "Dmitriy Bespalov", "Zhe Wang", "Ninad Kulkarni", "Yanjun Qi"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "14 pages, 5 figures", "summary": "As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety."}
{"id": "2502.07855", "pdf": "https://arxiv.org/pdf/2502.07855.pdf", "abs": "https://arxiv.org/abs/2502.07855", "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey", "authors": ["Ahmed Sharshar", "Latif U. Khan", "Waseem Ullah", "Mohsen Guizani"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings."}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514.pdf", "abs": "https://arxiv.org/abs/2504.10514", "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "36 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI."}
{"id": "2504.11190", "pdf": "https://arxiv.org/pdf/2504.11190.pdf", "abs": "https://arxiv.org/abs/2504.11190", "title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods."}
{"id": "2504.13128", "pdf": "https://arxiv.org/pdf/2504.13128.pdf", "abs": "https://arxiv.org/abs/2504.13128", "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents", "authors": ["Nandan Thakur", "Jimmy Lin", "Sam Havens", "Michael Carbin", "Omar Khattab", "Andrew Drozdov"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "21 pages, 4 figures, 8 tables", "summary": "We introduce FreshStack, a holistic framework for automatically building\ninformation retrieval (IR) evaluation benchmarks by incorporating challenging\nquestions and answers. FreshStack conducts the following steps: (1) automatic\ncorpus collection from code and technical documentation, (2) nugget generation\nfrom community-asked questions and answers, and (3) nugget-level support,\nretrieving documents using a fusion of retrieval techniques and hybrid\narchitectures. We use FreshStack to build five datasets on fast-growing,\nrecent, and niche topics to ensure the tasks are sufficiently challenging. On\nFreshStack, existing retrieval models, when applied out-of-the-box,\nsignificantly underperform oracle approaches on all five topics, denoting\nplenty of headroom to improve IR quality. In addition, we identify cases where\nrerankers do not improve first-stage retrieval accuracy (two out of five\ntopics) and oracle context helps an LLM generator generate a high-quality RAG\nanswer. We hope FreshStack will facilitate future work toward constructing\nrealistic, scalable, and uncontaminated IR and RAG evaluation benchmarks."}
{"id": "2506.00073", "pdf": "https://arxiv.org/pdf/2506.00073.pdf", "abs": "https://arxiv.org/abs/2506.00073", "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."}
{"id": "2506.01115", "pdf": "https://arxiv.org/pdf/2506.01115.pdf", "abs": "https://arxiv.org/abs/2506.01115", "title": "Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer", "authors": ["Yihe Dong", "Lorenzo Noci", "Mikhail Khodak", "Mufan Li"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The Transformer architecture is central to the success of modern Large\nLanguage Models (LLMs), in part due to its surprising ability to perform a wide\nrange of algorithmic tasks -- including mathematical reasoning, memorization,\nand retrieval -- using only gradient-based training on next-token prediction.\nWhile the core component of a Transformer is the self-attention mechanism, we\nquestion how much, and which aspects, of the performance gains can be\nattributed to it. To this end, we compare standard Transformers to variants in\nwhich either the multi-layer perceptron (MLP) layers or the attention\nprojectors (queries and keys) are frozen at initialization. To further isolate\nthe contribution of attention, we introduce MixiT -- the Mixing Transformer --\na simplified, principled model in which the attention coefficients are entirely\nrandom and fixed at initialization, eliminating any input-dependent computation\nor learning in attention. Surprisingly, we find that MixiT matches the\nperformance of fully trained Transformers on various algorithmic tasks,\nespecially those involving basic arithmetic or focusing heavily on\nmemorization. For retrieval-based tasks, we observe that having input-dependent\nattention coefficients is consistently beneficial, while MixiT underperforms.\nWe attribute this failure to its inability to form specialized circuits such as\ninduction heads -- a specific circuit known to be crucial for learning and\nexploiting repeating patterns in input sequences. Even more interestingly, we\nfind that attention with frozen key and query projectors is not only able to\nform induction heads, but can also perform competitively on language modeling.\nOur results underscore the importance of architectural heterogeneity, where\ndistinct components contribute complementary inductive biases crucial for\nsolving different classes of tasks."}
{"id": "2506.04210", "pdf": "https://arxiv.org/pdf/2506.04210.pdf", "abs": "https://arxiv.org/abs/2506.04210", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models."}
{"id": "2506.04518", "pdf": "https://arxiv.org/pdf/2506.04518.pdf", "abs": "https://arxiv.org/abs/2506.04518", "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model", "authors": ["Haibin Wu", "Yuxuan Hu", "Ruchao Fan", "Xiaofei Wang", "Kenichi Kumatani", "Bo Ren", "Jianwei Yu", "Heng Lu", "Lijuan Wang", "Yao Qian", "Jinyu Li"], "categories": ["eess.AS", "cs.CL"], "comment": "Our company need to do internal review", "summary": "Speech language models (Speech LMs) enable end-to-end speech-text modelling\nwithin a single model, offering a promising direction for spoken dialogue\nsystems. The choice of speech-text jointly decoding paradigm plays a critical\nrole in performance, efficiency, and alignment quality. In this work, we\nsystematically compare representative joint speech-text decoding\nstrategies-including the interleaved, and parallel generation paradigms-under a\ncontrolled experimental setup using the same base language model, speech\ntokenizer and training data. Our results show that the interleaved approach\nachieves the best alignment. However it suffers from slow inference due to long\ntoken sequence length. To address this, we propose a novel early-stop\ninterleaved (ESI) pattern that not only significantly accelerates decoding but\nalso yields slightly better performance. Additionally, we curate high-quality\nquestion answering (QA) datasets to further improve speech QA performance."}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196.pdf", "abs": "https://arxiv.org/abs/2506.07196", "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "categories": ["cs.CV", "cs.CL"], "comment": "The authors could not reach a consensus on the final version of this\n  paper, necessitating its withdrawal", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance."}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833.pdf", "abs": "https://arxiv.org/abs/2506.07833", "title": "Improving Large Language Models with Concept-Aware Fine-Tuning", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm"}
{"id": "2506.08967", "pdf": "https://arxiv.org/pdf/2506.08967.pdf", "abs": "https://arxiv.org/abs/2506.08967", "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "12 pages, 3 figures", "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks."}
{"id": "2506.09026", "pdf": "https://arxiv.org/pdf/2506.09026.pdf", "abs": "https://arxiv.org/abs/2506.09026", "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs", "authors": ["Amrith Setlur", "Matthew Y. R. Yang", "Charlie Snell", "Jeremy Greer", "Ian Wu", "Virginia Smith", "Max Simchowitz", "Aviral Kumar"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model."}
{"id": "2506.10521", "pdf": "https://arxiv.org/pdf/2506.10521.pdf", "abs": "https://arxiv.org/abs/2506.10521", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "categories": ["cs.AI", "cs.CL"], "comment": "82 pages", "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries."}
{"id": "2506.10963", "pdf": "https://arxiv.org/pdf/2506.10963.pdf", "abs": "https://arxiv.org/abs/2506.10963", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "categories": ["cs.CV", "cs.CL"], "comment": "85 pages, 70 figures, code: https://github.com/MMMGBench/MMMG,\n  project page: https://mmmgbench.github.io/", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning -- a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits -- low entity fidelity,\nweak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only\n50.20, underscoring the benchmark's difficulty. To spur further progress, we\nrelease FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that\ncombines a reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs."}
