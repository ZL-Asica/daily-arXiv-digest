{"id": "2508.06512", "pdf": "https://arxiv.org/pdf/2508.06512.pdf", "abs": "https://arxiv.org/abs/2508.06512", "title": "Accessibility Literacy: Increasing accessibility awareness among young content creators", "authors": ["Alina Karakanta"], "categories": ["cs.HC", "cs.CY"], "comment": "Master thesis: MASTER OF ARTS IN ACCESSIBILITY TO MEDIA, ARTS AND\n  CULTURE", "summary": "The proliferation of audiovisual and web content has created an increasing\nneed for media accessibility education in various fields. However,\naccessibility remains a low priority in university curricula. This project\nexplores the feasibility of an alternative learning experience aimed at\nincreasing the accessibility literacy of young content creators, taking web\naccessibility as a case study. We propose a mini module that uses simple,\neasy-to-use training materials, such as infographics and short quizzes, and can\nbe easily incorporated in educational programmes along existing courses. A\nsurvey was conducted to investigate the participants' accessibility literacy\nbefore and after training. The findings show that young content creators\ngenerally have limited accessibility literacy but even brief exposure to\naccessibility materials contributed to a shift in perceptions. After training,\nparticipants expressed more willingness to implement accessibility tools in\ntheir content, with ways varying depending on content type and purpose. This\nsuggests that small, yet targeted interventions could be an alternative for\nintegrating accessibility training into formal education across various\ndisciplines. While some responses reflected traces of the medical model of\ndisability and a particularlist view of accessibility, accessibility was\nrecognised as important for increasing inclusion, improving content, and\nshaping a fairer society."}
{"id": "2508.06732", "pdf": "https://arxiv.org/pdf/2508.06732.pdf", "abs": "https://arxiv.org/abs/2508.06732", "title": "ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets", "authors": ["Yuya Kawakami", "Daniel Cayan", "Dongyu Liu", "Kwan-Liu Ma"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Ensemble datasets are ever more prevalent in various scientific domains. In\nclimate science, ensemble datasets are used to capture variability in\nprojections under plausible future conditions including greenhouse and aerosol\nemissions. Each ensemble model run produces projections that are fundamentally\nsimilar yet meaningfully distinct. Understanding this variability among\nensemble model runs and analyzing its magnitude and patterns is a vital task\nfor climate scientists. In this paper, we present ClimateSOM, a visual analysis\nworkflow that leverages a self-organizing map (SOM) and Large Language Models\n(LLMs) to support interactive exploration and interpretation of climate\nensemble datasets. The workflow abstracts climate ensemble model runs -\nspatiotemporal time series - into a distribution over a 2D space that captures\nthe variability among the ensemble model runs using a SOM. LLMs are integrated\nto assist in sensemaking of this SOM-defined 2D space, the basis for the visual\nanalysis tasks. In all, ClimateSOM enables users to explore the variability\namong ensemble model runs, identify patterns, compare and cluster the ensemble\nmodel runs. To demonstrate the utility of ClimateSOM, we apply the workflow to\nan ensemble dataset of precipitation projections over California and the\nNorthwestern United States. Furthermore, we conduct a short evaluation of our\nLLM integration, and conduct an expert review of the visual workflow and the\ninsights from the case studies with six domain experts to evaluate our approach\nand its utility."}
{"id": "2508.06751", "pdf": "https://arxiv.org/pdf/2508.06751.pdf", "abs": "https://arxiv.org/abs/2508.06751", "title": "Toward a Logic of Generalization about Visualization as a Decision Aid", "authors": ["Alex Kale"], "categories": ["cs.HC"], "comment": null, "summary": "Visualization as a discipline often grapples with generalization by reasoning\nabout how study results on the efficacy of a tool in one context might apply to\nanother context. This work offers an account of the logic of generalization in\nvisualization research and argues that it struggles in particular with\napplications of visualization as a decision aid. We use decision theory to\ndefine the dimensions on which decision problems can vary, and we present an\nanalysis of heterogeneity in scenarios where visualization supports\ndecision-making. Our findings identify utility as a focal and under-examined\nconcept in visualization research on decision-making, demonstrating how the\nvisualization community's logic of generalization might benefit from using\ndecision theory as a lens for understanding context variation."}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772.pdf", "abs": "https://arxiv.org/abs/2508.06772", "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues."}
{"id": "2508.06495", "pdf": "https://arxiv.org/pdf/2508.06495.pdf", "abs": "https://arxiv.org/abs/2508.06495", "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galvão Filho"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora."}
{"id": "2508.06773", "pdf": "https://arxiv.org/pdf/2508.06773.pdf", "abs": "https://arxiv.org/abs/2508.06773", "title": "Methodology for Business Intelligence Solutions in Internet Banking Companies", "authors": ["Alex Escalante Viteri", "Javier Gamboa Cruzado", "Leonidas Asto Huaman"], "categories": ["cs.HC"], "comment": "9 pages 3 figures", "summary": "Business intelligence in the banking industry has been studied extensively in\nthe last decade; however, business executives still do not perceive efficiency\nin the decision-making process since the management and treatment of\ninformation are very timeconsuming for the deliverer, generating costs in the\nprocess. On the other hand, there is no formal methodology for developing\nbusiness intelligence solutions in this sector. This work aims to optimize\ndecision-making in a business unit that works with internet banking companies,\nreducing the time, the number of people, and the costs involved in\ndecision-making. To meet the objective, basic and applied research was\nconducted. The basic research allowed the construction of a new methodology\nfrom a study of critical success factors and approaches from the business\nintelligence literature. The applied research involved the implementation of a\nbusiness intelligence solution applying the new methodology in a\npre-experimental study. Thirty decision-making processes were analyzed using\npre-test and post-test data. Tools such as a stopwatch and observation were\nused to collect and record data on time spent, the number of people, and the\ndecision-making costs. This information was processed in the specialized\nMinitab18 statistical software, which allowed the observation and confirmation\nof relevant results regarding time reduction, the number of people, and the\ncosts generated. Therefore, it was concluded that the business intelligence\nsolution, applying the new methodology, optimized decision making in the\nbusiness unit that works with internet banking for companies."}
{"id": "2508.06504", "pdf": "https://arxiv.org/pdf/2508.06504.pdf", "abs": "https://arxiv.org/abs/2508.06504", "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER."}
{"id": "2508.06775", "pdf": "https://arxiv.org/pdf/2508.06775.pdf", "abs": "https://arxiv.org/abs/2508.06775", "title": "Visualization Vibes: The Socio-Indexical Function of Visualization Design", "authors": ["Michelle Morgenstern", "Amy Rae Fox", "Graham M. Jones", "Arvind Satyanarayan"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "In contemporary information ecologies saturated with misinformation,\ndisinformation, and a distrust of science itself, public data communication\nfaces significant hurdles. Although visualization research has broadened\ncriteria for effective design, governing paradigms privilege the accurate and\nefficient transmission of data. Drawing on theory from linguistic anthropology,\nwe argue that such approaches-focused on encoding and decoding propositional\ncontent-cannot fully account for how people engage with visualizations and why\nparticular visualizations might invite adversarial or receptive responses. In\nthis paper, we present evidence that data visualizations communicate not only\nsemantic, propositional meaning$\\unicode{x2013}$meaning about\ndata$\\unicode{x2013}$but also social, indexical meaning$\\unicode{x2013}$meaning\nbeyond data. From a series of ethnographically-informed interviews, we document\nhow readers make rich and varied assessments of a visualization's\n\"vibes\"$\\unicode{x2013}$inferences about the social provenance of a\nvisualization based on its design features. Furthermore, these social\nattributions have the power to influence reception, as readers' decisions about\nhow to engage with a visualization concern not only content, or even aesthetic\nappeal, but also their sense of alignment or disalignment with the entities\nthey imagine to be involved in its production and circulation. We argue these\ninferences hinge on a function of human sign systems that has thus far been\nlittle studied in data visualization: socio-indexicality, whereby the formal\nfeatures (rather than the content) of communication evoke social contexts,\nidentities, and characteristics. Demonstrating the presence and significance of\nthis socio-indexical function in visualization, this paper offers both a\nconceptual foundation and practical intervention for troubleshooting breakdowns\nin public data communication."}
{"id": "2508.06524", "pdf": "https://arxiv.org/pdf/2508.06524.pdf", "abs": "https://arxiv.org/abs/2508.06524", "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "authors": ["Lei Jiang", "Fan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs."}
{"id": "2508.06778", "pdf": "https://arxiv.org/pdf/2508.06778.pdf", "abs": "https://arxiv.org/abs/2508.06778", "title": "Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers", "authors": ["Pyeonghwa Kim", "Steve Sawyer", "Michael Dunn"], "categories": ["cs.HC"], "comment": "Accepted to CSCW 2025", "summary": "We advance gender-inclusive research within the CSCW field by investigating\nthe long-term gendered experiences of online freelancers on digital labor\nplatforms. The prevalence of gender-based inequalities has attracted\nsignificant attention within the CSCW community. Yet, insights remain limited\non how these inequalities shape workers' long-term experiences on digital labor\nplatforms. Through a five-year longitudinal study of 105 freelancers on Upwork,\nwe reveal persistent gender disparities that influence workers' long-term work\nand career trajectories, raising concerns about the sustainability of\nplatform-mediated work. We advance the ongoing dialogue on gender inclusivity\nin the community by introducing the concepts of career disempowerment and\nplatform-mediated motherhood penalty and by offering research and design\nimplications for CSCW to foster more sustainable, equitable platform work\nenvironments for all genders."}
{"id": "2508.06533", "pdf": "https://arxiv.org/pdf/2508.06533.pdf", "abs": "https://arxiv.org/abs/2508.06533", "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs"}
{"id": "2508.06786", "pdf": "https://arxiv.org/pdf/2508.06786.pdf", "abs": "https://arxiv.org/abs/2508.06786", "title": "Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale", "authors": ["Amy Rae Fox", "Michelle Morgenstern", "Graham M. Jones", "Arvind Satyanarayan"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "What impressions might readers form with visualizations that go beyond the\ndata they encode? In this paper, we build on recent work that demonstrates the\nsocio-indexical function of visualization, showing that visualizations\ncommunicate more than the data they explicitly encode. Bridging this with prior\nwork examining public discourse about visualizations, we contribute an analytic\nframework for describing inferences about an artifact's social provenance. Via\na series of attribution-elicitation surveys, we offer descriptive evidence that\nthese social inferences: (1) can be studied asynchronously, (2) are not unique\nto a particular sociocultural group or a function of limited data literacy, and\n(3) may influence assessments of trust. Further, we demonstrate (4) how design\nfeatures act in concert with the topic and underlying messages of an artifact's\ndata to give rise to such 'beyond-data' readings. We conclude by discussing the\ndesign and research implications of inferences about social provenance, and why\nwe believe broadening the scope of research on human factors in visualization\nto include sociocultural phenomena can yield actionable design recommendations\nto address urgent challenges in public data communication."}
{"id": "2508.06548", "pdf": "https://arxiv.org/pdf/2508.06548.pdf", "abs": "https://arxiv.org/abs/2508.06548", "title": "Factor Augmented Supervised Learning with Text Embeddings", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods."}
{"id": "2508.06791", "pdf": "https://arxiv.org/pdf/2508.06791.pdf", "abs": "https://arxiv.org/abs/2508.06791", "title": "Entendimento de Campanhas no Contexto da Atenção Primária à Saúde: Um Processo de Design Socialmente Consciente", "authors": ["Deógenes P. da Silva Junior", "Jonas Lopes Guerra", "Krissia Menezes", "Marisa Sel Franco", "Roberto Pereira"], "categories": ["cs.HC"], "comment": "62 pages, in Portuguese language, 8 figures", "summary": "This report presents the results of an exploratory analysis of the work\ncontext of Community Health Agents and Endemic Disease Control Agents in\nPrimary Health Care (PHC), with a particular focus on Health Campaigns. To\nunderstand this context, the study adopted the Socially Aware Design framework,\nwhich employs artifacts and techniques to examine problem domains in a\ncomprehensive and sociotechnical manner. Methods such as the Stakeholder\nIdentification Diagram, Evaluation Frame, and Semiotic Framework were applied\nto identify stakeholders, anticipate challenges, and elicit social and\ntechnical requirements for the solution. Personas and Scenarios were also used\nto illustrate the potential impacts of a solution on various stakeholders and\ntheir life contexts within health campaigns. This report presents the analysis\nmethod, its application, and results, discussing the study's findings to inform\nthe development of medium-fidelity prototypes for a PHC health campaign\nmanagement solution."}
{"id": "2508.06583", "pdf": "https://arxiv.org/pdf/2508.06583.pdf", "abs": "https://arxiv.org/abs/2508.06583", "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs."}
{"id": "2508.06801", "pdf": "https://arxiv.org/pdf/2508.06801.pdf", "abs": "https://arxiv.org/abs/2508.06801", "title": "Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Callum Parker", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "Pedestrian gestures play an important role in traffic communication,\nparticularly in interactions with autonomous vehicles (AVs), yet their subtle,\nambiguous, and context-dependent nature poses persistent challenges for machine\ninterpretation. This study investigates these challenges by using GPT-4V, a\nvision-language model, not as a performance benchmark but as a diagnostic tool\nto reveal patterns and causes of gesture misrecognition. We analysed a public\ndataset of pedestrian-vehicle interactions, combining manual video review with\nthematic analysis of the model's qualitative reasoning. This dual approach\nsurfaced recurring factors influencing misrecognition, including gesture\nvisibility, pedestrian behaviour, interaction context, and environmental\nconditions. The findings suggest practical considerations for gesture design,\nincluding the value of salience and contextual redundancy, and highlight\nopportunities to improve AV recognition systems through richer context\nmodelling and uncertainty-aware interpretations. While centred on AV-pedestrian\ninteraction, the method and insights are applicable to other domains where\nmachines interpret human gestures, such as wearable AR and assistive\ntechnologies."}
{"id": "2508.06595", "pdf": "https://arxiv.org/pdf/2508.06595.pdf", "abs": "https://arxiv.org/abs/2508.06595", "title": "LLM Unlearning Without an Expert Curated Dataset", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook."}
{"id": "2508.06826", "pdf": "https://arxiv.org/pdf/2508.06826.pdf", "abs": "https://arxiv.org/abs/2508.06826", "title": "AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content", "authors": ["Nels Numan", "Jessica Van Brummelen", "Ziwen Lu", "Anthony Steed"], "categories": ["cs.HC"], "comment": "4 pages, 1 figure, ACM UIST 2025 Poster", "summary": "Site-specific outdoor AR experiences are typically authored using static 3D\nmodels, but are deployed in physical environments that change over time. As a\nresult, virtual content may become misaligned with its intended real-world\nreferents, degrading user experience and compromising contextual\ninterpretation. We present AdjustAR, a system that supports in-situ correction\nof AR content in dynamic environments using multimodal large language models\n(MLLMs). Given a composite image comprising the originally authored view and\nthe current live user view from the same perspective, an MLLM detects\ncontextual misalignments and proposes revised 2D placements for affected AR\nelements. These corrections are backprojected into 3D space to update the scene\nat runtime. By leveraging MLLMs for visual-semantic reasoning, this approach\nenables automated runtime corrections to maintain alignment with the authored\nintent as real-world target environments evolve."}
{"id": "2508.06600", "pdf": "https://arxiv.org/pdf/2508.06600.pdf", "abs": "https://arxiv.org/abs/2508.06600", "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system."}
{"id": "2508.06846", "pdf": "https://arxiv.org/pdf/2508.06846.pdf", "abs": "https://arxiv.org/abs/2508.06846", "title": "Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators", "authors": ["Hyo Jin Do", "Rachel Ostrand", "Werner Geyer", "Keerthiram Murugesan", "Dennis Wei", "Justin Weisz"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 8 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models (LLMs) are susceptible to generating inaccurate or\nfalse information, often referred to as \"hallucinations\" or \"confabulations.\"\nWhile several technical advancements have been made to detect hallucinated\ncontent by assessing the factuality of the model's responses, there is still\nlimited research on how to effectively communicate this information to users.\nTo address this gap, we conducted two scenario-based experiments with a total\nof 208 participants to systematically compare the effects of various design\nstrategies for communicating factuality scores by assessing participants'\nratings of trust, ease in validating response accuracy, and preference. Our\nfindings reveal that participants preferred and trusted a design in which all\nphrases within a response were color-coded based on factuality scores.\nParticipants also found it easier to validate accuracy of the response in this\nstyle compared to a baseline with no style applied. Our study offers practical\ndesign guidelines for LLM application developers and designers, aimed at\ncalibrating user trust, aligning with user preferences, and enhancing users'\nability to scrutinize LLM outputs."}
{"id": "2508.06621", "pdf": "https://arxiv.org/pdf/2508.06621.pdf", "abs": "https://arxiv.org/abs/2508.06621", "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance."}
{"id": "2508.06872", "pdf": "https://arxiv.org/pdf/2508.06872.pdf", "abs": "https://arxiv.org/abs/2508.06872", "title": "Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions", "authors": ["Danyang Fan", "Walker Smith", "Takako Fujioka", "Chris Chage", "Sile O'Modhrain", "Diana Deutsch", "Sean Follmer"], "categories": ["cs.HC"], "comment": null, "summary": "Sonification offers a non-visual way to understand data, with pitch-based\nencodings being the most common. Yet, how well people perceive slope and\nacceleration-key features of data trends-remains poorly understood. Drawing on\npeople's natural abilities to perceive tempo, we introduce a novel sampling\nmethod for pitch-based sonification to enhance the perception of slope and\nacceleration in univariate functions. While traditional sonification methods\noften sample data at uniform x-spacing, yielding notes played at a fixed tempo\nwith variable pitch intervals (Variable Pitch Interval), our approach samples\nat uniform y-spacing, producing notes with consistent pitch intervals but\nvariable tempo (Variable Tempo). We conducted psychoacoustic experiments to\nunderstand slope and acceleration perception across three sampling methods:\nVariable Pitch Interval, Variable Tempo, and a Continuous (no sampling)\nbaseline. In slope comparison tasks, Variable Tempo was more accurate than the\nother methods when modulated by the magnitude ratio between slopes. For\nacceleration perception, just-noticeable differences under Variable Tempo were\nover 13 times finer than with other methods. Participants also commonly\nreported higher confidence, lower mental effort, and a stronger preference for\nVariable Tempo compared to other methods. This work contributes models of slope\nand acceleration perception across pitch-based sonification techniques,\nintroduces Variable Tempo as a novel and preferred sampling method, and\nprovides promising initial evidence that leveraging timing can lead to more\nsensitive, accurate, and precise interpretation of derivative-based data\nfeatures."}
{"id": "2508.06649", "pdf": "https://arxiv.org/pdf/2508.06649.pdf", "abs": "https://arxiv.org/abs/2508.06649", "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs."}
{"id": "2508.06889", "pdf": "https://arxiv.org/pdf/2508.06889.pdf", "abs": "https://arxiv.org/abs/2508.06889", "title": "Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display", "authors": ["Dooyoung Kim", "Jinseok Hong", "Heejeong Ko", "Woontack Woo"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures, 3 tables, Accepted in TVCG Special Issue on the\n  2025 IEEE Symposium on Mixed and Augmented Reality (IEEE ISMAR)", "summary": "We proposed viewpoint-tolerant shared depth perception without individual\ntracking by leveraging human cognitive compensation in universally 3D rendered\nimages on a wall-sized display. While traditional 3D perception-enabled display\nsystems have primarily focused on single-user scenarios-adapting rendering\nbased on head and eye tracking the use of wall-sized displays to extend spatial\nexperiences and support perceptually coherent multi-user interactions remains\nunderexplored. We investigated the effects of virtual depths (dv) and absolute\nviewing distance (da) on human cognitive compensation factors (perceived\ndistance difference, viewing angle threshold, and perceived presence) to\nconstruct the wall display-based eXtended Reality (XR) space. Results show that\nparticipants experienced a compelling depth perception even from off-center\nangles of 23 to 37 degrees, and largely increasing virtual depth worsens depth\nperception and presence factors, highlighting the importance of balancing\nextended depth of virtual space and viewing distance from the wall-sized\ndisplay. Drawing on these findings, wall-sized displays in venues such as\nmuseums, galleries, and classrooms can evolve beyond 2D information sharing to\noffer immersive, spatially extended group experiences without individualized\ntracking or wearables."}
{"id": "2508.06665", "pdf": "https://arxiv.org/pdf/2508.06665.pdf", "abs": "https://arxiv.org/abs/2508.06665", "title": "Testing the Limits of Machine Translation from One Book", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "categories": ["cs.CL"], "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation."}
{"id": "2508.06955", "pdf": "https://arxiv.org/pdf/2508.06955.pdf", "abs": "https://arxiv.org/abs/2508.06955", "title": "Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent", "authors": ["Kyuwon Kim", "Jaeryeong Hwang", "Younseo Lee", "Jeanhee Lee", "Sung-Eun Kimm", "Hyo-Jeong So"], "categories": ["cs.HC"], "comment": null, "summary": "As complex societal issues continue to emerge, fostering democratic skills\nlike valuing diverse perspectives and collaborative decision-making is\nincreasingly vital in education. In this paper, we propose a Peer Agent (PA)\nsystem designed to simulate a deliberative conversational partner that induces\nsocio-cognitive conflict within dilemma-based game play. Drawing on by the\nInner Thoughts framework and grounded in value-sensitive discourse analysis,\nthe PA actively participates in voice-based multi-party deliberation with human\nplayers. The system architecture consists of five core modules: Context\nInterpreter, Agent State Manager, Thought Generator, Thought Evaluator, and\nThought Articulator."}
{"id": "2508.06671", "pdf": "https://arxiv.org/pdf/2508.06671.pdf", "abs": "https://arxiv.org/abs/2508.06671", "title": "Do Biased Models Have Biased Thoughts?", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts."}
{"id": "2508.07057", "pdf": "https://arxiv.org/pdf/2508.07057.pdf", "abs": "https://arxiv.org/abs/2508.07057", "title": "Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders", "authors": ["Syed Ibrahim Mustafa Shah Bukhari", "Maha Sajid", "Bo Ji", "Brendan David-John"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": null, "summary": "As Extended Reality (XR) devices become increasingly prevalent in everyday\nsettings, they raise significant privacy concerns for bystanders: individuals\nin the vicinity of an XR device during its use, whom the device sensors may\naccidentally capture. Current privacy indicators, such as small LEDs, often\npresume that bystanders are attentive enough to interpret the privacy signals.\nHowever, these cues can be easily overlooked when bystanders are distracted or\nhave limited vision. We define such individuals as situationally impaired\nbystanders. This study explores XR privacy indicator designs that are effective\nfor situationally impaired bystanders. A focus group with eight participants\nwas conducted to design five novel privacy indicators. We evaluated these\ndesigns through a user study with seven additional participants. Our results\nshow that visual-only indicators, typical in commercial XR devices, received\nlow ratings for perceived usefulness in impairment scenarios. In contrast,\nmultimodal indicators were preferred in privacy-sensitive scenarios with\nsituationally impaired bystanders. Ultimately, our results highlight the need\nto move toward adaptable, multimodal, and situationally aware designs that\neffectively support bystander privacy in everyday XR environments."}
{"id": "2508.06709", "pdf": "https://arxiv.org/pdf/2508.06709.pdf", "abs": "https://arxiv.org/abs/2508.06709", "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations."}
{"id": "2508.07058", "pdf": "https://arxiv.org/pdf/2508.07058.pdf", "abs": "https://arxiv.org/abs/2508.07058", "title": "Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design", "authors": ["Paul C. Parsons", "Prakash Chandra Shukla"], "categories": ["cs.HC"], "comment": "Author version; article accepted to IEEE VIS 2025; will be published\n  in IEEE TVCG in 2026", "summary": "Visualization design is often described as the process of solving a\nwell-defined problem by navigating a design space. While existing visualization\ndesign models have provided valuable structure and guidance, they tend to\nforeground technical problem-solving and underemphasize the interpretive,\njudgment-based aspects of design. In contrast, research in other design\ndisciplines has emphasized the importance of framing--how designers define and\nredefine what the problem is--and the co-evolution of problem and solution\nspaces through reflective practice. These dimensions remain underexplored in\nvisualization research, particularly from the perspective of expert\npractitioners. This paper investigates how visualization designers frame\nproblems and navigate the dynamic interplay between problem understanding and\nsolution development. We conducted a mixed-methods study with 11 expert\npractitioners using design challenges, diary entries, and semi-structured\ninterviews. Through reflexive thematic analysis, we identified key strategies\nthat participants used to frame problems, reframe them in response to evolving\nconstraints or insights, and build bridges between problem and solution spaces.\nThese included using metaphors, heuristics, sketching, primary generators, and\nreflective evaluation of failed or incomplete ideas. Our findings contribute an\nempirically grounded account of visualization design as a reflective,\nco-evolutionary practice, where framing is not a preliminary step but a\ncontinuous activity embedded in design. Participants often reshaped their\nunderstanding of the problem based on solution attempts, tool feedback, and\nethical or narrative concerns. These insights extend current visualization\ndesign models and highlight the need for frameworks that better account for\nframing and interpretive judgment. (See paper for full abstract.)"}
{"id": "2508.06729", "pdf": "https://arxiv.org/pdf/2508.06729.pdf", "abs": "https://arxiv.org/abs/2508.06729", "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis."}
{"id": "2508.07095", "pdf": "https://arxiv.org/pdf/2508.07095.pdf", "abs": "https://arxiv.org/abs/2508.07095", "title": "Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust", "authors": ["Hyo Jin Do", "Werner Geyer"], "categories": ["cs.HC", "cs.AI"], "comment": "17 pages, 3 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models are known to produce outputs that are plausible but\nfactually incorrect. To prevent people from making erroneous decisions by\nblindly trusting AI, researchers have explored various ways of communicating\nfactuality estimates in AI-generated outputs to end-users. However, little is\nknown about whether revealing content estimated to be factually incorrect\ninfluences users' trust when compared to hiding it altogether. We tested four\ndifferent ways of disclosing an AI-generated output with factuality\nassessments: transparent (highlights less factual content), attention\n(highlights factual content), opaque (removes less factual content), ambiguity\n(makes less factual content vague), and compared them with a baseline response\nwithout factuality information. We conducted a human subjects research (N =\n148) using the strategies in question-answering scenarios. We found that the\nopaque and ambiguity strategies led to higher trust while maintaining perceived\nanswer quality, compared to the other strategies. We discuss the efficacy of\nhiding presumably less factual content to build end-user trust."}
{"id": "2508.06755", "pdf": "https://arxiv.org/pdf/2508.06755.pdf", "abs": "https://arxiv.org/abs/2508.06755", "title": "Many-Turn Jailbreaking", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs."}
{"id": "2508.07129", "pdf": "https://arxiv.org/pdf/2508.07129.pdf", "abs": "https://arxiv.org/abs/2508.07129", "title": "Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers", "authors": ["Caroline M. Johnston", "Olga Koumoundouros", "Angel Hsing-Chi Hwang", "Laura Onasch-Vera", "Eric Rice", "Phebe Vayanos"], "categories": ["cs.HC", "cs.AI"], "comment": "21 pages, 1 figure, 2 tables", "summary": "Artificial intelligence researchers have proposed various data-driven\nalgorithms to improve the processes that match individuals experiencing\nhomelessness to scarce housing resources. It remains unclear whether and how\nthese algorithms are received or adopted by practitioners and what their\ncorresponding consequences are. Through semi-structured interviews with 13\npolicymakers in homeless services in Los Angeles, we investigate whether such\nchange-makers are open to the idea of integrating AI into the housing resource\nmatching process, identifying where they see potential gains and drawbacks from\nsuch a system in issues of efficiency, fairness, and transparency. Our\nqualitative analysis indicates that, even when aware of various complicating\nfactors, policymakers welcome the idea of an AI matching tool if thoughtfully\ndesigned and used in tandem with human decision-makers. Though there is no\nconsensus as to the exact design of such an AI system, insights from\npolicymakers raise open questions and design considerations that can be\nenlightening for future researchers and practitioners who aim to build\nresponsible algorithmic systems to support decision-making in low-resource\nscenarios."}
{"id": "2508.06803", "pdf": "https://arxiv.org/pdf/2508.06803.pdf", "abs": "https://arxiv.org/abs/2508.06803", "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score."}
{"id": "2508.07135", "pdf": "https://arxiv.org/pdf/2508.07135.pdf", "abs": "https://arxiv.org/abs/2508.07135", "title": "Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas", "authors": ["Runlin Duan", "Yuzhao Chen", "Rahul Jain", "Yichen Hu", "Jingyu Shi", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI (GenAI) has significantly advanced the ease and flexibility of\nimage creation. However, it remains a challenge to precisely control spatial\ncompositions, including object arrangement and scene conditions. To bridge this\ngap, we propose Canvas3D, an interactive system leveraging a 3D engine to\nenable precise spatial manipulation for image generation. Upon user prompt,\nCanvas3D automatically converts textual descriptions into interactive objects\nwithin a 3D engine-driven virtual canvas, empowering direct and precise spatial\nconfiguration. These user-defined arrangements generate explicit spatial\nconstraints that guide generative models in accurately reflecting user\nintentions in the resulting images. We conducted a closed-end comparative study\nbetween Canvas3D and a baseline system. And an open-ended study to evaluate our\nsystem \"in the wild\". The result indicates that Canvas3D outperforms the\nbaseline on spatial control, interactivity, and overall user experience."}
{"id": "2508.06810", "pdf": "https://arxiv.org/pdf/2508.06810.pdf", "abs": "https://arxiv.org/abs/2508.06810", "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Camélia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated."}
{"id": "2508.07141", "pdf": "https://arxiv.org/pdf/2508.07141.pdf", "abs": "https://arxiv.org/abs/2508.07141", "title": "SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI", "authors": ["Runlin Duan", "Chenfei Zhu", "Yuzhao Chen", "Dizhi Ma", "Jingyu Shi", "Ziyi Liu", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Conceptual product design requires designers to explore the design space of\nvisual and functional concepts simultaneously. Sketching has long been adopted\nto empower concept exploration. However, current sketch-based design tools\nmostly emphasize visual design using emerging techniques. We present\nSketchConcept, a design support tool that decomposes design concepts into\nvisual representations and functionality of concepts using sketches and textual\ndescriptions. We propose a function-to-visual mapping workflow that maps the\nfunction descriptions generated by a Large Language Model to a component of the\nconcept produced by image Generative Artificial Intelligence(GenAI). The\nfunction-to-visual mapping allows our system to leverage multimodal GenAI to\ndecompose, generate, and edit the design concept to satisfy the overall\nfunction and behavior. We present multiple use cases enabled by SketchConcept\nto validate the workflow. Finally, we evaluated the efficacy and usability of\nour system with a two-session user study."}
{"id": "2508.06870", "pdf": "https://arxiv.org/pdf/2508.06870.pdf", "abs": "https://arxiv.org/abs/2508.06870", "title": "Text to Speech System for Meitei Mayek Script", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language\n  using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we\nintroduce a neural TTS\n  architecture adapted to support tonal phonology and under-resourced\nlinguistic environments. We\n  develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a\nsingle-speaker dataset, and\n  demonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective\n  metrics. This system lays the groundwork for linguistic preservation and\ntechnological inclusion of\n  Manipuri."}
{"id": "2508.07183", "pdf": "https://arxiv.org/pdf/2508.07183.pdf", "abs": "https://arxiv.org/abs/2508.07183", "title": "Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI", "authors": ["Ahmed M. Abuzuraiq", "Philippe Pasquier"], "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MM", "I.2; J.5"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "Explainable AI (XAI) in creative contexts can go beyond transparency to\nsupport artistic engagement, modifiability, and sustained practice. While\ncurated datasets and training human-scale models can offer artists greater\nagency and control, large-scale generative models like text-to-image diffusion\nsystems often obscure these possibilities. We suggest that even large models\ncan be treated as creative materials if their internal structure is exposed and\nmanipulable. We propose a craft-based approach to explainability rooted in\nlong-term, hands-on engagement akin to Sch\\\"on's \"reflection-in-action\" and\ndemonstrate its application through a model-bending and inspection plugin\nintegrated into the node-based interface of ComfyUI. We demonstrate that by\ninteractively manipulating different parts of a generative model, artists can\ndevelop an intuition about how each component influences the output."}
{"id": "2508.06877", "pdf": "https://arxiv.org/pdf/2508.06877.pdf", "abs": "https://arxiv.org/abs/2508.06877", "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora."}
{"id": "2508.07203", "pdf": "https://arxiv.org/pdf/2508.07203.pdf", "abs": "https://arxiv.org/abs/2508.07203", "title": "Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools", "authors": ["Prashant Sharma"], "categories": ["cs.HC", "cs.CR", "cs.SE"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "Current digital government literature focuses on professional in-house IT\nteams, specialized digital service teams, vendor-developed systems, or\nproprietary low-code/no-code tools. Almost no scholarship addresses a growing\nmiddle ground: technically skilled civil servants outside formal IT roles who\ncan write real code but lack a sanctioned, secure path to deploy their work.\nThis paper introduces a limits-aware, open-source and replicable platform that\nenables such public servants to develop, peer review, and deploy small-scale,\ndomain-specific applications within government networks via a sandboxed,\nauditable workflow. By combining Jupyter Notebooks, preapproved open-source\nlibraries, and lightweight governance, the platform works within institutional\nconstraints such as procurement rules and IT security policies while avoiding\nvendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil\nservants' programming skills, keeping them technically competitive with their\nprivate-sector peers. This contribution fills a critical gap, offering a\nreplicable model for public-sector skill retention, resilience, and bottom-up\ndigital transformation."}
{"id": "2508.06880", "pdf": "https://arxiv.org/pdf/2508.06880.pdf", "abs": "https://arxiv.org/abs/2508.06880", "title": "The ReQAP System for Question Answering over Personal Information", "authors": ["Philipp Christmann", "Gerhard Weikum"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system."}
{"id": "2508.07256", "pdf": "https://arxiv.org/pdf/2508.07256.pdf", "abs": "https://arxiv.org/abs/2508.07256", "title": "Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos", "authors": ["Wei Xiang", "Chuyue Zhang", "Jie Yan"], "categories": ["cs.HC"], "comment": "31 pages, 5 figures, under review", "summary": "Automated driving in level 3 autonomy has been adopted by multiple companies\nsuch as Tesla and BMW, alleviating the burden on drivers while unveiling new\ncomplexities. This article focused on the under-explored territory of micro\naccidents during automated driving, characterized as not fatal but abnormal\naberrations such as abrupt deceleration and snake driving. These micro\naccidents are basic yet pervasive events that might results in more severe\naccidents. Through collecting a comprehensive dataset of user generated video\nrecording such micro accidents in natural driving scenarios, this article\nlocates key variables pertaining to environments and autonomous agents using\nmachine learning methods. Subsequently, crowdsourcing method provides insights\ninto human risk perceptions and reactions to these micro accidents. This\narticle thus describes features of safety critical scenarios other than crashes\nand fatal accidents, informing and potentially advancing the design of\nautomated driving systems."}
{"id": "2508.06886", "pdf": "https://arxiv.org/pdf/2508.06886.pdf", "abs": "https://arxiv.org/abs/2508.06886", "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "categories": ["cs.CL"], "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak"}
{"id": "2508.07283", "pdf": "https://arxiv.org/pdf/2508.07283.pdf", "abs": "https://arxiv.org/abs/2508.07283", "title": "Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment", "authors": ["Bujar Raufi"], "categories": ["cs.HC", "cs.AI", "eess.SP", "q-bio.NC", "97R40", "I.2"], "comment": "17 Pages, 7 figures, 3 tables and one prompt template", "summary": "This study explores the intersection of electroencephalography (EEG)\nmicrostates and Large Language Models (LLMs) to enhance the assessment of\ncognitive load states. By utilizing EEG microstate features, the research aims\nto fine-tune LLMs for improved predictions of distinct cognitive states,\nspecifically 'Rest' and 'Load'. The experimental design is delineated in four\ncomprehensive stages: dataset collection and preprocessing, microstate\nsegmentation and EEG backfitting, feature extraction paired with prompt\nengineering, and meticulous LLM model selection and refinement. Employing a\nsupervised learning paradigm, the LLM is trained to identify cognitive load\nstates based on EEG microstate features integrated into prompts, producing\naccurate discrimination of cognitive load. A curated dataset, linking EEG\nfeatures to specified cognitive load conditions, underpins the experimental\nframework. The results indicate a significant improvement in model performance\nfollowing the proposed fine-tuning, showcasing the potential of EEG-informed\nLLMs in cognitive neuroscience and cognitive AI applications. This approach not\nonly contributes to the understanding of brain dynamics but also paves the way\nfor advancements in machine learning techniques applicable to cognitive load\nand cognitive AI research."}
{"id": "2508.06913", "pdf": "https://arxiv.org/pdf/2508.06913.pdf", "abs": "https://arxiv.org/abs/2508.06913", "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios."}
{"id": "2508.07301", "pdf": "https://arxiv.org/pdf/2508.07301.pdf", "abs": "https://arxiv.org/abs/2508.07301", "title": "In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons", "authors": ["Abasi-amefon Obot Affia-Jomants", "Alexander Serebrenik", "James D. Herbsleb", "Alexander Nolte"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted in Proceedings of the ACM on Human Computer Interaction\n  (CSCW'25)", "summary": "Hybrid hackathons, which combine in-person and online participation, present\nunique challenges for organizers and participants. Although such events are\nincreasingly conducted, research on them remains fragmented, with limited\nintegration between hackathon studies and hybrid collaboration. Existing\nstrategies for in-person or online-only events often fail to address the unique\nchallenges of hybrid formats, such as managing communication across physical\nand virtual spaces. Our work addresses this gap by examining how hybrid\nhackathons function, analyzing how organizers structure these events and how\nparticipants navigate hybrid-specific challenges. Drawing on established\ntheories of hybrid collaboration, we examine key dimensions - synchronicity,\nphysical distribution, dynamic transitions, and technological infrastructure -\nthat shape collaboration in hybrid events. Through an exploratory case study of\nthree hackathon events, we analyze how these dimensions are implemented and\ntheir effects on participant experiences. Our findings reveal differing\norganizer considerations of the hybrid dimensions in the hackathon design,\nleading to distinct experiences for participants. Implementation styles -\nfavoring in-person, online, or balanced participation - led to varied\nparticipant experiences, affecting access to resources, communication, and team\ncoordination. Organizers in our study also relied on technology to bridge\nhybrid interactions, but overlooked critical aspects like time-zone management,\ndynamic transitions, and targeted support for hybrid teams. Additionally,\nparticipants in their teams responded to gaps in event scaffolding by adapting\ncollaboration strategies, revealing gaps in organizers' preparedness for hybrid\nevents. Learning from our findings, we offer practical recommendations when\norganizing hybrid hackathon events and recommendations to participants when\nattending them."}
{"id": "2508.06971", "pdf": "https://arxiv.org/pdf/2508.06971.pdf", "abs": "https://arxiv.org/abs/2508.06971", "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "categories": ["cs.CL", "cs.IR"], "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains."}
{"id": "2508.07390", "pdf": "https://arxiv.org/pdf/2508.07390.pdf", "abs": "https://arxiv.org/abs/2508.07390", "title": "Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics", "authors": ["Gustavo Moreira", "Leonardo Ferreira", "Carolina Veiga", "Maryam Hosseini", "Fabio Miranda"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. Urbanite is available at\n  https://urbantk.org/urbanite", "summary": "With the growing availability of urban data and the increasing complexity of\nsocietal challenges, visual analytics has become essential for deriving\ninsights into pressing real-world problems. However, analyzing such data is\ninherently complex and iterative, requiring expertise across multiple domains.\nThe need to manage diverse datasets, distill intricate workflows, and integrate\nvarious analytical methods presents a high barrier to entry, especially for\nresearchers and urban experts who lack proficiency in data management, machine\nlearning, and visualization. Advancements in large language models offer a\npromising solution to lower the barriers to the construction of analytics\nsystems by enabling users to specify intent rather than define precise\ncomputational operations. However, this shift from explicit operations to\nintent-based interaction introduces challenges in ensuring alignment throughout\nthe design and development process. Without proper mechanisms, gaps can emerge\nbetween user intent, system behavior, and analytical outcomes. To address these\nchallenges, we propose Urbanite, a framework for human-AI collaboration in\nurban visual analytics. Urbanite leverages a dataflow-based model that allows\nusers to specify intent at multiple scopes, enabling interactive alignment\nacross the specification, process, and evaluation stages of urban analytics.\nBased on findings from a survey to uncover challenges, Urbanite incorporates\nfeatures to facilitate explainability, multi-resolution definition of tasks\nacross dataflows, nodes, and parameters, while supporting the provenance of\ninteractions. We demonstrate Urbanite's effectiveness through usage scenarios\ncreated in collaboration with urban experts. Urbanite is available at\nhttps://urbantk.org/urbanite."}
{"id": "2508.06974", "pdf": "https://arxiv.org/pdf/2508.06974.pdf", "abs": "https://arxiv.org/abs/2508.06974", "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch."}
{"id": "2508.07496", "pdf": "https://arxiv.org/pdf/2508.07496.pdf", "abs": "https://arxiv.org/abs/2508.07496", "title": "StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data", "authors": ["Sanjana Srabanti", "G. Elisabeta Marai", "Fabio Miranda"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at IEEE VIS 2025. StreetWeave is available at\n  https://urbantk.org/streetweave", "summary": "The visualization and analysis of street and pedestrian networks are\nimportant to various domain experts, including urban planners, climate\nresearchers, and health experts. This has led to the development of new\ntechniques for street and pedestrian network visualization, expanding how data\ncan be shown and understood more effectively. Despite their increasing\nadoption, there is no established design framework to guide the creation of\nthese visualizations while addressing the diverse requirements of various\ndomains. When exploring a feature of interest, domain experts often need to\ntransform, integrate, and visualize a combination of thematic data (e.g.,\ndemographic, socioeconomic, pollution) and physical data (e.g., zip codes,\nstreet networks), often spanning multiple spatial and temporal scales. This not\nonly complicates the process of visual data exploration and system\nimplementation for developers but also creates significant entry barriers for\nexperts who lack a background in programming. With this in mind, in this paper,\nwe reviewed 45 studies utilizing street-overlaid visualizations to understand\nhow they are used. Through qualitative coding of these visualizations, we\nanalyzed three key aspects of street and pedestrian network visualization\nusage: the analytical purpose they serve, the visualization approaches\nemployed, and the data sources used in their creation. Building on this design\nspace, we introduce StreetWeave, a declarative grammar for designing custom\nvisualizations of multivariate spatial network data across multiple\nresolutions. We demonstrate how StreetWeave can be used to create various\nstreet-overlaid visualizations, enabling effective exploration and analysis of\nspatial data. StreetWeave is available at https://urbantk.org/streetweave."}
{"id": "2508.07017", "pdf": "https://arxiv.org/pdf/2508.07017.pdf", "abs": "https://arxiv.org/abs/2508.07017", "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "categories": ["cs.CL"], "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized."}
{"id": "2508.07497", "pdf": "https://arxiv.org/pdf/2508.07497.pdf", "abs": "https://arxiv.org/abs/2508.07497", "title": "VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design", "authors": ["Leonardo Ferreira", "Gustavo Moreira", "Fabio Miranda"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. VA-Blueprint is available at\n  https://urbantk.org/va-blueprint", "summary": "Designing and building visual analytics (VA) systems is a complex, iterative\nprocess that requires the seamless integration of data processing, analytics\ncapabilities, and visualization techniques. While prior research has\nextensively examined the social and collaborative aspects of VA system\nauthoring, the practical challenges of developing these systems remain\nunderexplored. As a result, despite the growing number of VA systems, there are\nonly a few structured knowledge bases to guide their design and development. To\ntackle this gap, we propose VA-Blueprint, a methodology and knowledge base that\nsystematically reviews and categorizes the fundamental building blocks of urban\nVA systems, a domain particularly rich and representative due to its intricate\ndata and unique problem sets. Applying this methodology to an initial set of 20\nsystems, we identify and organize their core components into a multi-level\nstructure, forming an initial knowledge base with a structured blueprint for VA\nsystem development. To scale this effort, we leverage a large language model to\nautomate the extraction of these components for other 81 papers (completing a\ncorpus of 101 papers), assessing its effectiveness in scaling knowledge base\nconstruction. We evaluate our method through interviews with experts and a\nquantitative analysis of annotation metrics. Our contributions provide a deeper\nunderstanding of VA systems' composition and establish a practical foundation\nto support more structured, reproducible, and efficient system development.\nVA-Blueprint is available at https://urbantk.org/va-blueprint."}
{"id": "2508.07069", "pdf": "https://arxiv.org/pdf/2508.07069.pdf", "abs": "https://arxiv.org/abs/2508.07069", "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents."}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520.pdf", "abs": "https://arxiv.org/abs/2508.07520", "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "authors": ["Baihan Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds."}
{"id": "2508.07090", "pdf": "https://arxiv.org/pdf/2508.07090.pdf", "abs": "https://arxiv.org/abs/2508.07090", "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation."}
{"id": "2508.07576", "pdf": "https://arxiv.org/pdf/2508.07576.pdf", "abs": "https://arxiv.org/abs/2508.07576", "title": "Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor", "authors": ["Kenneth Ge", "Ryan Paul", "Priscilla Zhang", "JooYoung Seo"], "categories": ["cs.HC"], "comment": "Published at ASSETS '25", "summary": "Writing mathematical notation requires substantial effort, diverting\ncognitive resources from conceptual understanding to documentation mechanics,\nsignificantly impacting individuals with fine motor disabilities (FMDs).\nCurrent limits of speech-based math technologies rely on precise dictation of\nmath symbols and unintuitive command-based interfaces. We present a novel\nvoice-powered math workspace, applying neuroscience insights to create an\nintuitive problem-solving environment. To minimize cognitive load, we leverage\nlarge language models with our novel context engine to support natural language\ninteraction. Ultimately, we enable fluid mathematical engagement for\nindividuals with FMDs -- freed from mechanical constraints."}
{"id": "2508.07101", "pdf": "https://arxiv.org/pdf/2508.07101.pdf", "abs": "https://arxiv.org/abs/2508.07101", "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods."}
{"id": "2508.07617", "pdf": "https://arxiv.org/pdf/2508.07617.pdf", "abs": "https://arxiv.org/abs/2508.07617", "title": "On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making", "authors": ["Sarah Jabbour", "David Fouhey", "Nikola Banovic", "Stephanie D. Shepard", "Ella Kazerooni", "Michael W. Sjoding", "Jenna Wiens"], "categories": ["cs.HC", "cs.AI"], "comment": "14 pages, 10 figures, 5 tables", "summary": "AI has the potential to augment human decision making. However, even\nhigh-performing models can produce inaccurate predictions when deployed. These\ninaccuracies, combined with automation bias, where humans overrely on AI\npredictions, can result in worse decisions. Selective prediction, in which\npotentially unreliable model predictions are hidden from users, has been\nproposed as a solution. This approach assumes that when AI abstains and informs\nthe user so, humans make decisions as they would without AI involvement. To\ntest this assumption, we study the effects of selective prediction on human\ndecisions in a clinical context. We conducted a user study of 259 clinicians\ntasked with diagnosing and treating hospitalized patients. We compared their\nbaseline performance without any AI involvement to their AI-assisted accuracy\nwith and without selective prediction. Our findings indicate that selective\nprediction mitigates the negative effects of inaccurate AI in terms of decision\naccuracy. Compared to no AI assistance, clinician accuracy declined when shown\ninaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),\nbut recovered under selective prediction (64% [95% CI: 54%-73%]). However,\nwhile selective prediction nearly maintains overall accuracy, our results\nsuggest that it alters patterns of mistakes: when informed the AI abstains,\nclinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%\nincrease in missed treatments) compared to no AI input at all. Our findings\nunderscore the importance of empirically validating assumptions about how\nhumans engage with AI within human-AI systems."}
{"id": "2508.07111", "pdf": "https://arxiv.org/pdf/2508.07111.pdf", "abs": "https://arxiv.org/abs/2508.07111", "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm."}
{"id": "2508.07620", "pdf": "https://arxiv.org/pdf/2508.07620.pdf", "abs": "https://arxiv.org/abs/2508.07620", "title": "Are UX evaluation methods truly accessible", "authors": ["Andrés Eduardo Fuentes-Cortázar", "Alejandra Rivera-Hernández", "José Rafael Rojano-Cáceres"], "categories": ["cs.HC", "H.5.2"], "comment": "24 pages, 8 figures, 8 tables, submitted to TecnoL\\'ogicas ISNN\n  0123-7799", "summary": "Providing an equitable and inclusive user experience (UX) for people with\ndisabilities (PWD) is a central goal of accessible design. In the specific case\nof Deaf users, whose hearing impairments impact language development and\ncommunication, it is essential to consider their specific needs during software\nevaluation processes. This study aimed to analyze a set of UX evaluation\nmethods suggested in the literature as suitable for Deaf individuals, with the\ngoal of validating their level of accessibility in real-world contexts. The\nresearch was based on a critical review and practical application of these\nmethods, identifying their strengths and limitations in relation to the\ninteraction, perception, and comprehension of Deaf users. Traditional\nevaluation instruments, commonly designed for hearing individuals, pose\nsignificant barriers when applied to Deaf users due to their re-liance on\nauditory and cognitive abilities, as well as the lack of consideration for\ncommu-nicational accessibility. The results show that although these methods\nare frequently rec-ommended, they exhibit critical shortcomings that hinder the\ncollection of accurate and representative data. It is concluded that it is\nessential to adapt UX evaluation methods to ensure genuinely accessible\nprocesses that address the communicative and cognitive needs of the Deaf\ncommunity and accurately reflect their user experience."}
{"id": "2508.07143", "pdf": "https://arxiv.org/pdf/2508.07143.pdf", "abs": "https://arxiv.org/abs/2508.07143", "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy."}
{"id": "2508.07658", "pdf": "https://arxiv.org/pdf/2508.07658.pdf", "abs": "https://arxiv.org/abs/2508.07658", "title": "Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models", "authors": ["Shuning Zhang", "Gengrui Zhang", "Yibo Meng", "Ziyi Zhang", "Hantao Zhao", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of Visual Language Models (VLMs) has enabled\nsophisticated analysis of visual content, leading to concerns about the\ninference of sensitive user attributes and subsequent privacy risks. While\ntechnical capabilities of VLMs are increasingly studied, users' understanding,\nperceptions, and reactions to these inferences remain less explored, especially\nconcerning videos uploaded on the social media. This paper addresses this gap\nthrough a semi-structured interview (N=17), investigating user perspectives on\nVLM-driven sensitive attribute inference from their visual data. Findings\nreveal that users perceive VLMs as capable of inferring a range of attributes,\nincluding location, demographics, and socioeconomic indicators, often with\nunsettling accuracy. Key concerns include unauthorized identification, misuse\nof personal information, pervasive surveillance, and harm from inaccurate\ninferences. Participants reported employing various mitigation strategies,\nthough with skepticism about their ultimate effectiveness against advanced AI.\nUsers also articulate clear expectations for platforms and regulators,\nemphasizing the need for enhanced transparency, user control, and proactive\nprivacy safeguards. These insights are crucial for guiding the development of\nresponsible AI systems, effective privacy-enhancing technologies, and informed\npolicymaking that aligns with user expectations and societal values."}
{"id": "2508.07172", "pdf": "https://arxiv.org/pdf/2508.07172.pdf", "abs": "https://arxiv.org/abs/2508.07172", "title": "Gradient Surgery for Safe LLM Fine-Tuning", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity."}
{"id": "2508.07664", "pdf": "https://arxiv.org/pdf/2508.07664.pdf", "abs": "https://arxiv.org/abs/2508.07664", "title": "Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory", "authors": ["Shuning Zhang", "Rongjun Ma", "Ying Ma", "Shixuan Li", "Yiqun Xu", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrating memory\nfunctionalities to provide personalized and context-aware interactions.\nHowever, user understanding, practices and expectations regarding these memory\nsystems are not yet well understood. This paper presents a thematic analysis of\nsemi-structured interviews with 18 users to explore their mental models of\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\npractices, perceived benefits and drawbacks, privacy concerns and expectations\nfor future memory systems. Our findings reveal diverse and often incomplete\nmental models of how memory operates. While users appreciate the potential for\nenhanced personalization and efficiency, significant concerns exist regarding\nprivacy, control and the accuracy of remembered information. Users express a\ndesire for granular control over memory generation, management, usage and\nupdating, including clear mechanisms for reviewing, editing, deleting and\ncategorizing memories, as well as transparent insight into how memories and\ninferred information are used. We discuss design implications for creating more\nuser-centric, transparent, and trustworthy LLM memory systems."}
{"id": "2508.07173", "pdf": "https://arxiv.org/pdf/2508.07173.pdf", "abs": "https://arxiv.org/abs/2508.07173", "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements."}
{"id": "2508.07672", "pdf": "https://arxiv.org/pdf/2508.07672.pdf", "abs": "https://arxiv.org/abs/2508.07672", "title": "Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences", "authors": ["Shuning Zhang", "Ying Ma", "Jingruo Chen", "Simin Li", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of AI agents, with their complex and context-dependent\nactions, renders conventional privacy paradigms obsolete. This position paper\nargues that the current model of privacy management, rooted in a user's\nunilateral control over a passive tool, is inherently mismatched with the\ndynamic and interactive nature of AI agents. We contend that ensuring effective\nprivacy protection necessitates that the agents proactively align with users'\nprivacy preferences instead of passively waiting for the user to control. To\nground this shift, and using personalized conversational recommendation agents\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\ncontrolling users' privacy as an alignment problem, where AI agents initially\ndid not know users' preferences, and would learn their privacy preferences\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\nthe agents used alignment and Pareto optimization for aligning preferences and\nbalancing privacy and utility. We introduced formulations and instantiations,\npotential applications, as well as five challenges."}
{"id": "2508.07178", "pdf": "https://arxiv.org/pdf/2508.07178.pdf", "abs": "https://arxiv.org/abs/2508.07178", "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF."}
{"id": "2508.07677", "pdf": "https://arxiv.org/pdf/2508.07677.pdf", "abs": "https://arxiv.org/abs/2508.07677", "title": "Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration", "authors": ["Parth G. Dangi", "Yogesh Kumar Meena"], "categories": ["cs.HC"], "comment": "7 pages, 5 figures, 2 tables, selected for presentation in special\n  session of System, Man, Cybernetics (SMC) conference, 2025. The codes\n  developed for this paper is available in the GitHub repository given in the\n  conclusion section of this paper", "summary": "Brain-machine interfaces (BMIs) have significantly advanced\nneuro-rehabilitation by enhancing motor control. However, accurately decoding\ncontinuous grasp force remains a challenge, limiting the effectiveness of BMI\napplications for fine motor tasks. Current models tend to prioritise\nalgorithmic complexity rather than incorporating neurophysiological insights\ninto force control, which is essential for developing effective neural\nengineering solutions. To address this, we propose EEGForceMap, an EEG-based\nmethodology that isolates signals from the premotor-parietal region and\nextracts task-specific components. We construct three distinct time-frequency\nfeature sets, which are validated by comparing them with prior studies, and use\nthem for force prediction with linear, non-linear, and deep learning-based\nregressors. The performance of these regressors was evaluated on the\nWAY-EEG-GAL dataset that includes 12 subjects. Our results show that\nintegrating EEGForceMap approach with regressor models yields a 61.7%\nimprovement in subject-specific conditions (R-squared = 0.815) and a 55.7%\nimprovement in subject-independent conditions (R-squared = 0.785) over the\nstate-of-the-art kinematic decoder models. Furthermore, an ablation study\nconfirms that each preprocessing step significantly enhances decoding accuracy.\nThis work contributes to the advancement of responsive BMIs for stroke\nrehabilitation and assistive robotics by improving EEG-based decoding of\ndynamic grasp force."}
{"id": "2508.07179", "pdf": "https://arxiv.org/pdf/2508.07179.pdf", "abs": "https://arxiv.org/abs/2508.07179", "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications."}
{"id": "2508.07730", "pdf": "https://arxiv.org/pdf/2508.07730.pdf", "abs": "https://arxiv.org/abs/2508.07730", "title": "SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums", "authors": ["Mingyang Su", "Chao Liu", "Jingling Zhang", "WU Shuang", "Mingming Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Offering diverse perspectives on a museum artifact can deepen visitors'\nunderstanding and help avoid the cognitive limitations of a single narrative,\nultimately enhancing their overall experience. Physical museums promote\ndiversity through visitor interactions. However, it remains a challenge to\npresent multiple voices appropriately while attracting and sustaining a\nvisitor's attention in the virtual museum. Inspired by recent studies that show\nthe effectiveness of LLM-powered multi-agents in presenting different opinions\nabout an event, we propose SimViews, an interactive multi-agent system that\nsimulates visitor-to-visitor conversational patterns to promote the\npresentation of diverse perspectives. The system employs LLM-powered\nmulti-agents that simulate virtual visitors with different professional\nidentities, providing diverse interpretations of artifacts. Additionally, we\nconstructed 4 conversational patterns between users and agents to simulate\nvisitor interactions. We conducted a within-subject study with 20 participants,\ncomparing SimViews to a traditional single-agent condition. Our results show\nthat SimViews effectively facilitates the presentation of diverse perspectives\nthrough conversations, enhancing participants' understanding of viewpoints and\nengagement within the virtual museum."}
{"id": "2508.07185", "pdf": "https://arxiv.org/pdf/2508.07185.pdf", "abs": "https://arxiv.org/abs/2508.07185", "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world."}
{"id": "2508.07731", "pdf": "https://arxiv.org/pdf/2508.07731.pdf", "abs": "https://arxiv.org/abs/2508.07731", "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning", "authors": ["Abdul Basit", "Maha Nawaz", "Saim Rehman", "Muhammad Shafique"], "categories": ["cs.HC", "cs.AI", "68T50, 68T40, 68T07, 92C55", "I.2.7; I.2.9"], "comment": "7 pages, 12 figures, Accepted to 62nd DAC 2025", "summary": "Efficient control of prosthetic limbs via non-invasive brain-computer\ninterfaces (BCIs) requires advanced EEG processing, including pre-filtering,\nfeature extraction, and action prediction, performed in real time on edge AI\nhardware. Achieving this on resource-constrained devices presents challenges in\nbalancing model complexity, computational efficiency, and latency. We present\nCognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on\nembedded AI hardware, achieving real-time operation without compromising\naccuracy. The system integrates BrainFlow, an open-source library for EEG data\nacquisition and streaming, with optimized deep learning (DL) models for precise\nbrain signal classification. Using evolutionary search, we identify\nPareto-optimal DL configurations through hyperparameter tuning, optimizer\nanalysis, and window selection, analyzed individually and in ensemble\nconfigurations. We apply model compression techniques such as pruning and\nquantization to optimize models for embedded deployment, balancing efficiency\nand accuracy. We collected an EEG dataset and designed an annotation pipeline\nenabling precise labeling of brain signals corresponding to specific intended\nactions, forming the basis for training our optimized DL models. CognitiveArm\nalso supports voice commands for seamless mode switching, enabling control of\nthe prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded\nhardware, it ensures low latency and real-time responsiveness. A full-scale\nprototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,\nachieved up to 90% accuracy in classifying three core actions (left, right,\nidle). Voice integration enables multiplexed, variable movement for everyday\ntasks (e.g., handshake, cup picking), enhancing real-world performance and\ndemonstrating CognitiveArm's potential for advanced prosthetic control."}
{"id": "2508.07195", "pdf": "https://arxiv.org/pdf/2508.07195.pdf", "abs": "https://arxiv.org/abs/2508.07195", "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON."}
{"id": "2508.07854", "pdf": "https://arxiv.org/pdf/2508.07854.pdf", "abs": "https://arxiv.org/abs/2508.07854", "title": "Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms", "authors": ["Valerie Tan", "Jens Gerken"], "categories": ["cs.HC"], "comment": "3 pages, Submitted as a position paper to a workshop (\"Envisioning\n  the Future of Accessible Immersive Technology\") at the Mensch und Computer\n  2024 conference", "summary": "In this position paper, we discuss symptoms of attention deficit\nhyperactivity disorder (ADHD) in adults, as well as available forms of\ntreatment or assistance in the context of mixed reality. Mixed reality offers\nmany potentials for assisting adults with symptoms commonly found in (but not\nlimited to) ADHD, but the availability of mixed reality solutions is not only\nlimited commercially, but also limited in terms of proof-of-concept prototypes.\nWe discuss two major challenges with attention assistance using mixed reality\nsolutions: the limited availability of adult-specific prototypes and studies,\nas well as the limited number of solutions that offer continuous intervention\nof ADHD-like symptoms that users can employ in their daily life."}
{"id": "2508.07209", "pdf": "https://arxiv.org/pdf/2508.07209.pdf", "abs": "https://arxiv.org/abs/2508.07209", "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "categories": ["cs.CL", "cs.SI"], "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features."}
{"id": "2508.07980", "pdf": "https://arxiv.org/pdf/2508.07980.pdf", "abs": "https://arxiv.org/abs/2508.07980", "title": "Early Explorations of Recommender Systems for Physical Activity and Well-being", "authors": ["Alan Said"], "categories": ["cs.HC", "cs.IR"], "comment": "Second International Workshop on Recommender Systems for\n  Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys\n  2025", "summary": "As recommender systems increasingly guide physical actions, often through\nwearables and coaching tools, new challenges arise around how users interpret,\ntrust, and respond to this advice. This paper introduces a conceptual framework\nfor tangible recommendations that influence users' bodies, routines, and\nwell-being. We describe three design dimensions: trust and interpretation,\nintent alignment, and consequence awareness. These highlight key limitations in\napplying conventional recommender logic to embodied settings. Through examples\nand design reflections, we outline how future systems can support long-term\nwell-being, behavioral alignment, and socially responsible personalization."}
{"id": "2508.07229", "pdf": "https://arxiv.org/pdf/2508.07229.pdf", "abs": "https://arxiv.org/abs/2508.07229", "title": "How Does a Deep Neural Network Look at Lexical Stress?", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli."}
{"id": "2508.08020", "pdf": "https://arxiv.org/pdf/2508.08020.pdf", "abs": "https://arxiv.org/abs/2508.08020", "title": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community", "authors": ["Zeyu Yang", "Zheng Wei", "Yang Zhang", "Xian Xu", "Changyang He", "Muzhi Zhou", "Pan Hui"], "categories": ["cs.HC"], "comment": "Paper for CSCW 2025", "summary": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users."}
{"id": "2508.07248", "pdf": "https://arxiv.org/pdf/2508.07248.pdf", "abs": "https://arxiv.org/abs/2508.07248", "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "authors": ["Zhe Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER."}
{"id": "2508.08101", "pdf": "https://arxiv.org/pdf/2508.08101.pdf", "abs": "https://arxiv.org/abs/2508.08101", "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience", "authors": ["Yeana Lee Bond", "Mungyeong Choe", "Baker Kasim Hasan", "Arsh Siddiqui", "Myounghoon Jeon"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": "Submitted to International Journal of Human-Computer Studies. Bond\n  and Choe: Drafting, Review, Editing, Validation, Software, Methodology,\n  Investigation, Data Analysis, Conceptualization, Experiment training. Hasan\n  and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision,\n  Review, Resources, Project Admin, Methodology, Conceptualization. Total 34\n  pages", "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions."}
{"id": "2508.07262", "pdf": "https://arxiv.org/pdf/2508.07262.pdf", "abs": "https://arxiv.org/abs/2508.07262", "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "authors": ["Bernd J. Kröger"], "categories": ["cs.CL", "cs.RO"], "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism."}
{"id": "2508.08128", "pdf": "https://arxiv.org/pdf/2508.08128.pdf", "abs": "https://arxiv.org/abs/2508.08128", "title": "Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration", "authors": ["Vladimir Zhurov", "John Kausch", "Kamran Sedig", "Mostafa Milani"], "categories": ["cs.HC"], "comment": "Journal submission", "summary": "Ontologies play a central role in structuring knowledge across domains,\nsupporting tasks such as reasoning, data integration, and semantic search.\nHowever, their large size and complexity, particularly in fields such as\nbiomedicine, computational biology, law, and engineering, make them difficult\nfor non-experts to navigate. Formal query languages such as SPARQL offer\nexpressive access but require users to understand the ontology's structure and\nsyntax. In contrast, visual exploration tools and basic keyword-based search\ninterfaces are easier to use but often lack flexibility and expressiveness. We\nintroduce FuzzyVis, a proof-of-concept system that enables intuitive and\nexpressive exploration of complex ontologies. FuzzyVis integrates two key\ncomponents: a fuzzy logic-based querying model built on fuzzy ontology\nembeddings, and an interactive visual interface for building and interpreting\nqueries. Users can construct new composite concepts by selecting and combining\nexisting ontology concepts using logical operators such as conjunction,\ndisjunction, and negation. These composite concepts are matched against the\nontology using fuzzy membership-based embeddings, which capture degrees of\nmembership and support approximate, concept-level similarity search. The visual\ninterface supports browsing, query composition, and partial search without\nrequiring formal syntax. By combining fuzzy semantics with embedding-based\nreasoning, FuzzyVis enables flexible interpretation, efficient computation, and\nexploratory learning. Case studies demonstrate how FuzzyVis supports subtle\ninformation needs and helps users uncover relevant concepts in large, complex\nontologies."}
{"id": "2508.07273", "pdf": "https://arxiv.org/pdf/2508.07273.pdf", "abs": "https://arxiv.org/abs/2508.07273", "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability."}
{"id": "2508.08158", "pdf": "https://arxiv.org/pdf/2508.08158.pdf", "abs": "https://arxiv.org/abs/2508.08158", "title": "Can AI Explanations Make You Change Your Mind?", "authors": ["Laura Spillner", "Rachel Ringe", "Robert Porzel", "Rainer Malaka"], "categories": ["cs.HC", "cs.AI"], "comment": "This paper was presented at the Explainable AI workshop at IJCAI\n  2025: https://sites.google.com/view/xai2025/proceedings", "summary": "In the context of AI-based decision support systems, explanations can help\nusers to judge when to trust the AI's suggestion, and when to question it. In\nthis way, human oversight can prevent AI errors and biased decision-making.\nHowever, this rests on the assumption that users will consider explanations in\nenough detail to be able to catch such errors. We conducted an online study on\ntrust in explainable DSS, and were surprised to find that in many cases,\nparticipants spent little time on the explanation and did not always consider\nit in detail. We present an exploratory analysis of this data, investigating\nwhat factors impact how carefully study participants consider AI explanations,\nand how this in turn impacts whether they are open to changing their mind based\non what the AI suggests."}
{"id": "2508.07279", "pdf": "https://arxiv.org/pdf/2508.07279.pdf", "abs": "https://arxiv.org/abs/2508.07279", "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows."}
{"id": "2508.08242", "pdf": "https://arxiv.org/pdf/2508.08242.pdf", "abs": "https://arxiv.org/abs/2508.08242", "title": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making", "authors": ["Mohammed Alsobay", "David M. Rothschild", "Jake M. Hofman", "Daniel G. Goldstein"], "categories": ["cs.HC"], "comment": null, "summary": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool."}
{"id": "2508.07284", "pdf": "https://arxiv.org/pdf/2508.07284.pdf", "abs": "https://arxiv.org/abs/2508.07284", "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why."}
{"id": "2508.06849", "pdf": "https://arxiv.org/pdf/2508.06849.pdf", "abs": "https://arxiv.org/abs/2508.06849", "title": "Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development", "authors": ["Sanjana Gautam", "Mohit Chandra", "Ankolika De", "Tatiana Chakravorti", "Girik Malik", "Munmun De Choudhury"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Lived experiences fundamentally shape how individuals interact with AI\nsystems, influencing perceptions of safety, trust, and usability. While prior\nresearch has focused on developing techniques to emulate human preferences, and\nproposed taxonomies to categorize risks (such as psychological harms and\nalgorithmic biases), these efforts have provided limited systematic\nunderstanding of lived human experiences or actionable strategies for embedding\nthem meaningfully into the AI development lifecycle. This work proposes a\nframework for meaningfully integrating lived experience into the design and\nevaluation of AI systems. We synthesize interdisciplinary literature across\nlived experience philosophy, human-centered design, and human-AI interaction,\narguing that centering lived experience can lead to models that more accurately\nreflect the retrospective, emotional, and contextual dimensions of human\ncognition. Drawing from a wide body of work across psychology, education,\nhealthcare, and social policy, we present a targeted taxonomy of lived\nexperiences with specific applicability to AI systems. To ground our framework,\nwe examine three application domains (i) education, (ii) healthcare, and (iii)\ncultural alignment, illustrating how lived experience informs user goals,\nsystem expectations, and ethical considerations in each context. We further\nincorporate insights from AI system operators and human-AI partnerships to\nhighlight challenges in responsibility allocation, mental model calibration,\nand long-term system adaptation. We conclude with actionable recommendations\nfor developing experience-centered AI systems that are not only technically\nrobust but also empathetic, context-aware, and aligned with human realities.\nThis work offers a foundation for future research that bridges technical\ndevelopment with the lived experiences of those impacted by AI systems."}
{"id": "2508.07286", "pdf": "https://arxiv.org/pdf/2508.07286.pdf", "abs": "https://arxiv.org/abs/2508.07286", "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE."}
{"id": "2508.06997", "pdf": "https://arxiv.org/pdf/2508.06997.pdf", "abs": "https://arxiv.org/abs/2508.06997", "title": "Conformal Set-based Human-AI Complementarity with Multiple Experts", "authors": ["Helbert Paat", "Guohao Shen"], "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "comment": "Accepted at AAMAS 2025. Code available at:\n  https://github.com/paathelb/conformal_hai_multiple", "summary": "Decision support systems are designed to assist human experts in\nclassification tasks by providing conformal prediction sets derived from a\npre-trained model. This human-AI collaboration has demonstrated enhanced\nclassification performance compared to using either the model or the expert\nindependently. In this study, we focus on the selection of instance-specific\nexperts from a pool of multiple human experts, contrasting it with existing\nresearch that typically focuses on single-expert scenarios. We characterize the\nconditions under which multiple experts can benefit from the conformal sets.\nWith the insight that only certain experts may be relevant for each instance,\nwe explore the problem of subset selection and introduce a greedy algorithm\nthat utilizes conformal sets to identify the subset of expert predictions that\nwill be used in classifying an instance. This approach is shown to yield better\nperformance compared to naive methods for human subset selection. Based on real\nexpert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation\nstudy indicates that our proposed greedy algorithm achieves near-optimal\nsubsets, resulting in improved classification performance among multiple\nexperts."}
{"id": "2508.07295", "pdf": "https://arxiv.org/pdf/2508.07295.pdf", "abs": "https://arxiv.org/abs/2508.07295", "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa."}
{"id": "2508.07010", "pdf": "https://arxiv.org/pdf/2508.07010.pdf", "abs": "https://arxiv.org/abs/2508.07010", "title": "Narrative Memory in Machines: Multi-Agent Arc Extraction in Serialized TV", "authors": ["Roberto Balestri", "Guglielmo Pescatore"], "categories": ["cs.MM", "cs.HC", "cs.MA"], "comment": null, "summary": "Serialized television narratives present significant analytical challenges\ndue to their complex, temporally distributed storylines that necessitate\nsophisticated information management. This paper introduces a multi-agent\nsystem (MAS) designed to extract and analyze narrative arcs by implementing\nprinciples of computational memory architectures. The system conceptualizes\nnarrative understanding through analogues of human memory: Large Language\nModels (LLMs) provide a form of semantic memory for general narrative patterns,\nwhile a vector database stores specific arc progressions as episodic memories.\nA multi-agent workflow simulates working memory processes to integrate these\ninformation types. Tested on the first season of Grey's Anatomy (ABC 2005-),\nthe MAS identifies three arc types: Anthology (self-contained), Soap\n(relationship-focused), and Genre-Specific. These arcs and their episodic\ndevelopments are stored in a vector database, facilitating structured analysis\nand semantic comparison. To bridge automation with critical interpretation, a\ngraphical interface enables human oversight and refinement of the system's\nnarrative memory. While demonstrating strong performance in identifying\nAnthology Arcs and character entities, the system's reliance on textual\nparatexts (episode summaries) revealed limitations in discerning overlapping\narcs and opaque dynamics, underscoring the challenges in computational memory\nconsolidation versus human holistic understanding. This memory-centric approach\nhighlights the potential of combining AI-driven memory processing with human\nexpertise. Beyond television, it offers promise for serialized written formats\nwhere narrative is entirely text-based. Future work will focus on integrating\nmultimodal inputs to enrich episodic memory, refining memory integration\nmechanisms within the MAS, and expanding testing across diverse genres."}
{"id": "2508.07308", "pdf": "https://arxiv.org/pdf/2508.07308.pdf", "abs": "https://arxiv.org/abs/2508.07308", "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Liò"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes."}
{"id": "2508.07230", "pdf": "https://arxiv.org/pdf/2508.07230.pdf", "abs": "https://arxiv.org/abs/2508.07230", "title": "Shaping a Profession, Building a Community: A Practitioner-Led Investigation of Public Interest Technologists in Civil Society", "authors": ["Mallory Knodel", "Mallika Balakrishnan", "Lauren M. Chambers"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "The label `public interest technology' (PIT) is growing in popularity among\nthose seeking to use `tech for good' - especially among technical practitioners\nworking in civil society and nonprofit organizations. PIT encompasses a broad\nrange of sociotechnical work across professional domains and sectors; however,\nthe trend remains understudied within sociotechnical research. This paper\ndescribes a mixed-methods study, designed and conducted by PIT practitioners at\nthe Center for Democracy and Technology, that characterizes technologists\nwithin the specific context of civil society, civil rights, and advocacy\norganizations in North America and Western Europe. We conducted interviews with\ncivil society leaders to investigate how PIT practitioners position the field\nand themselves, and we held a roundtable discussion bringing diverse voices\ntogether to make meaning of this growing phenomenon. Ultimately, we find that\nPIT remains both defined and plagued by its expansiveness, and that today's\ncivil society public interest technologists see a need for both (a) more robust\nprofessionalization infrastructures, including philanthropic attention, and (b)\nmore engaged, coherent community. This study illuminates a nascent intersection\nof technology and policy on-the-ground that is of growing relevance to critical\nsociotechnical research on the shifting relationship between computing and\nsociety."}
{"id": "2508.07321", "pdf": "https://arxiv.org/pdf/2508.07321.pdf", "abs": "https://arxiv.org/abs/2508.07321", "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available."}
{"id": "2508.07501", "pdf": "https://arxiv.org/pdf/2508.07501.pdf", "abs": "https://arxiv.org/abs/2508.07501", "title": "FormCoach: Lift Smarter, Not Harder", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI."}
{"id": "2508.07325", "pdf": "https://arxiv.org/pdf/2508.07325.pdf", "abs": "https://arxiv.org/abs/2508.07325", "title": "Strategies of Code-switching in Human-Machine Dialogs", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use."}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517.pdf", "abs": "https://arxiv.org/abs/2508.07517", "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "authors": ["Joseph T. Colonel", "Baihan Lin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'')."}
{"id": "2508.07375", "pdf": "https://arxiv.org/pdf/2508.07375.pdf", "abs": "https://arxiv.org/abs/2508.07375", "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide."}
{"id": "2508.07579", "pdf": "https://arxiv.org/pdf/2508.07579.pdf", "abs": "https://arxiv.org/abs/2508.07579", "title": "From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women", "authors": ["Ziqi Pan", "Runhua Zhang", "Jiehui Luo", "Yuanhao Zhang", "Yue Deng", "Xiaojuan Ma"], "categories": ["cs.SI", "cs.CY", "cs.HC"], "comment": null, "summary": "Hashtags serve as identity markers and connection tools in online queer\ncommunities. Recently, the Western-origin #wlw (women-loving-women) hashtag has\nrisen in the Chinese lesbian community on RedNote, coinciding with user\nmigration triggered by the temporary US TikTok ban. This event provides a\nunique lens to study cross-cultural hashtag ingress and diffusion through the\npopulations' responsive behaviors in cyber-migration. In this paper, we\nconducted a two-phase content analysis of 418 #wlw posts from January and\nApril, examining different usage patterns during the hashtag's ingress and\ndiffusion. Results indicate that the successful introduction of #wlw was\nfacilitated by TikTok immigrants' bold importation, both populations' mutual\ninterpretation, and RedNote natives' discussions. In current manifestation of\ndiffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer\nlife, and semantically expands to support feminism discourse. Our findings\nprovide empirical insights for enhancing the marginalized communities'\ncross-cultural communication."}
{"id": "2508.07414", "pdf": "https://arxiv.org/pdf/2508.07414.pdf", "abs": "https://arxiv.org/abs/2508.07414", "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems."}
{"id": "2508.07671", "pdf": "https://arxiv.org/pdf/2508.07671.pdf", "abs": "https://arxiv.org/abs/2508.07671", "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled."}
{"id": "2508.07434", "pdf": "https://arxiv.org/pdf/2508.07434.pdf", "abs": "https://arxiv.org/abs/2508.07434", "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods."}
{"id": "2508.07872", "pdf": "https://arxiv.org/pdf/2508.07872.pdf", "abs": "https://arxiv.org/abs/2508.07872", "title": "Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI", "authors": ["Holli Sargeant", "Mackenzie Jorgensen", "Arina Shah", "Adrian Weller", "Umang Bhatt"], "categories": ["cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Uncertainty in artificial intelligence (AI) predictions poses urgent legal\nand ethical challenges for AI-assisted decision-making. We examine two\nalgorithmic interventions that act as guardrails for human-AI collaboration:\nselective abstention, which withholds high-uncertainty predictions from human\ndecision-makers, and selective friction, which delivers those predictions\ntogether with salient warnings or disclosures that slow the decision process.\nResearch has shown that selective abstention based on uncertainty can\ninadvertently exacerbate disparities and disadvantage under-represented groups\nthat disproportionately receive uncertain predictions. In this paper, we\nprovide the first integrated socio-technical and legal analysis of\nuncertainty-based algorithmic interventions. Through two case studies,\nAI-assisted consumer credit decisions and AI-assisted content moderation, we\ndemonstrate how the seemingly neutral use of uncertainty thresholds can trigger\ndiscriminatory impacts. We argue that, although both interventions pose risks\nof unlawful discrimination under UK law, selective frictions offer a promising\npathway toward fairer and more accountable AI-assisted decision-making by\npreserving transparency and encouraging more cautious human judgment."}
{"id": "2508.07479", "pdf": "https://arxiv.org/pdf/2508.07479.pdf", "abs": "https://arxiv.org/abs/2508.07479", "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs."}
{"id": "2508.07875", "pdf": "https://arxiv.org/pdf/2508.07875.pdf", "abs": "https://arxiv.org/abs/2508.07875", "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images", "authors": ["Shuo Han", "Ahmed Karam Eldaly", "Solomon Sunday Oyelere"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics."}
{"id": "2508.07484", "pdf": "https://arxiv.org/pdf/2508.07484.pdf", "abs": "https://arxiv.org/abs/2508.07484", "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities."}
{"id": "2508.07923", "pdf": "https://arxiv.org/pdf/2508.07923.pdf", "abs": "https://arxiv.org/abs/2508.07923", "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection", "authors": ["Jakub Binda", "Valentina Paneta", "Vasileios Eleftheriadis", "Hongkyou Chung", "Panagiotis Papadimitroulas", "Neo Christopher Chung"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance."}
{"id": "2508.07516", "pdf": "https://arxiv.org/pdf/2508.07516.pdf", "abs": "https://arxiv.org/abs/2508.07516", "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "categories": ["cs.CL"], "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models."}
{"id": "2508.07989", "pdf": "https://arxiv.org/pdf/2508.07989.pdf", "abs": "https://arxiv.org/abs/2508.07989", "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility", "authors": ["Xiantao Zhang"], "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments."}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517.pdf", "abs": "https://arxiv.org/abs/2508.07517", "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "authors": ["Joseph T. Colonel", "Baihan Lin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'')."}
{"id": "2508.08043", "pdf": "https://arxiv.org/pdf/2508.08043.pdf", "abs": "https://arxiv.org/abs/2508.08043", "title": "False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability", "authors": ["Yancheng Jiang", "Yan Jiang", "Ruochen Zhou", "Yi-Chao Chen", "Xiaoyu Ji", "Wenyuan Xu"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Virtual Reality (VR) techniques, serving as the bridge between the real and\nvirtual worlds, have boomed and are widely used in manufacturing, remote\nhealthcare, gaming, etc. Specifically, VR systems offer users immersive\nexperiences that include both perceptions and actions. Various studies have\ndemonstrated that attackers can manipulate VR software to influence users'\ninteractions, including perception and actions. However, such attacks typically\nrequire strong access and specialized expertise. In this paper, we are the\nfirst to present a systematic analysis of physical attacks against VR systems\nand introduce False Reality, a new attack threat to VR devices without\nrequiring access to or modification of their software. False Reality disturbs\nVR system services by tampering with sensor measurements, and further spoofing\nusers' perception even inducing harmful actions, e.g., inducing dizziness or\ncausing users to crash into obstacles, by exploiting perceptual and\npsychological effects. We formalize these threats through an attack pathway\nframework and validate three representative pathways via physical experiments\nand user studies on five commercial VR devices. Finally, we further propose a\ndefense prototype to mitigate such threats. Our findings shall provide valuable\ninsights for enhancing the security and resilience of future VR systems."}
{"id": "2508.07534", "pdf": "https://arxiv.org/pdf/2508.07534.pdf", "abs": "https://arxiv.org/abs/2508.07534", "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems."}
{"id": "2311.00721", "pdf": "https://arxiv.org/pdf/2311.00721.pdf", "abs": "https://arxiv.org/abs/2311.00721", "title": "Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Shreya Ghosh", "Aneesh Krishna", "Tom Gedeon"], "categories": ["cs.HC", "cs.LG", "cs.SI"], "comment": "26 pages, combining the main content and the appendices, unlike\n  having them separated in the published version at IEEE Xplore\n  (https://doi.org/10.1109/TAFFC.2025.3590107)", "summary": "Empathy indicates an individual's ability to understand others. Over the past\nfew years, empathy has drawn attention from various disciplines, including but\nnot limited to Affective Computing, Cognitive Science, and Psychology.\nDetecting empathy has potential applications in society, healthcare and\neducation. Despite being a broad and overlapping topic, the avenue of empathy\ndetection leveraging Machine Learning remains underexplored from a systematic\nliterature review perspective. We collected 849 papers from 10 well-known\nacademic databases, systematically screened them and analysed the final 82\npapers. Our analyses reveal several prominent task formulations - including\nempathy on localised utterances or overall expressions, unidirectional or\nparallel empathy, and emotional contagion - in monadic, dyadic and group\ninteractions. Empathy detection methods are summarised based on four input\nmodalities - text, audiovisual, audio and physiological signals - thereby\npresenting modality-specific network architecture design protocols. We discuss\nchallenges, research gaps and potential applications in the Affective\nComputing-based empathy domain, which can facilitate new avenues of\nexploration. We further enlist the public availability of datasets and codes.\nThis paper, therefore, provides a structured overview of recent advancements\nand remaining challenges towards developing a robust empathy detection system\nthat could meaningfully contribute to enhancing human well-being."}
{"id": "2508.07592", "pdf": "https://arxiv.org/pdf/2508.07592.pdf", "abs": "https://arxiv.org/abs/2508.07592", "title": "IBPS: Indian Bail Prediction System", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system."}
{"id": "2405.13701", "pdf": "https://arxiv.org/pdf/2405.13701.pdf", "abs": "https://arxiv.org/abs/2405.13701", "title": "Metabook: A Mobile-to-Headset Pipeline for 3D Story Book Creation in Augmented Reality", "authors": ["Yibo Wang", "Yuanyuan Mao", "Lik-Hang Lee", "Shi-ting Ni", "Zeyu Wang", "Xiaole Gu", "Pan Hui"], "categories": ["cs.HC"], "comment": null, "summary": "The AR 3D book has shown significant potential in enhancing students'\nlearning outcomes. However, the creation process of 3D books requires a\nsignificant investment of time, effort, and specialized skills. Thus, in this\npaper, we first conduct a three-day workshop investigating how AI can support\nthe automated creation of 3D books. Informed by the design insights derived\nfrom the workshop, we developed Metabook, a system that enables even novice\nusers to create 3D books from text automatically. To our knowledge, Metabook is\nthe first system to offer end-to-end 3D book generation. A follow-up study with\nadult users indicates that Metabook enables inexperienced users to create 3D\nbooks, achieving reduced efforts and shortened preparation time. We\nsubsequently recruited 22 children to examine the effects of AR 3D books on\nchildren's learning compared with paper-based books. The findings indicate that\n3D books significantly enhance children's interest, improve memory retention,\nand reduce cognitive load, though no significant improvement was observed in\ncomprehension. We conclude by discussing strategies for more effectively\nleveraging 3D books to support children's learning and offer practical\nrecommendations for educators."}
{"id": "2508.07598", "pdf": "https://arxiv.org/pdf/2508.07598.pdf", "abs": "https://arxiv.org/abs/2508.07598", "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "categories": ["cs.CL"], "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection."}
{"id": "2407.02810", "pdf": "https://arxiv.org/pdf/2407.02810.pdf", "abs": "https://arxiv.org/abs/2407.02810", "title": "Understanding the Prevalence of Caste: A Critical Discourse Analysis of Caste-based Marginalization on X", "authors": ["Nayana Kirasur", "Shagun Jhaver"], "categories": ["cs.HC"], "comment": "35 pages, 11 figures, 1 table", "summary": "Despite decades of anti-caste efforts, sociocultural practices that\nmarginalize lower-caste groups in India remain prevalent and have even\nproliferated with the use of social media. This paper examines how groups\nengaged in caste-based discrimination leverage platform affordances of the\nsocial media site X (formerly Twitter) to circulate and reinforce caste\nideologies. Using a critical discourse analysis (CDA) approach, we examine the\nrhetorical and organizing strategies of 50 X profiles representing upper-caste\ncollectives. We find that these profiles leverage platform affordances such as\ninformation control, bandwidth, visibility, searchability, and shareability to\nconstruct two main arguments: (1) that their upper caste culture deserves a\nsuperior status and (2) that they are the \"true\" victims of oppression in\nsociety. These profiles' digitally mediated discursive strategies contribute to\nthe marginalization of lower castes by normalizing caste cultures,\nstrengthening caste networks, reinforcing caste discrimination, and diminishing\nanti-caste measures. Our analysis builds upon previous HCI conceptualizations\nof online harms and safety to inform how to address caste-based\nmarginalization. We offer theoretical and methodological suggestions for\ncritical HCI research focused on studying the mechanisms of power along other\nsocial categories such as race and gender."}
{"id": "2508.07630", "pdf": "https://arxiv.org/pdf/2508.07630.pdf", "abs": "https://arxiv.org/abs/2508.07630", "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments."}
{"id": "2409.01399", "pdf": "https://arxiv.org/pdf/2409.01399.pdf", "abs": "https://arxiv.org/abs/2409.01399", "title": "Intents, Techniques, and Components: a Unified Analysis of Interaction Authoring Tasks in Data Visualization", "authors": ["Hyemi Song", "Sai Gopinath", "Zhicheng Liu"], "categories": ["cs.HC"], "comment": null, "summary": "There is a growing interest in designing tools to support interactivity\nspecification and authoring in data visualization. To develop expressive and\nflexible tools, we need theories and models that describe the task space of\ninteraction authoring. Although multiple taxonomies and frameworks exist for\ninteractive visualization, they primarily focus on how visualizations are used,\nnot how interactivity is composed. To fill this gap, we conduct an analysis of\n592 interaction units from 47 real-world visualization applications. Based on\nthe analysis, we present a unified analysis of interaction authoring tasks\nacross three levels of description: intents, representative techniques, and\nlow-level implementation components. We examine our framework's descriptive,\nevaluative, and generative powers for critiquing existing interactivity\nauthoring tools and informing new tool development."}
{"id": "2508.07690", "pdf": "https://arxiv.org/pdf/2508.07690.pdf", "abs": "https://arxiv.org/abs/2508.07690", "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval", "authors": ["Luyao Zhuang", "Qinggang Zhang", "Huachi Zhou", "Juhua Liu", "Qing Li", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting."}
{"id": "2409.02053", "pdf": "https://arxiv.org/pdf/2409.02053.pdf", "abs": "https://arxiv.org/abs/2409.02053", "title": "Augmented Reality Assistive Technologies for Disabled Individuals", "authors": ["Riju Marwah", "Jyotin Singh Thakur", "Pranav Tanwar"], "categories": ["cs.HC"], "comment": "8 pages, 12 figures, 5 tables", "summary": "Augmented Reality (AR) technologies hold immense potential for\nrevolutionizing the way individuals with disabilities interact with the world.\nAR systems can provide real-time assistance and support by overlaying digital\ninformation over the physical environment based on the requirements of the use,\nhence addressing different types of disabilities. Through an in-depth analysis\nof four case studies, this paper aims to provide a comprehensive overview of\nthe current-state-of-the-art in AR assistive technologies for individuals with\ndisabilities, highlighting their potential to assist and transform their lives.\nThe findings show the significance that AR has made to bridge the accessibility\ngap, while also discussing the challenges faced and ethical considerations\nassociated with the implementation across the various cases. This is done\nthrough theory analysis, practical examples, and future projections that will\nmotivate and seek to inspire further innovation in this very relevant area of\nexploration."}
{"id": "2508.07702", "pdf": "https://arxiv.org/pdf/2508.07702.pdf", "abs": "https://arxiv.org/abs/2508.07702", "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities."}
{"id": "2410.14252", "pdf": "https://arxiv.org/pdf/2410.14252.pdf", "abs": "https://arxiv.org/abs/2410.14252", "title": "Harmony: A Human-Aware, Responsive, Modular Assistant with a Locally Deployed Large Language Model", "authors": ["Ziqi Yin", "Mingxin Zhang", "Daisuke Kawahara"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) offer powerful capabilities for natural language\nunderstanding, enabling more intelligent smart home assistants. However,\nexisting systems often rely on cloud-based LLMs, raising concerns around user\nprivacy and system dependency on external connectivity. In this work, we\npresent Harmony, a privacy-preserving and robust smart home assistant powered\nby the locally deployable Llama3-8B model. Beyond protecting user data, Harmony\nalso addresses reliability challenges of smaller models, such as hallucination\nand instruction misinterpretation, through structured prompting and modular\nagent design. Experimental results in both virtual environments and user\nstudies show that Harmony achieves performance comparable to GPT-4-based\nsystems, while enabling offline, proactive, and personalized smart home\ninteraction."}
{"id": "2508.07753", "pdf": "https://arxiv.org/pdf/2508.07753.pdf", "abs": "https://arxiv.org/abs/2508.07753", "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models", "authors": ["Zhenliang Zhang", "Junzhe Zhang", "Xinyu Hu", "HuiXuan Zhang", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": "Accepted by CIKM 2025 (Full Paper)", "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation."}
{"id": "2410.21596", "pdf": "https://arxiv.org/pdf/2410.21596.pdf", "abs": "https://arxiv.org/abs/2410.21596", "title": "Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users", "authors": ["Auren R. Liu", "Pat Pataranutaporn", "Pattie Maes"], "categories": ["cs.HC"], "comment": "31 pages, 14 figures, accepted to AIES 2025", "summary": "Companion chatbots offer a potential solution to the growing epidemic of\nloneliness, but their impact on users' psychosocial well-being remains poorly\nunderstood, raising critical ethical questions about their deployment and\ndesign. This study presents a large-scale survey (n = 404) of regular users of\ncompanion chatbots, investigating the relationship between chatbot usage and\nloneliness. We develop a model explaining approximately 50% of variance in\nloneliness; while usage does not directly predict loneliness, we identify\nfactors including neuroticism, social network size, and problematic use.\nThrough cluster analysis and mixed-methods thematic analysis combining manual\ncoding with automated theme extraction, we identify seven distinct user\nprofiles demonstrating that companion chatbots can either enhance or\npotentially harm psychological well-being depending on user characteristics.\nDifferent usage patterns can lead to markedly different outcomes, with some\nusers experiencing enhanced social confidence while others risk further\nisolation. These findings have significant implications for responsible AI\ndevelopment, suggesting that one-size-fits-all approaches to AI companionship\nmay be ethically problematic. Our work contributes to the ongoing dialogue\nabout the role of AI in social and emotional support, offering insights for\ndeveloping more targeted and ethical approaches to AI companionship that\ncomplement rather than replace human connections."}
{"id": "2508.07781", "pdf": "https://arxiv.org/pdf/2508.07781.pdf", "abs": "https://arxiv.org/abs/2508.07781", "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation", "authors": ["Zeyu Yang", "Lai Wei", "Roman Koshkin", "Xi Chen", "Satoshi Nakamura"], "categories": ["cs.CL"], "comment": null, "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems."}
{"id": "2411.09969", "pdf": "https://arxiv.org/pdf/2411.09969.pdf", "abs": "https://arxiv.org/abs/2411.09969", "title": "Steering AI-Driven Personalization of Scientific Text for General Audiences", "authors": ["Taewook Kim", "Dhruv Agarwal", "Jordan Ackerman", "Manaswi Saha"], "categories": ["cs.HC", "cs.AI"], "comment": "28 pages, 7 figures, 1 table. Accepted to PACM HCI (CSCW 2025)", "summary": "Digital media platforms (e.g., science blogs) offer opportunities to\ncommunicate scientific content to general audiences at scale. However, these\naudiences vary in their scientific expertise, literacy levels, and personal\nbackgrounds, making effective science communication challenging. To address\nthis challenge, we designed TranSlider, an AI-powered tool that generates\npersonalized translations of scientific text based on individual user profiles\n(e.g., hobbies, location, and education). Our tool features an interactive\nslider that allows users to steer the degree of personalization from 0 (weakly\nrelatable) to 100 (strongly relatable), leveraging LLMs to generate the\ntranslations with chosen degrees. Through an exploratory study with 15\nparticipants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Drawing on these findings, we discuss several implications\nfor facilitating science communication and designing steerable interfaces to\nsupport human-AI alignment."}
{"id": "2508.07785", "pdf": "https://arxiv.org/pdf/2508.07785.pdf", "abs": "https://arxiv.org/abs/2508.07785", "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "authors": ["Haoyuan Wu", "Haoxing Chen", "Xiaodong Chen", "Zhanchao Zhou", "Tieyuan Chen", "Yihong Zhuang", "Guoshan Lu", "Zenan Huang", "Junbo Zhao", "Lin Liu", "Zhenzhong Lan", "Bei Yu", "Jianguo Li"], "categories": ["cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size."}
{"id": "2411.17589", "pdf": "https://arxiv.org/pdf/2411.17589.pdf", "abs": "https://arxiv.org/abs/2411.17589", "title": "Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust Dynamics", "authors": ["Julia Ive", "Vishal Yadav", "Mariia Ignashina", "Matthew Rand", "Paulina Bondaronek"], "categories": ["cs.HC"], "comment": "10 pages, 25 references, 2 figures and 3 tables", "summary": "Introduction: The use of chatbots is becoming increasingly important across\nvarious aspects of daily life. However, the privacy concerns associated with\nthese communications have not yet been thoroughly addressed. The aim of this\nstudy was to investigate user awareness of privacy risks in chatbot\ninteractions, the privacy-preserving behaviours users practice, and how these\nbehaviours relate to their awareness of privacy threats, even when no immediate\nthreat is perceived. Methods: We developed a novel \"privacy-safe\" setup to\nanalyse user behaviour under the guarantees of anonymization and non-sharing.\nWe employed a mixed-methods approach, starting with the quantification of\nbroader trends by coding responses, followed by conducting a qualitative\ncontent analysis to gain deeper insights. Results: Overall, there was a\nsubstantial lack of understanding among users about how chatbot providers\nhandle data (27% of the participants) and the basics of privacy risks (76% of\nthe participants). Older users, in particular, expressed fears that chatbot\nproviders might sell their data. Moreover, even users with privacy knowledge do\nnot consistently exhibit privacy-preserving behaviours when assured of\ntransparent data processing by chatbots. Notably, under-protective behaviours\nwere observed among more expert users. Discussion: These findings highlight the\nneed for a strategic approach to enhance user education on privacy concepts to\nensure informed decision when interacting with chatbot technology. This\nincludes the development of tools to help users monitor and control the\ninformation they share with chatbots"}
{"id": "2508.07805", "pdf": "https://arxiv.org/pdf/2508.07805.pdf", "abs": "https://arxiv.org/abs/2508.07805", "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks."}
{"id": "2412.08185", "pdf": "https://arxiv.org/pdf/2412.08185.pdf", "abs": "https://arxiv.org/abs/2412.08185", "title": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers", "authors": ["Houjiang Liu", "Jacek Gwizdka", "Matthew Lease"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Accepted at CSCW 2025", "summary": "Given the volume of potentially false claims online, claim prioritization is\nessential in allocating limited human resources available for fact-checking. In\nthis study, we perceive claim prioritization as an information retrieval (IR)\ntask: just as multidimensional IR relevance, with many factors influencing\nwhich search results a user deems relevant, checkworthiness is also\nmulti-faceted, subjective, and even personal, with many factors influencing how\nfact-checkers triage and select which claims to check. Our study investigates\nboth the multidimensional nature of checkworthiness and effective tool support\nto assist fact-checkers in claim prioritization. Methodologically, we pursue\nResearch through Design combined with mixed-method evaluation.\n  Specifically, we develop an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncover a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration."}
{"id": "2508.07810", "pdf": "https://arxiv.org/pdf/2508.07810.pdf", "abs": "https://arxiv.org/abs/2508.07810", "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis", "authors": ["Olga Kellert", "Muhammad Imran", "Nicholas Hill Matlis", "Mahmud Uz Zaman", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL"], "comment": null, "summary": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA."}
{"id": "2502.03788", "pdf": "https://arxiv.org/pdf/2502.03788.pdf", "abs": "https://arxiv.org/abs/2502.03788", "title": "Frontend Diffusion: Empowering Self-Representation of Junior Researchers and Designers Through Multi-agent System", "authors": ["Zijian Ding", "Qinshi Zhang", "Mohan Chi", "Ziyi Wang"], "categories": ["cs.HC"], "comment": null, "summary": "With the continuous development of generative AI's logical reasoning\nabilities, AI's growing code-generation potential poses challenges for both\ntechnical and creative professionals. But how can these advances be directed\ntoward empowering junior researchers and designers who often require additional\nhelp to build and express their professional and personal identities? We\nintroduce Frontend Diffusion, a multi-agent coding system transforming\nuser-drawn layouts and textual prompts into refined website code, thereby\nsupporting self-representation goals. A user study with 13 junior researchers\nand designers shows AI as a human capability enhancer rather than a\nreplacement, and highlights the importance of bidirectional human-AI alignment.\nWe then discuss future work such as leveraging AI for career development and\nfostering bidirectional human-AI alignment of multi-agent systems."}
{"id": "2508.07827", "pdf": "https://arxiv.org/pdf/2508.07827.pdf", "abs": "https://arxiv.org/abs/2508.07827", "title": "Evaluating Large Language Models as Expert Annotators", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning."}
{"id": "2504.14571", "pdf": "https://arxiv.org/pdf/2504.14571.pdf", "abs": "https://arxiv.org/abs/2504.14571", "title": "Prompt-Hacking: The New p-Hacking?", "authors": ["Thomas Kosch", "Sebastian Feger"], "categories": ["cs.HC"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly embedded in empirical\nresearch workflows, their use as analytical tools for quantitative or\nqualitative data raises pressing concerns for scientific integrity. This\nopinion paper draws a parallel between \"prompt-hacking\", the strategic tweaking\nof prompts to elicit desirable outputs from LLMs, and the well-documented\npractice of \"p-hacking\" in statistical analysis. We argue that the inherent\nbiases, non-determinism, and opacity of LLMs make them unsuitable for data\nanalysis tasks demanding rigor, impartiality, and reproducibility. We emphasize\nhow researchers may inadvertently, or even deliberately, adjust prompts to\nconfirm hypotheses while undermining research validity. We advocate for a\ncritical view of using LLMs in research, transparent prompt documentation, and\nclear standards for when LLM use is appropriate. We discuss how LLMs can\nreplace traditional analytical methods, whereas we recommend that LLMs should\nonly be used with caution, oversight, and justification."}
{"id": "2508.07849", "pdf": "https://arxiv.org/pdf/2508.07849.pdf", "abs": "https://arxiv.org/abs/2508.07849", "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding", "authors": ["Amrita Singh", "H. Suhan Karaca", "Aditya Joshi", "Hye-young Paik", "Jiaojiao Jiang"], "categories": ["cs.CL"], "comment": "Under review. 4 pages + references", "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems."}
{"id": "2505.00821", "pdf": "https://arxiv.org/pdf/2505.00821.pdf", "abs": "https://arxiv.org/abs/2505.00821", "title": "Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users", "authors": ["Jeffrey Basoah", "Jay L. Cunningham", "Erica Adams", "Alisha Bose", "Aditi Jain", "Kaustubh Yadav", "Zhengyang Yang", "Katharina Reinecke", "Daniela Rosner"], "categories": ["cs.HC"], "comment": "accepted to CSCW 2025", "summary": "AI-supported writing technologies (AISWT) that provide grammatical\nsuggestions, autocomplete sentences, or generate and rewrite text are now a\nregular feature integrated into many people's workflows. However, little is\nknown about how people perceive the suggestions these tools provide. In this\npaper, we investigate how Black American users perceive AISWT, motivated by\nprior findings in natural language processing that highlight how the underlying\nlarge language models can contain racial biases. Using interviews and\nobservational user studies with 13 Black American users of AISWT, we found a\nstrong tradeoff between the perceived benefits of using AISWT to enhance their\nwriting style and feeling like \"it wasn't built for us\". Specifically,\nparticipants reported AISWT's failure to recognize commonly used names and\nexpressions in African American Vernacular English, experiencing its\ncorrections as hurtful and alienating and fearing it might further minoritize\ntheir culture. We end with a reflection on the tension between AISWT that fail\nto include Black American culture and language, and AISWT that attempt to mimic\nit, with attention to accuracy, authenticity, and the production of social\ndifference."}
{"id": "2508.07860", "pdf": "https://arxiv.org/pdf/2508.07860.pdf", "abs": "https://arxiv.org/abs/2508.07860", "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area."}
{"id": "2505.05660", "pdf": "https://arxiv.org/pdf/2505.05660.pdf", "abs": "https://arxiv.org/abs/2505.05660", "title": "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs", "authors": ["Jeffrey Basoah", "Daniel Chechelnitsky", "Tao Long", "Katharina Reinecke", "Chrysoula Zerva", "Kaitlyn Zhou", "Mark Díaz", "Maarten Sap"], "categories": ["cs.HC"], "comment": "accepted to FAccT 2025", "summary": "As large language models (LLMs) increasingly adapt and personalize to diverse\nsets of users, there is an increased risk of systems appropriating sociolects,\ni.e., language styles or dialects that are associated with specific minoritized\nlived experiences (e.g., African American English, Queer slang). In this work,\nwe examine whether sociolect usage by an LLM agent affects user reliance on its\noutputs and user perception (satisfaction, frustration, trust, and social\npresence). We designed and conducted user studies where 498 African American\nEnglish (AAE) speakers and 487 Queer slang speakers performed a set of\nquestion-answering tasks with LLM-based suggestions in either standard American\nEnglish (SAE) or their self-identified sociolect. Our findings showed that\nsociolect usage by LLMs influenced both reliance and perceptions, though in\nsome surprising ways. Results suggest that both AAE and Queer slang speakers\nrelied more on the SAE agent, and had more positive perceptions of the SAE\nagent. Yet, only Queer slang speakers felt more social presence from the Queer\nslang agent over the SAE one, whereas only AAE speakers preferred and trusted\nthe SAE agent over the AAE one. These findings emphasize the need to test for\nbehavioral outcomes rather than simply assume that personalization would lead\nto a better and safer reliance outcome. They also highlight the nuanced\ndynamics of minoritized language in machine interactions, underscoring the need\nfor LLMs to be carefully designed to respect cultural and linguistic boundaries\nwhile fostering genuine user engagement and trust."}
{"id": "2508.07866", "pdf": "https://arxiv.org/pdf/2508.07866.pdf", "abs": "https://arxiv.org/abs/2508.07866", "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective."}
{"id": "2506.09212", "pdf": "https://arxiv.org/pdf/2506.09212.pdf", "abs": "https://arxiv.org/abs/2506.09212", "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "categories": ["cs.HC"], "comment": null, "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing."}
{"id": "2508.07902", "pdf": "https://arxiv.org/pdf/2508.07902.pdf", "abs": "https://arxiv.org/abs/2508.07902", "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity", "authors": ["Chen Cecilia Liu", "Hiba Arnaout", "Nils Kovačić", "Dana Atzil-Slonim", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Under review; joint first authors", "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists."}
{"id": "2507.06561", "pdf": "https://arxiv.org/pdf/2507.06561.pdf", "abs": "https://arxiv.org/abs/2507.06561", "title": "Towards Designing Social Interventions For Online Climate Change Denialism Discussions", "authors": ["Ruican Zhong", "Shruti Phadke", "Beth Goldberg", "Tanushree Mitra"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "As conspiracy theories gain traction, it has become crucial to research\neffective intervention strategies that can foster evidence and science-based\ndiscussions in conspiracy theory communities online. This study presents a\nnovel framework using insider language to contest conspiracy theory ideology in\nclimate change denialism on Reddit. Focusing on discussions in two Reddit\ncommunities, our research investigates reactions to pro-social and\nevidence-based intervention messages for two cohorts of users: climate change\ndeniers and climate change supporters. Specifically, we combine manual and\ngenerative AI-based methods to craft intervention messages and deploy the\ninterventions as replies on Reddit posts and comments through transparently\nlabeled bot accounts. On the one hand, we find that evidence-based\ninterventions with neutral language foster positive engagement, encouraging\nopen discussions among believers of climate change denialism. On the other,\nclimate change supporters respond positively, actively participating and\npresenting additional evidence. Our study contributes valuable insights into\nthe process and challenges of automatically delivering interventions in\nconspiracy theory communities on social media, and helps inform future research\non social media interventions."}
{"id": "2508.07937", "pdf": "https://arxiv.org/pdf/2508.07937.pdf", "abs": "https://arxiv.org/abs/2508.07937", "title": "Challenges and opportunities in portraying emotion in generated sign language", "authors": ["John C. McDonald", "Rosalee Wolfe", "Fabrizio Nunnari"], "categories": ["cs.CL"], "comment": null, "summary": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars."}
{"id": "2507.10024", "pdf": "https://arxiv.org/pdf/2507.10024.pdf", "abs": "https://arxiv.org/abs/2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."}
{"id": "2508.07955", "pdf": "https://arxiv.org/pdf/2508.07955.pdf", "abs": "https://arxiv.org/abs/2508.07955", "title": "Expert Preference-based Evaluation of Automated Related Work Generation", "authors": ["Furkan Şahinuç", "Subhabrata Dutta", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/", "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well."}
{"id": "2507.15202", "pdf": "https://arxiv.org/pdf/2507.15202.pdf", "abs": "https://arxiv.org/abs/2507.15202", "title": "TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style", "authors": ["Karim Benharrak", "Puyuan Peng", "Amy Pavel"], "categories": ["cs.HC"], "comment": "Accepted to The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST '25), September 28-October 1, 2025, Busan, Republic of\n  Korea. 19 pages", "summary": "Millions of people listen to podcasts, audio stories, and lectures, but\nediting speech remains tedious and time-consuming. Creators remove unnecessary\nwords, cut tangential discussions, and even re-record speech to make recordings\nconcise and engaging. Prior work automatically summarized speech by removing\nfull sentences (extraction), but rigid extraction limits expressivity. AI tools\ncan summarize then re-synthesize speech (abstraction), but abstraction strips\nthe speaker's style. We present TalkLess, a system that flexibly combines\nextraction and abstraction to condense speech while preserving its content and\nstyle. To edit speech, TalkLess first generates possible transcript edits,\nselects edits to maximize compression, coverage, and audio quality, then uses a\nspeech editing model to translate transcript edits into audio edits. TalkLess's\ninterface provides creators control over automated edits by separating\nlow-level wording edits (via the compression pane) from major content edits\n(via the outline pane). TalkLess achieves higher coverage and removes more\nspeech errors than a state-of-the-art extractive approach. A comparison study\n(N=12) showed that TalkLess significantly decreased cognitive load and editing\neffort in speech editing. We further demonstrate TalkLess's potential in an\nexploratory study (N=3) where creators edited their own speech."}
{"id": "2508.07959", "pdf": "https://arxiv.org/pdf/2508.07959.pdf", "abs": "https://arxiv.org/abs/2508.07959", "title": "Large Language Models for Subjective Language Understanding: A Survey", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Ben Yao", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels."}
{"id": "2507.22134", "pdf": "https://arxiv.org/pdf/2507.22134.pdf", "abs": "https://arxiv.org/abs/2507.22134", "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks", "authors": ["Yoonsu Kim", "Brandon Chin", "Kihoon Son", "Seoyoung Kim", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "While large language models (LLMs) are widely used for writing, users often\nstruggle to express their nuanced and evolving intents through prompt-based\ninterfaces. Intents -- low-level strategies or preferences for achieving a\nwriting goal -- are often vague, fluid, or even subconscious, making it\ndifficult for users to articulate and adjust them. To address this, we present\nIntentFlow, which supports the communication of dynamically evolving intents\nthroughout LLM-assisted writing. IntentFlow extracts goals and intents from\nuser prompts and presents them as editable interface components, which users\ncan revise, remove, or refine via direct manipulation or follow-up prompts.\nVisual links connect each component to the output segments it influences,\nhelping users understand model behavior. In a within-subjects study (N=12),\nparticipants using IntentFlow, compared to a chat-based baseline, expressed\ntheir intents more easily and in detail, engaged in more meaningful actions to\ncommunicate intents, such as adjusting and deleting, and produced outputs that\nbetter aligned with their evolving intents. We found that editable intent\nrepresentations help users refine and consolidate a final set of intents, which\ncan be reused across similar tasks to support consistent and transferable\nLLM-assisted writing."}
{"id": "2508.07964", "pdf": "https://arxiv.org/pdf/2508.07964.pdf", "abs": "https://arxiv.org/abs/2508.07964", "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies", "authors": ["Matthias Sperber", "Maureen de Seyssel", "Jiajun Bao", "Matthias Paulik"], "categories": ["cs.CL"], "comment": null, "summary": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting."}
{"id": "2507.22329", "pdf": "https://arxiv.org/pdf/2507.22329.pdf", "abs": "https://arxiv.org/abs/2507.22329", "title": "A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production", "authors": ["Erin Gatz", "Yasmine Kotturi", "Andrea Afua Kwamya", "Sarah Fox"], "categories": ["cs.HC"], "comment": null, "summary": "Feminist makerspaces offer community led alternatives to dominant tech\ncultures by centering care, mutual aid, and collective knowledge production.\nWhile prior CSCW research has explored their inclusive practices, less is known\nabout how these spaces sustain themselves over time. Drawing on interviews with\n18 founders and members across 8 U.S. feminist makerspaces as well as\nautoethnographic reflection, we examine the organizational and relational\npractices that support long-term endurance. We find that sustainability is not\nachieved through growth or institutionalization, but through care-driven\nstewardship, solidarity with local justice movements, and shared governance.\nThese social practices position feminist makerspaces as prefigurative\ncounterspaces - sites that enact, rather than defer, feminist values in\neveryday practice. This paper offers empirical insight into how feminist\nmakerspaces persist amid structural precarity, and highlights the forms of\nlabor and coalition-building that underpin alternative sociotechnical\ninfrastructures."}
{"id": "2508.07969", "pdf": "https://arxiv.org/pdf/2508.07969.pdf", "abs": "https://arxiv.org/abs/2508.07969", "title": "Understanding Syntactic Generalization in Structure-inducing Language Models", "authors": ["David Arps", "Hassan Sajjad", "Laura Kallmeyer"], "categories": ["cs.CL"], "comment": "Code available at https://github.com/davidarps/silm", "summary": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties."}
{"id": "2508.01765", "pdf": "https://arxiv.org/pdf/2508.01765.pdf", "abs": "https://arxiv.org/abs/2508.01765", "title": "HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion", "authors": ["Kaining Zhang", "Catarina Moreira", "Pedro Belchior", "Gun Lee", "Mark Billinghurst", "Joaquim Jorge"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We introduce \\textit{HeadZoom}, a hands-free interaction technique for\nnavigating two-dimensional visual content using head movements. HeadZoom\nenables fluid zooming and panning using only real-time head tracking. It\nsupports natural control in applications such as map exploration, radiograph\ninspection, and image browsing, where physical interaction is limited. We\nevaluated HeadZoom in a within-subjects study comparing three interaction\ntechniques-Static, Tilt Zoom, and Parallel Zoom-across spatial, error, and\nsubjective metrics. Parallel Zoom significantly reduced total head movement\ncompared to Static and Tilt modes. Users reported significantly lower perceived\nexertion for Parallel Zoom, confirming its suitability for prolonged or\nprecision-based tasks. By minimizing movement demands while maintaining task\neffectiveness, HeadZoom advances the design of head-based 2D interaction in VR\nand creates new opportunities for accessible hands-free systems for image\nexploration."}
{"id": "2508.07976", "pdf": "https://arxiv.org/pdf/2508.07976.pdf", "abs": "https://arxiv.org/abs/2508.07976", "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133.pdf", "abs": "https://arxiv.org/abs/2508.02133", "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "authors": ["Yitong Zhu", "Lei Han", "Guanxuan Jiang", "Pengyuan Zhou", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments."}
{"id": "2508.07993", "pdf": "https://arxiv.org/pdf/2508.07993.pdf", "abs": "https://arxiv.org/abs/2508.07993", "title": "The Medical Metaphors Corpus (MCC)", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "categories": ["cs.CL"], "comment": null, "summary": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools."}
{"id": "2508.02413", "pdf": "https://arxiv.org/pdf/2508.02413.pdf", "abs": "https://arxiv.org/abs/2508.02413", "title": "Improving Knowledge Graph Understanding with Contextual Views -- Extended", "authors": ["Antrea Christou", "Cogan Shimizu"], "categories": ["cs.HC"], "comment": "12 pages", "summary": "Navigating, visualizing, and discovery in graph data is frequently a\ndifficult prospect. This is especially true for knowledge graphs (KGs), due to\nhigh number of possible labeled connections to other data. However, KGs are\nfrequently equipped with an ontology as a schema. That is, it informs how the\nrelationships between data may be constrained. This additional information can\nbe leveraged to improve how (knowledge) graph data can be navigated,\nvisualized, or otherwise utilized in a discovery process. In this manuscript,\nwe introduce the Interactive Knowledge (InK) Browser. This tool specifically\ntakes advantage ontological information (i.e., knowledge) when found in KGs.\nSpecifically, we use modular views that provide various perspectives over the\ngraph, including an interactive schema view, data listings based on type,\nneighborhood connections, and geospatial depiction (where appropriate). For\nthis manuscript, we have evaluated the basic premise of this tool over a user\ngroup ($n= With this grown user survey, we continue to evaluate how scalable\ntools, including flexible views, can make KG exploration easier for a range of\napplications.)"}
{"id": "2508.07999", "pdf": "https://arxiv.org/pdf/2508.07999.pdf", "abs": "https://arxiv.org/abs/2508.07999", "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "authors": ["Ryan Wong", "Jiawei Wang", "Junjie Zhao", "Li Chen", "Yan Gao", "Long Zhang", "Xuan Zhou", "Zuo Wang", "Kai Xiang", "Ge Zhang", "Wenhao Huang", "Yang Wang", "Ke Wang"], "categories": ["cs.CL"], "comment": null, "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/"}
{"id": "2508.03974", "pdf": "https://arxiv.org/pdf/2508.03974.pdf", "abs": "https://arxiv.org/abs/2508.03974", "title": "Managing Data for Scalable and Interactive Event Sequence Visualization", "authors": ["Sayef Azad Sakin", "Katherine E. Isaacs"], "categories": ["cs.HC"], "comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization", "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."}
{"id": "2508.08011", "pdf": "https://arxiv.org/pdf/2508.08011.pdf", "abs": "https://arxiv.org/abs/2508.08011", "title": "Progressive Depth Up-scaling via Optimal Transport", "authors": ["Mingzi Cao", "Xi Wang", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains."}
{"id": "2508.05231", "pdf": "https://arxiv.org/pdf/2508.05231.pdf", "abs": "https://arxiv.org/abs/2508.05231", "title": "FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing", "authors": ["Wenjia Dong", "Xueyuan Xu", "Tianze Yu", "Junming Zhang", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Electroencephalogram (EEG)-based emotion recognition holds significant value\nin affective computing and brain-computer interfaces. However, in practical\napplications, EEG recordings are susceptible to the effects of various\nphysiological artifacts. Current approaches typically treat denoising and\nemotion recognition as independent tasks using cascaded architectures, which\nnot only leads to error accumulation, but also fails to exploit potential\nsynergies between these tasks. Moreover, conventional EEG-based emotion\nrecognition models often rely on the idealized assumption of \"perfectly\ndenoised data\", lacking a systematic design for noise robustness. To address\nthese challenges, a novel framework that deeply couples denoising and emotion\nrecognition tasks is proposed for end-to-end noise-robust emotion recognition,\ntermed as Feedback-Driven Collaborative Network for Denoising-Classification\nNexus (FDC-Net). Our primary innovation lies in establishing a dynamic\ncollaborative mechanism between artifact removal and emotion recognition\nthrough: (1) bidirectional gradient propagation with joint optimization\nstrategies; (2) a gated attention mechanism integrated with frequency-adaptive\nTransformer using learnable band-position encoding. Two most popular EEG-based\nemotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels\nwere employed to compare the artifact removal and emotion recognition\nperformance between FDC-Net and nine state-of-the-art methods. In terms of the\ndenoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of\n96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the\nemotion recognition task under physiological artifact interference, FDC-Net\nachieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on\nDREAMER."}
{"id": "2508.08050", "pdf": "https://arxiv.org/pdf/2508.08050.pdf", "abs": "https://arxiv.org/abs/2508.08050", "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)", "authors": ["Fabrizio Nunnari", "Cristina Luna Jiménez", "Rosalee Wolfe", "John C. McDonald", "Michael Filhol", "Eleni Efthimiou", "Evita Fotinea", "Thomas Hanke"], "categories": ["cs.CL"], "comment": null, "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing."}
{"id": "2501.06250", "pdf": "https://arxiv.org/pdf/2501.06250.pdf", "abs": "https://arxiv.org/abs/2501.06250", "title": "Generative AI for Cel-Animation: A Survey", "authors": ["Yunlong Tang", "Junjia Guo", "Pinxin Liu", "Zhiyuan Wang", "Hang Hua", "Jia-Xing Zhong", "Yunzhong Xiao", "Chao Huang", "Luchuan Song", "Susan Liang", "Yizhi Song", "Liu He", "Jing Bi", "Mingqian Feng", "Xinyang Li", "Zeliang Zhang", "Chenliang Xu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted by ICCV 2025 AISTORY Workshop", "summary": "Traditional Celluloid (Cel) Animation production pipeline encompasses\nmultiple essential steps, including storyboarding, layout design, keyframe\nanimation, inbetweening, and colorization, which demand substantial manual\neffort, technical expertise, and significant time investment. These challenges\nhave historically impeded the efficiency and scalability of Cel-Animation\nproduction. The rise of generative artificial intelligence (GenAI),\nencompassing large language models, multimodal models, and diffusion models,\noffers innovative solutions by automating tasks such as inbetween frame\ngeneration, colorization, and storyboard creation. This survey explores how\nGenAI integration is revolutionizing traditional animation workflows by\nlowering technical barriers, broadening accessibility for a wider range of\ncreators through tools like AniDoc, ToonCrafter, and AniSora, and enabling\nartists to focus more on creative expression and artistic innovation. Despite\nits potential, challenges like visual consistency, stylistic coherence, and\nethical considerations persist. Additionally, this paper explores future\ndirections and advancements in AI-assisted animation. For further exploration\nand resources, please visit our GitHub repository:\nhttps://github.com/yunlong10/Awesome-AI4Animation"}
{"id": "2508.08095", "pdf": "https://arxiv.org/pdf/2508.08095.pdf", "abs": "https://arxiv.org/abs/2508.08095", "title": "Dual Information Speech Language Models for Emotional Conversations", "authors": ["Chun Wang", "Chenyang Liu", "Wenze Xu", "Weihong Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at IEEE ICME 2025", "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings."}
{"id": "2504.08954", "pdf": "https://arxiv.org/pdf/2504.08954.pdf", "abs": "https://arxiv.org/abs/2504.08954", "title": "Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation", "authors": ["Terrence Neumann", "Maria De-Arteaga", "Sina Fazelpour"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "The emergent capabilities of large language models (LLMs) have prompted\ninterest in using them as surrogates for human subjects in opinion surveys.\nHowever, prior evaluations of LLM-based opinion simulation have relied heavily\non costly, domain-specific survey data, and mixed empirical results leave their\nreliability in question. To enable cost-effective, early-stage evaluation, we\nintroduce a quality control assessment designed to test the viability of\nLLM-simulated opinions on Likert-scale tasks without requiring large-scale\nhuman data for validation. This assessment comprises two key tests:\n\\emph{logical consistency} and \\emph{alignment with stakeholder expectations},\noffering a low-cost, domain-adaptable validation tool. We apply our quality\ncontrol assessment to an opinion simulation task relevant to AI-assisted\ncontent moderation and fact-checking workflows -- a socially impactful use case\n-- and evaluate seven LLMs using a baseline prompt engineering method\n(backstory prompting), as well as fine-tuning and in-context learning variants.\nNone of the models or methods pass the full assessment, revealing several\nfailure modes. We conclude with a discussion of the risk management\nimplications and release \\texttt{TopicMisinfo}, a benchmark dataset with paired\nhuman and LLM annotations simulated by various models and approaches, to\nsupport future research."}
{"id": "2508.08096", "pdf": "https://arxiv.org/pdf/2508.08096.pdf", "abs": "https://arxiv.org/abs/2508.08096", "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?", "authors": ["Lukas Gehring", "Benjamin Paaßen"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)", "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts."}
{"id": "2504.10808", "pdf": "https://arxiv.org/pdf/2504.10808.pdf", "abs": "https://arxiv.org/abs/2504.10808", "title": "TFMPathy: Tabular Foundation Model for Privacy-Aware, Generalisable Empathy Detection from Videos", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Detecting empathy from video interactions is an emerging area of research,\nparticularly in healthcare and social robotics. However, privacy and ethical\nconcerns often prevent the release of raw video data, with many datasets\ninstead shared as pre-extracted tabular features. Previous work on such\ndatasets has established classical tree-based models as the state of the art.\nMotivated by recent successes of large-scale foundation models for text, we\ninvestigate the potential of tabular foundation models (TFMs) for empathy\ndetection from video-derived tabular data. Our proposed system, TFMPathy, is\ndemonstrated with two recent TFMs (TabPFN v2 and TabICL) under both in-context\nlearning and fine-tuning paradigms. On a public human-robot interaction\nbenchmark, TFMPathy significantly improves empathy detection accuracy reported\nin the literature. While the established evaluation protocol in the literature\ndoes not ensure cross-subject generalisation, our evaluation scheme also\ncaptures such generalisation. We show that TFMPathy under a fine-tuning setup\nhas better cross-subject generalisation capacity over baseline methods\n(accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow 0.669$). Given\nthe ongoing privacy and ethical constraints around raw video sharing, the\nproposed TFMPathy system provides a practical and scalable path toward building\nAI systems dependent on human-centred video datasets. Our code is publicly\navailable at https://github.com/hasan-rakibul/TFMPathy (will be made available\nupon acceptance of this paper)."}
{"id": "2508.08110", "pdf": "https://arxiv.org/pdf/2508.08110.pdf", "abs": "https://arxiv.org/abs/2508.08110", "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0", "authors": ["Robin Huo", "Ewan Dunbar"], "categories": ["cs.CL"], "comment": "Proceedings of Interspeech 2025", "summary": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations."}
{"id": "2506.12524", "pdf": "https://arxiv.org/pdf/2506.12524.pdf", "abs": "https://arxiv.org/abs/2506.12524", "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing", "authors": ["Nuwan Bandara", "Thivya Kandappu", "Archan Misra"], "categories": ["cs.CV", "cs.HC", "cs.LG", "eess.IV"], "comment": "Accepted at 4DMR@IJCAI25: International IJCAI Workshop on 1st\n  Challenge and Workshop for 4D Micro-Expression Recognition for Mind Reading,\n  August 29, 2025, Guangzhou, China", "summary": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments. Our code\nimplementations can be found at\nhttps://github.com/eye-tracking-for-physiological-sensing/EyeLoRiN."}
{"id": "2508.08125", "pdf": "https://arxiv.org/pdf/2508.08125.pdf", "abs": "https://arxiv.org/abs/2508.08125", "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Ondřej Pražák", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/", "summary": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes."}
{"id": "2508.04651", "pdf": "https://arxiv.org/pdf/2508.04651.pdf", "abs": "https://arxiv.org/abs/2508.04651", "title": "Live Music Models", "authors": ["Lyria Team", "Antoine Caillon", "Brian McWilliams", "Cassie Tarakajian", "Ian Simon", "Ilaria Manco", "Jesse Engel", "Noah Constant", "Yunpeng Li", "Timo I. Denk", "Alberto Lalama", "Andrea Agostinelli", "Cheng-Zhi Anna Huang", "Ethan Manilow", "George Brower", "Hakan Erdogan", "Heidi Lei", "Itai Rolnick", "Ivan Grishchenko", "Manu Orsini", "Matej Kastelic", "Mauricio Zuluaga", "Mauro Verzetti", "Michael Dooley", "Ondrej Skopek", "Rafael Ferrer", "Zalán Borsos", "Äaron van den Oord", "Douglas Eck", "Eli Collins", "Jason Baldridge", "Tom Hume", "Chris Donahue", "Kehang Han", "Adam Roberts"], "categories": ["cs.SD", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance."}
{"id": "2508.08131", "pdf": "https://arxiv.org/pdf/2508.08131.pdf", "abs": "https://arxiv.org/abs/2508.08131", "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models", "authors": ["Wenze Xu", "Chun Wang", "Jiazhen Yu", "Sheng Chen", "Liang Gao", "Weihong Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "To be presented at ACPR 2025 Conference", "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets."}
{"id": "2508.04889", "pdf": "https://arxiv.org/pdf/2508.04889.pdf", "abs": "https://arxiv.org/abs/2508.04889", "title": "Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications", "authors": ["Theia Henderson", "David R. Karger", "David D. Clark"], "categories": ["cs.SI", "cs.HC", "cs.SE"], "comment": "Accepted to The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST '25), September 28-October 1, 2025, Busan, Republic of\n  Korea. 21 pages", "summary": "Most social applications, from Twitter to Wikipedia, have rigid\none-size-fits-all designs, but building new social applications is both\ntechnically challenging and results in applications that are siloed away from\nexisting communities. We present Graffiti, a system that can be used to build a\nwide variety of personalized social applications with relative ease that also\ninteroperate with each other. People can freely move between a plurality of\ndesigns -- each with its own aesthetic, feature set, and moderation -- all\nwithout losing their friends or data.\n  Our concept of total reification makes it possible for seemingly\ncontradictory designs, including conflicting moderation rules, to interoperate.\nConversely, our concept of channels prevents interoperation from occurring by\naccident, avoiding context collapse.\n  Graffiti applications interact through a minimal client-side API, which we\nshow admits at least two decentralized implementations. Above the API, we built\na Vue plugin, which we use to develop applications similar to Twitter,\nMessenger, and Wikipedia using only client-side code. Our case studies explore\nhow these and other novel applications interoperate, as well as the broader\necosystem that Graffiti enables."}
{"id": "2508.08139", "pdf": "https://arxiv.org/pdf/2508.08139.pdf", "abs": "https://arxiv.org/abs/2508.08139", "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "authors": ["Tianyi Zhou", "Johanne Medina", "Sanjay Chawla"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation."}
{"id": "2508.08140", "pdf": "https://arxiv.org/pdf/2508.08140.pdf", "abs": "https://arxiv.org/abs/2508.08140", "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective", "authors": ["Jun Wang", "Zaifu Zhan", "Qixin Zhang", "Mingquan Lin", "Meijia Song", "Rui Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency."}
{"id": "2508.08149", "pdf": "https://arxiv.org/pdf/2508.08149.pdf", "abs": "https://arxiv.org/abs/2508.08149", "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "authors": ["Wentao Jiang", "Xiang Feng", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Zhe Chen", "Bo Du", "Jing Zhang"], "categories": ["cs.CL"], "comment": "17 pages, 4 figures", "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG."}
{"id": "2508.08163", "pdf": "https://arxiv.org/pdf/2508.08163.pdf", "abs": "https://arxiv.org/abs/2508.08163", "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo", "authors": ["Mandira Sawkar", "Samay U. Shetty", "Deepak Pandita", "Tharindu Cyril Weerasooriya", "Christopher M. Homan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata."}
{"id": "2508.08192", "pdf": "https://arxiv.org/pdf/2508.08192.pdf", "abs": "https://arxiv.org/abs/2508.08192", "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions", "authors": ["Bangsheng Tang", "Carl Chengyan Fu", "Fei Kou", "Grigory Sizov", "Haoci Zhang", "Jason Park", "Jiawen Liu", "Jie You", "Qirui Yang", "Sachin Mehta", "Shengyong Cai", "Xiaodong Wang", "Xingyu Liu", "Yunlu Li", "Yanjun Zhou", "Wei Wei", "Zhiwei Zhao", "Zixi Qi", "Adolfo Victoria", "Aya Ibrahim", "Bram Wasti", "Changkyu Kim", "Daniel Haziza", "Fei Sun", "Giancarlo Delfin", "Emily Guo", "Jialin Ouyang", "Jaewon Lee", "Jianyu Huang", "Jeremy Reizenstein", "Lu Fang", "Quinn Zhu", "Ria Verma", "Vlad Mihailescu", "Xingwen Guo", "Yan Cui", "Ye Hu", "Yejin Lee"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale."}
{"id": "2508.08204", "pdf": "https://arxiv.org/pdf/2508.08204.pdf", "abs": "https://arxiv.org/abs/2508.08204", "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models", "authors": ["Kyle Moore", "Jesse Roberts", "Daryl Watson"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint, under review", "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis."}
{"id": "2508.08211", "pdf": "https://arxiv.org/pdf/2508.08211.pdf", "abs": "https://arxiv.org/abs/2508.08211", "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution."}
{"id": "2508.08224", "pdf": "https://arxiv.org/pdf/2508.08224.pdf", "abs": "https://arxiv.org/abs/2508.08224", "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems."}
{"id": "2508.08236", "pdf": "https://arxiv.org/pdf/2508.08236.pdf", "abs": "https://arxiv.org/abs/2508.08236", "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge", "authors": ["Yunna Cai", "Fan Wang", "Haowei Wang", "Kun Wang", "Kailai Yang", "Sophia Ananiadou", "Moyan Li", "Mingming Fan"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research."}
{"id": "2508.08243", "pdf": "https://arxiv.org/pdf/2508.08243.pdf", "abs": "https://arxiv.org/abs/2508.08243", "title": "Jinx: Unlimited LLMs for Probing Alignment Failures", "authors": ["Jiahao Zhao", "Liwei Dong"], "categories": ["cs.CL"], "comment": "https://huggingface.co/Jinx-org", "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety."}
{"id": "2508.06591", "pdf": "https://arxiv.org/pdf/2508.06591.pdf", "abs": "https://arxiv.org/abs/2508.06591", "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials", "authors": ["Rachel K. Luu", "Jingyu Deng", "Mohammed Shahrudin Ibrahim", "Nam-Joon Cho", "Ming Dao", "Subra Suresh", "Markus J. Buehler"], "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.other", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration."}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772.pdf", "abs": "https://arxiv.org/abs/2508.06772", "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues."}
{"id": "2508.06890", "pdf": "https://arxiv.org/pdf/2508.06890.pdf", "abs": "https://arxiv.org/abs/2508.06890", "title": "Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody", "authors": ["Jinsung Yoon", "Wooyeol Jeong", "Jio Gim", "Young-Joo Suh"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at ASRU 2025", "summary": "Emotional voice conversion (EVC) aims to modify the emotional style of speech\nwhile preserving its linguistic content. In practical EVC, controllability, the\nability to independently control speaker identity and emotional style using\ndistinct references, is crucial. However, existing methods often struggle to\nfully disentangle these attributes and lack the ability to model fine-grained\nemotional expressions such as temporal dynamics. We propose Maestro-EVC, a\ncontrollable EVC framework that enables independent control of content, speaker\nidentity, and emotion by effectively disentangling each attribute from separate\nreferences. We further introduce a temporal emotion representation and an\nexplicit prosody modeling with prosody augmentation to robustly capture and\ntransfer the temporal dynamics of the target emotion, even under\nprosody-mismatched conditions. Experimental results confirm that Maestro-EVC\nachieves high-quality, controllable, and emotionally expressive speech\nsynthesis."}
{"id": "2508.06944", "pdf": "https://arxiv.org/pdf/2508.06944.pdf", "abs": "https://arxiv.org/abs/2508.06944", "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT."}
{"id": "2508.06960", "pdf": "https://arxiv.org/pdf/2508.06960.pdf", "abs": "https://arxiv.org/abs/2508.06960", "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch."}
{"id": "2508.07014", "pdf": "https://arxiv.org/pdf/2508.07014.pdf", "abs": "https://arxiv.org/abs/2508.07014", "title": "TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree", "authors": ["Andrei Andrusenko", "Vladimir Bataev", "Lilit Grigoryan", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted by ASRU 2025", "summary": "Recognizing specific key phrases is an essential task for contextualized\nAutomatic Speech Recognition (ASR). However, most existing context-biasing\napproaches have limitations associated with the necessity of additional model\ntraining, significantly slow down the decoding process, or constrain the choice\nof the ASR system type. This paper proposes a universal ASR context-biasing\nframework that supports all major types: CTC, Transducers, and Attention\nEncoder-Decoder models. The framework is based on a GPU-accelerated word\nboosting tree, which enables it to be used in shallow fusion mode for greedy\nand beam search decoding without noticeable speed degradation, even with a vast\nnumber of key phrases (up to 20K items). The obtained results showed high\nefficiency of the proposed method, surpassing the considered open-source\ncontext-biasing approaches in accuracy and decoding speed. Our context-biasing\nframework is open-sourced as a part of the NeMo toolkit."}
{"id": "2508.07022", "pdf": "https://arxiv.org/pdf/2508.07022.pdf", "abs": "https://arxiv.org/abs/2508.07022", "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture."}
{"id": "2508.07050", "pdf": "https://arxiv.org/pdf/2508.07050.pdf", "abs": "https://arxiv.org/abs/2508.07050", "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability", "authors": ["Wenhan Liu", "Xinyu Ma", "Weiwei Sun", "Yutao Zhu", "Yuchen Li", "Dawei Yin", "Zhicheng Dou"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages", "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker \\textbf{ReasonRank} outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. \\textbf{Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank."}
{"id": "2508.07087", "pdf": "https://arxiv.org/pdf/2508.07087.pdf", "abs": "https://arxiv.org/abs/2508.07087", "title": "SQL-Exchange: Transforming SQL Queries Across Domains", "authors": ["Mohammadreza Daviran", "Brian Lin", "Davood Rafiei"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SQL-Exchange, a framework for mapping SQL queries across\ndifferent database schemas by preserving the source query structure while\nadapting domain-specific elements to align with the target schema. We\ninvestigate the conditions under which such mappings are feasible and\nbeneficial, and examine their impact on enhancing the in-context learning\nperformance of text-to-SQL systems as a downstream task. Our comprehensive\nevaluation across multiple model families and benchmark datasets--assessing\nstructural alignment with source queries, execution validity on target\ndatabases, and semantic correctness--demonstrates that SQL-Exchange is\neffective across a wide range of schemas and query types. Our results further\nshow that using mapped queries as in-context examples consistently improves\ntext-to-SQL performance over using queries from the source schema."}
{"id": "2508.07201", "pdf": "https://arxiv.org/pdf/2508.07201.pdf", "abs": "https://arxiv.org/abs/2508.07201", "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection", "authors": ["Chaoqun Cui", "Caiyan Jia"], "categories": ["cs.SI", "cs.AI", "cs.CL"], "comment": "This paper is accepted by AAAI2024", "summary": "Rumor detection on social media has become increasingly important. Most\nexisting graph-based models presume rumor propagation trees (RPTs) have deep\nstructures and learn sequential stance features along branches. However,\nthrough statistical analysis on real-world datasets, we find RPTs exhibit wide\nstructures, with most nodes being shallow 1-level replies. To focus learning on\nintensive substructures, we propose Rumor Adaptive Graph Contrastive Learning\n(RAGCL) method with adaptive view augmentation guided by node centralities. We\nsummarize three principles for RPT augmentation: 1) exempt root nodes, 2)\nretain deep reply nodes, 3) preserve lower-level nodes in deep sections. We\nemploy node dropping, attribute masking and edge dropping with probabilities\nfrom centrality-based importance scores to generate views. A graph contrastive\nobjective then learns robust rumor representations. Extensive experiments on\nfour benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.\nOur work reveals the wide-structure nature of RPTs and contributes an effective\ngraph contrastive learning approach tailored for rumor detection through\nprincipled adaptive augmentation. The proposed principles and augmentation\ntechniques can potentially benefit other applications involving tree-structured\ngraphs."}
{"id": "2508.07205", "pdf": "https://arxiv.org/pdf/2508.07205.pdf", "abs": "https://arxiv.org/abs/2508.07205", "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning", "authors": ["Chaoqun Cui", "Caiyan Jia"], "categories": ["cs.SI", "cs.CL"], "comment": "This paper is accepted by COLING2025", "summary": "Current rumor detection methods based on propagation structure learning\npredominately treat rumor detection as a class-balanced classification task on\nlimited labeled data. However, real-world social media data exhibits an\nimbalanced distribution with a minority of rumors among massive regular posts.\nTo address the data scarcity and imbalance issues, we construct two large-scale\nconversation datasets from Weibo and Twitter and analyze the domain\ndistributions. We find obvious differences between rumor and non-rumor\ndistributions, with non-rumors mostly in entertainment domains while rumors\nconcentrate in news, indicating the conformity of rumor detection to an anomaly\ndetection paradigm. Correspondingly, we propose the Anomaly Detection framework\nwith Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats\nunlabeled data as non-rumors and adapts graph contrastive learning for rumor\ndetection. Extensive experiments demonstrate AD-GSCL's superiority under\nclass-balanced, imbalanced, and few-shot conditions. Our findings provide\nvaluable insights for real-world rumor detection featuring imbalanced data\ndistributions."}
{"id": "2508.07292", "pdf": "https://arxiv.org/pdf/2508.07292.pdf", "abs": "https://arxiv.org/abs/2508.07292", "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities."}
{"id": "2508.07315", "pdf": "https://arxiv.org/pdf/2508.07315.pdf", "abs": "https://arxiv.org/abs/2508.07315", "title": "FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Nikolay Karpov", "Andrei Andrusenko", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop\n  (ASRU) 2025", "summary": "While beam search improves speech recognition quality over greedy decoding,\nstandard implementations are slow, often sequential, and CPU-bound. To fully\nleverage modern hardware capabilities, we present a novel open-source FlexCTC\ntoolkit for fully GPU-based beam decoding, designed for Connectionist Temporal\nClassification (CTC) models. Developed entirely in Python and PyTorch, it\noffers a fast, user-friendly, and extensible alternative to traditional C++,\nCUDA, or WFST-based decoders. The toolkit features a high-performance, fully\nbatched GPU implementation with eliminated CPU-GPU synchronization and\nminimized kernel launch overhead via CUDA Graphs. It also supports advanced\ncontextualization techniques, including GPU-powered N-gram language model\nfusion and phrase-level boosting. These features enable accurate and efficient\ndecoding, making them suitable for both research and production use."}
{"id": "2508.07342", "pdf": "https://arxiv.org/pdf/2508.07342.pdf", "abs": "https://arxiv.org/abs/2508.07342", "title": "PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization", "authors": ["Kepu Zhang", "Teng Shi", "Weijie Yu", "Jun Xu"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Personalized retrieval-augmented generation (RAG) aims to produce\nuser-tailored responses by incorporating retrieved user profiles alongside the\ninput query. Existing methods primarily focus on improving retrieval and rely\non large language models (LLMs) to implicitly integrate the retrieved context\nwith the query. However, such models are often sensitive to retrieval quality\nand may generate responses that are misaligned with user preferences. To\naddress this limitation, we propose PrLM, a reinforcement learning framework\nthat trains LLMs to explicitly reason over retrieved user profiles. Guided by a\ncontrastively trained personalization reward model, PrLM effectively learns\nfrom user responses without requiring annotated reasoning paths. Experiments on\nthree personalized text generation datasets show that PrLM outperforms existing\nmethods and remains robust across varying numbers of retrieved profiles and\ndifferent retrievers."}
{"id": "2508.07353", "pdf": "https://arxiv.org/pdf/2508.07353.pdf", "abs": "https://arxiv.org/abs/2508.07353", "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains."}
{"id": "2508.07405", "pdf": "https://arxiv.org/pdf/2508.07405.pdf", "abs": "https://arxiv.org/abs/2508.07405", "title": "Generative AI for Strategic Plan Development", "authors": ["Jesse Ponnock"], "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans."}
{"id": "2508.07407", "pdf": "https://arxiv.org/pdf/2508.07407.pdf", "abs": "https://arxiv.org/abs/2508.07407", "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems."}
{"id": "2508.07408", "pdf": "https://arxiv.org/pdf/2508.07408.pdf", "abs": "https://arxiv.org/abs/2508.07408", "title": "Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading", "authors": ["Yueyi Wang", "Qiyao Wei"], "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures, accepted at ICML 2025 New in ML Workshop", "summary": "In this study, we wish to showcase the unique utility of large language\nmodels (LLMs) in financial semantic annotation and alpha signal discovery.\nLeveraging a corpus of company-related tweets, we use an LLM to automatically\nassign multi-label event categories to high-sentiment-intensity tweets. We\nalign these labeled sentiment signals with forward returns over 1-to-7-day\nhorizons to evaluate their statistical efficacy and market tradability. Our\nexperiments reveal that certain event labels consistently yield negative alpha,\nwith Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,\nall statistically significant at the 95\\% confidence level. This study\nestablishes the feasibility of transforming unstructured social media text into\nstructured, multi-label event variables. A key contribution of this work is its\ncommitment to transparency and reproducibility; all code and methodologies are\nmade publicly available. Our results provide compelling evidence that social\nmedia sentiment is a valuable, albeit noisy, signal in financial forecasting\nand underscore the potential of open-source frameworks to democratize\nalgorithmic trading research."}
{"id": "2508.07468", "pdf": "https://arxiv.org/pdf/2508.07468.pdf", "abs": "https://arxiv.org/abs/2508.07468", "title": "CP-Agent: Agentic Constraint Programming", "authors": ["Stefan Szeider"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows."}
{"id": "2508.07485", "pdf": "https://arxiv.org/pdf/2508.07485.pdf", "abs": "https://arxiv.org/abs/2508.07485", "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced."}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520.pdf", "abs": "https://arxiv.org/abs/2508.07520", "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "authors": ["Baihan Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds."}
{"id": "2508.07616", "pdf": "https://arxiv.org/pdf/2508.07616.pdf", "abs": "https://arxiv.org/abs/2508.07616", "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning."}
{"id": "2508.07629", "pdf": "https://arxiv.org/pdf/2508.07629.pdf", "abs": "https://arxiv.org/abs/2508.07629", "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization", "authors": ["Zhenpeng Su", "Leiyu Pan", "Xue Bai", "Dening Liu", "Guanting Dong", "Jiaming Huang", "Wenping Hu", "Guorui Zhou"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6."}
{"id": "2508.07642", "pdf": "https://arxiv.org/pdf/2508.07642.pdf", "abs": "https://arxiv.org/abs/2508.07642", "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments."}
{"id": "2508.07662", "pdf": "https://arxiv.org/pdf/2508.07662.pdf", "abs": "https://arxiv.org/abs/2508.07662", "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks", "authors": ["Ihor Stepanov", "Mykhailo Shtopko", "Dmytro Vodianytskyi", "Oleksandr Lukashov", "Alexander Yavorskyi", "Mykyta Yaroshenko"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 7 tables, 2 figures", "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback."}
{"id": "2508.07750", "pdf": "https://arxiv.org/pdf/2508.07750.pdf", "abs": "https://arxiv.org/abs/2508.07750", "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment", "authors": ["Haowen Wang", "Yun Yue", "Zhiling Ye", "Shuowen Zhang", "Lei Fan", "Jiaxin Liang", "Jiadi Jiang", "Cheng Wei", "Jingyuan Deng", "Xudong Han", "Ji Li", "Chunxiao Guo", "Peng Wei", "Jian Wang", "Jinjie Gu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 7 tables", "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models."}
{"id": "2508.07768", "pdf": "https://arxiv.org/pdf/2508.07768.pdf", "abs": "https://arxiv.org/abs/2508.07768", "title": "Pareto Multi-Objective Alignment for Language Models", "authors": ["Qiang He", "Setareh Maghsudi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ECML/PKDD 2025", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments."}
{"id": "2508.07973", "pdf": "https://arxiv.org/pdf/2508.07973.pdf", "abs": "https://arxiv.org/abs/2508.07973", "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords", "authors": ["Sebastian Murgul", "Johannes Schimper", "Michael Heizmann"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "Automatic transcription of guitar strumming is an underrepresented and\nchallenging task in Music Information Retrieval (MIR), particularly for\nextracting both strumming directions and chord progressions from audio signals.\nWhile existing methods show promise, their effectiveness is often hindered by\nlimited datasets. In this work, we extend a multimodal approach to guitar\nstrumming transcription by introducing a novel dataset and a deep\nlearning-based transcription model. We collect 90 min of real-world guitar\nrecordings using an ESP32 smartwatch motion sensor and a structured recording\nprotocol, complemented by a synthetic dataset of 4h of labeled strumming audio.\nA Convolutional Recurrent Neural Network (CRNN) model is trained to detect\nstrumming events, classify their direction, and identify the corresponding\nchords using only microphone audio. Our evaluation demonstrates significant\nimprovements over baseline onset detection algorithms, with a hybrid method\ncombining synthetic and real-world data achieving the highest accuracy for both\nstrumming action detection and chord classification. These results highlight\nthe potential of deep learning for robust guitar strumming transcription and\nopen new avenues for automatic rhythm guitar analysis."}
{"id": "2508.07975", "pdf": "https://arxiv.org/pdf/2508.07975.pdf", "abs": "https://arxiv.org/abs/2508.07975", "title": "Improving Document Retrieval Coherence for Semantically Equivalent Queries", "authors": ["Stefano Campese", "Alessandro Moschitti", "Ivano Lauriola"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Dense Retrieval (DR) models have proven to be effective for Document\nRetrieval and Information Grounding tasks. Usually, these models are trained\nand optimized for improving the relevance of top-ranked documents for a given\nquery. Previous work has shown that popular DR models are sensitive to the\nquery and document lexicon: small variations of it may lead to a significant\ndifference in the set of retrieved documents. In this paper, we propose a\nvariation of the Multi-Negative Ranking loss for training DR that improves the\ncoherence of models in retrieving the same documents with respect to\nsemantically similar queries. The loss penalizes discrepancies between the\ntop-k ranked documents retrieved for diverse but semantic equivalent queries.\nWe conducted extensive experiments on various datasets, MS-MARCO, Natural\nQuestions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes\nby our loss are subject to lower sensitivity, and, (ii) interestingly, higher\naccuracy."}
{"id": "2508.07987", "pdf": "https://arxiv.org/pdf/2508.07987.pdf", "abs": "https://arxiv.org/abs/2508.07987", "title": "Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription", "authors": ["Sebastian Murgul", "Michael Heizmann"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025", "summary": "Automatic transcription of acoustic guitar fingerpicking performances remains\na challenging task due to the scarcity of labeled training data and legal\nconstraints connected with musical recordings. This work investigates a\nprocedural data generation pipeline as an alternative to real audio recordings\nfor training transcription models. Our approach synthesizes training data\nthrough four stages: knowledge-based fingerpicking tablature composition, MIDI\nperformance rendering, physical modeling using an extended Karplus-Strong\nalgorithm, and audio augmentation including reverb and distortion. We train and\nevaluate a CRNN-based note-tracking model on both real and synthetic datasets,\ndemonstrating that procedural data can be used to achieve reasonable\nnote-tracking results. Finetuning with a small amount of real data further\nenhances transcription accuracy, improving over models trained exclusively on\nreal recordings. These results highlight the potential of procedurally\ngenerated audio for data-scarce music information retrieval tasks."}
{"id": "2508.08039", "pdf": "https://arxiv.org/pdf/2508.08039.pdf", "abs": "https://arxiv.org/abs/2508.08039", "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning", "authors": ["Shu Wu", "Chenxing Li", "Wenfu Wang", "Hao Zhang", "Hualei Wang", "Meng Yu", "Dong Yu"], "categories": ["cs.SD", "cs.CL", "cs.MM"], "comment": "preprint", "summary": "Recent advancements in large language models, multimodal large language\nmodels, and large audio language models (LALMs) have significantly improved\ntheir reasoning capabilities through reinforcement learning with rule-based\nrewards. However, the explicit reasoning process has yet to show significant\nbenefits for audio question answering, and effectively leveraging deep\nreasoning remains an open challenge, with LALMs still falling short of\nhuman-level auditory-language reasoning. To address these limitations, we\npropose Audio-Thinker, a reinforcement learning framework designed to enhance\nthe reasoning capabilities of LALMs, with a focus on improving adaptability,\nconsistency, and effectiveness. Our approach introduces an adaptive think\naccuracy reward, enabling the model to adjust its reasoning strategies based on\ntask complexity dynamically. Furthermore, we incorporate an external reward\nmodel to evaluate the overall consistency and quality of the reasoning process,\ncomplemented by think-based rewards that help the model distinguish between\nvalid and flawed reasoning paths during training. Experimental results\ndemonstrate that our Audio-Thinker model outperforms existing\nreasoning-oriented LALMs across various benchmark tasks, exhibiting superior\nreasoning and generalization capabilities."}
{"id": "2508.08061", "pdf": "https://arxiv.org/pdf/2508.08061.pdf", "abs": "https://arxiv.org/abs/2508.08061", "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin Käppel", "Weixin Wang", "Martin Matzner"], "categories": ["cs.LG", "cs.CL", "cs.DB"], "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries."}
{"id": "2508.08066", "pdf": "https://arxiv.org/pdf/2508.08066.pdf", "abs": "https://arxiv.org/abs/2508.08066", "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model", "authors": ["Weitai Kang", "Weiming Zhuang", "Zhizhong Li", "Yan Yan", "Lingjuan Lyu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages for the main paper", "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5."}
{"id": "2508.08088", "pdf": "https://arxiv.org/pdf/2508.08088.pdf", "abs": "https://arxiv.org/abs/2508.08088", "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches", "authors": ["Jiejun Tan", "Zhicheng Dou", "Yan Yu", "Jiehan Cheng", "Qiang Ju", "Jian Xie", "Ji-Rong Wen"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code and datasets are available at\n  https://github.com/plageon/HierSearch", "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains."}
{"id": "2508.08221", "pdf": "https://arxiv.org/pdf/2508.08221.pdf", "abs": "https://arxiv.org/abs/2508.08221", "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "authors": ["Zihe Liu", "Jiashun Liu", "Yancheng He", "Weixun Wang", "Jiaheng Liu", "Ling Pan", "Xinyu Hu", "Shaopan Xiong", "Ju Huang", "Jian Hu", "Shengyi Huang", "Siran Yang", "Jiamang Wang", "Wenbo Su", "Bo Zheng"], "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 21 figures", "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO."}
{"id": "2102.11037", "pdf": "https://arxiv.org/pdf/2102.11037.pdf", "abs": "https://arxiv.org/abs/2102.11037", "title": "Highly Fast Text Segmentation With Pairwise Markov Chains", "authors": ["Elie Azeraf", "Emmanuel Monfrini", "Emmanuel Vignon", "Wojciech Pieczynski"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 5 figures, 4 tables, MNLP 2020", "summary": "Natural Language Processing (NLP) models' current trend consists of using\nincreasingly more extra-data to build the best models as possible. It implies\nmore expensive computational costs and training time, difficulties for\ndeployment, and worries about these models' carbon footprint reveal a critical\nproblem in the future. Against this trend, our goal is to develop NLP models\nrequiring no extra-data and minimizing training time. To do so, in this paper,\nwe explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov\nChain (PMC), for NLP segmentation tasks. We apply these models for three\nclassic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We\ndevelop an original method to adapt these models for text segmentation's\nspecific challenges to obtain relevant performances with very short training\nand execution times. PMC achieves equivalent results to those obtained by\nConditional Random Fields (CRF), one of the most applied models for these tasks\nwhen no extra-data are used. Moreover, PMC has training times 30 times shorter\nthan the CRF ones, which validates this model given our objectives."}
{"id": "2407.09652", "pdf": "https://arxiv.org/pdf/2407.09652.pdf", "abs": "https://arxiv.org/abs/2407.09652", "title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs", "authors": ["Andrea W Wen-Yi", "Unso Eun Seo Jo", "Lu Jia Lin", "David Mimno"], "categories": ["cs.CL"], "comment": "We have reworked the paper substantially. Please refer to the new,\n  updated article: arXiv:2504.00289", "summary": "Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models."}
{"id": "2407.12856", "pdf": "https://arxiv.org/pdf/2407.12856.pdf", "abs": "https://arxiv.org/abs/2407.12856", "title": "AI-AI Bias: large language models favor communications generated by large language models", "authors": ["Walter Laurito", "Benjamin Davis", "Peli Grietzer", "Tomáš Gavenčiak", "Ada Böhm", "Jan Kulveit"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 pages, 4 figures", "summary": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage."}
{"id": "2408.08651", "pdf": "https://arxiv.org/pdf/2408.08651.pdf", "abs": "https://arxiv.org/abs/2408.08651", "title": "Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow", "authors": ["Kyle Moore", "Jesse Roberts", "Thao Pham", "Douglas Fisher"], "categories": ["cs.CL", "cs.AI"], "comment": "Final version. Published In Proceedings of the Annual Meeting of the\n  Cognitive Science Society (Vol. 47) 2025", "summary": "Language models are known to absorb biases from their training data, leading\nto predictions driven by statistical regularities rather than semantic\nrelevance. We investigate the impact of these biases on answer choice\npreferences in the Massive Multi-Task Language Understanding (MMLU) task. Our\nfindings show that these biases are predictive of model preference and mirror\nhuman test-taking strategies even when chain of thought (CoT) reasoning is\nused. To address this issue, we introduce Counterfactual Prompting with\nAgnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual\nPrompting with CoT alone is insufficient to mitigate bias, APriCoT effectively\nreduces the influence of base-rate probabilities while improving overall\naccuracy. Our results suggest that mitigating bias requires a slow thinking\nprocess which CoT alone may not provide as it tends to reinforce fast thinking\nmodel bias under some prompting methodologies. APriCoT is a step toward\ndeveloping more robust and fair language models that can think slow."}
{"id": "2409.06624", "pdf": "https://arxiv.org/pdf/2409.06624.pdf", "abs": "https://arxiv.org/abs/2409.06624", "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio", "authors": ["Ningyuan Xi", "Yetao Wu", "Kun Fan", "Teng Chen", "Qingqing Gu", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 2 figures", "summary": "Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to\nobtain unfamiliar language skills or adapt to new domains. The huge training\ncost of CPT often asks for cautious choice of key hyper-parameters such as the\nmixture ratio of extra language or domain corpus. However, there is no\nsystematic study that bridges the gap between the optimal mixture ratio and the\nactual model performance, and the gap between experimental scaling law and the\nactual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicates the optimal\nexperimental setup. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark but also in some specific domains including math, coding, and\nemotional intelligence. We deploy the final 70B version of LLM on a real-life\nchat system which obtains satisfying performance."}
{"id": "2409.12962", "pdf": "https://arxiv.org/pdf/2409.12962.pdf", "abs": "https://arxiv.org/abs/2409.12962", "title": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions", "authors": ["Tsung-Han Wu", "Joseph E. Gonzalez", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to ASRU 2025; Code is publicly available at\n  https://github.com/DavidMChan/clair-a", "summary": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a."}
{"id": "2410.08109", "pdf": "https://arxiv.org/pdf/2410.08109.pdf", "abs": "https://arxiv.org/abs/2410.08109", "title": "A Closer Look at Machine Unlearning for Large Language Models", "authors": ["Xiaojian Yuan", "Tianyu Pang", "Chao Du", "Kejiang Chen", "Weiming Zhang", "Min Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning."}
{"id": "2410.09426", "pdf": "https://arxiv.org/pdf/2410.09426.pdf", "abs": "https://arxiv.org/abs/2410.09426", "title": "FlatQuant: Flatness Matters for LLM Quantization", "authors": ["Yuxuan Sun", "Ruikang Liu", "Haoli Bai", "Han Bao", "Kang Zhao", "Yuening Li", "Jiaxin Hu", "Xianzhi Yu", "Lu Hou", "Chun Yuan", "Xin Jiang", "Wulong Liu", "Jun Yao"], "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, accepted to ICML 2025", "summary": "Recently, quantization has been widely used for the compression and\nacceleration of large language models (LLMs). Due to the outliers in LLMs, it\nis crucial to flatten weights and activations to minimize quantization error\nwith equally spaced quantization points. Prior research explores various\npre-quantization transformations to suppress outliers, such as per-channel\nscaling and Hadamard transformation. However, we observe that these transformed\nweights and activations can still exhibit steep and dispersed distributions. In\nthis paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a\nnew post-training quantization approach that enhances the flatness of weights\nand activations. Our approach identifies optimal affine transformations for\neach linear layer, calibrated in hours via a lightweight objective. To reduce\nruntime overhead of affine transformation, we apply Kronecker product with two\nlightweight matrices, and fuse all operations in FlatQuant into a single\nkernel. Extensive experiments demonstrate that FlatQuant establishes a new\nstate-of-the-art benchmark for quantization. For example, it achieves less than\n1\\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing\nSpinQuant by 7.5\\%. Additionally, it provides up to 2.3x prefill speedup and\n1.7x decoding speedup compared to the FP16 model. Code is available at:\nhttps://github.com/ruikangliu/FlatQuant."}
{"id": "2411.12703", "pdf": "https://arxiv.org/pdf/2411.12703.pdf", "abs": "https://arxiv.org/abs/2411.12703", "title": "Strengthening False Information Propagation Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques in comparison to BERT", "authors": ["Ahmed Akib Jawad Karim", "Kazi Hafiz Md Asad", "Aznur Azam"], "categories": ["cs.CL"], "comment": "The results in table 2 are invalid due to an error in the data\n  preprocessing step, which affected the reported accuracy and conclusions", "summary": "The rapid spread of misinformation, particularly through online platforms,\nunderscores the urgent need for reliable detection systems. This study explores\nthe utilization of machine learning and natural language processing,\nspecifically Support Vector Machines (SVM) and BERT, to detect fake news. We\nemploy three distinct text vectorization methods for SVM: Term Frequency\nInverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW),\nevaluating their effectiveness in distinguishing between genuine and fake news.\nAdditionally, we compare these methods against the transformer large language\nmodel, BERT. Our comprehensive approach includes detailed preprocessing steps,\nrigorous model implementation, and thorough evaluation to determine the most\neffective techniques. The results demonstrate that while BERT achieves superior\naccuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear\nkernel and BoW vectorization also performs exceptionally well, achieving 99.81%\naccuracy and an F1-score of 0.9980. These findings highlight that, despite\nBERT's superior performance, SVM models with BoW and TF-IDF vectorization\nmethods come remarkably close, offering highly competitive performance with the\nadvantage of lower computational requirements."}
{"id": "2501.07572", "pdf": "https://arxiv.org/pdf/2501.07572.pdf", "abs": "https://arxiv.org/abs/2501.07572", "title": "WebWalker: Benchmarking LLMs in Web Traversal", "authors": ["Jialong Wu", "Wenbiao Yin", "Yong Jiang", "Zhenglin Wang", "Zekun Xi", "Runnan Fang", "Linhai Zhang", "Yulan He", "Deyu Zhou", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios."}
{"id": "2501.13428", "pdf": "https://arxiv.org/pdf/2501.13428.pdf", "abs": "https://arxiv.org/abs/2501.13428", "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models", "authors": ["Bo Gao", "Michael W. Spratling"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages and 3 figures", "summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by proposing a new design principle for attention, viewing it as a\ntwo-stage process. We first decompose the Softmax operation into a non-linear\npositivity transformation and an $l_1$-normalisation step, identifying the\nlatter as essential for maintaining model performance. In the first stage, we\nreplace the standard exponential function with the more numerically stable\nSoftplus activation and introduce a dynamic scale factor based on invariance\nentropy, creating a novel attention mechanism that outperforms conventional\nSoftmax attention. In the second stage, we introduce a re-weighting mechanism\nthat sharpens the attention distribution, amplifying significant weights while\ndiminishing weaker ones. This enables the model to concentrate more effectively\non relevant tokens and fundamentally improves length extrapolation. When\ncombined, this two-stage approach ensures numerical stability and dramatically\nimproves length extrapolation, maintaining a nearly constant validation loss at\n16$\\times$ the training length while achieving superior results on challenging\nlong-context retrieval tasks and standard downstream benchmarks."}
{"id": "2501.17858", "pdf": "https://arxiv.org/pdf/2501.17858.pdf", "abs": "https://arxiv.org/abs/2501.17858", "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging", "authors": ["Rui Min", "Tianyu Pang", "Chao Du", "Qian Liu", "Minhao Cheng", "Min Lin"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "ICML 2025", "summary": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena."}
{"id": "2502.01578", "pdf": "https://arxiv.org/pdf/2502.01578.pdf", "abs": "https://arxiv.org/abs/2502.01578", "title": "ReGLA: Refining Gated Linear Attention", "authors": ["Peng Lu", "Ivan Kobyzev", "Mehdi Rezagholizadeh", "Boxing Chen", "Philippe Langlais"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 (main)", "summary": "Recent advancements in Large Language Models (LLMs) have set themselves apart\nwith their exceptional performance in complex language modelling tasks.\nHowever, these models are also known for their significant computational and\nstorage requirements, primarily due to the quadratic computation complexity of\nsoftmax attention. To mitigate this issue, linear attention has been designed\nto reduce the quadratic space-time complexity that is inherent in standard\ntransformers. In this work, we embarked on a comprehensive exploration of three\nkey components that substantially impact the performance of the Gated Linear\nAttention module: feature maps, normalization, and the gating mechanism. We\ndeveloped a feature mapping function to address some crucial issues that\nprevious suggestions overlooked. Then we offered further rationale for the\nintegration of normalization layers to stabilize the training process.\nMoreover, we explored the saturation phenomenon of the gating mechanism and\naugmented it with a refining module. We conducted extensive experiments and\nshowed our architecture outperforms previous Gated Linear Attention mechanisms\nin extensive tasks including training from scratch and post-linearization with\ncontinual pre-training."}
{"id": "2502.12204", "pdf": "https://arxiv.org/pdf/2502.12204.pdf", "abs": "https://arxiv.org/abs/2502.12204", "title": "Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration", "authors": ["Xianbing Zhao", "Yiqing Lyu", "Di Wang", "Buzhou Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL2025", "summary": "Automatic depression detection provides cues for early clinical intervention\nby clinicians. Clinical interviews for depression detection involve dialogues\ncentered around multiple themes. Existing studies primarily design end-to-end\nneural network models to capture the hierarchical structure of clinical\ninterview dialogues. However, these methods exhibit defects in modeling the\nthematic content of clinical interviews: 1) they fail to capture intra-theme\nand inter-theme correlation explicitly, and 2) they do not allow clinicians to\nintervene and focus on themes of interest. To address these issues, this paper\nintroduces an interactive depression detection framework. This framework\nleverages in-context learning techniques to identify themes in clinical\ninterviews and then models both intra-theme and inter-theme correlation.\nAdditionally, it employs AI-driven feedback to simulate the interests of\nclinicians, enabling interactive adjustment of theme importance. PDIMC achieves\nabsolute improvements of 35\\% and 12\\% compared to the state-of-the-art on the\ndepression detection dataset DAIC-WOZ, which demonstrates the effectiveness of\nmodeling theme correlation and incorporating interactive external feedback."}
{"id": "2502.14860", "pdf": "https://arxiv.org/pdf/2502.14860.pdf", "abs": "https://arxiv.org/abs/2502.14860", "title": "ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning", "authors": ["Shuyue Stella Li", "Jimin Mun", "Faeze Brahman", "Pedram Hosseini", "Bryceton G. Thomas", "Jessica M. Sin", "Bing Ren", "Jonathan S. Ilgen", "Yulia Tsvetkov", "Maarten Sap"], "categories": ["cs.CL"], "comment": "29 pages, 8 figures, 12 tables", "summary": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decision-making. We present ALignment\nvia Fine-grained Attributes, (ALFA) a framework that improves LLM\nquestion-asking by (i) decomposing the notion of a \"good\" question into a set\nof theory-grounded attributes (e.g., clarity, relevance), (ii) controllably\nsynthesizing attribute-specific question variations, and (iii) aligning models\nvia preference-based optimization to explicitly learn to ask better questions\nalong these fine-grained attributes. Focusing on clinical reasoning as a case\nstudy, we introduce the MediQ-AskDocs dataset, composed of 17k real-world\nclinical interactions augmented with 80k attribute-specific preference pairs of\nfollow-up questions, as well as a novel expert-annotated interactive healthcare\nQA task to evaluate question-asking abilities. Models aligned with ALFA reduce\ndiagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned\nLLMs, with a question-level win-rate of 64.4% and strong generalizability. Our\nfindings suggest that explicitly guiding question-asking with structured,\nfine-grained attributes offers a scalable path to improve LLMs, especially in\nexpert application domains."}
{"id": "2502.17810", "pdf": "https://arxiv.org/pdf/2502.17810.pdf", "abs": "https://arxiv.org/abs/2502.17810", "title": "URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models", "authors": ["Ruiqi Yan", "Xiquan Li", "Wenxi Chen", "Zhikang Niu", "Chen Yang", "Ziyang Ma", "Kai Yu", "Xie Chen"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven significant\nprogress in end-to-end spoken dialogue models (SDMs). In contrast to text-based\nLLMs, the evaluation framework for SDMs should encompass both cognitive\ndimensions (e.g., logical reasoning, knowledge) and speech-related aspects\n(e.g., paralinguistic cues, audio quality). However, there is still a lack of\ncomprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To\naddress this gap, we propose URO-Bench, an extensive benchmark for SDMs.\nNotably, URO-Bench is the first S2S benchmark that covers evaluations about\nmultilingualism, multi-round dialogues, and paralinguistics. Our benchmark is\ndivided into two difficulty levels: basic track and pro track, each comprising\n20 test sets, evaluating the spoken dialogue model's abilities in\nUnderstanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can facilitate the development of spoken dialogue models\nby providing a multifaceted evaluation of existing models and helping to track\nprogress in this area."}
{"id": "2503.04773", "pdf": "https://arxiv.org/pdf/2503.04773.pdf", "abs": "https://arxiv.org/abs/2503.04773", "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content", "authors": ["Bingbing Fan", "Lin Chen", "Songwei Li", "Jian Yuan", "Fengli Xu", "Pan Hui", "Yong Li"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": "11 pages, 6 figures", "summary": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI."}
{"id": "2503.11132", "pdf": "https://arxiv.org/pdf/2503.11132.pdf", "abs": "https://arxiv.org/abs/2503.11132", "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression", "authors": ["Guihong Li", "Mehdi Rezagholizadeh", "Mingyu Yang", "Vikram Appia", "Emad Barsoum"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."}
{"id": "2503.20850", "pdf": "https://arxiv.org/pdf/2503.20850.pdf", "abs": "https://arxiv.org/abs/2503.20850", "title": "Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models", "authors": ["Qing Yao", "Kanishka Misra", "Leonie Weissweiler", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) tend to show human-like preferences on a number of\nsyntactic phenomena, but the extent to which these are attributable to direct\nexposure to the phenomena or more general properties of language is unclear. We\nexplore this with the English dative alternation (DO: \"gave Y the X\" vs. PO:\n\"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively\ntrain small LMs on systematically manipulated input. We focus on two properties\nthat affect the choice of alternant: length and animacy. Both properties are\ndirectly present in datives but also reflect more global tendencies for shorter\nelements to precede longer ones and animates to precede inanimates. First, by\nmanipulating and ablating datives for these biases in the input, we show that\ndirect evidence of length and animacy matters, but easy-first preferences\npersist even without such evidence. Then, using LMs trained on systematically\nperturbed datasets to manipulate global length effects (re-linearizing\nsentences globally while preserving dependency structure), we find that dative\npreferences can emerge from indirect evidence. We conclude that LMs' emergent\nsyntactic preferences come from a mix of direct and indirect sources."}
{"id": "2504.01196", "pdf": "https://arxiv.org/pdf/2504.01196.pdf", "abs": "https://arxiv.org/abs/2504.01196", "title": "$μ$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models", "authors": ["Zian Su", "Ziyang Huang", "Kaiyuan Zhang", "Xiangyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025. The first two authors contributed equally to this work", "summary": "Large language models (LLMs) have emerged as powerful knowledge bases yet are\nlimited by static training data, leading to issues such as hallucinations and\nsafety risks. Editing a model's internal knowledge through the locate-and-edit\nparadigm has proven a cost-effective alternative to retraining, though current\nunstructured approaches, especially window-based autoregressive methods, often\ndisrupt the causal dependency between early memory updates and later output\ntokens. In this work, we first theoretically analyze these limitations and then\nintroduce Matryoshka Unstructured Knowledge Editing ($\\mu$KE), a novel memory\nupdate mechanism that preserves such dependencies via a Matryoshka-style\nobjective and adaptive loss coefficients. Empirical evaluations on two models\nacross four benchmarks demonstrate that $\\mu$KE improves edit efficacy by up to\n12.33% over state-of-the-art methods, and remains robust when applied to\ndiverse formatted edits, underscoring its potential for effective unstructured\nknowledge editing in LLMs."}
{"id": "2504.02122", "pdf": "https://arxiv.org/pdf/2504.02122.pdf", "abs": "https://arxiv.org/abs/2504.02122", "title": "Overcoming Vocabulary Constraints with Pixel-level Fallback", "authors": ["Jonas F. Lotz", "Hendra Setiawan", "Stephan Peitz", "Yova Kementchedjhieva"], "categories": ["cs.CL"], "comment": "COLM 2025", "summary": "Subword tokenization requires balancing computational efficiency and\nvocabulary coverage, which often leads to suboptimal performance on languages\nand scripts not prioritized during training. We propose to augment pretrained\nlanguage models with a vocabulary-free encoder that generates input embeddings\nfrom text rendered as pixels. Through experiments on English-centric language\nmodels, we demonstrate that our approach substantially improves machine\ntranslation performance and facilitates effective cross-lingual transfer,\noutperforming tokenizer-based methods. Furthermore, we find that pixel-based\nrepresentations outperform byte-level approaches and standard vocabulary\nexpansion. Our approach enhances the multilingual capabilities of monolingual\nlanguage models without extensive retraining and reduces decoding latency via\ninput compression."}
{"id": "2504.02904", "pdf": "https://arxiv.org/pdf/2504.02904.pdf", "abs": "https://arxiv.org/abs/2504.02904", "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence", "authors": ["Hongzhe Du", "Weikai Li", "Min Cai", "Karim Saraipour", "Zimin Zhang", "Himabindu Lakkaraju", "Yizhou Sun", "Shichang Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis."}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228.pdf", "abs": "https://arxiv.org/abs/2504.05228", "title": "NoveltyBench: Evaluating Language Models for Humanlike Diversity", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize diversity alongside quality."}
{"id": "2504.07583", "pdf": "https://arxiv.org/pdf/2504.07583.pdf", "abs": "https://arxiv.org/abs/2504.07583", "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "authors": ["Patrick Fernandes", "Sweta Agrawal", "Emmanouil Zaranis", "André F. T. Martins", "Graham Neubig"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"}
{"id": "2504.09373", "pdf": "https://arxiv.org/pdf/2504.09373.pdf", "abs": "https://arxiv.org/abs/2504.09373", "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text", "authors": ["Ramya Namuduri", "Yating Wu", "Anshun Asher Zheng", "Manya Wadhwa", "Greg Durrett", "Junyi Jessy Li"], "categories": ["cs.CL"], "comment": "COLM 2025 Camera Ready", "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use."}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834.pdf", "abs": "https://arxiv.org/abs/2504.13834", "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it difficult to track\nprogress and high-level conceptual links across broad disciplines. While tools\nlike citation networks and search engines help retrieve related papers, they\nlack the abstraction needed to capture the needed to represent the density and\nstructure of activity across subfields.\n  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific\nliterature into a high-quality hierarchical structure that spans multiple\nlevels of abstraction -- from broad domains to specific studies. Such a\nrepresentation can provide insights into which fields are well-explored and\nwhich are under-explored. To achieve this goal, we develop a hybrid approach\nthat combines efficient embedding-based clustering with LLM-based prompting,\nstriking a balance between scalability and semantic precision. Compared to\nLLM-heavy methods like iterative tree construction, our approach achieves\nsuperior quality-speed trade-offs. Our hierarchies capture different dimensions\nof research contributions, reflecting the interdisciplinary and multifaceted\nnature of modern science. We evaluate its utility by measuring how effectively\nan LLM-based agent can navigate the hierarchy to locate target papers. Results\nshow that our method improves interpretability and offers an alternative\npathway for exploring scientific literature beyond traditional search methods.\nCode, data and demo are available:\nhttps://github.com/JHU-CLSP/science-hierarchography"}
{"id": "2504.16832", "pdf": "https://arxiv.org/pdf/2504.16832.pdf", "abs": "https://arxiv.org/abs/2504.16832", "title": "GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning", "authors": ["Luu Quy Tung", "Hoang Quoc Viet", "Pham Bao Loc", "Vo Trong Thu"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques."}
{"id": "2504.16858", "pdf": "https://arxiv.org/pdf/2504.16858.pdf", "abs": "https://arxiv.org/abs/2504.16858", "title": "Planning with Diffusion Models for Target-Oriented Dialogue Systems", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance toward diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://github.com/ninglab/DiffTOD."}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130.pdf", "abs": "https://arxiv.org/abs/2504.17130", "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "authors": ["Hannah Cyberey", "David Evans"], "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering"}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938.pdf", "abs": "https://arxiv.org/abs/2504.18938", "title": "RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "authors": ["Junhong Liang", "Yu Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. Traditional CSC focuses on equal length correction and uses\npretrained language models (PLMs). While Large Language Models (LLMs) have\nshown remarkable success in identifying and rectifying potential errors, they\noften struggle with adapting to domain-specific corrections, especially when\nencountering terminologies in specialized domains. To address domain\nadaptation, we propose a \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{I}terative \\textbf{R}efinement (RAIR) framework. Our approach\nconstructs a retrieval corpus adaptively from domain-specific training data and\ndictionaries, employing a fine-tuned retriever to ensure that the retriever\ncatches the error correction pattern. We also extend equal-length into\nvariable-length correction scenarios. Extensive experiments demonstrate that\nour framework outperforms current approaches in domain spelling correction and\nsignificantly improves the performance of LLMs in variable-length scenarios."}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733.pdf", "abs": "https://arxiv.org/abs/2505.03733", "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model."}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930.pdf", "abs": "https://arxiv.org/abs/2505.09930", "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "categories": ["cs.CL"], "comment": "28 pages, 14 figures", "summary": "Prompt optimization (PO) provides a practical way to improve response quality\nwhen users lack the time or expertise to manually craft effective prompts.\nExisting methods typically rely on LLMs' self-generation ability to optimize\nprompts. However, due to limited downward compatibility, the instruction-heavy\nprompts generated by advanced LLMs can overwhelm lightweight inference models\nand degrade response quality, while also lacking interpretability due to\nimplicit optimization. In this work, we rethink prompt optimization through the\nlens of explicit and interpretable design. We first identify a set of\nmodel-agnostic prompt quality merits and empirically validate their\neffectiveness in enhancing prompt and response quality. We then introduce MePO,\na merit-guided, locally deployable prompt optimizer trained on our merit-guided\nprompt preference dataset generated by a lightweight LLM. MePO avoids online\noptimization, reduces privacy concerns, and, by learning clear, interpretable\nmerits, generalizes effectively to both large-scale and lightweight inference\nmodels. Experiments demonstrate that MePO achieves better results across\ndiverse tasks and model types, offering a scalable and robust solution for\nreal-world deployment.The code, model and dataset can be found in\nhttps://github.com/MidiyaZhu/MePO"}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356.pdf", "abs": "https://arxiv.org/abs/2505.10356", "title": "Decoding the Multimodal Mind: Generalizable Brain-to-Text Translation via Multimodal Alignment and Adaptive Routing", "authors": ["Chunyu Ye", "Yunhao Zhang", "Jingyuan Sun", "Chong Li", "Chengqing Zong", "Shaonan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Decoding language from the human brain remains a grand challenge for\nBrain-Computer Interfaces (BCIs). Current approaches typically rely on unimodal\nbrain representations, neglecting the brain's inherently multimodal processing.\nInspired by the brain's associative mechanisms, where viewing an image can\nevoke related sounds and linguistic representations, we propose a unified\nframework that leverages Multimodal Large Language Models (MLLMs) to align\nbrain signals with a shared semantic space encompassing text, images, and\naudio. A router module dynamically selects and fuses modality-specific brain\nfeatures according to the characteristics of each stimulus. Experiments on\nvarious fMRI datasets with textual, visual, and auditory stimuli demonstrate\nstate-of-the-art performance, achieving an 8.48% improvement on the most\ncommonly used benchmark. We further extend our framework to EEG and MEG data,\ndemonstrating flexibility and robustness across varying temporal and spatial\nresolutions. To our knowledge, this is the first unified BCI architecture\ncapable of robustly decoding multimodal brain activity across diverse brain\nsignals and stimulus types, offering a flexible solution for real-world\napplications."}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560.pdf", "abs": "https://arxiv.org/abs/2505.12560", "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "authors": ["Hiram Ring"], "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large tagged parallel dataset which has been developed to\npartially address this issue. The taggedPBC contains POS-tagged parallel text\ndata from more than 1,940 languages, representing 155 language families and 78\nisolates, dwarfing previously available resources. The accuracy of particular\ntags in this dataset is shown to correlate well with both existing SOTA taggers\nfor high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora\n(Universal Dependencies Treebanks). Additionally, a novel measure derived from\nthis dataset, the N1 ratio, correlates with expert determinations of\nintransitive word order in three typological databases (WALS, Grambank,\nAutotyp) such that a Gaussian Naive Bayes classifier trained on this feature\ncan accurately identify basic intransitive word order for languages not in\nthose databases. While much work is still needed to expand and develop this\ndataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub."}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918.pdf", "abs": "https://arxiv.org/abs/2505.15918", "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "In this work, we evaluate the potential of Large Language Models (LLMs) in\nbuilding Bayesian Networks (BNs) by approximating domain expert priors. LLMs\nhave demonstrated potential as factual knowledge bases; however, their\ncapability to generate probabilistic knowledge about real-world events remains\nunderstudied. We explore utilizing the probabilistic knowledge inherent in LLMs\nto derive probability estimates for statements regarding events and their\nrelationships within a BN. Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Our experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\ndata, especially when data is scarce. Overall, this work introduces a promising\nstrategy for automatically constructing Bayesian Networks by combining\nprobabilistic knowledge extracted from LLMs with real-world data. Additionally,\nwe establish the first comprehensive baseline for assessing LLM performance in\nextracting probabilistic knowledge."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648.pdf", "abs": "https://arxiv.org/abs/2505.22648", "title": "WebDancer: Towards Autonomous Information Seeking Agency", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Gang Fu", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842.pdf", "abs": "https://arxiv.org/abs/2505.23842", "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings."}
{"id": "2506.00160", "pdf": "https://arxiv.org/pdf/2506.00160.pdf", "abs": "https://arxiv.org/abs/2506.00160", "title": "Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework", "authors": ["Qihui Fan", "Wenbo Li", "Enfu Nan", "Yixiao Chen", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The growing popularity of social deduction games has created an increasing\nneed for intelligent frameworks where humans can collaborate with AI agents,\nparticularly in post-pandemic contexts with heightened psychological and social\npressures. Social deduction games like Werewolf, traditionally played through\nverbal communication, present an ideal application for Large Language Models\n(LLMs) given their advanced reasoning and conversational capabilities. Prior\nstudies have shown that LLMs can outperform humans in Werewolf games, but their\nreliance on external modules introduces latency that left their contribution in\nacademic domain only, and omit such game should be user-facing. We propose\n\\textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes\ntwo parallel pipelines: gameplay powered by state-of-the-art LLMs and a\nfine-tuned Text-to-Speech (TTS) module that brings text output to life. Our\nsystem operates in near real-time without external decision-making modules,\nleveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3\nto create a more engaging and anthropomorphic gaming experience that\nsignificantly improves user engagement compared to existing text-only\nframeworks."}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250.pdf", "abs": "https://arxiv.org/abs/2506.00250", "title": "PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of Natural Language Processing (NLP) benchmarks, often surpassing\nhuman-level accuracy. However, their reliability in high-stakes domains such as\nmedicine, particularly in low-resource languages, remains underexplored. In\nthis work, we introduce PersianMedQA, a large-scale dataset of 20,785\nexpert-validated multiple-choice Persian medical questions from 14 years of\nIranian national medical exams, spanning 23 medical specialties and designed to\nevaluate LLMs in both Persian and English. We benchmark 40 state-of-the-art\nmodels, including general-purpose, Persian fine-tuned, and medical LLMs, in\nzero-shot and chain-of-thought (CoT) settings. Our results show that\nclosed-source general models (e.g., GPT-4.1) consistently outperform all other\ncategories, achieving 83.09% accuracy in Persian and 80.7% in English, while\nPersian fine-tuned models such as Dorna underperform significantly (e.g., 34.9%\nin Persian), often struggling with both instruction-following and domain\nreasoning. We also analyze the impact of translation, showing that while\nEnglish performance is generally higher, 3-10% of questions can only be\nanswered correctly in Persian due to cultural and clinical contextual cues that\nare lost in translation. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating bilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset is\navailable: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA ."}
{"id": "2506.00826", "pdf": "https://arxiv.org/pdf/2506.00826.pdf", "abs": "https://arxiv.org/abs/2506.00826", "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs", "authors": ["Yongkang Xiao", "Rui Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)\nby incorporating diverse modalities such as images and text. multimodal\nknowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals\nto infer missing facts, thereby mitigating the intrinsic incompleteness of\nMMKGs. Existing MMKGC methods typically leverage only the information contained\nin the MMKGs under the closed-world assumption and adopt discriminative\ntraining objectives, which limits their reasoning capacity during completion.\nRecent large language models (LLMs), empowered by massive parameter scales and\npretraining on vast corpora, have demonstrated strong reasoning abilities\nacross various tasks. However, their potential in MMKGC remains largely\nunexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous\nExperts Representation and Generative Completion framework for MMKGs. HERGC\nfirst deploys a Heterogeneous Experts Representation Retriever that enriches\nand fuses multimodal information and retrieves a compact candidate set for each\nincomplete triple. It then uses a Generative LLM Predictor, implemented via\neither in-context learning or lightweight fine-tuning, to accurately identify\nthe correct answer from these candidates. Extensive experiments on three\nstandard MMKG benchmarks demonstrate HERGC's effectiveness and robustness,\nachieving superior performance over existing methods."}
{"id": "2506.00964", "pdf": "https://arxiv.org/pdf/2506.00964.pdf", "abs": "https://arxiv.org/abs/2506.00964", "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness", "authors": ["Dren Fazlija", "Arkadij Orlov", "Sandipan Sikdar"], "categories": ["cs.CL"], "comment": "20 pages, 4 figures, 8 tables, ACL 2025 (Findings), Figure 3 was\n  corrected and is now aligned with the values of Table 2, Project Page:\n  https://drenfazlija.github.io/AccessDeniedInc/", "summary": "Large language models (LLMs) are increasingly becoming valuable to corporate\ndata management due to their ability to process text from various document\nformats and facilitate user interactions through natural language queries.\nHowever, LLMs must consider the sensitivity of information when communicating\nwith employees, especially given access restrictions. Simple filtering based on\nuser clearance levels can pose both performance and privacy challenges. To\naddress this, we propose the concept of sensitivity awareness (SA), which\nenables LLMs to adhere to predefined access rights rules. In addition, we\ndeveloped a benchmarking environment called ACCESS DENIED INC to evaluate SA.\nOur experimental findings reveal significant variations in model behavior,\nparticularly in managing unauthorized data requests while effectively\naddressing legitimate queries. This work establishes a foundation for\nbenchmarking sensitivity-aware language models and provides insights to enhance\nprivacy-centric AI systems in corporate environments."}
{"id": "2506.05386", "pdf": "https://arxiv.org/pdf/2506.05386.pdf", "abs": "https://arxiv.org/abs/2506.05386", "title": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes", "authors": ["Lo Pang-Yun Ting", "Chengshuai Zhao", "Yu-Hua Zeng", "Yuan Jee Lim", "Kun-Ta Chuang", "Huan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning."}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364.pdf", "abs": "https://arxiv.org/abs/2506.08364", "title": "Structure-Augmented Reasoning Generation", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems fail at complex multi-hop\nreasoning because they rely on large language models to implicitly connect\ninformation from unstructured document collections. This fundamental limitation\nstems from treating retrieved passages as independent context rather than\nrecognizing the intricate relationships that enable coherent reasoning chains.\n  We introduce SARG (Structure-Augmented Reasoning Generation), a\npost-retrieval framework that transforms traditional RAG pipelines by\nmaterializing explicit reasoning structures. SARG extracts {cause, relation,\neffect} triples from retrieved documents, constructs domain-adaptive graphs,\nand performs multi-hop traversal to discover reasoning chains that bridge query\nconcepts to answers. Unlike existing approaches that modify retrieval\nmechanisms, SARG operates as a plug-and-play reasoning layer compatible with\nany RAG system.\n  Extensive evaluation across diverse domains: general QA, biomedical\nliterature, and financial analysis demonstrates that SARG achieves substantial\nimprovements over state-of-the-art RAG baselines. Crucially, SARG also provides\nfull reasoning traceability through explicit inference chains, addressing the\ncritical interpretability gap in current RAG systems.\n  Our results establish that explicit structural reasoning is not merely\nbeneficial but essential for reliable complex question answering, offering a\nsolution to RAG's implicit reasoning bottleneck."}
{"id": "2506.13380", "pdf": "https://arxiv.org/pdf/2506.13380.pdf", "abs": "https://arxiv.org/abs/2506.13380", "title": "DAGR: Decomposition Augmented Graph Retrieval with LLMs", "authors": ["Valentin Six", "Evan Dufraisse", "Gaël de Chalendar"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) excel at many Natural Language Processing (NLP)\ntasks, but struggle with multi-hop reasoning and factual consistency, limiting\ntheir effectiveness on knowledge-intensive tasks like complex question\nanswering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising\nresults, but LLMs generally lack the ability to reason efficiently over\ngraph-structured information. To address this challenge, we introduce DAGR, a\nretrieval method that leverages both complex questions and their decomposition\nin subquestions to extract relevant, linked textual subgraphs. DAGR first\nbreaks down complex queries, retrieves subgraphs guided by a weighted\nsimilarity function over both the original and decomposed queries, and creates\na question-specific knowledge graph to guide answer generation. The resulting\nGraph-RAG pipeline is suited to handle complex multi-hop questions and\neffectively reason over graph-structured data. We evaluate DAGR on standard\nmulti-hop QA benchmarks and show that it achieves comparable or superior\nperformance to competitive existing methods, using smaller models and fewer LLM\ncalls."}
{"id": "2506.17001", "pdf": "https://arxiv.org/pdf/2506.17001.pdf", "abs": "https://arxiv.org/abs/2506.17001", "title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents", "authors": ["Mikhail Menschikov", "Dmitry Evseev", "Victoria Dochkina", "Ruslan Kostoev", "Ilia Perepechkin", "Petr Anokhin", "Evgeny Burnaev", "Nikita Semenov"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Personalizing language models by effectively incorporating user interaction\nhistory remains a central challenge in the development of adaptive AI systems.\nWhile large language models (LLMs) combined with Retrieval-Augmented Generation\n(RAG) have improved factual accuracy, they often lack structured memory and\nfail to scale in complex, long-term interactions. To address this, we propose a\nflexible external memory framework based on knowledge graphs, automatically\nconstructed and updated by the LLM itself, and capable of encoding information\nin multiple formats-including nodes, triplets, higher-order propositions, and\nepisodic traces. Building upon the AriGraph architecture, we introduce a novel\nhybrid graph design that supports both standard edges and two types of\nhyperedges, enabling rich and dynamic semantic and temporal representations.\nOur framework also supports diverse retrieval mechanisms, including A*,\nwater-circle propagation, beam search, and hybrid methods, making it adaptable\nto different datasets and LLM capacities. We evaluate our system on three\nbenchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory\nand retrieval configurations yield optimal performance depending on the task.\nAdditionally, we extend the DiaASQ benchmark with temporal annotations and\ninternally contradictory statements, showing that our system remains robust and\neffective in managing temporal dependencies and context-aware reasoning."}
{"id": "2506.19952", "pdf": "https://arxiv.org/pdf/2506.19952.pdf", "abs": "https://arxiv.org/abs/2506.19952", "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality."}
{"id": "2506.22062", "pdf": "https://arxiv.org/pdf/2506.22062.pdf", "abs": "https://arxiv.org/abs/2506.22062", "title": "MDC-R: The Minecraft Dialogue Corpus with Reference", "authors": ["Chris Madge", "Maris Camilleri", "Paloma Carretero Garcia", "Vanja Karan", "Juexi Shao", "Prashant Jayannavar", "Julian Hough", "Benjamin Roth", "Massimo Poesio"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a\nnew language resource that supplements the original Minecraft Dialogue Corpus\n(MDC) with expert annotations of anaphoric and deictic reference. MDC's\ntask-orientated, multi-turn, situated dialogue in a dynamic environment has\nmotivated multiple annotation efforts, owing to the interesting linguistic\nphenomena that this setting gives rise to. We believe it can serve as a\nvaluable resource when annotated with reference, too. Here, we discuss our\nmethod of annotation and the resulting corpus, and provide both a quantitative\nand a qualitative analysis of the data. Furthermore, we carry out a short\nexperiment demonstrating the usefulness of our corpus for referring expression\ncomprehension."}
{"id": "2506.23998", "pdf": "https://arxiv.org/pdf/2506.23998.pdf", "abs": "https://arxiv.org/abs/2506.23998", "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "categories": ["cs.CL"], "comment": "Presented at ACL 2025 SRW", "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts."}
{"id": "2506.24006", "pdf": "https://arxiv.org/pdf/2506.24006.pdf", "abs": "https://arxiv.org/abs/2506.24006", "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Seßler", "Brian Greer", "Lieven Verschaffel"], "categories": ["cs.CL", "math.HO"], "comment": "v2: added analyses for GPT-5, also leading to small adjustments in\n  the text, no major new interpretations", "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, o3, and GPT-5 on 287 word\nproblems shows that most recent LLMs solve these s-problems with near-perfect\naccuracy, including a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms."}
{"id": "2507.05385", "pdf": "https://arxiv.org/pdf/2507.05385.pdf", "abs": "https://arxiv.org/abs/2507.05385", "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Lucía Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available."}
{"id": "2507.17634", "pdf": "https://arxiv.org/pdf/2507.17634.pdf", "abs": "https://arxiv.org/abs/2507.17634", "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement."}
{"id": "2507.17702", "pdf": "https://arxiv.org/pdf/2507.17702.pdf", "abs": "https://arxiv.org/abs/2507.17702", "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels."}
{"id": "2507.22608", "pdf": "https://arxiv.org/pdf/2507.22608.pdf", "abs": "https://arxiv.org/abs/2507.22608", "title": "Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Yusser Al Ghussin", "Tanja Baeumel", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."}
{"id": "2507.22919", "pdf": "https://arxiv.org/pdf/2507.22919.pdf", "abs": "https://arxiv.org/abs/2507.22919", "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 4 figures. Updated to include Table 2, Supplementary Table\n  1, and an additional baseline random forest model", "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be better designed and monitored. We evaluated methods for\npredicting serious adverse event (SAE) results in clinical trials using\ninformation only from their registrations prior to the trial. Material and\nMethods: We analyzed 22,107 two-arm parallel interventional clinical trials\nfrom ClinicalTrials.gov with structured summary results. Two prediction models\nwere developed: a classifier predicting whether a greater proportion of\nparticipants in an experimental arm would have SAEs (area under the receiver\noperating characteristic curve; AUC) compared to the control arm, and a\nregression model to predict the proportion of participants with SAEs in the\ncontrol arms (root mean squared error; RMSE). A transfer learning approach\nusing pretrained language models (e.g., ClinicalT5, BioBERT) was used for\nfeature extraction, combined with a downstream model for prediction. To\nmaintain semantic representation in long trial texts exceeding localized\nlanguage model input limits, a sliding window method was developed for\nembedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had\n77.6% AUC when predicting which trial arm had a higher proportion of SAEs. When\npredicting SAE proportion in the control arm, the same model achieved RMSE of\n18.6%. The sliding window approach consistently outperformed direct\ncomparisons. Across 12 classifiers, the average absolute AUC increase was\n2.00%, and absolute RMSE reduction was 1.58% across 12 regressors. Discussion:\nSummary results data from ClinicalTrials.gov remains underutilized. Predicted\nresults of publicly reported trials provides an opportunity to identify\ndiscrepancies between expected and reported safety results."}
{"id": "2508.00121", "pdf": "https://arxiv.org/pdf/2508.00121.pdf", "abs": "https://arxiv.org/abs/2508.00121", "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "authors": ["Xiao Zhang", "Johan bos"], "categories": ["cs.CL"], "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation helped improve the parsing results. The reason for\nthe difficulty of parsing elided phrases is not that copying semantic material\nis hard, but that usually occur in linguistically complicated contexts causing\nmost of the parsing errors."}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429.pdf", "abs": "https://arxiv.org/abs/2508.00429", "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning."}
{"id": "2508.03199", "pdf": "https://arxiv.org/pdf/2508.03199.pdf", "abs": "https://arxiv.org/abs/2508.03199", "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "categories": ["cs.CL"], "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems."}
{"id": "2508.03211", "pdf": "https://arxiv.org/pdf/2508.03211.pdf", "abs": "https://arxiv.org/abs/2508.03211", "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "authors": ["Pablo J. Diego-Simón", "Emmanuel Chemla", "Jean-Rémi King", "Yair Lakretz"], "categories": ["cs.CL"], "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance."}
{"id": "2508.03865", "pdf": "https://arxiv.org/pdf/2508.03865.pdf", "abs": "https://arxiv.org/abs/2508.03865", "title": "An Entity Linking Agent for Question Answering", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent."}
{"id": "2508.04903", "pdf": "https://arxiv.org/pdf/2508.04903.pdf", "abs": "https://arxiv.org/abs/2508.04903", "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems."}
{"id": "2508.05097", "pdf": "https://arxiv.org/pdf/2508.05097.pdf", "abs": "https://arxiv.org/abs/2508.05097", "title": "Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations", "authors": ["Aditya Kishore", "Gaurav Kumar", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": null, "summary": "The growing rate of multimodal misinformation, where claims are supported by\nboth text and images, poses significant challenges to fact-checking systems\nthat rely primarily on textual evidence. In this work, we have proposed a\nunified framework for fine-grained multimodal fact verification called\n\"MultiCheck\", designed to reason over structured textual and visual signals.\nOur architecture combines dedicated encoders for text and images with a fusion\nmodule that captures cross-modal relationships using element-wise interactions.\nA classification head then predicts the veracity of a claim, supported by a\ncontrastive learning objective that encourages semantic alignment between\nclaim-evidence pairs in a shared latent space. We evaluate our approach on the\nFactify 2 dataset, achieving a weighted F1 score of 0.84, substantially\noutperforming the baseline. These results highlight the effectiveness of\nexplicit multimodal reasoning and demonstrate the potential of our approach for\nscalable and interpretable fact-checking in complex, real-world scenarios."}
{"id": "2508.05509", "pdf": "https://arxiv.org/pdf/2508.05509.pdf", "abs": "https://arxiv.org/abs/2508.05509", "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Su Dong", "Shengyuan Chen", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems."}
{"id": "2508.05592", "pdf": "https://arxiv.org/pdf/2508.05592.pdf", "abs": "https://arxiv.org/abs/2508.05592", "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy", "authors": ["Shaoxiong Zhan", "Yanlin Lai", "Ziyu Lu", "Dahua Lin", "Ziqing Yang", "Fei Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."}
{"id": "2204.13805", "pdf": "https://arxiv.org/pdf/2204.13805.pdf", "abs": "https://arxiv.org/abs/2204.13805", "title": "Investigating writing style as a contributor to gender gaps in science and technology", "authors": ["Kara Kedrick", "Ekaterina Levitskaya", "Russell J. Funk"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "A growing stream of research finds that scientific contributions are\nevaluated differently depending on the gender of the author. In this article,\nwe consider whether gender differences in writing styles - how men and women\ncommunicate their work - may contribute to these observed gender gaps. We\nground our investigation in a framework for characterizing the linguistic style\nof written text, with two sets of features - informational (i.e., features that\nemphasize facts) and involved (i.e., features that emphasize relationships).\nUsing a large sample of academic papers and patents, we find significant\ndifferences in writing style by gender, with women using more involved features\nin their writing. Papers and patents with more involved features also tend to\nbe cited more by women. Our findings suggest that scientific text is not devoid\nof personal character, which could contribute to bias in evaluation, thereby\ncompromising the norm of universalism as a foundational principle of science."}
{"id": "2311.13171", "pdf": "https://arxiv.org/pdf/2311.13171.pdf", "abs": "https://arxiv.org/abs/2311.13171", "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization", "authors": ["Prateek Yadav", "Leshem Choshen", "Colin Raffel", "Mohit Bansal"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "25 Pages, 6 Figures, 16 Tables", "summary": "Parameter-efficient fine-tuning (PEFT) techniques make it possible to\nefficiently adapt a language model to create \"expert\" models that specialize to\nnew tasks or domains. Recent techniques in model merging and compositional\ngeneralization leverage these expert models by dynamically composing modules to\nimprove zero/few-shot generalization. Despite the efficiency of PEFT methods,\nthe size of expert models can make it onerous to retrieve expert models per\nquery over high-latency networks like the Internet or serve multiple experts on\na single GPU. To address these issues, we present ComPEFT, a novel method for\ncompressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT\nemploys sparsification and ternary quantization to reduce the size of the PEFT\nmodule without performing any additional retraining while preserving or\nenhancing model performance. In extensive evaluation across T5, T0, and\nLLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression\nratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -\nstronger models exhibit higher compressibility and better performance. For\nexample, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on\nMMLU with a storage size reduction of up to 26x. In addition, we show that the\ncompressed experts produced by ComPEFT maintain few-shot compositional\ngeneralization capabilities, facilitate efficient communication and\ncomputation, and exhibit enhanced performance when merged. Lastly, we provide\nan analysis of different method components, compare it with other PEFT methods,\nand test ComPEFT's efficacy for compressing the residual of full-finetuning.\nOur code is available at https://github.com/prateeky2806/compeft."}
{"id": "2407.20756", "pdf": "https://arxiv.org/pdf/2407.20756.pdf", "abs": "https://arxiv.org/abs/2407.20756", "title": "SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption Datasets for Vision-Language Models", "authors": ["Zheng Liu", "Hao Liang", "Bozhou Li", "Wentao Xiong", "Chong Chen", "Conghui He", "Wentao Zhang", "Bin Cui"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable\nvision-understanding capabilities. However, training these models requires\nlarge-scale datasets, which brings challenges related to efficiency,\neffectiveness, and quality of web data. In this paper, we introduce SynthVLM, a\nnew data synthesis and curation method for generating image-caption pairs.\nUnlike traditional methods, where captions are generated from images, SynthVLM\nutilizes advanced diffusion models and high-quality captions to synthesize and\nselect images from text captions, thereby creating precisely aligned image-text\npairs. We further introduce SynthVLM-100K, a high-quality dataset consisting of\n100K curated and synthesized image-caption pairs. In both model and human\nevaluations, SynthVLM-100K outperforms traditional real-world datasets.\nLeveraging this dataset, we develop a new family of multimodal large language\nmodels (MLLMs), SynthVLM-7B and SynthVLM-13B, which achieve state-of-the-art\n(SOTA) performance on various vision question-answering (VQA) tasks. Notably,\nour models outperform LLaVA across most metrics with only 18\\% pretrain data.\nFurthermore, SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU\nbenchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves\nlanguage abilities."}
{"id": "2408.07057", "pdf": "https://arxiv.org/pdf/2408.07057.pdf", "abs": "https://arxiv.org/abs/2408.07057", "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning", "authors": ["Prateek Yadav", "Colin Raffel", "Mohammed Muqeeth", "Lucas Caccia", "Haokun Liu", "Tianlong Chen", "Mohit Bansal", "Leshem Choshen", "Alessandro Sordoni"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field."}
{"id": "2408.07543", "pdf": "https://arxiv.org/pdf/2408.07543.pdf", "abs": "https://arxiv.org/abs/2408.07543", "title": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts", "authors": ["Hao Liang", "Linzhuang Sun", "Minxuan Zhou", "Zirong Chen", "Meiyi Qiang", "Mingan Lin", "Tianpeng Li", "Fan Yang", "Zenan Zhou", "Wentao Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "With the rapid progress of Multimodal LLMs, evaluating their mathematical\nreasoning capabilities has become an increasingly important research direction.\nIn particular, visual-textual mathematical reasoning serves as a key indicator\nof an MLLM's ability to comprehend and solve complex, multi-step quantitative\nproblems. While existing benchmarks such as MathVista and MathVerse have\nadvanced the evaluation of multimodal math proficiency, they primarily rely on\ndigitally rendered content and fall short in capturing the complexity of\nreal-world scenarios. To bridge this gap, we introduce MathScape, a novel\nbenchmark focused on assessing MLLMs' reasoning ability in realistic\nmathematical contexts. MathScape comprises 1,369 high-quality math problems\npaired with human-captured real-world images, closely reflecting the challenges\nencountered in practical educational settings. We conduct a thorough\nmulti-dimensional evaluation across nine leading closed-source MLLMs, three\nopen-source MLLMs with over 20 billion parameters, and seven smaller-scale\nMLLMs. Our results show that even SOTA models struggle with real-world math\ntasks, lagging behind human performance -- highlighting critical limitations in\ncurrent model capabilities. Moreover, we find that strong performance on\nsynthetic or digitally rendered images does not guarantee similar effectiveness\non real-world tasks. This underscores the necessity of MathScape in the next\nstage of multimodal mathematical reasoning."}
{"id": "2410.12777", "pdf": "https://arxiv.org/pdf/2410.12777.pdf", "abs": "https://arxiv.org/abs/2410.12777", "title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts", "authors": ["Hongcheng Gao", "Tianyu Pang", "Chao Du", "Taihang Hu", "Zhijie Deng", "Min Lin"], "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "comment": "ICCV 2025", "summary": "With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning."}
{"id": "2412.02141", "pdf": "https://arxiv.org/pdf/2412.02141.pdf", "abs": "https://arxiv.org/abs/2412.02141", "title": "WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image", "authors": ["Yuci Liang", "Xinheng Lyu", "Meidan Ding", "Wenting Chen", "Jipeng Zhang", "Yuexiang Ren", "Xiangjian He", "Song Wu", "Xiyue Wang", "Sen Yang", "Xiaohan Xing", "Linlin Shen"], "categories": ["cs.CV", "cs.CL"], "comment": "ICCV 2025, 38 pages, 22 figures, 35 tables", "summary": "Recent advancements in computational pathology have produced patch-level\nMulti-modal Large Language Models (MLLMs), but these models are limited by\ntheir inability to analyze whole slide images (WSIs) comprehensively and their\ntendency to bypass crucial morphological features that pathologists rely on for\ndiagnosis. To address these challenges, we first introduce WSI-Bench, a\nlarge-scale morphology-aware benchmark containing 180k VQA pairs from 9,850\nWSIs across 30 cancer types, designed to evaluate MLLMs' understanding of\nmorphological characteristics crucial for accurate diagnosis. Building upon\nthis benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI\nunderstanding that employs a three-stage training approach: WSI-text alignment,\nfeature space alignment, and task-specific instruction tuning. To better assess\nmodel performance in pathological contexts, we develop two specialized WSI\nmetrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that\nWSI-LLaVA outperforms existing models across all capability dimensions, with a\nsignificant improvement in morphological analysis, establishing a clear\ncorrelation between morphological understanding and diagnostic accuracy."}
{"id": "2502.01926", "pdf": "https://arxiv.org/pdf/2502.01926.pdf", "abs": "https://arxiv.org/abs/2502.01926", "title": "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs", "authors": ["Angelina Wang", "Michelle Phan", "Daniel E. Ho", "Sanmi Koyejo"], "categories": ["cs.CY", "cs.CL"], "comment": "Best Paper award at ACL 2025; dataset available at\n  https://github.com/Angelina-Wang/difference_awareness", "summary": "Algorithmic fairness has conventionally adopted the mathematically convenient\nperspective of racial color-blindness (i.e., difference unaware treatment).\nHowever, we contend that in a range of important settings, group difference\nawareness matters. For example, differentiating between groups may be necessary\nin legal contexts (e.g., the U.S. compulsory draft applies to men but not\nwomen) and harm assessments (e.g., referring to girls as ``terrorists'' may be\nless harmful than referring to Muslim people as such). Thus, in contrast to\nmost fairness work, we study fairness through the perspective of treating\npeople differently -- when it is contextually appropriate to. We first\nintroduce an important distinction between descriptive (fact-based), normative\n(value-based), and correlation (association-based) benchmarks. This distinction\nis significant because each category requires separate interpretation and\nmitigation tailored to its specific characteristics. Then, we present a\nbenchmark suite composed of eight different scenarios for a total of 16k\nquestions that enables us to assess difference awareness. Finally, we show\nresults across ten models that demonstrate difference awareness is a distinct\ndimension to fairness where existing bias mitigation strategies may backfire."}
{"id": "2502.06891", "pdf": "https://arxiv.org/pdf/2502.06891.pdf", "abs": "https://arxiv.org/abs/2502.06891", "title": "ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization", "authors": ["Xuefeng Liu", "Songhao Jiang", "Ian Foster", "Jinbo Xu", "Rick Stevens"], "categories": ["q-bio.BM", "cs.CL", "cs.LG"], "comment": null, "summary": "Drug optimization has become increasingly crucial in light of fast-mutating\nvirus strains and drug-resistant cancer cells. Nevertheless, it remains\nchallenging as it necessitates retaining the beneficial properties of the\noriginal drug while simultaneously enhancing desired attributes beyond its\nscope. In this work, we aim to tackle this challenge by introducing\nScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug\noptimization based on molecular scaffolds. Our work comprises three key\ncomponents: (1) A three-stage drug optimization approach that integrates\npretraining, finetuning, and decoding optimization. (2) A novel two-phase\nincremental pre-training strategy for scaffold-based drug optimization. (3) A\ntoken-level decoding optimization strategy, Top-N, that enabling controlled,\nreward-guided generation using the pretrained or finetuned GPT. We demonstrate\nvia a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT\noutperforms the competing baselines in drug optimization benchmarks, while\nexcelling in preserving original functional scaffold and enhancing desired\nproperties."}
{"id": "2502.13811", "pdf": "https://arxiv.org/pdf/2502.13811.pdf", "abs": "https://arxiv.org/abs/2502.13811", "title": "On the Duality between Gradient Transformations and Adapters", "authors": ["Lucas Torroba-Hennigen", "Hunter Lang", "Han Guo", "Yoon Kim"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 2 figures", "summary": "We study memory-efficient optimization of neural networks (in particular\nlanguage models) with linear gradient transformations, where the gradients are\nlinearly mapped to a lower dimensional space than the full parameter space,\nthus saving memory required for gradient accumulation and optimizer state\npersistence. The model parameters are updated by first performing an\noptimization step in the lower dimensional space and then going back into the\noriginal parameter space via the linear map's transpose. We show that\noptimizing the model in this transformed space is equivalent to\nreparameterizing the original model through a linear adapter that additively\nmodifies the model parameters, and then only optimizing the adapter's\nparameters. When the transformation is Kronecker-factored, this establishes an\nequivalence between GaLore and one-sided LoRA. We show that this duality\nbetween gradient transformations and adapter-based reparameterizations unifies\nexisting approaches to memory-efficient training and suggests new techniques\nfor improving training efficiency and memory use."}
{"id": "2502.20758", "pdf": "https://arxiv.org/pdf/2502.20758.pdf", "abs": "https://arxiv.org/abs/2502.20758", "title": "Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth", "authors": ["Seyed Pouyan Mousavi Davoudi", "Amin Gholami Davodi", "Alireza Amiri-Margavi", "Alireza Shafiee Fard", "Mahdi Jafari"], "categories": ["stat.AP", "cs.AI", "cs.CL"], "comment": "6pages", "summary": "We introduce a new approach in which several advanced large language\nmodels-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct,\nClaude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer\nintricate, doctoral-level probability problems without relying on any single\n\"correct\" reference. Rather than depending on an established ground truth, our\ninvestigation focuses on how agreement among diverse models can signal the\nreliability of their outputs and, by extension, reflect the overall quality of\nthe generated questions. To measure this inter-model alignment, we apply a\nsuite of statistical evaluations, including chi-square tests, Fleiss' Kappa\ncoefficients, and confidence interval calculations, thereby capturing both\nprecision in answers and clarity in question phrasing. Our analysis reveals\nthat Claude and Gemini tend to frame questions more coherently and\nunambiguously, which is evidenced by their tighter confidence intervals and\ngreater concordance with responding agents. In contrast, LLAMA exhibits wider\nconfidence bands and a lower level of agreement, indicating more variability\nand reduced consistency in its question formulations. These observations\nsupport the notion that a multi-model collaborative strategy not only improves\nanswer dependability but also offers an effective, data-driven mechanism for\nevaluating and refining question quality when no definitive solution exists.\nUltimately, this work delivers actionable insights into enhancing AI-guided\nreasoning processes through coordinated interactions among heterogeneous\nlanguage models."}
{"id": "2502.20988", "pdf": "https://arxiv.org/pdf/2502.20988.pdf", "abs": "https://arxiv.org/abs/2502.20988", "title": "Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond", "authors": ["Qiyuan Li", "Haijiang Liu", "Caicai Guo", "Chao Gao", "Deyu Chen", "Meng Wang", "Feng Gao", "Frank van Harmelen", "Jinguang Gu"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for publication in Knowledge-Based Systems. The arXiv\n  version is the pre-peer-review preprint, and the final published version is\n  not available here due to publisher policy", "summary": "The large-scale development of large language models (LLMs) in medical\ncontexts, such as diagnostic assistance and treatment recommendations,\nnecessitates that these models possess accurate medical knowledge and deliver\ntraceable decision-making processes. Clinical knowledge, encompassing the\ninsights gained from research on the causes, prognosis, diagnosis, and\ntreatment of diseases, has been extensively examined within real-world medical\npractices. Recently, there has been a notable increase in research efforts\naimed at integrating this type of knowledge into LLMs, encompassing not only\ntraditional text and multimodal data integration but also technologies such as\nknowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper,\nwe review the various initiatives to embed clinical knowledge into\ntraining-based, KG-supported, and RAG-assisted LLMs. We begin by gathering\nreliable knowledge sources from the medical domain, including databases and\ndatasets. Next, we evaluate implementations for integrating clinical knowledge\nthrough specialized datasets and collaborations with external knowledge sources\nsuch as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess\nthe disparity between models developed in academic settings and those in\nindustry. We conclude the survey by presenting evaluation systems applicable to\nrelevant tasks and identifying potential challenges facing this field. In this\nreview, we do not aim for completeness, since any ostensibly complete review\nwould soon be outdated. Our goal is to illustrate diversity by selecting\nrepresentative and accessible items from current research and industry\npractices, reflecting real-world situations rather than claiming completeness.\nThus, we emphasize showcasing diverse approaches."}
{"id": "2503.00566", "pdf": "https://arxiv.org/pdf/2503.00566.pdf", "abs": "https://arxiv.org/abs/2503.00566", "title": "Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires", "authors": ["Kyle Gao", "Dening Lu", "Liangzhi Li", "Nan Chen", "Hongjie He", "Linlin Xu", "Jonathan Li"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires."}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543.pdf", "abs": "https://arxiv.org/abs/2505.10543", "title": "Reasoning Capabilities of Large Language Models on Dynamic Tasks", "authors": ["Annie Wong", "Thomas Bäck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models excel on static benchmarks, but their ability as\nself-learning agents in dynamic environments remains unclear. We evaluate three\nprompting strategies: self-reflection, heuristic mutation, and planning across\ndynamic tasks with open-source models. We find that larger models generally\noutperform smaller ones, but that strategic prompting can close this\nperformance gap. Second, an overly long prompt can negatively impact smaller\nmodels on basic reactive tasks, while larger models show more robust behaviour.\nThird, advanced prompting techniques primarily benefit smaller models on\ncomplex games, but offer less improvement for already high-performing large\nlanguage models. Yet, we find that advanced reasoning methods yield highly\nvariable outcomes: while capable of significantly improving performance when\nreasoning and decision-making align, they also introduce instability and can\nlead to big performance drops. Compared to human performance, our findings\nreveal little evidence of true emergent reasoning. Instead, large language\nmodel performance exhibits persistent limitations in areas like planning and\nspatial coordination, suggesting that large language models still suffer\nfundamental shortcomings that may not be fully overcome through self-reflective\nprompting alone. Reasoning is a multi-faceted task, and while methods like\nChain-of-thought improve multi-step reasoning on math word problems, our\nfindings using dynamic benchmarks highlight important shortcomings in general\nreasoning capabilities, indicating a need to move beyond static benchmarks to\ncapture the complexity of reasoning."}
{"id": "2505.19590", "pdf": "https://arxiv.org/pdf/2505.19590.pdf", "abs": "https://arxiv.org/abs/2505.19590", "title": "Learning to Reason without External Rewards", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19997", "pdf": "https://arxiv.org/pdf/2505.19997.pdf", "abs": "https://arxiv.org/abs/2505.19997", "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "ACL 2025", "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy."}
{"id": "2506.03530", "pdf": "https://arxiv.org/pdf/2506.03530.pdf", "abs": "https://arxiv.org/abs/2506.03530", "title": "How Far Are We from Generating Missing Modalities with Foundation Models?", "authors": ["Guanzhou Ke", "Bo Wang", "Guoqing Chao", "Weiming Hu", "Shengfeng He"], "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal foundation models have demonstrated impressive capabilities across\ndiverse tasks. However, their potential as plug-and-play solutions for missing\nmodality reconstruction remains underexplored. To bridge this gap, we identify\nand formalize three potential paradigms for missing modality reconstruction,\nand perform a comprehensive evaluation across these paradigms, covering 42\nmodel variants in terms of reconstruction accuracy and adaptability to\ndownstream tasks. Our analysis reveals that current foundation models often\nfall short in two critical aspects: (i) fine-grained semantic extraction from\nthe available modalities, and (ii) robust validation of generated modalities.\nThese limitations lead to suboptimal and, at times, misaligned generations. To\naddress these challenges, we propose an agentic framework tailored for missing\nmodality reconstruction. This framework dynamically formulates modality-aware\nmining strategies based on the input context, facilitating the extraction of\nricher and more discriminative semantic features. In addition, we introduce a\nself-refinement mechanism, which iteratively verifies and enhances the quality\nof generated modalities through internal feedback. Experimental results show\nthat our method reduces FID for missing image reconstruction by at least 14\\%\nand MER for missing text reconstruction by at least 10\\% compared to baselines.\nCode are released at: https://github.com/Guanzhou-Ke/AFM2."}
{"id": "2506.04450", "pdf": "https://arxiv.org/pdf/2506.04450.pdf", "abs": "https://arxiv.org/abs/2506.04450", "title": "Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Geoffrey D. Rubin", "Joseph Y. Lo", "Nirav Merchant", "Heidi Hanson", "John Gounley", "Ravi Tandon"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "18 pages, 5 figures, 2 tables", "summary": "Purpose: This study proposes a framework for fine-tuning large language\nmodels (LLMs) with differential privacy (DP) to perform multi-abnormality\nclassification on radiology report text. By injecting calibrated noise during\nfine-tuning, the framework seeks to mitigate the privacy risks associated with\nsensitive patient data and protect against data leakage while maintaining\nclassification performance. Materials and Methods: We used 50,232 radiology\nreports from the publicly available MIMIC-CXR chest radiography and CT-RATE\ncomputed tomography datasets, collected between 2011 and 2019. Fine-tuning of\nLLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels\nfrom CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)\nin high and moderate privacy regimes (across a range of privacy budgets =\n{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1\nscore across three model architectures: BERT-medium, BERT-small, and\nALBERT-base. Statistical analyses compared model performance across different\nprivacy levels to quantify the privacy-utility trade-off. Results: We observe a\nclear privacy-utility trade-off through our experiments on 2 different datasets\nand 3 different models. Under moderate privacy guarantees the DP fine-tuned\nmodels achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on\nCT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.\nConclusion: Differentially private fine-tuning using LoRA enables effective and\nprivacy-preserving multi-abnormality classification from radiology reports,\naddressing a key challenge in fine-tuning LLMs on sensitive medical data."}
{"id": "2506.21298", "pdf": "https://arxiv.org/pdf/2506.21298.pdf", "abs": "https://arxiv.org/abs/2506.21298", "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "comment": "9 pages, 4 figures", "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations."}
{"id": "2506.21839", "pdf": "https://arxiv.org/pdf/2506.21839.pdf", "abs": "https://arxiv.org/abs/2506.21839", "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality."}
{"id": "2506.21931", "pdf": "https://arxiv.org/pdf/2506.21931.pdf", "abs": "https://arxiv.org/abs/2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."}
{"id": "2506.22376", "pdf": "https://arxiv.org/pdf/2506.22376.pdf", "abs": "https://arxiv.org/abs/2506.22376", "title": "Probabilistic Optimality for Inference-time Scaling", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop OptScale, a\npractical algorithm that dynamically determines the optimal number of sampled\nresponses. OptScale employs a language model-based predictor to estimate\nprobabilistic prior parameters, enabling the decision of the minimal number of\nsamples needed that satisfy predefined performance thresholds and confidence\nlevels. Extensive experiments on mathematical reasoning benchmarks (including\nMATH-500, GSM8K, AIME, and AMC) demonstrate that OptScale significantly reduces\nsampling overhead while remaining better or on par with state-of-the-art\nreasoning performance. Our work offers both a theoretical foundation and a\npractical solution for principled inference-time scaling, addressing a critical\ngap in the efficient deployment of LLMs for complex reasoning. The source code\nis publicly available at https://github.com/Albertwyk/OptScale."}
{"id": "2507.04377", "pdf": "https://arxiv.org/pdf/2507.04377.pdf", "abs": "https://arxiv.org/abs/2507.04377", "title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions", "authors": ["Xiao Zhang", "Johan Bos"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "ACMMM 2025", "summary": "Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation."}
{"id": "2507.22025", "pdf": "https://arxiv.org/pdf/2507.22025.pdf", "abs": "https://arxiv.org/abs/2507.22025", "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Yifan Ding", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE for enhancing GUI agents at both\ntraining and inference. For training, we propose a suite of improvements to the\nSupervised Fine-Tuning (SFT) process: 1) a continuous reward function to\nincentivize high-precision grounding; 2) a ``Simple Thinking'' reward to\nbalance planning with speed and grounding accuracy; and 3) a cropping-based\nresampling strategy to mitigate the sparse reward problem and improve learning\non complex tasks. For inference, we present decomposed grounding with selection\nto dramatically improve grounding accuracy on high-resolution displays by\nbreaking the image into smaller, manageable parts. Experiments show that\nUI-AGILE achieves the state-of-the-art grounding performance on two benchmarks\nScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent\ncapabilities. For instance, using both our training and inference enhancement\nmethods brings 23\\% grounding accuracy improvement over the best baseline on\nScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE."}
{"id": "2507.23701", "pdf": "https://arxiv.org/pdf/2507.23701.pdf", "abs": "https://arxiv.org/abs/2507.23701", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."}
