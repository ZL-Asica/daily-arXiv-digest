{"id": "2506.03385", "pdf": "https://arxiv.org/pdf/2506.03385.pdf", "abs": "https://arxiv.org/abs/2506.03385", "title": "From Reality to Recognition: Evaluating Visualization Analogies for Novice Chart Comprehension", "authors": ["Oliver Huang", "Patrick Lee", "Carolina Nobre"], "categories": ["cs.HC"], "comment": null, "summary": "Novice learners often have difficulty learning new visualization types\nbecause they tend to interpret novel visualizations through the mental models\nof simpler charts they have previously encountered. Traditional visualization\nteaching methods, which usually rely on directly translating conceptual aspects\nof data into concrete data visualizations, often fail to attend to the needs of\nnovice learners navigating this tension. To address this, we conducted an\nempirical exploration of how analogies can be used to help novices with chart\ncomprehension. We introduced visualization analogies: visualizations that map\ndata structures to real-world contexts to facilitate an intuitive understanding\nof novel chart types. We evaluated this pedagogical technique using a\nwithin-subject study (N=128) where we taught 8 chart types using visualization\nanalogies. Our findings show that visualization analogies improve visual\nanalysis skills and help learners transfer their understanding to actual\ncharts. They effectively introduce visual embellishments, cater to diverse\nlearning preferences, and are preferred by novice learners over traditional\nchart visualizations. This study offers empirical insights and open-source\ntools to advance visualization education through analogical reasoning.", "AI": {"tldr": "This study explores how visualization analogies can improve comprehension of novel chart types for novice learners, showing significant improvements in visual analysis skills compared to traditional methods.", "motivation": "Novice learners struggle with understanding new visualizations due to reliance on previous simpler charts, necessitating better pedagogical strategies for teaching visualization.", "method": "An empirical study involving 128 participants was conducted where 8 chart types were taught using visualization analogies, mapping data structures to real-world contexts.", "result": "Findings indicate that visualization analogies enhance visual analysis skills and facilitate the transfer of understanding to actual charts, preferred by novices over traditional methods.", "conclusion": "Visualization analogies effectively support diverse learning preferences and improve educational outcomes in visualization comprehension for novices, providing insights and tools for educators.", "key_contributions": ["Introduction of visualization analogies for teaching chart comprehension", "Empirical evidence showing better student outcomes compared to traditional methods", "Open-source tools to support visualization education"], "limitations": "", "keywords": ["visualization", "chart comprehension", "education", "analogies", "novice learners"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03399", "pdf": "https://arxiv.org/pdf/2506.03399.pdf", "abs": "https://arxiv.org/abs/2506.03399", "title": "Sampling Preferences Yields Simple Trustworthiness Scores", "authors": ["Sean Steinle"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the onset of large language models (LLMs), the performance of artificial\nintelligence (AI) models is becoming increasingly multi-dimensional.\nAccordingly, there have been several large, multi-dimensional evaluation\nframeworks put forward to evaluate LLMs. Though these frameworks are much more\nrealistic than previous attempts which only used a single score like accuracy,\nmulti-dimensional evaluations can complicate decision-making since there is no\nobvious way to select an optimal model. This work introduces preference\nsampling, a method to extract a scalar trustworthiness score from\nmulti-dimensional evaluation results by considering the many characteristics of\nmodel performance which users value. We show that preference sampling improves\nupon alternate aggregation methods by using multi-dimensional trustworthiness\nevaluations of LLMs from TrustLLM and DecodingTrust. We find that preference\nsampling is consistently reductive, fully reducing the set of candidate models\n100% of the time whereas Pareto optimality never reduces the set by more than\n50%. Likewise, preference sampling is consistently sensitive to user\npriors-allowing users to specify the relative weighting and confidence of their\npreferences-whereas averaging scores is intransigent to the users' prior\nknowledge.", "AI": {"tldr": "This paper introduces preference sampling, a method to derive a single trustworthiness score from multi-dimensional evaluations of large language models (LLMs). It compares preference sampling to other aggregation methods, highlighting its effectiveness in reducing model candidates while incorporating user preferences.", "motivation": "The increasing complexity of evaluating LLMs necessitates a robust method for simplifying decisions on model selection among multi-dimensional performance metrics.", "method": "The authors propose preference sampling as a technique to extract a scalar trustworthiness score influenced by user-defined preferences, comparing its efficacy against alternate aggregation methods.", "result": "Preference sampling consistently reduces the set of candidate models by 100% of the time, outperforming Pareto optimality, which does not reduce the set by more than 50%.", "conclusion": "Preference sampling enables a more user-sensitive method for selecting LLMs by incorporating users' prior knowledge and preferences effectively.", "key_contributions": ["Introduction of preference sampling for LLM evaluation", "Demonstration of improved model reduction compared to traditional methods", "Emphasis on user sensitivity in model selection process"], "limitations": "", "keywords": ["large language models", "trustworthiness", "preference sampling", "model evaluation", "user preferences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03402", "pdf": "https://arxiv.org/pdf/2506.03402.pdf", "abs": "https://arxiv.org/abs/2506.03402", "title": "The Stress of Improvisation: Instructors' Perspectives on Live Coding in Programming Classes", "authors": ["Xiaotian Su", "April Wang"], "categories": ["cs.HC", "68N01", "K.3.2; H.5.2"], "comment": "6 pages", "summary": "Live coding is a pedagogical technique in which an instructor writes and\nexecutes code in front of students to impart skills like incremental\ndevelopment and debugging. Although live coding offers many benefits,\ninstructors face many challenges in the classroom, like cognitive challenges\nand psychological stress, most of which have yet to be formally studied. To\nunderstand the obstacles faced by instructors in CS classes, we conducted (1) a\nformative interview with five teaching assistants in exercise sessions and (2)\na contextual inquiry study with four lecturers for large-scale classes. We\nfound that the improvisational and unpredictable nature of live coding makes it\ndifficult for instructors to manage their time and keep students engaged,\nresulting in more mental stress than presenting static slides. We discussed\nopportunities for augmenting existing IDEs and presentation setups to help\nenhance live coding experience.", "AI": {"tldr": "The paper investigates the challenges instructors face while using live coding in computer science education and proposes potential enhancements to improve the experience.", "motivation": "To understand the cognitive and psychological challenges instructors encounter while using live coding as a teaching technique in computer science classes.", "method": "Conducted formative interviews with five teaching assistants and a contextual inquiry study with four lecturers in large-scale CS classes.", "result": "Identified that the unpredictability of live coding increases mental stress for instructors compared to static presentations, affecting their ability to engage students and manage time.", "conclusion": "There is a need for improved tools and environments to support instructors during live coding sessions to enhance the teaching experience.", "key_contributions": ["Identified specific cognitive and psychological challenges faced by instructors during live coding.", "Proposed augmentations for IDEs and presentation setups to alleviate these challenges.", "Provided insights from empirical studies with teaching assistants and lecturers."], "limitations": "Focuses primarily on the challenges without extensive exploration of solutions or effectiveness of proposed tools.", "keywords": ["live coding", "computer science education", "instructor challenges", "pedagogical techniques", "IDE enhancements"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2506.03520", "pdf": "https://arxiv.org/pdf/2506.03520.pdf", "abs": "https://arxiv.org/abs/2506.03520", "title": "VChatter: Exploring Generative Conversational Agents for Simulating Exposure Therapy to Reduce Social Anxiety", "authors": ["Han Zhang", "KaWing Tsang", "Zhenhui Peng"], "categories": ["cs.HC"], "comment": null, "summary": "Many people struggle with social anxiety, feeling fear, or even physically\nuncomfortable in social situations like talking to strangers. Exposure therapy,\na clinical method that gradually and repeatedly exposes individuals to the\nsource of their fear and helps them build coping mechanisms, can reduce social\nanxiety but traditionally requires human therapists' guidance and constructions\nof situations. In this paper, we developed a multi-agent system VChatter to\nexplore large language models(LLMs)-based conversational agents for simulating\nexposure therapy with users. Based on a survey study (N=36) and an expert\ninterview, VChatter includes an Agent-P, which acts as a psychotherapist to\ndesign the exposure therapy plans for users, and two Agent-Hs, which can take\non different interactive roles in low, medium, and high exposure scenarios. A\nsix-day qualitative study (N=10) showcases VChatter's usefulness in reducing\nusers' social anxiety, feelings of isolation, and avoidance of social\ninteractions. We demonstrated the feasibility of using LLMs-based\nconversational agents to simulate exposure therapy for addressing social\nanxiety and discussed future concerns for designing agents tailored to social\nanxiety.", "AI": {"tldr": "This paper introduces VChatter, a multi-agent system utilizing large language models for simulating exposure therapy to help reduce social anxiety.", "motivation": "The study aims to alleviate the challenges of social anxiety by employing a technology-driven approach for exposure therapy, traditionally reliant on human therapists.", "method": "The authors developed VChatter, which includes an Agent-P that designs therapy plans and two Agent-Hs that interact with users in varying exposure scenarios, based on a survey (N=36) and qualitative study (N=10).", "result": "The qualitative study demonstrated that VChatter effectively reduced users' social anxiety, feelings of isolation, and avoidance of social interactions.", "conclusion": "The research shows the feasibility of using conversational agents based on LLMs for simulating exposure therapy and highlights the need for tailored agent design for social anxiety.", "key_contributions": ["Development of VChatter as a multi-agent system for therapy simulation", "Empirical evidence demonstrating effectiveness in reducing social anxiety", "Discussion on future design considerations for anxiety-specific agents"], "limitations": "Study sample size was small (N=10), and further research is needed for broader applicability and long-term effects.", "keywords": ["social anxiety", "exposure therapy", "LLMs", "conversational agents", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259.pdf", "abs": "https://arxiv.org/abs/2506.03259", "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "AI": {"tldr": "This study evaluates the use of large language models for automating disease annotation in CT radiology reports, comparing their performance against a rule-based algorithm.", "motivation": "To assess the effectiveness of LLMs in automating annotations of CT radiology reports and to compare them with traditional methods.", "method": "The study analyzed 40,833 CT reports and evaluated three lightweight open-weight LLMs using zero-shot prompting, compared to a rule-based algorithm. Performance metrics included Cohen's Kappa and F1 scores.", "result": "Llama-3.1 8B and Gemma-3 27B exhibited the highest performance, with top scores of 0.87 for Kappa and 0.82 for macro-F1, outperforming the rule-based algorithm.", "conclusion": "Lightweight LLMs are superior to rule-based methods for CT report annotation, although binary labels do not encompass the full complexity of report language.", "key_contributions": ["Demonstrated the effectiveness of LLMs in medical annotations.", "Provided insights into performance metrics in multi-disease labeling.", "Showed that lightweight models can enhance clinical efficiency."], "limitations": "Binary labels do not fully represent the nuances in radiology reports.", "keywords": ["large language models", "CT radiology reports", "disease annotation", "automated reporting", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03687", "pdf": "https://arxiv.org/pdf/2506.03687.pdf", "abs": "https://arxiv.org/abs/2506.03687", "title": "Understanding Visually Impaired Tramway Passengers Interaction with Public Transport Systems", "authors": ["Dominik Mimra", "Dominik Kaar", "Enrico Del Re", "Novel Certad", "Joshua Cherian Varughese", "David Seibt", "Cristina Olaverri-Monreal"], "categories": ["cs.HC"], "comment": "16th International Conference on Applied Human Factors and Ergonomics\n  (AHFE 2025) and the Affiliated Conferences", "summary": "Designing inclusive public transport services is crucial to developing\nmodern, barrier-free smart city infrastructure. This research contributes to\nthe design of inclusive public transport by considering accessibility\nchallenges emerging from socio-technical systems, thus demanding the\nintegration of technological and social solutions. Using Actor-Network Theory\n(ANT) as a theoretical framework and a mixed-method approach, including\nshadowing and a focus group, this study examines the socio-technical networks\nthat shape accessibility experiences for visually impaired passengers utilizing\nthe tram in Linz, Austria. Key dimensions that influence public transport\naccessibility are identified: network configuration, mobility patterns,\ntechnology integration, and warning systems. The results show that\naccessibility emerges from complex interactions between human actors\n(passengers, staff) and non-human actors (assistive devices, infrastructure)\nrather than being an inherent property of transport systems. Digital\ntechnologies serve multiple functions, from navigational assistance to broader\nsocial inclusion, although users comfort with technology varies. Participants\nemphasized the importance of the two-sense principle for warning signals, with\ndirectional audio and tactile feedback particularly valuable.", "AI": {"tldr": "This study examines the socio-technical networks affecting accessibility in public transport for visually impaired passengers, highlighting the integration of technology and social solutions.", "motivation": "To improve inclusive public transport services and address accessibility challenges in smart city infrastructure.", "method": "Utilized Actor-Network Theory and a mixed-method approach, including shadowing and focus groups, to study visually impaired passengers' experiences in Linz's tram system.", "result": "Identified key dimensions influencing accessibility: network configuration, mobility patterns, technology integration, and warning systems; findings indicate accessibility arises from interactions between human and non-human actors.", "conclusion": "Digital technology plays a significant role in enhancing public transport accessibility but varies in user comfort and effectiveness; two-sense principles for warnings are vital.", "key_contributions": ["Integration of socio-technical systems in accessibility research", "Identification of critical dimensions for public transport accessibility", "Insights into user comfort with digital technologies in transport settings"], "limitations": "", "keywords": ["public transport", "accessibility", "socio-technical systems", "Actor-Network Theory", "visually impaired"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268.pdf", "abs": "https://arxiv.org/abs/2506.03268", "title": "A conclusive remark on linguistic theorizing and language modeling", "authors": ["Cristiano Chesi"], "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics", "AI": {"tldr": "", "motivation": "", "method": "", "result": "", "conclusion": "", "key_contributions": [], "limitations": "", "keywords": [], "importance_score": 0, "read_time_minutes": 0}}
{"id": "2506.03720", "pdf": "https://arxiv.org/pdf/2506.03720.pdf", "abs": "https://arxiv.org/abs/2506.03720", "title": "Design of a visual environment for programming by direct data manipulation", "authors": ["Michel Adam", "Patrice Frison", "Moncef Daoud", "Sabine Letellier Zarshenas"], "categories": ["cs.HC"], "comment": "in French language", "summary": "The use of applications on computers, smartphones, and tablets has been\nconsiderably simplified thanks to interactive and dynamic graphical interfaces\ncoupled with the mouse and touch screens. It is no longer necessary to be a\ncomputer specialist to use them. Paradoxically, the development of computer\nprograms generally requires writing lines of code in a programming language\nwhose syntax is particularly strict. This process poses many difficulties for\nprogrammers. We propose an original tool in which arbitrary programs\n(Turing-complete) can be developed in a completely visual manner by direct\nmanipulation of the data, without writing a line of code. The user can thus\ndevelop an algorithm by directly visualizing the result of actions taken on the\ndata. A method for constructing iterations is associated with the tool. It\nproposes to create each part, including the loop body, in a non-linear manner\nunder visual control of the state of the data. In addition, the tool supports\nthe production of lines of code in several languages including Python, C, Java,\nthat correspond to the actions performed. In this article, we present the tool,\nthe design choices, the problems to be solved, and the limits and the\ncontributions of the direct-data-manipulation approach.", "AI": {"tldr": "This paper presents a tool for developing Turing-complete programs visually without coding, allowing users to manipulate data directly and visualize algorithm results, including support for generating code in several programming languages.", "motivation": "To simplify the programming process for non-specialists by eliminating the need for traditional coding syntax, making programming more accessible.", "method": "The tool allows users to create algorithms and iterations visually, controlling the state of the data and seeing the results of their manipulations in real time.", "result": "The tool successfully enables the visual development of arbitrary programs and generates equivalent code in languages like Python, C, and Java.", "conclusion": "This direct-data-manipulation approach facilitates a more intuitive method to program, although it comes with certain limitations and challenges.", "key_contributions": ["Developing Turing-complete programs through visual manipulation", "Generating code in multiple programming languages from visual designs", "Providing real-time feedback on data manipulation during algorithm development"], "limitations": "There are inherent challenges in accurately representing complex algorithms visually and ensuring the generated code is efficient and compact.", "keywords": ["visual programming", "Turing-complete", "data manipulation", "graphical interfaces", "algorithm development"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278.pdf", "abs": "https://arxiv.org/abs/2506.03278", "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ.", "AI": {"tldr": "FailureSensorIQ is a MCQA benchmarking system for assessing LLMs' reasoning in Industry 4.0, revealing performance issues across various models.", "motivation": "To assess LLM reasoning capabilities in complex, domain-specific scenarios related to Industry 4.0.", "method": "Developed a novel MCQA benchmarking system focusing on failure modes and sensor data relationships, evaluating LLMs through various analytical methods.", "result": "LLMs show a significant performance drop when faced with perturbations and distractions, despite some models nearing expert-level performance.", "conclusion": "LLMs can influence modeling decisions in industrial contexts, with detailed findings and resources provided for further study.", "key_contributions": ["Introduction of FailureSensorIQ for LLM assessment in Industry 4.0", "Detailed evaluation of LLMs including GPT-4 and Llama", "Real-world case study showcasing LLM applicability in failure prediction"], "limitations": "Performance drops in LLMs are fragile and sensitive to various disruptions.", "keywords": ["Large Language Models", "Multi-Choice Question-Answering", "Industry 4.0", "feature selection", "failure prediction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03731", "pdf": "https://arxiv.org/pdf/2506.03731.pdf", "abs": "https://arxiv.org/abs/2506.03731", "title": "Enhancing Text Comprehension for Dyslexic Readers: A 3D Semantic Visualization Approach Using Transformer Mode", "authors": ["Zhengyang Li"], "categories": ["cs.HC"], "comment": null, "summary": "Dyslexic individuals often face significant challenges with traditional\nreading, particularly when engaging with complex texts such as mystery novels.\nThese texts typically demand advanced narrative tracking and information\nintegration skills, making it difficult for dyslexic readers to fully\ncomprehend the content. However, research indicates that while dyslexic\nindividuals may struggle with textual processing, they often possess strong\nspatial imagination abilities. Leveraging this strength, this study proposes an\ninnovative approach using Transformer models to map sentences and words into\nthree-dimensional vector representations. This process clusters semantically\nsimilar sentences and words in spatial proximity, allowing dyslexic readers to\ninterpret the semantic structure and narrative flow of the text through spatial\nperception. Experimental results demonstrate that, compared to direct text\nreading, this three-dimensional semantic visualization method significantly\nenhances dyslexic readers' comprehension of complex texts. In particular, it\nshows marked advantages in identifying narrative relationships and character\nconnections. This study provides a novel pathway for improving textual\ncomprehension among dyslexic individuals", "AI": {"tldr": "This study introduces a Transformer model-based approach utilizing three-dimensional semantic visualization to enhance text comprehension for dyslexic readers.", "motivation": "Dyslexic individuals struggle with traditional reading, especially complex texts, but they often have strong spatial imagination skills.", "method": "The study employs Transformer models to convert sentences and words into three-dimensional vector representations, clustering semantically similar content to aid comprehension.", "result": "Experimental results show that this 3D semantic visualization method significantly improves dyslexic readers' comprehension of complex texts, enhancing narrative tracking and understanding character relationships.", "conclusion": "This innovative approach offers a promising pathway to improve reading comprehension for dyslexic individuals by leveraging their spatial abilities.", "key_contributions": ["Development of a three-dimensional semantic visualization technique for reading comprehension", "Demonstration of improved narrative relationship identification and character connection understanding", "Application of Transformer models in educational support for dyslexic readers."], "limitations": "", "keywords": ["dyslexia", "reading comprehension", "Transformer models", "semantic visualization", "narrative tracking"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292.pdf", "abs": "https://arxiv.org/abs/2506.03292", "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting.", "AI": {"tldr": "HyperSteer is introduced as a hypernetwork-based architecture for generating steering vectors for language models, achieving superior performance compared to existing methods.", "motivation": "The paper addresses the limitations of current methods for generating steering vectors for language models, specifically the trade-off between unsupervised methods that lack efficacy guarantees and supervised methods that require extensive data.", "method": "HyperSteer uses a family of hypernetwork-based architectures that generate steering vectors based on natural language steering prompts and the internal states of the language model.", "result": "HyperSteer outperforms state-of-the-art activation steering methods even with unseen steering prompts and matches the effectiveness of steering via prompting.", "conclusion": "The study demonstrates that HyperSteer is a scalable and effective solution for controlling language model outputs by generating appropriate steering vectors.", "key_contributions": ["Introduction of HyperSteer for generating steering vectors", "End-to-end training of hypernetworks conditioned on prompts", "Performance exceeds existing state-of-the-art methods"], "limitations": "", "keywords": ["language models", "steering vectors", "hypernetwork", "control mechanisms", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03741", "pdf": "https://arxiv.org/pdf/2506.03741.pdf", "abs": "https://arxiv.org/abs/2506.03741", "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "authors": ["Rifat Mehreen Amin", "Oliver Hans KÃ¼hle", "Daniel Buschek", "Andreas Butz"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI.", "AI": {"tldr": "PromptCanvas is a widget-based interactive tool that enhances AI-generated content creation by allowing users to customize and organize their prompts on an infinite canvas, leading to improved creativity and reduced cognitive load.", "motivation": "To provide users with greater control and flexibility in generating AI content through a visual and interactive approach that enhances engagement.", "method": "A lab study involving 18 participants and a follow-up field study with 10 participants compared PromptCanvas's effectiveness against a traditional conversational UI, measuring creativity support and cognitive load.", "result": "PromptCanvas outperformed traditional UI on the Creativity Support Index, showing lower mental demand and frustration among participants, while enhancing collaborative writing experiences.", "conclusion": "Dynamic, customizable interfaces like PromptCanvas can significantly improve user engagement and creativity in AI-supported writing tasks.", "key_contributions": ["Introduction of an infinite canvas for prompt customization", "Demonstrated reduction in cognitive load compared to traditional UIs", "Provided evidence for improved creativity in collaborative writing through visual organization."], "limitations": "", "keywords": ["PromptCanvas", "AI-generated content", "Creativity Support Index", "collaborative writing", "interactive widgets"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295.pdf", "abs": "https://arxiv.org/abs/2506.03295", "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "AI": {"tldr": "Critique Fine-Tuning (CFT) efficiently enhances the reasoning abilities of large language models using minimal training on single problems, outperforming traditional reinforcement learning approaches.", "motivation": "To find a more efficient method than reinforcement learning (RL) for improving reasoning capabilities in large language models (LLMs).", "method": "The authors employ Critique Fine-Tuning (CFT) by generating diverse model responses to a single problem and use teacher LLMs to critique these responses.", "result": "CFT significantly improves the performance of Qwen and Llama models, achieving average improvements of 15% on math benchmarks and 16% on logic reasoning benchmarks with only 5 GPU hours of training.", "conclusion": "One-shot CFT is a simple, general, and compute-efficient method to enhance reasoning capabilities in modern LLMs, providing competitive results compared to traditional RL methods at a fraction of the computational cost.", "key_contributions": ["Introduction of Critique Fine-Tuning (CFT) for enhancing LLM reasoning with low compute", "Demonstration of significant performance gains on math and logic reasoning tasks", "Comparison of CFT effectiveness against traditional RL approaches with 20x less compute usage."], "limitations": "CFT relies on generating critiques from teacher LLMs, which may limit generalizability to novel tasks beyond those the model has been fine-tuned on.", "keywords": ["Critique Fine-Tuning", "Large Language Models", "Reinforcement Learning", "Reasoning", "Compute Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03807", "pdf": "https://arxiv.org/pdf/2506.03807.pdf", "abs": "https://arxiv.org/abs/2506.03807", "title": "Understanding Mental Models of Generative Conversational Search and The Effect of Interface Transparency", "authors": ["Chadha Degachi", "Samuel Kernan Freire", "Evangelos Niforatos", "Gerd Kortuem"], "categories": ["cs.HC", "cs.IR"], "comment": "Work in Progress", "summary": "The experience and adoption of conversational search is tied to the accuracy\nand completeness of users' mental models -- their internal frameworks for\nunderstanding and predicting system behaviour. Thus, understanding these models\ncan reveal areas for design interventions. Transparency is one such\nintervention which can improve system interpretability and enable mental model\nalignment. While past research has explored mental models of search engines,\nthose of generative conversational search remain underexplored, even while the\npopularity of these systems soars. To address this, we conducted a study with\n16 participants, who performed 4 search tasks using 4 conversational interfaces\nof varying transparency levels. Our analysis revealed that most user mental\nmodels were too abstract to support users in explaining individual search\ninstances. These results suggest that 1) mental models may pose a barrier to\nappropriate trust in conversational search, and 2) hybrid web-conversational\nsearch is a promising novel direction for future search interface design.", "AI": {"tldr": "This paper explores users' mental models of generative conversational search and the impact of transparency on user experience and trust.", "motivation": "Understanding users' mental models of conversational search can reveal design intervention opportunities for improving interaction and trust.", "method": "Conducted a study with 16 participants performing 4 search tasks across conversational interfaces with varying transparency levels.", "result": "Most user mental models were found to be too abstract, limiting their ability to explain search instances, and indicating mental models may hinder trust in conversational search.", "conclusion": "Improving transparency in conversational search interfaces could enhance user trust and interface design; hybrid search approaches are a promising future direction.", "key_contributions": ["Investigated user mental models in generative conversational search interfaces.", "Identified transparency as a key factor in user trust.", "Proposed hybrid web-conversational search as a novel design direction."], "limitations": "Small participant size, limiting generalizability.", "keywords": ["conversational search", "mental models", "transparency", "user trust", "search interface design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301.pdf", "abs": "https://arxiv.org/abs/2506.03301", "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph.", "AI": {"tldr": "This study proposes a method to automatically generate usage policies in the W3C Open Digital Rights Language (ODRL) using large language models like GPT-4, with a focus on improving accuracy through ontology documentation.", "motivation": "To automate the generation of usage policies from natural language instructions, improving the efficiency and accuracy of policy creation in distributed data exchange contexts.", "method": "Leveraging the ODRL ontology and its documentation to guide a knowledge graph (KG) construction process, including various heuristics for adaptation.", "result": "The approach achieved an accuracy of up to 91.95% in generating knowledge graphs based on a benchmark of 12 use cases.", "conclusion": "The study demonstrates the potential of using large language models for effective policy generation in the cultural domain, suggesting further applications in trustworthy data exchange.", "key_contributions": ["Development of a method for automatic policy generation from natural language using large language models", "Integration of ODRL ontology for better guidance in knowledge graph construction", "Creation of a benchmark for evaluating policy generation effectiveness"], "limitations": "", "keywords": ["large language models", "ODRL", "knowledge graphs", "data exchange", "policy generation"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.04167", "pdf": "https://arxiv.org/pdf/2506.04167.pdf", "abs": "https://arxiv.org/abs/2506.04167", "title": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity on Human-LLM Collaboration", "authors": ["Matthew Russell", "Aman Shah", "Giles Blaney", "Judith Amores", "Mary Czerwinski", "Robert J. K. Jacob"], "categories": ["cs.HC"], "comment": "15 pages, 12 figures", "summary": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration.", "AI": {"tldr": "This paper analyzes the effects of Microsoft's Copilot, a LLM-based assistant, on users during various tasks, indicating that its effectiveness varies significantly based on the nature of the task.", "motivation": "To explore the under-researched effects of AI-based interactive assistants on users' mental and physiological states.", "method": "The study involved objective tasks (SAT reading comprehension) and subjective tasks (personal reflection), utilizing fNIRS, Empatica E4, NASA-TLX, and questionnaires to measure performance and user experience with and without Copilot.", "result": "Participants reported reduced workload and increased enjoyment in objective tasks, with improved performance. In contrast, subjective tasks showed no performance change with Copilot, highlighting task-dependent effectiveness.", "conclusion": "AI assistants like Copilot demonstrate varying effectiveness based on task type, posing implications for human-AI collaboration in tasks that require episodic memory.", "key_contributions": ["Analysis of task-dependent effectiveness of AI assistants", "Introduction of brain-network based insights into human-AI collaboration", "Empirical measurements of user performance and experience across different task types"], "limitations": "No physiological changes recorded; effectiveness based on subjective user reporting.", "keywords": ["AI assistants", "human-AI collaboration", "task performance", "user experience", "cognitive processes"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303.pdf", "abs": "https://arxiv.org/abs/2506.03303", "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks.", "AI": {"tldr": "Hopscotch is a method for optimizing causal language models by identifying and skipping less important attention blocks, achieving high efficiency with minimal performance loss.", "motivation": "To enhance the efficiency of modern causal language models by skipping unnecessary attention blocks without degrading output quality.", "method": "Hopscotch identifies and skips attention blocks based on their contribution to a task and adaptively scales the outputs of the remaining layers using trainable parameters.", "result": "When implemented on models like Llama-3.1-8B and Qwen2.5-7B, Hopscotch resulted in less than a 2% performance drop despite skipping four attention blocks.", "conclusion": "Hopscotch improves model efficiency by optimizing attention block usage while maintaining performance, and it is compatible with existing model compression techniques.", "key_contributions": ["Introduced a method to skip unnecessary attention blocks in language models", "Developed lightweight, trainable scaling parameters to mitigate performance drops", "Demonstrated low performance loss when applied to large language models"], "limitations": "", "keywords": ["language models", "attention blocks", "model efficiency", "scalable outputs"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310.pdf", "abs": "https://arxiv.org/abs/2506.03310", "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "authors": ["Guillermo Marco", "Julio Gonzalo", "VÃ­ctor Fresno"], "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "This paper explores how reader preferences shape the evaluation of AI-generated and human-authored literary texts, revealing two distinct reader profiles.", "motivation": "The study aims to understand why previous research on AI-generated vs. human-authored literature has produced conflicting results, focusing on reader interpretation and value rather than intrinsic text quality.", "method": "The authors utilize five public datasets, extracting 17 textual features, and model individual reader preferences to analyze their importance in a shared preference space.", "result": "The analysis reveals two reader profiles: 'surface-focused readers' prioritize readability, while 'holistic readers' value thematic depth. The findings show that literary quality assessments are influenced by how text features align with readers' preferences.", "conclusion": "The results advocate for incorporating reader-sensitive evaluation frameworks in creative text generation assessments, emphasizing the importance of reader interpretation in quality evaluation.", "key_contributions": ["Identification of two reader profiles: surface-focused and holistic readers", "Quantitative explanation of literary quality measurements based on reader preferences", "Advocacy for reader-sensitive evaluation frameworks in literature"], "limitations": "", "keywords": ["AI-generated texts", "reader preferences", "literary evaluation", "HCI", "text generation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310.pdf", "abs": "https://arxiv.org/abs/2506.03310", "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "authors": ["Guillermo Marco", "Julio Gonzalo", "VÃ­ctor Fresno"], "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "This study explores the divergence in evaluations of AI-generated versus human-authored literary texts, attributing differences to reader interpretation and value preferences rather than text quality.", "motivation": "To understand why evaluations of AI-generated and human-written texts yield conflicting results, focusing on reader interpretation and value frameworks.", "method": "Analyzed 1,471 stories with 101 annotators, extracting 17 textual features and modeling reader preferences to evaluate literary quality against individual priorities.", "result": "Two reader profiles emerged: 'surface-focused' readers who value readability, and 'holistic' readers who prioritize thematic and rhetorical qualities. This analysis quantitatively links literary quality assessments with reader preferences.", "conclusion": "The findings suggest the need for reader-sensitive evaluation frameworks in creative text generation to better account for individual preferences in literary quality assessment.", "key_contributions": ["Identification of two distinct reader profiles in literary evaluation", "Quantitative analysis linking text features with reader preferences", "Advocacy for reader-sensitive frameworks in text quality assessment"], "limitations": "", "keywords": ["AI-generated texts", "literary quality", "reader preferences", "creative text generation", "evaluation frameworks"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312.pdf", "abs": "https://arxiv.org/abs/2506.03312", "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media.", "AI": {"tldr": "The paper presents a dataset of 30,000 hand-coded posts for violent threats and evaluates machine learning classification accuracy across different platforms.", "motivation": "To create high-quality data that aids in the understanding and detection of violent content on social media.", "method": "A cross-platform dataset of 30,000 posts was hand-coded for violent threats and analyzed using machine learning in comparison to an existing dataset from YouTube.", "result": "High classification accuracy was achieved when training on one dataset and testing on another, indicating effectiveness in identifying violent content across different platforms.", "conclusion": "The findings enhance content-classification strategies and contribute to the broader understanding of violent content on social media.", "key_contributions": ["Introduction of a large cross-platform dataset for violent threats", "High classification accuracy between datasets", "Insights into content-classification strategies for violent content"], "limitations": "", "keywords": ["violent threats", "social media", "machine learning", "dataset", "content classification"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357.pdf", "abs": "https://arxiv.org/abs/2506.03357", "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "authors": ["Aldan Creo", "HÃ©ctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano HormazÃ¡bal-Lagos"], "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "AI": {"tldr": "A novel method called 'Ask a Local' is introduced for detecting hallucinations in large language models by evaluating perplexity distributions of specialized models in multiple languages.", "motivation": "To address the significant challenge of hallucinations in large language models, where they generate factually incorrect information.", "method": "The method calculates divergence between perplexity distributions of language-specialized models to detect potentially hallucinated information.", "result": "The approach shows consistent performance across 14 languages, particularly excelling in Italian and Catalan, with notable IoU scores and Spearman correlation values.", "conclusion": "The method is effective in multilingual contexts without needing language-specific adaptations and the authors release their code for further research.", "key_contributions": ["Introduction of 'Ask a Local' for hallucination detection", "Efficiency in multilingual applications without adaptation", "Strong performance in specific languages like Italian and Catalan"], "limitations": "", "keywords": ["hallucination detection", "large language models", "multilingual processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735.pdf", "abs": "https://arxiv.org/abs/2506.03735", "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "Math2Visual is an automated framework for generating educational visuals from math word problems (MWPs) text descriptions, aimed at improving the teaching of mathematics.", "motivation": "Automated methods for creating visuals for math word problems are lacking despite their importance in helping young learners interpret and solve problems.", "method": "Math2Visual utilizes a pre-defined visual language and design space based on teacher interviews, constructing an annotated dataset of 1,903 visuals to evaluate and fine-tune Text-to-Image models for educational purposes.", "result": "The framework improves the generation of educational visuals from MWPs, establishing a new benchmark for automated creation while addressing challenges in multimodal content.", "conclusion": "Math2Visual enhances the educational process by automating visual generation for math problems and provides insights into the challenges of creating effective educational visuals.", "key_contributions": ["Introduction of the Math2Visual framework for generating educational visuals from text", "Creation of a large annotated dataset of visuals for evaluating TTI models", "Establishment of benchmarks and insights into challenges in multimodal educational content"], "limitations": "The study identifies challenges such as misrepresentation of mathematical relationships and omission of essential visual elements in generated visuals.", "keywords": ["Math Word Problems", "Visual Generation", "Education Technology", "Text-to-Image Models", "Human-Computer Interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360.pdf", "abs": "https://arxiv.org/abs/2506.03360", "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "AI": {"tldr": "This paper proposes a 3M pipeline using multimodal large language models for rapid disaster damage assessment from social media data.", "motivation": "Effective emergency response requires rapid and fine-grained disaster damage assessment, but traditional methods face challenges due to limited sensors and delayed reporting.", "method": "The study introduces a structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that utilizes multimodal large language models to analyze data from social media during two major earthquake events.", "result": "MLLMs were found to effectively integrate image and text signals, showing a strong correlation with seismic data, although performance varied by language and input type.", "conclusion": "The research demonstrates the potential of MLLMs for disaster assessment and provides a foundation for future research in real-time crisis applications.", "key_contributions": ["Introduction of a 3M pipeline for disaster assessment", "Demonstrated effectiveness of MLLMs in integrating multimodal data", "Released code and data for further research"], "limitations": "Performance varies based on language and input modality, which may affect overall assessment accuracy.", "keywords": ["disaster assessment", "multimodal large language models", "social media", "earthquake analysis", "real-time crisis management"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043.pdf", "abs": "https://arxiv.org/abs/2506.04043", "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "This paper evaluates the effectiveness of Large Language Model-generated counter-narratives for online hate speech, highlighting concerns about accessibility and emotional tone.", "motivation": "To address the growing issue of online hate speech through automated counter-narratives, while examining their ethical implications and effectiveness.", "method": "A framework is proposed to assess LLM-generated counter-narratives based on persona framing, verbosity, readability, affective tone, and ethical robustness, using three different LLMs and datasets.", "result": "The study found that LLM-generated counter-narratives are often verbose and tailored for higher literacy levels, being less accessible to the general public. Emotional prompts improve empathy but raise safety concerns.", "conclusion": "While LLMs can create counter-narratives, issues with verbosity and accessibility need to be addressed to ensure effectiveness in real-world applications.", "key_contributions": ["A comprehensive evaluation framework for LLM-generated counter-narratives.", "Comparative analysis of different LLMs on hate speech datasets.", "Insights into the balance between emotional tone and ethical considerations in generated content."], "limitations": "Lack of accessibility due to literacy level requirements and unresolved safety concerns.", "keywords": ["automated counter-narratives", "hate speech", "LLMs", "ethical implications", "accessibility"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408.pdf", "abs": "https://arxiv.org/abs/2506.03408", "title": "Trajectory Prediction Meets Large Language Models: A Survey", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.", "AI": {"tldr": "This survey explores the integration of large language models (LLMs) into trajectory prediction across five key areas, analyzing methods and identifying challenges.", "motivation": "To bridge natural language processing and trajectory prediction by leveraging the semantic and reasoning capabilities of LLMs.", "method": "The paper categorizes recent work into five directions: trajectory prediction via language modeling, direct prediction with pretrained LLMs, language-guided scene understanding, language-driven data generation, and language-based reasoning.", "result": "It highlights representative methods within these categories, core design choices, and identifies open challenges in the field.", "conclusion": "The survey provides a unified perspective on how language can enhance trajectory prediction in autonomous systems.", "key_contributions": ["Comprehensive overview of the intersection of LLMs and trajectory prediction.", "Categorization of methods into five distinct areas.", "Identification of core challenges and design choices in the field."], "limitations": "", "keywords": ["large language models", "trajectory prediction", "natural language processing", "autonomous systems", "data generation"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.04072", "pdf": "https://arxiv.org/pdf/2506.04072.pdf", "abs": "https://arxiv.org/abs/2506.04072", "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.HC", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "AI": {"tldr": "This paper explores how controllable generation techniques can make large language models more suitable for beginner language learners by adapting the complexity of output text.", "motivation": "There's a need to improve language learning tools for absolute beginners who struggle with the complexity of LLM outputs.", "method": "The authors investigate modular controllable generation methods that do not require fine-tuning the language models, evaluating effectiveness through automatic metrics and a user study focused on Japanese language learners.", "result": "The study finds that traditional prompting methods are inadequate for controlling output difficulty, but using future discriminators substantially increases comprehensibility from 40.4% to 84.3%.", "conclusion": "The introduction of a novel token-level evaluation metric, Token Miss Rate (TMR), provides a strong correlation with human understanding and can guide future research in AI-supported language learning.", "key_contributions": ["Demonstration of controllable generation techniques for beginner language learners.", "Development of the Token Miss Rate (TMR) evaluation metric.", "Release of code, models, annotation tools, and dataset for further research."], "limitations": "", "keywords": ["large language models", "language learning", "controllable generation", "output comprehensibility", "Token Miss Rate"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424.pdf", "abs": "https://arxiv.org/abs/2506.03424", "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.", "AI": {"tldr": "DistRAG enhances LLMs' spatial reasoning by integrating geodesic distance retrieval to improve distance-based question answering.", "motivation": "To improve the spatial reasoning capabilities of LLMs for tasks like POI recommendation and itinerary planning.", "method": "DistRAG encodes geodesic distances between cities and towns in a graph, retrieving a relevant context subgraph to answer distance-based questions.", "result": "The method enables LLMs to answer previously unanswerable distance reasoning questions by utilizing external spatial information.", "conclusion": "DistRAG serves as a foundational step toward equipping LLMs with a basic world model that supplements their linguistic knowledge.", "key_contributions": ["Development of DistRAG for spatial reasoning in LLMs", "Integration of geodesic distance information for improved question answering", "Establishment of a flexible method to create a rudimentary world model for LLMs."], "limitations": "", "keywords": ["Large Language Models", "spatial reasoning", "geodesic distance", "point of interest", "itinerary planning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434.pdf", "abs": "https://arxiv.org/abs/2506.03434", "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich SchÃ¼tze"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs.", "AI": {"tldr": "The paper analyzes how the OLMo-7B model represents factual knowledge throughout its pre-training phase, focusing on the roles and stability of attention heads and feed forward networks.", "motivation": "Understanding the acquisition and storage of factual knowledge in LLMs is essential for improving their interpretability and reliability.", "method": "The study categorizes attention heads and feed forward networks into four roles and tracks their evolution over the pre-training of the OLMo-7B model.", "result": "The findings reveal that LLM components initially rely on general-purpose capabilities and then become more specialized, with evidence of adaptive learning as components repurpose for reliable answer prediction.", "conclusion": "Attention heads show significant turnover while FFNs demonstrate stability, with certain relations converging to accuracy earlier based on task complexity, providing insights into LLM knowledge formation dynamics.", "key_contributions": ["Classification of roles of model components in LLMs", "Insights into adaptive learning processes in LLM training", "Demonstration of how relations' complexity affects knowledge acquisition"], "limitations": "", "keywords": ["large language models", "knowledge representation", "attention heads", "feed forward networks", "adaptive learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.20491", "pdf": "https://arxiv.org/pdf/2502.20491.pdf", "abs": "https://arxiv.org/abs/2502.20491", "title": "Examining Algorithmic Curation on Social Media: An Empirical Audit of Reddit's r/popular Feed", "authors": ["Jackie Chan", "Fred Choi", "Koustuv Saha", "Eshwar Chandrasekharan"], "categories": ["cs.HC", "cs.SI"], "comment": "15 pages, 5 figures", "summary": "Platforms are increasingly relying on algorithms to curate the content within\nusers' social media feeds. However, the growing prominence of proprietary,\nalgorithmically curated feeds has concealed what factors influence the\npresentation of content on social media feeds and how that presentation affects\nuser behavior. This lack of transparency can be detrimental to users, from\nreducing users' agency over their content consumption to the propagation of\nmisinformation and toxic content. To uncover details about how these feeds\noperate and influence user behavior, we conduct an empirical audit of Reddit's\nalgorithmically curated trending feed called r/popular. Using 10K r/popular\nposts collected by taking snapshots of the feed over 11 months, we find that\nrecent comments help a post remain on r/popular longer and climb the feed. We\nalso find that posts below rank 80 correspond to a sharp decline in activity\ncompared to posts above. When examining the effects of having a higher\nproportion of undesired behavior -- i.e., moderator-removed and toxic comments\n-- we find no significant evidence that it helps posts stay on r/popular for\nlonger. Although posts closer to the top receive more undesired comments, we\nfind this increase to coincide with a broader increase in overall engagement --\nrather than indicating a disproportionate effect on undesired activity. The\nrelationships between algorithmic rank and engagement highlight the extent to\nwhich algorithms employed by social media platforms essentially determine which\ncontent is prioritized and which is not. We conclude by discussing how content\ncreators, consumers, and moderators on social media platforms can benefit from\nempirical audits aimed at improving transparency in algorithmically curated\nfeeds.", "AI": {"tldr": "An empirical audit of Reddit's algorithmically curated feed reveals how algorithms prioritize content, influencing user engagement and behavior.", "motivation": "To investigate the opacity of algorithmic decision-making in social media feeds and its impact on user behavior.", "method": "Conducted an empirical audit of Reddit's r/popular feed, analyzing 10K posts over 11 months to examine how content is ranked and engages users.", "result": "Recent comments tend to help posts remain higher longer on r/popular. Posts ranked below 80 suffer a sharp engagement drop, while undesired comments do not significantly impact post longevity on the feed.", "conclusion": "Empirical audits can enhance transparency in social media algorithms, benefiting creators, consumers, and moderators by elucidating content prioritization.", "key_contributions": ["Empirical evidence on how recent comments affect post ranking and engagement.", "Identification of a sharp engagement drop for posts ranked below 80.", "Analysis of undesired content impact on algorithmic visibility."], "limitations": "", "keywords": ["algorithmic transparency", "social media", "engagement", "Reddit", "content moderation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458.pdf", "abs": "https://arxiv.org/abs/2506.03458", "title": "Culture Matters in Toxic Language Detection in Persian", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection.", "AI": {"tldr": "This paper explores toxic language detection in Persian using various methods and highlights the influence of cultural context on transfer learning effectiveness.", "motivation": "To enhance online safety by improving toxic language detection, particularly in under-researched languages like Persian.", "method": "Comparison of methods including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning.", "result": "Finding that transfer learning from culturally similar languages yields better results for toxic language detection in Persian than from culturally distinct languages.", "conclusion": "Emphasizes the importance of cultural context in effective transfer learning for natural language processing tasks like toxic language detection.", "key_contributions": ["Comprehensive evaluation of toxic language detection methods in Persian.", "Insight into the role of cultural similarity in transfer learning outcomes.", "First extensive study on toxic language detection in the Persian language."], "limitations": "Focuses mainly on Persian; findings may not generalize to all languages or contexts.", "keywords": ["toxic language detection", "transfer learning", "Persian language", "cultural context", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.07879", "pdf": "https://arxiv.org/pdf/2504.07879.pdf", "abs": "https://arxiv.org/abs/2504.07879", "title": "Towards Sustainable Creativity Support: An Exploratory Study on Prompt Based Image Generation", "authors": ["Daniel Hove Paludan", "Julie FredsgÃ¥rd", "Kasper Patrick BÃ¤hrentz", "Ilhan Aslan", "Niels van Berkel"], "categories": ["cs.HC"], "comment": "20 pages, 8 figures", "summary": "Creativity is a valuable human skill that has long been augmented through\nboth analog and digital tools. Recent progress in generative AI, such as image\ngeneration, provides a disruptive technological solution to supporting human\ncreativity further and helping humans generate solutions faster. While AI image\ngenerators can help to rapidly visualize ideas based on user prompts, the use\nof such AI systems has also been critiqued due to their considerable energy\nusage. In this paper, we report on a user study (N = 24) to understand whether\nenergy consumption can be reduced without impeding on the tool's perceived\ncreativity support. Our results highlight that, for example, a main effect of\n(image generation) condition on energy consumption, and index of creativity\nsupport per prompt but not per task, which seem mainly attributed to image\nquantity per prompt. We provide details of our analysis on the relation between\nenergy usage, creativity support, and prompting behavior, including attitudes\ntowards designing with AI and its environmental impact.", "AI": {"tldr": "The paper investigates the balance between energy consumption and perceived creativity support in AI image generators through a user study.", "motivation": "To evaluate if energy consumption in AI image generation can be reduced while maintaining perceived support for human creativity.", "method": "A user study involving 24 participants was conducted to assess the effects of different conditions in image generation on energy consumption and creativity support.", "result": "The study found a significant relationship between energy consumption and the perceived support for creativity, influenced mainly by the number of images generated per prompt.", "conclusion": "The paper concludes that it is possible to reduce energy usage in AI tools for creativity without compromising their effectiveness, provided users adjust their prompting behavior.", "key_contributions": ["Exploration of the relationship between AI energy consumption and creativity support.", "User study findings that inform AI design for reducing environmental impact.", "Recommendations for prompt optimization to achieve better energy efficiency in AI generation."], "limitations": "The study is limited to a specific user group and may not generalize across other demographics or settings.", "keywords": ["generative AI", "creativity", "energy consumption", "user study", "image generation"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476.pdf", "abs": "https://arxiv.org/abs/2506.03476", "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) for diagnosing Alzheimer's Disease from patient-generated text through a novel in-context learning method called Delta-KNN.", "motivation": "There is a need for effective early intervention methods for Alzheimer's Disease, particularly through the analysis of linguistic patterns in patient text.", "method": "The authors introduce Delta-KNN, a new strategy that uses a delta score to improve the selection of training examples and enhances in-context learning performance for AD diagnosis.", "result": "Delta-KNN consistently outperforms conventional in-context learning baselines and achieves state-of-the-art results on AD detection datasets when using the Llama-3.1 model.", "conclusion": "The introduction of Delta-KNN significantly improves the performance of LLMs in diagnosing Alzheimer's Disease from text, suggesting that advanced selection strategies can enhance healthcare applications of AI.", "key_contributions": ["Introduction of Delta-KNN for better demonstration selection", "Empirical validation showing superiority over traditional ICL methods", "Achieving state-of-the-art results in Alzheimer's Disease diagnosis using LLMs"], "limitations": "", "keywords": ["Alzheimer's Disease", "Large Language Models", "In-Context Learning", "Delta-KNN", "Health Informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865.pdf", "abs": "https://arxiv.org/abs/2504.13865", "title": "A Survey on (M)LLM-Based GUI Agents", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.", "AI": {"tldr": "This survey examines the evolution and capabilities of LLM-based GUI Agents in HCI, focusing on their components and evaluation methodologies.", "motivation": "To provide a comprehensive overview of the rapidly advancing field of LLM-based GUI Agents and identify challenges and future directions.", "method": "Systematic analysis of architectural foundations, technical components, evaluation methodologies, and critical examination of current evaluation frameworks for GUI Agents.", "result": "Identifies fundamental components of GUI Agents, reveals advances in automation facilitated by LLMs, and highlights methodological limitations in existing benchmarks.", "conclusion": "The review offers insights into the state of intelligent interface automation and provides directions for future research to enhance GUI Agents' capabilities.", "key_contributions": ["Systematic examination of LLM-based GUI Agents and their components.", "Identification of key technical challenges and future research directions.", "Critical analysis of current evaluation methodologies and standardization proposals."], "limitations": "Methodological limitations in existing benchmarks for evaluation of GUI Agents.", "keywords": ["GUI Agents", "Human-Computer Interaction", "Large Language Models", "Automation", "Evaluation Methodologies"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483.pdf", "abs": "https://arxiv.org/abs/2506.03483", "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "AI": {"tldr": "APT enhances domain-specific performance of LLMs while maintaining general capabilities by using self-generated dis-preferred weakness data for targeted training.", "motivation": "The need to improve domain-specific performance of LLMs without degrading their general capabilities.", "method": "A novel approach named APT focuses on training using only samples where errors occur, paired with similar samples.", "result": "APT shows no reduction in generic capacity and achieves superior performance on downstream tasks on LLama-2 and Mistral-V0.3 models across various benchmarks.", "conclusion": "APT is validated as an effective strategy for balancing domain-specific enhancements and general applicability of LLMs.", "key_contributions": ["Introduction of APT for targeted training of LLMs", "Use of self-generated dis-preferred weakness data", "Demonstration of superior performance without sacrificing general capabilities"], "limitations": "", "keywords": ["Large Language Models", "Domain-specific fine-tuning", "Machine Learning", "Human-Computer Interaction", "Performance enhancement"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.14522", "pdf": "https://arxiv.org/pdf/2504.14522.pdf", "abs": "https://arxiv.org/abs/2504.14522", "title": "Biased by Design: Leveraging AI Inherent Biases to Enhance Critical Thinking of News Readers", "authors": ["Liudmila Zavolokina", "Kilian Sprenkamp", "Zoya Katashinskaya", "Daniel Gordon Jones"], "categories": ["cs.HC", "cs.AI"], "comment": "European Conference on Information Systems (ECIS)", "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.", "AI": {"tldr": "This paper presents a design for a propaganda detection tool utilizing Large Language Models, focusing on user biases to promote critical thinking in news consumption.", "motivation": "To explore how biases in AI models can enhance critical thinking rather than hinder it, particularly in political contexts.", "method": "The research involves a qualitative user study exploring user choices, personalization, and leveraging psychological concepts to design an effective propaganda detection tool.", "result": "The findings suggest that user choice, personalization, and awareness of biases can improve the efficacy of propaganda detection tools, encouraging exploration of diverse viewpoints.", "conclusion": "The study concludes that incorporating psychological strategies can assist users in navigating potential biases in news consumption and enhance the overall design of AI tools.", "key_contributions": ["Design recommendations for AI tools in propaganda detection", "Insights into user choice and personalization based on political stance", "Application of psychological concepts like confirmation bias in tool design"], "limitations": "The qualitative nature of the study may limit generalizability, and findings need further validation in diverse contexts.", "keywords": ["propaganda detection", "Large Language Models", "user personalization", "confirmation bias", "AI tools"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484.pdf", "abs": "https://arxiv.org/abs/2506.03484", "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "AI": {"tldr": "This paper proposes an XAI-guided framework for data augmentation that improves AI model performance by selectively modifying features based on explainability insights.", "motivation": "The paper addresses the challenge of scarcity of labeled data in developing robust AI models, especially for low-resource languages, and the limitations of conventional data augmentation techniques.", "method": "A novel framework called XAI-Guided Context-Aware Data Augmentation is introduced, employing XAI techniques to modify less critical features while preserving task-relevant ones through an iterative feedback loop.", "result": "Experimental results show significant improvements in model accuracy for hate speech and sentiment analysis tasks on the Amharic dataset, outperforming existing augmentation techniques.", "conclusion": "The study demonstrates that the proposed methods provide a more controlled and interpretable solution for data augmentation, significantly enhancing AI model training.", "key_contributions": ["Introduces XAI-guided data augmentation for low-resource languages", "Implements an iterative feedback loop based on explainability", "Demonstrates significant performance improvements in specific NLP tasks"], "limitations": "", "keywords": ["Explainable AI", "Data Augmentation", "Low-resource languages", "NLP", "Model performance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.22769", "pdf": "https://arxiv.org/pdf/2505.22769.pdf", "abs": "https://arxiv.org/abs/2505.22769", "title": "MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking", "authors": ["Yaxiong Lei", "Mingyue Zhao", "Yuheng Wang", "Shijing He", "Yusuke Sugano", "Mohamed Khamis", "Juan Ye"], "categories": ["cs.HC", "cs.CV", "68T10, 68U35", "H.5.2; H.1.2; C.2.4; I.5.4"], "comment": "24 pages, 7 figures", "summary": "Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as\nusers naturally change their postures and device orientations. Traditional\ncalibration approaches, like one-off, fail to adapt to these dynamic\nconditions, leading to degraded performance over time. We present MAC-Gaze, a\nMotion-Aware continual Calibration approach that leverages smartphone Inertial\nmeasurement unit (IMU) sensors and continual learning techniques to\nautomatically detect changes in user motion states and update the gaze tracking\nmodel accordingly. Our system integrates a pre-trained visual gaze estimator\nand an IMU-based activity recognition model with a clustering-based hybrid\ndecision-making mechanism that triggers recalibration when motion patterns\ndeviate significantly from previously encountered states. To enable\naccumulative learning of new motion conditions while mitigating catastrophic\nforgetting, we employ replay-based continual learning, allowing the model to\nmaintain performance across previously encountered motion conditions. We\nevaluate our system through extensive experiments on the publicly available\nRGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+\nimages, 800K+ IMU readings), encompassing a wide range of postures under\nvarious motion conditions including sitting, standing, lying, and walking.\nResults demonstrate that our method reduces gaze estimation error by 19.9% on\nRGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to\n1.92 cm) compared to traditional calibration approaches. Our framework provides\na robust solution for maintaining gaze estimation accuracy in mobile scenarios.", "AI": {"tldr": "MAC-Gaze is a Motion-Aware continual Calibration approach for mobile gaze tracking that leverages IMU sensors and continual learning techniques to maintain accuracy amid user motion changes.", "motivation": "To address the issue of degraded gaze tracking performance due to user posture and device orientation changes, traditional calibration methods are insufficient, prompting the need for a dynamic and adaptive solution.", "method": "The system uses a combination of a pre-trained gaze estimator and an IMU-based activity recognition model, implementing a clustering-based decision mechanism to trigger recalibration as user motion states change, while employing replay-based continual learning to prevent catastrophic forgetting.", "result": "The method reduces gaze estimation error by 19.9% on the RGBDGaze dataset and 31.7% on the MotionGaze dataset compared to traditional methods, demonstrating improved performance across various postures and motion conditions.", "conclusion": "MAC-Gaze provides an effective solution for maintaining gaze estimation accuracy in mobile settings, adapting to users' changing motion states dynamically.", "key_contributions": ["Introduction of a Motion-Aware continual calibration technique for gaze tracking.", "Integration of IMU sensors for real-time motion detection and model recalibration.", "Implementation of replay-based continual learning to mitigate catastrophic forgetting."], "limitations": "The method relies on the quality of the IMU sensors and may require additional work to further generalize across a wider range of activities and environments.", "keywords": ["gaze tracking", "motion awareness", "continual learning", "IMU sensors", "calibration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489.pdf", "abs": "https://arxiv.org/abs/2506.03489", "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "AI": {"tldr": "Introducing EpiCoDe, a novel method that enhances LLM performance in data-scarcity scenarios without the need for additional training data.", "motivation": "To improve model performance in scenarios where annotated data is limited, addressing the challenges posed by high costs of data acquisition.", "method": "The method employs model extrapolation to enhance a finetuned model with its inferior version, and uses contrastive decoding to minimize predicted errors by comparing logit scores.", "result": "EpiCoDe consistently outperforms existing methods across three tasks and four different LLMs, demonstrating significant and robust improvement.", "conclusion": "A theoretical framework is proposed to explain the effectiveness of contrastive decoding in data-scarcity scenarios, as validated by the experiments conducted.", "key_contributions": ["Introduction of the EpiCoDe method for enhancing LLM performance in data-scarcity", "Establishment of a new theoretical framework for understanding contrastive decoding effects", "Empirical validation across multiple tasks and models showing significant improvement."], "limitations": "", "keywords": ["Large Language Models", "Data-scarcity", "Contrastive Decoding", "Model Extrapolation", "Performance Enhancement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490.pdf", "abs": "https://arxiv.org/abs/2506.03490", "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "AI": {"tldr": "This paper presents MedEditBench, a framework for evaluating knowledge editing methods in the medical domain, addressing the limitations of current approaches.", "motivation": "The need for effective knowledge editing methods in Large Language Models (LLMs) specifically for the complex medical domain, where generalization to unseen scenarios is crucial.", "method": "The authors introduce a novel framework called MedEditBench, which includes a medical knowledge editing benchmark and three different editing paradigms to assess knowledge sources.", "result": "The study finds that current knowledge editing methods only achieve superficial memorization of facts and do not generalize well to new scenarios. The proposed SGR-Edit method shows significant improvements by using model-derived rationales for editing.", "conclusion": "The paper provides insights into the challenges of medical knowledge editing, and recommends SGR-Edit as a more effective approach for real-world medical applications.", "key_contributions": ["Introduction of MedEditBench for evaluating KE methods in medicine", "Development of Self-Generated Rationale Editing (SGR-Edit)", "Insights on localization of medical knowledge in LLMs and the effects of sequential editing."], "limitations": "Current KE methods tend to result in superficial memorization without effective generalization to new scenarios.", "keywords": ["knowledge editing", "large language models", "medical informatics", "Machine Learning", "evaluation framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501.pdf", "abs": "https://arxiv.org/abs/2506.03501", "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "AI": {"tldr": "This paper addresses the limitations of binary classification in detecting human involvement in AI-generated text, proposing a multi-task RoBERTa-based regressor and BERTScore for improved measurement.", "motivation": "To improve the detection of human involvement in content creation using generative AI, as current methods lack robustness and overlook the nuances of human-machine collaboration.", "method": "A multi-task RoBERTa-based regressor was trained on a token classification task, using BERTScore as a metric to measure the nuances of human involvement in AI-generated text.", "result": "The proposed method achieved an F1 score of 0.9423 and a mean squared error of 0.004, demonstrating effectiveness in detecting human involvement across generative models.", "conclusion": "The paper presents a more reliable approach to detect varying levels of human involvement in AI-generated texts compared to existing binary classification methods, highlighting its utility in academic scenarios.", "key_contributions": ["Introduced BERTScore for measuring human involvement in AI text generation.", "Developed a multi-task RoBERTa-based regressor for nuanced detection.", "Showed significant performance improvement over traditional binary classification methods."], "limitations": "", "keywords": ["AI-generated text", "human involvement", "BERTScore", "multi-task learning", "RoBERTa"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510.pdf", "abs": "https://arxiv.org/abs/2506.03510", "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "AI": {"tldr": "SPRINT is a novel sublayer pruning method for large language models that improves inference speed without sacrificing accuracy by selectively removing redundant sublayers based on latency and tunability.", "motivation": "To address the slow inference speed of large language models that limits their application despite their high performance.", "method": "SPRINT prunes sublayers iteratively by assessing the latency reduction and tunability of each sublayer to optimize the model's efficiency and effectiveness.", "result": "SPRINT provides a superior accuracy-speedup trade-off, achieving up to 23.88% higher accuracy on zero-shot commonsense reasoning benchmarks compared to existing methods.", "conclusion": "The proposed SPRINT method offers an effective solution to accelerate large language models while maintaining or improving accuracy, making it suitable for real-world applications.", "key_contributions": ["Introduction of SPRINT, a novel sublayer pruning method for LLMs.", "Improved accuracy-speed trade-off compared to existing methods.", "A systematic approach considering latency and tunability for pruning decisions."], "limitations": "", "keywords": ["large language models", "sublayer pruning", "performance optimization", "latency reduction", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519.pdf", "abs": "https://arxiv.org/abs/2506.03519", "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "AI": {"tldr": "This paper presents a novel approach that combines Evolutionary Algorithms (EA) with Deep Reinforcement Learning (DRL) to enhance dialogue systems, addressing the balance between exploration and exploitation.", "motivation": "The paper addresses the challenges faced by Deep Reinforcement Learning in task-oriented dialogue systems, specifically the balance between exploration and exploitation due to high dimensional state and action spaces.", "method": "The proposed method combines the global search capabilities of Evolutionary Algorithms with the local optimization of Deep Reinforcement Learning, and introduces an elite individual injection mechanism to improve the efficiency of the solution search.", "result": "Experiments show significant improvements in the performance of dialogue systems, along with a reduction in exploration time with the elite individual injection mechanism.", "conclusion": "The integration of Evolutionary Algorithms and Deep Reinforcement Learning shows promise for improving task-oriented dialogue systems, effectively optimizing exploration and exploitation.", "key_contributions": ["Novel integration of EA and DRL for dialogue policy optimization", "Introduction of an elite individual injection mechanism to enhance search efficiency", "Demonstrated significant improvements in performance across multiple datasets"], "limitations": "The flexibility of natural language in dialogue tasks complicates the integration of EA and DRL, leading to longer evolutionary times.", "keywords": ["Deep Reinforcement Learning", "Evolutionary Algorithms", "Dialogue Systems", "Exploration-Exploitation", "Task-Oriented Dialogue"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2502.14019", "pdf": "https://arxiv.org/pdf/2502.14019.pdf", "abs": "https://arxiv.org/abs/2502.14019", "title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems", "authors": ["Myra Cheng", "Su Lin Blodgett", "Alicia DeVrio", "Lisa Egede", "Alexandra Olteanu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "ACL 2025", "summary": "As text generation systems' outputs are increasingly anthropomorphic --\nperceived as human-like -- scholars have also increasingly raised concerns\nabout how such outputs can lead to harmful outcomes, such as users over-relying\nor developing emotional dependence on these systems. How to intervene on such\nsystem outputs to mitigate anthropomorphic behaviors and their attendant\nharmful outcomes, however, remains understudied. With this work, we aim to\nprovide empirical and theoretical grounding for developing such interventions.\nTo do so, we compile an inventory of interventions grounded both in prior\nliterature and a crowdsourcing study where participants edited system outputs\nto make them less human-like. Drawing on this inventory, we also develop a\nconceptual framework to help characterize the landscape of possible\ninterventions, articulate distinctions between different types of\ninterventions, and provide a theoretical basis for evaluating the effectiveness\nof different interventions.", "AI": {"tldr": "This paper addresses the issue of anthropomorphic outputs in text generation systems and proposes an inventory of interventions to mitigate their harmful effects.", "motivation": "With the increasing anthropomorphism of text generation systems, there are growing concerns about the adverse effects, such as over-reliance and emotional dependence on these systems.", "method": "The study compiles an inventory of interventions based on literature and a crowdsourcing study where participants modified system outputs to be less human-like. A conceptual framework is developed to categorize and evaluate these interventions.", "result": "An inventory of potential interventions for reducing anthropomorphic behaviors in text generated by AI systems was created, providing a theoretical foundation for assessing their effectiveness.", "conclusion": "The paper outlines the need for and provides a framework for future research on interventions to address the issues arising from anthropomorphic behavior in AI text generation systems.", "key_contributions": ["Compilation of a diverse inventory of interventions to mitigate anthropomorphic effects", "Development of a conceptual framework for characterizing interventions", "Empirical grounding from a crowdsourcing study on editing outputs"], "limitations": "", "keywords": ["text generation", "anthropomorphism", "interventions", "human-computer interaction", "AI ethics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523.pdf", "abs": "https://arxiv.org/abs/2506.03523", "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "AI": {"tldr": "TokAlign is a method for aligning vocabularies in Large Language Models (LLMs) to improve their efficiency and performance in multilingual contexts.", "motivation": "To address inefficiencies in tokenization for LLMs, especially in new domains or languages, and enhance knowledge transfer between models.", "method": "TokAlign learns a one-to-one mapping matrix for token IDs to align source and target vocabularies, rearranges model parameters, and fine-tunes for new vocabularies.", "result": "Improved multilingual text compression rates and vocabulary initialization for LLMs, reducing perplexity significantly after initialization.", "conclusion": "TokAlign enables better token-level distillation and enhances model performance with fewer training steps and tokens.", "key_contributions": ["Introduction of TokAlign method for vocabulary alignment in LLMs", "Demonstrated significant reduction in perplexity for LLMs", "Showed improved effectiveness in token-level distillation compared to sentence-level methods."], "limitations": "", "keywords": ["tokenization", "Large Language Models", "multilingual NLP", "knowledge transfer", "TokAlign"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524.pdf", "abs": "https://arxiv.org/abs/2506.03524", "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "AI": {"tldr": "Seed-Coder is a series of open-source LLMs designed to produce code pretraining data with minimal human involvement, achieving state-of-the-art results in various code-related tasks.", "motivation": "To overcome the limitations of current open-source LLMs that rely heavily on human effort for code pretraining data production, which is costly, subjective, and not scalable.", "method": "Introduces a model-centric data pipeline that uses LLMs for scoring and filtering code data, followed by supervised fine-tuning and preference optimization for the instruct model, and Long-Chain-of-Thought reinforcement learning for the reasoning model.", "result": "Seed-Coder achieves state-of-the-art performance among open-source models of similar size and surpasses larger models in code generation, completion, editing, reasoning, and other software engineering tasks.", "conclusion": "Seed-Coder demonstrates that it is possible to create high-performing code models with reduced human input and improved scalability.", "key_contributions": ["Development of a model-centric data pipeline for code pretraining", "Implementation of Long-Chain-of-Thought reinforcement learning for improved reasoning", "Achievement of state-of-the-art results among open-source LLMs in code tasks."], "limitations": "", "keywords": ["Code Generation", "Code Reasoning", "Language Models", "Human-Computer Interaction", "Software Engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533.pdf", "abs": "https://arxiv.org/abs/2506.03533", "title": "Go-Browse: Training Web Agents with Structured Exploration", "authors": ["Apurva Gandhi", "Graham Neubig"], "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "AI": {"tldr": "Go-Browse is a method for automating diverse web agent data collection through structured exploration, achieving improved performance on the WebArena benchmark.", "motivation": "Digital agents often struggle to understand their environment, hindering their goal achievement, particularly in unfamiliar web contexts.", "method": "Go-Browse frames the data collection process as a graph search to enable structured exploration and reuse of information across different episodes.", "result": "The method successfully collected a dataset consisting of 10K task-solving trajectories and 40K interaction steps, leading to a fine-tuned language model achieving a success rate of 21.7% on the WebArena benchmark.", "conclusion": "Go-Browse surpasses previous methods, showing improved results on web agent performance benchmarks and providing a robust dataset for future research.", "key_contributions": ["Introduction of Go-Browse for structured web exploration", "Collection of a novel dataset with diverse web agent interactions", "Improved performance benchmarks for sub-10B language models"], "limitations": "", "keywords": ["Digital agents", "Web exploration", "Language models", "Dataset collection", "Benchmarking"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541.pdf", "abs": "https://arxiv.org/abs/2506.03541", "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "AI": {"tldr": "The paper introduces a Debate and Reflect (D&R) framework that enhances the performance of smaller language models through multi-turn debates with stronger teacher models, and applies Tree-structured Direct Preference Optimization (T-DPO) for effective training utilizing debate logs.", "motivation": "Large Language Models (LLMs) face adoption challenges due to high computational demands, necessitating the development of more efficient distillation techniques.", "method": "The Debate and Reflect (D&R) framework facilitates structured multi-turn debates between smaller and larger models, along with Tree-structured Direct Preference Optimization (T-DPO) to organize and utilize the debate logs for model training.", "result": "Empirical evaluations show that the D&R framework significantly boosts the accuracy, robustness, and generalization of smaller models, surpassing traditional distillation methods.", "conclusion": "The proposed framework and methods lead to substantial improvements in the performance of smaller models in NLP tasks, making them more viable for practical applications.", "key_contributions": ["Introduction of the Debate and Reflect (D&R) framework for model distillation.", "Development of Tree-structured Direct Preference Optimization (T-DPO) for organizing debate interactions.", "Demonstrated significant performance improvements on various NLP benchmarks."], "limitations": "", "keywords": ["Large Language Models", "model distillation", "debate framework", "NLP benchmarks", "Tree-structured Direct Preference Optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557.pdf", "abs": "https://arxiv.org/abs/2506.03557", "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "AI": {"tldr": "Balanced Preference Optimization (BPO) improves Large Language Models' alignment with human preferences by addressing the issue of Degraded Chosen Responses (DCR) in Direct Preference Optimization (DPO).", "motivation": "To address the limitations of Direct Preference Optimization (DPO) in preserving absolute reward magnitudes and to prevent Degraded Chosen Responses (DCR) that lead to poor model performance.", "method": "Balanced Preference Optimization (BPO) introduces a dynamic balance in the optimization process through balanced reward margin and gap adaptor without adding constraints to the loss function.", "result": "BPO significantly surpasses DPO and its variants in multiple mathematical reasoning tasks, improving model accuracy by rates up to +11.7% and requiring only a minimal code change for implementation.", "conclusion": "BPO effectively resolves the DCR issue inherent in DPO, offering a more robust method for aligning LLMs with human preferences and enhancing performance.", "key_contributions": ["Introduction of Balanced Preference Optimization (BPO) framework", "Dynamic balance of optimization for chosen and rejected responses", "Higher accuracy in LLM performance on reasoning tasks compared to DPO and its variants"], "limitations": "", "keywords": ["Large Language Models", "Direct Preference Optimization", "Balanced Preference Optimization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558.pdf", "abs": "https://arxiv.org/abs/2506.03558", "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "AI": {"tldr": "Skeleton-Guided Multi-Turn Dialogue Generation improves multi-turn instruction synthesis by modeling human conversational intent and generates a dataset named ConsistentChat.", "motivation": "To improve task completion rates in extended conversations by addressing lack of cross-turn coherence in current instruction data synthesis methods.", "method": "The proposed framework operates in two stages: Intent Modeling captures global structure of dialogues based on intent trajectories, and Skeleton Generation creates a sequence of user queries aligned with modeled intent.", "result": "Models fine-tuned on the ConsistentChat dataset show a 20-30% improvement in chat consistency and a 15% increase in task success rate compared to existing datasets.", "conclusion": "The framework provides a more coherent and goal-oriented approach to multi-turn instruction synthesis, significantly enhancing dialogue consistency and task success.", "key_contributions": ["Introduction of Skeleton-Guided Multi-Turn Dialogue Generation framework", "Development of ConsistentChat dataset with 15,000 multi-turn conversations", "Demonstrated significant performance improvements in chat consistency and task success rates."], "limitations": "", "keywords": ["multi-turn dialogue", "instruction synthesis", "conversational intent", "dataset generation", "task success"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566.pdf", "abs": "https://arxiv.org/abs/2506.03566", "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "AI": {"tldr": "Position Specialists improve token generation in LLM inference by using specialized draft layers for different token positions.", "motivation": "Improve the accuracy of token predictions in Large Language Model inference by addressing error accumulation in draft models at later positions.", "method": "The paper proposes Position Specialists (PosS), which consist of multiple position-specialized draft layers that generate tokens at assigned positions, improving prediction quality.", "result": "Experiments show that PosS enhances acceptance rates for token generation and accelerates inference speed across multiple datasets compared to existing methods.", "conclusion": "PosS effectively addresses the challenges of error accumulation in token prediction by utilizing specialized draft layers, leading to significant improvements in performance.", "key_contributions": ["Introduction of Position Specialists for LLM inference", "Demonstrated improvement in token acceptance rates at later positions", "Codebase available for further research and implementation."], "limitations": "", "keywords": ["Large Language Models", "Inference acceleration", "Token prediction", "Position Specialists", "Error accumulation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569.pdf", "abs": "https://arxiv.org/abs/2506.03569", "title": "MiMo-VL Technical Report", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "AI": {"tldr": "Introduction of two open-source vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, demonstrating superior performance in visual understanding and multimodal reasoning.", "motivation": "To advance the field of vision-language models by developing open-source solutions that perform exceptionally well on multimodal tasks and setting new benchmarks for GUI grounding applications.", "method": "The models were trained using a four-stage pre-training approach with 2.4 trillion tokens and integrated Mixed On-policy Reinforcement Learning (MORL) that utilizes diverse reward signals, focusing on high-quality reasoning data.", "result": "MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 out of 40 tasks and achieves a score of 59.4 on OlympiadBench, along with setting a new standard of 56.1 on OSWorld-G for GUI grounding applications.", "conclusion": "The study highlights the effectiveness of incorporating high-quality reasoning data in pre-training and the advantages of mixed RL while providing resources for reproducibility and evaluation.", "key_contributions": ["Open-sourcing MiMo-VL-7B-SFT and MiMo-VL-7B-RL models.", "Establishing new performance benchmarks on various visual and multimodal tasks.", "Providing a comprehensive evaluation suite for over 50 tasks to promote reproducibility."], "limitations": "Challenges in simultaneous multi-domain optimization during training.", "keywords": ["vision-language models", "multimodal reasoning", "GUI grounding", "reinforcement learning", "open-source"], "importance_score": 8, "read_time_minutes": 32}}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570.pdf", "abs": "https://arxiv.org/abs/2506.03570", "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "AI": {"tldr": "FreePRM is a weakly supervised framework that trains Process Reward Models without requiring ground-truth step-level labels, successfully generating pseudo labels and improving performance on various benchmarks.", "motivation": "To address the high costs and challenges of obtaining step-level labels for training Process Reward Models (PRMs).", "method": "FreePRM generates pseudo step-level labels based on final outcome correctness and uses Buffer Probability to reduce noise from pseudo labeling.", "result": "FreePRM achieved an average F1 score of 53.0% on ProcessBench, outperforming fully supervised models and several competitive open-source PRMs.", "conclusion": "FreePRM introduces a new approach to PRM training that minimizes reliance on costly annotations while achieving strong performance.", "key_contributions": ["Introduction of FreePRM, a framework for weakly supervised training", "Achieved significant performance improvements over existing PRMs", "Reduction in reliance on manual annotations for PRM training"], "limitations": "", "keywords": ["Process Reward Models", "weak supervision", "Large Language Models", "pseudo labeling", "health informatics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573.pdf", "abs": "https://arxiv.org/abs/2506.03573", "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "authors": ["Lin Sun", "Can Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "AI": {"tldr": "The paper introduces the Exchange-of-Perspective (EoP) framework to enhance the performance of large language models (LLMs) in natural language processing tasks by facilitating a broader understanding of problems.", "motivation": "LLMs often struggle with problem comprehension, which limits their effectiveness in NLP tasks.", "method": "The Exchange-of-Perspective (EoP) framework is proposed to allow LLMs to exchange perspectives across different definitions of problems, aiming to overcome fixed mindsets related to specific question formulations.", "result": "EoP significantly improves performance in various benchmarks, achieving notable enhancements such as 3.6% improvement on AQuA with GPT-3.5-Turbo and up to 7.7% accuracy increase on Math tasks with GPT-4.", "conclusion": "The EoP framework demonstrates substantial performance gains for LLMs across diverse NLP benchmarks, suggesting a promising direction for improving comprehension and versatility in problem-solving.", "key_contributions": ["Introduction of the EoP framework", "Demonstrated improvement in performance metrics across multiple NLP benchmarks", "Evaluation using state-of-the-art models like GPT-3.5 and GPT-4"], "limitations": "", "keywords": ["large language models", "natural language processing", "exchange of perspective", "framework", "AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576.pdf", "abs": "https://arxiv.org/abs/2506.03576", "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "AI": {"tldr": "KG-BiLM is a new bidirectional language model framework that integrates knowledge graphs with language models for improved semantic understanding and reasoning.", "motivation": "To unify symbolic knowledge graphs with language models for richer semantic understanding, filling the gap between graph structure and textual semantics.", "method": "KG-BiLM introduces three components: Bidirectional Knowledge Attention for full token interaction, Knowledge-Masked Prediction for leveraging both local and global information, and Contrastive Graph Semantic Aggregation for preserving KG structure.", "result": "KG-BiLM outperforms strong baselines in link prediction tasks, particularly in large-scale graphs with multi-hop relations.", "conclusion": "KG-BiLM effectively combines structural information from knowledge graphs with the semantic capabilities of generative transformers, proving its validity through extensive experiments.", "key_contributions": ["Introduction of a unified framework combining KGs and LMs", "Bidirectional Knowledge Attention for enhanced token interaction", "Contrastive Graph Semantic Aggregation for preserving graph structures"], "limitations": "", "keywords": ["Knowledge Representation Learning", "Language Models", "Knowledge Graphs", "Semantic Understanding", "Graph Neural Networks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580.pdf", "abs": "https://arxiv.org/abs/2506.03580", "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "AI": {"tldr": "This study explores using Pre-trained Language Models (PLMs) to generate and evaluate example sentences for L2 Japanese learners, highlighting preferences for retrieval over generative methods.", "motivation": "To provide diverse example sentences that align with learners' proficiency levels for effective language acquisition.", "method": "The study employs PLMs in two ways: as quality scoring components in a retrieval system from a curated corpus of Japanese sentences and as direct sentence generators using zero-shot learning, evaluated by learners, native speakers, and GPT-4.", "result": "Evaluate the quality of sentences based on difficulty, diversity, and naturalness, with mixed preferences for retrieval methods, particularly among beginner and advanced learners.", "conclusion": "The findings indicate that PLMs can enhance sentence suggestion systems, improving the adaptability and effectiveness of language learning tools despite variability in quality ratings.", "key_contributions": ["Demonstrates the role of PLMs in language learning applications.", "Provides insights into learner preferences for sentence types.", "Shows the potential for improved adaptability in educational tools using PLMs."], "limitations": "Inherent disagreement among raters on sentence quality ratings, except for difficulty.", "keywords": ["Pre-trained Language Models", "language acquisition", "example sentences", "Japanese language", "Machine Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592.pdf", "abs": "https://arxiv.org/abs/2506.03592", "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "authors": ["Viktor Hangya", "Fabian KÃ¼ch", "Darina Gold"], "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "AI": {"tldr": "This paper presents a method to reduce the computational burden of evaluating large language models (LLMs) during training by reformulating NLG tasks into cheaper NLU alternatives.", "motivation": "The need for iterative evaluation of LLMs while addressing the high computational costs associated with NLG tasks.", "method": "We reformulate generative tasks into computationally cheaper NLU tasks and evaluate 8 LMs across 4 capabilities to assess the performance correlation between the original and reformulated tasks.", "result": "Our approach shows a strong correlation between NLG and the reformulated NLU tasks, leading to an average reduction of over 35x in evaluation time.", "conclusion": "The findings support the use of cheaper alternatives for monitoring LLM capabilities and we intend to release the benchmark adaptations.", "key_contributions": ["Development of a method to reformulate NLG tasks into NLU tasks", "Validation of performance correlation between NLG and NLU formats", "Achieved significant reduction in evaluation time for LLM capability assessment"], "limitations": "", "keywords": ["large language models", "NLG", "NLU", "capability assessment", "evaluation reduction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593.pdf", "abs": "https://arxiv.org/abs/2506.03593", "title": "Is linguistically-motivated data augmentation worth it?", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "AI": {"tldr": "This paper compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages in machine translation and interlinear glossing tasks.", "motivation": "To empirically compare the effectiveness of data augmentation approaches for low-resource languages, given the lack of previous systematic studies on the topic.", "method": "A comprehensive evaluation of various augmentation strategies applied to two low-resource languages (Uspanteko and Arapaho) across machine translation and interlinear glossing tasks.", "result": "Linguistically-motivated data augmentation strategies can outperform naive methods, but only if the new examples align closely with the distribution of the training data.", "conclusion": "Linguistic expertise can improve data augmentation effectiveness, but it is contingent on the relevance of the generated examples to existing training data.", "key_contributions": ["Systematic empirical comparison of data augmentation strategies for low-resource languages.", "Evaluation of augmentation strategies on machine translation and interlinear glossing tasks.", "Insights on the conditions under which linguistically-motivated strategies are beneficial."], "limitations": "The study is limited to two specific low-resource languages and may not generalize to all languages or tasks.", "keywords": ["data augmentation", "low-resource languages", "machine translation", "linguistic strategies", "sequence-to-sequence"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598.pdf", "abs": "https://arxiv.org/abs/2506.03598", "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "AI": {"tldr": "Auto Prompt SQL (AP-SQL) is a novel architecture that enhances Text-to-SQL translation using resource-efficient models, leveraging prompt engineering and schema linking techniques.", "motivation": "Address the challenges of using resource-intensive models for Text-to-SQL tasks in resource-constrained environments.", "method": "AP-SQL decomposes Text-to-SQL tasks into schema filtering, retrieval-augmented generation via in-context examples, and prompt-driven schema linking and SQL generation, with fine-tuning of large language models for better schema selection.", "result": "Comprehensive evaluations on the Spider benchmarks show that AP-SQL significantly improves accuracy in SQL generation compared to existing methods.", "conclusion": "AP-SQL effectively bridges the gap between small, resource-efficient models and large, closed-source models, enhancing Text-to-SQL translation performance.", "key_contributions": ["Introduction of Auto Prompt SQL (AP-SQL) architecture", "Utilization of prompt engineering techniques such as Chain-of-Thought and Graph-of-Thought", "Demonstrated effectiveness on Spider benchmarks for schema selection and SQL generation"], "limitations": "", "keywords": ["Text-to-SQL", "prompt engineering", "resource-efficient models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616.pdf", "abs": "https://arxiv.org/abs/2506.03616", "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "AI": {"tldr": "The paper introduces Dynamic Inserting Tokens Training (DIT), a method that enhances reasoning in transformer-based LLMs by strategically inserting [PAUSE] tokens at positions of low model confidence, resulting in improved predictive capabilities and accuracy across multiple datasets.", "motivation": "To enhance the reasoning capabilities of transformer-based large language models (LLMs) by improving the learning mechanism through token insertion strategies.", "method": "The method identifies positions within sequences of low model confidence based on token log-likelihood and inserts [PAUSE] tokens at these locations to bolster predictive capabilities for subsequent tokens.", "result": "DIT outperforms traditional fine-tuning and previous token insertion strategies across diverse datasets and models, achieving accuracy gains up to 4.7% on GSM8K and improvements of 3.4% pass@1 on MBPP datasets.", "conclusion": "The study demonstrates that a model-based, dynamic approach to token insertion broadens the scope of reasoning research in large language models.", "key_contributions": ["Introduction of Dynamic Inserting Tokens Training (DIT) method", "Demonstration of significant accuracy gains on multiple datasets", "Shift from heuristic to model-based dynamic approach for token insertion"], "limitations": "", "keywords": ["Dynamic Inserting Tokens", "transformer models", "large language models", "reasoning", "token insertion"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619.pdf", "abs": "https://arxiv.org/abs/2506.03619", "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "AI": {"tldr": "This paper evaluates the cultural awareness of Large Language Models (LLMs) through a benchmark dataset focused on Japanese folktales, particularly Yokai.", "motivation": "To address the cultural limitations of LLMs, particularly in non-English speaking communities, by evaluating their knowledge of folktales as cultural expressions.", "method": "The study introduces YokaiEval, a benchmark dataset with 809 multiple-choice questions about yokai, and evaluates 31 Japanese and multilingual LLMs on it.", "result": "Japanese language models outperformed English-centric models in accuracy, especially those that underwent continued pretraining in Japanese, such as Llama-3.", "conclusion": "The findings indicate that models trained with Japanese resources exhibit better cultural awareness, and the dataset can aid in further research on cultural inclusivity in LLMs.", "key_contributions": ["Introduction of YokaiEval dataset for evaluating LLM cultural awareness", "Comparison of performance between Japanese and multilingual LLMs", "Insights on the importance of language-specific training for cultural understanding"], "limitations": "", "keywords": ["Large Language Models", "Yokai", "Cultural Awareness", "Folktales", "Benchmark Dataset"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627.pdf", "abs": "https://arxiv.org/abs/2506.03627", "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "AI": {"tldr": "The paper introduces Robustness of Prompting (RoP), a novel prompting strategy to enhance the robustness of Large Language Models against input perturbations, showing significant improvements in model performance across various reasoning tasks.", "motivation": "To address the challenge of LLMs being sensitive to input perturbations like typographical errors, which can significantly degrade their performance despite advances in prompting techniques.", "method": "RoP consists of two stages: Error Correction, which generates adversarial examples to automatically correct input errors, and Guidance, which generates optimal prompts based on corrected inputs to enhance model inference accuracy.", "result": "Comprehensive experiments demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations while maintaining model accuracy with minimal degradation compared to clean input scenarios.", "conclusion": "RoP establishes itself as a practical and effective approach to improve LLM robustness in real-world applications, enhancing performance across arithmetic, commonsense, and logical reasoning tasks.", "key_contributions": ["Introduction of Robustness of Prompting (RoP) strategy", "Demonstration of significant robustness improvements for LLMs", "Application of error correction and guidance prompting techniques"], "limitations": "", "keywords": ["Large Language Models", "Robustness", "Prompting Strategies", "Error Correction", "Guidance"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637.pdf", "abs": "https://arxiv.org/abs/2506.03637", "title": "RewardAnything: Generalizable Principle-Following Reward Models", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "AI": {"tldr": "This paper presents RewardAnything, a novel reward model that adheres to natural language specifications, addressing the limitations of traditional reward models in generalizing across diverse principles.", "motivation": "Current reward models are limited by fixed preference datasets and biased rewards, making them inflexible for real-world application. There is a need for reward models that can adapt to various task requirements dynamically without extensive retraining.", "method": "The paper proposes a new approach where reward models are trained to follow natural language specifications of reward principles. A benchmark called RABench is introduced to evaluate the generalization capabilities of reward models across varying principles.", "result": "RewardAnything achieves state-of-the-art performance on traditional reward model benchmarks and demonstrates strong adaptability to novel principles using natural language specifications, without the need for retraining.", "conclusion": "The proposed RewardAnything model offers a promising solution for developing flexible reward models in reinforcement learning, allowing for efficient alignment of large language models to user specifications.", "key_contributions": ["Introduction of RABench for benchmarking reward model generalization.", "Development of RewardAnything, a reward model that adheres to dynamic natural language principles.", "Demonstration of seamless integration of RewardAnything with existing reinforcement learning from human feedback (RLHF) methods."], "limitations": "The effectiveness of RewardAnything is evaluated primarily on benchmark tasks, and its performance in unstructured real-world scenarios remains to be assessed.", "keywords": ["Reward Models", "Natural Language Principles", "Reinforcement Learning"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659.pdf", "abs": "https://arxiv.org/abs/2506.03659", "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "AI": {"tldr": "The paper surveys the trustworthiness of healthcare question-answering systems that utilize large language models, highlighting key dimensions and challenges.", "motivation": "To address the importance of trustworthiness in healthcare QA systems, which affects patient safety and clinical outcomes as LLMs become more integrated into healthcare.", "method": "The survey systematically examines six dimensions of trustworthiness (Factuality, Robustness, Fairness, Safety, Explainability, Calibration) and reviews their evaluation in LLM-based medical QA systems.", "result": "The review identifies existing benchmarks for assessing these dimensions and analyzes techniques for improving model performance, such as retrieval-augmented grounding and adversarial fine-tuning.", "conclusion": "The paper identifies open challenges and proposes directions for future research to enhance the safe and reliable use of LLMs in medical QA.", "key_contributions": ["Framework for assessing trustworthiness dimensions in medical QA systems", "Comparison of major benchmarks for evaluation", "Identification of future research directions to improve LLM-powered medical QA"], "limitations": "", "keywords": ["trustworthiness", "healthcare QA", "large language models", "evaluation metrics", "safety alignment"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665.pdf", "abs": "https://arxiv.org/abs/2506.03665", "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "authors": ["HernÃ¡n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge SÃ¡nchez", "Luciana Benotti"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "AI": {"tldr": "The paper introduces ROSA, a decoding strategy that improves Visual Question Answering (VQA) performance for visually impaired users by addressing challenges in recognizing misaligned text in images.", "motivation": "Visually impaired individuals can benefit from VQA systems, but current models struggle with recognizing text in images due to orientation issues.", "method": "In-depth interviews with visually impaired individuals were conducted to identify common text framing challenges, leading to the development of ROSA, a decoding strategy designed to enhance VQA performance in these scenarios.", "result": "ROSA demonstrates significant performance improvement, outpacing Greedy decoding by 11.7 absolute points in the best-performing model.", "conclusion": "ROSA effectively addresses the shortcomings of existing VQA models in recognizing misaligned text for visually impaired users.", "key_contributions": ["Introduction of ROtated SAmpling (ROSA) for VQA", "Identification of common framing conventions causing misaligned text", "Benchmarks demonstrating ROSA's superior performance over traditional methods"], "limitations": "", "keywords": ["Visually Impaired", "Visual Question Answering", "Text Recognition", "Decoding Strategy", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681.pdf", "abs": "https://arxiv.org/abs/2506.03681", "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "AI": {"tldr": "Proposes a robust data selection pipeline for fine-tuning ASR models, improving performance with limited data.", "motivation": "To address challenges faced by small organizations in fine-tuning ASR models due to limited labeled data and resources.", "method": "The study integrates multiple selection strategies including WER prediction, NER, and CER analysis to filter pseudo-labels from Whisper and Zipformer models.", "result": "Fine-tuning on 7500 hours of pseudo-labeled data achieves 12.3% WER, but with filtering, the dataset is reduced to 100 hours with a similar performance (1.4% WER).", "conclusion": "The proposed filtering strategy significantly reduces the amount of data needed for effective ASR fine-tuning, maintaining high performance.", "key_contributions": ["Development of a robust data selection pipeline for ASR", "Integration of multiple selection strategies for better filtering", "Demonstrated significant reduction in training data while preserving ASR performance"], "limitations": "", "keywords": ["ASR", "data selection", "fine-tuning", "Whisper", "Zipformer"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690.pdf", "abs": "https://arxiv.org/abs/2506.03690", "title": "Robust Preference Optimization via Dynamic Target Margins", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "This paper introduces $\u0003b3$-PO, a dynamic algorithm for optimizing Large Language Models (LLMs) using preference pairs, enhancing alignment by addressing data quality issues.", "motivation": "The need for reliable alignment of LLMs for safety and efficacy in applications drives the development of efficient optimization methods like Direct Preference Optimization (DPO).", "method": "The proposed $\u0003b3$-PO algorithm adjusts reward margins at the pairwise level, prioritizing high-confidence preference pairs and minimizing noise from ambiguous pairs.", "result": "Across various benchmarks, $\u0003b3$-PO achieved an average improvement of 4.4% over existing baselines, presenting a new state-of-the-art performance in LLM alignment.", "conclusion": "$\u0003b3$-PO is a straightforward and robust method for enhancing LLM alignment with minimal changes to existing code and negligible effects on training efficiency.", "key_contributions": ["Introduction of $\u0003b3$-PO algorithm for dynamic optimization of LLMs.", "Significant improvements in performance over existing baseline methods.", "Minimal implementation overhead for broader applicability."], "limitations": "", "keywords": ["Large Language Models", "Direct Preference Optimization", "Algorithm Optimization"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700.pdf", "abs": "https://arxiv.org/abs/2506.03700", "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "AI": {"tldr": "AdaDecode improves long-content generation in large language models by enabling parallel decoding at intermediate layers, significantly increasing throughput while maintaining output consistency.", "motivation": "The paper addresses the inefficiency of sequential autoregressive decoding in LLMs, which limits the ability to utilize parallel processing capabilities of modern hardware.", "method": "AdaDecode accelerates token generation by adaptively predicting tokens at intermediate layers based on confidence levels, allowing parallel computations for deferred predictions.", "result": "AdaDecode achieves decoding throughput improvements of up to 1.73x compared to standard autoregressive decoding while ensuring output parity.", "conclusion": "The proposed method enhances efficiency in LLM decoding and leverages hardware better without needing auxiliary models, contributing to advances in long-content generation.", "key_contributions": ["Introduction of AdaDecode for LLMs", "Achieving up to 1.73x speedup in decoding", "Ensuring output consistency without auxiliary models"], "limitations": "", "keywords": ["Large Language Models", "Decoding Efficiency", "Adaptive Token Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704.pdf", "abs": "https://arxiv.org/abs/2506.03704", "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "AI": {"tldr": "ScoreRAG is a multi-stage framework designed to enhance the quality of automated news generation by integrating retrieval-augmented generation with consistency evaluation and structured summarization.", "motivation": "To address issues in current automated news generation methods such as hallucinations, factual inconsistencies, and lack of domain expertise.", "method": "The framework retrieves relevant news documents from a vector database, maps them to complete news items, evaluates consistency relevance, reranks documents, and generates summaries that guide the large language model in producing quality news articles.", "result": "ScoreRAG significantly improves the accuracy, coherence, informativeness, and professionalism of generated news articles.", "conclusion": "The method shows potential for stable and consistent generation of high-quality news articles, adhering to professional journalistic standards.", "key_contributions": ["Introduction of ScoreRAG framework for automated news generation", "Integration of retrieval-augmented generation and consistency relevance evaluation", "Provision of a demo and code for public access"], "limitations": "", "keywords": ["ScoreRAG", "automated news generation", "retrieval-augmented generation", "natural language processing", "large language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722.pdf", "abs": "https://arxiv.org/abs/2506.03722", "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "AI": {"tldr": "This paper proposes a novel prefix-to-prefix training framework for integrating large pre-trained speech models like Whisper into streaming systems, addressing challenges in streaming recognition.", "motivation": "Integrating large pre-trained speech models into streaming systems effectively while minimizing training costs.", "method": "The authors introduce a prefix-to-prefix training framework, utilizing a Continuous Integrate-and-Fire mechanism for quasi-monotonic alignment and Monotonic Finite Look-ahead Attention for efficient token attention. A wait-k decoding strategy is also employed to simplify the decoding process.", "result": "The proposed framework achieves a controllable balance between latency and quality in streaming applications, as shown through theoretical analysis and experimental validation.", "conclusion": "The novel approaches enhance the performance of Whisper in streaming systems, making it an effective solution for various applications that require real-time transcription or speech recognition.", "key_contributions": ["Novel prefix-to-prefix training framework for streaming recognition", "Continuous Integrate-and-Fire mechanism for quasi-monotonic alignment", "Monotonic Finite Look-ahead Attention for enhanced token attention"], "limitations": "", "keywords": ["speech recognition", "streaming systems", "Whisper model"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723.pdf", "abs": "https://arxiv.org/abs/2506.03723", "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "AI": {"tldr": "This paper explores uncertainty calibration in large language models (LLMs) for chain-of-thought reasoning by focusing on confidence calibration and self-verification behavior through supervised fine-tuning.", "motivation": "Uncertainty calibration is crucial for deploying LLMs that users depend on for reliable confidence estimates, yet it has not been thoroughly investigated in the context of CoT reasoning.", "method": "The study employs supervised fine-tuning with scalar confidence labels, allowing LLMs to generate self-verifying responses and adapting their answers based on confidence levels without explicit reasoning training or reinforcement learning.", "result": "Experiments demonstrate that confidence-aware fine-tuning enhances both calibration and accuracy, particularly in long-form reasoning tasks, while improving interpretability through alignment of reasoning paths with confidence levels.", "conclusion": "The proposed approaches notably improve the self-verification capabilities of LLMs, enhancing both their performance and the trust users can place in their outputs.", "key_contributions": ["Demonstrated effectiveness of fine-tuning LLMs with scalar confidence labels for self-verification in reasoning tasks.", "Introduced a test-time scaling method to improve model performance based on calibrated uncertainty.", "Empirical evidence of enhanced calibration and accuracy in reasoning tasks such as GSM8K, MATH-500, and ARC-Challenge."], "limitations": "The study primarily focuses on confidence calibration without addressing other aspects of model interpretability or external validation of results.", "keywords": ["uncertainty calibration", "language models", "confidence scores", "self-verification", "chain-of-thought reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735.pdf", "abs": "https://arxiv.org/abs/2506.03735", "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "This paper presents Math2Visual, an automated framework for generating educational visuals from math word problems to aid young learners.", "motivation": "There is a demand for automated tools to create visuals that help students interpret math word problems, overcoming the laborious task of manual creation.", "method": "The authors developed Math2Visual, which utilizes a predefined visual language and a dataset of 1,903 visuals to evaluate and enhance Text-to-Image models for generating educational visuals.", "result": "The evaluation shows that fine-tuning TTI models with the Math2Visual dataset leads to improved generation of visuals that align with educational requirements.", "conclusion": "Math2Visual establishes a benchmark for automated educational visual generation and highlights challenges in ensuring accurate representation of mathematical concepts.", "key_contributions": ["Introduction of the Math2Visual framework for automating visual generation from math word problems", "Creation of an annotated dataset of 1,903 visuals for educational purposes", "Evaluation and enhancement of Text-to-Image models using the dataset."], "limitations": "The study may face challenges in accurately representing all mathematical relationships and ensuring essential visual elements are included.", "keywords": ["educational visuals", "math word problems", "Text-to-Image", "automated generation", "pedagogical design"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761.pdf", "abs": "https://arxiv.org/abs/2506.03761", "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "AI": {"tldr": "The paper introduces Pet-Bench, a benchmark for evaluating LLMs in the context of virtual pet companionship, focusing on both self-interaction and human-interaction dimensions.", "motivation": "There is growing interest in using LLMs for creating interactive and emotionally engaging experiences, particularly in virtual pet companionship, an area that is currently underexplored.", "method": "The paper presents the Pet-Bench framework, featuring diverse tasks designed to evaluate LLMs through more realistic pet companionship scenarios, including memory-based dialogues and intelligent scheduling.", "result": "Evaluation of 28 LLMs shows significant performance variations based on model size and capabilities, highlighting the necessity for specialized optimization for better pet-related interactions.", "conclusion": "Pet-Bench provides a foundational resource to benchmark LLM capabilities in pet companionship and aims to advance more immersive human-pet interactions.", "key_contributions": ["Introduction of Pet-Bench as a comprehensive benchmarking tool for LLMs in pet companionship", "Focus on self-evolution and developmental behaviors in pet interactions", "Evaluation of 28 LLMs revealing performance variations that inform future optimizations"], "limitations": "", "keywords": ["Large Language Models", "virtual pets", "benchmarking", "human-computer interaction", "emotionally immersive experiences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762.pdf", "abs": "https://arxiv.org/abs/2506.03762", "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "AI": {"tldr": "This paper introduces Adaptive holistic attention KV (AhaKV), a method to reduce bias in Key-Value (KV) cache used in Large Language Models (LLMs) during inference, leading to better retention of important tokens and improved performance on benchmark tasks.", "motivation": "The deployment of Large Language Models is resource-intensive due to the memory consumption of the KV cache, and existing methods for token eviction rely on a biased attention score.", "method": "AhaKV adapts the scale of the softmax function based on the expectation of information entropy of attention scores rather than solely relying on accumulated attention scores.", "result": "Experiments demonstrate that AhaKV mitigates bias in token retention and achieves state-of-the-art results across several benchmark tasks compared to existing methods.", "conclusion": "The proposed AhaKV method effectively reduces bias and improves the retention of important tokens, allowing LLMs to better utilize global context during inference.", "key_contributions": ["Introduction of AhaKV for adaptive tuning of softmax scale", "Utilization of value vectors to refine adaptive scoring", "Demonstration of state-of-the-art results on benchmark tasks"], "limitations": "", "keywords": ["Large Language Models", "Key-Value cache", "Adaptive holistic attention", "Bias reduction", "Token importance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763.pdf", "abs": "https://arxiv.org/abs/2506.03763", "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "AI": {"tldr": "ClozeMath is a new approach for enhancing large language models' mathematical reasoning through a text-infilling task that predicts masked equations from solutions, showing superior performance compared to existing methods.", "motivation": "To improve large language models' (LLMs) ability to perform mathematical reasoning by proposing a method inspired by human learning processes.", "method": "ClozeMath uses a text-infilling task to predict masked equations from given solutions, akin to cloze exercises, and incorporates tests with Beam Search and Chain-of-Thought decoding algorithms.", "result": "ClozeMath outperforms the baseline method Masked Thought in both performance and robustness across benchmarks like GSM8K and MATH.", "conclusion": "The method offers significant improvements in mathematical reasoning for LLMs and provides insights through an ablation study on architectural implementations.", "key_contributions": ["Introduction of ClozeMath for mathematical reasoning in LLMs", "Demonstration of superior performance compared to Masked Thought", "Analysis of implementation choices affecting performance."], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "text-infilling", "ClozeMath"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781.pdf", "abs": "https://arxiv.org/abs/2506.03781", "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "AI": {"tldr": "Proposes UniQuanF, a quantization method for large language models that combines the strengths of binary-coding quantization and uniform quantization for improved accuracy.", "motivation": "To efficiently deploy large language models while preserving their accuracy through effective quantization methods.", "method": "UniQuanF unifies flexible mapping from uniform quantization with non-uniform quantization levels from binary-coding quantization, incorporating unified initialization and local/periodic mapping techniques for precise optimization.", "result": "UniQuanF outperforms existing quantization methods, achieving up to 4.60% higher accuracy on the GSM8K benchmark.", "conclusion": "The proposed method provides superior accuracy without incurring additional deployment costs related to the unification process.", "key_contributions": ["Introduction of UniQuanF method for LLM quantization", "Combination of flexible mapping and non-uniform quantization", "Demonstrated significant accuracy improvement over existing methods"], "limitations": "", "keywords": ["quantization", "large language models", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785.pdf", "abs": "https://arxiv.org/abs/2506.03785", "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "AI": {"tldr": "Knockout Assessment improves LLM evaluation method using iterative pairwise comparisons.", "motivation": "To enhance LLM evaluation capabilities by allowing continuous comparison for better global ranking.", "method": "Knockout Assessment employs a tournament-style system allowing iterative pairwise comparisons to develop a comprehensive ranking.", "result": "The new method improved scoring accuracy by increasing Pearson correlation with expert evaluations, particularly in university exam scoring and machine translation.", "conclusion": "Knockout Assessment aligns LLM evaluations more closely with human scoring than traditional methods.", "key_contributions": ["Introduces a knockout tournament system for LLM evaluations", "Demonstrates improved correlation with expert evaluations", "Offers a novel approach for developing global rankings in LLM assessments"], "limitations": "", "keywords": ["Large Language Models", "evaluation", "Knockout Assessment", "pairwise comparisons", "machine translation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793.pdf", "abs": "https://arxiv.org/abs/2506.03793", "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "AI": {"tldr": "Cadence is a state-of-the-art model for punctuation restoration in both written and spontaneous spoken text, improving multilingual support and performance.", "motivation": "Current punctuation restoration models struggle with spontaneous speech and disfluencies, which affects downstream NLP tasks.", "method": "Cadence adapts a pretrained large language model for punctuation restoration, focusing on both clean and spontaneous text across multiple languages.", "result": "Cadence outperforms previous models in multilingual punctuation restoration, supporting all 22 Indian languages and English, addressing issues with rare punctuation in domain shifts.", "conclusion": "The study illustrates the effectiveness of pretrained language models for enhancing punctuation restoration in low-resource NLP applications.", "key_contributions": ["Introduces Cadence, a novel model for multilingual punctuation restoration.", "Expands punctuation restoration support from 14 to 22 Indian languages plus English.", "Provides a comprehensive analysis of model behavior across different punctuation types and languages."], "limitations": "Challenges persist under domain shift and with rare punctuation marks.", "keywords": ["punctuation restoration", "NLP", "multilingual models", "spontaneous speech", "Cadence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820.pdf", "abs": "https://arxiv.org/abs/2506.03820", "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "AI": {"tldr": "The paper proposes a method for correcting writing anomalies in Hausa texts using fine-tuned transformer models and a large dataset of noisy-clean sentence pairs.", "motivation": "To address writing anomalies in Hausa texts that hinder NLP applications by automatically correcting them.", "method": "The approach involves fine-tuning transformer-based models on a large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs created from public sources with synthetically generated noise.", "result": "The experiments showed significant improvements in F1, BLEU, and METEOR scores, and reductions in Character Error Rate (CER) and Word Error Rate (WER).", "conclusion": "The research offers a robust methodology, a publicly available dataset, and effective models for improving Hausa text quality, enhancing NLP capabilities for the language.", "key_contributions": ["Large-scale parallel dataset of noisy-clean Hausa sentence pairs", "Fine-tuned multilingual and African language models for anomaly correction", "Demonstrated improvement in NLP metrics for Hausa language"], "limitations": "", "keywords": ["Hausa", "NLP", "transformer models", "text correction", "low-resource languages"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822.pdf", "abs": "https://arxiv.org/abs/2506.03822", "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "authors": ["Fabian Karl", "Ansgar Scherp"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "AI": {"tldr": "CRAWLDoc is a method for contextual ranking of linked web documents to improve metadata extraction from diverse web sources.", "motivation": "To address challenges in metadata extraction due to variations in web layouts and data formats.", "method": "CRAWLDoc retrieves a publication's landing page and linked resources, embedding them into a unified representation for ranking.", "result": "CRAWLDoc provides robust and layout-independent ranking of relevant documents across different publishers and data formats.", "conclusion": "CRAWLDoc lays a foundation for improved metadata extraction, offering a source code and dataset for further research.", "key_contributions": ["Introduction of CRAWLDoc for contextual ranking of web documents.", "Creation of a new labeled dataset of 600 publications for evaluation.", "Demonstration of robust performance independent of layout and format."], "limitations": "", "keywords": ["metadata extraction", "web documents", "contextual ranking"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827.pdf", "abs": "https://arxiv.org/abs/2506.03827", "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "AI": {"tldr": "This paper presents a Multi-objective aligned Bidword Generation Model (MoBGM) to effectively match user queries with relevant advertisements in e-commerce search advertising.", "motivation": "The paper addresses the challenge of non-recall of advertisements due to the diversity of user queries and the limitations of current query rewriting methods.", "method": "The proposed MoBGM uses a discriminator, generator, and preference alignment module to optimize the relevance and authenticity of user queries while maximizing ad revenue.", "result": "Extensive experiments demonstrate that MoBGM significantly outperforms existing methods and enhances commercial value for the platform.", "conclusion": "The algorithm's deployment has proven its feasibility and robustness in improving advertisement recall and user experience.", "key_contributions": ["Introduces MoBGM for multi-objective optimization in query rewriting.", "Implements a discriminator to balance query relevance and authenticity with ad revenue.", "Shows significant improvements over state-of-the-art methods."], "limitations": "", "keywords": ["query rewriting", "advertisement matching", "multi-objective optimization"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832.pdf", "abs": "https://arxiv.org/abs/2506.03832", "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "authors": ["Omer Moussa", "Mariya Toneva"], "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "Brain-tuned speech models better align with human speech processing hierarchies than pretrained models, showing improved semantics and structured processing.", "motivation": "To investigate how brain-tuning enhances speech models' alignment with human speech processing stages and semantics.", "method": "Comparison of brain-tuned models with pretrained models, using layer-wise probing to assess semantic understanding and acoustic feature representation.", "result": "Late layers of brain-tuned models align substantially better with semantic language regions compared to pretrained models; early layers focus on acoustic features.", "conclusion": "Brain-tuned models exhibit a clearer hierarchical structure in speech processing, making them more suitable for understanding human speech.", "key_contributions": ["Improvement in semantic alignment of late model layers through brain-tuning", "Demonstration of hierarchical processing in speech models", "Insights into the distribution of acoustic vs. semantic features in model layers."], "limitations": "", "keywords": ["brain-tuning", "speech models", "semantic understanding", "hierarchical processing", "acoustic features"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861.pdf", "abs": "https://arxiv.org/abs/2506.03861", "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "AI": {"tldr": "This paper introduces PulseReddit, a dataset that combines Reddit discussion data with cryptocurrency market statistics, and evaluates its impact on high-frequency trading using LLM-based Multi-Agent Systems.", "motivation": "To leverage underexplored social media information for improving decision-making in high-frequency cryptocurrency trading.", "method": "An empirical study using Large Language Model-based Multi-Agent Systems to analyze how social sentiment from PulseReddit influences trading performance.", "result": "The experiments show that the Multi-Agent Systems with PulseReddit data outperform traditional trading models, especially in bull markets, and adapt well across various market conditions.", "conclusion": "The integration of social media insights significantly enhances trading strategies in high-frequency trading, offering a framework for future research and practical applications.", "key_contributions": ["Introduction of the PulseReddit dataset for HFT analysis.", "Demonstration of the efficacy of LLM-based Multi-Agent Systems in trading.", "Insights into performance-efficiency trade-offs for LLM model selection."], "limitations": "", "keywords": ["High-Frequency Trading", "PulseReddit", "Multi-Agent Systems", "Social Sentiment", "Cryptocurrency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867.pdf", "abs": "https://arxiv.org/abs/2506.03867", "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "AI": {"tldr": "This paper introduces EuroGEST, a dataset for measuring gender-stereotypical reasoning in multilingual LLMs, revealing strong gender stereotypes encoded in different languages and highlighting the need for broader evaluations of fairness in LLMs.", "motivation": "The motivation is to address the lack of multilingual benchmarks for evaluating gender bias in large language models, as existing benchmarks are predominantly English-focused.", "method": "The authors create the EuroGEST dataset by expanding an expert-informed benchmark using translation tools and quality estimation metrics, followed by human evaluations for accuracy.", "result": "The evaluation of 24 multilingual language models reveals that stereotypical representations of women and men vary across languages but persist, with larger models exhibiting stronger gendered stereotypes.", "conclusion": "The study emphasizes the necessity for more comprehensive multilingual audits of fairness in LLMs and provides scalable methods and resources to facilitate this process.", "key_contributions": ["Introduction of EuroGEST dataset for multilingual gender bias measurement", "Demonstration of gender stereotypes across 30 languages", "Highlighting that larger models encode stereotypes more strongly"], "limitations": "The study may not encompass all language models or fully address all aspects of gender bias.", "keywords": ["multilingual language models", "gender bias", "fairness", "EuroGEST", "language stereotypes"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880.pdf", "abs": "https://arxiv.org/abs/2506.03880", "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "AI": {"tldr": "RadialRouter is a novel framework for optimizing large language model routing, leveraging a lightweight Transformer-based architecture to improve LLM selection based on user queries.", "motivation": "Current routing methods for selecting large language models (LLMs) are ineffective due to limited exploration of the relationship between user queries and LLM characteristics.", "method": "RadialRouter employs a Transformer-based structure, RadialFormer, to articulate the query-LLM relationship, and uses an objective function combining Kullback-Leibler divergence with query-query contrastive loss for optimization.", "result": "RadialRouter significantly outperforms existing methods, showing improvements of 9.2% and 5.8% on RouterBench in Balance and Cost First scenarios, respectively.", "conclusion": "RadialRouter demonstrates strong adaptability to diverse performance-cost trade-offs and suggests practical application potential in LLM routing.", "key_contributions": ["Introduction of RadialRouter framework for LLM routing", "Use of RadialFormer structure for query-LLM relationship", "Enhanced routing performance with innovative objective function"], "limitations": "", "keywords": ["large language models", "routing techniques", "Transformer", "RadialFormer", "cost optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884.pdf", "abs": "https://arxiv.org/abs/2506.03884", "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "AI": {"tldr": "The paper presents a zero-shot text-to-speech (TTS) synthesis approach for under-represented Indian languages using shared phone representations and modified text parsing rules.", "motivation": "To address the challenges of training TTS systems for the 1369 languages of India, many of which lack digital resources.", "method": "The study employs zero-shot synthesis techniques by augmenting a shared phone representation and adjusting text parsing rules based on the phonotactics of the target language.", "result": "Intelligible and natural speech for languages such as Sanskrit, Maharashtrian and Canara Konkani, Maithili, and Kurukh was generated, proving the method's efficacy.", "conclusion": "The approach demonstrates potential for expanding access to speech technology for under-represented languages in India.", "key_contributions": ["Development of zero-shot TTS synthesis for multiple Indian languages", "Utilization of shared phone representation for cross-language synthesis", "Modification of text parsing rules to adapt to target language phonotactics"], "limitations": "", "keywords": ["Text-to-Speech", "Zero-shot synthesis", "Indian languages"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887.pdf", "abs": "https://arxiv.org/abs/2506.03887", "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "AI": {"tldr": "Pre$^3$ is a method that optimizes structured generation in LLMs using deterministic pushdown automata for better efficiency.", "motivation": "To improve output generation efficiency for large language models (LLMs) when handling LR(1) grammars, focusing on reducing runtime execution overhead during context-dependent token processing.", "method": "Pre$^3$ utilizes precomputed prefix-conditioned edges for parallel transition processing and transforms LR(1) transition graphs into deterministic pushdown automata (DPDA) to minimize runtime path exploration overhead.", "result": "Pre$^3$ significantly reduces the time per output token by up to 40% and increases throughput by up to 36% in experimental setups.", "conclusion": "Integrating Pre$^3$ into standard LLM inference frameworks can greatly enhance decoding efficiency without compromising performance.", "key_contributions": ["Introduced a method to precompute edges for efficient parallel processing", "Transformed LR(1) grammars into DPDA to minimize runtime exploration", "Demonstrated substantial improvements in output token generation efficiency"], "limitations": "", "keywords": ["LLM", "structured generation", "LR(1) grammars", "deterministic pushdown automata", "efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901.pdf", "abs": "https://arxiv.org/abs/2506.03901", "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "AI": {"tldr": "The paper introduces Magic Mushroom, a benchmark for evaluating RAG systems under various types of real-world retrieval noise, aiming to improve performance robustness.", "motivation": "To address the sensitivity of RAG systems to retrieval noise that is common in real-world applications and improve their robustness assessment through better benchmarks.", "method": "The authors define four categories of retrieval noise based on linguistic properties and introduce Magic Mushroom, a benchmark consisting of 7,468 single-hop and 3,925 multi-hop question-answer pairs. They evaluate LLM generators and denoising strategies under diverse noise distributions to assess performance dynamics.", "result": "The analysis shows that current LLM generators and denoising strategies are extremely sensitive to noise distributions and have significant room for improvement.", "conclusion": "Magic Mushroom serves as a promising tool for evaluating RAG systems under realistic noise conditions, facilitating better deployment in practical applications.", "key_contributions": ["Definition of four categories of retrieval noise", "Introduction of the Magic Mushroom benchmark for evaluating RAG systems", "Evaluation of LLM generators and denoising strategies under various noise distributions"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Benchmarking", "Noise Robustness", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902.pdf", "abs": "https://arxiv.org/abs/2506.03902", "title": "The Harmonic Structure of Information Contours", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "AI": {"tldr": "This paper explores how fluctuations in information density in language are influenced by periodic rhythms, proposing a framework for analyzing these patterns across multiple languages.", "motivation": "The research aims to provide a deeper understanding of how information is structured in language and the underlying pressures that contribute to this organization.", "method": "The authors employ harmonic regression and a new technique called time scaling to analyze information density patterns in texts from various languages.", "result": "The analysis reveals consistent periodic patterns in information rate across several languages, with dominant frequencies correlating with discourse structure.", "conclusion": "The findings suggest that these periodic oscillations in information rate not only indicate linguistic organization but also provide a framework for studying structural pressures in language.", "key_contributions": ["Introduction of time scaling for harmonic regression in linguistic analysis", "Evidence of periodic patterns in information rates across different languages", "Correlational findings between information oscillation and discourse structure"], "limitations": "", "keywords": ["information density", "periodicity", "linguistic structure", "harmonic regression", "discourse analysis"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913.pdf", "abs": "https://arxiv.org/abs/2506.03913", "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "AI": {"tldr": "This paper evaluates the effectiveness of machine learning techniques in assessing fairness in legal decisions, particularly in refugee adjudication.", "motivation": "To assess the fairness of legal decisions in high-stakes domains like refugee adjudication using machine learning techniques.", "method": "The study empirically evaluates three common ML approachesâfeature-based analysis, semantic clustering, and predictive modelingâon a dataset containing over 59,000 Canadian refugee decisions.", "result": "The results indicate that different ML methods yield inconsistent and sometimes contradictory insights, highlighting that predictive modeling often relies on context rather than legal characteristics, and that semantic clustering fails to reflect substantive legal reasoning.", "conclusion": "The authors argue that statistical fairness evaluation has significant limitations in legal contexts characterized by discretion, and they advocate for integrating legal reasoning with data-driven methods.", "key_contributions": ["Empirical evaluation of common ML approaches in the context of legal fairness.", "Highlighting the inconsistencies and limitations of ML in assessing legal outcomes.", "Arguing for a reevaluation of fairness assessment methods in law, emphasizing the need for legal reasoning."], "limitations": "The study emphasizes the limitations of statistical methods in evaluating fairness, particularly in discretionary legal contexts.", "keywords": ["Legal fairness", "Machine Learning", "Refugee adjudication", "Statistical evaluation", "Legal reasoning"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916.pdf", "abs": "https://arxiv.org/abs/2506.03916", "title": "Compositional Generalisation for Explainable Hate Speech Detection", "authors": ["Agostina Calabrese", "Tom Sherborne", "BjÃ¶rn Ross", "Mirella Lapata"], "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "AI": {"tldr": "This paper addresses the challenges in hate speech detection models, particularly their inability to generalize beyond training data due to biases and inadequate labeling methods.", "motivation": "Current hate speech detection models struggle with generalization due to dataset biases and the limitations of sentence-level labels.", "method": "The authors introduce U-PLEAD, a dataset comprising ~364,000 synthetic posts and a compositional generalisation benchmark with ~8,000 validated posts, and investigate its impact on model training.", "result": "Training on a combination of the U-PLEAD dataset and real data improves compositional generalisation and achieves state-of-the-art performance on the human-sourced PLEAD dataset.", "conclusion": "The findings suggest that more balanced and contextually varied training data can enhance model performance in hate speech detection applications.", "key_contributions": ["Introduction of U-PLEAD dataset with balanced expression occurrences", "Development of a novel compositional generalisation benchmark", "Demonstration of improved performance on PLEAD dataset"], "limitations": "", "keywords": ["hate speech detection", "dataset bias", "generalization", "compositional generalization", "NLP"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922.pdf", "abs": "https://arxiv.org/abs/2506.03922", "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "AI": {"tldr": "HSSBench is a benchmark for evaluating Multimodal Large Language Models (MLLMs) on Humanities and Social Sciences tasks, addressing the unique challenges in interdisciplinary reasoning and visual concept association.", "motivation": "Current benchmarks for MLLMs focus on STEM disciplines and neglect the distinct needs of the Humanities and Social Sciences (HSS), which require interdisciplinary thinking and deep integration of knowledge.", "method": "Introduction of HSSBench, a benchmark designed to assess MLLM capabilities on HSS tasks in multiple languages, supported by a novel data generation pipeline involving domain experts and automated agents.", "result": "Benchmarking over 20 mainstream MLLMs on HSSBench revealed significant challenges for state-of-the-art models while highlighting the necessity for improved cross-disciplinary reasoning capabilities.", "conclusion": "HSSBench aims to inspire further research into the interdisciplinary reasoning abilities of MLLMs, emphasizing the need for models to connect knowledge across different fields.", "key_contributions": ["Development of HSSBench benchmark for multidisciplinary evaluation of MLLMs", "Introduction of a novel data generation pipeline for HSS tasks", "Benchmarking results that demonstrate challenges for existing MLLMs in HSS tasks"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Humanities and Social Sciences", "Benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923.pdf", "abs": "https://arxiv.org/abs/2506.03923", "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "AI": {"tldr": "The paper investigates how semantic cues in comparative math problems influence the reasoning of large language models (LLMs), revealing a framing bias and proposing a benchmark called MathComp to evaluate this effect.", "motivation": "To understand the mechanisms by which semantic cues impact reasoning in LLMs, specifically in the context of comparative math problems that have objective ground truth.", "method": "The authors introduced MathComp, a controlled benchmark consisting of 300 comparison scenarios, each evaluated with 14 prompt variants across three different LLM families to study the impact of linguistic steering.", "result": "The study found that the presence of terms like 'more', 'less', or 'equal' biases model predictions towards those terms. Errors often reflected this bias, and chain-of-thought prompting could reduce but not eliminate the bias, with free-form reasoning being more effective than structured formats.", "conclusion": "The findings suggest the need for framing-aware benchmarks in LLM assessments to improve reasoning robustness and fairness, especially considering that demographic identity terms can amplify biases.", "key_contributions": ["Introduction of the MathComp benchmark for comparative reasoning in LLMs", "Identification of systematic linguistic steering in LLM predictions", "Demonstration of the impact of demographic identity terms on model biases"], "limitations": "The study focuses on a specific context of comparative math problems, which may not generalize to all reasoning tasks.", "keywords": ["Large Language Models", "Framing Bias", "Comparative Reasoning", "Benchmarking", "Demographic Identity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941.pdf", "abs": "https://arxiv.org/abs/2506.03941", "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "AI": {"tldr": "A method for detecting pivotal moments in conversations, particularly in mental health counseling, that can drastically change outcomes.", "motivation": "To assist conversationalists in crucial moments during conversations, particularly in high-stakes situations like mental health crisis counseling.", "method": "An unsupervised computational method that detects pivotal moments in real-time, based on the variation in expected outcomes from potential participant responses.", "result": "The method aligns with human perception of pivotal moments; counselors take longer to respond during these detected moments, which correlates with a significant change in conversational trajectory.", "conclusion": "The framework not only identifies pivotal moments but also explores how counselor responses during these moments relate to the outcomes of counseling sessions.", "key_contributions": ["Introduction of an unsupervised method for real-time detection of pivotal conversational moments.", "Validation of the method through alignment with human perception and conversational outcomes.", "Exploration of counselor response strategies during pivotal moments and their impact on session outcomes."], "limitations": "May require adaptation for different conversational contexts outside mental health counseling.", "keywords": ["pivotal moments", "conversation analysis", "mental health counseling", "unsupervised learning", "real-time detection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949.pdf", "abs": "https://arxiv.org/abs/2506.03949", "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "Introduction of a new benchmark, TableEval, to evaluate LLMs in TableQA tasks including diverse table structures and cross-lingual scenarios.", "motivation": "Existing TableQA benchmarks are limited in scope, focusing on simple tables, suffering from data leakage, and lacking cross-domain and cross-lingual tests.", "method": "TableEval consists of diverse table structures from multiple domains, and introduces SEAT, an evaluation framework assessing model responses at the sub-question level.", "result": "Experimental results on TableEval indicate significant gaps in state-of-the-art LLMs' capabilities in handling complex TableQA tasks.", "conclusion": "The development of TableEval and SEAT provides a necessary framework and dataset for evaluating LLMs on real-world TableQA challenges, encouraging improvements in this area.", "key_contributions": ["Introduction of the TableEval benchmark", "Development of the SEAT evaluation framework", "Inclusion of cross-lingual and multi-domain table structures"], "limitations": "The benchmark might still have limitations in fully replicating every real-world scenario.", "keywords": ["TableQA", "Benchmarks", "LLMs", "Evaluation Framework", "Cross-lingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968.pdf", "abs": "https://arxiv.org/abs/2506.03968", "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "AI": {"tldr": "This paper presents a framework for synthesizing diverse and complex instructions for large language models using attributed grounding.", "motivation": "The need for diverse and meaningful instruction data for automatically aligning large language models (LLMs) is critical, yet existing methods fall short in complexity or grounding.", "method": "The framework involves a top-down attribution process grounding real instructions to specific users, and a bottom-up synthesis process generating situations and meaningful instructions from web documents.", "result": "The construction of a dataset named SynthQuestions with 1 million instructions, which led to improved performance on several benchmarks for models trained on this data, scaling performance with more web corpora.", "conclusion": "The proposed method demonstrates that it is possible to efficiently harvest diverse and complex instructions at scale using the resources available on the web.", "key_contributions": ["Introduction of attributed grounding for synthesizing instructions", "Development of the SynthQuestions dataset with 1 million instructions", "Models trained on SynthQuestions achieve leading performance benchmarks."], "limitations": "", "keywords": ["instruction synthesis", "large language models", "grounding", "web documents", "datasets"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978.pdf", "abs": "https://arxiv.org/abs/2506.03978", "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "AI": {"tldr": "This paper presents SPRINT, a novel contrastive learning framework that improves reasoning performance in transformer-based language models through selective pruning of attention heads.", "motivation": "Investigate the unexpected benefit of model pruning, which traditionally focuses on computational savings, on reasoning capabilities in language models.", "method": "SPRINT dynamically selects optimal attention heads and layers for pruning during inference by aligning question embeddings with head embeddings.", "result": "SPRINT significantly outperforms existing strategies in head selection on reasoning tasks, specifically on the MATH500 and GSM8K datasets.", "conclusion": "Selective pruning through SPRINT not only saves computation but also enhances reasoning performance, challenging traditional views on model pruning.", "key_contributions": ["Introduction of SPRINT, a dynamic pruning method for transformers", "Demonstration of improved reasoning results on benchmark datasets", "Insight into the relationship between head pruning and reasoning performance"], "limitations": "", "keywords": ["model pruning", "transformer models", "contrastive learning", "reasoning performance", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980.pdf", "abs": "https://arxiv.org/abs/2506.03980", "title": "Voice Activity Projection Model with Multimodal Encoders", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "AI": {"tldr": "A multimodal model using pre-trained audio and face encoders improves turn-taking prediction in human-machine interaction.", "motivation": "To enhance turn-taking management in human-machine interactions by utilizing multimodal inputs that capture subtle expressions, overcoming limitations of conventional models.", "method": "The proposed model integrates pre-trained audio and face encoders, leveraging a unified representation of turn-taking behaviors to predict interactions more effectively.", "result": "The multimodal model showed competitive performance, outperforming previous state-of-the-art models on turn-taking metrics in some evaluations.", "conclusion": "The incorporation of audio and facial expression encoders significantly enhances the prediction of turn-taking behaviors in human-machine interactions, thus demonstrating the potential for improved interactive systems.", "key_contributions": ["Introduction of a multimodal approach combining audio and facial encoders for turn-taking prediction.", "Demonstration of improved performance over existing models in certain metrics.", "Availability of source codes and pretrained models for further research."], "limitations": "", "keywords": ["turn-taking", "human-machine interaction", "multimodal", "voice activity projection", "pre-trained encoders"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984.pdf", "abs": "https://arxiv.org/abs/2506.03984", "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "authors": ["Carolin Holtermann", "Paul RÃ¶ttger", "Anne Lauscher"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "AI": {"tldr": "This paper evaluates language models' abilities to reason over time and space using a unique dataset, GeoTemp, revealing strengths in temporal reasoning but limitations in integrating both temporal and spatial knowledge.", "motivation": "To explore the largely unexplored capabilities of language models in reasoning over time and space, as previous research focused on these domains in isolation or in simplistic conditions.", "method": "The study created a dataset called GeoTemp featuring 320k prompts related to 289 cities across 217 countries and 37 time zones, then assessed the performance of eight open chat models from three model families on reasoning tasks combining temporal and geographic knowledge.", "result": "Most language models excelled at tasks involving only temporal knowledge, and performance generally improved with model size. However, they struggled with tasks requiring a combination of temporal and spatial reasoning. Higher performance was noted for location names with lower model perplexity, and prompt formulation significantly affected outcomes.", "conclusion": "While language models perform well on isolated temporal reasoning tasks, their capability to integrate and reason with both time and space is still limited and influenced by specific dataset characteristics and prompting techniques.", "key_contributions": ["Introduction of the GeoTemp dataset for evaluating language models on joint reasoning over time and space", "Demonstration of the impact of prompt formulation on model performance", "Insights into model performance variations based on geographical knowledge"], "limitations": "Performance constraints in connecting temporal and geographical information, and variability of outcomes based on prompt design.", "keywords": ["Language Models", "Temporal Reasoning", "Geospatial Reasoning", "GeoTemp Dataset", "Prompt Engineering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989.pdf", "abs": "https://arxiv.org/abs/2506.03989", "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "AI": {"tldr": "This paper evaluates the effectiveness of multi-stage retrieval-augmented generation (RAG) pipelines compared to simpler, single-stage methods for QA tasks, finding that a straightforward retrieve-then-read method outperforms complex pipelines.", "motivation": "To assess whether multi-stage RAG pipelines provide benefits over simpler, single-stage retrieval methods in light of the capabilities of long-context language models.", "method": "Controlled evaluation of QA tasks comparing two multi-stage pipelines (ReadAgent and RAPTOR) against three baselines including the straightforward DOS RAG method.", "result": "The DOS RAG method consistently matches or outperforms more complex multi-stage methods across multiple long-context QA benchmarks.", "conclusion": "The study recommends adopting DOS RAG as a strong baseline for future RAG evaluations as it balances simplicity and effectiveness.", "key_contributions": ["Establishment of DOS RAG as a new baseline for RAG evaluations", "Comparison of multi-stage and single-stage methods in long-context QA tasks", "Insights on the trade-offs between complexity and effectiveness in retrieval-based models."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Long-context Language Models", "Question Answering", "Baseline Evaluation", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990.pdf", "abs": "https://arxiv.org/abs/2506.03990", "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "AI": {"tldr": "DynTok is a dynamic token compression strategy for video modeling that reduces visual token size significantly while maintaining performance.", "motivation": "To address the computational overhead caused by the massive number of visual tokens in long videos, DynTok seeks to optimize video token representation.", "method": "DynTok adaptively splits visual tokens into groups and merges them within each group to achieve compression, particularly in low information density regions.", "result": "DynTok reduces the number of tokens to 44.4% of the original size while maintaining performance, achieving 65.3% on Video-MME and 72.5% on MLVU.", "conclusion": "The method exposes redundancy in video token representations and provides insights for more efficient video modeling techniques.", "key_contributions": ["Introduction of DynTok for dynamic visual token compression", "Achieves significant reduction in token size with retained performance", "Insights for designing more efficient video modeling techniques"], "limitations": "", "keywords": ["video modeling", "token compression", "dynamic tokenization", "LLM", "visual information"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993.pdf", "abs": "https://arxiv.org/abs/2506.03993", "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "authors": ["Saif M. Mohammad"], "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "AI": {"tldr": "The paper introduces Words of Warmth, a repository of word associations related to warmth, trust, and sociability, demonstrating their reliability and applications in understanding development and biases.", "motivation": "To provide a comprehensive resource that explores the dimensions of Warmth and Competence, particularly focusing on the components of warmth, and to facilitate research on biases and stereotypes.", "method": "Development of a large-scale repository of word associations for over 26k English words related to warmth, trust, and sociability, along with reliability assessments and case studies.", "result": "The repository shows high reliability in word associations and illustrates how children acquire WCTS words as they age, alongside applications in bias and stereotype research.", "conclusion": "Words of Warmth serves as a valuable tool for social psychology researchers, offering insights into the development of social dimensions and a basis for further research on biases.", "key_contributions": ["Creation of the first large-scale repository of word associations for warmth, trust, and sociability.", "Assessment of the developmental acquisition of warmth-related words in children.", "Facilitation of bias and stereotype research with real-world applications."], "limitations": "", "keywords": ["Warmth", "Competence", "Sociability", "Trust", "Bias research"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994.pdf", "abs": "https://arxiv.org/abs/2506.03994", "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "AI": {"tldr": "This paper investigates the ability of large-scale models to represent the semantic features of concrete object concepts by evaluating different types of encoders on a set of probing tasks.", "motivation": "To explore how well large-scale trained models capture semantic feature norms based on sensorimotor experiences compared to traditional methods.", "method": "The authors conducted probing tasks to assess the performance of image encoders (trained on image data, multimodally, and language-only) in predicting attribute ratings from datasets like the McRae norms and Binder dataset.", "result": "Multimodal image encoders slightly outperformed language-only models, while image-only encoders showed comparable performance to language models, even on attributes not directly visual in nature.", "conclusion": "The findings suggest valuable insights into the effectiveness of unimodal learning and highlight the complementarity between different modalities in model training.", "key_contributions": ["Demonstrated the capabilities of unimodal image encoders in representing semantic attributes.", "Examined the performance of multimodal vs. language-only models in detail.", "Provided new insights into the learning dynamics of large-scale models."], "limitations": "", "keywords": ["semantic feature norms", "large-scale models", "image encoders"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020.pdf", "abs": "https://arxiv.org/abs/2506.04020", "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "AI": {"tldr": "This paper presents QQSUM, a novel task for summarizing diverse customer opinions in Product Question Answering (PQA) using a model called QQSUM-RAG, which improves upon Retrieval-Augmented Generation by capturing the diversity of user reviews.", "motivation": "Existing PQA systems fail to capture diverse perspectives in customer reviews, limiting their effectiveness in addressing queries.", "method": "The authors introduce QQSUM, a quantitative query-focused summarization task, and develop QQSUM-RAG, which combines few-shot learning with a Key Point (KP)-oriented retriever and summary generator to create comprehensive summaries from diverse opinions.", "result": "Experimental results show that QQSUM-RAG outperforms state-of-the-art RAG models in both the quality of textual output and the accuracy of opinion quantification.", "conclusion": "QQSUM-RAG effectively addresses the key limitations of existing PQA systems by providing richer, multi-perspective summaries that are more representative of customer sentiments.", "key_contributions": ["Introduction of the QQSUM task for summarizing customer opinions", "Development of QQSUM-RAG model that improves retrieval and summarization of diverse viewpoints", "Demonstration of superior performance in opinion diversity quantification compared to existing models."], "limitations": "", "keywords": ["Product Question Answering", "customer reviews", "quantitative summarization", "few-shot learning", "retrieval-augmented generation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032.pdf", "abs": "https://arxiv.org/abs/2506.04032", "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "AI": {"tldr": "Development of a Patient Simulator using real EHR data for training AI health agents.", "motivation": "To create realistic test subjects for healthcare agentic models based on actual patient encounters.", "method": "Constructed clinical scenarios from real-world EHR data and evaluated the simulator's performance with expert clinicians.", "result": "Clinicians found the Patient Simulator consistent with patient vignettes in 97.7% of cases, and case summaries were 99% relevant.", "conclusion": "The methodology allows for large-scale training and testing of conversational AI agents using realistic patient data.", "key_contributions": ["Patient Simulator utilizing real EHR data", "Evaluation framework aligning simulated encounters with clinical expertise", "High consistency and relevance in simulated patient encounters"], "limitations": "", "keywords": ["Patient Simulator", "EHR data", "Conversational AI", "Healthcare", "Simulation"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037.pdf", "abs": "https://arxiv.org/abs/2506.04037", "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "AI": {"tldr": "This paper analyzes the mutual exclusivity (ME) bias in bilingual visually grounded speech models compared to monolingual models, finding a generally weaker ME bias in the former with interesting implications for language learning.", "motivation": "To explore mutual exclusivity in bilingual language learning contexts and to understand its computational implications on visually grounded speech models.", "method": "Bilingual visually grounded speech models were trained on combinations of English, French, and Dutch, and their ME bias was compared to that of monolingual models.", "result": "Bilingual models generally exhibited a weaker ME bias than monolingual ones, with exceptions. The visual embeddings of bilingual models showed smaller variance for familiar data, leading to increased confusion between novel and familiar concepts.", "conclusion": "The findings provide insights into how bilingualism affects mutual exclusivity and suggest reasons for the observed biases in VGS models.", "key_contributions": ["Analysis of ME bias in bilingual vs. monolingual models", "Insights into visual embeddings and variance correlations", "Provision of code and data for further research."], "limitations": "The study focuses on a limited set of languages and may not generalize across all bilingual contexts.", "keywords": ["mutual exclusivity", "bilingual models", "visually grounded speech"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041.pdf", "abs": "https://arxiv.org/abs/2506.04041", "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "AI": {"tldr": "LexTime is a dataset for assessing LLMs' event ordering in legal texts, revealing accuracy variations and challenges in legal linguistic complexities.", "motivation": "To address the lack of expert language evaluation in existing datasets for legal temporal reasoning and understand how LLMs manage event ordering in legal contexts.", "method": "Introduction of LexTime, a novel dataset comprising 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations; analysis of model performance based on context length and event characteristics.", "result": "LLMs demonstrated greater accuracy in legal event ordering compared to narrative contexts, with the highest accuracy reaching 80.8% for implicit-explicit event pairs; longer contexts and implicit events improved outcomes, but legal complexities posed challenges.", "conclusion": "Highlighting specific modeling strategies to improve LLMs' capability in temporal event reasoning within legal texts is critical.", "key_contributions": ["Introduction of the LexTime dataset for evaluating LLMs in legal text event ordering.", "Demonstration of improved accuracy of LLMs on legal texts vs. narrative texts.", "Identification of challenges posed by legal linguistic complexities and nested clauses."], "limitations": "The challenges of legal linguistic complexities and nested clauses remain unaddressed in terms of model adaptation.", "keywords": ["legal texts", "temporal reasoning", "event ordering", "LLMs", "LexTime"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042.pdf", "abs": "https://arxiv.org/abs/2506.04042", "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "AI": {"tldr": "This paper presents a novel two-stage optimization process for controllable knowledge editing in large language models that prevents shortcut learning and enhances prediction performance.", "motivation": "The need for controllable knowledge editing in large language models to avoid unintended modifications to unrelated knowledge.", "method": "A two-stage optimization process that balances the learning of subject and relation features during the editing of knowledge predictions.", "result": "The proposed method effectively prevents shortcut learning and improves overall performance in knowledge editing tasks.", "conclusion": "A better approach to knowledge editing is provided by addressing shortcut learning issues, allowing for more precise modifications without unwanted side effects.", "key_contributions": ["Introduces a two-stage optimization process for knowledge editing.", "Identifies crucial features (subject and relation features) for model learning.", "Demonstrates the effectiveness of the approach in preventing shortcut learning."], "limitations": "", "keywords": ["Knowledge Editing", "Large Language Models", "Optimization Process", "Controllable Knowledge Modification", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043.pdf", "abs": "https://arxiv.org/abs/2506.04043", "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "The paper evaluates the effectiveness of LLM-generated counter-narratives for online hate speech, highlighting issues of tone, accessibility, and ethical risks.", "motivation": "To address the challenges associated with automated counter-narratives in mitigating hate speech online, particularly focusing on their tone, accessibility, and ethical implications.", "method": "A framework was proposed for evaluating LLM-generated counter-narratives across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Various models (GPT-4o-Mini, CommandR-7B, LLaMA 3.1-70B) were assessed using datasets MT-Conan and HatEval.", "result": "The evaluation showed that LLM-generated counter-narratives tend to be verbose and geared towards individuals with college-level literacy, thus limiting their accessibility. Emotionally guided prompts resulted in more empathetic and readable outputs, but safety and effectiveness concerns persist.", "conclusion": "The study concludes that while LLM capabilities can enhance counter-narratives, significant issues regarding accessibility and ethical robustness need to be addressed before they can be reliably used.", "key_contributions": ["Proposed an evaluation framework for LLM-generated counter-narratives.", "Demonstrated the impact of affective tone on the effectiveness of counter-narratives.", "Identified limitations in accessibility of LLM-generated content."], "limitations": "Concerns remain about the safety and effectiveness of LLM-generated counter-narratives despite potential improvements in empathetic and readable responses.", "keywords": ["counter-narratives", "hate speech", "large language models", "accessibility", "ethical risks"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044.pdf", "abs": "https://arxiv.org/abs/2506.04044", "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "AI": {"tldr": "LIBU proposes a novel algorithm for unlearning specific data from large language models without complete retraining.", "motivation": "Address the challenge of unlearning sensitive information from language models while maintaining their utility.", "method": "The algorithm leverages influence functions to eliminate the impact of specific data points and employs second-order optimization techniques to stabilize the model's performance.", "result": "Experiments demonstrate the effectiveness of LIBU in various tasks related to unlearning in large language models.", "conclusion": "LIBU presents a lightweight and effective method for removing specific knowledge from LLMs, offering a practical solution without full retraining.", "key_contributions": ["Introduction of LIBU algorithm for unlearning", "Use of classical influence functions in LLM context", "Combination of influence functions with second-order optimization for improved stability"], "limitations": "", "keywords": ["unlearning", "large language models", "influence functions", "second-order optimization", "sensitive content"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047.pdf", "abs": "https://arxiv.org/abs/2506.04047", "title": "On Support Samples of Next Word Prediction", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "AI": {"tldr": "Investigates data-centric interpretability in language models, focusing on support samples in next-word prediction.", "motivation": "To understand the rationale behind language model decisions, particularly in next-word prediction tasks.", "method": "Utilizes representer theorem to identify support samples that promote or deter predictions.", "result": "Support samples are intrinsic and predictable before training; non-support samples prevent overfitting and shape representation learning, especially in deeper layers.", "conclusion": "Insights on data's role in model decisions enhance the understanding of language model interpretability.", "key_contributions": ["Introduces the concept of data-centric interpretability for language models.", "Identifies intrinsic properties of support samples prior to training.", "Highlights the critical role of non-support samples in generalization and representation learning."], "limitations": "", "keywords": ["data-centric interpretability", "language models", "next-word prediction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050.pdf", "abs": "https://arxiv.org/abs/2506.04050", "title": "Explainability-Based Token Replacement on LLM-Generated Text", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "AI": {"tldr": "This paper investigates using explainable AI methods to reduce the detectability of AI-generated text while proposing a robust ensemble-based detection approach.", "motivation": "To address the distinguishability of AI-generated text from human-written text, which is a growing concern with the advancement of generative models like LLMs.", "method": "An ensemble classifier was trained to distinguish between AI-generated text and human-written text. SHAP and LIME were used to identify influential tokens, followed by the implementation of four token replacement strategies to reduce detectability.", "result": "The token replacement strategies significantly decreased the ability of a single classifier to detect AI-generated text, while the ensemble classifier retained strong performance across multiple languages and domains.", "conclusion": "Explainable AI methods can effectively obscure AI-generated text detection, but maintaining an ensemble-based detection strategy is crucial for adapting to evolving manipulative techniques.", "key_contributions": ["Introduction of token replacement strategies based on XAI methods to reduce AIGT detectability.", "Development of a robust ensemble classifier that performs well across different languages and domains.", "Demonstration of the impact of influential token manipulations on detection strategies."], "limitations": "The effectiveness of the proposed strategies may vary with different classifiers and the evolving nature of AI-generated content.", "keywords": ["explainable AI", "large language models", "AI-generated text detection", "ensemble classifier", "token replacement strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051.pdf", "abs": "https://arxiv.org/abs/2506.04051", "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "AI": {"tldr": "This paper proposes HALT, a method for post-training Large Language Models (LLMs) to generate content confidently, reducing hallucinations by encoding what the model can and cannot reliably generate.", "motivation": "To address the issue of hallucinations in LLMs, which produce incorrect answers when lacking knowledge or capability.", "method": "HALT generates capability-aligned post-training data by splitting responses of pretrained LLMs into factual fragments and identifying incorrect fragments using ground truth information.", "result": "Using HALT for finetuning, the mean correctness of response fragments increased by 15% on average, and a single Llama3-70B model achieved a correctness improvement from 51% to 87%.", "conclusion": "HALT effectively balances the trade-off between response completeness and correctness in LLM responses, leading to significant improvements in reliability.", "key_contributions": ["Development of HALT for capability-aligned post-training of LLMs", "Demonstration of significant correctness improvements while maintaining response completeness", "Evaluation across multiple domains: biography writing, mathematics, coding, and medicine."], "limitations": "", "keywords": ["Large Language Models", "hallucination", "post-training", "capability-aligned", "finetuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065.pdf", "abs": "https://arxiv.org/abs/2506.04065", "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "AI": {"tldr": "This paper introduces Customized Curriculum Learning (CCL), a framework that improves post-training efficiency and model performance by customizing difficulty metrics and using guided prompting for training with challenging samples.", "motivation": "To overcome the limitations of inefficient sample utilization and inflexible difficulty processing in post-training of Large Language Models (LLMs).", "method": "The proposed CCL framework includes model-adaptive difficulty definitions to customize datasets and 'Guided Prompting' to dynamically adjust sample difficulty through hints.", "result": "CCL shows significant performance improvements over uniform training methods in mathematical reasoning tasks, demonstrating better sample utilization and enhanced model performance across multiple benchmarks.", "conclusion": "The CCL framework is effective in improving the performance of LLMs in reasoning tasks by customizing sample difficulty and using guidance for tougher samples.", "key_contributions": ["Model-adaptive difficulty definition for curriculum datasets.", "Guided Prompting for dynamic sample difficulty adjustment.", "Demonstrated effectiveness across five mathematical reasoning benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Curriculum Learning", "Guided Prompting"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.04070", "pdf": "https://arxiv.org/pdf/2506.04070.pdf", "abs": "https://arxiv.org/abs/2506.04070", "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.", "AI": {"tldr": "This study introduces LaF-GRPO, an approach for generating navigation instructions for visually impaired individuals using a Vision-Language Model and a new open-sourced benchmark called NIG4VI.", "motivation": "The generation of precise navigation instructions for visually impaired individuals is critical for enhancing their mobility, yet it remains underexplored.", "method": "The research proposes LaF-GRPO, which uses an LLM to simulate responses from visually impaired users to produce rewards that guide the post-training of a Vision-Language Model.", "result": "Experiments on the NIG4VI benchmark demonstrate that LaF-GRPO significantly improves instruction quality, increasing BLEU and METEOR scores compared to existing models.", "conclusion": "The proposed method enhances the usability of navigation instructions for visually impaired users while minimizing the dependency on costly real-world data.", "key_contributions": ["Introduction of LaF-GRPO for generating navigation instructions for visually impaired users", "Development of NIG4VI, a comprehensive benchmark for instruction generation in diverse navigation scenarios", "Demonstrated superior performance in instruction quality compared to existing models"], "limitations": "", "keywords": ["Navigation instruction generation", "Visually impaired", "Vision-Language Model", "Benchmark", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04072", "pdf": "https://arxiv.org/pdf/2506.04072.pdf", "abs": "https://arxiv.org/abs/2506.04072", "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.HC", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "AI": {"tldr": "This paper explores how controllable generation techniques can modify LLM outputs to better support beginner language learners, resulting in significantly improved comprehensibility.", "motivation": "Traditional LLMs generate complex text, making them unsuitable for beginner language learners. This research seeks to make LLMs more accessible for absolute beginners through controllable generation methods.", "method": "The study employs modular controllable generation techniques without fine-tuning the LLM. It evaluates these techniques using automatic metrics and a user study with university students learning Japanese.", "result": "Using future discriminators, the study demonstrated a significant improvement in output comprehensibility, increasing from 40.4% to 84.3%. A novel evaluation metric, Token Miss Rate (TMR), was introduced to measure the proportion of incomprehensible tokens and showed a strong correlation with human judgments.", "conclusion": "The research shows that controllable generation can effectively aid beginner language learners using LLMs, and tools and datasets are provided for further research.", "key_contributions": ["Introduction of future discriminators for output control", "Development of the Token Miss Rate (TMR) evaluation metric", "Release of resources for AI-assisted language learning research"], "limitations": "", "keywords": ["Large Language Models", "Language Learning", "Controllable Generation", "Comprehensibility", "Token Miss Rate"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04076", "pdf": "https://arxiv.org/pdf/2506.04076.pdf", "abs": "https://arxiv.org/abs/2506.04076", "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.", "AI": {"tldr": "This paper fine-tunes Whisper models for automatic speaking assessment, focusing on accurate transcription of disfluencies using different annotation schemes to improve ASR accuracy.", "motivation": "The accurate capture of disfluencies is crucial for automatic speaking assessment, especially for tasks like error analysis and providing effective feedback.", "method": "Fine-tuning of Whisper models on the Speak & Improve 2025 corpus employing low-rank adaptation, exploring three annotation schemes regarding hesitations.", "result": "The challenge system achieved 6.47% WER with the Pure scheme and 5.81% with the Extra scheme, with an 11.3% relative improvement observed when using realistic filled-pause labeling.", "conclusion": "Explicit and realistic labeling of filled pauses enhances ASR accuracy significantly for verbatim L2 speech transcription.", "key_contributions": ["Fine-tuning Whisper models with low-rank adaptation", "Comparison of annotation schemes for disfluencies", "Significant improvements in ASR accuracy for verbatim speech transcription"], "limitations": "", "keywords": ["automatic speech recognition", "disfluencies", "fine-tuning", "speech assessment", "filled pauses"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.04077", "pdf": "https://arxiv.org/pdf/2506.04077.pdf", "abs": "https://arxiv.org/abs/2506.04077", "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.", "AI": {"tldr": "A novel training paradigm for automated speaking assessment leverages large language models to generate diverse responses and uses multimodal approaches to predict proficiency scores.", "motivation": "Address the scarcity of labeled recordings in automated speaking assessment, which limits prompt diversity and scoring reliability.", "method": "Leverage large language models to generate diverse responses, convert these into synthesized speech using speaker-aware text-to-speech synthesis, and apply a dynamic importance loss to adaptively reweight training instances based on feature distribution differences.", "result": "Experiments on the LTTC dataset show improved performance over methods using real data or conventional augmentation, effectively addressing low-resource constraints.", "conclusion": "The proposed approach facilitates automated speaking assessment on opinion expressions by integrating cross-modal information and generating diverse training data.", "key_contributions": ["Introduction of a novel training paradigm using LLMs for response generation.", "Development of a dynamic importance loss to handle feature distribution differences.", "Integration of multimodal features for accurate proficiency score prediction."], "limitations": "", "keywords": ["Automated Speaking Assessment", "Large Language Models", "Text-to-Speech Synthesis"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078.pdf", "abs": "https://arxiv.org/abs/2506.04078", "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "AI": {"tldr": "LLMEval-Med is a new benchmark for evaluating large language models in medicine, addressing limitations of current medical benchmarks with real-world data and an automated evaluation pipeline.", "motivation": "To improve the evaluation of LLMs in medical applications, which require high accuracy and complex reasoning assessment.", "method": "Introduction of LLMEval-Med benchmark with 2,996 questions from real electronic health records and expert scenarios, along with an automated evaluation pipeline using an LLM-as-Judge framework.", "result": "Evaluation of 13 LLMs revealed insights into the performance of specialized medical models versus open-source and closed-source models, highlighting the benchmark's effectiveness.", "conclusion": "LLMEval-Med provides a reliable framework for assessing LLMs in the medical field and is crucial for their safe deployment in clinical settings.", "key_contributions": ["Development of LLMEval-Med benchmark covering five core medical areas", "Integration of real-world electronic health record data into evaluation", "Creation of an automated evaluation pipeline with dynamic expert feedback"], "limitations": "", "keywords": ["large language models", "medical benchmarks", "automated evaluation", "healthcare AI", "clinical scenarios"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.04079", "pdf": "https://arxiv.org/pdf/2506.04079.pdf", "abs": "https://arxiv.org/abs/2506.04079", "title": "EuroLLM-9B: Technical Report", "authors": ["Pedro Henrique Martins", "JoÃ£o Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "JosÃ© Pombal", "Manuel Faysse", "Pierre Colombo", "FranÃ§ois Yvon", "Barry Haddow", "JosÃ© G. C. de Souza", "Alexandra Birch", "AndrÃ© F. T. Martins"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "56 pages", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "AI": {"tldr": "EuroLLM-9B is a large language model developed to support multilingual needs across Europe, trained to cover 24 EU languages and 11 additional languages.", "motivation": "To address the underrepresentation of European languages in existing open large language models.", "method": "The paper details the development of EuroLLM-9B, which includes tokenizer design, architecture specifications, and data collection methods. It introduces EuroFilter for multilingual data filtering and EuroBlocks-Synthetic for enhanced post-training language coverage.", "result": "Evaluation shows EuroLLM-9B performs competitively on multilingual benchmarks and machine translation tasks, establishing it as a leading open European-made LLM.", "conclusion": "EuroLLM-9B's release includes all major components to promote open research and usage in the field.", "key_contributions": ["Introduction of EuroLLM-9B model covering multiple European languages", "Development of EuroFilter for multilingual data filtering", "Creation of EuroBlocks-Synthetic dataset for enhanced language coverage"], "limitations": "", "keywords": ["EuroLLM-9B", "large language model", "multilingual support", "EuroFilter", "synthetic dataset"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2506.04098", "pdf": "https://arxiv.org/pdf/2506.04098.pdf", "abs": "https://arxiv.org/abs/2506.04098", "title": "TextAtari: 100K Frames Game Playing with Language Agents", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "51 pages, 39 figures", "summary": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.", "AI": {"tldr": "TextAtari is a benchmark for evaluating language agents on long-horizon decision-making tasks using textual descriptions of Atari games, highlighting performance gaps and challenges in sequential reasoning.", "motivation": "To create a benchmark that bridges sequential decision-making with natural language processing, specifically for tasks spanning up to 100,000 steps in classic Atari games.", "method": "An unsupervised representation learning framework (AtariARI) is used to translate visual states into textual descriptions, creating nearly 100 distinct tasks of varying complexity for evaluation of three open-source large language models across different agent frameworks.", "result": "Significant performance gaps are revealed between language agents and human players in tasks requiring extensive planning, specifically in terms of sequential reasoning, state tracking, and strategic planning.", "conclusion": "TextAtari offers standardized evaluation protocols and baseline implementations to advance research at the intersection of language models and long-term decision-making.", "key_contributions": ["Introduction of a comprehensive benchmark for long-horizon decision-making in language agents.", "Evaluation of the impact of different reasoning frameworks and prior knowledge on performance across various tasks.", "Standardized protocols for future research in language models and planning."], "limitations": "The results indicate performance gaps, suggesting that the models still struggle significantly with human-level strategic planning and reasoning tasks.", "keywords": ["Language models", "Decision-making", "Natural language processing", "Atari games", "Benchmark"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2506.04108", "pdf": "https://arxiv.org/pdf/2506.04108.pdf", "abs": "https://arxiv.org/abs/2506.04108", "title": "Rectified Sparse Attention", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.", "AI": {"tldr": "ReSA offers an efficient method for long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification to improve generation quality.", "motivation": "The challenge of efficient long-sequence generation in Large Language Models due to KV cache misalignment and its impact on generation quality.", "method": "Proposes Rectified Sparse Attention (ReSA), which combines block-sparse attention with periodic dense rectification, refreshing the KV cache at fixed intervals to limit error accumulation.", "result": "ReSA achieves near-lossless generation quality with significant efficiency improvements, including up to 2.42Ã speedup for 256K sequence lengths in various tasks.", "conclusion": "ReSA is presented as a practical solution for scalable long-context inference, maintaining alignment with pretraining distribution and enhancing generation quality.", "key_contributions": ["Introduction of Rectified Sparse Attention (ReSA) method", "Demonstrated significant efficiency improvements in long-sequence generation", "Code availability for practical implementation"], "limitations": "", "keywords": ["Long-sequence generation", "Large Language Models", "Sparse attention"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.04131", "pdf": "https://arxiv.org/pdf/2506.04131.pdf", "abs": "https://arxiv.org/abs/2506.04131", "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SICon 2025 ACL", "summary": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.", "AI": {"tldr": "The paper introduces LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework for enhanced manipulation analysis.", "motivation": "To address the gap in applying NLP for detecting and analyzing manipulation in the legal domain.", "method": "The study introduces LegalCon, a dataset of 1,063 annotated courtroom conversations, and proposes CLAIM, an Intent-driven Multi-agent framework for manipulation analysis.", "result": "Results indicate the potential of agentic frameworks like CLAIM to improve fairness and transparency in judicial processes.", "conclusion": "The research contributes to the application of NLP in legal discourse analysis and aims to develop robust tools for supporting fairness in legal decision-making.", "key_contributions": ["Introduction of LegalCon dataset for courtroom manipulation detection.", "Development of the CLAIM framework for context-aware decision-making.", "Highlighting the potential for NLP to enhance fairness in legal processes."], "limitations": "", "keywords": ["NLP", "Legal Analysis", "Manipulation Detection", "Courtroom Conversations", "AI in Law"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.04139", "pdf": "https://arxiv.org/pdf/2506.04139.pdf", "abs": "https://arxiv.org/abs/2506.04139", "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?", "authors": ["Ratna Kandala", "Katie Hoemann"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.", "AI": {"tldr": "The study explores the effectiveness of Dutch-specific LLMs in capturing emotional valence in everyday narratives compared to traditional tools like LIWC and Pattern, highlighting the need for tailored models for accurate analysis.", "motivation": "The research aims to improve understanding of everyday language nuances in computational linguistics and emotions research by examining LLM performance in valence prediction.", "method": "A study with 25,000 textual responses from 102 Dutch-speaking participants was conducted, where narratives about current feelings were collected alongside self-assessed valence ratings. The performance of three LLMs was compared to traditional lexicon-based tools.", "result": "The findings revealed that the Dutch-tuned LLMs struggle to accurately predict emotional valence from spontaneous narratives, showing limitations in capturing the context-dependent nature of everyday language.", "conclusion": "The study emphasizes the necessity for culturally and linguistically adapted models to improve automated emotional analysis and suggests focusing on low-resource languages like Flemish.", "key_contributions": ["Evaluation of LLMs' emotional analysis in spontaneous narratives", "Comparison of LLM outputs with traditional tools", "Call for tailored models for low-resource languages"], "limitations": "The study is limited to Dutch-speaking participants and focuses on only a few LLMs, which may not represent the broader capabilities of all LLMs in emotional analysis.", "keywords": ["LLM", "emotional valence", "natural language processing", "Flemish", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04142", "pdf": "https://arxiv.org/pdf/2506.04142.pdf", "abs": "https://arxiv.org/abs/2506.04142", "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis", "authors": ["Kejian Zhu", "Shangqing Tu", "Zhuoran Jin", "Lei Hou", "Juanzi Li", "Jun Zhao"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation", "AI": {"tldr": "The paper addresses data contamination in LLM evaluations, proposing a method to identify and suppress shortcut neurons for more trustworthy results.", "motivation": "Ensuring trustworthy evaluation of large language models (LLMs) is crucial as current public benchmarks are prone to data contamination, compromising fairness.", "method": "The authors propose a novel method for identifying shortcut neurons through comparative and causal analysis, and introduce 'shortcut neuron patching' to suppress these neurons.", "result": "Experiments validate that the proposed method effectively mitigates contamination, achieving a high Spearman coefficient with MixEval, indicating strong correlation with true model capabilities.", "conclusion": "The method demonstrates generalizability across various benchmarks and hyperparameter settings, validating its effectiveness in enhancing evaluations of LLMs.", "key_contributions": ["Identification of shortcut neurons in LLMs", "Introduction of shortcut neuron patching", "Validation of the method's effectiveness through correlation with MixEval."], "limitations": "", "keywords": ["large language models", "evaluation", "data contamination", "shortcut neurons", "trustworthy benchmarks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.04156", "pdf": "https://arxiv.org/pdf/2506.04156.pdf", "abs": "https://arxiv.org/abs/2506.04156", "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "categories": ["cs.CL"], "comment": null, "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "AI": {"tldr": "ArchEHR-QA introduces a novel dataset for evaluating AI responses to patient questions using clinical evidence from EHRs, focusing on their factual accuracy and relevance.", "motivation": "There is a lack of robust datasets that capture patient information needs related to their hospitalizations, which can be addressed using AI systems.", "method": "The study introduces ArchEHR-QA, an expert-annotated dataset of 134 patient cases that includes questions from patients, clinician interpretations, clinical note excerpts, and clinician-authored answers. It evaluates the performance of three large language models (Llama 4, Llama 3, and Mixtral) using different prompting strategies.", "result": "The answer-first prompting method with Llama 4 outperformed the other models, achieving the highest scores in factuality and relevance, validated through manual error analysis.", "conclusion": "ArchEHR-QA sets a benchmark for patient-centered EHR question answering systems and highlights the need for generating accurate and relevant AI responses in clinical contexts.", "key_contributions": ["Introduction of ArchEHR-QA, a novel dataset for patient-centered EHR QA systems", "Evaluation of multiple LLMs with novel prompting strategies", "Identifying common issues in AI-generated clinical responses"], "limitations": "The dataset is limited to 134 patient cases, and findings may not generalize broadly across different patient populations or contexts.", "keywords": ["EHR", "AI", "QA", "patient-centered", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.04179", "pdf": "https://arxiv.org/pdf/2506.04179.pdf", "abs": "https://arxiv.org/abs/2506.04179", "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling", "authors": ["Anhao Zhao", "Fanghua Ye", "Yingqi Fan", "Junlong Tong", "Zhiwei Fei", "Hui Su", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.", "AI": {"tldr": "SkipGPT introduces a dynamic layer pruning framework that optimizes large language models' computational efficiency by implementing global token-aware routing and decoupled pruning policies for different layers, achieving substantial parameter reduction while maintaining performance.", "motivation": "Large language models are computationally expensive; thus, efficient models are needed to balance resource usage and performance.", "method": "SkipGPT employs a two-stage optimization process with a disentangled training phase for routing strategies and LoRA fine-tuning to maintain model performance post-pruning.", "result": "SkipGPT can reduce model parameters by over 40% while matching or exceeding the performance of the original dense model on various benchmarks.", "conclusion": "The framework harmonizes efficiency and performance, advancing scalable deployment of LLMs in real-world applications.", "key_contributions": ["Introduction of dynamic layer pruning for LLMs", "Global token-aware routing mechanism", "Decoupled pruning strategies for different model components"], "limitations": "", "keywords": ["dynamic layer pruning", "large language models", "token-aware routing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.04180", "pdf": "https://arxiv.org/pdf/2506.04180.pdf", "abs": "https://arxiv.org/abs/2506.04180", "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "AI": {"tldr": "SuperWriter-Agent is an agent-based framework that enhances long-form text generation quality and consistency for LLMs through structured thinking and hierarchical optimization.", "motivation": "Long-form text generation in LLMs faces challenges in coherence, logical consistency, and quality, particularly as sequence lengths increase.", "method": "The SuperWriter-Agent integrates structured thinking and introduces planning and refinement stages into the text generation pipeline. A 7B SuperWriter-LM is trained on a newly constructed supervised fine-tuning dataset, employing a hierarchical Direct Preference Optimization procedure guided by Monte Carlo Tree Search.", "result": "SuperWriter-LM achieves state-of-the-art results on various benchmarks and outperforms larger models in both automatic and human evaluations.", "conclusion": "The framework's incorporation of structured thinking steps significantly improves long-form text generation quality, as evidenced by empirical results and ablation studies.", "key_contributions": ["Introduction of SuperWriter-Agent framework for LLM text generation.", "Development of hierarchical Direct Preference Optimization for refining text quality.", "Creation of a new supervised fine-tuning dataset for training long-form LLMs."], "limitations": "", "keywords": ["long-form text generation", "language models", "structured thinking", "preference optimization", "Monte Carlo Tree Search"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.04182", "pdf": "https://arxiv.org/pdf/2506.04182.pdf", "abs": "https://arxiv.org/abs/2506.04182", "title": "Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models", "authors": ["Ruiqi Zhang", "Changyi Xiao", "Yixin Cao"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone.", "AI": {"tldr": "This paper presents SwitchCoT, a framework for selecting between long and short Chain-of-Thought strategies based on task context, which improves computational efficiency while maintaining accuracy.", "motivation": "As large reasoning models advance, understanding when to use long vs. short Chain-of-Thought prompting is essential to balance between performance and token consumption.", "method": "Empirical analysis comparing long and short Chain-of-Thought prompting strategies under varying resource constraints, leading to the development of an automatic framework, SwitchCoT.", "result": "SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy, achieving performance comparable to or exceeding that of using either long or short CoT alone under limited token budgets.", "conclusion": "A dynamic and budget-aware approach to Chain-of-Thought prompting can significantly optimize performance across different resource availability scenarios.", "key_contributions": ["Introduction of SwitchCoT framework that dynamically selects CoT strategies", "Empirical analysis highlighting the trade-off between token usage and performance", "Proven reduction in inference costs while maintaining accuracy"], "limitations": "Focus on budget constraints may limit generalizability to all Chain-of-Thought tasks or scenarios without such constraints.", "keywords": ["Chain-of-Thought", "prompting strategies", "SwitchCoT", "computational efficiency", "token consumption"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.04185", "pdf": "https://arxiv.org/pdf/2506.04185.pdf", "abs": "https://arxiv.org/abs/2506.04185", "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "categories": ["cs.CL"], "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.", "AI": {"tldr": "R-Search is a framework for integrating reasoning with search in large language models, using reinforcement learning to improve response quality in complex tasks.", "motivation": "To enhance the reasoning capabilities of LLMs by optimizing their interaction with search mechanisms, addressing the current limitations in producing optimal responses during reasoning-search interactions.", "method": "A reinforcement learning framework that autonomously determines when a model should retrieve information or reason, employing multi-reward signals to optimize reasoning-search interaction trajectories.", "result": "R-Search significantly outperforms existing RAG baselines, achieving improvements of up to 32.2% in-domain and 25.1% out-of-domain across seven datasets.", "conclusion": "The introduction of R-Search provides a robust method for facilitating deep interactions between reasoning and search, leading to better performance in logic and knowledge-intensive tasks.", "key_contributions": ["Development of R-Search for reasoning-search integration in LLMs", "Utilization of multi-reward signals to optimize reasoning trajectories", "Demonstrated significant performance improvements over current RAG methodologies"], "limitations": "", "keywords": ["large language models", "reasoning-search integration", "reinforcement learning", "multi-step reasoning", "search interaction"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.04226", "pdf": "https://arxiv.org/pdf/2506.04226.pdf", "abs": "https://arxiv.org/abs/2506.04226", "title": "Efficient Knowledge Editing via Minimal Precomputation", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes.", "AI": {"tldr": "This paper reduces the precomputation cost of MEMIT knowledge editing by showing that fewer hidden vectors are needed, significantly saving time during the editing process.", "motivation": "To decrease the excessive computational cost associated with the precomputation step in knowledge editing methods like MEMIT, ROME, and EMMET.", "method": "The authors determine the theoretical minimum number of hidden vector precomputations needed for effective knowledge editing and empirically validate that less than 0.3% of the originally stipulated hidden vectors are sufficient.", "result": "The study demonstrates that knowledge editing can be accomplished with a drastically reduced number of hidden vectors, resulting in a precomputation time cut down to a few minutes from the preceding hours.", "conclusion": "By minimizing the precomputation step, the paper enables more efficient knowledge editing in large language models, providing a practical improvement for users.", "key_contributions": ["Theoretical analysis of minimum hidden vector precomputation for knowledge editing.", "Empirical evidence showing significantly reduced hidden vector requirements for effective editing.", "Demonstration of practical time savings in model editing processes."], "limitations": "", "keywords": ["Knowledge editing", "MEMIT", "hidden vectors", "machine learning", "language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.03741", "pdf": "https://arxiv.org/pdf/2506.03741.pdf", "abs": "https://arxiv.org/abs/2506.03741", "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "authors": ["Rifat Mehreen Amin", "Oliver Hans KÃ¼hle", "Daniel Buschek", "Andreas Butz"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI.", "AI": {"tldr": "PromptCanvas is a widget-based interface that enhances user control over AI-generated content, demonstrating better creativity support than traditional UIs.", "motivation": "The paper aims to improve user engagement and control in AI-generated content generation by providing a flexible, customizable interface.", "method": "The study investigates the use of PromptCanvas through a lab study with 18 participants and a follow-up field study with 10 participants, measuring creativity support and cognitive load.", "result": "PromptCanvas resulted in lower mental demand and frustration compared to traditional UIs, and received positive qualitative feedback for facilitating new ideas.", "conclusion": "Dynamic, customizable interfaces like PromptCanvas can significantly enhance collaborative writing experiences with AI.", "key_contributions": ["Introduction of the PromptCanvas concept", "Demonstrated improvement in the Creativity Support Index", "Showed reduced cognitive load in users"], "limitations": "The sample size is small, limiting the generalizability of the findings.", "keywords": ["PromptCanvas", "AI content generation", "Creativity support", "Cognitive load", "Collaborative writing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2303.11607", "pdf": "https://arxiv.org/pdf/2303.11607.pdf", "abs": "https://arxiv.org/abs/2303.11607", "title": "Transformers in Speech Processing: A Survey", "authors": ["Siddique Latif", "Aun Zaidi", "Heriberto Cuayahuitl", "Fahad Shamshad", "Moazzam Shoukat", "Muhammad Usama", "Junaid Qadir"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Computer Science Review 2025", "summary": "The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues.", "AI": {"tldr": "This paper surveys the application of transformers in various speech processing domains including recognition, synthesis, and dialogue systems, addressing their challenges and potential solutions.", "motivation": "To consolidate and present research on the use of transformers in speech processing, a rapidly developing area influenced by advances in natural language processing.", "method": "A comprehensive survey of the literature and research studies focusing on transformer applications in different subfields of speech technology.", "result": "The survey reveals the broad adoption of transformers in speech processing and identifies significant challenges they face, along with possible solutions.", "conclusion": "This work serves as a resource for researchers to understand the potential of transformers in advancing speech technology and guides them in overcoming existing challenges.", "key_contributions": ["Comprehensive survey of transformers in speech technology", "Identification of challenges and solutions for using transformers", "Resource for future research in speech processing"], "limitations": "", "keywords": ["transformers", "speech processing", "natural language processing", "survey", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2308.09583", "pdf": "https://arxiv.org/pdf/2308.09583.pdf", "abs": "https://arxiv.org/abs/2308.09583", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct", "authors": ["Haipeng Luo", "Qingfeng Sun", "Can Xu", "Pu Zhao", "Jianguang Lou", "Chongyang Tao", "Xiubo Geng", "Qingwei Lin", "Shifeng Chen", "Yansong Tang", "Dongmei Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ICLR 2025 as an Oral presentation", "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM", "AI": {"tldr": "This paper introduces WizardMath, a model that enhances LLMs' mathematical reasoning using RLEIF, outperforming existing models in math-related NLP tasks.", "motivation": "Existing open-source LLMs lack math-related optimization despite their strong NLP performance.", "method": "The authors developed WizardMath, utilizing their Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method specifically for enhancing mathematical reasoning in LLMs.", "result": "WizardMath-Mistral 7B demonstrates superior performance on GSM8k and MATH benchmarks, exceeding top LLM competitors and achieving higher data efficiency.", "conclusion": "The study accentuates the importance of instruction evolution and process supervision in improving mathematical reasoning in LLMs.", "key_contributions": ["Introduction of WizardMath for improved mathematical reasoning in LLMs", "Demonstration of significant performance improvements over existing open-source models", "Highlighting the role of instruction evolution in LLM training"], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Reinforcement Learning", "NLP", "Instruction Evolution"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2310.12049", "pdf": "https://arxiv.org/pdf/2310.12049.pdf", "abs": "https://arxiv.org/abs/2310.12049", "title": "Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models", "authors": ["Patrick Y. Wu", "Jonathan Nagler", "Joshua A. Tucker", "Solomon Messing"], "categories": ["cs.CL", "cs.CY"], "comment": "10 pages, 2 figures. Appears in 2024 IEEE International Conference on\n  Big Data (BigData). Please cite the published version:\n  10.1109/BigData62323.2024.10825235", "summary": "Existing text scoring methods require a large corpus, struggle with short\ntexts, or require hand-labeled data. We develop a text scoring framework that\nleverages generative large language models (LLMs) to (1) set texts against the\nbackdrop of information from the near-totality of the web and digitized media,\nand (2) effectively transform pairwise text comparisons from a reasoning\nproblem to a pattern recognition task. Our approach, concept-guided\nchain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with\nan LLM to generate a concept-specific breakdown for each text, akin to guidance\nprovided to human coders. We then pairwise compare breakdowns using an LLM and\naggregate answers into a score using a probability model. We apply this\napproach to better understand speech reflecting aversion to specific political\nparties on Twitter, a topic that has commanded increasing interest because of\nits potential contributions to democratic backsliding. We achieve stronger\ncorrelations with human judgments than widely used unsupervised text scoring\nmethods like Wordfish. In a supervised setting, besides a small pilot dataset\nto develop CGCoT prompts, our measures require no additional hand-labeled data\nand produce predictions on par with RoBERTa-Large fine-tuned on thousands of\nhand-labeled tweets. This project showcases the potential of combining human\nexpertise and LLMs for scoring tasks.", "AI": {"tldr": "This paper introduces a novel text scoring framework utilizing generative large language models (LLMs) for effective short text comparison, demonstrating superior performance to traditional methods.", "motivation": "Existing text scoring methods face challenges such as requiring large corpora or hand-labeled data. This study aims to develop an efficient framework addressing these limitations.", "method": "The authors propose a concept-guided chain-of-thought (CGCoT) approach which includes generating concept-specific breakdowns of texts using an LLM and conducting pairwise comparisons to produce scores.", "result": "The CGCoT method achieves stronger correlations with human judgments compared to unsupervised methods like Wordfish and performs on par with a fine-tuned RoBERTa-Large model without additional hand-labeled data.", "conclusion": "Combining human expertise with LLMs shows promise for improving text scoring tasks, particularly in political discourse analysis on platforms like Twitter.", "key_contributions": ["Introduction of the CGCoT framework for text scoring", "Achieving superior performance to traditional unsupervised methods", "Ability to score texts without requiring extensive hand-labeled datasets"], "limitations": "", "keywords": ["text scoring", "large language models", "HCI", "political discourse", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.02524", "pdf": "https://arxiv.org/pdf/2406.02524.pdf", "abs": "https://arxiv.org/abs/2406.02524", "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks", "authors": ["Maciej Besta", "Lorenzo Paleari", "Marcin Copik", "Robert Gerstenberger", "Ales Kubicek", "Piotr Nyczyk", "Patrick Iff", "Eric Schreiber", "Tanja Srindran", "Tomasz Lehmann", "Hubert Niewiadomski", "Torsten Hoefler"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.", "AI": {"tldr": "CheckEmbed (CE) is a new verification method for LLM outputs that uses embedding vectors for accurate and scalable verification, outperforming previous methods in detecting hallucinations.", "motivation": "The verification of outputs from Large Language Models (LLMs) is challenging, particularly for complex tasks, making the need for new methods critical.", "method": "CE reduces LLM answers to a single embedding vector using modern embedding models, allowing for whole-answer level comparisons that improve accuracy and scalability.", "result": "CE demonstrates superior effectiveness and efficiency in detecting hallucinations across various tasks compared to 13 existing verification methods.", "conclusion": "CE establishes itself as a versatile framework for output verification in both text and other modalities such as vision.", "key_contributions": ["Introduction of CheckEmbed (CE) for whole-answer verification of LLM outputs", "Empirical demonstration of CE's effectiveness in detecting hallucinations", "Generalization of CE beyond text to vision tasks"], "limitations": "", "keywords": ["Large Language Models", "verification", "CheckEmbed", "embeddings", "hallucinations"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2406.09295", "pdf": "https://arxiv.org/pdf/2406.09295.pdf", "abs": "https://arxiv.org/abs/2406.09295", "title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models", "authors": ["Yuhang Wu", "Wenmeng Yu", "Yean Cheng", "Yan Wang", "Xiaohan Zhang", "Jiazheng Xu", "Ming Ding", "Yuxiao Dong"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, which provides more nuanced\nevaluations of alignment capabilities and is the first benchmark specifically\ndesigned for Chinese visual contexts. This benchmark is meticulously curated\nfrom real-world scenarios and internet sources, encompassing thirteen specific\ntasks across three categories, and includes both single-turn and multi-turn\ndialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench\nencompasses 1,054 images and 4,978 question-answer pairs. To facilitate the\nevaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that\nexceeds GPT-4's evaluation ability. Additionally, we measure the \"alignment\nscore\", a quantitative metric designed to assess the robustness and stability\nof models across diverse prompts. Finally, we evaluate the performance of\nrepresentative VLMs on AlignMMBench, offering insights into the capabilities\nand limitations of different VLM architectures. The evaluation code and data\nare available at https://github.com/THUDM/AlignMMBench.", "AI": {"tldr": "This paper introduces AlignMMBench, a benchmark for evaluating the alignment capabilities of large Vision-Language Models (VLMs) in Chinese visual contexts, offering a nuanced assessment beyond existing benchmarks.", "motivation": "There is a lack of comprehensive benchmarks that evaluate the alignment capabilities of VLMs in real-world, Chinese-specific contexts using nuanced methodologies.", "method": "AlignMMBench consists of 1,054 images and 4,978 QA pairs across thirteen tasks, incorporating both single-turn and multi-turn dialogues, along with a rule-calibrated evaluator called CritiqueVLM for performance assessment.", "result": "The evaluation reveals varying capabilities and limitations among different VLM architectures when assessed using AlignMMBench.", "conclusion": "AlignMMBench provides a significant advancement in evaluating the alignment capabilities of VLMs, incorporating novel metrics and methodologies that can benefit future research in this area.", "key_contributions": ["Introduction of AlignMMBench for nuanced VLM evaluation in Chinese contexts", "Development of CritiqueVLM as a superior evaluator compared to existing models like GPT-4", "Creation of a quantitative 'alignment score' for model robustness assessment"], "limitations": "", "keywords": ["Vision-Language Models", "alignment capabilities", "benchmark", "Chinese visual contexts", "CritiqueVLM"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2406.12784", "pdf": "https://arxiv.org/pdf/2406.12784.pdf", "abs": "https://arxiv.org/abs/2406.12784", "title": "UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions", "authors": ["Xunzhi Wang", "Zhuowei Zhang", "Gaonan Chen", "Qiongyu Li", "Bitong Luo", "Zhixin Han", "Haotian Wang", "Zhiyu li", "Hang Gao", "Mengting Hu"], "categories": ["cs.CL"], "comment": "accepted by ACL Findings (2025)", "summary": "Despite recent progress in systematic evaluation frameworks, benchmarking the\nuncertainty of large language models (LLMs) remains a highly challenging task.\nExisting methods for benchmarking the uncertainty of LLMs face three key\nchallenges: the need for internal model access, additional training, or high\ncomputational costs. This is particularly unfavorable for closed-source models.\nTo this end, we introduce UBench, a new benchmark for evaluating the\nuncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence\nintervals. It encompasses 11,978 multiple-choice questions spanning knowledge,\nlanguage, understanding, and reasoning capabilities. Based on this, we conduct\nextensive experiments. This includes comparisons with other advanced\nuncertainty estimation methods, the assessment of the uncertainty of 20 LLMs,\nand an exploration of the effects of Chain-of-Thought (CoT) prompts,\nrole-playing (RP) prompts, and temperature on model uncertainty. Our analysis\nreveals several crucial insights: 1) Our confidence interval-based methods are\nhighly effective for uncertainty quantification; 2) Regarding uncertainty,\noutstanding open-source models show competitive performance versus\nclosed-source models; 3) CoT and RP prompts present potential ways to improve\nmodel reliability, while the influence of temperature changes follows no\nuniversal rule. Our implementation is available at\nhttps://github.com/Cyno2232/UBENCH.", "AI": {"tldr": "Introducing UBench, a new benchmark for evaluating the uncertainty of large language models (LLMs) based on confidence intervals.", "motivation": "There are significant challenges in benchmarking the uncertainty of large language models (LLMs) due to the need for internal model access, additional training, and high computational costs, particularly for closed-source models.", "method": "UBench utilizes confidence intervals and includes 11,978 multiple-choice questions to evaluate various capabilities across 20 LLMs, performing comprehensive comparisons with advanced uncertainty estimation methods.", "result": "UBench demonstrates that confidence interval-based methods are effective for uncertainty quantification, with open-source models performing competitively compared to closed-source models. The analysis also uncovers the potential of CoT and RP prompts to improve model reliability.", "conclusion": "The UBench framework presents valuable insights into estimating uncertainty in LLMs, showing that certain prompting strategies can enhance reliability and that performance can vary between open-source and closed-source models.", "key_contributions": ["Introduction of a new benchmark (UBench) for LLM uncertainty evaluation", "Use of confidence intervals for uncertainty quantification", "Insights on the effectiveness of various prompting strategies for improving LLM reliability"], "limitations": "The research is limited to specific models and questions, leaving potential explorations of other models and question types unexplored.", "keywords": ["Large Language Models", "Benchmarking", "Uncertainty Quantification", "Confidence Intervals", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.18173", "pdf": "https://arxiv.org/pdf/2406.18173.pdf", "abs": "https://arxiv.org/abs/2406.18173", "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs", "authors": ["Wenhao Li", "Mingbao Lin", "Yunshan Zhong", "Shuicheng Yan", "Rongrong Ji"], "categories": ["cs.CL"], "comment": "The experimental results of the paper require further validation", "summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.", "AI": {"tldr": "This paper presents UIO-LLMs, an innovative memory-enhanced transformer approach designed to manage long texts in large language models by extending the context window significantly while optimizing training and inference.", "motivation": "Despite advancements in large language models, managing extended contexts remains a significant challenge due to limitations in context window sizes. This study seeks to enhance the capability of LLMs in handling long texts efficiently.", "method": "The method involves an encoder-decoder framework utilizing shared weights, where the encoder creates memory from context segments to aid in predicting subsequent outputs. Additionally, the Truncated Backpropagation Through Time (TBPTT) algorithm is used to enhance the training process by reducing time complexity and improving gradient computation efficiency.", "result": "UIO-LLMs extend the context window of models like Llama2-7b-chat from 4K to 100K tokens with only a 2% increase in parameters, while maintaining nearly linear inference costs as context length grows.", "conclusion": "The proposed approach enables more effective handling of long context in large language models, although further experimental validation is needed for the results presented.", "key_contributions": ["Introduction of UIO-LLMs for long-context management", "Use of TBPTT for training optimization", "Significant extension of context window with minimal parameter increase"], "limitations": "The experimental results of the paper require further validation.", "keywords": ["memory-enhanced transformers", "long-context", "incremental optimization", "UIO-LLMs", "Llama2"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.17169", "pdf": "https://arxiv.org/pdf/2409.17169.pdf", "abs": "https://arxiv.org/abs/2409.17169", "title": "REAL: Response Embedding-based Alignment for LLMs", "authors": ["Honggen Zhang", "Xufeng Zhao", "Igor Molybog", "June Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work.", "AI": {"tldr": "Proposes REAL, a method for selecting distinct response pairs to improve LLM alignment while reducing annotation efforts by 65%.", "motivation": "To address the challenges of human bias and inefficiency in the preference dataset creation for LLMs alignment.", "method": "Response Embedding-based Alignment, focusing on acquiring less ambiguous preference pairs using similarity of response embeddings independent of prompts.", "result": "Experimental results show improved alignment efficiency with better margins and win rates on dialogue tasks by selecting dissimilar response pairs.", "conclusion": "Focusing on distinct response pairs can enhance LLM alignment efficiency and significantly reduce labeling errors and effort.", "key_contributions": ["Introduction of REAL for LLM alignment", "Demonstration of reduced label errors", "Significant savings in annotation workload"], "limitations": "", "keywords": ["Large Language Models", "Human Preferences", "Annotation Efficiency", "Alignment", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.01444", "pdf": "https://arxiv.org/pdf/2410.01444.pdf", "abs": "https://arxiv.org/abs/2410.01444", "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "authors": ["Jin Hwa Lee", "Thomas Jiralerspong", "Lei Yu", "Yoshua Bengio", "Emily Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "Under review at ARR", "summary": "By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition.", "AI": {"tldr": "This paper explores the relationship between linguistic compositionality and the intrinsic dimensions of language model representations, demonstrating how dataset compositionality correlates with representation complexity.", "motivation": "To investigate whether contemporary language models mirror the simplicity of language through compositional structures and to understand how these structures relate to representation complexity.", "method": "The authors take a geometric approach by analyzing the intrinsic dimension of representations from language models in relation to the compositionality of datasets.", "result": "The study shows that the degree of dataset compositionality influences the intrinsic dimensions of LM representations, indicating that learned linguistic features capture both semantic and superficial aspects differently.", "conclusion": "The findings suggest a significant interaction between compositionality and representation complexity, emphasizing the role of training on linguistic features in LMs.", "key_contributions": ["Establishing a link between the degree of compositionality and the intrinsic dimension of LM representations.", "Demonstrating that nonlinear dimensionality captures semantic aspects while linear dimensionality reflects superficial features.", "Providing insights into how training shapes the representation of linguistic features in language models."], "limitations": "", "keywords": ["compositionality", "intrinsic dimension", "language models", "representation complexity", "geometric analysis"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.09300", "pdf": "https://arxiv.org/pdf/2410.09300.pdf", "abs": "https://arxiv.org/abs/2410.09300", "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding", "authors": ["Yu Fei", "Yasaman Razeghi", "Sameer Singh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (main)", "summary": "Large language models (LLMs) require alignment to effectively and safely\nfollow user instructions. This process necessitates training an aligned version\nfor every base model, resulting in significant computational overhead. In this\nwork, we propose NUDGING, a simple, training-free algorithm that aligns any\nbase model at inference time using a small aligned model. NUDGING is motivated\nby recent findings that alignment primarily alters the model's behavior on a\nsmall subset of stylistic tokens (e.g., discourse markers). We find that base\nmodels are significantly more uncertain when generating these tokens. Building\non this insight, NUDGING employs a small aligned model to generate nudging\ntokens to guide the base model's output during decoding when the base model's\nuncertainty is high, with only a minor additional inference overhead. We\nevaluate NUDGING across 3 model families on a diverse range of open-instruction\ntasks. Without any training, nudging a large base model with a 7x-14x smaller\naligned model achieves zero-shot performance comparable to, and sometimes\nsurpassing, that of large aligned models. By operating at the token level,\nNUDGING enables off-the-shelf collaboration between model families. For\ninstance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat\non various tasks. Overall, our work offers a modular and cost-efficient\nsolution to LLM alignment. Our code and demo are available at:\nhttps://fywalter.github.io/nudging/ .", "AI": {"tldr": "NUDGING is a training-free algorithm for aligning large language models at inference time using a small aligned model, improving their performance on specific tasks.", "motivation": "Large language models require alignment to safely follow instructions, but training requires high computational resources. NUDGING provides a way to achieve alignment efficiently at inference time by leveraging insights about model uncertainty.", "method": "NUDGING generates nudging tokens from a small aligned model to guide the output of a base model during decoding, particularly when the base model is uncertain about generating stylistic tokens.", "result": "NUDGING achieves zero-shot performance comparable to and sometimes surpassing large aligned models while employing a small aligned model, with minimal additional inference cost.", "conclusion": "NUDGING offers a modular and efficient solution for aligning LLMs, enabling effective collaboration between different model families without extensive training.", "key_contributions": ["Introduces NUDGING, a training-free alignment method for LLMs", "Demonstrates improved performance of base models nudged by smaller aligned models", "Facilitates cross-model collaboration and efficient alignment at inference time"], "limitations": "", "keywords": ["large language models", "alignment", "inference", "ML", "NLU"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.16502", "pdf": "https://arxiv.org/pdf/2410.16502.pdf", "abs": "https://arxiv.org/abs/2410.16502", "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning", "authors": ["Jason Chan", "Robert Gaizauskas", "Zhixue Zhao"], "categories": ["cs.CL"], "comment": "Preprint. Accepted by ICML 2025", "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning.", "AI": {"tldr": "The study introduces a dataset called RULEBREAKERS to evaluate LLMs' reasoning capabilities in unconventional scenarios, revealing limitations in their logic application compared to human reasoning.", "motivation": "Formal logic often fails in unique scenarios where common sense is required; the study aims to assess LLMs' ability to handle such 'rulebreaker' situations.", "method": "The researchers developed the RULEBREAKERS dataset and evaluated its impact on seven LLMs, observing their performance and accuracy in recognizing rulebreaking instances.", "result": "The evaluation revealed that most LLMs, including GPT-4o, achieve mediocre accuracy and tend to apply logical rules too rigidly, diverging from human reasoning.", "conclusion": "The findings underline a significant limitation in current LLMs' reasoning capabilities and caution against solutions reliant on formal logic, which may exacerbate the gap between LLMs and human-like reasoning.", "key_contributions": ["Introduction of the RULEBREAKERS dataset for LLM evaluation.", "Empirical findings on LLM performance in 'rulebreaker' contexts.", "Insights into the relationship between LLM performance and their knowledge utilization."], "limitations": "The study focuses solely on LLMs' performance and does not address the potential remedies or improvements for their reasoning abilities.", "keywords": ["Large Language Models", "Formal Logic", "Human-like Reasoning", "Cognitive Science", "RULEBREAKERS"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.01747", "pdf": "https://arxiv.org/pdf/2411.01747.pdf", "abs": "https://arxiv.org/abs/2411.01747", "title": "DynaSaur: Large Language Agents Beyond Predefined Actions", "authors": ["Dang Nguyen", "Viet Dac Lai", "Seunghyun Yoon", "Ryan A. Rossi", "Handong Zhao", "Ruiyi Zhang", "Puneet Mathur", "Nedim Lipka", "Yu Wang", "Trung Bui", "Franck Dernoncourt", "Tianyi Zhou"], "categories": ["cs.CL"], "comment": "19 pages, 10 figures", "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.", "AI": {"tldr": "The paper presents a dynamic LLM agent framework that generates and composes actions in real-time, overcoming limitations of fixed action sets in complex environments.", "motivation": "Existing LLM agents struggle in open-ended scenarios due to fixed action sets which limit flexibility and require extensive human effort to define actions.", "method": "The proposed framework allows agents to interact with their environment by generating and executing programs in a general-purpose programming language, dynamically creating actions and accumulating them for future use.", "result": "Experiments demonstrate that the dynamic framework significantly enhances flexibility and outperforms traditional methods that use fixed set actions, especially in unexpected scenarios.", "conclusion": "The dynamic agent framework enables better adaptation and recovery in complex environments, addressing shortcomings of conventional LLM agents.", "key_contributions": ["Introduction of dynamic action creation for LLM agents", "Demonstration of improved flexibility in open-ended scenarios", "Accumulation of generated actions for future reuse"], "limitations": "", "keywords": ["LLM agents", "dynamic action generation", "program execution"], "importance_score": 9, "read_time_minutes": 19}}
{"id": "2411.04920", "pdf": "https://arxiv.org/pdf/2411.04920.pdf", "abs": "https://arxiv.org/abs/2411.04920", "title": "Enabling LLM Knowledge Analysis via Extensive Materialization", "authors": ["Yujia Hu", "Tuan-Phong Nguyen", "Shrestha Ghosh", "Simon Razniewski"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "14 pages, 4 tables, 12 figures", "summary": "Large language models (LLMs) have majorly advanced NLP and AI, and next to\ntheir ability to perform a wide range of procedural tasks, a major success\nfactor is their internalized factual knowledge. Since Petroni et al. (2019),\nanalyzing this knowledge has gained attention. However, most approaches\ninvestigate one question at a time via modest-sized pre-defined samples,\nintroducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents\nthe analysis of knowledge (or beliefs) of LLMs beyond the experimenter's\npredisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterialize an LLM's factual knowledge through recursive querying and result\nconsolidation. Our approach is a milestone for LLM research, for the first time\nproviding constructive insights into the scope and structure of LLM knowledge\n(or beliefs).\n  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million\nrelational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB\nto exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,\naccuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible\nat https://gptkb.org", "AI": {"tldr": "This paper introduces GPTKB, a knowledge base derived from an LLM, aimed at analyzing its factual knowledge comprehensively.", "motivation": "To address the limitations of existing methods that analyze LLM knowledge based on availability bias, introducing a comprehensive analysis of knowledge structure.", "method": "The authors propose a novel recursive querying methodology to materialize and analyze the factual knowledge of LLMs, specifically creating a knowledge base called GPTKB.", "result": "GPTKB contains 101 million relational triples for over 2.9 million entities, allowing for extensive analysis of GPT-4o-mini's knowledge, including aspects like scale, accuracy, and bias.", "conclusion": "GPTKB provides valuable insights into the scope and structure of LLM knowledge, overcoming previous biases in knowledge assessment.", "key_contributions": ["Introduction of a novel methodology for LLM knowledge analysis", "Creation of GPTKB, a large knowledge base for LLMs", "Analysis of LLM factual knowledge on multiple dimensions simultaneously."], "limitations": "", "keywords": ["large language models", "knowledge base", "factual knowledge", "NLP", "GPT"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.05042", "pdf": "https://arxiv.org/pdf/2411.05042.pdf", "abs": "https://arxiv.org/abs/2411.05042", "title": "Improving Radiology Report Conciseness and Structure via Local Large Language Models", "authors": ["Iryna Hartsock", "Cyrillo Araujo", "Les Folio", "Ghulam Rasool"], "categories": ["cs.CL", "cs.AI"], "comment": "published version", "summary": "Radiology reports are often lengthy and unstructured, posing challenges for\nreferring physicians to quickly identify critical imaging findings while\nincreasing the risk of missed information. This retrospective study aimed to\nenhance radiology reports by making them concise and well-structured, with\nfindings organized by relevant organs. To achieve this, we utilized private\nlarge language models (LLMs) deployed locally within our institution's\nfirewall, ensuring data security and minimizing computational costs. Using a\ndataset of 814 radiology reports from seven board-certified body radiologists\nat Moffitt Cancer Center, we tested five prompting strategies within the\nLangChain framework. After evaluating several models, the Mixtral LLM\ndemonstrated superior adherence to formatting requirements compared to\nalternatives like Llama. The optimal strategy involved condensing reports first\nand then applying structured formatting based on specific instructions,\nreducing verbosity while improving clarity. Across all radiologists and\nreports, the Mixtral LLM reduced redundant word counts by more than 53%. These\nfindings highlight the potential of locally deployed, open-source LLMs to\nstreamline radiology reporting. By generating concise, well-structured reports,\nthese models enhance information retrieval and better meet the needs of\nreferring physicians, ultimately improving clinical workflows.", "AI": {"tldr": "This study enhances radiology reports by using large language models to create concise, structured formats, improving clarity and reducing redundancy.", "motivation": "To address the challenges physicians face with lengthy, unstructured radiology reports, leading to missed critical information.", "method": "The study employed private large language models (LLMs) deployed locally to structure and condense 814 radiology reports using five prompting strategies within the LangChain framework.", "result": "The Mixtral LLM showed the best results in adhering to formatting requirements, reducing word counts by over 53%, and improving clarity in radiology reporting.", "conclusion": "Locally-deployed, open-source LLMs can significantly enhance the efficiency of radiology reporting, meeting the needs of referring physicians and improving clinical workflows.", "key_contributions": ["Utilization of private LLMs for creating structured radiology reports", "Demonstrated superiority of Mixtral LLM in formatting", "Reduction of verbosity and improvement in clarity of reports"], "limitations": "", "keywords": ["radiology reports", "large language models", "clinical workflows", "information retrieval", "structured formatting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.10371", "pdf": "https://arxiv.org/pdf/2411.10371.pdf", "abs": "https://arxiv.org/abs/2411.10371", "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects", "authors": ["Qing Cheng", "Zefan Zeng", "Xingchen Hu", "Yuehang Si", "Zhong Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Causality Identification (ECI) has emerged as a pivotal task in natural\nlanguage processing (NLP), aimed at automatically detecting causal\nrelationships between events in text. In this comprehensive survey, we\nsystematically elucidate the foundational principles and technical frameworks\nof ECI, proposing a novel classification framework to categorize and clarify\nexisting methods. {We discuss associated challenges, provide quantitative\nevaluations, and outline future directions for this dynamic and rapidly\nevolving field. We first delineate key definitions, problem formalization, and\nevaluation protocols of ECI. Our classification framework organizes ECI methods\nbased on two primary tasks: Sentence-level Event Causality Identification\n(SECI) and Document-level Event Causality Identification (DECI). For SECI, we\nreview methods including feature pattern-based matching, machine learning-based\nclassification, deep semantic encoding, prompt-based fine-tuning, and causal\nknowledge pre-training, alongside common data augmentation strategies. For\nDECI, we focus on techniques such as deep semantic encoding, event graph\nreasoning, and prompt-based fine-tuning. We dedicate specific discussions to\nadvancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI\nleveraging Large Language Models (LLMs). Furthermore, we analyze the strengths,\nlimitations, and unresolved challenges of each method. Extensive quantitative\nevaluations are conducted on four benchmark datasets to assess various ECI\nmethods. Finally, we explore future research directions.", "AI": {"tldr": "This survey on Event Causality Identification (ECI) reviews methods to detect causal relationships in text, offering a classification framework and discussing challenges, evaluations, and future directions.", "motivation": "To systematically elucidate the foundational principles and frameworks of Event Causality Identification (ECI) in NLP.", "method": "The paper proposes a novel classification framework that organizes ECI methods into Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). It reviews various methods including machine learning, deep learning, and prompt-based techniques, and conducts quantitative evaluations on benchmark datasets.", "result": "The evaluation of various ECI methods showed strengths and limitations across different models and approaches.", "conclusion": "The paper highlights the current state of ECI, discusses unresolved challenges, and outlines potential future research directions.", "key_contributions": ["Introduction of a novel classification framework for ECI methods.", "Extensive quantitative evaluations on benchmark datasets assessing various ECI methods.", "Discussion on advancements in multilingual, cross-lingual, and zero-shot ECI using LLMs."], "limitations": "Challenges and unresolved aspects of current ECI methods are noted.", "keywords": ["Event Causality Identification", "Natural Language Processing", "Machine Learning", "Large Language Models", "Quantitative Evaluation"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2412.05237", "pdf": "https://arxiv.org/pdf/2412.05237.pdf", "abs": "https://arxiv.org/abs/2412.05237", "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale", "authors": ["Jarvis Guo", "Tuney Zheng", "Yuelin Bai", "Bo Li", "Yubo Wang", "King Zhu", "Yizhi Li", "Graham Neubig", "Wenhu Chen", "Xiang Yue"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Open-source multimodal large language models (MLLMs) have shown significant\npotential in a broad range of multimodal tasks. However, their reasoning\ncapabilities remain constrained by existing instruction-tuning datasets, which\nwere predominately repurposed from academic datasets such as VQA, AI2D, and\nChartQA. These datasets target simplistic tasks, and only provide phrase-level\nanswers without any intermediate rationales. To address these challenges, we\nintroduce a scalable and cost-effective method to construct a large-scale\nmultimodal instruction-tuning dataset with rich intermediate rationales\ndesigned to elicit CoT reasoning. Using only open models, we create a dataset\ncontaining 12M instruction-response pairs to cover diverse, reasoning-intensive\ntasks with detailed and faithful rationales. Experiments demonstrate that\ntraining MLLMs on this dataset significantly improves reasoning capabilities,\nachieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%),\nMMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates\nnotable improvements of up to 4% on non-reasoning-based benchmarks. Ablation\nstudies further highlight the importance of key components, such as rewriting\nand self-filtering, in the dataset construction process.", "AI": {"tldr": "This paper presents a novel dataset creation method for training multimodal large language models (MLLMs) that focuses on enhanced reasoning capabilities through rich intermediate rationales.", "motivation": "To improve the reasoning abilities of open-source MLLMs, which are currently limited by existing simplistic instruction-tuning datasets.", "method": "The authors developed a scalable method to construct a large-scale multimodal instruction-tuning dataset with 12M instruction-response pairs that include detailed rationales to support Chain-of-Thought (CoT) reasoning.", "result": "Training MLLMs on the new dataset led to significant improvements in reasoning performance, achieving state-of-the-art results on multiple benchmarks including MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%).", "conclusion": "The new dataset construction approach greatly enhances MLLMs' reasoning capabilities, with important findings on the necessity of components like rewriting and self-filtering during the dataset creation process.", "key_contributions": ["Development of a new multimodal instruction-tuning dataset with elaborate rationales.", "Demonstration of improved reasoning capabilities in MLLMs through comprehensive experimentation.", "Insights into critical dataset construction techniques, such as rewriting and self-filtering."], "limitations": "", "keywords": ["multimodal large language models", "instruction-tuning dataset", "reasoning capabilities", "Chain-of-Thought reasoning", "dataset construction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.15255", "pdf": "https://arxiv.org/pdf/2412.15255.pdf", "abs": "https://arxiv.org/abs/2412.15255", "title": "Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation", "authors": ["Jonibek Mansurov", "Akhmed Sakip", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "In this paper, we show that knowledge distillation can be subverted to\nmanipulate language model benchmark scores, revealing a critical vulnerability\nin current evaluation practices. We introduce \"Data Laundering,\" a process that\nenables the covert transfer of benchmark-specific knowledge through seemingly\nlegitimate intermediate training steps. Through extensive experiments with a\n2-layer BERT student model, we show how this approach can achieve substantial\nimprovements in benchmark accuracy (up to 75\\% on GPQA) without developing\ngenuine reasoning capabilities. Notably, this method can be exploited\nintentionally or even unintentionally, as researchers may inadvertently adopt\nthis method and inflate scores without realising the implications. While our\nfindings demonstrate the effectiveness of this technique, we present them as a\ncautionary tale highlighting the urgent need for more robust evaluation methods\nin AI. This work aims to contribute to the ongoing discussion about evaluation\nintegrity in AI development and the need for benchmarks that more accurately\nreflect true model capabilities. The code is available at\nhttps://github.com/mbzuai-nlp/data_laundering.", "AI": {"tldr": "This paper reveals vulnerabilities in language model evaluations through a method called 'Data Laundering,' which can inflate benchmark scores without improving true reasoning capabilities.", "motivation": "To highlight critical vulnerabilities in current evaluation practices of language models and emphasize the need for more robust evaluation methods.", "method": "The paper introduces 'Data Laundering,' a process that manipulates language model benchmark scores by transferring benchmark-specific knowledge through intermediate training steps.", "result": "The method demonstrated substantial improvements in benchmark accuracy (up to 75% on GPQA) using a 2-layer BERT student model without developing genuine reasoning capabilities.", "conclusion": "The findings serve as a cautionary tale regarding the integrity of evaluation methods in AI, urging for the development of benchmarks that better reflect true model capabilities.", "key_contributions": ["Introduction of 'Data Laundering' as a method for manipulating benchmark scores", "Demonstration of the technique's effectiveness in improving benchmark performance", "Highlighting the need for more robust evaluation practices in AI."], "limitations": "", "keywords": ["Knowledge Distillation", "Data Laundering", "Language Models", "Evaluation Integrity"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2412.21006", "pdf": "https://arxiv.org/pdf/2412.21006.pdf", "abs": "https://arxiv.org/abs/2412.21006", "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria", "authors": ["Joonwon Jang", "Jaehee Kim", "Wonbin Kweon", "Seonghyeon Lee", "Hwanjo Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 FINDINGS", "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While this approach has proven effective,\nit inevitably increases substantial inference costs. Previous methods adopting\ntoken-level reduction without clear criteria result in poor performance\ncompared to models trained with complete rationale. To address this challenge,\nwe propose a novel sentence-level rationale reduction framework leveraging\nlikelihood-based criteria, verbosity, to identify and remove redundant\nreasoning sentences. Unlike previous approaches, our method leverages verbosity\nto selectively remove redundant reasoning sentences while preserving reasoning\ncapabilities. Our experimental results across various reasoning tasks\ndemonstrate that our method improves performance by an average of 7.71% while\nreducing token generation by 19.87% compared to model trained with complete\nreasoning paths.", "AI": {"tldr": "This paper introduces a sentence-level rationale reduction framework that optimizes inference costs while preserving reasoning capabilities in large language models.", "motivation": "To address the increased inference costs in large language models (LLMs) caused by extensive intermediate reasoning units, while maintaining performance on complex tasks.", "method": "A novel framework that utilizes likelihood-based criteria, specifically verbosity, to identify and remove redundant reasoning sentences during inference.", "result": "The proposed method improves performance by an average of 7.71% and reduces token generation by 19.87% compared to models trained with complete reasoning paths.", "conclusion": "The framework effectively balances performance and efficiency in reasoning tasks, contributing to more cost-effective applications of LLMs.", "key_contributions": ["Introduces a sentence-level rationale reduction framework for LLMs.", "Utilizes likelihood-based criteria for identifying redundant sentences.", "Demonstrates significant performance improvements and reductions in inference costs."], "limitations": "", "keywords": ["Large Language Models", "Inference Costs", "Rationale Reduction", "Verbosity", "Reasoning Tasks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.05855", "pdf": "https://arxiv.org/pdf/2501.05855.pdf", "abs": "https://arxiv.org/abs/2501.05855", "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability", "authors": ["Antonin PochÃ©", "Alon Jacovi", "Agustin Martin Picard", "Victor Boutin", "Fanny Jourdan"], "categories": ["cs.CL"], "comment": null, "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.", "AI": {"tldr": "This paper introduces an evaluation framework for concept-based explanations using automated simulatability, particularly leveraging large language models (LLMs) to assess explanation methods.", "motivation": "Existing evaluation metrics for concept-based explanations fail to consider the effectiveness of communicating chosen concepts to users alongside the quality of the induced concept space.", "method": "The authors propose an evaluation framework that measures concept explanations by simulating a model's outputs based on provided explanations, utilizing LLMs as simulators for empirical evaluation.", "result": "The evaluation framework allows for consistent and scalable assessment of explanation methods across various models and datasets, showing that LLMs can reliably rank these methods.", "conclusion": "The proposed method provides a new avenue for rigorous evaluation of concept-based explanations and facilitates broader empirical studies.", "key_contributions": ["Introduction of automated simulatability for evaluating concept explanations", "Use of LLMs as simulators for scalable evaluation", "Comprehensive empirical evaluation demonstrating consistent rankings of explanation methods"], "limitations": "Human studies for simulatability remain challenging and might not capture all nuances of user understanding.", "keywords": ["concept-based explanations", "automated simulatability", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.16748", "pdf": "https://arxiv.org/pdf/2501.16748.pdf", "abs": "https://arxiv.org/abs/2501.16748", "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions", "authors": ["Garima Chhikara", "Abhishek Kumar", "Abhijnan Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.", "AI": {"tldr": "This study evaluates LLMs' ability to recognize and respond to Indian cultural practices, highlighting challenges in balancing cultural diversity within AI systems.", "motivation": "To address concerns about cultural bias in LLMs, particularly regarding their interactions with under-represented subcultures.", "method": "The paper utilizes case studies to assess LLMs' recognition of Little Traditions in Indian society and tests various prompting strategies, including the use of regional languages.", "result": "The study finds LLMs can articulate cultural nuances but often fail to apply this understanding in practical contexts.", "conclusion": "This is the first analysis of LLMs engagement with Indian subcultures, providing insights into the difficulties of integrating cultural diversity into AI systems.", "key_contributions": ["First study analyzing LLMs' engagement with Indian subcultures", "Evaluation of cultural responsiveness of LLMs", "Insights on prompting strategies to enhance cultural sensitivity"], "limitations": "LLMs' understanding often fails in context-specific scenarios despite showing awareness of cultural nuances.", "keywords": ["cultural bias", "Little Traditions", "Large Language Models", "AI systems", "cultural diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.13656", "pdf": "https://arxiv.org/pdf/2502.13656.pdf", "abs": "https://arxiv.org/abs/2502.13656", "title": "Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models", "authors": ["Liyang He", "Chenglong Liu", "Rui Li", "Zhenya Huang", "Shulan Ruan", "Jun Zhou", "Enhong Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.", "AI": {"tldr": "This paper introduces a novel approach for controlling the generation direction of LLMs to enhance sentence embedding without heavy reliance on manual annotations.", "motivation": "To address the limitations of existing contrastive learning methods that depend on manually annotated datasets, thereby improving the scalability of sentence embedding.", "method": "The authors propose a controlled generation method for large language models (LLMs) in the latent space, integrating ranking information and semantic information into existing sentence embedding models.", "result": "Experiments on multiple benchmarks indicate that the proposed method achieves new state-of-the-art performance in sentence embedding tasks with only a modest increase in the cost of ranking sentence synthesis.", "conclusion": "The proposed approach effectively balances the need for semantic distinction in embeddings while minimizing the reliance on labeled data, setting a new standard in the field.", "key_contributions": ["Introduced a controlled generation method for LLMs to improve sentence embeddings.", "Integrated ranking information into sentence embedding models for enhanced semantic distinction.", "Achieved state-of-the-art performance on multiple benchmarks."], "limitations": "", "keywords": ["Sentence Embedding", "Contrastive Learning", "Large Language Models", "NLP", "Ranking Information"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13946", "pdf": "https://arxiv.org/pdf/2502.13946.pdf", "abs": "https://arxiv.org/abs/2502.13946", "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", "authors": ["Chak Tou Leong", "Qingyu Yin", "Jian Wang", "Wenjie Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ACL 2025 Main", "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.", "AI": {"tldr": "This paper investigates vulnerabilities in large language models (LLMs) related to safety alignment caused by template dependence and proposes methods for improvement.", "motivation": "To address vulnerabilities in large language models due to their dependence on fixed templates during safety alignment, which can be exploited by jailbreak attacks.", "method": "The authors conduct extensive experiments on various aligned LLMs and perform mechanistic analyses to explore the impact of template-anchored safety alignment on model vulnerability.", "result": "The experiments reveal that template-anchored safety alignment is a common issue across LLMs, leading to susceptibility under certain inference-time attacks.", "conclusion": "Detaching safety mechanisms from input templates shows promise in improving the robustness of LLMs against jailbreak attacks; further research is needed to advance safety alignment techniques.", "key_contributions": ["Identification of template-anchored safety alignment vulnerabilities in LLMs", "Mechanistic analysis of susceptibility to jailbreak attacks", "Proposed solutions to mitigate risks associated with template dependence"], "limitations": "", "keywords": ["large language models", "safety alignment", "jailbreak attacks", "template dependence", "robustness"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.14019", "pdf": "https://arxiv.org/pdf/2502.14019.pdf", "abs": "https://arxiv.org/abs/2502.14019", "title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems", "authors": ["Myra Cheng", "Su Lin Blodgett", "Alicia DeVrio", "Lisa Egede", "Alexandra Olteanu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "ACL 2025", "summary": "As text generation systems' outputs are increasingly anthropomorphic --\nperceived as human-like -- scholars have also increasingly raised concerns\nabout how such outputs can lead to harmful outcomes, such as users over-relying\nor developing emotional dependence on these systems. How to intervene on such\nsystem outputs to mitigate anthropomorphic behaviors and their attendant\nharmful outcomes, however, remains understudied. With this work, we aim to\nprovide empirical and theoretical grounding for developing such interventions.\nTo do so, we compile an inventory of interventions grounded both in prior\nliterature and a crowdsourcing study where participants edited system outputs\nto make them less human-like. Drawing on this inventory, we also develop a\nconceptual framework to help characterize the landscape of possible\ninterventions, articulate distinctions between different types of\ninterventions, and provide a theoretical basis for evaluating the effectiveness\nof different interventions.", "AI": {"tldr": "This paper investigates interventions to reduce harmful anthropomorphic behaviors in text generation systems that users may overly rely on or become emotionally dependent upon.", "motivation": "The increasing perception of text generation systems as human-like raises concerns about users developing harmful dependencies and over-reliance on these systems.", "method": "The authors compile an inventory of interventions based on literature and a crowdsourcing study, then develop a conceptual framework to classify and evaluate these interventions.", "result": "The paper outlines various types of interventions and provides a theoretical basis to assess their effectiveness in mitigating anthropomorphic behavior.", "conclusion": "The study highlights the need for empirical and theoretical work to effectively intervene on anthropomorphic outputs in text generation systems.", "key_contributions": ["Compilation of interventions from literature and user input", "Development of a conceptual framework for categorizing interventions", "Provision of a theoretical basis for evaluating intervention effectiveness"], "limitations": "", "keywords": ["anthropomorphic behavior", "interventions", "text generation", "human-computer interaction", "emotional dependence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.14748", "pdf": "https://arxiv.org/pdf/2502.14748.pdf", "abs": "https://arxiv.org/abs/2502.14748", "title": "Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models", "authors": ["Zongxia Li", "Lorena Calvo-BartolomÃ©", "Alexander Hoyle", "Paiheng Xu", "Alden Dima", "Juan Francisco Fung", "Jordan Boyd-Graber"], "categories": ["cs.CL"], "comment": "22 Pages. LLM for Data Exploration and content analysis, Topic\n  Models. 63rd Annual Meeting of the Association for Computational Linguistics\n  (2025)", "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.", "AI": {"tldr": "The study compares LLM-based methods and traditional topic models for understanding large document collections, revealing strengths and limitations of each approach.", "motivation": "To explore the effectiveness of Large Language Models (LLMs) versus traditional topic models in understanding large datasets in real-world applications.", "method": "Evaluation of knowledge acquisition from LLM-based exploratory approaches and traditional topic models on two datasets, measuring readability of topics and average win probabilities.", "result": "LLM-based methods yield more human-readable topics and higher win probabilities, but generate overly generic topics for domain-specific data. Human supervision enhances exploration but increases user effort; traditional models remain effective but are less user-friendly.", "conclusion": "LLMs require human assistance for effective domain-specific data exploration due to limitations in scaling and propensity for hallucination.", "key_contributions": ["Empirical comparison of LLMs and traditional models in data exploration", "Insights on the impact of human supervision on LLM outputs", "Identification of limitations in LLM performance with domain-specific datasets"], "limitations": "LLMs struggle with generative specificity for domain-specific datasets and have scaling and hallucination constraints due to context length.", "keywords": ["NLP", "Large Language Models", "topic models", "data exploration", "human supervision"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.16540", "pdf": "https://arxiv.org/pdf/2502.16540.pdf", "abs": "https://arxiv.org/abs/2502.16540", "title": "D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models", "authors": ["Hong Cai Chen", "Yi Pin Xu", "Yang Zhang"], "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.IR", "cs.LG"], "comment": "14 pages, 18 figures", "summary": "In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design.", "AI": {"tldr": "D2S-FLOW is an automated framework using LLMs to extract electrical parameters from datasheets and generate SPICE models, enhancing efficiency and precision.", "motivation": "Manual extraction of component parameters from extensive documents for SPICE models is labor-intensive and time-consuming.", "method": "D2S-FLOW utilizes three mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN) to improve precision and efficiency in document handling.", "result": "The framework achieves an Exact Match of 0.86, F1 score of 0.92, and Exact Correctness of 0.96, outperforming the strongest baseline significantly. It also reduces API token consumption by 38% and minimizes irrelevant information to 4%.", "conclusion": "D2S-FLOW presents a substantial improvement in automating circuit design, showcasing high efficiency and precision in extracting parameters.", "key_contributions": ["Introduction of D2S-FLOW framework", "Three innovative mechanisms for improved document handling", "Significant performance improvements over baseline systems"], "limitations": "", "keywords": ["Large Language Models", "SPICE models", "Automated extraction", "Circuit design", "Document processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.18845", "pdf": "https://arxiv.org/pdf/2502.18845.pdf", "abs": "https://arxiv.org/abs/2502.18845", "title": "Sliding Window Attention Training for Efficient Large Language Models", "authors": ["Zichuan Fu", "Wentao Song", "Yejing Wang", "Xian Wu", "Yefeng Zheng", "Yingying Zhang", "Derong Xu", "Xuetao Wei", "Tong Xu", "Xiangyu Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps://github.com/Fzkuji/swat-attention.", "AI": {"tldr": "SWAT is a new model that enhances the efficiency of long-context handling in Transformer-based architectures by replacing softmax with the sigmoid function and employing balanced ALiBi and Rotary Position Embedding.", "motivation": "To address the computational inefficiency of Transformers with long sequences due to quadratic complexity and the attention sink phenomenon.", "method": "SWAT introduces a novel approach by substituting the softmax operation with the sigmoid function, integrating balanced ALiBi and Rotary Position Embedding for effective information compression.", "result": "Experiments show that SWAT achieves state-of-the-art performance on eight benchmarks, outperforming existing linear recurrent architectures.", "conclusion": "SWAT represents a simple yet effective modification to the Transformer architecture, maintaining its fundamental structure while improving efficiency for long-context processing.", "key_contributions": ["Introduction of Sliding Window Attention Training (SWAT) for efficiency", "Replacement of softmax with sigmoid for improved performance", "Utilization of balanced ALiBi and Rotary Position Embedding."], "limitations": "", "keywords": ["Long Language Models", "Sliding Window Attention", "Transformer Efficiency", "ALiBi", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2502.19582", "pdf": "https://arxiv.org/pdf/2502.19582.pdf", "abs": "https://arxiv.org/abs/2502.19582", "title": "Where Are We? Evaluating LLM Performance on African Languages", "authors": ["Ife Adebara", "Hawau Olamide Toyin", "Nahom Tesfu Ghebremichael", "AbdelRahim Elmadany", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities.", "AI": {"tldr": "This paper evaluates the performance of large language models on African languages using a comprehensive benchmark, highlighting the impact of data inequities on model effectiveness.", "motivation": "To address the underrepresentation of Africa's linguistic heritage in NLP due to historical policies favoring foreign languages and data inequities.", "method": "The paper integrates theoretical insights with an empirical evaluation using Sahara, a benchmark of large-scale publicly accessible datasets that capture Africa's linguistic diversity.", "result": "The assessment reveals that while some languages perform reasonably well, many Indigenous languages are marginalized due to sparse data, with significant variations in model performance linked to policy-induced data disparities.", "conclusion": "The findings call for actionable policy reforms and inclusive data practices to foster linguistic diversity in AI, emphasizing the need for both theoretical understanding and empirical assessment.", "key_contributions": ["Empirical evaluation of LLM performance on a comprehensive benchmark for African languages.", "Insights into how historical policies affect NLP and data representation for Indigenous languages.", "Recommendations for policy reforms to improve data practices in AI for African languages."], "limitations": "Focuses primarily on large language models; may not cover other types of NLP applications.", "keywords": ["large language models", "African languages", "data equity", "NLP", "linguistic diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.01622", "pdf": "https://arxiv.org/pdf/2503.01622.pdf", "abs": "https://arxiv.org/abs/2503.01622", "title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation", "authors": ["Eliya Habba", "Ofir Arviv", "Itay Itzhak", "Yotam Perlitz", "Elron Bandel", "Leshem Choshen", "Michal Shmueli-Scheuer", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/", "AI": {"tldr": "DOVE is a dataset aimed at evaluating LLM sensitivity to prompt variations, offering insights into effective prompt design and model performance.", "motivation": "To address the issues with current single-prompt evaluation practices by examining LLM sensitivity to various prompt perturbations holistically.", "method": "DOVE includes a large-scale dataset with thousands of prompt perturbations for various evaluation benchmarks, focusing on joint effects of multiple dimensions.", "result": "Findings include methods for optimizing prompt selection, the impact of few-shot examples on reducing sensitivity, and the identification of consistently hard instances across perturbations.", "conclusion": "DOVE provides a resource to improve LLM evaluation methodologies and encourages community involvement in testing and refining prompt performance.", "key_contributions": ["Introduces the DOVE dataset with over 250M prompt perturbations", "Examines LLM sensitivity from a holistic perspective", "Identifies effective prompt design strategies and model output characteristics"], "limitations": "", "keywords": ["LLM evaluation", "prompt perturbations", "dataset", "model sensitivity", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.03962", "pdf": "https://arxiv.org/pdf/2503.03962.pdf", "abs": "https://arxiv.org/abs/2503.03962", "title": "On the Acquisition of Shared Grammatical Representations in Bilingual Language Models", "authors": ["Catherine Arnett", "Tyler A. Chang", "James A. Michaelov", "Benjamin K. Bergen"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures. Accepted at ACL 2025", "summary": "Crosslingual transfer is crucial to contemporary language models'\nmultilingual capabilities, but how it occurs is not well understood. We ask\nwhat happens to a monolingual language model when it begins to be trained on a\nsecond language. Specifically, we train small bilingual models for which we\ncontrol the amount of data for each language and the order of language\nexposure. To find evidence of shared multilingual representations, we turn to\nstructural priming, a method used to study grammatical representations in\nhumans. We first replicate previous crosslingual structural priming results and\nfind that after controlling for training data quantity and language exposure,\nthere are asymmetrical effects across language pairs and directions. We argue\nthat this asymmetry may shape hypotheses about human structural priming\neffects. We also find that structural priming effects are less robust for less\nsimilar language pairs, highlighting potential limitations of crosslingual\ntransfer learning and shared representations for typologically diverse\nlanguages.", "AI": {"tldr": "The paper investigates the effects of training monolingual models on bilingual data and how this influences crosslingual transfer, particularly through structural priming.", "motivation": "Understanding the mechanisms behind crosslingual transfer in multilingual language models is vital for improving their effectiveness and comprehension.", "method": "Small bilingual models were trained with varying amounts of data for each language and different orders of language exposure. Structural priming methods were used to examine grammatical representations.", "result": "Asymmetrical effects were observed across different language pairs and training conditions, indicating that language similarity influences structural priming outcomes.", "conclusion": "The asymmetry in priming effects may provide insights into human linguistic processing and highlights the limitations of crosslingual transfer for languages that are typologically diverse.", "key_contributions": ["Demonstrates the impact of language exposure order on transfer learning effects.", "Highlights asymmetries in structural priming across different language pairs.", "Identifies challenges in applying crosslingual transfer to diverse languages."], "limitations": "Structural priming effects are less robust for less similar languages, indicating potential limitations in crosslingual representations.", "keywords": ["crosslingual transfer", "structural priming", "multilingual models", "transfer learning", "language similarity"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2503.04721", "pdf": "https://arxiv.org/pdf/2503.04721.pdf", "abs": "https://arxiv.org/abs/2503.04721", "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities", "authors": ["Guan-Ting Lin", "Jiachen Lian", "Tingle Li", "Qirui Wang", "Gopala Anumanchipalli", "Alexander H. Liu", "Hung-yi Lee"], "categories": ["cs.CL", "eess.AS"], "comment": "Work in Progress", "summary": "Spoken dialogue modeling poses challenges beyond text-based language\nmodeling, requiring real-time interaction, turn-taking, and backchanneling.\nWhile most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing\none turn at a time - emerging full-duplex SDMs can listen and speak\nsimultaneously, enabling more natural conversations. However, current\nevaluations remain limited, focusing mainly on turn-based metrics or coarse\ncorpus-level analyses. To address this, we introduce Full-Duplex-Bench, a\nbenchmark that systematically evaluates key interactive behaviors: pause\nhandling, backchanneling, turn-taking, and interruption management. Our\nframework uses automatic metrics for consistent, reproducible assessment and\nprovides a fair, fast evaluation setup. By releasing our benchmark and code, we\naim to advance spoken dialogue modeling and foster the development of more\nnatural and engaging SDMs.", "AI": {"tldr": "This paper introduces Full-Duplex-Bench, a benchmark for evaluating full-duplex spoken dialogue models (SDMs) that enables simultaneous listening and speaking, assessing key interactive behaviors beyond traditional metrics.", "motivation": "The field of spoken dialogue modeling faces limitations in evaluation methods, primarily focusing on turn-based interactions. The need for a benchmark that captures the complexities of real-time communication is critical for developing more natural SDMs.", "method": "The authors developed Full-Duplex-Bench, which systematically evaluates interactive behaviors like pause handling, backchanneling, turn-taking, and interruption management using automatic metrics for reproducibility.", "result": "The benchmark provides a rapid and fair evaluation setup for full-duplex SDMs, aiming to facilitate advancements in spoken dialogue systems by presenting clear assessment criteria.", "conclusion": "By releasing Full-Duplex-Bench and its accompanying code, the authors hope to promote more effective development and evaluation of spoken dialogue models, contributing to more engaging conversational agents.", "key_contributions": ["Introduction of Full-Duplex-Bench for evaluating full-duplex spoken dialogue models.", "Focus on interactive behaviors such as pause handling and backchanneling.", "Provision of a reproducible and fast evaluation framework."], "limitations": "Current evaluations are still limited to automatic metrics and may not fully capture human conversational nuances.", "keywords": ["Spoken Dialogue Models", "Benchmark", "Evaluation", "Full-Duplex", "Natural Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.06706", "pdf": "https://arxiv.org/pdf/2503.06706.pdf", "abs": "https://arxiv.org/abs/2503.06706", "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts", "authors": ["Ming Zhang", "Yuhui Wang", "Yujiong Shen", "Tingyi Yang", "Changhao Jiang", "Yilong Wu", "Shihan Dou", "Qinhao Chen", "Zhiheng Xi", "Zhihao Zhang", "Yi Dong", "Zhen Wang", "Zhihui Fei", "Mingyang Wan", "Tao Liang", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.", "AI": {"tldr": "The study presents the PFDial dataset for process-driven dialogue systems and evaluates LLMs in constrained dialogue tasks.", "motivation": "To improve dialogue systems in customer service and equipment maintenance by providing a dataset with predefined process constraints.", "method": "Construction of the PFDial dataset from UML flowcharts, followed by training various models to evaluate their performance on dialogue instructions.", "result": "Models, including a 7B and an 8B model, demonstrated over 90% accuracy on dialogue tasks, outperforming GPT-4o.", "conclusion": "The findings highlight the effectiveness of LLMs in constrained dialogue tasks and the importance of dataset formats on model performance.", "key_contributions": ["Introduction of the PFDial dataset", "High accuracy achieved by smaller LLMs on constrained dialogue tasks", "In-depth analysis of dataset formats and their impact on model performance"], "limitations": "", "keywords": ["process-driven dialogue", "large language models", "dataset", "customer service", "UML flowcharts"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.08042", "pdf": "https://arxiv.org/pdf/2503.08042.pdf", "abs": "https://arxiv.org/abs/2503.08042", "title": "LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data", "authors": ["Naomi Baes", "RaphaÃ«l Merx", "Nick Haslam", "Ekaterina Vylomova", "Haim Dubossarsky"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings (9-page long paper; 35 pages total including\n  limitations, appendices and references)", "summary": "Lexical Semantic Change (LSC) provides insight into cultural and social\ndynamics. Yet, the validity of methods for measuring different kinds of LSC\nremains unestablished due to the absence of historical benchmark datasets. To\naddress this gap, we propose LSC-Eval, a novel three-stage general-purpose\nevaluation framework to: (1) develop a scalable methodology for generating\nsynthetic datasets that simulate theory-driven LSC using In-Context Learning\nand a lexical database; (2) use these datasets to evaluate the sensitivity of\ncomputational methods to synthetic change; and (3) assess their suitability for\ndetecting change in specific dimensions and domains. We apply LSC-Eval to\nsimulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions,\nas defined in the SIBling framework, using examples from psychology. We then\nevaluate the ability of selected methods to detect these controlled\ninterventions. Our findings validate the use of synthetic benchmarks,\ndemonstrate that tailored methods effectively detect changes along SIB\ndimensions, and reveal that a state-of-the-art LSC model faces challenges in\ndetecting affective dimensions of LSC. LSC-Eval offers a valuable tool for\ndimension- and domain-specific benchmarking of LSC methods, with particular\nrelevance to the social sciences.", "AI": {"tldr": "The paper introduces LSC-Eval, a framework to evaluate methods for measuring Lexical Semantic Change (LSC) using synthetic datasets.", "motivation": "To address the lack of historical benchmark datasets for measuring different kinds of Lexical Semantic Change, which are essential for evaluating computational methods in social sciences.", "method": "A three-stage framework is developed: generating synthetic datasets simulating LSC, evaluating computational methods' sensitivity to changes, and assessing the methods' suitability for specific domains.", "result": "The study validated the use of synthetic benchmarks and demonstrated that tailored methods are effective in detecting changes along defined dimensions, while revealing challenges faced by state-of-the-art models in identifying affective dimensions of LSC.", "conclusion": "LSC-Eval provides a valuable tool for dimension- and domain-specific benchmarking of LSC methods, particularly in social sciences.", "key_contributions": ["Development of LSC-Eval framework for evaluating LSC methods.", "Created synthetic datasets for simulating LSC changes.", "Revealed limitations of existing LSC models in detecting affective changes."], "limitations": "The paper may have limitations in generalizability beyond the tested dimensions and domains.", "keywords": ["Lexical Semantic Change", "Evaluation Framework", "Synthetic Datasets", "In-Context Learning", "Social Sciences"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2503.10267", "pdf": "https://arxiv.org/pdf/2503.10267.pdf", "abs": "https://arxiv.org/abs/2503.10267", "title": "An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)", "authors": ["Laurie Burchell", "Ona de Gibert", "Nikolay Arefyev", "Mikko Aulamo", "Marta BaÃ±Ã³n", "Pinzhen Chen", "Mariia Fedorova", "Liane Guillou", "Barry Haddow", "Jan HajiÄ", "JindÅich Helcl", "Erik Henriksson", "Mateusz Klimaszewski", "Ville Komulainen", "Andrey Kutuzov", "Joona KytÃ¶niemi", "Veronika Laippala", "Petter MÃ¦hlum", "Bhavitvya Malik", "Farrokh Mehryary", "Vladislav Mikhailov", "Nikita Moghe", "Amanda Myntti", "DayyÃ¡n O'Brien", "Stephan Oepen", "Proyag Pal", "Jousia Piha", "Sampo Pyysalo", "Gema RamÃ­rez-SÃ¡nchez", "David Samuel", "Pavel Stepachev", "JÃ¶rg Tiedemann", "DuÅ¡an VariÅ¡", "Tereza VojtÄchovÃ¡", "Jaume Zaragoza-Bernabeu"], "categories": ["cs.CL"], "comment": "ACL'2025 Main Proceedings", "summary": "Training state-of-the-art large language models requires vast amounts of\nclean and diverse textual data. However, building suitable multilingual\ndatasets remains a challenge. In this work, we present HPLT v2, a collection of\nhigh-quality multilingual monolingual and parallel corpora, extending prior\nwork of the HPLT project. The monolingual portion of the data contains 8T\ntokens covering 193 languages, while the parallel data contains 380M sentence\npairs covering 51 languages. We document the entire data pipeline and release\nthe code to reproduce it. We provide extensive analysis of the quality and\ncharacteristics of our data. Finally, we evaluate the performance of language\nmodels and machine translation systems trained on HPLT v2, demonstrating its\nvalue.", "AI": {"tldr": "HPLT v2 presents a rich multilingual dataset for training large language models, with extensive documentation and evaluation.", "motivation": "Building suitable multilingual datasets for training large language models is a significant challenge.", "method": "We introduce HPLT v2, a collection of high-quality multilingual datasets including monolingual and parallel corpora, and document the entire data pipeline.", "result": "HPLT v2 includes 8T tokens in 193 languages and 380M sentence pairs in 51 languages, with a thorough analysis of data quality and model performance evaluations.", "conclusion": "The evaluation demonstrates the superiority of models trained on HPLT v2 for both natural language processing and machine translation tasks.", "key_contributions": ["Introduction of a large-scale multilingual dataset (HPLT v2) for training language models.", "Documented data pipeline for reproducibility.", "Extensive performance evaluation of language models and machine translation systems using HPLT v2."], "limitations": "", "keywords": ["multilingual datasets", "large language models", "machine translation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.10515", "pdf": "https://arxiv.org/pdf/2503.10515.pdf", "abs": "https://arxiv.org/abs/2503.10515", "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set", "authors": ["Florian Eichin", "Yang Janet Liu", "Barbara Plank", "Michael A. Hedderich"], "categories": ["cs.CL"], "comment": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes, camera-ready revision for ACL\n  2025", "summary": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.", "AI": {"tldr": "Investigation of LLMs' ability to capture and generalize discourse knowledge across languages and frameworks.", "motivation": "Understanding discourse is vital for NLP tasks, yet current discourse representations are framework-dependent, limiting their applicability.", "method": "Developed a unified discourse relation label set and probed 23 LLMs of various sizes to evaluate their discourse knowledge generalization.", "result": "LLMs, particularly those trained on multilingual data, show capability to generalize discourse relations across languages; significant insights from layer-wise analyses indicate that this generalization is most prominent in intermediate layers.", "conclusion": "The study confirms LLMs possess generalizable discourse knowledge, aiding in cross-lingual and cross-framework discourse tasks; comprehensive error analysis suggests specific challenging areas.", "key_contributions": ["Unified discourse relation label set for analysis", "Evaluation of 23 LLMs' discourse knowledge", "Layer-wise analysis revealing generalization in intermediate layers"], "limitations": "The research may face limitations regarding the diversity of discourse relations and the evaluation framework used.", "keywords": ["discourse understanding", "large language models", "multilingual NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.15850", "pdf": "https://arxiv.org/pdf/2503.15850.pdf", "abs": "https://arxiv.org/abs/2503.15850", "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "authors": ["Xiaoou Liu", "Tiejin Chen", "Longchao Da", "Chacha Chen", "Zhen Lin", "Hua Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.", "AI": {"tldr": "This paper addresses the reliability concerns of Large Language Models (LLMs) in high-stakes fields by exploring uncertainty quantification (UQ) methods.", "motivation": "The reliability of LLMs is critical as they are increasingly adopted in sensitive areas like healthcare and law, where incorrect outputs can have significant consequences. Uncertainty quantification can help enhance trustworthiness by estimating confidence in the generated outputs.", "method": "The paper introduces a new taxonomy for uncertainty quantification methods based on computational efficiency and various uncertainty dimensions, including input, reasoning, parameter, and prediction uncertainty. It also evaluates existing techniques and their applicability in real-world situations.", "result": "The study highlights unique uncertainty sources introduced by LLMs, which traditional UQ methods struggle to address, identifying a need for more scalable and interpretable approaches.", "conclusion": "A robust approach to uncertainty quantification is necessary to improve the reliability of LLMs, suggesting that advancements in UQ could facilitate more responsible use of these models in critical applications.", "key_contributions": ["Introduction of a new taxonomy for UQ methods specific to LLMs", "Evaluation of existing UQ techniques in the context of LLM reliability", "Identification of open challenges and the need for scalable and interpretable UQ approaches."], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Quantification", "Healthcare", "Reliability", "AI"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512.pdf", "abs": "https://arxiv.org/abs/2503.23512", "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Xinhang Yuan", "Li Sun", "Yi Xin", "Keqin Li", "Kuan Lu", "Menghao Huo", "Jiaqi Chen", "Tianyu Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives.", "AI": {"tldr": "Introducing SCORE, a framework for enhancing narrative coherence in AI-generated stories.", "motivation": "To address the challenge of maintaining coherence and emotional depth in narratives generated by large language models.", "method": "SCORE employs a Retrieval-Augmented Generation (RAG) approach that utilizes TF-IDF and cosine similarity to detect narrative inconsistencies and enhance story structure by tracking item statuses and generating episode summaries.", "result": "SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models in LLM-generated stories.", "conclusion": "The framework offers a more robust method for evaluating and refining AI-generated narratives.", "key_contributions": ["Development of the SCORE framework for narrative coherence", "Application of Retrieval-Augmented Generation techniques", "Demonstrated improvement over baseline models in narrative consistency"], "limitations": "", "keywords": ["Large Language Models", "narrative coherence", "Retrieval-Augmented Generation", "AI-generated stories", "story structure"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.23899", "pdf": "https://arxiv.org/pdf/2503.23899.pdf", "abs": "https://arxiv.org/abs/2503.23899", "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset", "authors": ["Diana Galvan-Sosa", "Gabrielle Gaudeau", "Pride Kavumba", "Yunmeng Li", "Hongyi gu", "Zheng Yuan", "Keisuke Sakaguchi", "Paula Buttery"], "categories": ["cs.CL", "I.2.7"], "comment": "10 main pages (24 appendix pages), 9 figures, accepted to ACL 2025", "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube.", "AI": {"tldr": "This paper presents Rubrik's CUBE, a rubric and dataset for evaluating the quality of explanations generated by Large-Language Models (LLMs).", "motivation": "Despite the adoption of LLMs in generating explanations, their reliability remains questionable, leading to the need for a robust evaluation framework.", "method": "The authors introduce Rubrik's CUBE, an education-inspired rubric, and a dataset of 26,000 explanations, quality-annotated by humans and LLMs focusing on various reasoning and language tasks.", "result": "Analysis reveals that low-quality explanations are primarily due to a lack of conciseness, influenced by the task and perceived difficulty rather than cohesion or word choice.", "conclusion": "The findings highlight the need for improved evaluation of LLM-generated explanations; the dataset and resources are made available to the community.", "key_contributions": ["Introduction of Rubrik's CUBE for evaluating LLM explanations", "Creation of a large dataset of annotated explanations", "Insights into factors affecting the quality of LLM explanations"], "limitations": "The dataset may not cover all possible explanation tasks or scenarios.", "keywords": ["Large-Language Models", "explanation generation", "evaluation rubric", "dataset", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2504.00030", "pdf": "https://arxiv.org/pdf/2504.00030.pdf", "abs": "https://arxiv.org/abs/2504.00030", "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding", "authors": ["Aayush Gautam", "Susav Shrestha", "Narasimha Reddy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 1 table", "summary": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.", "AI": {"tldr": "GammaTune and GammaTune+ are adaptive algorithms for optimizing speculation length in speculative decoding for LLM inference, achieving a significant speedup.", "motivation": "To enhance the efficiency of LLM inference through optimal speculation length while reducing computation waste.", "method": "GammaTune and GammaTune+ utilize a training-free adaptive approach that adjusts speculation length based on token acceptance rates with a heuristic-based switching mechanism.", "result": "The proposed methods, evaluated on SpecBench, achieve an average speedup of 15% (Â±5%) for GammaTune and 16% (Â±3%) for GammaTune+, outperforming existing methods and reducing performance variance.", "conclusion": "GammaTune provides a robust and efficient solution for real-world deployment in LLM inference environments.", "key_contributions": ["Introduction of training-free algorithms for adaptive speculation length adjustment", "Demonstrated significant speedup in LLM inference", "Reduction in performance variance compared to fixed-length approaches"], "limitations": "", "keywords": ["speculative decoding", "large language models", "adaptive algorithms", "inference optimization", "GammaTune"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154.pdf", "abs": "https://arxiv.org/abs/2504.05154", "title": "CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL"], "comment": "27 pages", "summary": "Language Models (LMs) are typically tuned with human preferences to produce\nhelpful responses, but the impact of preference tuning on the ability to handle\nculturally diverse queries remains understudied. In this paper, we\nsystematically analyze how native human cultural preferences can be\nincorporated into the preference learning process to train more culturally\naware LMs. We introduce \\textbf{CARE}, a multilingual resource containing 3,490\nculturally specific questions and 31.7k responses with native judgments. We\ndemonstrate how a modest amount of high-quality native preferences improves\ncultural awareness across various LMs, outperforming larger generic preference\ndata. Our analyses reveal that models with stronger initial cultural\nperformance benefit more from alignment, leading to gaps among models developed\nin different regions with varying access to culturally relevant data. CARE will\nbe made publicly available at https://github.com/Guochry/CARE.", "AI": {"tldr": "This paper introduces CARE, a resource for training culturally aware language models (LMs) using native human cultural preferences.", "motivation": "To improve language models' handling of culturally diverse queries and incorporate native human cultural preferences in preference tuning.", "method": "The paper presents CARE, a multilingual resource with culturally specific questions and responses, and analyzes its impact on training culturally aware LMs.", "result": "Incorporating native preferences from CARE significantly enhances cultural awareness in various LMs, outperforming larger generic data sets.", "conclusion": "Models with better initial cultural performance show more significant improvement from using native cultural data; this research highlights regional disparities in access to culturally relevant data.", "key_contributions": ["Introduction of CARE, a multilingual resource for culturally specific query training", "Demonstration of improved cultural awareness in LMs through native preference incorporation", "Analysis of the disparity in cultural performance among models from different regions"], "limitations": "The study does not explore the long-term effects of using culturally aware models in real-world applications.", "keywords": ["Cultural Awareness", "Language Models", "Human Preferences", "Multilingual", "CARE"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2504.05276", "pdf": "https://arxiv.org/pdf/2504.05276.pdf", "abs": "https://arxiv.org/abs/2504.05276", "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "categories": ["cs.CL"], "comment": "EDM 2025 Short Paper", "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains.", "AI": {"tldr": "This paper presents an adaptive retrieval-augmented generation (RAG) framework for automated grading in science education, improving grading accuracy by incorporating domain-specific knowledge.", "motivation": "To improve the automated grading process in science education by addressing the limitations of large language models (LLMs) in domain knowledge and task-specific understanding.", "method": "The proposed adaptive RAG framework dynamically retrieves domain-specific knowledge during assessment by combining semantic search and curated educational sources based on the context of questions and student answers.", "result": "Experimental results indicate that the RAG framework significantly enhances grading accuracy compared to baseline LLM approaches in a science education dataset.", "conclusion": "The RAG-enhanced grading system shows potential as a reliable tool to support human graders with improved performance in evaluating student responses.", "key_contributions": ["Development of an adaptive RAG framework for automated grading", "Demonstration of improved grading accuracy with domain-specific knowledge retrieval", "Integration of semantic search with curated educational resources"], "limitations": "", "keywords": ["RAG", "automated grading", "large language models", "science education", "domain-specific knowledge"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.06910", "pdf": "https://arxiv.org/pdf/2504.06910.pdf", "abs": "https://arxiv.org/abs/2504.06910", "title": "Identifying Aspects in Peer Reviews", "authors": ["Sheng Lu", "Ilia Kuznetsov", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": null, "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review.", "AI": {"tldr": "This paper proposes a data-driven approach to identify aspects in peer reviews to improve the peer review process and support computational methods.", "motivation": "The increasing volume of academic submissions is straining the peer review process, necessitating computational support and standardization.", "method": "The authors define a conceptual framework for 'aspect' in peer reviews and develop a data-driven schema to extract these aspects from a corpus of peer reviews.", "result": "They introduce an augmented dataset of peer reviews with identified aspects and demonstrate the implications for community-level review analysis and LLM-generated review detection.", "conclusion": "The findings establish a foundation for using NLP to enhance peer review methodologies and applications.", "key_contributions": ["Proposes an operational definition of aspect in peer reviews", "Develops a data-driven schema for aspect identification from peer reviews", "Introduces an augmented dataset for community-level analysis of peer reviews"], "limitations": "The practicality of the approach may depend on the diversity and quality of the data used for aspect extraction.", "keywords": ["peer review", "aspect identification", "data-driven schema", "NLP", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.13677", "pdf": "https://arxiv.org/pdf/2504.13677.pdf", "abs": "https://arxiv.org/abs/2504.13677", "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "authors": ["Andrea Santilli", "Adam Golinski", "Michael Kirchhof", "Federico Danieli", "Arno Blaas", "Miao Xiong", "Luca Zappella", "Sinead Williamson"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025 (Main)", "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving\ntheir safety and reliability. Evaluations often use metrics like AUROC to\nassess how well UQ methods (e.g., negative sequence probabilities) correlate\nwith task correctness functions (e.g., ROUGE-L). We show that mutual\nbiases--when both UQ methods and correctness functions are biased by the same\nfactors--systematically distort evaluation. First, we formally prove that any\nmutual bias non-randomly skews AUROC rankings, compromising benchmark\nintegrity. Second, we confirm this happens empirically by testing 7 widely used\ncorrectness functions, from lexical-based and embedding-based metrics to\nLM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our\nanalysis shows that length biases in correctness functions distort UQ\nassessments by interacting with length biases in UQ methods. We identify\nLM-as-a-judge methods as the least length-biased, offering a promising path for\na fairer UQ evaluation.", "AI": {"tldr": "The paper investigates the impact of mutual biases on the evaluation of uncertainty quantification methods in language models, concluding that length biases distort assessments and proposing LM-as-a-judge methods as more reliable.", "motivation": "To enhance safety and reliability in language models through better uncertainty quantification evaluations.", "method": "Analyzes the biases in UQ methods and correctness functions using theoretical proofs and empirical tests across multiple datasets, models, and evaluation metrics.", "result": "Demonstrated that mutual biases skew AUROC rankings non-randomly, compromising benchmark integrity and highlighting the distortion caused by length biases during evaluation.", "conclusion": "LM-as-a-judge methods show the least length bias, potentially leading to fairer evaluations of uncertainty quantification methods in language models.", "key_contributions": ["Proof of biases affecting AUROC rankings in UQ evaluations", "Empirical validation with multiple metrics and datasets", "Identification of LM-as-a-judge methods as a promising evaluation approach"], "limitations": "", "keywords": ["Uncertainty Quantification", "Language Models", "Evaluation Metrics", "Biases", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.14175", "pdf": "https://arxiv.org/pdf/2504.14175.pdf", "abs": "https://arxiv.org/abs/2504.14175", "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025 (Findings)", "summary": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyze whether the generated documents contain information\nentailed by ground-truth evidence and assess their impact on performance. Our\nfindings indicate that, on average, performance improvements consistently\noccurred for claims whose generated documents included sentences entailed by\ngold evidence. This suggests that knowledge leakage may be present in\nfact-verification benchmarks, potentially inflating the perceived performance\nof LLM-based query expansion methods.", "AI": {"tldr": "The paper investigates the impact of knowledge leakage on the performance of LLM-powered query expansion methods in fact verification tasks, suggesting that apparent improvements may be due to previously known information rather than genuine enhancement.", "motivation": "To challenge the assumption that LLMs can generate useful hypothetical documents for query expansion in retrieval tasks without knowledge leakage affecting benchmark results.", "method": "Analyzed the performance of LLM-generated documents in a zero-shot retrieval context, focusing on fact verification tasks and assessing the entailed information against ground-truth evidence.", "result": "Performance improvements were consistently associated with claims whose LLM-generated documents contained sentences that were entailed by actual evidence, indicating potential knowledge leakage in benchmarks.", "conclusion": "The findings imply that knowledge leakage may inflate the perceived effectiveness of query expansion methods utilizing LLMs in retrieval tasks, raising questions about benchmark integrity.", "key_contributions": ["Investigation of knowledge leakage in LLM query expansion methods", "Analysis using fact verification as a testbed", "Insights into the integrity of fact verification benchmarks"], "limitations": "The study primarily focuses on a specific testbed (fact verification) and may not generalize across all retrieval tasks.", "keywords": ["query expansion", "large language models", "fact verification", "knowledge leakage", "retrieval performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23276", "pdf": "https://arxiv.org/pdf/2505.23276.pdf", "abs": "https://arxiv.org/abs/2505.23276", "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text", "authors": ["Maged S. Al-Shaibani", "Moataz Ahmed"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.", "AI": {"tldr": "This paper investigates Arabic machine-generated text, revealing detectable patterns in LLM outputs across various contexts and developing effective BERT-based detection models.", "motivation": "To address the significant challenges posed by misinformation generated by LLMs in low-resource languages like Arabic, particularly in critical domains such as healthcare and education.", "method": "A comprehensive analysis of Arabic machine-generated text was conducted, exploring multiple generation strategies and model architectures, followed by stylometric analysis to differentiate human and machine outputs, culminating in the development of BERT-based detection models.", "result": "The analysis showed distinctive linguistic patterns in machine-generated Arabic text, and the BERT-based models achieved up to 99.9% F1-score in detecting formal contexts, despite generalization challenges.", "conclusion": "This work is the most comprehensive investigation of Arabic machine-generated text, providing insights that are essential for building robust detection systems to preserve information integrity.", "key_contributions": ["Comprehensive analysis of Arabic machine-generated text using various generation strategies and model architectures.", "Stylometric analysis revealing unique patterns between human-written and machine-generated text.", "Development of high-performance BERT-based detection models for Arabic text."], "limitations": "Generalization challenges in detection across different contexts were noted, which requires future attention.", "keywords": ["Machine Learning", "Natural Language Processing", "Arabic Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865.pdf", "abs": "https://arxiv.org/abs/2504.13865", "title": "A Survey on (M)LLM-Based GUI Agents", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.", "AI": {"tldr": "This survey examines the advancements in LLM-based GUI Agents, focusing on their components, evaluation methodologies, and future developments.", "motivation": "To provide a comprehensive understanding of LLM-based GUI Agents and their role in modern HCI.", "method": "Systematic analysis of architectural foundations, technical components, and evaluation methodologies of GUI Agents.", "result": "Identified four fundamental components of GUI Agents: perception systems, exploration mechanisms, planning frameworks, and interaction systems; analyzed their technological advancements and evaluation frameworks.", "conclusion": "The survey reveals critical challenges and future research directions for enhancing GUI Agents' capabilities in intelligent interface automation.", "key_contributions": ["Comprehensive examination of LLM-based GUI Agents and their components.", "Analysis of technical challenges in current GUI automation.", "Proposed directions for standardization in evaluation methodologies."], "limitations": "Methodological limitations in existing evaluation benchmarks.", "keywords": ["GUI Agents", "Human-Computer Interaction", "Large Language Models", "Multimodal Learning", "Evaluation Frameworks"], "importance_score": 9, "read_time_minutes": 15}}
