{"id": "2507.12547", "pdf": "https://arxiv.org/pdf/2507.12547.pdf", "abs": "https://arxiv.org/abs/2507.12547", "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "This paper proposes the Model Synthesis Architecture (MSA) as a computational implementation of how humans use combined distributed and symbolic representations to reason in novel situations. It evaluates MSA using a unique dataset based on sports vignettes, demonstrating its superior performance over standard language models in capturing human-like reasoning.", "motivation": "To understand how people draw on global background knowledge and reason coherently in novel situations.", "method": "The paper introduces the Model Synthesis Architecture (MSA) that combines language models for global relevance-based retrieval and probabilistic programs for coherent world model synthesis. It evaluates this model against human judgments using a novel reasoning dataset focusing on sports scenarios.", "result": "The MSA captures human judgments more effectively than language model-only baselines, indicating that it can mirror human reasoning capabilities in open-ended scenarios.", "conclusion": "The MSA approach offers insights into replicating human reasoning in complex domains, suggesting a feasible computational framework for better understanding human cognitive processes.", "key_contributions": ["Introduction of the Model Synthesis Architecture (MSA) for reasoning", "Evaluation against a novel reasoning dataset related to sports", "Demonstration of MSA's effectiveness over conventional language models"], "limitations": "", "keywords": ["Model Synthesis Architecture", "human reasoning", "language models", "open-ended reasoning", "probabilistic programming"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.12553", "pdf": "https://arxiv.org/pdf/2507.12553.pdf", "abs": "https://arxiv.org/abs/2507.12553", "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "This paper explores how language models discern modal categories of sentences and identifies reliable linear representations, termed modal difference vectors, that demonstrate consistent patterns as model competence increases.", "motivation": "The paper addresses concerns regarding language models' ability to categorize sentences based on modality, which has implications for their application in various tasks such as question answering and narrative construction.", "method": "The authors identify and analyze modal difference vectors in language model activations and correlate these with human categorization behaviors through interpretability techniques.", "result": "The findings reveal that language models have more reliable modalities than previously reported and that the emergence of modal difference vectors correlates with model competence, effectively modeling human categorization behaviors.", "conclusion": "The insights drawn from the analysis of modal difference vectors could enhance our understanding of both language models and human modal categorization, with potential applications in improving language model performance.", "key_contributions": ["Identification of modal difference vectors in language models", "Demonstration of consistent emergence of modal categorization as models improve", "Correlation of model activations with human categorization behavior"], "limitations": "", "keywords": ["language models", "modal categorization", "mechanistic interpretability", "human categorization", "NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.12672", "pdf": "https://arxiv.org/pdf/2507.12672.pdf", "abs": "https://arxiv.org/abs/2507.12672", "title": "The first open machine translation system for the Chechen language", "authors": ["Abu-Viskhan A. Umishov", "Vladislav A. Grigorian"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "AI": {"tldr": "This paper presents an open-source translation model for the Chechen language and Russian, including a training dataset and evaluation metrics.", "motivation": "To create a translation model for the Chechen language, which is under-resourced, and to support multilingual capabilities in the NLLB-200 system.", "method": "The authors developed a translation model and fine-tuned it using collected parallel datasets for Chechen and Russian.", "result": "The model achieved BLEU scores of 8.34 (Russian to Chechen) and 20.89 (Chechen to Russian), indicating its efficacy in translation tasks.", "conclusion": "The paper highlights the need for multilingual translation systems and provides resources for further research on the Chechen language.", "key_contributions": ["First open-source translation model for the Chechen language and Russian.", "Creation and provision of a dataset for model training and evaluation.", "Release of parallel corpora and a multilingual sentence encoder for Chechen."], "limitations": "The low BLEU scores suggest room for improvement in translation quality.", "keywords": ["Chechen language", "translation model", "multilingual translation", "NLLB-200", "language resources"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2507.12679", "pdf": "https://arxiv.org/pdf/2507.12679.pdf", "abs": "https://arxiv.org/abs/2507.12679", "title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "authors": ["Arthur J. Funnell", "Panayiotis Petousis", "Fabrice Harel-Canada", "Ruby Romero", "Alex A. T. Bui", "Adam Koncsol", "Hritika Chaturvedi", "Chelsea Shover", "David Goodman-Meza"], "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "comment": "30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)", "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "AI": {"tldr": "This paper explores the use of NLP models, specifically fine-tuned BioClinicalBERT, to automate the classification of drug-related deaths from unstructured coroner reports, addressing issues related to traditional ICD-10 coding.", "motivation": "To improve the timeliness and accuracy of overdose surveillance, especially given the increasing drug-related deaths linked to fentanyl in the US.", "method": "The study employed a dataset of 35,433 death records for training and internal testing of various NLP approaches, including traditional classifiers and fine-tuned models like BioClinicalBERT. External validation used a separate dataset of 3,335 records.", "result": "Fine-tuned BioClinicalBERT models achieved exceptional performance on both internal (F1 >= 0.998) and external validation (F1 = 0.966), outperforming traditional machine learning methods and other large language models.", "conclusion": "NLP models, particularly BioClinicalBERT, provide a highly accurate and scalable method for classifying overdose deaths from free-text reports, enhancing surveillance workflows and enabling faster detection of substance use trends.", "key_contributions": ["Demonstration of NLP's effectiveness in interpreting unstructured coroner reports for overdose classification", "Creation of a novel dataset for external validation of NLP models", "Comparison of various NLP approaches, highlighting the superiority of fine-tuned clinical models"], "limitations": "The study primarily focuses on specific NLP models and may not generalize to all forms of death reporting or overdose data classification.", "keywords": ["NLP", "overdose surveillance", "BioClinicalBERT", "drug-related deaths", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12580", "pdf": "https://arxiv.org/pdf/2507.12580.pdf", "abs": "https://arxiv.org/abs/2507.12580", "title": "\"How to Explore Biases in Speech Emotion AI with Users?\" A Speech-Emotion-Acting Study Exploring Age and Language Biases", "authors": ["Josephine Beatrice Skovbo Borre", "Malene Gorm Wold", "Sara Kj√¶r Rasmussen", "Ilhan Aslan"], "categories": ["cs.HC"], "comment": "20 pages", "summary": "This study explores how age and language shape the deliberate vocal\nexpression of emotion, addressing underexplored user groups, Teenagers (N = 12)\nand Adults 55+ (N = 12), within speech emotion recognition (SER). While most\nSER systems are trained on spontaneous, monolingual English data, our research\nevaluates how such models interpret intentionally performed emotional speech\nacross age groups and languages (Danish and English). To support this, we\ndeveloped a novel experimental paradigm combining a custom user interface with\na backend for real-time SER prediction and data logging. Participants were\nprompted to hit visual targets in valence-arousal space by deliberately\nexpressing four emotion targets. While limitations include some reliance on\nself-managed voice recordings and inconsistent task execution, the results\nsuggest contrary to expectations, no significant differences between language\nor age groups, and a degree of cross-linguistic and age robustness in model\ninterpretation. Though some limitations in high-arousal emotion recognition\nwere evident. Our qualitative findings highlight the need to move beyond\nsystem-centered accuracy metrics and embrace more inclusive, human-centered SER\nmodels. By framing emotional expression as a goal-directed act and logging the\nreal-time gap between human intent and machine interpretation, we expose the\nrisks of affective misalignment.", "AI": {"tldr": "This study investigates how age and language affect vocal emotional expression in speech emotion recognition (SER) across Teenagers and Adults 55+, revealing no significant differences in SER model interpretations despite limitations.", "motivation": "To explore the interaction between age, language, and deliberate vocal expression of emotion in speech emotion recognition (SER), particularly focusing on underexplored groups.", "method": "Developed a custom user interface combined with a backend for real-time SER prediction to evaluate intentional emotional speech in different age groups and languages (Danish and English).", "result": "No significant differences in SER interpretations across age or language groups were found, suggesting robustness of SER models despite some limitations in high-arousal emotion recognition.", "conclusion": "Highlights the importance of moving towards more human-centered SER models that account for affective misalignment between human intent and machine interpretation.", "key_contributions": ["Provides new insights into user groups often overlooked in SER studies.", "Develops a novel experimental paradigm for evaluating SER across age and language.", "Recommends a shift towards inclusive SER models that consider human emotional intent."], "limitations": "Some reliance on self-managed voice recordings and inconsistent task execution.", "keywords": ["Speech Emotion Recognition", "Human-Computer Interaction", "Age and Language"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.12695", "pdf": "https://arxiv.org/pdf/2507.12695.pdf", "abs": "https://arxiv.org/abs/2507.12695", "title": "AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis", "authors": ["S M Rafiuddin", "Sadia Kamal", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen"], "categories": ["cs.CL"], "comment": "12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)", "summary": "We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.", "AI": {"tldr": "AdaptiSent is a new framework for Multimodal Aspect-Based Sentiment Analysis that enhances sentiment classification by using adaptive cross-modal attention mechanisms and demonstrates superior performance on Twitter datasets.", "motivation": "The goal is to improve sentiment classification and aspect term extraction from multimodal data, particularly integrating text and images.", "method": "The framework employs adaptive cross-modal attention with dynamic modality weighting and context-adaptive attention mechanisms to enhance sentiment and aspect extraction.", "result": "AdaptiSent outperformed traditional and other multimodal methods in precision, recall, and F1 score on standard Twitter datasets, particularly excelling at identifying complex inter-modal relationships.", "conclusion": "The model sets a new standard in MABSA, showing significant improvements in analyzing multimodal information.", "key_contributions": ["Introduction of adaptive cross-modal attention mechanisms for sentiment analysis.", "Dynamic modality weighting that improves aspect extraction.", "Enhanced performance on nuanced sentiment analysis tasks over existing models."], "limitations": "", "keywords": ["Multimodal Sentiment Analysis", "Aspect Extraction", "Adaptive Attention"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.12621", "pdf": "https://arxiv.org/pdf/2507.12621.pdf", "abs": "https://arxiv.org/abs/2507.12621", "title": "NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting", "authors": ["Kuangshi Ai", "Kaiyuan Tang", "Chaoli Wang"], "categories": ["cs.HC", "cs.GR", "cs.MA"], "comment": "IEEE VIS 2025. Project Page: https://nli4volvis.github.io/", "summary": "Traditional volume visualization (VolVis) methods, like direct volume\nrendering, suffer from rigid transfer function designs and high computational\ncosts. Although novel view synthesis approaches enhance rendering efficiency,\nthey require additional learning effort for non-experts and lack support for\nsemantic-level interaction. To bridge this gap, we propose NLI4VolVis, an\ninteractive system that enables users to explore, query, and edit volumetric\nscenes using natural language. NLI4VolVis integrates multi-view semantic\nsegmentation and vision-language models to extract and understand semantic\ncomponents in a scene. We introduce a multi-agent large language model\narchitecture equipped with extensive function-calling tools to interpret user\nintents and execute visualization tasks. The agents leverage external tools and\ndeclarative VolVis commands to interact with the VolVis engine powered by 3D\neditable Gaussians, enabling open-vocabulary object querying, real-time scene\nediting, best-view selection, and 2D stylization. We validate our system\nthrough case studies and a user study, highlighting its improved accessibility\nand usability in volumetric data exploration. We strongly recommend readers\ncheck our case studies, demo video, and source code at\nhttps://nli4volvis.github.io/.", "AI": {"tldr": "NLI4VolVis is an interactive system that allows users to explore and edit volumetric scenes using natural language, improving usability and accessibility in volume visualization.", "motivation": "Traditional volume visualization methods are limited by rigid transfer functions and high computational costs, which hinder user interaction and efficiency.", "method": "NLI4VolVis employs multi-view semantic segmentation and vision-language models to analyze and interpret user intents, utilizing a multi-agent large language model architecture with function-calling tools for interactive visualization tasks.", "result": "The system facilitates open-vocabulary object querying, real-time scene editing, and best-view selection, demonstrating improved user accessibility and efficiency through case studies and user testing.", "conclusion": "NLI4VolVis significantly enhances the interactive capabilities of volumetric data visualization while being accessible to non-expert users.", "key_contributions": ["Interactive volumetric visualization using natural language", "Integration of vision-language models with semantic segmentation", "Real-time scene editing and object querying capabilities"], "limitations": "", "keywords": ["Volume Visualization", "Natural Language Interaction", "Semantic Segmentation"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.12705", "pdf": "https://arxiv.org/pdf/2507.12705.pdf", "abs": "https://arxiv.org/abs/2507.12705", "title": "AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Michael J. Ryan", "Ali Sartaz Khan", "Warit Sirichotedumrong", "Kunat Pipatanakul", "William Held", "Diyi Yang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Current speech evaluation suffers from two critical limitations: the need and\ndifficulty of designing specialized systems targeting individual audio\ncharacteristics, and poor correlation between automatic evaluation methods and\nhuman preferences. This work presents a systematic study of Large Audio Model\n(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified\nevaluation framework that addresses both challenges. We systematically explore\nAudioJudge across audio characteristic detection tasks, including\npronunciation, speaking rate, speaker identification and speech quality, and\nsystem-level human preference simulation for automated benchmarking. We\ninvestigate different prompt engineering strategies, finding that audio\nconcatenation combined with in-context learning significantly improves\nperformance across both audio characteristic detection and human preference\nsimulation tasks. We further introduce a multi-aspect ensemble AudioJudge to\nenable general-purpose multi-aspect audio evaluation. This method decomposes\nspeech assessment into specialized judges for lexical content, speech quality,\nand paralinguistic features, achieving up to 0.91 Spearman correlation with\nhuman preferences on our system ranking benchmark. Robustness analysis reveals\nthat while LAMs maintain strong performance under acoustic noise, they exhibit\nsignificant verbosity and positional biases that require careful mitigation.", "AI": {"tldr": "This paper introduces AudioJudge, a Large Audio Model-based system for unified audio evaluation, addressing limitations in current speech evaluation methods.", "motivation": "The need for specialized systems in speech evaluation and the poor correlation between automatic methods and human preferences motivates the investigation of AudioJudge as a unified evaluation framework.", "method": "A systematic study of AudioJudge is conducted, focusing on audio characteristic detection tasks such as pronunciation and speech quality, using various prompt engineering strategies and a multi-aspect ensemble approach for comprehensive evaluation.", "result": "AudioJudge achieves a Spearman correlation of up to 0.91 with human preferences on benchmark tasks, indicating significant improvement in performance and versatility across multiple audio characteristics.", "conclusion": "The proposed method provides an effective solution for audio evaluation but highlights the need to address verbosity and positional biases in LAM models.", "key_contributions": ["Introduction of AudioJudge for unified audio evaluation", "Successful implementation of multi-aspect ensemble approach", "Significant correlation with human preferences in speech assessment."], "limitations": "LAMs show significant verbosity and positional biases that need careful mitigation.", "keywords": ["Audio Evaluation", "Large Audio Model", "Human Preference Simulation", "Speech Assessment", "Prompt Engineering"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.12721", "pdf": "https://arxiv.org/pdf/2507.12721.pdf", "abs": "https://arxiv.org/abs/2507.12721", "title": "Design Patterns of Human-AI Interfaces in Healthcare", "authors": ["Rui Sheng", "Chuhan Shi", "Sobhan Lotfi", "Shiyi Liu", "Adam Perer", "Huamin Qu", "Furui Cheng"], "categories": ["cs.HC"], "comment": null, "summary": "Human-AI interfaces play a crucial role in advancing practices and research\nwithin the healthcare domain. However, designing such interfaces presents a\nsubstantial challenge for designers. In this paper, we propose systematic\nguidance for designing human-AI interfaces in typical healthcare scenarios by\nsummarizing the design patterns for presenting and interacting with common\ninformation entities. To deepen our understanding of these 12 design patterns,\nwe interviewed 12 healthcare professionals to explore potential usage scenarios\nand important considerations. Furthermore, we conducted workshops with 14\nparticipants recruited online to evaluate our design patterns. Finally, we\ndiscussed the generalizability of the design patterns to other application\ndomains, the limitations, and the future work.", "AI": {"tldr": "The paper provides guidance for designing human-AI interfaces in healthcare by summarizing 12 design patterns, based on interviews and workshops with healthcare professionals.", "motivation": "To improve the design of human-AI interfaces in the healthcare domain, which is essential for effective human-AI collaboration.", "method": "Interviews with 12 healthcare professionals and workshops with 14 participants to evaluate and explore usage scenarios for 12 identified design patterns.", "result": "The study identified and summarized 12 design patterns that enhance human-AI interactions in healthcare, providing a systematic framework for interface designers.", "conclusion": "The design patterns are potentially generalizable to other application domains, and the work highlights limitations and future research directions.", "key_contributions": ["Systematic guidance for designing human-AI interfaces in healthcare.", "Identification and evaluation of 12 key design patterns.", "Insights from healthcare professionals on usage scenarios and considerations."], "limitations": "The generalizability of the design patterns beyond healthcare has not been extensively tested yet.", "keywords": ["Human-AI interfaces", "Healthcare", "Design patterns", "User experience", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12720", "pdf": "https://arxiv.org/pdf/2507.12720.pdf", "abs": "https://arxiv.org/abs/2507.12720", "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens", "AI": {"tldr": "This paper introduces FLEXITOKENS, a byte-level language model with a learnable tokenizer that adapts to new data distributions, reducing token over-fragmentation and improving performance on various benchmarks.", "motivation": "To address the inefficiency of existing subword tokenizers that hinder the adaptation of language models to new or diverse data distributions.", "method": "FLEXITOKENS employs a boundary predictor that learns to create variable-length segments from input byte sequences, enhancing flexibility without enforcing a fixed compression rate.", "result": "FLEXITOKENS consistently reduces token over-fragmentation and shows up to 10% improvement in performance on multilingual benchmarks compared to traditional subword tokenizers.", "conclusion": "The approach demonstrates significant enhancements in adaptability of language models to diverse languages and tasks, promoting better performance and reduced fragmentation in tokenization.", "key_contributions": ["Introduction of FLEXITOKENS for adaptive tokenization", "Demonstration of improved performance on multilingual tasks", "Reduction of token over-fragmentation through variable-length segment encoding"], "limitations": "", "keywords": ["language models", "adaptive tokenization", "FLEXITOKENS"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.12734", "pdf": "https://arxiv.org/pdf/2507.12734.pdf", "abs": "https://arxiv.org/abs/2507.12734", "title": "An Age-based Study into Interactive Narrative Visualization Engagement", "authors": ["Nina Errey", "Yi Chen", "Yu Dong", "Quang Vinh Nguyen", "Xiaoru Yuan", "Tuck Wah Leong", "Christy Jie Liang"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Research has shown that an audiences' age impacts their engagement in digital\nmedia. Interactive narrative visualization is an increasingly popular form of\ndigital media that combines data visualization and storytelling to convey\nimportant information. However, audience age is often overlooked by interactive\nnarrative visualization authors. Using an established visualization engagement\nquestionnaire, we ran an empirical experiment where we compared end-user\nengagement to audience age. We found a small difference in engagement scores\nwhere older age cohorts were less engaged than the youngest age cohort. Our\nqualitative analysis revealed that the terminology and overall understanding of\ninteractive narrative patterns integrated into narrative visualization was more\napparent in the feedback from younger age cohorts relative to the older age\ncohorts. We conclude this paper with a series of recommendations for authors of\ninteractive narrative visualization on how to design inclusively for audiences\naccording to their age.", "AI": {"tldr": "Age impacts engagement in interactive narrative visualization, with younger audiences showing higher engagement levels.", "motivation": "To explore how audience age affects engagement in interactive narrative visualization and address this gap in literature.", "method": "An empirical experiment using an established visualization engagement questionnaire to compare engagement scores across different age cohorts.", "result": "Younger audiences showed higher engagement scores than older ones, and qualitative feedback indicated a better understanding of narrative patterns among younger participants.", "conclusion": "The paper recommends inclusive design strategies for interactive narrative visualizations tailored to different age groups.", "key_contributions": ["Empirical evidence of age-related engagement differences in narrative visualizations.", "Qualitative insights into audience interactions with narrative patterns.", "Practical recommendations for inclusive design in interactive visualizations."], "limitations": "The study may not account for other demographic factors that could influence engagement.", "keywords": ["interactive visualization", "audience engagement", "age differences", "inclusive design", "data storytelling"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12724", "pdf": "https://arxiv.org/pdf/2507.12724.pdf", "abs": "https://arxiv.org/abs/2507.12724", "title": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations", "authors": ["Richard Sproat", "Tianyu Zhao", "Llion Jones"], "categories": ["cs.CL"], "comment": null, "summary": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.", "AI": {"tldr": "TransEvalnia is a prompting-based translation evaluation system that provides finetuned scoring and ranking of translations, outperforming state-of-the-art methods on various language pairs.", "motivation": "To improve the accuracy and granularity of translation evaluation using reasoning and fine-grained metrics.", "method": "TransEvalnia employs prompt-based evaluation, utilizing a subset of the Multidimensional Quality Metrics and leveraging LLMs like Claude-3.5-Sonnet for assessing translations across multiple dimensions.", "result": "TransEvalnia matches or surpasses the performance of the MT-Ranker on English-Japanese and other WMT datasets, with high correlation of LLM scores to human assessments.", "conclusion": "The system is sensitive to translation presentation order, which can affect evaluations, and proposes solutions for this bias while releasing all data and code for public use.", "key_contributions": ["Introduction of a novel prompting-based evaluation system for translations", "Demonstrated better or equivalent performance compared to the MT-Ranker", "Addressing position bias in translation evaluations."], "limitations": "Sensitivity to the order of presented translations.", "keywords": ["Translation Evaluation", "Machine Translation", "Natural Language Processing", "Language Models", "Position Bias"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12741", "pdf": "https://arxiv.org/pdf/2507.12741.pdf", "abs": "https://arxiv.org/abs/2507.12741", "title": "Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land", "authors": ["Lotfi El Hafi", "Kazuma Onishi", "Shoichi Hasegawa", "Akira Oyama", "Tomochika Ishikawa", "Masashi Osada", "Carl Tornberg", "Ryoma Kado", "Kento Murata", "Saki Hashimoto", "Sebastian Carrera Villalobos", "Akira Taniguchi", "Gustavo Alfonso Garcia Ricardez", "Yoshinobu Hagiwara", "Tatsuya Aoki", "Kensuke Iwata", "Takato Horii", "Yukiko Horikawa", "Takahiro Miyashita", "Tadahiro Taniguchi", "Hiroshi Ishiguro"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Advanced Robotics and its Social Impacts (ARSO), Osaka, Japan", "summary": "Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,\nenabling individuals to overcome physical limitations through virtual agents\nand robotic assistants. While semi-autonomous CAs intermittently require human\nteleoperation and supervision, the deployment of fully autonomous CAs remains a\nchallenge. This study evaluates public perception and potential social impacts\nof fully autonomous CAs for physical support in daily life. To this end, we\nconducted a large-scale demonstration and survey during Avatar Land, a 19-day\npublic event in Osaka, Japan, where fully autonomous robotic CAs, alongside\nsemi-autonomous CAs, performed daily object retrieval tasks. Specifically, we\nanalyzed responses from 2,285 visitors who engaged with various CAs, including\na subset of 333 participants who interacted with fully autonomous CAs and\nshared their perceptions and concerns through a survey questionnaire. The\nsurvey results indicate interest in CAs for physical support in daily life and\nat work. However, concerns were raised regarding task execution reliability. In\ncontrast, cost and human-like interaction were not dominant concerns. Project\npage: https://lotfielhafi.github.io/FACA-Survey/.", "AI": {"tldr": "The study assesses public perceptions of fully autonomous cybernetic avatars (CAs) for physical support, highlighting interest and concerns regarding task reliability based on survey responses from a large-scale demonstration.", "motivation": "To evaluate public perception and potential social impacts of fully autonomous cybernetic avatars (CAs) in addressing physical limitations in daily life.", "method": "Conducted a large-scale demonstration and survey during Avatar Land in Osaka, Japan, with 2,285 visitors engaging with various CAs, alongside a focused survey for 333 participants who interacted with fully autonomous CAs.", "result": "Survey results show high interest in CAs for physical support, but significant concerns regarding reliability of task execution were noted, while cost and human-like interaction were less concerning.", "conclusion": "The findings demonstrate a positive inclination towards integrating fully autonomous CAs in daily life, though addressing reliability is crucial for broader acceptance.", "key_contributions": ["Large-scale evaluation of public perceptions towards fully autonomous cybernetic avatars.", "Insights on reliability concerns versus other factors like cost and interaction quality.", "Data-driven understanding of societal impacts from integrating CAs in everyday tasks."], "limitations": "Limited to survey responses from a specific event and location, which may not generalize to broader populations.", "keywords": ["cybernetic avatars", "human-robot interaction", "autonomous systems", "public perception", "robotic assistance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12732", "pdf": "https://arxiv.org/pdf/2507.12732.pdf", "abs": "https://arxiv.org/abs/2507.12732", "title": "Strategy Adaptation in Large Language Model Werewolf Agents", "authors": ["Fuya Nakamori", "Yin Jou Huang", "Fei Cheng"], "categories": ["cs.CL", "I.2.7"], "comment": "7 pages, 2 figures", "summary": "This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.", "AI": {"tldr": "This study introduces a method for Werewolf agents to dynamically adapt their strategies based on player attitudes and conversation context.", "motivation": "To enhance the adaptability and performance of Werewolf agents in games by allowing them to switch strategies based on the situational context and player interactions.", "method": "The proposed method involves the explicit selection of strategies for Werewolf agents that respond to the game context and perceived roles of other players, contrasting with previous approaches that utilized fixed or implicit strategies.", "result": "Experimental comparisons demonstrate that strategy adaptation in Werewolf agents leads to improved performance compared to baseline agents with static strategies.", "conclusion": "The findings support the effectiveness of context-aware strategy selection for enhancing Werewolf agents' gameplay.", "key_contributions": ["Development of a dynamic strategy adaptation method for Werewolf agents", "Comparison with baseline agents leading to demonstrable performance improvements", "Insights into the impact of player attitudes on strategy selection"], "limitations": "", "keywords": ["Werewolf agents", "strategy adaptation", "game performance", "contextual analysis", "player interaction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.12749", "pdf": "https://arxiv.org/pdf/2507.12749.pdf", "abs": "https://arxiv.org/abs/2507.12749", "title": "PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts", "authors": ["Xumeng Wang", "Xiangxuan Zhang", "Zhiqi Gao", "Shuangcheng Jiao", "Yuxin Ma"], "categories": ["cs.HC"], "comment": null, "summary": "The boom in visualization generation tools has significantly lowered the\nthreshold for chart authoring. Nevertheless, chart authors with an insufficient\nunderstanding of perceptual theories may encounter difficulties in evaluating\nthe effectiveness of chart representations, thereby struggling to identify the\nappropriate chart design to convey the intended data patterns. To address this\nissue, we propose a perception simulation model that can assess the perceptual\neffectiveness of charts by predicting graphical patterns that chart viewers are\nlikely to notice. The perception simulation model integrates perceptual theory\ninto visual feature extraction of chart elements to provide interpretable model\noutcomes. Human perceptual results proved that the outcome of our model can\nsimulate the perceptual grouping behaviors of most chart viewers and cover\ndiverse perceptual results. We also embed the model into a prototype interface\ncalled PatternSight to facilitate chart authors in assessing whether the chart\ndesign can satisfy their pattern representation requirements as expected and\ndetermining feasible improvements of visual design. According to the results of\na user experiment, PatternSight can effectively assist chart authors in\noptimizing chart design for representing data patterns.", "AI": {"tldr": "The paper introduces a perception simulation model that helps chart authors evaluate the effectiveness of their chart designs by predicting perceptual patterns that viewers notice, and presents a prototype interface called PatternSight to assist in optimizing chart designs.", "motivation": "Chart authors may struggle to design effective visual representations without understanding perceptual theories, leading to ineffective communication of data patterns.", "method": "The paper proposes a perception simulation model that integrates perceptual theory with visual feature extraction to predict graphical patterns noticed by viewers. This model is implemented in a prototype interface named PatternSight.", "result": "Human perceptual results demonstrate that the model successfully simulates the grouping behaviors of chart viewers and encapsulates diverse perceptual outcomes. User experiments show that PatternSight effectively aids authors in improving chart designs to better represent data patterns.", "conclusion": "The integration of perceptual theory into chart design via the PatternSight interface provides a valuable tool for authors to create more effective charts that communicate data patterns clearly.", "key_contributions": ["Development of a perception simulation model for chart design", "Creation of the PatternSight interface for evaluating chart effectiveness", "Validation through human perceptual experiments showing model accuracy"], "limitations": "The model may not cover all individual differences in perception and may require additional testing across diverse datasets.", "keywords": ["perception simulation", "chart design", "visualization", "PatternSight", "human perception"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.12759", "pdf": "https://arxiv.org/pdf/2507.12759.pdf", "abs": "https://arxiv.org/abs/2507.12759", "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Lechen Zhang", "Xin Liu", "Ayoung Lee", "Xinliang Frederick Zhang", "Farima Fatahi Bayat", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.", "AI": {"tldr": "This paper presents ThinkLogit, a decoding-time approach that enhances long reasoning abilities in large reasoning models without extensive training.", "motivation": "To investigate if long reasoning behavior in large reasoning models can be elicited without additional training and to improve performance using a smaller guide model.", "method": "The paper proposes a decoding-time approach called ThinkLogit that uses logits arithmetic to tune a larger model for long reasoning, guided by a substantially smaller model. It also introduces ThinkLogit-DPO, which trains the guider model using preference optimization over reasoning pairs.", "result": "ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 scores of 26% and 29%, respectively, on four mathematical datasets, leveraging the smaller guider model for enhanced reasoning.", "conclusion": "The proposed methods demonstrate that it is possible to elicit long reasoning in large models efficiently with minimal training, providing significant performance improvements.", "key_contributions": ["Introduction of ThinkLogit, a decoding-time approach for long reasoning", "Demonstration of improved performance via guided training with a smaller model", "Evidence that long reasoning skills can be transferred through reinforcement learning"], "limitations": "", "keywords": ["long reasoning", "large reasoning models", "logits arithmetic", "preference optimization", "thinking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12767", "pdf": "https://arxiv.org/pdf/2507.12767.pdf", "abs": "https://arxiv.org/abs/2507.12767", "title": "Autonomy for Older Adult-Agent Interaction", "authors": ["Jiaxin An"], "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "As the global population ages, artificial intelligence (AI)-powered agents\nhave emerged as potential tools to support older adults' caregiving. Prior\nresearch has explored agent autonomy by identifying key interaction stages in\ntask processes and defining the agent's role at each stage. However, ensuring\nthat agents align with older adults' autonomy preferences remains a critical\nchallenge. Drawing on interdisciplinary conceptualizations of autonomy, this\npaper examines four key dimensions of autonomy for older adults:\ndecision-making autonomy, goal-oriented autonomy, control autonomy, and social\nresponsibility autonomy. This paper then proposes the following research\ndirections: (1) Addressing social responsibility autonomy, which concerns the\nethical and social implications of agent use in communal settings; (2)\nOperationalizing agent autonomy from the task perspective; and (3) Developing\nautonomy measures.", "AI": {"tldr": "The paper explores AI-powered agents in caregiving for older adults and their alignment with autonomy preferences.", "motivation": "To address the challenges in ensuring AI agents support older adults' autonomy in caregiving as the population ages.", "method": "The paper examines four dimensions of autonomy: decision-making, goal-oriented, control, and social responsibility, and proposes research directions for enhancing agent autonomy.", "result": "Identified the need for new measures and frameworks to better align AI agents with the autonomy of older adults in caregiving roles.", "conclusion": "The paper highlights the necessity of addressing ethical and operational aspects of agent autonomy in communal caregiving settings.", "key_contributions": ["Proposes a framework for understanding autonomy in AI agents for older adults", "Identifies key dimensions of autonomy relevant to caregiving", "Suggests future research directions for operationalizing autonomy measures."], "limitations": "", "keywords": ["AI agents", "older adults", "autonomy", "caregiving", "ethical implications"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12769", "pdf": "https://arxiv.org/pdf/2507.12769.pdf", "abs": "https://arxiv.org/abs/2507.12769", "title": "Synergy: End-to-end Concept Model", "authors": ["Keli Zheng", "Zerong Xie"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.", "AI": {"tldr": "This paper presents Synergy, a byte-level language model that improves tokenization efficiency and demonstrates better performance than existing models.", "motivation": "To create a model that effectively bridges different levels of abstraction without relying heavily on traditional tokenization methods.", "method": "The model is trained as a byte-level language model with a learned routing mechanism, evaluating performance against Llama3.", "result": "Synergy produces fewer concept tokens than BBPE tokenizers while maintaining competitive performance; it shows advantages in specific model configurations.", "conclusion": "The research supports the viability of tokenizer-free architectures, which could lead to more robust AI pipelines.", "key_contributions": ["Introduction of Synergy, a novel byte-level language model", "Demonstration of reduced concept tokens compared to BBPE", "Findings on position-independent concepts in higher abstraction layers"], "limitations": "", "keywords": ["language model", "tokenization", "byte-level", "positional encodings", "abstraction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13052", "pdf": "https://arxiv.org/pdf/2507.13052.pdf", "abs": "https://arxiv.org/abs/2507.13052", "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication", "authors": ["Tianyu Song", "Feng Li", "Yuan Bi", "Angelos Karlas", "Amir Yousefi", "Daniela Branzan", "Zhongliang Jiang", "Ulrich Eck", "Nassir Navab"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted at MICCAI 2025", "summary": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.", "AI": {"tldr": "This paper introduces an intelligent virtual sonographer (IVS) that enhances physician-robot-patient interactions during robotic ultrasound procedures.", "motivation": "To bridge the communication gap between physicians, robotic systems, and patients in robotic ultrasound applications.", "method": "The study develops a conversational virtual agent within Extended Reality (XR) that employs LLM-powered dialogue, including speech-to-text and text-to-speech capabilities, to facilitate real-time communication and robotic control.", "result": "The system improves the efficiency and accessibility of robotic ultrasound acquisition, leading to better patient experiences and increased trust in physician-robot interactions.", "conclusion": "The introduction of an IVS paves the way for more effective communication in medical procedures involving robotics, enhancing both usability for physicians and reassurance for patients.", "key_contributions": ["Development of an intelligent virtual sonographer for robotic ultrasound", "Integration of LLM-powered dialogue with robotic control", "Improvement in physician-robot-patient communication dynamics"], "limitations": "", "keywords": ["Human-Computer Interaction", "Large Language Models", "Robotics", "Ultrasound", "Virtual Agent"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.12782", "pdf": "https://arxiv.org/pdf/2507.12782.pdf", "abs": "https://arxiv.org/abs/2507.12782", "title": "Learning Robust Negation Text Representations", "authors": ["Thinh Hung Truong", "Karin Verspoor", "Trevor Cohn", "Timothy Baldwin"], "categories": ["cs.CL"], "comment": null, "summary": "Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks.", "AI": {"tldr": "This paper proposes a strategy to enhance the negation robustness of text encoders through knowledge distillation from large language models, using contrastive learning to improve a BERT-based model's performance on negation tasks.", "motivation": "To address the inadequate handling of negation by current text encoders which affects the performance in various applications that rely on text embeddings.", "method": "The authors use knowledge distillation from large language models to teach a BERT-based model diverse patterns of negation and hedging, followed by fine-tuning with a contrastive learning strategy.", "result": "The fine-tuned BERT-based model demonstrates significant improvements in understanding negation while maintaining strong performance on general text benchmarks.", "conclusion": "The proposed strategy successfully adapts to large language models as well, enhancing their performance in handling negation tasks.", "key_contributions": ["Proposed a novel distillation strategy for improving negation understanding in text encoders.", "Implemented a contrastive learning approach to fine-tune a BERT-based model.", "Showed adaptability of the method to large language models for better negation handling."], "limitations": "", "keywords": ["negation", "text encoders", "contrastive learning", "large language models", "BERT"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.13065", "pdf": "https://arxiv.org/pdf/2507.13065.pdf", "abs": "https://arxiv.org/abs/2507.13065", "title": "\"What do you expect? You're part of the internet\": Analyzing Celebrities' Experiences as Usees of Deepfake Technology", "authors": ["John Twomey", "Sarah Foley", "Sarah Robinson", "Michael Quayle", "Matthew Peter Aylett", "Conor Linehan", "Gillian Murphy"], "categories": ["cs.HC"], "comment": null, "summary": "Deepfake technology is often used to create non-consensual synthetic intimate\nimagery (NSII), mainly of celebrity women. Through Critical Discursive\nPsychological analysis we ask; i) how celebrities construct being targeted by\ndeepfakes and ii) how they navigate infrastructural and social obstacles when\nseeking recourse. In this paper, we adopt Baumers concept of Usees\n(stakeholders who are non-consenting, unaware and directly targeted by\ntechnology), to understand public statements made by eight celebrity women and\none non-binary individual targeted with NSII. Celebrities describe harms of\nbeing non-consensually targeted by deepfakes and the distress of becoming aware\nof these videos. They describe various infrastructural/social factors (e.g.\nblaming/ silencing narratives and the industry behind deepfake abuse) which\nhinder activism and recourse. This work has implications in recognizing the\nroles of various stakeholders in the infrastructures underlying deepfake abuse\nand the potential of human-computer interaction to improve existing recourses\nfor NSII. We also contribute to understanding how false beliefs online\nfacilitate deepfake abuse. Future work should involve interventions which\nchallenge the values and false beliefs which motivate NSII\ncreation/dissemination.", "AI": {"tldr": "The paper explores the experiences of celebrity victims of non-consensual synthetic intimate imagery (NSII) and identifies social and infrastructural barriers to recourse.", "motivation": "To understand the harms faced by celebrities targeted by deepfakes and the challenges in seeking recourse.", "method": "Critical Discursive Psychological analysis of public statements by eight celebrity women and one non-binary individual who were targeted by NSII.", "result": "Celebrities articulated the distress caused by non-consensual deepfakes and identified social/infrastructural factors that hinder their activism and recourse efforts.", "conclusion": "The study highlights the need for improved HCI interventions and challenges false online beliefs that facilitate deepfake abuse.", "key_contributions": ["Adoption of Baumers concept of Usees to analyze celebrity experiences with deepfakes", "Identification of infrastructural and social obstacles to recourse", "Implications for HCI in improving responses to NSII"], "limitations": "", "keywords": ["deepfake", "non-consensual", "synthetic imagery", "human-computer interaction", "celebrity"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.12808", "pdf": "https://arxiv.org/pdf/2507.12808.pdf", "abs": "https://arxiv.org/abs/2507.12808", "title": "Large Language Models' Internal Perception of Symbolic Music", "authors": ["Andrew Shin", "Kunitake Kaneko"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.", "AI": {"tldr": "This paper investigates how large language models (LLMs) represent and generate symbolic music data from textual descriptions, producing a dataset of MIDI files for musical classification and melody completion tasks.", "motivation": "To explore the potential of large language models (LLMs) in modeling and generating symbolic music data and to understand their limitations in the musical context.", "method": "The authors generated a dataset of MIDI files from textual prompts and trained neural networks on this LLM-generated dataset to perform genre classification and melody completion, comparing performance with established models.", "result": "The findings indicate that while LLMs can infer basic musical structures and temporal relationships from text, they have limitations due to a lack of explicit musical training.", "conclusion": "The study enhances understanding of LLMs' generative capabilities in symbolic music, suggesting both potential and constraints in their musical encoding.", "key_contributions": ["Creation of a new dataset of LLM-generated MIDI files", "Evaluation of neural networks trained on this dataset for music tasks", "Insights into LLMs‚Äô ability to model musical structures"], "limitations": "LLMs lack explicit musical context, impacting their performance in more complex musical tasks.", "keywords": ["large language models", "symbolic music", "MIDI generation", "genre classification", "machine learning"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2507.13167", "pdf": "https://arxiv.org/pdf/2507.13167.pdf", "abs": "https://arxiv.org/abs/2507.13167", "title": "On tangible user interfaces, humans and spatiality", "authors": ["Ehud Sharlin", "Benjamin Watson", "Yoshifumi Kitamura", "Fumio Kishino", "Yuichi Itoh"], "categories": ["cs.HC"], "comment": null, "summary": "Like the prehistoric twig and stone, tangible user interfaces (TUIs) are\nobjects manipulated by humans. TUI success will depend on how well they exploit\nspatiality, the intuitive spatial skills humans have with the objects they use.\nIn this paper we carefully examine the relationship between humans and physical\nobjects, and related previous research. From this examination we distill a set\nof observations, and turn these into heuristics for incorporation of spatiality\ninto TUI application design, a cornerstone for their success. Following this\nline of thought, we identify spatial TUIs, the subset of TUIs that mediate\ninteraction with shape, space and structure. We then examine several existing\nspatial TUIs using our heuristics.", "AI": {"tldr": "This paper explores the role of spatiality in the design of tangible user interfaces (TUIs) and provides heuristics for their successful application.", "motivation": "To evaluate how effective tangible user interfaces can be when they leverage human spatial skills with physical objects.", "method": "The authors analyze past research on human-object interaction to develop heuristics related to spatiality in TUI design.", "result": "The study identifies key heuristics for integrating spatiality into TUI applications, supported by an analysis of existing spatial TUIs.", "conclusion": "Incorporating spatiality into the design of TUIs can enhance their effectiveness and user experience.", "key_contributions": ["Development of heuristics for TUI design based on spatiality.", "Classification of spatial TUIs that focus on shape, space, and structure.", "Analysis of existing spatial TUIs with respect to the proposed heuristics."], "limitations": "", "keywords": ["Tangible User Interfaces", "Spatiality", "Human-Object Interaction", "Heuristics", "Application Design"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.12838", "pdf": "https://arxiv.org/pdf/2507.12838.pdf", "abs": "https://arxiv.org/abs/2507.12838", "title": "Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?", "authors": ["Xi Ai", "Mahardika Krisna Ihsani", "Min-Yen Kan"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual consistency should be considered to assess cross-lingual\ntransferability, maintain the factuality of the model knowledge across\nlanguages, and preserve the parity of language model performance. We are thus\ninterested in analyzing, evaluating, and interpreting cross-lingual consistency\nfor factual knowledge. We examine code-mixed coreferential statements conveyed\nidentical knowledge across languages to study cross-lingual knowledge\nconsistency. We use some interpretability approaches to analyze the behavior of\na model in cross-lingual contexts, discovering that multilingual models show\ndifferent levels of consistency, subject to language families, linguistic\nfactors, and a bottleneck in cross-lingual consistency on a particular layer.\nIn addition, we evaluate common strategies aimed at improving multilingual\nperformance to observe whether these strategies can improve knowledge\nconsistency at the same time. While knowledge is not cross-lingual consistency\nin many cases, code-switching training and cross-lingual word alignment\nobjectives show the most promising results, emphasizing the noteworthiness of\ncross-lingual alignment supervision and code-switching training for both\nmultilingual performance and cross-lingual consistency enhancement.", "AI": {"tldr": "This paper analyzes cross-lingual consistency in knowledge transfer across languages, focusing on multilingual models' performance and their interpretability mechanisms.", "motivation": "To assess cross-lingual transferability and maintain factual knowledge consistency across languages in multilingual models.", "method": "The study examines code-mixed coreferential statements in multilingual contexts using interpretability approaches to analyze model behavior and consistency levels based on linguistic factors.", "result": "Multilingual models show varying levels of cross-lingual consistency influenced by language families and specific bottlenecks; strategies like code-switching training and cross-lingual alignment showed promising improvements in consistency and performance.", "conclusion": "Cross-lingual consistency is crucial for multilingual models, with specific training strategies proving effective in enhancing both performance and consistency across languages.", "key_contributions": ["Analysis of cross-lingual consistency for factual knowledge in multilingual models.", "Identification of bottlenecks in cross-lingual consistency related to linguistic factors.", "Evaluation of training strategies that improve both multilingual performance and cross-lingual consistency."], "limitations": "The study identifies that knowledge does not always equate to cross-lingual consistency.", "keywords": ["cross-lingual consistency", "multilingual models", "factual knowledge", "code-switching training", "cross-lingual alignment"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13235", "pdf": "https://arxiv.org/pdf/2507.13235.pdf", "abs": "https://arxiv.org/abs/2507.13235", "title": "Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item", "authors": ["Minghao Cai", "Guher Gorgun", "Carrie Demmans Epp"], "categories": ["cs.HC"], "comment": "13 pages, presented at AERA 2025 Annual Meeting, Denver, Colorado,\n  April 2025", "summary": "Cognitive load is key to ensuring an optimal learning experience. However,\nmeasuring the cognitive load of educational tasks typically relies on\nself-report measures which has been criticized by researchers for being\nsubjective. In this study, we investigated the feasibility of using item\ndifficulty parameters as a proxy for measuring cognitive load in an online\nlearning platform. Difficulty values that were derived using item-response\ntheory were consistent with theories of how intrinsic and extraneous load\ncontribute to cognitive load. This finding suggests that we can use item\ndifficulty to represent intrinsic load when modelling cognitive load in\nlearning games.", "AI": {"tldr": "This study explores using item difficulty parameters as a proxy for measuring cognitive load in online learning environments, suggesting that item difficulty can represent intrinsic cognitive load.", "motivation": "To find an objective method for measuring cognitive load in educational tasks, as traditional self-report measures are criticized for their subjectivity.", "method": "The study investigated item difficulty parameters derived from item-response theory to assess their consistency with cognitive load theories in a learning platform.", "result": "Item difficulty values aligned with models of intrinsic and extraneous load, indicating their potential as proxies for cognitive load in learning games.", "conclusion": "Using item difficulty parameters can effectively represent intrinsic cognitive load, offering a more objective measurement approach.", "key_contributions": ["Proposes a novel method for measuring cognitive load using item difficulty parameters.", "Demonstrates the connection between item difficulty and cognitive load theories.", "Suggests practical implications for designing online educational tools."], "limitations": "The study focuses on item difficulty in a specific online learning platform, limiting generalizability across different educational contexts.", "keywords": ["Cognitive Load", "Item Difficulty", "Educational Technology", "Learning Games", "Item-Response Theory"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.12930", "pdf": "https://arxiv.org/pdf/2507.12930.pdf", "abs": "https://arxiv.org/abs/2507.12930", "title": "Making Language Model a Hierarchical Classifier and Generator", "authors": ["Yihong Wang", "Zhonglin Jiang", "Ningyuan Xi", "Yue Zhao", "Qingqing Gu", "Xiyuan Chen", "Hao Wu", "Sheng Xu", "Hange Zhou", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Decoder-only language models, such as GPT and LLaMA, generally decode on the\nlast layer. Motivated by human's hierarchical thinking capability, we propose\nthat a hierarchical decoder architecture could be built with different layers\ndecoding texts simultaneously. Due to limited time and computationally\nresources, we choose to adapt a pretrained language model into this form of\nhierarchical decoder. Language heads of the last layer are copied to different\nselected intermediate layers, and fine-tuned with different task inputs. By\nthorough experiments, we validate that these selective intermediate layers\ncould be adapted to speak meaningful and reasonable contents, and this paradigm\nof hierarchical decoder can obtain state-of-the-art performances on multiple\ntasks such as hierarchical text classification, classification-guided\ngeneration, and hierarchical text generation. This study suggests the\npossibility of a generalized hierarchical reasoner, pretraining from scratch.", "AI": {"tldr": "This paper proposes a hierarchical decoder architecture for language models that decodes text using multiple layers simultaneously, leading to improved performance on various tasks.", "motivation": "The study is motivated by the desire to emulate human hierarchical thinking capabilities in language models.", "method": "A pretrained language model is adapted by copying language heads from the last layer to selected intermediate layers which are then fine-tuned on different task inputs.", "result": "The model exhibits meaningful output from intermediate layers and achieves state-of-the-art performance on tasks like hierarchical text classification and generation.", "conclusion": "The findings support the feasibility of developing a generalized hierarchical reasoner that could be pretrained from scratch.", "key_contributions": ["Introduction of a hierarchical decoder architecture for language models", "Demonstration of improved performance on text classification and generation tasks", "Validation of meaningful outputs from intermediate layers"], "limitations": "", "keywords": ["hierarchical decoder", "language models", "text classification", "text generation", "fine-tuning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.13247", "pdf": "https://arxiv.org/pdf/2507.13247.pdf", "abs": "https://arxiv.org/abs/2507.13247", "title": "RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality", "authors": ["Ruohao Li", "Jiawei Li", "Jia Sun", "Zhiqing Wu", "Zisu Li", "Ziyan Wang", "Ge Lin Kan", "Mingming Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Reminiscence activities, which involve recalling and sharing past\nexperiences, have proven beneficial for improving cognitive function, mood, and\noverall well-being. However, urbanization has led to the disappearance of\nfamiliar environments, removing visual and audio cues for effective\nreminiscence. While old photos can serve as visual cues to aid reminiscence, it\nis challenging for people to reconstruct the reminisced content and environment\nthat are not in the photos. Virtual reality (VR) and artificial intelligence\n(AI) offer the ability to reconstruct an immersive environment with dynamic\ncontent and to converse with people to help them gradually reminisce. We\ndesigned RemVerse, an AI-empowered VR prototype aimed to support reminiscence\nactivities. Integrating generative models and AI agent into a VR environment,\nRemVerse helps older adults reminisce with AI-generated visual cues and\ninteractive dialogues. Our user study with 14 older adults showed that RemVerse\neffectively supported reminiscence activities by triggering, concretizing, and\ndeepening personal memories, while fostering increased engagement and autonomy\namong older adults. Based on our findings, we proposed design implications to\nmake reminiscence activities in AI-assisted VR more accessible and engaging for\nolder adults.", "AI": {"tldr": "RemVerse is an AI-powered VR prototype designed to aid reminiscence activities for older adults by using generative models and interactive dialogues to enhance memory recall and engagement.", "motivation": "To address the challenge of facilitating effective reminiscence activities for older adults who are losing familiar environments due to urbanization.", "method": "Developed an AI-empowered VR prototype, RemVerse, that incorporates generative models and an AI agent to enhance reminiscence by providing visual cues and interactive conversations.", "result": "The user study with 14 older adults indicated that RemVerse effectively supported reminiscence activities by enhancing memory recall, engagement, and autonomy.", "conclusion": "Design implications were proposed to enhance accessibility and engagement in AI-assisted VR reminiscence activities for older adults.", "key_contributions": ["Development of an AI-enhanced VR prototype for reminiscence activities", "Insights from a user study demonstrating effectiveness for older adults", "Design implications for improving accessibility and engagement in reminiscence activities"], "limitations": "", "keywords": ["virtual reality", "reminiscence", "AI", "older adults", "user study"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12981", "pdf": "https://arxiv.org/pdf/2507.12981.pdf", "abs": "https://arxiv.org/abs/2507.12981", "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps", "authors": ["Maximiliano Hormaz√°bal Lagos", "√Ålvaro Bueno S√°ez", "H√©ctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as an official challenge paper in the PRESTA: Questions and\n  Answers over Tabular Data shared task at IberLEF 2025, colocated with the\n  41st SEPLN Conference in Zaragoza, Spain", "summary": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.", "AI": {"tldr": "This paper discusses a method for answering questions about tables in Spanish using Python code generation with LLMs, achieving 85% accuracy.", "motivation": "To address the challenge of answering questions about tabular data in Spanish, enhancing the capabilities of LLMs in this domain.", "method": "The solution involves analyzing table content, selecting relevant columns, generating natural language instructions, translating them to Python code, executing it, and managing errors, utilizing open-source LLMs and optimized prompts.", "result": "The proposed method achieved an accuracy score of 85% in answering questions about tables in Spanish.", "conclusion": "The approach demonstrates the viability of using LLMs for effectively interacting with tabular data and suggests further exploration in contextual understanding of such data.", "key_contributions": ["Implementation of LLMs for code generation in answering tabular data questions in Spanish", "Achieved a high accuracy score of 85%", "Developed a multi-step process for table data interaction using LLMs"], "limitations": "", "keywords": ["LLM", "Python code generation", "tabular data", "Spanish", "natural language processing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.13309", "pdf": "https://arxiv.org/pdf/2507.13309.pdf", "abs": "https://arxiv.org/abs/2507.13309", "title": "FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD", "authors": ["Hanxiu 'Hazel' Zhu", "Ruijia Chen", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "While videos have become increasingly prevalent in delivering information\nacross different educational and professional contexts, individuals with ADHD\noften face attention challenges when watching informational videos due to the\ndynamic, multimodal, yet potentially distracting video elements. To understand\nand address this critical challenge, we designed \\textit{FocusView}, a video\ncustomization interface that allows viewers with ADHD to customize\ninformational videos from different aspects. We evaluated FocusView with 12\nparticipants with ADHD and found that FocusView significantly improved the\nviewability of videos by reducing distractions. Through the study, we uncovered\nparticipants' diverse perceptions of video distractions (e.g., background music\nas a distraction vs. stimulation boost) and their customization preferences,\nhighlighting unique ADHD-relevant needs in designing video customization\ninterfaces (e.g., reducing the number of options to avoid distraction caused by\ncustomization itself). We further derived design considerations for future\nvideo customization systems for the ADHD community.", "AI": {"tldr": "FocusView is a video customization interface designed for individuals with ADHD that helps improve video viewability by reducing distractions.", "motivation": "Individuals with ADHD struggle with attention challenges when watching informational videos due to distractions caused by dynamic video elements.", "method": "FocusView was evaluated with 12 participants with ADHD to assess its effectiveness in improving video viewability and understanding distractions.", "result": "The study found that FocusView significantly improved video viewability by reducing distractions, revealing diverse perceptions of video elements among participants.", "conclusion": "The findings highlight the need for tailored design considerations in video customization systems for individuals with ADHD.", "key_contributions": ["Introduction of FocusView for ADHD individuals", "Significant improvement in video viewability", "Identification of unique ADHD-relevant customization needs"], "limitations": "Study limited to 12 participants; results may not generalize to all ADHD individuals.", "keywords": ["ADHD", "video customization", "viewability", "distractions", "interface design"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13076", "pdf": "https://arxiv.org/pdf/2507.13076.pdf", "abs": "https://arxiv.org/abs/2507.13076", "title": "Formalizing Attack Scenario Description: A Proposed Model", "authors": ["Quentin Goux", "Nadira Lammari"], "categories": ["cs.CL"], "comment": null, "summary": "Organizations face an ever-changing threat landscape. They must continuously\ndedicate significant efforts to protect their assets, making their adoption of\nincreased cybersecurity automation inevitable. However, process automation\nrequires formalization of input data. Through this paper, we address this need\nfor processes that use attack scenarios as input. Among these processes, one\ncan mention both the generation of scripts for attack simulation and training\npurposes, as well as the analysis of attacks. Therefore, the paper's main\nresearch contribution is a novel formal model that encompasses the attack's\ncontext description and its scenario. It is abstracted using UML class model.\nOnce the description of our model done, we will show how it could serve an\nupstream attack analysis process. We will show also its use for an automatic\ngeneration of attack scripts in the context of cybersecurity training. These\ntwo uses cases constitute the second contribution of this present research\nwork.", "AI": {"tldr": "This paper presents a formal model for cybersecurity automation focused on attack scenarios, aiding in attack simulation and training.", "motivation": "Organizations need to automate cybersecurity processes due to a constantly evolving threat landscape.", "method": "The paper introduces a UML-based formal model that describes attack contexts and scenarios.", "result": "The proposed model facilitates upstream attack analysis and enables automatic generation of attack scripts for training.", "conclusion": "The novel model can enhance cybersecurity training and attack analysis processes effectively.", "key_contributions": ["A formal UML class model for attack scenarios", "Use of model for automating attack script generation", "Application of model in upstream attack analysis"], "limitations": "", "keywords": ["cybersecurity", "attack scenarios", "UML", "automation", "training"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13105", "pdf": "https://arxiv.org/pdf/2507.13105.pdf", "abs": "https://arxiv.org/abs/2507.13105", "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts", "authors": ["Marc Brinner", "Sina Zarriess"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.", "AI": {"tldr": "Introduction of SemCSE, an unsupervised method for learning semantic embeddings of scientific texts utilizing LLM-generated summaries.", "motivation": "To improve upon traditional citation-based approaches by capturing the true semantic content of scientific texts through unsupervised learning methods.", "method": "SemCSE employs contrastive learning on LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space.", "result": "SemCSE demonstrates stronger semantic separation in the embedding space and achieves state-of-the-art performance on the SciRepEval benchmark for scientific text embeddings.", "conclusion": "The study validates that a semantically focused training approach enhances the ability to encode the semantic content of scientific texts effectively.", "key_contributions": ["Introduction of a novel unsupervised method for semantic embeddings (SemCSE)", "Proposing a new benchmark for evaluating semantic understanding in scientific texts", "Achieving state-of-the-art performance on the SciRepEval benchmark for scientific text embeddings."], "limitations": "", "keywords": ["semantic embeddings", "contrastive learning", "scientific texts"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13115", "pdf": "https://arxiv.org/pdf/2507.13115.pdf", "abs": "https://arxiv.org/abs/2507.13115", "title": "A Computational Framework to Identify Self-Aspects in Text", "authors": ["Jaya Caporusso", "Matthew Purver", "Senja Pollak"], "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025", "summary": "This Ph.D. proposal introduces a plan to develop a computational framework to\nidentify Self-aspects in text. The Self is a multifaceted construct and it is\nreflected in language. While it is described across disciplines like cognitive\nscience and phenomenology, it remains underexplored in natural language\nprocessing (NLP). Many of the aspects of the Self align with psychological and\nother well-researched phenomena (e.g., those related to mental health),\nhighlighting the need for systematic NLP-based analysis. In line with this, we\nplan to introduce an ontology of Self-aspects and a gold-standard annotated\ndataset. Using this foundation, we will develop and evaluate conventional\ndiscriminative models, generative large language models, and embedding-based\nretrieval approaches against four main criteria: interpretability, ground-truth\nadherence, accuracy, and computational efficiency. Top-performing models will\nbe applied in case studies in mental health and empirical phenomenology.", "AI": {"tldr": "The paper proposes a computational framework to identify Self-aspects in text, focusing on mental health and phenomenology using NLP techniques.", "motivation": "The Self is a multifaceted construct that is underexplored in NLP, though it aligns with important psychological phenomena, especially in mental health contexts.", "method": "Develop an ontology of Self-aspects and a gold-standard annotated dataset; evaluate models including discriminative models, generative large language models, and embedding-based retrieval approaches based on interpretability, accuracy, and efficiency.", "result": "The framework aims to enhance understanding and analysis of Self-aspects in language, providing tools for mental health research.", "conclusion": "By systematically analyzing Self-aspects through NLP, this research could significantly impact mental health studies and phenomenological research.", "key_contributions": ["Development of an ontology of Self-aspects", "Creation of a gold-standard annotated dataset", "Application of advanced NLP models in mental health and phenomenology"], "limitations": "", "keywords": ["NLP", "Self-aspects", "Mental health", "Ontology", "Large language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13138", "pdf": "https://arxiv.org/pdf/2507.13138.pdf", "abs": "https://arxiv.org/abs/2507.13138", "title": "Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation", "authors": ["Hadi Mohammadi", "Tina Shahedi", "Pablo Mosteiro", "Massimo Poesio", "Ayoub Bagheri", "Anastasia Giachanou"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation.", "AI": {"tldr": "The study explores the impact of annotator demographics on labeling decisions in NLP tasks, finding content to be more influential than demographics. It evaluates the effectiveness of using Generative AI with demographic personas in improving human alignment, concluding that content-driven strategies are more reliable for fairness in NLP systems.", "motivation": "To understand the influence of annotator demographic features on labeling decisions in NLP tasks like sexism detection, and to enhance fairness in NLP systems.", "method": "The study utilizes a Generalized Linear Mixed Model to quantify the influence of demographic factors versus text content on annotation variability. It also assesses Generative AI models with persona prompting and evaluates performance against human judgments using explainable AI techniques.", "result": "Demographic features account for only 8% of the observed variance in labeling decisions, with tweet content being the primary factor. Guiding Generative AI models with demographic personas does not significantly enhance performance; in some cases, it degrades it.", "conclusion": "Focusing on content-driven explanations and robust annotation protocols is advocated as a more reliable approach for achieving fairness in NLP systems than relying on persona simulation.", "key_contributions": ["Quantification of the impact of demographic features on NLP labeling decisions.", "Evaluation of Generative AI models in the context of demographic persona prompting.", "Recommendations for improved fairness focusing on content-driven approaches."], "limitations": "The findings may not generalize across all NLP tasks or datasets, and the effectiveness of annotator demographics may vary in other contexts.", "keywords": ["NLP", "fairness", "Generative AI", "annotator demographics", "sexism detection"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.13164", "pdf": "https://arxiv.org/pdf/2507.13164.pdf", "abs": "https://arxiv.org/abs/2507.13164", "title": "Feature-based analysis of oral narratives from Afrikaans and isiXhosa children", "authors": ["Emma Sharratt", "Annelien Smith", "Retief Louw", "Daleen Klop", "Febe de Wet", "Herman Kamper"], "categories": ["cs.CL"], "comment": "SLaTE 2025 in Nijmegen, Netherlands", "summary": "Oral narrative skills are strong predictors of later literacy development.\nThis study examines the features of oral narratives from children who were\nidentified by experts as requiring intervention. Using simple machine learning\nmethods, we analyse recorded stories from four- and five-year-old Afrikaans-\nand isiXhosa-speaking children. Consistent with prior research, we identify\nlexical diversity (unique words) and length-based features (mean utterance\nlength) as indicators of typical development, but features like articulation\nrate prove less informative. Despite cross-linguistic variation in\npart-of-speech patterns, the use of specific verbs and auxiliaries associated\nwith goal-directed storytelling is correlated with a reduced likelihood of\nrequiring intervention. Our analysis of two linguistically distinct languages\nreveals both language-specific and shared predictors of narrative proficiency,\nwith implications for early assessment in multilingual contexts.", "AI": {"tldr": "The study analyzes oral narratives from young children to identify predictors of literacy development using machine learning methods.", "motivation": "To explore features of oral narratives in children needing intervention and their relationship to literacy development.", "method": "Simple machine learning methods to analyze recorded stories from four- and five-year-old Afrikaans- and isiXhosa-speaking children.", "result": "Identified lexical diversity and mean utterance length as key indicators of typical development, while articulation rate was less informative. Specific verbs and auxiliaries linked to storytelling success were also found.", "conclusion": "The findings highlight language-specific and common predictors of narrative proficiency, which could inform early assessment in multilingual contexts.", "key_contributions": ["Analysis of narrative skills in different languages", "Identification of key features predicting literacy development", "Implications for early assessment strategies in multilingual settings"], "limitations": "", "keywords": ["oral narratives", "literacy development", "machine learning", "multilingual", "early assessment"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13190", "pdf": "https://arxiv.org/pdf/2507.13190.pdf", "abs": "https://arxiv.org/abs/2507.13190", "title": "GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems", "authors": ["Jisoo Lee", "Raeyoung Chang", "Dongwook Kwon", "Harmanpreet Singh", "Nikhil Verma"], "categories": ["cs.CL"], "comment": "4 figures, 1 algorithm, 2 tables, 6 pages, under review at EMNLP\n  Industry track 2025", "summary": "Multi-agent systems built on language models have shown strong performance on\ncollaborative reasoning tasks. However, existing evaluations focus only on the\ncorrectness of the final output, overlooking how inefficient communication and\npoor coordination contribute to redundant reasoning and higher computational\ncosts. We introduce GEMMAS, a graph-based evaluation framework that analyzes\nthe internal collaboration process by modeling agent interactions as a directed\nacyclic graph. To capture collaboration quality, we propose two process-level\nmetrics: Information Diversity Score (IDS) to measure semantic variation in\ninter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant\nreasoning paths. We evaluate GEMMAS across five benchmarks and highlight\nresults on GSM8K, where systems with only a 2.1% difference in accuracy differ\nby 12.8% in IDS and 80% in UPR, revealing substantial variation in internal\ncollaboration. These findings demonstrate that outcome-only metrics are\ninsufficient for evaluating multi-agent performance and highlight the\nimportance of process-level diagnostics in designing more interpretable and\nresource-efficient collaborative AI systems.", "AI": {"tldr": "Introduces GEMMAS, a graph-based framework for evaluating multi-agent systems in collaborative reasoning tasks, focusing on the quality of agent communication and coordination.", "motivation": "Existing evaluations of multi-agent systems overlook the quality of communication and coordination, which can lead to inefficiencies and higher computational costs.", "method": "GEMMAS models agent interactions as a directed acyclic graph and introduces two metrics: Information Diversity Score (IDS) and Unnecessary Path Ratio (UPR) for assessing collaboration quality.", "result": "Evaluation of GEMMAS across five benchmarks reveals significant differences in internal collaboration, with systems showing small accuracy differences exhibiting substantial variation in IDS and UPR metrics.", "conclusion": "Outcome-only metrics are insufficient for evaluating multi-agent performance; process-level diagnostics are crucial for designing interpretable and resource-efficient AI systems.", "key_contributions": ["Introduction of GEMMAS framework for evaluating collaborative AI systems", "Development of IDS and UPR metrics for assessing collaboration quality", "Demonstration of substantial variations in multi-agent collaboration beyond outcome metrics"], "limitations": "", "keywords": ["multi-agent systems", "collaborative reasoning", "evaluation metrics"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2507.13205", "pdf": "https://arxiv.org/pdf/2507.13205.pdf", "abs": "https://arxiv.org/abs/2507.13205", "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa children", "authors": ["R. Louw", "E. Sharratt", "F. de Wet", "C. Jacobs", "A. Smith", "H. Kamper"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025", "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.", "AI": {"tldr": "The paper presents a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa, utilizing machine learning and large language models for enhanced comprehension evaluation.", "motivation": "To aid preschool teachers in identifying students needing intervention in narrative and comprehension skills more effectively.", "method": "The system employs automatic speech recognition and a machine learning scoring model, comparing a linear model to a large language model (LLM) for prediction accuracy.", "result": "The LLM-based system generally outperforms the linear model, achieving performance comparable to human experts in identifying at-risk children.", "conclusion": "The proposed system lays the groundwork for automatic oral assessments, enabling teachers to provide personalized support more effectively.", "key_contributions": ["Development of an automatic assessment system for preschool children's narratives.", "Comparison of linear and LLM scoring models for educational purposes.", "Demonstration of LLM's effectiveness in identifying children requiring support."], "limitations": "", "keywords": ["automated assessment", "natural language processing", "preschool education", "machine learning", "large language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.13236", "pdf": "https://arxiv.org/pdf/2507.13236.pdf", "abs": "https://arxiv.org/abs/2507.13236", "title": "Enhancing Cross-task Transfer of Large Language Models via Activation Steering", "authors": ["Xinyu Tang", "Zhihao Lv", "Xiaoxue Cheng", "Junyi Li", "Wayne Xin Zhao", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.", "AI": {"tldr": "The paper presents CAST, a novel framework for cross-task knowledge transfer in LLMs by manipulating activation states, which improves performance in data-scarce scenarios.", "motivation": "To address the limitations of large language models (LLMs) in unseen tasks, especially in data-scarce environments, by exploring latent space steering for effective knowledge transfer.", "method": "The proposed CAST framework selects influential samples from high-resource tasks and manipulates their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks without requiring parameter updates or input expansion.", "result": "CAST outperforms competitive baselines in cross-domain and cross-lingual transfer scenarios, demonstrating superior scalability and lower computational costs.", "conclusion": "The results support the feasibility of cross-task transfer using activation steering, paving the way for more efficient LLM applications in low-resource settings.", "key_contributions": ["Development of a novel Cross-task Activation Steering Transfer framework (CAST)", "Analysis of activation patterns in the latent space of LLMs for robust transfer", "Demonstration of improved performance in data-scarce scenarios with lower computational overhead."], "limitations": "", "keywords": ["large language models", "cross-task transfer", "latent space steering", "machine learning", "low-resource tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.01485", "pdf": "https://arxiv.org/pdf/2404.01485.pdf", "abs": "https://arxiv.org/abs/2404.01485", "title": "A Design Space for Multiscale Visualization", "authors": ["Mara Solen", "Matt Oddo", "Tamara Munzner"], "categories": ["cs.HC"], "comment": null, "summary": "Designing multiscale visualizations, particularly when the ratio between the\nlargest scale and the smallest item is large, can be challenging, and designers\nhave developed many approaches to overcome this challenge. We present a design\nspace for visualization with multiple scales. The design space includes three\ndimensions, with eight total subdimensions. We demonstrate its descriptive\npower by using it to code approaches from a corpus we compiled of 52 examples,\ncreated by a mix of academics and practitioners. We demonstrate descriptive\npower by analyzing and partitioning these examples into four high-level\nstrategies for designing multiscale visualizations, which are shared approaches\nwith respect to design space dimension choices. We demonstrate generative power\nby analyzing missed opportunities within the corpus of examples, identified\nthrough analysis of the design space, where we note how certain examples could\nhave benefited from different choices. We discuss patterns in the use of\ndifferent dimension and strategy choices in the different visualization\ncontexts of analysis and presentation.\n  Supplemental materials: https://osf.io/wbrdm/\n  Design space website: https://marasolen.github.io/multiscale-vis-ds/", "AI": {"tldr": "This paper presents a design space for creating multiscale visualizations, categorizing existing approaches, and identifying potential improvements through static and generative analysis.", "motivation": "To address the challenges of designing effective multiscale visualizations with a large scale ratio, harnessing a structured design framework.", "method": "A design space comprising three main dimensions and eight subdimensions was developed and used to categorize 52 different visualization examples from practitioners and academics, leading to the identification of strategies and gaps.", "result": "The analysis partitioned the examples into four high-level strategies for multiscale visualization design and revealed missed opportunities for improving existing designs based on the design space.", "conclusion": "The study discusses patterns in dimension choices and strategies employed in various visualization contexts, highlighting the importance of a structured approach to support better design decisions.", "key_contributions": ["Introduction of a structured design space for multiscale visualizations.", "Categorization of 52 existing designs into four strategic approaches.", "Identification of design gaps and missed opportunities in current multiscale visualization methods."], "limitations": "", "keywords": ["multiscale visualizations", "design space", "visualization strategies", "data visualization", "HCI"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13238", "pdf": "https://arxiv.org/pdf/2507.13238.pdf", "abs": "https://arxiv.org/abs/2507.13238", "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.", "AI": {"tldr": "Introduction of a new Hindi Analogy Test Set (HATS) to evaluate reasoning capabilities of large language models (LLMs) in Hindi.", "motivation": "To address the gap in studying LLM reasoning abilities in Indic languages and to provide a resource for evaluating models' performance in Hindi.", "method": "The authors developed a Hindi Analogy Test Set (HATS) comprising 405 multiple-choice questions from Indian government exams and employed various prompting strategies along with a grounded Chain of Thought approach.", "result": "LLMs performed best with English prompts regardless of the prompting strategy, highlighting the challenges faced when using Hindi prompts.", "conclusion": "The introduction of HATS provides a necessary resource for evaluating reasoning capabilities of LLMs in Hindi, revealing a performance gap when compared to English prompts.", "key_contributions": ["Creation of the Hindi Analogy Test Set (HATS) with 405 questions.", "Benchmarking state-of-the-art multilingual LLMs in Hindi.", "Introduction of a grounded Chain of Thought approach for improved reasoning ability."], "limitations": "Models predominantly perform better with English prompts, indicating a potential area for improvement in multilingual capabilities.", "keywords": ["Hindi Analogy Test Set", "Large Language Models", "Reasoning", "Cognitive Theories", "Multilingual Evaluation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2501.13020", "pdf": "https://arxiv.org/pdf/2501.13020.pdf", "abs": "https://arxiv.org/abs/2501.13020", "title": "Characterizing Collective Efforts in Content Sharing and Quality Control for ADHD-relevant Content on Video-sharing Platforms", "authors": ["Hanxiu 'Hazel' Zhu", "Avanthika Senthil Kumar", "Sihang Zhao", "Ru Wang", "Xin Tong", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Video-sharing platforms (VSPs) have become increasingly important for\nindividuals with ADHD to recognize symptoms, acquire knowledge, and receive\nsupport. While videos offer rich information and high engagement, they also\npresent unique challenges, such as information quality and accessibility issues\nto users with ADHD. However, little work has thoroughly examined the video\ncontent quality and accessibility issues, the impact, and the control\nstrategies in the ADHD community. We fill this gap by systematically collecting\n373 ADHD-relevant videos with comments from YouTube and TikTok and analyzing\nthe data with a mixed method. Our study identified the characteristics of\nADHD-relevant videos on VSPs (e.g., creator types, video presentation forms,\nquality issues) and revealed the collective efforts of creators and viewers in\nvideo quality control, such as authority building, collective quality checking,\nand accessibility improvement. We further derive actionable design implications\nfor VSPs to offer more reliable and ADHD-friendly contents.", "AI": {"tldr": "This study analyzes ADHD-relevant videos on VSPs, identifying quality and accessibility issues.", "motivation": "To address the lack of research on video content quality and accessibility issues for users with ADHD on video-sharing platforms.", "method": "Systematic collection and analysis of 373 ADHD-relevant videos and comments from YouTube and TikTok using mixed methods.", "result": "Identified characteristics of ADHD-relevant videos and collective creator-viewer efforts for video quality control.", "conclusion": "Developed actionable design implications for VSPs to enhance reliability and ADHD-friendliness of content.", "key_contributions": ["Systematic analysis of ADHD-related video content on VSPs", "Identification of key quality and accessibility challenges", "Actionable design implications for improving VSPs for ADHD users"], "limitations": "", "keywords": ["ADHD", "Video-sharing platforms", "Accessibility", "Content quality", "Human-Computer Interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.13255", "pdf": "https://arxiv.org/pdf/2507.13255.pdf", "abs": "https://arxiv.org/abs/2507.13255", "title": "Automating Steering for Safe Multimodal Large Language Models", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "AI": {"tldr": "AutoSteer is a new intervention technology for improving safety in Multimodal Large Language Models (MLLMs) during inference, using a Safety Awareness Score, adaptive safety prober, and a Refusal Head to handle safety risks without model fine-tuning.", "motivation": "Recent advancements in MLLMs raise safety concerns with adversarial inputs; a need for a safety-improving method during inference is crucial.", "method": "Develops AutoSteer, a modular system consisting of a Safety Awareness Score, an adaptive safety prober, and a Refusal Head for managing safety risks during language model output generation.", "result": "AutoSteer significantly lowers the Attack Success Rate for various adversarial inputs while maintaining the general capabilities of MLLMs, as demonstrated through experiments on diverse safety benchmarks.", "conclusion": "AutoSteer provides a practical and interpretable framework for enhancing the safety of multimodal AI systems without the need for model retraining.", "key_contributions": ["Introduction of AutoSteer for adaptive interventions in MLLMs.", "Development of a Safety Awareness Score for identifying safety-relevant distinctions.", "Demonstrated reduction of attack success rates while preserving model capabilities."], "limitations": "", "keywords": ["Multimodal Large Language Models", "safety", "AutoSteer", "adversarial inputs", "inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13266", "pdf": "https://arxiv.org/pdf/2507.13266.pdf", "abs": "https://arxiv.org/abs/2507.13266", "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation", "authors": ["Jiazheng Li", "Hong Lu", "Kaiyue Wen", "Zaiwen Yang", "Jiaxuan Gao", "Hongzhou Lin", "Yi Wu", "Jingzhao Zhang"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "19 pages, 8 figures", "summary": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.", "AI": {"tldr": "The paper presents QuestA, a reinforcement learning strategy that improves multi-step reasoning in large language models through question augmentation.", "motivation": "To improve the effectiveness of reinforcement learning in enhancing multi-step reasoning capabilities in large language models, particularly on challenging problems.", "method": "The proposed method introduces partial solutions during the training phase to reduce problem difficulty and enhance learning signals in reinforcement learning.", "result": "QuestA significantly boosts performance in mathematical reasoning tasks, achieving new state-of-the-art results, with notable gains in metrics like pass@1 and pass@k on multiple benchmarks.", "conclusion": "QuestA offers a practical approach to enhance reasoning capabilities in LLMs using reinforcement learning, demonstrating improved sample efficiency and effectiveness on difficult tasks.", "key_contributions": ["Introduction of the QuestA method for question augmentation during RL training", "Demonstration of significant performance improvements on math reasoning benchmarks", "Theoretical explanations supporting increased sample efficiency"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Multi-step reasoning"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2507.13275", "pdf": "https://arxiv.org/pdf/2507.13275.pdf", "abs": "https://arxiv.org/abs/2507.13275", "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management", "authors": ["Luis Gasco", "Hermenegildo Fabregat", "Laura Garc√≠a-Sardi√±a", "Paula Estrella", "Daniel Deniz", "Alvaro Rodrigo", "Rabih Zbib"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.", "AI": {"tldr": "TalentCLEF 2025 presents an evaluation campaign for skill and job title intelligence, focusing on multilingual job title matching and job title-based skill prediction.", "motivation": "There is an increasing need for reliable and fair language models in Human Capital Management to improve talent acquisition and workforce planning, but existing benchmarks and models have been lacking in this domain.", "method": "The evaluation consists of two tasks: multilingual job title matching across several languages and skill prediction based on job titles, with datasets built from real job applications.", "result": "The campaign attracted 76 teams and produced over 280 submissions, revealing that training strategies significantly impact performance beyond just model size.", "conclusion": "TalentCLEF provides the first public benchmark for skill and job title intelligence, fostering the development of effective language technologies for labor markets.", "key_contributions": ["First evaluation campaign for skill and job title intelligence.", "Two distinct tasks: multilingual job title matching and skill prediction.", "Public benchmark promoting fair and reliable language models."], "limitations": "The study may be limited to specific languages and job titles, potentially reducing its applicability to other contexts.", "keywords": ["Natural Language Processing", "Human Capital Management", "Skill Prediction", "Job Title Matching", "Fairness in AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13285", "pdf": "https://arxiv.org/pdf/2507.13285.pdf", "abs": "https://arxiv.org/abs/2507.13285", "title": "Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis", "authors": ["Wang Xi", "Quan Shi", "Tian Yu", "Yujie Peng", "Jiayi Sun", "Mengxing Ren", "Zenghui Ding", "Ningguang Yao"], "categories": ["cs.CL", "68T50, 68T07", "I.2.7; I.2.11; H.5.2"], "comment": "22 pages, 7 figures, 3 tables. Submitted to an ACL-style conference", "summary": "Automated generation of high-quality media presentations is challenging,\nrequiring robust content extraction, narrative planning, visual design, and\noverall quality optimization. Existing methods often produce presentations with\nlogical inconsistencies and suboptimal layouts, thereby struggling to meet\nprofessional standards. To address these challenges, we introduce RCPS\n(Reflective Coherent Presentation Synthesis), a novel framework integrating\nthree key components: (1) Deep Structured Narrative Planning; (2) Adaptive\nLayout Generation; (3) an Iterative Optimization Loop. Additionally, we propose\nPREVAL, a preference-based evaluation framework employing rationale-enhanced\nmulti-dimensional models to assess presentation quality across Content,\nCoherence, and Design. Experimental results demonstrate that RCPS significantly\noutperforms baseline methods across all quality dimensions, producing\npresentations that closely approximate human expert standards. PREVAL shows\nstrong correlation with human judgments, validating it as a reliable automated\ntool for assessing presentation quality.", "AI": {"tldr": "The paper presents RCPS, a framework for automated media presentation generation that enhances quality through narrative planning, layout generation, and optimization.", "motivation": "Automate the generation of high-quality media presentations to meet professional standards, addressing challenges in content extraction and layout optimization.", "method": "RCPS integrates three components: Deep Structured Narrative Planning, Adaptive Layout Generation, and an Iterative Optimization Loop, along with PREVAL for evaluating presentation quality.", "result": "RCPS significantly outperforms existing methods in all quality dimensions, yielding presentations that align closely with human expert standards.", "conclusion": "The framework and evaluation tool improve the process of generating and assessing media presentations, validating the potential for automation in this field.", "key_contributions": ["Introduction of RCPS framework for media presentation generation", "Development of PREVAL for presentation quality assessment", "Demonstrated superior performance over baseline methods"], "limitations": "The study focuses on specific quality dimensions and may not cover broader contextual factors influencing presentation effectiveness.", "keywords": ["media presentations", "narrative planning", "layout generation", "quality optimization", "automated evaluation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.13300", "pdf": "https://arxiv.org/pdf/2507.13300.pdf", "abs": "https://arxiv.org/abs/2507.13300", "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research", "authors": ["Yilun Zhao", "Weiyuan Chen", "Zhijian Xu", "Manasi Patwardhan", "Yixin Liu", "Chengye Wang", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.", "AI": {"tldr": "AbGen is a benchmark for assessing LLMs in designing ablation studies for scientific research, revealing gaps in performance compared to human experts and the limitations of automated evaluations.", "motivation": "To evaluate how well LLMs can design ablation studies, which are crucial for scientific research, and to highlight existing performance gaps and evaluation challenges.", "method": "AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers, where LLMs generate detailed ablation study designs based on research context.", "result": "Evaluation of models like DeepSeek-R1-0528 and o4-mini shows a performance gap compared to human experts; automated evaluation methods were found unreliable.", "conclusion": "Developing AbGen-Eval for better assessment of automated evaluation systems is crucial for advancing LLM performance on scientific tasks.", "key_contributions": ["Introduction of AbGen benchmark for ablation study design", "Identification of performance gaps between LLMs and human experts", "Creation of AbGen-Eval for reliable evaluation of LLMs"], "limitations": "Current automated evaluation methods do not reliably measure LLM performance compared to human assessment.", "keywords": ["ablation studies", "large language models", "benchmarking", "evaluation methods", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.13318", "pdf": "https://arxiv.org/pdf/2507.13318.pdf", "abs": "https://arxiv.org/abs/2507.13318", "title": "HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "categories": ["cs.CL"], "comment": null, "summary": "Haptic signals, from smartphone vibrations to virtual reality touch feedback,\ncan effectively convey information and enhance realism, but designing signals\nthat resonate meaningfully with users is challenging. To facilitate this, we\nintroduce a multimodal dataset and task, of matching user descriptions to\nvibration haptic signals, and highlight two primary challenges: (1) lack of\nlarge haptic vibration datasets annotated with textual descriptions as\ncollecting haptic descriptions is time-consuming, and (2) limited capability of\nexisting tasks and models to describe vibration signals in text. To advance\nthis area, we create HapticCap, the first fully human-annotated\nhaptic-captioned dataset, containing 92,070 haptic-text pairs for user\ndescriptions of sensory, emotional, and associative attributes of vibrations.\nBased on HapticCap, we propose the haptic-caption retrieval task and present\nthe results of this task from a supervised contrastive learning framework that\nbrings together text representations within specific categories and vibrations.\nOverall, the combination of language model T5 and audio model AST yields the\nbest performance in the haptic-caption retrieval task, especially when\nseparately trained for each description category.", "AI": {"tldr": "This paper introduces HapticCap, a multimodal dataset for matching user descriptions to vibration haptic signals, addressing the challenges of limited existing datasets and models.", "motivation": "The objective is to improve the design of haptic signals that resonate with users by providing a structured dataset that connects user descriptions to haptic vibration signals.", "method": "The authors created HapticCap, a dataset with 92,070 haptic-text pairs, and proposed a haptic-caption retrieval task using a supervised contrastive learning framework.", "result": "The study demonstrated that combining a language model (T5) with an audio model (AST) achieved the best results in the haptic-caption retrieval task, particularly when models were trained separately for different description categories.", "conclusion": "HapticCap serves as a significant resource to better understand and design haptic feedback by connecting textual descriptions to haptic signals.", "key_contributions": ["Introduction of the first fully human-annotated haptic-captioned dataset (HapticCap) with 92,070 pairs.", "Development of a haptic-caption retrieval task, addressing the challenges of relating descriptions to vibration signals.", "Highlighting the effectiveness of T5 and AST models in improving task performance."], "limitations": "The dataset may not cover the full range of human sensory experiences with haptics, and the model performance might vary with different user demographics.", "keywords": ["haptics", "dataset", "vibration signals", "machine learning", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13325", "pdf": "https://arxiv.org/pdf/2507.13325.pdf", "abs": "https://arxiv.org/abs/2507.13325", "title": "Social and Political Framing in Search Engine Results", "authors": ["Amrit Poudel", "Tim Weninger"], "categories": ["cs.CL"], "comment": "Accepted to ICWSM 2026", "summary": "Search engines play a crucial role in shaping public discourse by influencing\nhow information is accessed and framed. While prior research has extensively\nexamined various dimensions of search bias -- such as content prioritization,\nindexical bias, political polarization, and sources of bias -- an important\nquestion remains underexplored: how do search engines and\nideologically-motivated user queries contribute to bias in search results. This\nstudy analyzes the outputs of major search engines using a dataset of political\nand social topics. The findings reveal that search engines not only prioritize\ncontent in ways that reflect underlying biases but also that\nideologically-driven user queries exacerbate these biases, resulting in the\namplification of specific narratives. Moreover, significant differences were\nobserved across search engines in terms of the sources they prioritize. These\nresults suggest that search engines may play a pivotal role in shaping public\nperceptions by reinforcing ideological divides, thereby contributing to the\nbroader issue of information polarization.", "AI": {"tldr": "This study examines how search engines and ideologically-motivated user queries contribute to bias in search results, revealing that both factors amplify specific narratives and reinforce ideological divides.", "motivation": "To explore how search engines and user ideologies contribute to bias in the portrayal of political and social topics.", "method": "Analysis of outputs from major search engines using a dataset focused on political and social topics.", "result": "Search engines reflect biases in content prioritization and ideologically-driven user queries exacerbate these biases, leading to narrative amplification and differing sources across engines.", "conclusion": "Search engines significantly shape public perceptions and contribute to information polarization by reinforcing ideological divides.", "key_contributions": ["Demonstrates the link between ideologically-driven queries and search bias.", "Highlights differences in content prioritization across various search engines.", "Sheds light on the role of search engines in public discourse and polarization."], "limitations": "", "keywords": ["search engines", "search bias", "ideological polarization", "public discourse", "information access"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13328", "pdf": "https://arxiv.org/pdf/2507.13328.pdf", "abs": "https://arxiv.org/abs/2507.13328", "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It", "authors": ["Yulu Qin", "Dheeraj Varghese", "Adam Dahlgren Lindstr√∂m", "Lucia Donatelli", "Kanishka Misra", "Najoung Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.", "AI": {"tldr": "This paper investigates the impact of vision-and-language (VL) training on language models' linguistic representations, particularly in taxonomic organization of lexical-conceptual knowledge.", "motivation": "To explore whether VL training leads to significant changes in the linguistic representations of language models, particularly in the context of taxonomic organization of concepts.", "method": "The authors compare text-only language models with their VL-trained counterparts using a text-only question-answering task that assesses taxonomic understanding. They perform targeted behavioral and representational analyses to investigate differences.", "result": "VL-trained models outperform text-only models on a task requiring taxonomic knowledge, highlighting improved deployment of lexical-conceptual knowledge in VL models, though their underlying taxonomic knowledge remains similar.", "conclusion": "VL training enhances the ability of models to utilize their knowledge in task-specific contexts but does not fundamentally change the nature of their taxonomic knowledge.", "key_contributions": ["Demonstrated the superiority of VL models in text-only taxonomic understanding tasks.", "Revealed that while taxonomic knowledge remains consistent, the representation and deployment of this knowledge improve with VL training.", "Provided evidence that VL training affects how language models represent questions based on taxonomic relations."], "limitations": "", "keywords": ["vision-and-language", "language models", "taxonomic knowledge", "question-answering", "linguistic representation"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.13332", "pdf": "https://arxiv.org/pdf/2507.13332.pdf", "abs": "https://arxiv.org/abs/2507.13332", "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner", "authors": ["Zhouqi Hua", "Wenwei Zhang", "Chengqi Lyu", "Yuzhe Gu", "Songyang Gao", "Kuikun Liu", "Kai Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.", "AI": {"tldr": "The paper introduces Turing MAchine Imitation Learning (TAIL) to enhance the length generalization ability of large language models (LLMs) by imitating Turing Machine processes, significantly improving performance on reasoning tasks.", "motivation": "To address the challenge of length generalization in Transformer-based LLMs, which struggle with longer sequences than those observed during training.", "method": "TAIL synthesizes chain-of-thought data that imitates the execution of a Turing Machine by expanding reasoning steps into atomic states and employing explicit memory fetch mechanisms.", "result": "TAIL improves the performance of Qwen2.5-7B on various tasks, surpassing previous methods and demonstrating significant length generalization capabilities using synthetic data.", "conclusion": "This work highlights the importance of Turing Machine concepts for enhancing LLM reasoning, and provides a promising direction for future research in synthetic data learning.", "key_contributions": ["Introduction of Turing MAchine Imitation Learning (TAIL)", "Demonstration of improved length generalization in LLMs", "Creation of a synthetic dataset covering various algorithms and tasks"], "limitations": "", "keywords": ["Turing Machine", "Length Generalization", "Imitation Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13334", "pdf": "https://arxiv.org/pdf/2507.13334.pdf", "abs": "https://arxiv.org/abs/2507.13334", "title": "A Survey of Context Engineering for Large Language Models", "authors": ["Lingrui Mei", "Jiayu Yao", "Yuyao Ge", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Jiazhi Liu", "Mingyu Li", "Zhong-Zhi Li", "Duzhen Zhang", "Chenlin Zhou", "Jiayi Mao", "Tianze Xia", "Jiafeng Guo", "Shenghua Liu"], "categories": ["cs.CL"], "comment": "ongoing work; 165 pages, 1401 citations", "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.", "AI": {"tldr": "This survey introduces Context Engineering, a discipline focused on optimizing contextual information for Large Language Models (LLMs), analyzing foundational components and system implementations, and revealing gaps in current model capabilities.", "motivation": "To establish a formal framework for improving the performance of LLMs through systematic optimization of contextual information during inference.", "method": "The paper presents a comprehensive taxonomy of Context Engineering, examining components like context retrieval, processing, and management, and their architectural integration into systems such as retrieval-augmented generation (RAG) and multi-agent systems.", "result": "The survey analyzes over 1300 research papers and identifies a critical research gap: current LLMs show advanced understanding but struggle with generating sophisticated long-form outputs.", "conclusion": "A unified framework for advancing context-aware AI is provided, highlighting the need for further research to address the identified performance gap in LLMs.", "key_contributions": ["Introduction of the Context Engineering discipline", "Comprehensive taxonomy of context components", "Identification of critical research gaps in LLM performance"], "limitations": "", "keywords": ["Large Language Models", "Context Engineering", "Information Retrieval"], "importance_score": 9, "read_time_minutes": 60}}
{"id": "2507.13335", "pdf": "https://arxiv.org/pdf/2507.13335.pdf", "abs": "https://arxiv.org/abs/2507.13335", "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes", "authors": ["Tyler Loakman", "William Thorne", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.", "AI": {"tldr": "This paper investigates the ability of Large Language Models (LLMs) to explain different types of humour, focusing on a dataset of 600 jokes that includes various forms beyond simple puns.", "motivation": "To explore whether LLMs can explain humour effectively across different types of jokes, moving beyond the focus on short pun-based jokes.", "method": "The authors curate a dataset of 600 jokes categorized into four types, including puns and topical humour, and manually write high-quality explanations for each joke. They assess the zero-shot capabilities of various LLMs in explaining these jokes.", "result": "The study finds that none of the tested models, including those designed for reasoning, can consistently provide adequate explanations for all joke types, revealing limitations in their understanding of humour.", "conclusion": "The findings indicate a significant research gap in computational humour, particularly in models' abilities to handle complex jokes that require real-world knowledge.", "key_contributions": ["Curated a diverse dataset of jokes with comprehensive explanations.", "Identified limitations in LLMs' abilities to explain complex humour forms.", "Highlighted the need for broader investigation in computational humour beyond simple jokes."], "limitations": "The dataset is limited to a specific set of joke types and may not represent the entirety of humour forms.", "keywords": ["humour", "Large Language Models", "dataset", "jokes", "computational humour"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2312.16054", "pdf": "https://arxiv.org/pdf/2312.16054.pdf", "abs": "https://arxiv.org/abs/2312.16054", "title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection", "authors": ["Bowen Zhang", "Daijun Ding", "Liwen Jing", "Hu Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data.", "AI": {"tldr": "Introducing Logically Consistent Chain-of-Thought (LC-CoT) for zero-shot stance detection, enhancing knowledge extraction and logical consistency in predictions.", "motivation": "To improve stance detection in zero-shot settings by addressing knowledge-task disconnect and enhancing logical consistency in predictions.", "method": "LC-CoT employs a three-step process: assessing the need for external knowledge, retrieving knowledge via API calls for processing by a separate LLM, and guiding the LLM with a manual exemplar using an if-then logical structure.", "result": "The proposed method outperforms traditional supervised methods in stance detection without needing labeled data.", "conclusion": "LC-CoT effectively enhances the model's capabilities in stance detection tasks by ensuring relevant and logically sound knowledge extraction.", "key_contributions": ["Introduction of LC-CoT for zero-shot stance detection", "Improved logical consistency in predictions using an if-then approach", "Elimination of the need for labeled data in stance detection tasks"], "limitations": "", "keywords": ["Zero-shot stance detection", "Logical consistency", "Chain-of-Thought", "AI", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2402.13722", "pdf": "https://arxiv.org/pdf/2402.13722.pdf", "abs": "https://arxiv.org/abs/2402.13722", "title": "Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis", "authors": ["S M Rafiuddin", "Mohammed Rakib", "Sadia Kamal", "Arunkumar Bagavathi"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures, Accepted at PAKDD 2024", "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.", "AI": {"tldr": "This paper presents adaptive masking methods to improve Aspect-Based Sentiment Analysis (ABSA) by addressing challenges in context adaptation and relevance of words in complex sentences.", "motivation": "The need to enhance the accuracy of Aspect-Based Sentiment Analysis by addressing limitations in current methods related to static hyperparameters and context adaptation.", "method": "Proposes adaptive masking methods that dynamically remove irrelevant tokens based on context to improve Aspect Term Extraction and Aspect Sentiment Classification in ABSA tasks.", "result": "The proposed methods outperform baseline methods in accuracy and F1 scores across four benchmark online review datasets, demonstrating better handling of complex sentences.", "conclusion": "Adaptive masking can significantly improve ABSA effectiveness and can be extended with multiple adaptations, showing promising results in both quantitative and qualitative analyses.", "key_contributions": ["Introduction of adaptive masking methods for ABSA", "Demonstrated performance improvements in accuracy and F1 scores", "Qualitative analysis showcasing the effectiveness of proposed methods"], "limitations": "", "keywords": ["Aspect-Based Sentiment Analysis", "Adaptive Masking", "Natural Language Processing", "Contextual Relevance"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2404.04631", "pdf": "https://arxiv.org/pdf/2404.04631.pdf", "abs": "https://arxiv.org/abs/2404.04631", "title": "On the Limitations of Large Language Models (LLMs): False Attribution", "authors": ["Tosin Adewumi", "Nudrat Habib", "Lama Alkhaled", "Elisa Barney"], "categories": ["cs.CL"], "comment": "This paper was accepted for presentation by Recent Advances in NLP\n  (RANLP) 2025 conference", "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models.", "AI": {"tldr": "This paper introduces the Simple Hallucination Index (SHI) as a new metric to measure hallucinations in LLMs, evaluates three state-of-the-art LLMs for author attribution, and finds that prediction accuracy negatively correlates with hallucination levels.", "motivation": "To address the limitations of large language models (LLMs) in terms of false attribution and to propose a new metric for measuring hallucinations.", "method": "Evaluated three LLMs‚ÄîGemma-7B, Mixtral 8x7B, and LLaMA-2-13B‚Äîon their ability to perform automatic author attribution in a zero-shot setting using divided chunks of popular books, assessing accuracy and new hallucination metric metrics.", "result": "Mixtral 8x7B achieved the highest prediction accuracy and lowest SHI, but also had the highest hallucination level for three books. Strong negative correlations between accuracy and SHI were reported, indicating the new metric's potential for broader application.", "conclusion": "The results suggest that the SHI can be a reliable metric for evaluating hallucinations in LLMs and that prediction accuracies relate to the frequency of Wikipedia mentions, leading to public release of data and code for reproducibility.", "key_contributions": ["Introduction of the Simple Hallucination Index (SHI) metric", "Empirical evaluation of author attribution accuracy for LLMs", "Public release of annotated data for further research"], "limitations": "The dependence on specific text chunks and the inherent challenges in author attribution tasks.", "keywords": ["hallucination", "language models", "author attribution", "NLP", "evaluation metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.01772", "pdf": "https://arxiv.org/pdf/2410.01772.pdf", "abs": "https://arxiv.org/abs/2410.01772", "title": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles", "authors": ["Yebowen Hu", "Xiaoyang Wang", "Wenlin Yao", "Yiming Lu", "Daoan Zhang", "Hassan Foroosh", "Dong Yu", "Fei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025), Vienna, Austria", "summary": "LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital.", "AI": {"tldr": "The paper presents DeFine, a modular framework for LLMs to manage uncertainty in complex decision-making scenarios by constructing probabilistic factor profiles and leveraging past analogical reasoning.", "motivation": "To enhance LLM decision-making by systematically incorporating uncertainty from speech transcripts addressing complex scenarios.", "method": "The DeFine framework builds probabilistic factor profiles from complex scenarios and integrates these profiles with analogical reasoning based on similar past experiences.", "result": "DeFine improves LLMs' ability to make informed decisions under uncertainty, particularly relevant in consulting and financial contexts.", "conclusion": "The separation of uncertainty quantification and its incorporation into decision-making enhances the LLMs' performance in complex scenarios.", "key_contributions": ["Introduction of the DeFine framework for LLMs", "Probabilistic factor profiles for handling uncertainty", "Integration of analogical reasoning for improved decision-making"], "limitations": "", "keywords": ["Large Language Models", "Decision Making", "Uncertainty", "Analogical Reasoning", "Financial Deliberation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.12774", "pdf": "https://arxiv.org/pdf/2410.12774.pdf", "abs": "https://arxiv.org/abs/2410.12774", "title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information", "authors": ["Yingya Li", "Timothy Miller", "Steven Bethard", "Guergana Savova"], "categories": ["cs.CL", "cs.AI"], "comment": "main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables", "summary": "The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.", "AI": {"tldr": "This paper proposes a metric for grouping tasks in multi-task learning based on pointwise V-usable information (PVI) to enhance model performance by maximizing learned information utility.", "motivation": "To improve multi-task learning by identifying effective task groupings that avoid negative transfer, which can occur with random or naive task associations.", "method": "The authors introduce the PVI metric to quantify task relatedness based on task difficulty and perform experiments on 15 NLP datasets across various domains to assess the effectiveness of their grouping method.", "result": "Joint learners using task groupings based on similar PVI estimates achieved competitive performance with fewer parameters, consistently across general, biomedical, and clinical domains.", "conclusion": "Grouping tasks with similar PVI estimates leads to improved multi-task learning outcomes, suggesting the PVI metric is effective for identifying beneficial task associations.", "key_contributions": ["Introduction of PVI as a metric for task relatedness in multi-task learning.", "Empirical validation of PVI-based task grouping on various NLP datasets.", "Demonstration of competitive performance with reduced parameter count in joint learning settings."], "limitations": "The choice of datasets may limit generalizability; results could vary with different task domains or learning frameworks.", "keywords": ["multi-task learning", "task grouping", "pointwise V-usable information", "PVI", "NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.20788", "pdf": "https://arxiv.org/pdf/2410.20788.pdf", "abs": "https://arxiv.org/abs/2410.20788", "title": "SCULPT: Systematic Tuning of Long Prompts", "authors": ["Shanu Kumar", "Akhila Yesantarao Venkata", "Shubhanshu Khandelwal", "Bishal Santra", "Parag Agrawal", "Manish Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ACL Main 2025", "summary": "Prompt optimization is essential for effective utilization of large language\nmodels (LLMs) across diverse tasks. While existing optimization methods are\neffective in optimizing short prompts, they struggle with longer, more complex\nones, often risking information loss and being sensitive to small\nperturbations. To address these challenges, we propose SCULPT (Systematic\nTuning of Long Prompts), a framework that treats prompt optimization as a\nhierarchical tree refinement problem. SCULPT represents prompts as tree\nstructures, enabling targeted modifications while preserving contextual\nintegrity. It employs a Critic-Actor framework that generates reflections and\napplies actions to refine the prompt. Evaluations demonstrate SCULPT's\neffectiveness on long prompts, its robustness to adversarial perturbations, and\nits ability to generate high-performing prompts even without any initial\nhuman-written prompt. Compared to existing state of the art methods, SCULPT\nconsistently improves LLM performance by preserving essential task information\nwhile applying structured refinements. Both qualitative and quantitative\nanalyses show that SCULPT produces more stable and interpretable prompt\nmodifications, ensuring better generalization across tasks.", "AI": {"tldr": "SCULPT is a framework for optimizing long prompts for LLMs using a hierarchical tree refinement approach.", "motivation": "Existing prompt optimization methods are ineffective for long prompts, risking information loss and being sensitive to perturbations.", "method": "SCULPT treats prompt optimization as a hierarchical tree refinement problem, using a Critic-Actor framework to generate reflections and apply actions for prompt refinement.", "result": "SCULPT demonstrates improved effectiveness on long prompts and robustness against adversarial perturbations, outperforming existing methods by ensuring essential task information preservation.", "conclusion": "SCULPT's approach results in more stable and interpretable prompt modifications, enhancing generalization across tasks when working with LLMs.", "key_contributions": ["Introduces a hierarchical tree structure for prompt representation.", "Implements a novel Critic-Actor framework for prompt refinement.", "Achieves better performance on long prompts compared to prior methods."], "limitations": "", "keywords": ["prompt optimization", "large language models", "SCULPT", "hierarchical refinement", "Critic-Actor framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.06208", "pdf": "https://arxiv.org/pdf/2411.06208.pdf", "abs": "https://arxiv.org/abs/2411.06208", "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization", "authors": ["Xinghua Zhang", "Haiyang Yu", "Cheng Fu", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.", "AI": {"tldr": "This paper introduces TRACE, a benchmark for evaluating complex instruction-following abilities in LLMs, and the IOPO alignment method for enhancing instruction adherence.", "motivation": "To address the gap in the ability of large language models to follow complex instructions due to limited evaluation data and lack of dedicated algorithms.", "method": "TRACE benchmark with 120K training and 1K evaluation data; IOPO alignment method considering input-output preference pairs to enhance model performance on instructions.", "result": "IOPO demonstrates significant improvements in instruction following, with up to 8.15% and 6.29% gains on in-domain and out-of-domain datasets, respectively.", "conclusion": "The proposed methods effectively enhance the ability of LLMs to follow complex instructions, confirming their utility through empirical results.", "key_contributions": ["Introduction of TRACE benchmark for complex instruction evaluation", "Development of IOPO alignment method for better instruction adherence", "Demonstrated effectiveness through extensive experiments"], "limitations": "", "keywords": ["large language models", "instruction following", "TRACE benchmark", "IOPO alignment", "preference optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.04652", "pdf": "https://arxiv.org/pdf/2501.04652.pdf", "abs": "https://arxiv.org/abs/2501.04652", "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG", "authors": ["Patrice B√©chard", "Orlando Marquez Ayala"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "7 pages, 2 figures. Accepted at Workshop on Structured Knowledge for\n  Large Language Models (SKnowLLM) at KDD 2025", "summary": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.", "AI": {"tldr": "This paper presents a method to instruction fine-tune a retriever encoder for Retrieval-Augmented Generation (RAG) applications, enabling scalability and efficiency by serving multiple domain-specific tasks with a single encoder.", "motivation": "The motivation is to address the challenges in deploying RAG applications, particularly the computational expense of fine-tuning LLMs, and the need for a single retriever to accommodate various domain-specific tasks.", "method": "The methodology involves instruction fine-tuning a small retriever encoder on diverse domain-specific tasks to enhance the quality of data retrieved for LLM input.", "result": "The study demonstrates that the instruction fine-tuned encoder generalizes well to out-of-domain scenarios and is effective in unseen retrieval tasks in real-world enterprise applications.", "conclusion": "The paper concludes that a unified retriever can reduce costs and improve speed while maintaining performance across different tasks.", "key_contributions": ["Instruction fine-tuning approach for retriever encoder", "One encoder serving multiple domain-specific use cases", "Demonstrates generalization to out-of-domain tasks"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "instruction fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.01491", "pdf": "https://arxiv.org/pdf/2502.01491.pdf", "abs": "https://arxiv.org/abs/2502.01491", "title": "Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation", "authors": ["Verna Dankers", "Vikas Raunak"], "categories": ["cs.CL"], "comment": "To appear at ACL 2025; 15 pages total (5 in the main paper, 3 pages\n  of limitations and references and 7 pages with appendices)", "summary": "In this work, we explore how instance-level memorization in the teacher\nNeural Machine Translation (NMT) model gets inherited by the student model in\nsequence-level knowledge distillation (SeqKD). We find that despite not\ndirectly seeing the original training data, students memorize more than\nbaseline models (models of the same size, trained on the original data) -- 3.4%\nfor exact matches and 57% for extractive memorization -- and show increased\nhallucination rates. Further, under this SeqKD setting, we also characterize\nhow students behave on specific training data subgroups, such as subgroups with\nlow quality and specific counterfactual memorization (CM) scores, and find that\nstudents exhibit amplified denoising on low-quality subgroups. Finally, we\npropose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD\nto reduce memorization and hallucinations. Overall, we recommend caution when\napplying SeqKD: students inherit both their teachers' superior performance and\ntheir fault modes, thereby requiring active monitoring.", "AI": {"tldr": "This paper investigates how knowledge is transferred in sequence-level knowledge distillation for Neural Machine Translation, noting that student models inherit memorization from teacher models, leading to increased hallucinations and recommending a new method to mitigate these issues.", "motivation": "To understand how instance-level memorization in teacher models affects student models during knowledge distillation in Neural Machine Translation.", "method": "The authors analyze memorization and hallucination rates in student models trained under sequence-level knowledge distillation and introduce a modified approach called Adaptive-SeqKD to reduce these effects.", "result": "Students show 3.4% more exact matches and 57% higher extractive memorization than baseline models, with increased hallucination rates, particularly in low-quality data subgroups.", "conclusion": "Caution is advised when applying sequence-level knowledge distillation, as students inherit both benefits and weaknesses from their teachers, necessitating careful monitoring.", "key_contributions": ["Inventory of memorization and hallucination rates in student models", "Characterization of student behavior on specific training data subgroups", "Introduction of Adaptive-SeqKD to reduce memorization effects"], "limitations": "The paper includes an analysis of limitations and potential biases in the study and the proposed method.", "keywords": ["Neural Machine Translation", "knowledge distillation", "memorization", "hallucination", "Adaptive-SeqKD"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.18699", "pdf": "https://arxiv.org/pdf/2502.18699.pdf", "abs": "https://arxiv.org/abs/2502.18699", "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment", "authors": ["Tianze Wang", "Dongnan Gui", "Yifan Hu", "Shuhang Lin", "Linjun Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ME"], "comment": "ICML 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.", "AI": {"tldr": "This paper presents Mixing Preference Optimization (MPO) for combining single-objective policies to improve reinforcement learning from human feedback and reduce costs.", "motivation": "The reliance on singular reward models in Reinforcement Learning from Human Feedback (RLHF) fails to capture diverse human preferences, leading to costly and unstable training processes.", "method": "Mixing Preference Optimization (MPO) is proposed as a post-processing framework that log-linearly combines existing policies using batch stochastic mirror descent to create a unified policy from multiple single-objective policies.", "result": "Empirical results show that MPO achieves balanced performance across varied human preferences while outperforming or matching existing models with lower computational costs.", "conclusion": "MPO provides a viable alternative to multi-objective approaches in RLHF by effectively aggregating diverse human feedback into a single objective framework.", "key_contributions": ["Introduction of Mixing Preference Optimization (MPO) framework", "Improved performance on aligning large language models with diverse human preferences", "Significantly reduced computational costs compared to existing models"], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Preference Optimization", "Large Language Models", "Computational Efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.08161", "pdf": "https://arxiv.org/pdf/2503.08161.pdf", "abs": "https://arxiv.org/abs/2503.08161", "title": "OASIS: Order-Augmented Strategy for Improved Code Search", "authors": ["Zuchen Gao", "Zizheng Zhan", "Xianming Li", "Erxin Yu", "Ziqi Zhan", "Haotian Zhang", "Bin Chen", "Yuqun Zhang", "Jing Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.", "AI": {"tldr": "The OASIS model improves code embedding training for code search by using order-based similarity labels to capture subtle differences among negative pairs, outperforming existing methods.", "motivation": "To enhance code embeddings for better performance in code-related LLM applications like code search by addressing the limitations of existing training methods which focus predominantly on positive-negative pair comparisons.", "method": "The proposed approach, OASIS, uses order-based similarity labels during training to help models recognize subtle semantic differences among negative pairs, enhancing the training of code embeddings.", "result": "Extensive benchmarking shows that OASIS significantly outperforms previous state-of-the-art models by effectively leveraging subtle semantic distinctions in negative pairs.", "conclusion": "The approach highlights the importance of considering subtle differences among negative pairs, improving the effectiveness of code embedding models in code search applications.", "key_contributions": ["Introduction of order-augmented strategy for code embeddings", "Demonstration of improved performance in code search", "Highlighting the significance of subtle semantic differences in negative pair training"], "limitations": "", "keywords": ["code embeddings", "large language models", "code search", "order-based similarity", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.12347", "pdf": "https://arxiv.org/pdf/2503.12347.pdf", "abs": "https://arxiv.org/abs/2503.12347", "title": "Synthesizing Privacy-Preserving Text Data via Finetuning without Finetuning Billion-Scale LLMs", "authors": ["Bowen Tan", "Zheng Xu", "Eric Xing", "Zhiting Hu", "Shanshan Wu"], "categories": ["cs.CL"], "comment": "Code available at https://github.com/tanyuqian/synthetic-private-data", "summary": "Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach.", "AI": {"tldr": "CTCL is a novel framework for generating privacy-preserving synthetic data that overcomes limitations of existing methods by using a lightweight conditional generator and clustering-based topic model.", "motivation": "Synthetic data generation is crucial for training models while ensuring data privacy, particularly with increasing restrictions around data usage. Existing methods face challenges due to resource constraints and reliance on manual prompts.", "method": "CTCL employs a lightweight 140M conditional generator pre-trained on public data and a clustering-based topic model. The generator undergoes DP finetuning on private data, and the topic model creates a DP histogram for effective sampling of synthetic data.", "result": "Evaluation shows that CTCL effectively generates high-quality synthetic data across five diverse domains, maintaining strong privacy while being scalable and requiring less extensive resources than prior methods.", "conclusion": "CTCL provides a robust solution for synthetic data generation with better controllability and reduced dependency on large resources or manual prompt engineering, proving its effectiveness in data privacy applications.", "key_contributions": ["Introduction of CTCL framework for synthetic data generation", "Utilization of a lightweight conditional generator", "Development of a clustering-based topic model for DP histogram representation"], "limitations": "Limited to scenarios where strong privacy is paramount and may require further validation in broader applications.", "keywords": ["synthetic data", "differential privacy", "data generation", "large language models", "clustering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.12989", "pdf": "https://arxiv.org/pdf/2503.12989.pdf", "abs": "https://arxiv.org/abs/2503.12989", "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models", "authors": ["Palakorn Achananuparp", "Ee-Peng Lim", "Yao Lu"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Accepted to ICWSM'26", "summary": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs.", "AI": {"tldr": "This paper presents a framework for job data annotation using large language models (LLMs) that integrates taxonomy-guided reasoning to improve occupational classification.", "motivation": "The need for standardized occupation classification in labor market analysis and the limitations of manual annotations.", "method": "A multi-stage framework involving inference, retrieval, and reranking, enhanced with taxonomy-guided reasoning examples.", "result": "The proposed framework shows improved performance in occupation and skill classification tasks while significantly reducing computational costs compared to models like GPT-4o.", "conclusion": "The framework provides a practical and scalable solution for occupation classification using LLMs.", "key_contributions": ["A multi-stage approach to improve occupation classification", "Integration of taxonomy-guided reasoning examples", "Cost-effective solution compared to leading LLMs"], "limitations": "Smaller models struggle more than larger ones with taxonomic understanding.", "keywords": ["occupation classification", "large language models", "taxonomy-guided reasoning", "inferencing", "retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.13733", "pdf": "https://arxiv.org/pdf/2503.13733.pdf", "abs": "https://arxiv.org/abs/2503.13733", "title": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings", "authors": ["Daniil Orel", "Dilshod Azizov", "Preslav Nakov"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.", "AI": {"tldr": "This paper presents a framework for detecting whether code is written by humans or generated by large language models (LLMs), addressing the challenges of code generation and integrity.", "motivation": "The rise of LLMs in programming raises concerns about programming skills, ethics, and assessment integrity, necessitating effective detection of LLM-generated code to uphold accountability.", "method": "The authors developed a framework that utilizes a large-scale dataset from various platforms and code generators. The framework employs rigorous data quality checks, feature engineering, and comparative analysis of traditional machine learning models, pre-trained language models, and LLMs for code detection.", "result": "The framework successfully distinguishes between human and LLM-generated code across multiple programming languages and sets a new benchmark for code generation detection.", "conclusion": "This research highlights the importance of accurately identifying code authorship in the context of LLM advancements and demonstrates the effectiveness of the proposed framework.", "key_contributions": ["A novel multi-language code detection framework", "Extensive evaluation on out-of-domain scenarios", "Establishing new benchmarks for detecting LLM-generated code"], "limitations": "", "keywords": ["code generation", "large language models", "authorship detection", "programming ethics", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.13425", "pdf": "https://arxiv.org/pdf/2504.13425.pdf", "abs": "https://arxiv.org/abs/2504.13425", "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering", "authors": ["Grace Byun", "Shinsun Lee", "Nayoung Choi", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": null, "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.", "AI": {"tldr": "The SecMulti-RAG framework improves enterprise retrieval-augmented generation by combining internal documents, expert knowledge, and safe external LLM-generated knowledge, enhancing response quality while mitigating data security risks.", "motivation": "Challenges in existing RAG systems for enterprises due to limited retrieval scope and data security risks.", "method": "The SecMulti-RAG framework retrieves from internal documents, pre-generated expert knowledge, and on-demand external LLM-generated knowledge while employing a filtering mechanism for security.", "result": "SecMulti-RAG achieves 79.3 to 91.9 percent win rates in correctness, richness, and helpfulness, along with human evaluations showing 56.3 to 70.4 percent improvements over traditional RAG.", "conclusion": "SecMulti-RAG offers a practical and secure solution for enterprise RAG challenges.", "key_contributions": ["Introduction of the Secure Multifaceted-RAG (SecMulti-RAG) framework", "Use of a filtering mechanism for safe external LLM utilization", "Significant performance improvement in report generation tasks"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Secure LLM", "Enterprise AI", "Human-Computer Interaction", "Knowledge Retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23121", "pdf": "https://arxiv.org/pdf/2505.23121.pdf", "abs": "https://arxiv.org/abs/2505.23121", "title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations", "authors": ["Yiming Lei", "Zhizheng Yang", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.", "AI": {"tldr": "This paper introduces ContextQFormer, a new approach for enhancing multi-turn interactions in multi-modal large language models, and presents the TMDialog dataset for future research.", "motivation": "To improve the capabilities of open-source multi-modal models in multi-turn interaction, particularly for long contexts, which existing models struggle with.", "method": "The authors develop a context modeling module called ContextQFormer that uses a memory block to enhance the representation of contextual information. They also create TMDialog, a new dataset for pre-training and evaluation, which features longer conversations for more effective dialogue modeling.", "result": "ContextQFormer demonstrates an improvement of 2%-4% in available rate compared to three baseline models when evaluated on the TMDialog dataset.", "conclusion": "ContextQFormer improves multi-turn multi-modal dialogue capabilities, and the TMDialog dataset will support further research in this area.", "key_contributions": ["Introduction of ContextQFormer for better context modeling in multi-modal dialogues", "Creation of the TMDialog dataset for enhanced multi-turn dialogue capabilities", "Demonstration of improved interaction rates over existing models."], "limitations": "", "keywords": ["Multi-modal", "Large Language Models", "Dialogue Systems", "Context Modeling", "Dataset"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.20495", "pdf": "https://arxiv.org/pdf/2506.20495.pdf", "abs": "https://arxiv.org/abs/2506.20495", "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning", "authors": ["Haoze Wu", "Yunzhi Yao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.", "AI": {"tldr": "ReCode is a framework enhancing LLMs' code generation in dynamic API scenarios through reinforcement learning and version migration.", "motivation": "Address the limitations of LLMs in adapting to updates in external library APIs, which affect reliable code generation.", "method": "A dataset of 2,000 entries is created for training LLMs on version migration, using a modified string similarity metric for reinforcement learning rewards.", "result": "ReCode improves LLMs' performance significantly in dynamic API environments, particularly in the CodeUpdateArena task, with less impact on general code abilities compared to supervised fine-tuning.", "conclusion": "ReCode shows consistent improvements across various LLMs and reinforcement learning algorithms, with Qwen2.5-Coder-7B outperforming larger models.", "key_contributions": ["Introduction of ReCode framework for code update adaptation.", "Development of a dataset for training LLMs on version migration.", "Creation of a modified string similarity metric for code evaluation."], "limitations": "Work in progress; practical limitations of deployment not addressed.", "keywords": ["Large Language Models", "Code Generation", "Reinforcement Learning", "API Changes", "Version Migration"], "importance_score": 9, "read_time_minutes": 10}}
