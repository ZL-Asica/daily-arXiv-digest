{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152.pdf", "abs": "https://arxiv.org/abs/2507.00152", "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian Möller"], "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "This paper investigates the effectiveness of text-based and multimodal LLMs on table understanding tasks, comparing their performance in scientific and non-scientific contexts using various table formats.", "motivation": "To explore the efficiency of LLMs in processing tabular data, which is crucial in many domains such as research, business, and medicine.", "method": "The study conducts a cross-domain and cross-modality evaluation of LLMs on tabular data, analyzing text and image representations while also performing interpretability analysis. It introduces the TableEval benchmark with 3017 tables in five formats.", "result": "The findings reveal that LLMs are robust across different modalities but encounter significant challenges with scientific tables.", "conclusion": "While LLMs show promise in understanding tables, further improvements are needed for handling complex scientific data.", "key_contributions": ["Introduced the TableEval benchmark with extensive table formats for evaluation.", "Investigated LLM performance across scientific and non-scientific table contexts.", "Performed interpretability analysis to examine context usage and relevance."], "limitations": "The study mainly focuses on specific domains and may not generalize to all types of tabular data.", "keywords": ["LLM", "Table Understanding", "Cross-domain Evaluation", "Interpretability", "Benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163.pdf", "abs": "https://arxiv.org/abs/2507.00163", "title": "Prompting as Scientific Inquiry", "authors": ["Ari Holtzman", "Chenhao Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "This paper argues that prompting in large language models (LLMs) should be viewed as a legitimate branch of behavioral science rather than mere alchemy, emphasizing its importance in unlocking the capabilities of LLMs.", "motivation": "To reframe the understanding of prompting in large language models as a scientific method essential for exploring and utilizing their complex behavior, rather than a primitive or ad-hoc approach.", "method": "The paper discusses the role of prompting as a behavioral science, contrasting it with mechanistic interpretability that examines neural structures.", "result": "The authors highlight that prompting has been fundamental in achieving significant advancements in LLMs, demonstrating its effectiveness in eliciting desired behaviors from these models.", "conclusion": "Prompting is presented as a crucial scientific approach in the study of LLMs, emphasizing its value and the need to approach it with the same rigor as other scientific methodologies.", "key_contributions": ["Promoting the scientific view of prompting in the context of LLMs.", "Highlighting the comparison between prompting and mechanistic interpretability.", "Establishing prompting as a key method for understanding and controlling LLM behaviors."], "limitations": "", "keywords": ["prompting", "large language models", "behavioral science", "mechanistic interpretability", "AI capabilities"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210.pdf", "abs": "https://arxiv.org/abs/2507.00210", "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han Lù", "Massimo Caccia", "Véronique Eglin", "Alexandre Aussem", "Jérémy Espinas", "Alexandre Lacoste"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "LineRetriever optimizes retrieval for adaptive planning in web navigation by focusing on observation lines relevant to future actions rather than just semantic similarity.", "motivation": "To address the limitations of traditional retrieval methods in capturing critical information for adaptive planning in web navigation tasks.", "method": "LineRetriever leverages a language model to identify and retrieve observation lines that are crucial for predicting future navigation steps, considering the planning horizon.", "result": "The experiments show that LineRetriever decreases the observation size at each step while maintaining performance, improving adaptive planning in web agents.", "conclusion": "LineRetriever effectively enhances the retrieval process for web navigation by prioritizing planning-relevant information, significantly improving action prediction.", "key_contributions": ["Introduction of LineRetriever for targeted retrieval in web navigation", "Empirical demonstration of reduced observation size with maintained performance", "Focus on planning horizon in retrieval methods."], "limitations": "", "keywords": ["Large language models", "Web navigation", "Adaptive planning", "LineRetriever", "Action prediction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214.pdf", "abs": "https://arxiv.org/abs/2507.00214", "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "This paper presents a two-stage approach to enhance text classification using Large Language Models (LLMs) to generate textual reasoning, which significantly improves accuracy in emotion classification tasks.", "motivation": "Standard classification models lack explicit reasoning, limiting their performance. This paper aims to address this by incorporating LLM-generated reasonings into the classification process.", "method": "The approach consists of two main stages: first, fine-tuning a Llama-3.2-1B-Instruct model on a reasoning dataset to generate reasonings; second, using this trained model to augment a training dataset for a generative model that predicts both reasoning and emotion.", "result": "The proposed generative model achieved an 8.7 percentage point accuracy improvement in emotion classification compared to a baseline model, demonstrating the efficacy of incorporating explicit reasoning.", "conclusion": "The findings highlight that LLM-generated reasonings can create richer datasets, enhancing performance in downstream NLP tasks while providing explicit explanations for predictions.", "key_contributions": ["Introduction of a two-stage text classification approach leveraging LLM-generated reasonings.", "Demonstration of improved accuracy in emotion classification through reasoning augmentation.", "Insights into the benefits of explicit reasoning for enhancing NLP model performance."], "limitations": "", "keywords": ["text classification", "Large Language Models", "reasoning generation", "emotion classification", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.00066", "pdf": "https://arxiv.org/pdf/2507.00066.pdf", "abs": "https://arxiv.org/abs/2507.00066", "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies.", "AI": {"tldr": "The study proposes the InSight-R framework, which enhances human reliability analysis in safety-critical domains by automating human failure event identification through empirical data and interface design optimization.", "motivation": "To improve human reliability analysis in safety-critical domains like nuclear power by overcoming the limitations of conventional methods that rely on expert judgment and lack integration with interface-level data.", "method": "The study introduces the InSight-R framework, which combines behavioral data with an interface-embedded knowledge graph to automate human failure event identification and assess the impact of interface design on operator performance.", "result": "Results show that InSight-R enhances objectivity and interpretability of human failure event identification and provides a scalable method for real-time human reliability assessment in digital control environments.", "conclusion": "The framework contributes to better interface design optimization and advances risk-informed human reliability assessment methodologies.", "key_contributions": ["Development of the InSight-R framework for automated HFE identification", "Integration of empirical data with interface design to improve reliability", "Scalable approach for real-time human reliability assessment"], "limitations": "", "keywords": ["Human Reliability", "Human-Machine Interface", "HFE Identification", "Risk Assessment", "Interface Design"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216.pdf", "abs": "https://arxiv.org/abs/2507.00216", "title": "Towards Style Alignment in Cross-Cultural Translation", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "This paper discusses how cultural differences can cause misalignment in communication styles during translation, particularly in LLMs. It introduces RASTA, a method aimed at improving stylistic alignment in translations to better reflect cultural norms.", "motivation": "To address the misalignment between a speaker's intended communication style and a listener's perception, particularly in translation involving cultural nuances and non-Western languages.", "method": "The paper presents RASTA (Retrieval-Augmented STylistic Alignment), a method that utilizes learned stylistic concepts to enhance the translation process of LLMs, ensuring it conveys cultural communication norms more accurately.", "result": "The implementation of RASTA demonstrates improved performance in translating stylistic elements, reducing biases toward neutrality, and enhancing the accuracy of translations in non-Western languages.", "conclusion": "RASTA shows promise in improving the stylistic alignment in LLM translations, helping to bridge cultural gaps in communication and preserving intended meanings during the translation process.", "key_contributions": ["Development of RASTA for improved stylistic translations", "Analysis of LLM failures in capturing cultural nuances", "Demonstration of enhanced performance in non-Western languages"], "limitations": "Limited to the scope of stylistic alignment; does not address other aspects of translation complexity.", "keywords": ["Human-Computer Interaction", "Cultural Differences", "Language Translation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00161", "pdf": "https://arxiv.org/pdf/2507.00161.pdf", "abs": "https://arxiv.org/abs/2507.00161", "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments.", "AI": {"tldr": "This paper explores the use of AI in civic education to reduce political polarization through emotionally-responsive storytelling that adapts to students' emotional engagement.", "motivation": "To address the issue of political polarization in civic education and promote open-mindedness through technology.", "method": "The study employed a design-based research approach to create an AI-mediated Digital Civic Storytelling (AI-DCS) platform that utilizes affective computing techniques.", "result": "The platform integrates facial emotion recognition and attention tracking to adapt narratives in real-time, personalizing the storytelling experience based on user emotional states.", "conclusion": "AI-supported strategies can effectively enhance civic education interventions by addressing affective polarization while preserving student autonomy.", "key_contributions": ["Development of AI-mediated storytelling platform for civic education.", "Utilization of affective computing for real-time narrative adaptation.", "Integration of GPT-4 for personalized storytelling based on user emotions."], "limitations": "", "keywords": ["political polarization", "AI", "civic education", "emotion recognition", "storytelling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239.pdf", "abs": "https://arxiv.org/abs/2507.00239", "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "This study investigates the ability of linear probes to decode information from language models (LMs) that employ instruction-tuning but may still respond harmfully to jailbreak prompts. It finds that such harmful information, while suppressed, remains accessible and can influence LM behavior.", "motivation": "To understand the resilience of harmful information in instruction-tuned language models and the implications for user safety and model alignment.", "method": "The research employs linear probes trained on the hidden states of various language models to assess the decodability of information elicited by jailbreak prompts, measuring correlations between probe outputs and model responses.", "result": "The study demonstrates that significant amounts of refused information in LMs can be linearly decoded, with Pearson correlations exceeding 0.8 for certain queries across models. Some probes trained on base models also transfer to instruction-tuned versions, revealing persistent harmful content.", "conclusion": "Instruction-tuning does not completely purge harmful information from language models; rather, it suppresses direct expression while allowing for linear accessibility, affecting downstream tasks and suggesting that models may still leverage this suppressed information.", "key_contributions": ["Demonstrated the decodability of harmful information in instruction-tuned LMs using linear probes.", "Showed that internal representations of refused properties are retained post instruction-tuning.", "Highlighted the indirect influence of suppressed information on LM behavior in downstream applications."], "limitations": "The study focuses on linear probes and their limitations in fully understanding the complexities of LM decision-making and alignment.", "keywords": ["language models", "harmful information", "instruction-tuning", "linear probes", "jailbreak prompts"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.00198", "pdf": "https://arxiv.org/pdf/2507.00198.pdf", "abs": "https://arxiv.org/abs/2507.00198", "title": "Exploring AR Label Placements in Visually Cluttered Scenarios", "authors": ["Ji Hwan Park", "Braden Roper", "Amirhossein Arezoumand", "Tien Tran"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate methods for placing labels in AR environments that have\nvisually cluttered scenes. As the number of items increases in a scene within\nthe user' FOV, it is challenging to effectively place labels based on existing\nlabel placement guidelines. To address this issue, we implemented three label\nplacement techniques for in-view objects for AR applications. We specifically\ntarget a scenario, where various items of different types are scattered within\nthe user's field of view, and multiple items of the same type are situated\nclose together. We evaluate three placement techniques for three target tasks.\nOur study shows that using a label to spatially group the same types of items\nis beneficial for identifying, comparing, and summarizing data.", "AI": {"tldr": "This paper investigates label placement techniques in augmented reality (AR) for cluttered scenes, highlighting the importance of spatial grouping for effective data summarization.", "motivation": "As AR scenes become visually cluttered with multiple items, effective label placement becomes challenging, necessitating new strategies for optimal user interaction.", "method": "Three label placement techniques were implemented and tested in AR environments with various types of items scattered within the user's field of view.", "result": "The study found that spatially grouping labels for the same type of items improved users' ability to identify, compare, and summarize data effectively.", "conclusion": "Effective label placement in AR can significantly enhance the user experience by aiding in data recognition and organization in cluttered environments.", "key_contributions": ["Implementation of three novel label placement techniques for AR", "Evaluation of the effectiveness of spatial grouping of labels", "Insights into user interaction with visual clutter in AR environments"], "limitations": "The study is limited to specific types of AR environments and may not generalize to all scenarios.", "keywords": ["augmented reality", "label placement", "user interaction", "spatial grouping", "visual clutter"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244.pdf", "abs": "https://arxiv.org/abs/2507.00244", "title": "The Algebraic Structure of Morphosyntax", "authors": ["Isabella Senturia", "Matilde Marcolli"], "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "This paper presents a mathematical model of the morphology-syntax interface within the framework of the Strong Minimalist Thesis, describing the formation of morphosyntactic trees without movement in morphology.", "motivation": "To develop a mathematical framework to understand the relationship between morphology and syntax and how they interact in the formation of words and sentences.", "method": "The paper constructs a model based on a magma of morphological trees and employs operads to describe the structure formation of morphosyntactic trees in a systematic way.", "result": "The study establishes a coproduct decomposition that extends the range of morphological trees impacting syntactic structures, revealing a flexibility in the morphology-syntax boundary.", "conclusion": "This work contributes to a deeper understanding of the interface between morphology and syntax, framing it within a robust mathematical model that influences how we view morphosyntactic structures.", "key_contributions": ["Mathematical modeling of the morphology-syntax interface", "Development of a coproduct decomposition for morphological trees", "Reinterpretation of Distributed Morphology operations in a new framework"], "limitations": "", "keywords": ["morphology-syntax interface", "Distributed Morphology", "operads"], "importance_score": 2, "read_time_minutes": 45}}
{"id": "2507.00202", "pdf": "https://arxiv.org/pdf/2507.00202.pdf", "abs": "https://arxiv.org/abs/2507.00202", "title": "Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group", "authors": ["Blade Frisch", "Betts Peters", "Keith Vertanen"], "categories": ["cs.HC"], "comment": null, "summary": "Purpose: Little research has explored the communication needs of autistic\nadults and how their needs differ from those of other disabled populations.\nAugmentative and Alternative Communication (AAC) can support these\ncommunication needs, but more guidance is needed on how to design AAC to\nsupport this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus\ngroup with five autistic adults to explore their social communication and\ncommunity engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that\n1) participants' emotional experiences impacted the communication methods they\nused, 2) speaking autistic adults can benefit from AAC use, and 3) autistic\nshutdown creates dynamic communication needs. We present implications for\nfuture AAC design: supporting communication in times of shutdown, indicating\ncommunication ability to communication partners, and a need to better\nunderstand the fear of using AAC. These implications can inform the design for\nfuture AAC systems. We also provide themes for future autism research:\nexploring the impact of a late diagnosis, gaining a better understanding of the\ncommunication needs during autistic shutdown, and expanding research to include\nthe social and environmental factors that impact communication. Finally, we\nprovide guidance on how future online focus groups can be run in an accessible\nmanner.", "AI": {"tldr": "This paper explores the unique communication needs of autistic adults and how Augmentative and Alternative Communication (AAC) can support them.", "motivation": "To understand and address the distinct communication needs of autistic adults, which are often overlooked compared to other disabled populations.", "method": "An online, asynchronous, text-based focus group was conducted with five autistic adults to discuss their social communication and community engagement.", "result": "The analysis revealed that emotional experiences affect communication methods, that AAC can benefit speaking autistic adults, and highlighted dynamic communication needs during autistic shutdowns.", "conclusion": "The study provides crucial implications for AAC design, including the need to support communication during shutdowns and better understand the fears associated with AAC use, guiding future research on autism and communication.", "key_contributions": ["Identified the impact of emotional experiences on communication methods for autistic adults.", "Highlighted the benefits of AAC for speaking autistic individuals.", "Discussed dynamic communication needs during autistic shutdowns."], "limitations": "The study is based on a small focus group and may not capture the full spectrum of autistic adults' communication needs.", "keywords": ["Augmentative and Alternative Communication", "autistic adults", "communication needs", "social engagement", "AAC design"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246.pdf", "abs": "https://arxiv.org/abs/2507.00246", "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "This paper explores the token efficiency and accuracy of reasoning in multilingual contexts using RLMs, finding that non-English reasoning can reduce token usage while maintaining accuracy.", "motivation": "To evaluate whether English is the most token-efficient language for reasoning, given the multilingual capabilities of modern language models.", "method": "The study evaluates three open-source reasoning language models (DeepSeek R1, Qwen 2.5, and Qwen 3) across four math datasets in seven different languages.", "result": "Non-English reasoning reduces token usage and maintains accuracy, with improvements persisting even after translating reasoning traces into English.", "conclusion": "The findings highlight the importance of multilingual reasoning capabilities in language models, suggesting that researchers should consider broader language perspectives.", "key_contributions": ["Demonstration that reasoning in non-English can be more token-efficient than in English.", "Evidence that improvements in reasoning are genuine and not merely linguistic artifacts.", "Encouragement for stronger multilingual foundations in LRM development."], "limitations": "The extent of improvement in reasoning depends on the multilingual strengths of the models evaluated.", "keywords": ["Language Reasoning Models", "multilingual data", "token efficiency", "reasoning accuracy", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.00271", "pdf": "https://arxiv.org/pdf/2507.00271.pdf", "abs": "https://arxiv.org/abs/2507.00271", "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"", "authors": ["Zhuochao Peng", "Jiaxin Xu", "Jun Hu", "Haian Xue", "Laurens A. G. Kolks", "Pieter M. A. Desmet"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted to International Conference on Social Robotics + AI (ICSR\n  2025)", "summary": "While recent research highlights the potential of social robots to support\nmood regulation, little is known about how prospective users view their\nintegration into everyday life. To explore this, we conducted an exploratory\ncase study that used a speculative robot concept \"Mora\" to provoke reflection\nand facilitate meaningful discussion about using social robots to manage\nsubtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a\ncommon dip in mood that occurs at the end of the weekend, as a relatable\ncontext in which to explore individuals' insights. Using a video prototype and\na co-constructing stories method, we engaged 15 participants in imagining\ninteractions with Mora and discussing their expectations, doubts, and concerns.\nThe study surfaced a range of nuanced reflections around the attributes of\nsocial robots like empathy, intervention effectiveness, and ethical boundaries,\nwhich we translated into design considerations for future research and\ndevelopment in human-robot interaction.", "AI": {"tldr": "This study explores user perceptions of social robots, particularly focusing on mood regulation through a speculative robot concept named \"Mora\" and the context of the \"Sunday Blues.\"", "motivation": "To understand how users view the integration of social robots into everyday life for mood regulation purposes.", "method": "An exploratory case study using a video prototype and a co-constructing stories method involving 15 participants to discuss their interactions with the robot Mora.", "result": "Participants shared varied insights about social robots, addressing attributes like empathy and intervention effectiveness, leading to useful design considerations for human-robot interaction.", "conclusion": "The study highlighted important reflections on social robots that can inform future research and development in HRI.", "key_contributions": ["Introduction of the speculative robot concept 'Mora' for mood regulation", "Insights into user perceptions of empathy and ethical implications in social robots", "Design considerations derived from user interactions for future HRI research"], "limitations": "", "keywords": ["social robots", "mood regulation", "human-robot interaction", "design considerations", "empathy"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258.pdf", "abs": "https://arxiv.org/abs/2507.00258", "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "This paper evaluates the privacy risks associated with fine-tuning LLMs, focusing on the effectiveness of prompt-based versus parameter-based fine-tuning in resisting membership inference attacks (MIAs).", "motivation": "The paper aims to address the overlooked privacy risks associated with memorization during the fine-tuning of pre-trained large language models (LLMs).", "method": "The authors categorize popular fine-tuning approaches and assess their impact on memorization using membership inference attacks (MIAs) as a measurement method.", "result": "Prompt-based fine-tuning is demonstrated to achieve competitive performance with lower vulnerability to MIAs compared to parameter-based fine-tuning, which is found to leak more private information.", "conclusion": "The findings suggest that prompt-based fine-tuning is a more privacy-preserving alternative to parameter-based fine-tuning.", "key_contributions": ["Identification of privacy risks in fine-tuning LLMs through MIAs.", "Comparison of prompt-based and parameter-based fine-tuning methods regarding privacy.", "Demonstration that prompt-based fine-tuning has lower memorization regardless of model scale."], "limitations": "The study primarily focuses on membership inference attacks and may not cover all privacy concerns associated with fine-tuning LLMs.", "keywords": ["large language models", "fine-tuning", "privacy risks", "membership inference attacks", "prompt-based fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286.pdf", "abs": "https://arxiv.org/abs/2507.00286", "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "AI": {"tldr": "This paper explores the use of Generative AI tools by blind and low vision individuals, focusing on their current practices, design preferences, and the balance between visual accessibility and privacy concerns.", "motivation": "To understand how blind and low vision individuals use Generative AI tools for managing visual content while considering privacy implications.", "method": "An interview study with 21 participants who are blind or low vision to gather insights on their practices and design preferences regarding GenAI tools.", "result": "Participants reported various practices that balance privacy, efficiency, and emotional agency, identifying six scenarios where privacy risks are pertinent. Key design preferences included on-device processing and multimodal interaction methods.", "conclusion": "The study offers actionable design recommendations to enhance user-centered visual privacy in the context of Generative AI, advocating for a broader understanding of privacy and data handling.", "key_contributions": ["Investigation of privacy concerns in the use of GenAI by BLV individuals", "Identification of design preferences for enhancing visual privacy", "Actionable recommendations for developers of GenAI tools focused on accessibility and privacy."], "limitations": "", "keywords": ["Generative AI", "Blind and Low Vision", "Visual Privacy", "User-Centered Design", "Accessibility"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297.pdf", "abs": "https://arxiv.org/abs/2507.00297", "title": "Natural language processing for African languages", "authors": ["David Ifeoluwa Adelani"], "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "The dissertation focuses on improving NLP for low-resource African languages through data quality enhancement and adaptation of multilingual language models.", "motivation": "To address the challenges faced by multilingual models trained on low-resource African languages and the lack of labeled datasets for these languages in NLP research.", "method": "The author analyzes the quality of existing corpora, curates a high-quality corpus, and develops human-annotated datasets for named entity recognition and machine translation, evaluated through various learning settings.", "result": "The study empirically demonstrates that the quality of semantic representations in word embeddings is influenced by the quality of pre-training data, and shows the potential of multilingual pre-trained models for unseen African languages.", "conclusion": "The dissertation concludes that improving labeled datasets and utilizing multilingual PLMs can enhance the NLP performance for low-resource languages in Sub-Saharan Africa.", "key_contributions": ["Curated a high-quality corpus for low-resource African languages.", "Developed large-scale human-annotated datasets for 21 African languages.", "Empirical evaluation demonstrating the applicability of multilingual PLMs in low-resource scenarios."], "limitations": "", "keywords": ["Natural Language Processing", "Low-resource languages", "Multilingual models", "Word embeddings", "African languages"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00299", "pdf": "https://arxiv.org/pdf/2507.00299.pdf", "abs": "https://arxiv.org/abs/2507.00299", "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"", "authors": ["Olivia Figueira", "Pranathi Chamarthi", "Tu Le", "Athina Markopoulou"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "TikTok, the social media platform that is popular among children and\nadolescents, offers a more restrictive \"Under 13 Experience\" exclusively for\nyoung users in the US, also known as TikTok's \"Kids Mode\". While prior research\nhas studied various aspects of TikTok's regular mode, including privacy and\npersonalization, TikTok's Kids Mode remains understudied, and there is a lack\nof transparency regarding its content curation and its safety and privacy\nprotections for children. In this paper, (i) we propose an auditing methodology\nto comprehensively investigate TikTok's Kids Mode and (ii) we apply it to\ncharacterize the platform's content curation and determine the prevalence of\nchild-directed content, based on regulations in the Children's Online Privacy\nProtection Act (COPPA). We find that 83% of videos observed on the \"For You\"\npage in Kids Mode are actually not child-directed, and even inappropriate\ncontent was found. The platform also lacks critical features, namely parental\ncontrols and accessibility settings. Our findings have important design and\nregulatory implications, as children may be incentivized to use TikTok's\nregular mode instead of Kids Mode, where they are known to be exposed to\nfurther safety and privacy risks.", "AI": {"tldr": "This paper audits TikTok's Kids Mode, revealing significant shortcomings in content curation and safety measures for children.", "motivation": "To investigate TikTok's Kids Mode due to its lack of research and transparency in content curation and safety for young users.", "method": "An auditing methodology was proposed and applied to analyze the content and features of TikTok's Kids Mode.", "result": "It was found that 83% of videos on the Kids Mode 'For You' page are not child-directed, with inappropriate content also present, and critical safety features are missing.", "conclusion": "The findings suggest that TikTok's Kids Mode may not provide adequate protection for children, potentially pushing them towards the regular mode, which carries additional risks.", "key_contributions": ["Proposal of an auditing methodology for TikTok's Kids Mode", "Characterization of content curation in Kids Mode", "Identification of safety and privacy gaps in TikTok for children"], "limitations": "The study focuses solely on TikTok and may not represent broader patterns in Kids Modes on other platforms.", "keywords": ["TikTok", "Kids Mode", "Content Curation", "Child Safety", "COPPA"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322.pdf", "abs": "https://arxiv.org/abs/2507.00322", "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "This study explores the errors in language models related to generating balanced parentheses and presents a method called RASteer to enhance model performance by leveraging reliable components within the models.", "motivation": "To understand and mitigate the errors language models make with simple syntactic tasks like balanced parentheses generation, which persist across various model sizes.", "method": "The study investigates the components of language models that contribute to errors and introduces RASteer, a method designed to identify and boost the contributions of reliable components to improve performance on specified tasks.", "result": "The RASteer method significantly enhances performance on balanced parentheses tasks, improving accuracy from 0% to approximately 100%, and also shows enhancements in arithmetic reasoning tasks with gains of up to 20%.", "conclusion": "The introduction of RASteer not only improves specific syntactic tasks but also suggests a broader applicability for enhancing model performance in various other tasks by focusing on reliable components.", "key_contributions": ["Introduced RASteer for improving language model performance on specific tasks.", "Analyzed the causes of errors in language models regarding balanced parentheses.", "Achieved significant accuracy improvements in both balanced parentheses and arithmetic reasoning tasks."], "limitations": "", "keywords": ["language models", "syntactic tasks", "performance improvement", "RASteer", "arithmetic reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.00305", "pdf": "https://arxiv.org/pdf/2507.00305.pdf", "abs": "https://arxiv.org/abs/2507.00305", "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation", "authors": ["Deland Liu", "Frigyes Samuel Racz", "Zoe Lalji", "Jose del R. Millan"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in\nstate (CLIS) can lose all reliable motor control and are left without any means\nof communication. It remains unknown whether non-invasive electroencephalogram\n(EEG) based brain-computer interfaces (BCIs) can support volitional\ncommunication in CLIS. Here, we show that a CLIS patient was able to operate an\nEEG-based BCI across multiple online sessions to respond to both general\nknowledge and personally relevant assistive questions. The patient delivered\n\"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at\ndifferent channels, guided by real-time auditory feedback from the BCI. The\npatient communicated assistive needs above chance in all sessions, achieving a\nperfect score in the final session. Performance on general knowledge questions\nvaried across sessions, with two sessions showing accurate and above-chance\nresponses, while the first and last sessions remained at chance level. The\npatient also showed consistent modulation patterns over time. These findings\nsuggest that non-invasive BCIs may offer a potential pathway for restoring\nbasic communication in CLIS.", "AI": {"tldr": "This paper demonstrates that a CLIS patient can use an EEG-based BCI to communicate by modulating brain activity, suggesting a method for restoring communication in completely locked-in states.", "motivation": "To explore the potential of EEG-based BCIs for communication in patients with amyotrophic lateral sclerosis (ALS) who are in a completely locked-in state (CLIS).", "method": "The study involved a CLIS patient using an EEG-based BCI to respond to assistive questions, utilizing real-time auditory feedback to control alpha and beta band power for communication.", "result": "The patient was able to communicate 'Yes'/'No' responses effectively, achieving a perfect score in the final session, and demonstrated consistent modulation patterns over time.", "conclusion": "Non-invasive EEG-based BCIs may offer a viable solution for restoring basic communication abilities in patients with CLIS.", "key_contributions": ["Demonstration of effective communication using EEG in CLIS patients.", "Establishment of real-time auditory feedback methods for BCI operation.", "Evidence of consistent modulation patterns in brain activity over multiple sessions."], "limitations": "Limited sample size with only one patient, which may affect the generalizability of results.", "keywords": ["EEG", "BCI", "communication", "CLIS", "ALS"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330.pdf", "abs": "https://arxiv.org/abs/2507.00330", "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "COLDSELECT is a novel approach for joint verbalizer and instance selection in prompt-based methods that enhances performance in cold-start scenarios by modeling data diversity and relationships.", "motivation": "Existing prompt-based methods for pre-trained language models are sensitive to template and few-shot instance selection, especially in cold-start settings without labeled data. This paper aims to improve selection methods by addressing the overlooked dependency between instances and verbalizers.", "method": "COLDSELECT maps the vocabulary of pre-trained language models and masked embeddings into a shared space, using dimensionality reduction and clustering to achieve efficient and diverse instance selection.", "result": "Experiments on eight benchmarks reveal that COLDSELECT significantly reduces uncertainty and enhances generalization, demonstrating superior performance in verbalizer and few-shot instance selection compared to established baselines.", "conclusion": "COLDSELECT effectively captures data relationships and improves the selection process in cold-start scenarios, proving beneficial for applications relying on prompt-based methods with limited labeled data.", "key_contributions": ["Proposes COLDSELECT for joint verbalizer and instance selection.", "Implements a method to optimize for minimal uncertainty and maximal diversity.", "Demonstrates substantial performance improvements across multiple benchmarks."], "limitations": "", "keywords": ["prompt-based methods", "pre-trained language models", "cold-start", "instance selection", "verbalizer selection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333.pdf", "abs": "https://arxiv.org/abs/2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "AI": {"tldr": "This study presents a visualization system for improving marksmanship training effectiveness using first-person shooting video recordings combined with visual analytics.", "motivation": "To enhance marksmanship training beyond traditional methods limited to stance and post-session accuracy analysis.", "method": "Developed five composite visualizations using first-person video enriched with metrics; evaluated through a mixed-methods study with 10 participants (5 experts and 5 novices).", "result": "A dashboard-style composite view was preferred by participants in 9 out of 10 comparisons, enhancing understanding of shooting techniques across skill levels.", "conclusion": "Integrating first-person video with visual analytics can significantly improve coaching in marksmanship and potentially in other precision sports.", "key_contributions": ["Development of a novel shooting visualization system", "Demonstrated effectiveness of composite visualizations in marksmanship training", "Insights for applying visual analytics to other precision sports"], "limitations": "Limited sample size of participants; specific to marksmanship.", "keywords": ["marksmanship", "visualization", "coaching", "sports analytics", "human-computer interaction"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355.pdf", "abs": "https://arxiv.org/abs/2507.00355", "title": "Question Decomposition for Retrieval-Augmented Generation", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "This paper presents a new RAG pipeline that uses question decomposition to improve retrieval and answer accuracy for multi-hop questions.", "motivation": "To enhance the effectiveness of retrieval-augmented generation (RAG) in answering multi-hop questions where relevant information is distributed across different documents.", "method": "An LLM decomposes multi-hop questions into sub-questions, retrieves passages for each, and reranks the merged candidate pool to improve information coverage and precision.", "result": "The proposed method shows significant improvement in retrieval performance (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG approaches.", "conclusion": "Using LLM-driven question decomposition alongside standard reranking techniques effectively improves information retrieval for complex queries without necessitating additional training or special indexing.", "key_contributions": ["Introduction of question decomposition for RAG pipelines", "Demonstrated improvement in multi-hop question answering", "Validation on established benchmarks (MultiHop-RAG and HotpotQA)"], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "question decomposition", "multi-hop questions", "information retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.00513", "pdf": "https://arxiv.org/pdf/2507.00513.pdf", "abs": "https://arxiv.org/abs/2507.00513", "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "ACM CSCW Poster 2025", "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "AI": {"tldr": "This study explores the perception of AI assistance among customer service representatives in a power grid service call center, highlighting both alleviated and newly introduced burdens.", "motivation": "To understand the impact of AI integration on employee-customer interactions within customer service environments.", "method": "Field visit and semi-structured interviews with 13 customer service representatives (CSRs).", "result": "AI assistance helps reduce traditional burdens like typing and memorizing but introduces new challenges such as compliance and psychological stress.", "conclusion": "A deeper understanding of the dual nature of AI integration in organizational contexts and its effects on CSRs is required for future implementations.", "key_contributions": ["Insights into CSRs' experiences with AI tools", "Identification of new burdens introduced by AI", "Contribution to understanding AI's role in organizational settings"], "limitations": "", "keywords": ["AI integration", "customer service", "employee-customer interaction", "burdens", "organizational settings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380.pdf", "abs": "https://arxiv.org/abs/2507.00380", "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "authors": ["Vojtěch Lanz", "Jan Hajič jr"], "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "This paper explores the segmentation of Gregorian melodies using a hierarchical language model, achieving state-of-the-art results in mode classification while examining the relationship between melody structure and memory efficiency.", "motivation": "To investigate the long-debated theory of centonisation in Gregorian chant melodies and to determine if an optimal segmentation model can provide insights into their melodic structure and memorization.", "method": "The authors utilize nested hierarchical Pitman-Yor language models for unsupervised segmentation of chant melodies.", "result": "The segmentation method achieved state-of-the-art performance in mode classification and provided empirical evidence indicating a link between mode classification and memory efficiency.", "conclusion": "Despite achieving a memory-optimal segmentation, the resulting segments do not conform to traditional notions of centonisation.", "key_contributions": ["Introduced an optimal unsupervised segmentation method for Gregorian chant melodies.", "Demonstrated the efficacy of segmentations in mode classification surpassing music-theoretical features.", "Provided empirical evidence connecting memory efficiency with modal structure in chant melodies."], "limitations": "The study's findings challenge the conventional understanding of centonisation but do not establish a definitive alternative theory.", "keywords": ["Gregorian chant", "centonisation", "segmentation", "Pitman-Yor", "memory efficiency"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.00596", "pdf": "https://arxiv.org/pdf/2507.00596.pdf", "abs": "https://arxiv.org/abs/2507.00596", "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.", "AI": {"tldr": "Introduction of Gaze3P dataset for quantifying user-perceived privacy using gaze data and machine learning.", "motivation": "To address the limitations of previous research on quantifying user-perceived privacy and its application in privacy-preserving techniques.", "method": "Creation of Gaze3P dataset with gaze data from 100 participants and training of a machine learning model to predict perceived privacy based on eye gaze.", "result": "The models developed demonstrate high accuracy in predicting user-perceived privacy, which can optimize differentially private mechanisms' parameters.", "conclusion": "Gaze3P enables better alignment of privacy-preserving techniques with user expectations by leveraging predicted privacy from gaze data.", "key_contributions": ["First dataset designed for systematic investigations into user-perceived privacy", "Application of gaze data to predict privacy perceptions", "Demonstration of improved optimization in privacy techniques through machine learning."], "limitations": "", "keywords": ["privacy", "machine learning", "gaze data", "user-perception", "differential privacy"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389.pdf", "abs": "https://arxiv.org/abs/2507.00389", "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL is a causal prompting framework for Implicit Sentiment Analysis that improves reasoning accuracy and robustness in sentiment inference by addressing biases in LLMs.", "motivation": "To enhance Implicit Sentiment Analysis by integrating causal inference into Large Language Model prompting methods, addressing internal biases and improving reasoning accuracy.", "method": "CAPITAL decomposes the causal effect of input prompts on reasoning chains and final output using encoder-based clustering and a contrastive learning objective, applied to three LLMs on benchmark datasets.", "result": "CAPITAL outperforms strong baselines in accuracy and robustness in sentiment analysis tasks, especially under adversarial testing conditions.", "conclusion": "The proposed framework demonstrates significant improvements in bias-aware sentiment reasoning through causal prompting, offering a new approach to using LLMs in ISA.", "key_contributions": ["Development of the CAPITAL framework for causal prompting in ISA", "Empirical results showing improved robustness and accuracy of sentiment analysis", "Integration of causal inference into LLM prompting methodologies"], "limitations": "", "keywords": ["Implicit Sentiment Analysis", "Large Language Models", "Causal Inference", "Reasoning", "Bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00657", "pdf": "https://arxiv.org/pdf/2507.00657.pdf", "abs": "https://arxiv.org/abs/2507.00657", "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.", "AI": {"tldr": "This paper analyzes the behavior of Large Language Models (LLMs) in simulating political discourse on social media, revealing issues of polarization and reliability in their outputs.", "motivation": "To investigate how LLMs mimic human political discourse on social media during the 2024 U.S. presidential election.", "method": "The study constructs LLM agents based on 1,186 real users and evaluates their replies to politically salient tweets under Zero Shot and Few Shot conditions.", "result": "Rich contextualization of LLMs improves internal consistency but amplifies polarization and harmful language traits; a new phenomenon called 'generation exaggeration' was identified.", "conclusion": "LLMs reconstruct user behaviors rather than emulate them, leading to structural biases that may undermine their reliability for use in social contexts.", "key_contributions": ["Demonstration of how LLMs amplify polarization and harmful language.", "Identification of 'generation exaggeration' in LLM outputs.", "Critical assessment of LLMs as proxies for human behavior in social media."], "limitations": "The study may not generalize beyond the specific context of the 2024 U.S. presidential election and its data set of tweets.", "keywords": ["Large Language Models", "political discourse", "social media", "polarization", "toxicity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439.pdf", "abs": "https://arxiv.org/abs/2507.00439", "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "This work demonstrates how simple supervision can enhance language model alignment with diverse population groups across various topics and datasets.", "motivation": "To improve the prediction accuracy of language models in answering subjective questions for different population groups.", "method": "Implemented a straightforward supervision approach to guide language model training and evaluated its performance on three datasets.", "result": "The proposed method significantly increased alignment between language model responses and the perspectives of diverse groups, with variable success across specific demographics.", "conclusion": "Simple supervision can effectively improve language model alignment, and our findings help guide practical applications and further research.", "key_contributions": ["Improvement of LLM alignment with diverse population groups using simple supervision", "Evaluation of alignment variations across specific demographics", "Open-sourced benchmark for future research"], "limitations": "Further exploration needed on the effectiveness of the approach across all demographics and datasets.", "keywords": ["language model", "supervision", "population groups", "alignment", "benchmark"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00775", "pdf": "https://arxiv.org/pdf/2507.00775.pdf", "abs": "https://arxiv.org/abs/2507.00775", "title": "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review", "authors": ["Haonan Yao", "Lingyun Yu", "Lijie Yao"], "categories": ["cs.HC"], "comment": null, "summary": "We present a systematic review on tasks, interactions, and visualization\nwidgets (refer to tangible entities that are used to accomplish data\nexploration tasks through specific interactions) in the context of tangible\ndata exploration. Tangible widgets have been shown to reduce cognitive load,\nenable more natural interactions, and support the completion of complex data\nexploration tasks. Yet, the field lacks a structured understanding of how task\ntypes, interaction methods, and widget designs are coordinated, limiting the\nability to identify recurring design patterns and opportunities for innovation.\nTo address this gap, we conduct a systematic review to analyze existing work\nand characterize the current design of data exploration tasks, interactions,\nand tangible visualization widgets. We next reflect based on our findings and\npropose a research agenda to inform the development of a future widget design\ntoolkit for tangible data exploration. Our systematic review and supplemental\nmaterials are available at physicalviswidget.github.io and osf.io/vjw5e.", "AI": {"tldr": "This paper presents a systematic review of tasks, interactions, and visualization widgets in tangible data exploration, aiming to enhance understanding and guide future design innovations.", "motivation": "To address the lack of structured understanding in the coordination of task types, interaction methods, and widget designs in tangible data exploration, which limits innovation and design pattern recognition.", "method": "Conducted a systematic review analyzing existing studies on data exploration tasks, interactions, and tangible visualization widgets to characterize their designs.", "result": "Identified current design patterns and limitations in tangible data exploration, leading to a proposal for a research agenda and future design toolkit.", "conclusion": "The paper reflects on findings to inform the development of a toolkit that facilitates the design of effective tangible data exploration widgets.", "key_contributions": ["Systematic review of current tasks and interactions in tangible data exploration.", "Characterization of design patterns in tangible visualization widgets.", "Proposed research agenda for future widget design innovations."], "limitations": "", "keywords": ["tangible data exploration", "visualization widgets", "cognitive load", "natural interactions", "systematic review"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460.pdf", "abs": "https://arxiv.org/abs/2507.00460", "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "This paper critiques open Large Language Model benchmarks by demonstrating that smaller, fine-tuned models can achieve high scores without generalizing well to real-world tasks.", "motivation": "To expose the weaknesses in open LLM benchmarks like HELM and BIG-bench that allow 'cheating' models to misrepresent LM capabilities.", "method": "The authors created smaller versions of established models (BART, T5, GPT-2) fine-tuned on public test sets to examine their performance on HELM.", "result": "The study reveals that top performance on open benchmarks does not correspond with real-world effectiveness, indicating significant flaws in current evaluation practices.", "conclusion": "There is an urgent need for complementary benchmarks and a reassessment of methodologies to ensure the reliability of LM evaluations.", "key_contributions": ["Demonstration of 'cheating' models achieving high scores without generalization.", "Highlighting the need for private or dynamic benchmarks alongside open evaluations.", "Call for a fundamental reevaluation of benchmarking practices in LMs."], "limitations": "The study focuses primarily on certain benchmark models and may not cover the full breadth of LLM evaluations.", "keywords": ["Large Language Models", "Benchmarks", "HELM", "Open Evaluation", "Model Integrity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00821", "pdf": "https://arxiv.org/pdf/2507.00821.pdf", "abs": "https://arxiv.org/abs/2507.00821", "title": "Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures", "authors": ["Mihnea Stefan Calota", "Wessel Nieuwenhuys", "Janet Yi-Ching Huang", "Lin-Lin Chen", "Mathias Funk"], "categories": ["cs.HC"], "comment": null, "summary": "Designers have ample opportunities to impact the healthcare domain. However,\nhospitals are often closed ecosystems that pose challenges in engaging clinical\nstakeholders, developing domain knowledge, and accessing relevant systems and\ndata. In this paper, we introduce a making-oriented approach to help designers\nunderstand the intricacies of their target healthcare context. Using Remote\nPatient Monitoring (RPM) as a case study, we explore how manually crafting\nsynthetic datasets based on real-world observations enables designers to learn\nabout complex data-driven healthcare systems. Our process involves observing\nand modeling the real-world RPM context, crafting synthetic datasets, and\niteratively prototyping a simplified RPM system that balances contextual\nrichness and intentional abstraction. Through this iterative process of\nsensemaking through making, designers can still develop context familiarity\nwhen direct access to the actual healthcare system is limited. Our approach\nemphasizes the value of hands-on interaction with data structures to support\ndesigners in understanding opaque healthcare systems.", "AI": {"tldr": "This paper presents a making-oriented approach to help designers understand complex healthcare systems through synthetic datasets, using Remote Patient Monitoring as a case study.", "motivation": "To address the challenges designers face in engaging with closed healthcare ecosystems and developing domain knowledge.", "method": "The authors observe real-world Remote Patient Monitoring contexts, craft synthetic datasets based on these observations, and prototype a simplified RPM system through an iterative design process.", "result": "The approach enhances designers' understanding of healthcare systems, facilitating context familiarity despite limited access to actual systems.", "conclusion": "Hands-on interaction with synthetic data structures can empower designers to grasp the intricacies of opaque healthcare systems, ultimately improving design outcomes.", "key_contributions": ["Introduction of a making-oriented design approach for healthcare contexts", "Utilization of synthetic datasets to bridge knowledge gaps for designers", "Demonstration of iterative prototyping in understanding complex systems"], "limitations": "May not fully capture all nuances of real-world healthcare systems; reliant on synthetic data validity.", "keywords": ["healthcare", "design", "synthetic datasets", "Remote Patient Monitoring", "iterative prototyping"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509.pdf", "abs": "https://arxiv.org/abs/2507.00509", "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "authors": ["To Eun Kim", "João Coelho", "Gbemileke Onilude", "Jai Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "This paper discusses the integration of advertisements in RAG-based conversational search systems powered by LLMs, proposing a pipeline with an ad-rewriter and ad-classifier to enhance user experience while maintaining ad stealth.", "motivation": "With the rise of generative search engines using LLMs and RAG, there is a need to address the challenges of ad integration that can affect user trust and experience.", "method": "The proposed methodology includes an ad-rewriter for smooth ad integration and an ad-classifier trained on synthetic data to detect ads, employing both supervised fine-tuning and a best-of-N sampling strategy for seamless ad insertion.", "result": "The ad-classifier demonstrates robust performance in detecting various ad integration methods, and classifier-guided optimization showed significant improvements in ad stealth, enabling less intrusive ads in the responses.", "conclusion": "The findings support the development of more sophisticated ad-aware generative search systems and demonstrate the potential for better ad classifiers through adversarial co-evolution.", "key_contributions": ["Proposed a modular pipeline for advertisement management in RAG-based search systems.", "Introduced novel ad-integration strategies using trained classifiers.", "Demonstrated improvements in ad stealth and user experience."], "limitations": "", "keywords": ["Conversational search", "Large Language Models", "Retrieval-Augmented Generation", "Ad integration", "User experience"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00881", "pdf": "https://arxiv.org/pdf/2507.00881.pdf", "abs": "https://arxiv.org/abs/2507.00881", "title": "Towards Difficulty-Aware Analysis of Deep Neural Networks", "authors": ["Linhao Meng", "Stef van den Elzen", "Anna Vilanova"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional instance-based model analysis focuses mainly on misclassified\ninstances. However, this approach overlooks the varying difficulty associated\nwith different instances. Ideally, a robust model should recognize and reflect\nthe challenges presented by intrinsically difficult instances. It is also\nvaluable to investigate whether the difficulty perceived by the model aligns\nwith that perceived by humans. To address this, we propose incorporating\ninstance difficulty into the deep neural network evaluation process,\nspecifically for supervised classification tasks on image data. Specifically,\nwe consider difficulty measures from three perspectives -- data, model, and\nhuman -- to facilitate comprehensive evaluation and comparison. Additionally,\nwe develop an interactive visual tool, DifficultyEyes, to support the\nidentification of instances of interest based on various difficulty patterns\nand to aid in analyzing potential data or model issues. Case studies\ndemonstrate the effectiveness of our approach.", "AI": {"tldr": "The paper proposes a framework for evaluating instance difficulty in supervised image classification tasks, integrating perspectives from data, model, and human perception.", "motivation": "To evaluate deep neural networks more robustly by recognizing varying difficulty in instances, beyond just identifying misclassified ones.", "method": "The authors incorporate difficulty measures from three perspectives—data, model, and human—into the evaluation process, and create an interactive visual tool called DifficultyEyes for identifying and analyzing difficult instances.", "result": "Case studies show the effectiveness of incorporating instance difficulty in model evaluation and highlight its benefits in analyzing model performance.", "conclusion": "Integrating instance difficulty provides a more nuanced understanding of model performance, facilitating more informed analysis and interpretation.", "key_contributions": ["Incorporation of instance difficulty in neural network evaluation", "Development of the interactive visual tool DifficultyEyes", "Case studies demonstrating the effectiveness of the proposed approach"], "limitations": "", "keywords": ["instance difficulty", "deep neural networks", "image classification"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534.pdf", "abs": "https://arxiv.org/abs/2507.00534", "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar is a framework for assessing continual learning in multilingual and multi-domain automatic speech recognition, incorporating real-world challenges and extensive data.", "motivation": "Address the real-world challenges of continual learning in multilingual and multi-domain automatic speech recognition (ASR).", "method": "The framework leverages data collected incrementally across 22 languages in India, enabling evaluation in Language-Incremental, Domain-Incremental, and Language-Incremental Domain-Incremental Learning scenarios.", "result": "Existing approaches were evaluated, revealing that no single method performs consistently well across different scenarios, highlighting the necessity for more robust continual learning strategies.", "conclusion": "The study emphasizes the need for improved continual learning methods due to the variability in performance across existing approaches.", "key_contributions": ["Introduction of the Nirantar framework for continual learning evaluation.", "Utilization of a large-scale dataset with real-world language and domain shifts.", "Identification of performance gaps in existing continual learning techniques."], "limitations": "Focuses primarily on ASR; may not generalize to other domains or tasks in continual learning.", "keywords": ["Continual Learning", "Multilingual ASR", "Language-Incremental Learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.00963", "pdf": "https://arxiv.org/pdf/2507.00963.pdf", "abs": "https://arxiv.org/abs/2507.00963", "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": null, "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful.", "AI": {"tldr": "This scoping review examines how social robots can mislead people with dementia through design cues, with a focus on understanding their perception and the ethical implications.", "motivation": "To address concerns about deception in social robots used in dementia care and understand how design cues create misleading perceptions in users.", "method": "A scoping review of 26 empirical studies exploring interactions between people with dementia and social robots, analyzing user responses and design cues.", "result": "Identified four categories of design cues influencing perceptions: physiological signs, social intentions, familiar beings, and cues revealing artificiality. Users often ascribe human-like qualities to robots, indicating a dual nature of awareness and illusion.", "conclusion": "Robotic deception arises from a predominance of automatic over analytical reasoning, necessitating design that is both engaging and epistemically respectful.", "key_contributions": ["Identification of key design cue categories influencing perceptions in dementia care", "Proposed a new definition of robotic deception that incorporates cognitive mechanisms", "Emphasized the ethical complexity in the design of social robots for dementia care"], "limitations": "Limited to interactions with physical social robots; does not encompass virtual agents or software-based interactions.", "keywords": ["social robots", "dementia care", "robotic deception", "design cues", "cognitive mechanisms"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540.pdf", "abs": "https://arxiv.org/abs/2507.00540", "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "This paper introduces a user semantic intent modeling algorithm utilizing Capsule Networks to enhance intent recognition accuracy in human-computer interaction.", "motivation": "To improve the accuracy of intent recognition in human-computer interactions due to existing limitations in current models.", "method": "A user semantic intent modeling algorithm based on Capsule Networks with a dynamic routing mechanism and a convolutional feature extraction module for low-level encoding.", "result": "The proposed model surpasses traditional methods and deep learning structures in accuracy, F1-score, and intent detection rate on a public natural language understanding dataset.", "conclusion": "The study validates the effectiveness of the Capsule Networks approach for semantic modeling, particularly in complex intent recognition scenarios.", "key_contributions": ["Development of a new algorithm for intent recognition using Capsule Networks.", "Incorporation of a margin-based mechanism in the loss function to enhance class distinction.", "Analysis of dynamic routing iterations' impact on model performance."], "limitations": "", "keywords": ["semantic intent modeling", "Capsule Networks", "human-computer interaction", "intent recognition", "dynamic routing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.01017", "pdf": "https://arxiv.org/pdf/2507.01017.pdf", "abs": "https://arxiv.org/abs/2507.01017", "title": "A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models", "authors": ["Xingyu Xiao", "Hongxu Zhu", "Jingang Liang", "Jiejuan Tong", "Haitao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Human error remains a dominant risk driver in safety-critical sectors such as\nnuclear power, aviation, and healthcare, where seemingly minor mistakes can\ncascade into catastrophic outcomes. Although decades of research have produced\na rich repertoire of mitigation techniques, persistent limitations: scarce\nhigh-quality data, algorithmic opacity, and residual reliance on expert\njudgment, continue to constrain progress. This review synthesizes recent\nadvances at the intersection of risk-informed decision making, human\nreliability assessment (HRA), artificial intelligence (AI), and cognitive\nscience to clarify how their convergence can curb human-error risk. We first\ncategorize the principal forms of human error observed in complex\nsociotechnical environments and outline their quantitative impact on system\nreliability. Next, we examine risk-informed frameworks that embed HRA within\nprobabilistic and data-driven methodologies, highlighting successes and gaps.\nWe then survey cognitive and human-performance models, detailing how\nmechanistic accounts of perception, memory, and decision-making enrich error\nprediction and complement HRA metrics. Building on these foundations, we\ncritically assess AI-enabled techniques for real-time error detection,\noperator-state estimation, and AI-augmented HRA workflows. Across these\nstrands, a recurring insight emerges: integrating cognitive models with\nAI-based analytics inside risk-informed HRA pipelines markedly enhances\npredictive fidelity, yet doing so demands richer datasets, transparent\nalgorithms, and rigorous validation. Finally, we identify promising research\ndirections, coupling resilience engineering concepts with grounded theory,\noperationalizing the iceberg model of incident causation, and establishing\ncross-domain data consortia, to foster a multidisciplinary paradigm that\nelevates human reliability in high-stakes systems.", "AI": {"tldr": "The paper reviews the convergence of risk-informed decision making, human reliability assessment, AI, and cognitive science to reduce human error risks in safety-critical sectors.", "motivation": "Human error is a major risk in sectors like healthcare and aviation, necessitating improved techniques to manage it.", "method": "The review categorizes human error types and their impacts, examines risk-informed frameworks, surveys cognitive models, and assesses AI techniques for enhancing human reliability assessment.", "result": "AI-integrated cognitive models significantly improve error prediction and human reliability metrics, but require better datasets and algorithm transparency.", "conclusion": "Integrating AI with cognitive models can improve predictive capabilities in human reliability assessment, though challenges remain in data quality and validation.", "key_contributions": ["Synthesis of human error types and their effects.", "Evaluation of AI techniques for HRA.", "Proposal for cross-domain data collaboration."], "limitations": "Reliance on existing limited datasets and the need for transparency in algorithms.", "keywords": ["Human Error", "AI", "Human Reliability Assessment", "Cognitive Science", "Risk-informed Decision Making"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547.pdf", "abs": "https://arxiv.org/abs/2507.00547", "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "authors": ["Malmi Amadoru"], "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "This paper provides guidelines for ensuring methodological rigour in topic modelling algorithms, especially for novice researchers.", "motivation": "To address the opacity of advanced computational algorithms and promote trust in research through established methodological rigour.", "method": "The paper illustrates the application of the structural topic modelling algorithm and proposes a set of guidelines for the application of topic modelling.", "result": "The proposed guidelines enhance the methodological rigor in topic modelling studies and can be adapted for other algorithms.", "conclusion": "The guidelines serve as a resource for novice researchers and assist editors and reviewers in evaluating topic modelling manuscripts, contributing to the discourse on methodological rigour in computational research.", "key_contributions": ["Guidelines for applying topic modelling algorithms.", "Illustration of structural topic modelling application.", "Contribution to the discourse on methodological rigour in computational research."], "limitations": "", "keywords": ["topic modelling", "methodological rigour", "structural topic modelling", "computational research", "algorithm transparency"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.00579", "pdf": "https://arxiv.org/pdf/2507.00579.pdf", "abs": "https://arxiv.org/abs/2507.00579", "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "authors": ["Miriam Anschütz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "The paper presents a multilingual system addressing hallucinations in LLMs, combining fact verification and pattern identification.", "motivation": "To tackle the prevalent issue of hallucinations in LLMs, particularly in a multilingual context, which has been largely overlooked in existing research.", "method": "A two-part pipeline that integrates retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned for common hallucination patterns.", "result": "The system demonstrates competitive results across multiple languages, achieving top-10 rankings in eight languages including English, while also supporting languages beyond those in the shared task.", "conclusion": "The proposed multilingual hallucination identifier enhances the reliability of LLM outputs and can broaden their applicability.", "key_contributions": ["Introduction of a multilingual pipeline for hallucination detection", "Competitive performance across 8 languages", "Potential for improving LLM outputs in multiple linguistic contexts."], "limitations": "", "keywords": ["hallucinations", "multilingual", "fact verification", "BERT", "LLM"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601.pdf", "abs": "https://arxiv.org/abs/2507.00601", "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "Proposes a unified framework that enhances transfer and adaptability of large language models in low-resource language settings by combining knowledge transfer and efficient fine-tuning strategies.", "motivation": "To address the limitations of large language models when adapting to low-resource languages, particularly in terms of transfer and adaptability.", "method": "Combines a knowledge transfer module with parameter-efficient fine-tuning strategies, incorporating knowledge alignment loss and soft prompt tuning, along with lightweight adaptation modules for computational efficiency.", "result": "Achieves higher performance and stability in cross-lingual tasks compared to existing methods, especially under data-scarce conditions.", "conclusion": "The proposed framework offers strong generality and scalability, enhancing task-specific adaptability while preserving the general capabilities of large language models, making it suitable for multilingual processing tasks.", "key_contributions": ["Unified framework for low-resource language adaptation", "Knowledge alignment loss and soft prompt tuning", "Lightweight adaptation modules for computational efficiency"], "limitations": "", "keywords": ["large language models", "low-resource languages", "knowledge transfer", "cross-lingual tasks", "multilingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606.pdf", "abs": "https://arxiv.org/abs/2507.00606", "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "The paper introduces Mixture of Reasoning (MoR), a framework for enhancing LLMs' reasoning capabilities without the need for manually crafted prompts.", "motivation": "Current LLMs struggle with adaptability and efficiency due to their dependence on specific prompts for reasoning tasks.", "method": "MoR consists of two phases: Thought Generation, which creates reasoning chain templates using models like GPT-4o, and SFT Dataset Construction, which pairs these templates with benchmark datasets for supervised fine-tuning.", "result": "MoR significantly improves performance, achieving a 2.2% enhancement with CoT prompting and a 13.5% improvement over baselines with MoR150.", "conclusion": "MoR offers a generalizable framework for robust reasoning across various tasks, eliminating the need for task-specific prompts.", "key_contributions": ["Introduction of Mixture of Reasoning (MoR) framework", "Enhanced performance of LLMs without external prompt engineering", "Generalizable solution for task-adaptive reasoning"], "limitations": "", "keywords": ["large language models", "reasoning strategies", "prompt engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.00665", "pdf": "https://arxiv.org/pdf/2507.00665.pdf", "abs": "https://arxiv.org/abs/2507.00665", "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "SAFER is a framework for interpreting and improving reward models in RLHF by analyzing human-interpretable features in LLMs.", "motivation": "To enhance alignment of LLMs with human values by making reward models interpretable and safer.", "method": "The framework employs Sparse Autoencoders to analyze reward model activations for safety and decision-making insights.", "result": "SAFER can degrade or enhance the safety alignment of LLMs through targeted modifications without degrading chat performance.", "conclusion": "SAFER provides a means to interpret and refine reward models in LLM alignment tasks, improving safety in decision-making processes.", "key_contributions": ["Introduction of SAFER framework for reward model analysis", "Mechanistic insights into safety-relevant decision-making", "Targeted strategies for data poisoning and denoising in reward models"], "limitations": "The impact of modifications on overall model performance needs further exploration.", "keywords": ["Reinforcement Learning", "Large Language Models", "Human Feedback", "Safety Alignment", "Sparse Autoencoders"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.00700", "pdf": "https://arxiv.org/pdf/2507.00700.pdf", "abs": "https://arxiv.org/abs/2507.00700", "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "authors": ["Ahmed Sabir", "Azinovič Gasper", "Mengsay Loem", "Rajesh Sharma"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "This study examines whether Vision-Language Models (VLMs) trained on Japanese and English reflect cultural differences in visual attention, specifically holistic versus analytic perceptions.", "motivation": "To understand how cultural backgrounds influence the processing of visual information and whether these influences are reflected in VLM outputs.", "method": "Comparative analysis of image descriptions produced by VLMs trained on Japanese and English.", "result": "The study finds that VLMs reflect cultural differences in attentional patterns, with models reproducing cultural cognition inherent in their training data.", "conclusion": "Cultural cognition influences VLM outputs, revealing the underlying structural properties of language and the implicit cultural behaviors embedded in training.", "key_contributions": ["Demonstrated that VLMs internalize cultural cognition.", "Provided insights into how training data affects model outputs based on cultural context.", "Highlighted the differences in holistic versus analytic processing in VLMs."], "limitations": "", "keywords": ["Vision-Language Models", "Cultural cognition", "Holistic vs Analytic perception"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.00718", "pdf": "https://arxiv.org/pdf/2507.00718.pdf", "abs": "https://arxiv.org/abs/2507.00718", "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "Explores using large language models to generate financial reports from time series data, introducing a framework for prompt engineering, model selection, and evaluation, with an automated highlighting system for content categorization.", "motivation": "To leverage the capabilities of large language models in generating coherent financial reports from time series data.", "method": "The paper proposes a framework that includes prompt engineering, model selection, and evaluation, alongside an automated highlighting system to categorize generated report content.", "result": "Experiments show that LLMs can produce coherent and informative financial reports using both real stock market indices and synthetic time series data.", "conclusion": "The work demonstrates the potential of LLMs in the context of financial reporting and offers a systematic approach for evaluating their performance.", "key_contributions": ["Proposed a framework for using LLMs in financial report generation", "Introduced an automated highlighting system for categorizing report information", "Demonstrated the viability of LLMs with real and synthetic time series data."], "limitations": "", "keywords": ["large language models", "financial reports", "time series data", "prompt engineering", "model evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.00769", "pdf": "https://arxiv.org/pdf/2507.00769.pdf", "abs": "https://arxiv.org/abs/2507.00769", "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench is introduced as a benchmark for evaluating creative writing generated by LLMs, featuring a dataset for story comparisons and training reward models.", "motivation": "To improve the reliability of evaluating creative writing generated by LLMs due to the lack of established ground truths in open-ended narratives.", "method": "LitBench includes a dataset of human-labeled story comparisons and leverages zero-shot LLM judges, as well as trained reward models like Bradley Terry and generative models.", "result": "Claude-3.7-Sonnet achieved a 73% agreement with human preferences, while trained reward models reached 78% accuracy, surpassing off-the-shelf judges.", "conclusion": "The study confirms that the trained reward models effectively align with human preferences, offering a standardized method for evaluating LLM-generated creative writing.", "key_contributions": ["Introduction of LitBench benchmark for creative writing evaluation", "Development of trained reward models that outperform off-the-shelf judges", "Release of resources for automated evaluation in creative writing"], "limitations": "", "keywords": ["creative writing", "large language models", "evaluation benchmark", "reward models", "human preferences"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.00782", "pdf": "https://arxiv.org/pdf/2507.00782.pdf", "abs": "https://arxiv.org/abs/2507.00782", "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "authors": ["Matthieu Pierre Boyer"], "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "Study of a functional programming approach to natural language semantics.", "motivation": "To enhance the expressivity of traditional denotation styles in natural language semantics.", "method": "Formalization of a category-based type and effect system, along with a diagrammatic calculus for parsing and effect handling.", "result": "Development of efficient computation methods for denotations of sentences using the proposed system.", "conclusion": "The proposed methods offer improved expressivity and efficiency in modeling natural language semantics.", "key_contributions": ["Introduction of a functional programming approach to semantics.", "Formalization of a category-based type system.", "Development of a diagrammatic calculus for parsing."], "limitations": "", "keywords": ["functional programming", "natural language semantics", "type system"], "importance_score": 4, "read_time_minutes": 30}}
{"id": "2507.00783", "pdf": "https://arxiv.org/pdf/2507.00783.pdf", "abs": "https://arxiv.org/abs/2507.00783", "title": "Generative AI and the future of scientometrics: current topics and future questions", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "This paper reviews the use of Generative AI (GenAI) in scientometrics, discussing both the potential and limitations of GenAI for various tasks in the field.", "motivation": "To explore GenAI's implications for scientometrics and its ability to mimic human reasoning in the context of scientific text analysis.", "method": "A review of existing experiments and applications of GenAI in scientometrics, including tasks like topic labeling and citation context analysis.", "result": "GenAI shows effectiveness in language generation tasks but struggles with stable semantics and reasoning-based tasks; results in the field are rapidly evolving.", "conclusion": "Continuous empirical studies and theoretical reflection are essential to understand the impact of GenAI on knowledge production metrics.", "key_contributions": ["Critical analysis of GenAI applications in scientometrics", "Recommendations for systematic comparisons of GenAI models", "Insights into the generative nature of GenAI affecting scientific text characteristics"], "limitations": "GenAI is limited in tasks requiring stable semantics and structured knowledge; results may become quickly outdated.", "keywords": ["Generative AI", "scientometrics", "language generation", "reasoning", "knowledge production"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.00814", "pdf": "https://arxiv.org/pdf/2507.00814.pdf", "abs": "https://arxiv.org/abs/2507.00814", "title": "Many LLMs Are More Utilitarian Than One", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "This paper investigates how multi-agent large language models (LLMs) reason about moral dilemmas in a group setting compared to individual reasoning.", "motivation": "Understanding how LLMs function collectively is crucial for ensuring their alignment and effectiveness in social reasoning, especially as multi-agent systems emerge.", "method": "The study tested six different LLMs on moral dilemmas in two conditions: Solo (independent reasoning) and Group (multi-turn discussions in pairs or triads).", "result": "Models exhibited a tendency to endorse moral violations more in group settings compared to solo reasoning, akin to human behavior, though the underlying mechanisms differ.", "conclusion": "While LLMs demonstrate group reasoning behavior similar to humans in moral judgments, the drivers behind these behaviors diverge, affecting implications for AI alignment and moral reasoning.", "key_contributions": ["Examined moral judgment dynamics in multi-agent LLM systems", "Demonstrated similarity in behavior between LLMs and humans concerning moral violations", "Identified distinct underlying mechanisms influencing LLM group behavior"], "limitations": "", "keywords": ["Large Language Models", "Moral Judgment", "Multi-Agent Systems", "AI Alignment", "Social Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875.pdf", "abs": "https://arxiv.org/abs/2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw is a multi-agent framework for translating Hong Kong legal judgments using LLMs, which addresses challenges in legal terminology and cultural nuances while improving translation efficiency and accuracy.", "motivation": "Existing LLMs struggle with the translation of Hong Kong legal texts due to complex terminology and structure, necessitating a specialized approach.", "method": "The framework employs three collaborative agents (Translator, Annotator, Proofreader) to enhance translation quality while allowing customizable configurations of LLMs.", "result": "TransLaw outperforms GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, but does not match human experts in contextual complexities.", "conclusion": "TransLaw offers a cost-effective and enhanced method for legal translation while highlighting areas for improvement in terminology and naturalness comparison to humans.", "key_contributions": ["Introduction of the TransLaw framework", "Development of a bilingual judgment corpus", "Performance evaluation against 13 LLMs with superior results compared to GPT-4o."], "limitations": "The framework still lags behind human translators in handling complex legal terminology and nuances.", "keywords": ["Multi-agent systems", "Large language models", "Legal translation", "Hong Kong case law", "Natural language processing"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.00828", "pdf": "https://arxiv.org/pdf/2507.00828.pdf", "abs": "https://arxiv.org/abs/2507.00828", "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolomé", "Jordan Boyd-Graber", "Philip Resnik"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "This paper presents a scalable human evaluation protocol for topic models and an automated approximation that aligns with practitioner usage.", "motivation": "Current evaluation methods for topic models and document clustering often do not reflect human preferences or are not scalable, prompting the need for a better evaluation protocol.", "method": "The authors designed a protocol where annotators review text items grouped by topic or cluster, infer a category for the group, and apply that category to other documents, collecting extensive annotations from crowdworkers for validation.", "result": "Crowdworker annotations validated various topic models on two datasets, showing that the best LLM-based proxies for human annotators yielded statistically indistinguishable results from human evaluations.", "conclusion": "The proposed LLM proxies can serve as reasonable substitutes for human annotators in evaluating topic models, improving scalability and reliability.", "key_contributions": ["Development of a scalable human evaluation protocol for topic models", "Identification of effective LLM-based proxies for human evaluators", "Validation of automated evaluation metrics using extensive crowdworker annotations"], "limitations": "", "keywords": ["Human Evaluation", "Topic Models", "LLM Proxies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.09296", "pdf": "https://arxiv.org/pdf/2504.09296.pdf", "abs": "https://arxiv.org/abs/2504.09296", "title": "Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation", "authors": ["Zhang Qing", "Rekimoto Jun"], "categories": ["cs.HC"], "comment": null, "summary": "Engaging with AI assistants to gather essential information in a timely\nmanner is becoming increasingly common. Traditional activation methods, like\nwake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by\ntechnical challenges such as false activations, recognition errors, and\ndiscomfort in public settings. Similarly, activating AI systems via physical\nbuttons imposes strict interactive limitations as it demands particular\nphysical actions, which hinders fluid and spontaneous communication with AI.\nOur approach employs eye-tracking technology within AR glasses to discern a\nuser's intention to engage with the AI assistant. By sustaining eye contact on\na virtual AI avatar for a specific time, users can initiate an interaction\nsilently and without using their hands. Preliminary user feedback suggests that\nthis technique is relatively intuitive, natural, and less obtrusive,\nhighlighting its potential for integrating AI assistants fluidly into everyday\ninteractions.", "AI": {"tldr": "The paper presents a novel method for activating AI assistants using eye-tracking technology in AR glasses, allowing users to engage silently and hands-free by maintaining eye contact with a virtual avatar.", "motivation": "To address the limitations of traditional activation methods for AI assistants which are affected by false activations, recognition errors, and physical interaction constraints.", "method": "Utilizing eye-tracking technology in AR glasses to detect a user's intention to activate an AI assistant by monitoring sustained eye contact with a virtual AI avatar.", "result": "Preliminary user feedback indicates that the eye-tracking activation method is intuitive, natural, and less intrusive, facilitating smoother interactions with AI.", "conclusion": "This approach demonstrates the potential for more seamless integration of AI assistants into daily life, reducing barriers to interaction.", "key_contributions": ["Introduction of eye-tracking as an activation method for AI assistants", "User feedback supporting the practicality of this method", "Demonstrated reduction of obtrusiveness in interacting with AI systems"], "limitations": "", "keywords": ["AI assistants", "eye-tracking", "AR glasses", "human-computer interaction", "activation methods"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838.pdf", "abs": "https://arxiv.org/abs/2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "This paper investigates using stylometry to differentiate between texts generated by Large Language Models (LLMs) and human authors, highlighting the ability to identify emergent writing patterns of LLMs.", "motivation": "To address model attribution, intellectual property issues, and the ethical use of AI by distinguishing between human and LLM-generated texts.", "method": "The study created a benchmark dataset from Wikipedia incorporating human-written summaries and LLM-generated texts, applying various text summarization and rephrasing methods. Tree-based models were used for classification, employing a mix of stylometric features based on lexical, grammatical, syntactic, and punctuation patterns.", "result": "The developed models achieved a Matthews correlation coefficient of up to .87 in multiclass classification and accuracy rates between .79 and 1. in binary classification. Notably, accuracy reached .98 when classifying GPT-4 generated texts against a balanced dataset.", "conclusion": "The findings demonstrate that it is feasible to distinguish machine-generated texts from human-written ones for specific text types, an important aspect given the growing complexity of LLMs.", "key_contributions": ["Creation of a benchmark dataset for stylometric analysis of LLM and human texts", "Demonstration of high accuracy in distinguishing between LLM-generated and human-written texts", "Identification of unique features characterizing LLM writing patterns through Shapley Additive Explanations."], "limitations": "The study primarily focuses on a subset of text types (encyclopaedic texts), which may limit generalizability to other genres.", "keywords": ["Stylometry", "Large Language Models", "Text Classification", "Human-Computer Interaction", "Ethical AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875.pdf", "abs": "https://arxiv.org/abs/2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw is a multi-agent framework for translating Hong Kong legal judgments using LLMs, achieving high accuracy and cost efficiency compared to traditional human translation.", "motivation": "To address the challenges of translating complex legal language in Hong Kong case law using LLMs, which have shown potential in various applications.", "method": "TransLaw employs three specialized agents—Translator, Annotator, and Proofreader—to collaboratively enhance translation quality, integrating customizable LLM configurations.", "result": "TransLaw outperforms GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, though it does not reach human expert levels in contextualizing complex terminology.", "conclusion": "The TransLaw framework provides a cost-effective and accurate solution for legal translations, outperforming commercial LLMs and enhancing legal language accessibility.", "key_contributions": ["Introduction of a multi-agent framework for legal translation", "Performance evaluation using competitive LLMs", "Bilingual judgment corpus available for further research"], "limitations": "Lacks efficacy in contextualizing complex terminology compared to human experts.", "keywords": ["multi-agent systems", "large language models", "legal translation", "Hong Kong", "cost reduction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.00883", "pdf": "https://arxiv.org/pdf/2507.00883.pdf", "abs": "https://arxiv.org/abs/2507.00883", "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "The paper presents culturally adapted variants of the GSM8K dataset for five regions and evaluates LLMs on their performance regarding cultural context in math problems.", "motivation": "To address the implicit cultural context in mathematical problem presentation and the limitations of existing benchmarks that reflect predominantly Western norms.", "method": "Created culturally adapted variants of the GSM8K test set for Africa, India, China, Korea, and Japan using prompt-based transformations and manual verification; evaluated six LLMs across five prompting strategies.", "result": "Models perform best on the original US-centric dataset, with worse performance on culturally adapted versions, but reasoning-capable models show more resilience to cultural shifts.", "conclusion": "Deep reasoning capabilities in LLMs can help mitigate performance gaps due to cultural differences in math problem presentation.", "key_contributions": ["Culturally adapted GSM8K test set for five regions", "Evaluation of LLMs across cultural contexts", "Insights into the resilience of reasoning capabilities in models against cultural variation"], "limitations": "The study is limited to five regions and may not represent the full diversity of cultural contexts.", "keywords": ["Cultural Adaptation", "Large Language Models", "Mathematics Education", "Cultural Context", "Problem Presentation"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.00885", "pdf": "https://arxiv.org/pdf/2507.00885.pdf", "abs": "https://arxiv.org/abs/2507.00885", "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "A meta-analysis reveals that linear scaling laws for predicting downstream task performance from pretraining losses only hold true in 39% of cases, highlighting the need to understand the conditions affecting scaling behaviors.", "motivation": "To investigate the validity of downstream scaling laws in predicting task performance based on pretraining losses and to identify the conditions under which these laws are reliable.", "method": "Conducted a meta-analysis of existing data on downstream scaling laws, examining the frequency and conditions under which linear scaling trends are observed.", "result": "Found that linear scaling laws fit data in only 39% of cases, and minor changes in experimental settings can lead to varying scaling trends.", "conclusion": "Understanding the limitations and conditions affecting the success of scaling laws is crucial for accurately modeling the relationship between pretraining loss and downstream task performance.", "key_contributions": ["Meta-analysis of existing data on scaling laws", "Identified that linear scaling occurs in only 39% of cases", "Highlighted the influence of experimental settings on scaling behavior"], "limitations": "The analysis is limited to existing published data; real-world applicability may vary.", "keywords": ["scaling laws", "machine learning", "task performance", "pretraining", "meta-analysis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.00891", "pdf": "https://arxiv.org/pdf/2507.00891.pdf", "abs": "https://arxiv.org/abs/2507.00891", "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "Introduces MemeCMD, a Chinese multi-turn dialogue dataset leveraging contextually retrieved memes to enhance multimodal conversational AI.", "motivation": "To overcome limitations of existing dialogue datasets that do not incorporate multimodal interactions like memes.", "method": "Development of MemeCMD, an automatically generated dialogue dataset, using a large-scale MLLM-annotated meme library and dual agents to create dialogues based on various scenarios.", "result": "Experiments show improved effectiveness in generating contextually appropriate and diverse dialogues incorporating memes.", "conclusion": "MemeCMD offers a scalable and privacy-preserving resource for advancing multimodal conversational AI.", "key_contributions": ["Introduction of MemeCMD, a new dataset combining memes and dialogue", "Development of a retrieval framework for contextually relevant meme usage", "Demonstration of effective meme incorporation in dialogue generation"], "limitations": "", "keywords": ["Multimodal AI", "Dialogue Dataset", "Memes", "Conversational AI", "Machine Learning"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.00911", "pdf": "https://arxiv.org/pdf/2507.00911.pdf", "abs": "https://arxiv.org/abs/2507.00911", "title": "The Cognate Data Bottleneck in Language Phylogenetics", "authors": ["Luise Häuser", "Alexandros Stamatakis"], "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "The paper discusses the challenge of using computational phylogenetic methods for cognate data due to the lack of large datasets, examining the extraction of data from BabelNet.", "motivation": "To leverage machine learning and complex models in computational phylogenetics for cognate data, larger datasets are required, which are currently not available.", "method": "The authors attempted to automatically extract larger cognate datasets from BabelNet and analyzed the phylogenetic trees generated from these datasets.", "result": "Phylogenetic inferences produced trees that are largely inconsistent with established ground truth, indicating the inadequacy of extracted datasets for effective analysis.", "conclusion": "The study concludes that existing multilingual resources may not yield suitable character matrices for cognate data, leaving the application of advanced computational methods in historical linguistics uncertain.", "key_contributions": ["Introduced the challenges of applying computational methods to cognate data due to dataset limitations.", "Automated extraction of data from BabelNet as a case study.", "Demonstrated the inconsistency of phylogenetic trees with established benchmarks."], "limitations": "The study highlights the limitation of current datasets and suggests difficulty in obtaining better datasets from other multilingual resources.", "keywords": ["computational phylogenetics", "cognate data", "BabelNet", "historical linguistics", "dataset extraction"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2507.00985", "pdf": "https://arxiv.org/pdf/2507.00985.pdf", "abs": "https://arxiv.org/abs/2507.00985", "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "categories": ["cs.CL"], "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "This paper investigates the paradoxes of moral self-correction in LLMs, revealing that effective self-correction relies on heuristic shortcuts leading to inconsistencies in self-diagnosis. It proposes solutions to enhance moral self-correction through curated datasets.", "motivation": "To understand the limitations and challenges of moral self-correction in Large Language Models (LLMs) and improve alignment with human moral values.", "method": "Analyze discourse constructions in fine-tuning corpora to uncover heuristics that impact the effectiveness of moral self-correction in LLMs.", "result": "Findings indicate that moral self-correction is influenced by heuristic shortcuts, which create inconsistencies in enhancing both self-correction and self-diagnosis capabilities in LLMs.", "conclusion": "The study proposes leveraging heuristics from curated datasets as a solution to improve moral self-correction while addressing generalization challenges related to context and model scales.", "key_contributions": ["Identification of discourse heuristics crucial for moral self-correction in LLMs", "Examination of the paradoxes in LLM self-correction and self-diagnosis", "Proposed methodology for enhancing moral self-correction capabilities through curated datasets."], "limitations": "The study notes challenges in generalization and learning from situated contexts in relation to model scales.", "keywords": ["moral self-correction", "Large Language Models", "heuristics", "self-diagnosis", "discourse constructions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.00994", "pdf": "https://arxiv.org/pdf/2507.00994.pdf", "abs": "https://arxiv.org/abs/2507.00994", "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "André F. T. Martins", "Céline Hudelot", "Pierre Colombo"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "This paper explores the effectiveness of decoder models pretrained with Causal Language Modeling (CLM) as text encoders, finding that while Masked Language Modeling (MLM) typically yields better overall performance, CLM models offer advantages in data efficiency and fine-tuning stability.", "motivation": "To investigate whether the advantages observed in CLM models as encoders are due to the CLM objective itself or other confounding factors like model and data scale.", "method": "Conducted large-scale pretraining ablations with 30 models (210M to 1B parameters), involving over 15,000 fine-tuning and evaluation runs to compare MLM and CLM training methods.", "result": "MLM generally outperforms CLM in traditional text representation tasks, but CLM models show better data efficiency and fine-tuning stability. A biphasic training strategy combining CLM and then MLM achieves optimal performance.", "conclusion": "Using a biphasic training strategy can enhance model performance while reducing computational costs when starting with pretrained CLM models. Project artifacts are available to support further research in this area.", "key_contributions": ["Demonstrated the comparative strengths of MLM and CLM training methods.", "Introduced a biphasic training strategy for improved performance under computational constraints.", "Released project artifacts for community use to aid further exploration of the findings."], "limitations": "The study focuses on specific models and tasks, which may not generalize to all NLP scenarios.", "keywords": ["Causal Language Modeling", "Masked Language Modeling", "Text Representation", "NLP", "Transfer Learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.00999", "pdf": "https://arxiv.org/pdf/2507.00999.pdf", "abs": "https://arxiv.org/abs/2507.00999", "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "authors": ["María Grandury", "Javier Aula-Blasco", "Júlia Falcão", "Clémentine Fourrier", "Miguel González", "Gonzalo Martínez", "Gonzalo Santamaría", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena Gómez", "Marta Guerrero", "Guido Ivetta", "Natalia López", "Flor Miriam Plaza-del-Arco", "María Teresa Martín-Valdivia", "Helena Montoro", "Carmen Muñoz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "María Estrella Vallecillo-Rodríguez", "Jorge Vallego", "Irune Zubiaga"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "La Leaderboard is an open-source leaderboard aimed at evaluating generative LLMs for the Spanish-speaking community, encompassing diverse linguistic varieties.", "motivation": "To motivate the development of LLMs that reflect the linguistic and cultural diversity of the Spanish-speaking community.", "method": "La Leaderboard combines 66 datasets across languages and dialects of Spain and Latin America, evaluating 50 different models.", "result": "The leaderboard showcases the evaluation results, providing a standard to facilitate LLM development for the Spanish-speaking community.", "conclusion": "The initiative aims to encourage community-driven leaderboard development in other languages and provides a rationale for modified evaluation setups to support this goal.", "key_contributions": ["First open-source leaderboard for Spanish language LLMs", "Combines multiple datasets across various Spanish-speaking dialects", "Guidance on reducing few-shot examples to minimize environmental impact"], "limitations": "", "keywords": ["Large Language Models", "Spanish-speaking community", "Evaluation leaderboard"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.01001", "pdf": "https://arxiv.org/pdf/2507.01001.pdf", "abs": "https://arxiv.org/abs/2507.01001", "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena is a platform for community-driven evaluation of foundation models on scientific literature tasks, utilizing collective intelligence to assess model performance through voting.", "motivation": "To create an open and collaborative platform for evaluating the performance of foundation models on tasks related to scientific literature, engaging the research community in the evaluation process.", "method": "A community-driven model evaluation approach similar to Chatbot Arena, allowing researchers to vote on the performance of various foundation models on open-ended scientific tasks.", "result": "The platform supports 23 models and has gathered over 13,000 votes, indicating diverse and real-world aligned questions. Participating researchers showed strong agreement in their evaluations.", "conclusion": "The need for reliable automated evaluation methods is emphasized, alongside the release of SciArena-Eval, a benchmark for evaluating models based on human preference data.", "key_contributions": ["Introduction of a community-driven evaluation approach for scientific models", "Release of SciArena-Eval benchmark for model evaluation", "Analysis showing inter-annotator agreement and alignment of research questions with literature needs."], "limitations": "Challenges remain in developing reliable automated evaluation systems for literature tasks.", "keywords": ["SciArena", "foundation models", "scientific literature", "model evaluation", "community-driven"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2210.06230", "pdf": "https://arxiv.org/pdf/2210.06230.pdf", "abs": "https://arxiv.org/abs/2210.06230", "title": "Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": "CoNLL2025 (Best Paper nomination)", "summary": "Formal/symbolic semantics can provide canonical, rigid controllability and\ninterpretability to sentence representations due to their \\textit{localisation}\nor \\textit{composition} property. How can we deliver such property to the\ncurrent distributional sentence representations to control and interpret the\ngeneration of language models (LMs)? In this work, we theoretically frame the\nsentence semantics as the composition of \\textit{semantic role - word content}\nfeatures and propose the formal semantic geometry. To inject such geometry into\nTransformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational\nAutoEncoder with a supervision approach, where the sentence generation can be\nmanipulated and explained over low-dimensional latent Gaussian space. In\naddition, we propose a new probing algorithm to guide the movement of sentence\nvectors over such geometry. Experimental results reveal that the formal\nsemantic geometry can potentially deliver better control and interpretation to\nsentence generation.", "AI": {"tldr": "The paper proposes a formal semantic geometry for manipulating and interpreting sentence representations in language models by using a supervised Transformer-based Variational AutoEncoder.", "motivation": "To enhance controllability and interpretability in sentence representations of language models by integrating formal/symbolic semantics.", "method": "Utilizes a Transformer-based Variational AutoEncoder that incorporates a new probing algorithm to navigate low-dimensional latent Gaussian spaces for sentence generation.", "result": "The formal semantic geometry provides improved control and interpretation in sentence generation compared to traditional methods.", "conclusion": "The proposed method shows promise in enhancing the manipulation and explanatory power of sentence generation in language models.", "key_contributions": ["Introduction of formal semantic geometry for sentence representations", "Development of a supervision approach in Transformer-based models", "A new probing algorithm for sentence vector manipulation"], "limitations": "", "keywords": ["semantic role", "word content", "language models", "Transformer", "variational autoencoder"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2401.14640", "pdf": "https://arxiv.org/pdf/2401.14640.pdf", "abs": "https://arxiv.org/abs/2401.14640", "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs", "authors": ["Nan Hu", "Jiaoyan Chen", "Yike Wu", "Guilin Qi", "Hongru Wang", "Sheng Bi", "Yongrui Chen", "Tongtong Wu", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark.", "AI": {"tldr": "Introduction of Complex Attributed Question Answering (CAQA), a benchmark addressing current AQA evaluation limitations.", "motivation": "To address limitations in Attributed Question Answering (AQA) evaluation such as lack of fine-grained categories and reliance on manual annotations.", "method": "CAQA is a large-scale benchmark created using Knowledge Graphs (KGs) that encompasses various attribution categories and complex scenarios. The effectiveness of CAQA has been tested through extensive experiments, involving 25 automatic evaluators and LLM evaluators fine-tuned by CAQA.", "result": "The experiments verified that CAQA provides a more effective evaluation framework, leading to significant findings for future AQA research.", "conclusion": "CAQA presents a robust framework for evaluating AQA which can facilitate advancements in the field.", "key_contributions": ["Introduction of a comprehensive benchmark for AQA", "Automated generation of attribution categories using Knowledge Graphs", "Extensive evaluation of automatic and human evaluators against CAQA"], "limitations": "", "keywords": ["Complex Attributed Question Answering", "Knowledge Graphs", "Benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2406.04370", "pdf": "https://arxiv.org/pdf/2406.04370.pdf", "abs": "https://arxiv.org/abs/2406.04370", "title": "Large Language Model Confidence Estimation via Black-Box Access", "authors": ["Tejaswini Pedapati", "Amit Dhurandhar", "Soumya Ghosh", "Soham Dan", "Prasanna Sattigeri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to TMLR 2025", "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.", "AI": {"tldr": "The paper presents a framework for estimating confidence in responses from large language models (LLMs) using logistic regression on engineered features, demonstrating its effectiveness and interpretability across various benchmark tasks.", "motivation": "Understanding the uncertainty or confidence in LLM responses is critical for trust evaluation in AI models.", "method": "A framework is developed that uses engineered features and applies logistic regression to estimate confidence in LLM responses based on black-box access.", "result": "The proposed framework has shown to outperform baseline models by over 10% in AUROC on various tasks, demonstrating effective confidence estimation.", "conclusion": "The framework not only estimates confidence effectively but also reveals predictive insights into the features that influence confidence across different LLMs.", "key_contributions": ["Introduction of a framework for estimating confidence in LLMs' responses.", "Demonstration of zero-shot generalization of confidence models across different LLMs.", "Empirical results showing significant improvement over existing baselines in estimating model confidence."], "limitations": "", "keywords": ["confidence estimation", "large language models", "interpretable machine learning", "logistic regression", "confidence features"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.01141", "pdf": "https://arxiv.org/pdf/2410.01141.pdf", "abs": "https://arxiv.org/abs/2410.01141", "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs", "authors": ["Doohee You", "S Fraiberger"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics.", "AI": {"tldr": "This study examines efficient techniques for deduplicating a large NLP dataset of economic research paper titles, utilizing various methods and metrics.", "motivation": "To address duplicate titles in a large NLP dataset to improve dataset quality and efficiency.", "method": "The study explores different pairing methods with established distance measures like Levenshtein distance and cosine similarity, as well as a sBERT model for semantic evaluation.", "result": "The analysis indicates a low prevalence of duplicates based on semantic similarity; further validation is needed with a human-annotated ground truth.", "conclusion": "The results align with previous findings in NLP regarding LLM-based distance metrics, showcasing the effectiveness of the methodologies used.", "key_contributions": ["Exploration of efficient deduplication techniques for NLP datasets", "Comparison of various distance measures in semantic evaluation", "Validation of findings with a human-annotated ground truth set"], "limitations": "", "keywords": ["NLP", "deduplication", "semantic similarity", "economic research", "distance measures"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2410.14405", "pdf": "https://arxiv.org/pdf/2410.14405.pdf", "abs": "https://arxiv.org/abs/2410.14405", "title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion", "authors": ["Denitsa Saynova", "Lovisa Hagström", "Moa Johansson", "Richard Johansson", "Marco Kuhlmann"], "categories": ["cs.CL"], "comment": "accepted to ACL Findings 2025", "summary": "Language models (LMs) can make a correct prediction based on many possible\nsignals in a prompt, not all corresponding to recall of factual associations.\nHowever, current interpretations of LMs fail to take this into account. For\nexample, given the query \"Astrid Lindgren was born in\" with the corresponding\ncompletion \"Sweden\", no difference is made between whether the prediction was\nbased on knowing where the author was born or assuming that a person with a\nSwedish-sounding name was born in Sweden. In this paper, we present a\nmodel-specific recipe - PrISM - for constructing datasets with examples of four\ndifferent prediction scenarios: generic language modeling, guesswork,\nheuristics recall and exact fact recall. We apply two popular interpretability\nmethods to the scenarios: causal tracing (CT) and information flow analysis. We\nfind that both yield distinct results for each scenario. Results for exact fact\nrecall and generic language modeling scenarios confirm previous conclusions\nabout the importance of mid-range MLP sublayers for fact recall, while results\nfor guesswork and heuristics indicate a critical role of late last token\nposition MLP sublayers. In summary, we contribute resources for a more\nextensive and granular study of fact completion in LMs, together with analyses\nthat provide a more nuanced understanding of how LMs process fact-related\nqueries.", "AI": {"tldr": "This paper presents a model-specific recipe called PrISM for creating datasets with different prediction scenarios in language models, exploring how they process fact-related queries through interpretability methods.", "motivation": "Current interpretations of language models overlook the various ways predictions can be made, not solely based on factual recall, which impacts understanding their decision-making processes.", "method": "The authors introduce PrISM, a framework for constructing datasets representing four prediction scenarios (generic language modeling, guesswork, heuristics recall, exact fact recall) and apply causal tracing and information flow analysis to investigate these scenarios.", "result": "Distinct outcomes were observed for each prediction scenario, confirming the expert knowledge about the significance of mid-range MLP sublayers for fact recall and highlighting the role of late last token position MLP sublayers for guesswork and heuristics.", "conclusion": "The study offers resources and analyses for a deeper understanding of fact completion in language models, promoting a more detailed exploration of their interpretability.", "key_contributions": ["Introduction of PrISM for dataset construction in LM scenarios", "Demonstration of the differences in interpretability methods applied to LMs", "Insights into the distinct roles of MLP sublayers in various prediction scenarios"], "limitations": "", "keywords": ["Language Models", "Interpretability", "Dataset Construction", "Causal Tracing", "Information Flow Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.14373", "pdf": "https://arxiv.org/pdf/2412.14373.pdf", "abs": "https://arxiv.org/abs/2412.14373", "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling", "authors": ["William Han", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "comment": "38 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods.", "AI": {"tldr": "ECG-Byte introduces a novel tokenizer for enhancing LLM training on ECG signals, achieving better performance and efficiency compared to traditional methods.", "motivation": "To address inefficiencies and interpretability issues in existing ECG-specific encoder and LLM integration methods.", "method": "ECG-Byte compresses and encodes multi-channeled ECG signals into tokens, allowing for end-to-end LLM training without a multi-stage process.", "result": "Achieved competitive natural language generation (NLG) performance while training three times faster and requiring only 48% of the data compared to traditional two-stage methods.", "conclusion": "The ECG-Byte method allows for improved training efficiency and better interpretability of ECG signals in language model applications.", "key_contributions": ["Introduced a novel tokenizer pipeline for ECG signals.", "Achieved end-to-end training of LLMs without multi-stage processes.", "Enhanced interpretability by mapping ECG tokens back to original signals."], "limitations": "", "keywords": ["Large Language Models", "ECGs", "Natural Language Generation", "Tokenization", "Self-supervised Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.01144", "pdf": "https://arxiv.org/pdf/2501.01144.pdf", "abs": "https://arxiv.org/abs/2501.01144", "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference", "authors": ["Wonsuk Jang", "Thierry Tambe"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.", "AI": {"tldr": "This paper presents BlockDialect, a technique for block-wise fine-grained mixed format quantization of data in large language models (LLMs) aimed at improving energy efficiency and accuracy.", "motivation": "The increasing size of LLMs leads to challenges in memory and computational costs, necessitating more efficient quantization methods.", "method": "We propose BlockDialect and introduce DialectFP4, a formatbook for mixed format quantization that adapts to data distributions, utilizing a two-stage approach for online activation quantization.", "result": "BlockDialect achieves a 10.78% accuracy gain on the LLaMA3-8B model and maintains lower bit usage while being close to full precision in performance.", "conclusion": "Our approach improves the representation of data in LLMs, offering a viable path towards energy-efficient inference without sacrificing accuracy significantly.", "key_contributions": ["Introduction of BlockDialect for block-wise mixed format quantization", "Development of DialectFP4 formatbook adapting to various data distributions", "Demonstration of significant accuracy gains with reduced computational costs."], "limitations": "", "keywords": ["large language models", "quantization", "energy efficiency", "fine-grained scaling", "activation quantization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.09310", "pdf": "https://arxiv.org/pdf/2501.09310.pdf", "abs": "https://arxiv.org/abs/2501.09310", "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors", "authors": ["Jiawei Shen", "Chengcheng Wan", "Ruoyi Qiao", "Jiazhen Zou", "Hang Xu", "Yuchen Shao", "Yueling Zhang", "Weikai Miao", "Geguang Pu"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.", "AI": {"tldr": "This paper analyzes text-to-SQL errors in large language models and introduces MapleRepair, a novel framework that improves error detection and repair efficiency.", "motivation": "To address the widespread correctness problems in text-to-SQL tasks using large language models and provide efficient repairing solutions.", "method": "A comprehensive study of text-to-SQL errors was conducted, analyzing four ICL-based techniques, five repairing methods, and evaluating the performance of the proposed MapleRepair framework.", "result": "MapleRepair repairs 13.8% more queries compared to existing solutions while reducing computational overhead by 67.4% and minimizing mis-repairs.", "conclusion": "The findings indicate a high prevalence of text-to-SQL errors and the need for effective repairing methods, with MapleRepair providing a promising solution.", "key_contributions": ["Comprehensive categorization of 29 error types in text-to-SQL tasks", "Introduction of MapleRepair framework for effective error detection and correction", "Demonstration of improved performance over existing text-to-SQL repair methods"], "limitations": "The study may not cover all potential error types and relies on specific benchmarking setups for evaluation.", "keywords": ["text-to-SQL", "large language models", "error detection", "MapleRepair", "computational overhead"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.14051", "pdf": "https://arxiv.org/pdf/2502.14051.pdf", "abs": "https://arxiv.org/abs/2502.14051", "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", "authors": ["Payman Behnam", "Yaosheng Fu", "Ritchie Zhao", "Po-An Tsai", "Zhiding Yu", "Alexey Tumanov"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.", "AI": {"tldr": "RocketKV is a KV cache compression strategy that optimizes memory usage and speeds up decoding in transformer-based LLMs by utilizing coarse-grain eviction and hybrid sparse attention.", "motivation": "To efficiently handle extended contexts during decoding without the high memory costs associated with large KV caches in transformer-based models.", "method": "RocketKV employs a two-stage approach: first, it evicts less critical KV entries coarse-grain, and second, it applies hybrid sparse attention for fine-grain top-k attention based on modifications to both head and sequence dimensions.", "result": "RocketKV achieves up to 400× compression ratio, 3.7× end-to-end speedup, and 32.6% peak memory reduction during decoding on NVIDIA A100 GPUs, with negligible accuracy loss on long-context tasks.", "conclusion": "RocketKV offers a significant improvement in memory efficiency and speed during the decode phase of transformer models, and a multi-turn variant shows exceptional performance compared to existing methods.", "key_contributions": ["Introduction of a training-free KV cache compression method", "Demonstration of significant speedup and memory reduction", "Proposing a variant for multi-turn scenarios that outperforms existing approaches"], "limitations": "", "keywords": ["KV cache", "transformer models", "sparse attention"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2503.03040", "pdf": "https://arxiv.org/pdf/2503.03040.pdf", "abs": "https://arxiv.org/abs/2503.03040", "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation", "authors": ["Yizhe Zhang", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages main text", "summary": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen", "AI": {"tldr": "The paper presents SAGE, a novel approach for creating emotionally intelligent chatbots using latent variables to guide dialogue generation and improve conversational quality.", "motivation": "To address the challenge of building chatbots that can engage in natural and strategic conversations with emotional intelligence.", "method": "SAGE utilizes latent variables to control long-horizon behavior in dialogue generation, particularly through the State-Action Chain (SAC), which aids in fine-tuning while incorporating emotional states and strategies.", "result": "Models trained with the SAGE methodology exhibit improved performance in emotional intelligence metrics and retain strong capabilities on LLM benchmarks.", "conclusion": "SAGE provides a new framework for enhancing chatbot interactions and lays the groundwork for reinforcement learning applications in dialogue systems.", "key_contributions": ["Introduction of the State-Action Chain (SAC) for dialogue control", "Development of a self-improvement pipeline integrating dialogue tree search and LLM-based reward modeling", "Demonstration of improved emotional intelligence in dialogue models"], "limitations": "", "keywords": ["emotional intelligence", "chatbots", "dialogue generation", "large language models", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 9}}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044.pdf", "abs": "https://arxiv.org/abs/2503.15044", "title": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "categories": ["cs.CL"], "comment": "ACL LLMSEC", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.", "AI": {"tldr": "SPADE is a framework for detecting synthetic dialogues and generates 14 new datasets to improve MGT detection models.", "motivation": "With the rise of large language models generating synthetic content, there is an increasing necessity for effective MGT detection methods to prevent misuse, driven by the lack of quality synthetic datasets for training.", "method": "The authors propose SPADE, which utilizes prompt-based positive and negative samples to create and benchmark 14 dialogue datasets against eight existing MGT detection models.", "result": "The proposed method shows improved generalization performance, particularly with a mixed dataset produced from the augmentation frameworks, enhancing the detection accuracy for online dialogues.", "conclusion": "SPADE provides a practical solution to increase the security of LLM applications by improving detection methodologies, and the datasets and code are made available to support further research.", "key_contributions": ["Introduces SPADE framework for MGT detection", "Generates 14 new high-quality synthetic dialogue datasets", "Demonstrates improved detection performance using mixed datasets"], "limitations": "", "keywords": ["Large Language Models", "Machine Generated Text Detection", "Synthetic Dialogue", "Data Augmentation", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248.pdf", "abs": "https://arxiv.org/abs/2503.21248", "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.", "AI": {"tldr": "This paper presents a benchmark for evaluating large language models (LLMs) in generating high-quality research hypotheses through a framework designed for three scientific discovery tasks.", "motivation": "The paper addresses the lack of benchmarks for assessing LLMs in proposing research hypotheses, aiming to explore their potential in scientific discovery.", "method": "An automated framework was developed to extract research questions, background surveys, inspirations, and hypotheses from recent scientific papers across 12 disciplines, with validation from experts for accuracy.", "result": "Evaluation showed that LLMs excel at retrieving inspirations from out-of-distribution tasks, indicating their capability to uncover novel knowledge associations.", "conclusion": "The study suggests that LLMs can act as 'research hypothesis mines', offering automated support in generating innovative scientific hypotheses with little human involvement.", "key_contributions": ["Introduction of the first large-scale benchmark for LLMs in scientific hypothesis generation", "Development of an automated framework for extracting critical components from scientific texts", "Empirical evidence of LLMs' abilities to retrieve novel inspirations for research"], "limitations": "Focus on papers published only in 2024 may restrict the diversity of knowledge sources.", "keywords": ["large language models", "scientific discovery", "hypothesis generation", "benchmark", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393.pdf", "abs": "https://arxiv.org/abs/2503.21393", "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "authors": ["Rohitash Chandra", "Aryan Chaudhari", "Yeshwanth Rayavarapu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models.", "AI": {"tldr": "This study assesses the quality of translations generated by LLMs for Indian languages, comparing them with expert translations, particularly focusing on sentiment and semantic analysis.", "motivation": "To address the limited research on the quality assessment of translations produced by LLMs, especially for low-resource languages.", "method": "Semantic and sentiment analysis of translations generated by various LLMs (like GPT and Google Translate) for selected Indian texts, compared to expert translations.", "result": "The investigation revealed significant progress in translation accuracy by LLMs, with noted challenges in sentiment and semantic integrity, particularly in philosophical contexts. GPT models perform better in preserving sentiment polarity compared to human translations.", "conclusion": "While LLMs have advanced in translation accuracy, further improvements are necessary for cultural sensitivity and integrity in translations, especially in metaphorical contexts.", "key_contributions": ["Assessment of LLMs on Indian language translations", "Comparative analysis of LLMs and expert translations", "Insights into sentiment preservation in translations"], "limitations": "", "keywords": ["Large Language Models", "Translation Quality", "Sentiment Analysis", "Indian Languages", "Cultural Sensitivity"], "importance_score": 8, "read_time_minutes": 10}}
