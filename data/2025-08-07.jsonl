{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700.pdf", "abs": "https://arxiv.org/abs/2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Tao Gui", "Xuanjing Huang", "Yu-Gang Jiang", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."}
{"id": "2508.03705", "pdf": "https://arxiv.org/pdf/2508.03705.pdf", "abs": "https://arxiv.org/abs/2508.03705", "title": "Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth", "authors": ["Kanan Eldarov"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This study explores how different modes of digital interaction -- namely,\ncomputers versus smartphones -- affect attention, frustration, and creative\nperformance in adolescents. Using a combination of digital task logs,\nwebcam-based gaze estimation, and expert evaluation of task outcomes, we\nanalyzed data from a diverse sample of 824 students aged 11-17. Participants\nwere assigned to device groups in a randomized and stratified design to control\nfor age, gender, and prior experience. Results suggest moderate but\nstatistically significant differences in sustained attention, perceived\nfrustration, and creative output. These findings indicate that the nature of\ndigital interaction -- beyond mere screen time -- may influence cognitive and\nbehavioral outcomes relevant to educational design. Practical implications for\nuser interface development and learning environments are discussed."}
{"id": "2508.03713", "pdf": "https://arxiv.org/pdf/2508.03713.pdf", "abs": "https://arxiv.org/abs/2508.03713", "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Yuanhong Zhou", "Andreas Bulling", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "cs.CV"], "comment": "11 pages, 9 figures, Accepted to 2025 IEEE VIS (Visualization and\n  Visual Analytics)", "summary": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding."}
{"id": "2508.03714", "pdf": "https://arxiv.org/pdf/2508.03714.pdf", "abs": "https://arxiv.org/abs/2508.03714", "title": "\"Think First, Verify Always\": Training Humans to Face AI Risks", "authors": ["Yuksel Aydin"], "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Artificial intelligence enables unprecedented attacks on human cognition, yet\ncybersecurity remains predominantly device-centric. This paper introduces the\n\"Think First, Verify Always\" (TFVA) protocol, which repositions humans as\n'Firewall Zero', the first line of defense against AI-enabled threats. The\nprotocol is grounded in five operational principles: Awareness, Integrity,\nJudgment, Ethical Responsibility, and Transparency (AIJET). A randomized\ncontrolled trial (n=151) demonstrated that a minimal 3-minute intervention\nproduced statistically significant improvements in cognitive security task\nperformance, with participants showing an absolute +7.87% gains compared to\ncontrols. These results suggest that brief, principles-based training can\nrapidly enhance human resilience against AI-driven cognitive manipulation. We\nrecommend that GenAI platforms embed \"Think First, Verify Always\" as a standard\nprompt, replacing passive warnings with actionable protocols to enhance\ntrustworthy and ethical AI use. By bridging the gap between technical\ncybersecurity and human factors, the TFVA protocol establishes human-empowered\nsecurity as a vital component of trustworthy AI systems."}
{"id": "2508.03712", "pdf": "https://arxiv.org/pdf/2508.03712.pdf", "abs": "https://arxiv.org/abs/2508.03712", "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "categories": ["cs.CL"], "comment": "Accepted to AIES 2025", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs"}
{"id": "2508.03717", "pdf": "https://arxiv.org/pdf/2508.03717.pdf", "abs": "https://arxiv.org/abs/2508.03717", "title": "Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery", "authors": ["Muhammad Akmal Bin Mohammed Zaffir", "Daisuke Sakai", "Yuki Sato", "Takahiro Wada"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Studies suggest that involuntary eye movements exhibit greater stability\nduring active motion compared to passive motion, and this effect may also apply\nto the operation of ride-on machinery. Moreover, a study suggested that\nexperimentally manipulating the sense of agency (SoA) by introducing delays may\ninfluence the stability of involuntary eye movements. Although a preliminary\ninvestigation examined involuntary eye movements and perceived maneuverability\nunder two distinct machine dynamics with preserved SoA, it remains unclear how\nsystematic variations in motion dynamics influence these factors. Therefore,\nthe purpose of the present research was to investigate whether systematic\nvariations in the dynamic properties of a ride-on machine, where the perceived\nmaneuverability is modulated, influence the accuracy of involuntary eye\nmovements in human operators. Participants rode a yaw-rotational platform whose\ntime constant from joystick input to motor torque of a rotational machine was\nsystematically manipulated. During the operation, eye movements were recorded\nwhile participants fixated on a visual target. After each condition,\nparticipants provided subjective ratings of maneuverability and cognitive load.\nAs the platform's time constant increased, the perceived maneuverability scores\ndecreased while the cognitive loads increased. Concurrently, involuntary eye\nmovement accuracy decreased. Moderate to weak positive correlations emerged\nbetween the perceived maneuverability scores and the eye movement gain and\naccuracy, while a weak negative correlation was found with cognitive load."}
{"id": "2508.03716", "pdf": "https://arxiv.org/pdf/2508.03716.pdf", "abs": "https://arxiv.org/abs/2508.03716", "title": "FeynTune: Large Language Models for High-Energy Theory", "authors": ["Paul Richmond", "Prarit Agarwal", "Borun Chowdhury", "Vasilis Niarchos", "Constantinos Papageorgakis"], "categories": ["cs.CL", "cs.LG", "hep-th"], "comment": "16 pages", "summary": "We present specialized Large Language Models for theoretical High-Energy\nPhysics, obtained as 20 fine-tuned variants of the 8-billion parameter\nLlama-3.1 model. Each variant was trained on arXiv abstracts (through August\n2024) from different combinations of hep-th, hep-ph and gr-qc. For a\ncomparative study, we also trained models on datasets that contained abstracts\nfrom disparate fields such as the q-bio and cs categories. All models were\nfine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and\nvarying dataset sizes, and outperformed the base model on hep-th abstract\ncompletion tasks. We compare performance against leading commercial LLMs\n(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing\nspecialized language models for High-Energy Theoretical Physics."}
{"id": "2508.03792", "pdf": "https://arxiv.org/pdf/2508.03792.pdf", "abs": "https://arxiv.org/abs/2508.03792", "title": "Recommending With, Not For: Co-Designing Recommender Systems for Social Good", "authors": ["Michael D. Ekstrand", "Afsaneh Razi", "Aleksandra Sarcevic", "Maria Soledad Pera", "Robin Burke", "Katherine Landau Wright"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Accepted to ACM TORS Special Issue on Recommender Systems for Social\n  Good", "summary": "Recommender systems are usually designed by engineers, researchers,\ndesigners, and other members of development teams. These systems are then\nevaluated based on goals set by the aforementioned teams and other business\nunits of the platforms operating the recommender systems. This design approach\nemphasizes the designers' vision for how the system can best serve the\ninterests of users, providers, businesses, and other stakeholders. Although\ndesigners may be well-informed about user needs through user experience and\nmarket research, they are still the arbiters of the system's design and\nevaluation, with other stakeholders' interests less emphasized in user-centered\ndesign and evaluation. When extended to recommender systems for social good,\nthis approach results in systems that reflect the social objectives as\nenvisioned by the designers and evaluated as the designers understand them.\nInstead, social goals and operationalizations should be developed through\nparticipatory and democratic processes that are accountable to their\nstakeholders. We argue that recommender systems aimed at improving social good\nshould be designed *by* and *with*, not just *for*, the people who will\nexperience their benefits and harms. That is, they should be designed in\ncollaboration with their users, creators, and other stakeholders as full\nco-designers, not only as user study participants."}
{"id": "2508.03719", "pdf": "https://arxiv.org/pdf/2508.03719.pdf", "abs": "https://arxiv.org/abs/2508.03719", "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering", "authors": ["Abhay Vijayvargia", "Ajay Nagpal", "Kundeshwar Pundalik", "Atharva Savarkar", "Smita Gautam", "Pankaj Singh", "Rohit Saluja", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Indian farmers often lack timely, accessible, and language-friendly\nagricultural advice, especially in rural areas with low literacy. To address\nthis gap in accessibility, this paper presents a novel AI-powered agricultural\nchatbot, Krishi Sathi, designed to support Indian farmers by providing\npersonalized, easy-to-understand answers to their queries through both text and\nspeech. The system's intelligence stems from an IFT model, subsequently refined\nthrough fine-tuning on Indian agricultural knowledge across three curated\ndatasets. Unlike traditional chatbots that respond to one-off questions, Krishi\nSathi follows a structured, multi-turn conversation flow to gradually collect\nthe necessary details from the farmer, ensuring the query is fully understood\nbefore generating a response. Once the intent and context are extracted, the\nsystem performs Retrieval-Augmented Generation (RAG) by first fetching\ninformation from a curated agricultural database and then generating a tailored\nresponse using the IFT model. The chatbot supports both English and Hindi\nlanguages, with speech input and output features (via ASR and TTS) to make it\naccessible for users with low literacy or limited digital skills. This work\ndemonstrates how combining intent-driven dialogue flows, instruction-tuned\nmodels, and retrieval-based generation can improve the quality and\naccessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query\nresponse accuracy of 97.53%, 91.35% contextual relevance and personalization,\nand a query completion rate of 97.53%. The average response time remained under\n6 seconds, ensuring timely support for users across both English and Hindi\ninteractions."}
{"id": "2508.03852", "pdf": "https://arxiv.org/pdf/2508.03852.pdf", "abs": "https://arxiv.org/abs/2508.03852", "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers", "authors": ["Zhuohao", "Zhang", "Haichang Li", "Chun Meng Yu", "Faraz Faruqi", "Junan Xie", "Gene S-H Kim", "Mingming Fan", "Angus G. Forbes", "Jacob O. Wobbrock", "Anhong Guo", "Liang He"], "categories": ["cs.HC"], "comment": "ASSETS 2025", "summary": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals."}
{"id": "2508.03726", "pdf": "https://arxiv.org/pdf/2508.03726.pdf", "abs": "https://arxiv.org/abs/2508.03726", "title": "Hierarchical Verification of Speculative Beams for Accelerating LLM Inference", "authors": ["Jaydip Sen", "Harshitha Puvvala", "Subhasis Dasgupta"], "categories": ["cs.CL"], "comment": "This paper was accepted for oral presentation and publication in the\n  3rd International Conference on Data Science and Network Engineering (ICDSNE\n  2025), organized at NIT, Agartala, India, from July 25 to 26, 2025. The paper\n  is 12 pages long, and it contains 3 tables and 4 figures. This is NOT the\n  final paper, which will be published in the Springer-published proceedings", "summary": "Large language models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks but face persistent challenges in inference\nefficiency due to their autoregressive nature. While speculative decoding and\nbeam sampling offer notable improvements, traditional methods verify draft\nsequences sequentially without prioritization, leading to unnecessary\ncomputational overhead. This work proposes the Hierarchical Verification Tree\n(HVT), a novel framework that restructures speculative beam decoding by\nprioritizing high-likelihood drafts and enabling early pruning of suboptimal\ncandidates. Theoretical foundations and a formal verification-pruning algorithm\nare developed to ensure correctness and efficiency. Integration with standard\nLLM inference pipelines is achieved without requiring retraining or\narchitecture modification. Experimental evaluations across multiple datasets\nand models demonstrate that HVT consistently outperforms existing speculative\ndecoding schemes, achieving substantial reductions in inference time and energy\nconsumption while maintaining or enhancing output quality. The findings\nhighlight the potential of hierarchical verification strategies as a new\ndirection for accelerating large language model inference."}
{"id": "2508.03876", "pdf": "https://arxiv.org/pdf/2508.03876.pdf", "abs": "https://arxiv.org/abs/2508.03876", "title": "ReVISit 2: A Full Experiment Life Cycle User Study Framework", "authors": ["Zach Cutler", "Jack Wilburn", "Hilson Shrestha", "Yiren Ding", "Brian Bollen", "Khandaker Abrar Nadib", "Tingying He", "Andrew McNutt", "Lane Harrison", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "Online user studies of visualizations, visual encodings, and interaction\ntechniques are ubiquitous in visualization research. Yet, designing,\nconducting, and analyzing studies effectively is still a major burden. Although\nvarious packages support such user studies, most solutions address only facets\nof the experiment life cycle, make reproducibility difficult, or do not cater\nto nuanced study designs or interactions. We introduce reVISit 2, a software\nframework that supports visualization researchers at all stages of designing\nand conducting browser-based user studies. ReVISit supports researchers in the\ndesign, debug & pilot, data collection, analysis, and dissemination experiment\nphases by providing both technical affordances (such as replay of participant\ninteractions) and sociotechnical aids (such as a mindfully maintained community\nof support). It is a proven system that can be (and has been) used in\npublication-quality studies -- which we demonstrate through a series of\nexperimental replications. We reflect on the design of the system via\ninterviews and an analysis of its technical dimensions. Through this work, we\nseek to elevate the ease with which studies are conducted, improve the\nreproducibility of studies within our community, and support the construction\nof advanced interactive studies."}
{"id": "2508.03728", "pdf": "https://arxiv.org/pdf/2508.03728.pdf", "abs": "https://arxiv.org/abs/2508.03728", "title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Wikipedia, a vast and continuously consulted knowledge base, faces\nsignificant challenges in maintaining up-to-date content due to its reliance on\nmanual human editors. Inspired by the vision of continuous knowledge\nacquisition in NELL and fueled by advances in LLM-based agents, this paper\nintroduces WiNELL, an agentic framework for continuously updating Wikipedia\narticles. Our approach employs a multi-agent framework to aggregate online\ninformation, select new and important knowledge for a target entity in\nWikipedia, and then generate precise edit suggestions for human review. Our\nfine-grained editing models, trained on Wikipedia's extensive history of human\nedits, enable incorporating updates in a manner consistent with human editing\nbehavior. Our editor models outperform both open-source instruction-following\nbaselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and\nediting efficiency. End-to-end evaluation on high-activity Wikipedia pages\ndemonstrates WiNELL's ability to identify and suggest timely factual updates.\nThis opens up a promising research direction in LLM agents for automatically\nupdating knowledge bases in a never-ending fashion."}
{"id": "2508.03969", "pdf": "https://arxiv.org/pdf/2508.03969.pdf", "abs": "https://arxiv.org/abs/2508.03969", "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "authors": ["Wei Xu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters."}
{"id": "2508.03737", "pdf": "https://arxiv.org/pdf/2508.03737.pdf", "abs": "https://arxiv.org/abs/2508.03737", "title": "GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models", "authors": ["Ashutosh Bandooni", "Brindha Subburaj"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "6 pages, 3 figures. Accepted, Presented and Published as part of\n  Proceedings of the 6th International Conference on Recent Advantages in\n  Information Technology (RAIT) 2025", "summary": "Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on\nseveral fields and domains are being curated more frequently over the last few\nyears. However these are often monolingual, mostly available in English.\nAdditionally there also is a lack of datasets available in Hindi on tasks apart\nfrom comprehension and translation. We introduce GanitBench, a tough benchmark\nconsisting of 1527 vision-only questions covering several topics in Mathematics\n- available in languages English and Hindi. Collected from two major\nexaminations from India, the JEE Advanced and the CBSE Boards examinations,\nthis benchmark includes questions in the form of images comprising of figures\nessential to a question as well as text. We evaluate two closed source models\nfor the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.\nGPT-4o mini is found to be the more dominant model on the benchmark, with it's\nhighest average accuracy being 38.15%. We also evaluate models through a\n\"Double Lock\" constraint, which brings down the performance of the models by\nconsiderable margins. We observe that two-shot CoT appears to be a more\neffective setting under this environment. Performance of the two VLMs also\ndecreases when answering the same questions in the Hindi language. We hope to\nfacilitate the inclusion of languages like Hindi in research through our work."}
{"id": "2508.03974", "pdf": "https://arxiv.org/pdf/2508.03974.pdf", "abs": "https://arxiv.org/abs/2508.03974", "title": "Managing Data for Scalable and Interactive Event Sequence Visualization", "authors": ["Sayef Azad Sakin", "Katherine E. Isaacs"], "categories": ["cs.HC"], "comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization", "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."}
{"id": "2508.03793", "pdf": "https://arxiv.org/pdf/2508.03793.pdf", "abs": "https://arxiv.org/abs/2508.03793", "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs", "authors": ["Yanting Wang", "Runpeng Geng", "Ying Chen", "Jinyuan Jia"], "categories": ["cs.CL", "cs.CR"], "comment": "The code is available at https://github.com/Wang-Yanting/AttnTrace.\n  The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace", "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace."}
{"id": "2508.03980", "pdf": "https://arxiv.org/pdf/2508.03980.pdf", "abs": "https://arxiv.org/abs/2508.03980", "title": "SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions", "authors": ["Md Sabbir Ahmed", "Arafat Rahman", "Mark Rucker", "Laura E. Barnes"], "categories": ["cs.HC"], "comment": null, "summary": "Social interactions are a fundamental part of daily life and play a critical\nrole in well-being. As emerging technologies offer opportunities to\nunobtrusively monitor behavior, there is growing interest in using them to\nbetter understand social experiences. However, automatically detecting\ninteractions, particularly via wearable devices, remains underexplored.\nExisting systems are often limited to controlled environments, constrained to\nin-person interactions, and rely on rigid assumptions such as the presence of\ntwo speakers within a fixed time window. These limitations reduce their\ngeneralizability to capture diverse real-world interactions. To address these\nchallenges, we developed a real-time, on-watch system capable of detecting both\nin-person and virtual interactions. The system leverages transfer learning to\ndetect foreground speech (FS) and infers interaction boundaries based upon FS\nand conversational cues like whispering. In a real-world evaluation involving\n11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the\nsystem achieved an interaction detection accuracy of 73.18%. Follow-up with six\nparticipants indicated perfect recall for detecting interactions. These\npreliminary findings demonstrate the potential of our system to capture\ninteractions in daily life, providing a foundation for applications such as\npersonalized interventions targeting social anxiety."}
{"id": "2508.03829", "pdf": "https://arxiv.org/pdf/2508.03829.pdf", "abs": "https://arxiv.org/abs/2508.03829", "title": "Majority Bit-Aware Watermarking For Large Language Models", "authors": ["Jiahao Xu", "Rui Hu", "Zikai Zhang"], "categories": ["cs.CL", "cs.CR"], "comment": "Preprint", "summary": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines."}
{"id": "2508.04011", "pdf": "https://arxiv.org/pdf/2508.04011.pdf", "abs": "https://arxiv.org/abs/2508.04011", "title": "StepWrite: Adaptive Planning for Speech-Driven Text Generation", "authors": ["Hamza El Alaoui", "Atieh Taheri", "Yi-Hao Peng", "Jeffrey P. Bigham"], "categories": ["cs.HC", "cs.AI"], "comment": "This paper has been accepted to UIST 2025. For additional materials\n  and project details, please see:\n  https://www.cs.cmu.edu/~helalaou/publications/stepwrite", "summary": "People frequently use speech-to-text systems to compose short texts with\nvoice. However, current voice-based interfaces struggle to support composing\nmore detailed, contextually complex texts, especially in scenarios where users\nare on the move and cannot visually track progress. Longer-form communication,\nsuch as composing structured emails or thoughtful responses, requires\npersistent context tracking, structured guidance, and adaptability to evolving\nuser intentions--capabilities that conventional dictation tools and voice\nassistants do not support. We introduce StepWrite, a large language\nmodel-driven voice-based interaction system that augments human writing ability\nby enabling structured, hands-free and eyes-free composition of longer-form\ntexts while on the move. StepWrite decomposes the writing process into\nmanageable subtasks and sequentially guides users with contextually-aware\nnon-visual audio prompts. StepWrite reduces cognitive load by offloading the\ncontext-tracking and adaptive planning tasks to the models. Unlike baseline\nmethods like standard dictation features (e.g., Microsoft Word) and\nconversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite\ndynamically adapts its prompts based on the evolving context and user intent,\nand provides coherent guidance without compromising user autonomy. An empirical\nevaluation with 25 participants engaging in mobile or stationary hands-occupied\nactivities demonstrated that StepWrite significantly reduces cognitive load,\nimproves usability and user satisfaction compared to baseline methods.\nTechnical evaluations further confirmed StepWrite's capability in dynamic\ncontextual prompt generation, accurate tone alignment, and effective fact\nchecking. This work highlights the potential of structured, context-aware voice\ninteractions in enhancing hands-free and eye-free communication in everyday\nmultitasking scenarios."}
{"id": "2508.03860", "pdf": "https://arxiv.org/pdf/2508.03860.pdf", "abs": "https://arxiv.org/abs/2508.03860", "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence\n  Review for peer review", "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models."}
{"id": "2508.04026", "pdf": "https://arxiv.org/pdf/2508.04026.pdf", "abs": "https://arxiv.org/abs/2508.04026", "title": "VeriGUI: Verifiable Long-Chain GUI Dataset", "authors": ["Shunyu Liu", "Minghao Liu", "Huichi Zhou", "Zhenyu Cui", "Yang Zhou", "Yuhao Zhou", "Wendong Fan", "Ge Zhang", "Jiajun Shi", "Weihao Xuan", "Jiaxing Huang", "Shuang Luo", "Fang Wu", "Heli Qi", "Qingcheng Zeng", "Ziqi Ren", "Jialiang Gao", "Jindi Lv", "Junjie Wang", "Aosong Feng", "Heng Zhou", "Wangchunshu Zhou", "Zhenfei Yin", "Wenlong Zhang", "Guohao Li", "Wenhao Yu", "Irene Li", "Lei Ma", "Lei Bai", "Qunshu Lin", "Mingli Song", "Dacheng Tao"], "categories": ["cs.HC"], "comment": null, "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents."}
{"id": "2508.03865", "pdf": "https://arxiv.org/pdf/2508.03865.pdf", "abs": "https://arxiv.org/abs/2508.03865", "title": "An Entity Linking Agent for Question Answering", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures. Submitted to AAAI 2026 Conference", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent."}
{"id": "2508.04108", "pdf": "https://arxiv.org/pdf/2508.04108.pdf", "abs": "https://arxiv.org/abs/2508.04108", "title": "XARP Tools: An Extended Reality Platform for Humans and AI Agents", "authors": ["Arthur Caetano", "Misha Sra"], "categories": ["cs.HC", "H.5"], "comment": null, "summary": "This technical report presents XARP Tools, an extended reality (XR) framework\ndesigned for human and AI developers alike. XARP comprises a server-side Python\nlibrary and platform-specific XR clients. The library offers high-level APIs\nand communicates with clients via a JSON-based protocol over WebSockets. XR\nclients encapsulate device and runtime specifics, providing responsive,\nlow-latency user interaction. XARP can be utilized in three ways: (i) as a\nlibrary that abstracts XR development for humans; (ii) as a set of callable\ntools that allow AI agents to drive on-the-fly interactions with users; and\n(iii) as a Model Context Protocol server that plugs XR devices into AI\necosystems. XARP code and working examples are released openly at\nhttps://github.com/HAL-UCSB/xarp."}
{"id": "2508.03905", "pdf": "https://arxiv.org/pdf/2508.03905.pdf", "abs": "https://arxiv.org/abs/2508.03905", "title": "Sotopia-RL: Reward Design for Social Intelligence", "authors": ["Haofei Yu", "Zhengyang Qi", "Yining Zhao", "Kolby Nottingham", "Keyang Xuan", "Bodhisattwa Prasad Majumder", "Hao Zhu", "Paul Pu Liang", "Jiaxuan You"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl."}
{"id": "2508.04160", "pdf": "https://arxiv.org/pdf/2508.04160.pdf", "abs": "https://arxiv.org/abs/2508.04160", "title": "DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment", "authors": ["Angela Locoro", "Silvia Golia", "Davide Falessi"], "categories": ["cs.HC", "cs.CV", "K.3; K.3.2"], "comment": null, "summary": "The underspecification of progressive levels of difficulty in measurement\nconstructs design and assessment tests for data visualization literacy may\nhinder the expressivity of measurements in both test design and test reuse. To\nmitigate this problem, this paper proposes DRIVE-T (Discriminating and\nRepresentative Items for Validating Expressive Tests), a methodology designed\nto drive the construction and evaluation of assessment items. Given a data\nvizualization, DRIVE-T supports the identification of task-based items\ndiscriminability and representativeness for measuring levels of data\nvisualization literacy. DRIVE-T consists of three steps: (1) tagging task-based\nitems associated with a set of data vizualizations; (2) rating them by\nindependent raters for their difficulty; (3) analysing raters' raw scores\nthrough a Many-Facet Rasch Measurement model. In this way, we can observe the\nemergence of difficulty levels of the measurement construct, derived from the\ndiscriminability and representativeness of task-based items for each data\nvizualization, ordered into Many-Facets construct levels. In this study, we\nshow and apply each step of the methodology to an item bank, which models the\ndifficulty levels of a measurement construct approximating a latent construct\nfor data visualization literacy. This measurement construct is drawn from\nsemiotics, i.e., based on the syntax, semantics and pragmatics knowledge that\neach data visualization may require to be mastered by people. The DRIVE-T\nmethodology operationalises an inductive approach, observable in a post-design\nphase of the items preparation, for formative-style and practice-based\nmeasurement construct emergence. A pilot study with items selected through the\napplication of DRIVE-T is also presented to test our approach."}
{"id": "2508.03923", "pdf": "https://arxiv.org/pdf/2508.03923.pdf", "abs": "https://arxiv.org/abs/2508.03923", "title": "CoAct-1: Computer-using Agents with Coding as Actions", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation."}
{"id": "2508.04202", "pdf": "https://arxiv.org/pdf/2508.04202.pdf", "abs": "https://arxiv.org/abs/2508.04202", "title": "Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes", "authors": ["Abdulrhman Alorini", "Yufeng Wu", "Abdullah Bin Sawad", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC"], "comment": null, "summary": "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments."}
{"id": "2508.03935", "pdf": "https://arxiv.org/pdf/2508.03935.pdf", "abs": "https://arxiv.org/abs/2508.03935", "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation", "authors": ["Raymond Wilson", "Cole Graham", "Chase Carter", "Zefeng Yang", "Ruiqi Gu"], "categories": ["cs.CL"], "comment": null, "summary": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation."}
{"id": "2508.04357", "pdf": "https://arxiv.org/pdf/2508.04357.pdf", "abs": "https://arxiv.org/abs/2508.04357", "title": "Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows", "authors": ["Gloria Fernndez-Nieto", "Vanessa Echeverria", "Yuheng Li", "Yi-Shan Tsai", "Lele Sha", "Guanliang Chen", "Dragan Gasevic", "Zachari Swiecki"], "categories": ["cs.HC", "H.5.2"], "comment": "29 pages, 16 figures, 7 tables, submitted to Behaviours & Information\n  Technology", "summary": "Knowledge Management is crucial for capturing and transferring expertise\nwithin universities, especially in high staff turnover contexts where expertise\nloss disrupts teaching. Documenting teachers' workflows is time-intensive and\ndiverts experts from core responsibilities. Sequential Pattern Mining (SPM)\nleverages log data to identify expert workflows, offering an automated\nalternative to represent workflows but requiring transformation into intuitive\nformats for novice educators. This paper introduces Visual Process\nRepresentations (VPR), a design approach combining SPM, Knowledge Management\nprocesses, and storytelling techniques to convert expert log data into clear\nvisualisations. We detail the design phases and report a study evaluating\nvisual affordances (text lists vs. pictorial-style) and teachers' perceptions\nof four versions of the VPR with 160 higher teachers on Prolific. Results\nindicate improved task performance, usability, and engagement, particularly\nwith enriched visuals, though process memorability and task time improvements\nwere limited. The findings highlight VPR's potential to visualise workflows and\nsupport novice educators."}
{"id": "2508.03970", "pdf": "https://arxiv.org/pdf/2508.03970.pdf", "abs": "https://arxiv.org/abs/2508.03970", "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "comment": "Published in MIT Science Policy Review 6, 139-146 (2025)", "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications."}
{"id": "2508.04377", "pdf": "https://arxiv.org/pdf/2508.04377.pdf", "abs": "https://arxiv.org/abs/2508.04377", "title": "GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design", "authors": ["Gloria Fernndez-Nieto", "Lele Sha", "Yuheng Li", "Yi-Shan Tsai", "Guanliang Chen", "Yinwei Wei", "Weiqing Wang", "Jinchun Wen", "Shaveen Singh", "Ivan Silva", "Yuanfang Li", "Dragan Gasvi", "Zachari Swiecki"], "categories": ["cs.HC"], "comment": "38 pages, 10 tables, 7 figures, submitted to TOCHI", "summary": "Designing Knowledge Management Systems (KMSs) for higher education requires\naddressing complex human-technology interactions, especially where staff\nturnover and changing roles create ongoing challenges for reusing knowledge.\nWhile advances in process mining and Generative AI enable new ways of designing\nfeatures to support knowledge management, existing KMSs often overlook the\nrealities of educators' workflows, leading to low adoption and limited impact.\nThis paper presents findings from a two-year human-centred design study with\n108 higher education teachers, focused on the iterative co-design and\nevaluation of GoldMind, a KMS supporting in-the-flow knowledge management\nduring digital teaching tasks. Through three design-evaluation cycles, we\nexamined how teachers interacted with the system and how their feedback\ninformed successive refinements. Insights are synthesised across three themes:\n(1) Technology Lessons from user interaction data, (2) Design Considerations\nshaped by co-design and usability testing, and (3) Human Factors, including\ncognitive load and knowledge behaviours, analysed using Epistemic Network\nAnalysis."}
{"id": "2508.03979", "pdf": "https://arxiv.org/pdf/2508.03979.pdf", "abs": "https://arxiv.org/abs/2508.03979", "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency", "authors": ["Md Arafat Sultan", "Ramn Fernandez Astudillo"], "categories": ["cs.CL"], "comment": null, "summary": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases."}
{"id": "2508.04391", "pdf": "https://arxiv.org/pdf/2508.04391.pdf", "abs": "https://arxiv.org/abs/2508.04391", "title": "Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis", "authors": ["Ze Gao", "Mengyao Guo", "Zheng Wang", "Xiaolin Zhang", "Sihuang Man"], "categories": ["cs.HC"], "comment": null, "summary": "Digital ecological art represents an emergent frontier where biological media\nconverge with virtual environments. This study examines the paradigm shift from\nanthropocentric to plant-centered artistic narratives within the metaverse,\ncontextualizing how digital platforms transform ecological expression. However,\ncurrent frameworks fail to systematically guide artists in leveraging plant\nagency for digital symbiosis that transcends human-centered creation. We\npropose the Biocentric-Creation Transformation Ideology (BCTI) framework and\nvalidate it through multimodal case studies spanning bio-art, NFTs, and VR\necosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable\nunprecedented plant-algorithm co-creation, with biological artworks increasing\nby 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests\nthrough blockchain DAOs where plants govern human-plant collaborations; (3)\nAlgorithmic photosynthesis in VR environments reshapes ecological aesthetics\nthrough real-time biodata translation. The BCTI framework advances ecological\nart theory by systematizing the transition from representation to\nplant-centered agency, offering artists a blueprint for post-anthropocene\ncreation. This redefines environmental consciousness in virtual realms while\nestablishing new protocols for cross-species digital collaboration."}
{"id": "2508.03990", "pdf": "https://arxiv.org/pdf/2508.03990.pdf", "abs": "https://arxiv.org/abs/2508.03990", "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks."}
{"id": "2508.04541", "pdf": "https://arxiv.org/pdf/2508.04541.pdf", "abs": "https://arxiv.org/abs/2508.04541", "title": "Measuring Information Richness in Product Images: Implications for Online Sales", "authors": ["Zhu Yuting", "Cao Xinyu", "Su Yuzhuo", "Ma Yongbin"], "categories": ["cs.HC"], "comment": null, "summary": "A common challenge for e-commerce sellers is to decide what product images to\ndisplay on online shopping sites. In this paper, we propose and validate a\nnovel metric, k-value, to quantify the information richness of an image set,\nand we further investigate its effect on consumers' purchase decisions. We\nleverage patch-level embeddings from Vision Transformers (ViT) and apply\nk-means clustering to identify distinct visual features, defining k-value as\nthe number of clusters. An online experiment demonstrates that k-value aligns\nwith human-perceived information richness, validating the metric. A simulated\nonline shopping experiment further reveals a significant yet counterintuitive\nresult: while an image set with a higher k-value (richer information) shortens\ndecision time, it paradoxically reduces purchase propensity. Our findings\nilluminate the complex relationship between visual information richness and\nconsumer behavior, providing sellers a quantifiable tool for image selection."}
{"id": "2508.03998", "pdf": "https://arxiv.org/pdf/2508.03998.pdf", "abs": "https://arxiv.org/abs/2508.03998", "title": "Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models", "authors": ["Xinyu Zhao", "Zhen Tan", "Maya Enisman", "Minjae Seo", "Marta R. Durantini", "Dolores Albarracin", "Tianlong Chen"], "categories": ["cs.CL"], "comment": "27 pages, 7 figures", "summary": "Successful group meetings, such as those implemented in group\nbehavioral-change programs, work meetings, and other social contexts, must\npromote individual goal setting and execution while strengthening the social\nrelationships within the group. Consequently, an ideal facilitator must be\nsensitive to the subtle dynamics of disengagement, difficulties with individual\ngoal setting and execution, and interpersonal difficulties that signal a need\nfor intervention. The challenges and cognitive load experienced by facilitators\ncreate a critical gap for an embodied technology that can interpret social\nexchanges while remaining aware of the needs of the individuals in the group\nand providing transparent recommendations that go beyond powerful but \"black\nbox\" foundation models (FMs) that identify social cues. We address this\nimportant demand with a social robot co-facilitator that analyzes multimodal\nmeeting data and provides discreet cues to the facilitator. The robot's\nreasoning is powered by an agentic concept bottleneck model (CBM), which makes\ndecisions based on human-interpretable concepts like participant engagement and\nsentiments, ensuring transparency and trustworthiness. Our core contribution is\na transfer learning framework that distills the broad social understanding of\nan FM into our specialized and transparent CBM. This concept-driven system\nsignificantly outperforms direct zero-shot FMs in predicting the need for\nintervention and enables real-time human correction of its reasoning.\nCritically, we demonstrate robust knowledge transfer: the model generalizes\nacross different groups and successfully transfers the expertise of senior\nhuman facilitators to improve the performance of novices. By transferring an\nexpert's cognitive model into an interpretable robotic partner, our work\nprovides a powerful blueprint for augmenting human capabilities in complex\nsocial domains."}
{"id": "2508.04634", "pdf": "https://arxiv.org/pdf/2508.04634.pdf", "abs": "https://arxiv.org/abs/2508.04634", "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations", "authors": ["Mohammed Almutairi", "Charles Chiang", "Haoze Guo", "Matthew Belcher", "Nandini Banerjee", "Maria Milkowski", "Svitlana Volkova", "Daniel Nguyen", "Tim Weninger", "Michael Yankoski", "Trenton W. Ford", "Diego Gomez-Zara"], "categories": ["cs.HC"], "comment": "5 pages, 2 figures, UIST 2025", "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios."}
{"id": "2508.04010", "pdf": "https://arxiv.org/pdf/2508.04010.pdf", "abs": "https://arxiv.org/abs/2508.04010", "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Keting Yin", "Juncheng Li", "Zhuosheng Zhang", "Shengyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard."}
{"id": "2508.04667", "pdf": "https://arxiv.org/pdf/2508.04667.pdf", "abs": "https://arxiv.org/abs/2508.04667", "title": "How are CS students using resources and AI tools for coding tasks?", "authors": ["Natalia Echeverry", "Arun Lekshmi Narayanan"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors."}
{"id": "2508.04012", "pdf": "https://arxiv.org/pdf/2508.04012.pdf", "abs": "https://arxiv.org/abs/2508.04012", "title": "Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing", "authors": ["Xiaopeng Li", "Shasha Li", "Xi Wang", "Shezheng Song", "Bin Ji", "Shangwen Wang", "Jun Ma", "Xiaodong Liu", "Mina Liu", "Jie Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) underpin many AI applications, but their static\nnature makes updating knowledge costly. Model editing offers an efficient\nalternative by injecting new information through targeted parameter\nmodifications. In particular, meta-learning-based model editing (MLBME) methods\nhave demonstrated notable advantages in both editing effectiveness and\nefficiency. Despite this, we find that MLBME exhibits suboptimal performance in\nlow-data scenarios, and its training efficiency is bottlenecked by the\ncomputation of KL divergence. To address these, we propose $\\textbf{S}$tep\n$\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that\nadopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation\n$\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited\nsupervision and a norm regularization on weight updates to improve training\nefficiency. Experimental results on two datasets and two LLMs demonstrate that\nSMEdit outperforms prior MLBME baselines and the MBPS strategy can be\nseamlessly integrated into existing methods to further boost their performance.\nOur code will be released soon."}
{"id": "2508.04679", "pdf": "https://arxiv.org/pdf/2508.04679.pdf", "abs": "https://arxiv.org/abs/2508.04679", "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models", "authors": ["Amit Kumar Das", "Klaus Mueller"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication."}
{"id": "2508.04038", "pdf": "https://arxiv.org/pdf/2508.04038.pdf", "abs": "https://arxiv.org/abs/2508.04038", "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents", "authors": ["Zechen Li", "Baiyu Chen", "Hao Xue", "Flora D. Salim"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA."}
{"id": "2508.03698", "pdf": "https://arxiv.org/pdf/2508.03698.pdf", "abs": "https://arxiv.org/abs/2508.03698", "title": "Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024", "authors": ["Se Won Oh", "Hyuntae Jeong", "Seungeun Chung", "Jeong Mook Lim", "Kyoung Ju Noh", "Sunkyung Lee", "Gyuwon Jung"], "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": "This work is intended for submission to an IEEE conference. The\n  content is also relevant to the cs.HC category", "summary": "Improving human health and well-being requires an accurate and effective\nunderstanding of an individual's physical and mental state throughout daily\nlife. To support this goal, we utilized smartphones, smartwatches, and sleep\nsensors to collect data passively and continuously for 24 hours a day, with\nminimal interference to participants' usual behavior, enabling us to gather\nquantitative data on daily behaviors and sleep activities across multiple days.\nAdditionally, we gathered subjective self-reports of participants' fatigue,\nstress, and sleep quality through surveys conducted immediately before and\nafter sleep. This comprehensive lifelog dataset is expected to provide a\nfoundational resource for exploring meaningful insights into human daily life\nand lifestyle patterns, and a portion of the data has been anonymized and made\npublicly available for further research. In this paper, we introduce the ETRI\nLifelog Dataset 2024, detailing its structure and presenting potential\napplications, such as using machine learning models to predict sleep quality\nand stress."}
{"id": "2508.04039", "pdf": "https://arxiv.org/pdf/2508.04039.pdf", "abs": "https://arxiv.org/abs/2508.04039", "title": "Large Reasoning Models Are Autonomous Jailbreak Agents", "authors": ["Thilo Hagendorff", "Erik Derner", "Nuria Oliver"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has\ntraditionally required complex technical procedures or specialized human\nexpertise. In this study, we show that the persuasive capabilities of large\nreasoning models (LRMs) simplify and scale jailbreaking, converting it into an\ninexpensive activity accessible to non-experts. We evaluated the capabilities\nof four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as\nautonomous adversaries conducting multi-turn conversations with nine widely\nused target models. LRMs received instructions via a system prompt, before\nproceeding to planning and executing jailbreaks with no further supervision. We\nperformed extensive experiments with a benchmark of harmful prompts composed of\n70 items covering seven sensitive domains. This setup yielded an overall attack\nsuccess rate across all model combinations of 97.14%. Our study reveals an\nalignment regression, in which LRMs can systematically erode the safety\nguardrails of other models, highlighting the urgent need to further align\nfrontier models not only to resist jailbreak attempts, but also to prevent them\nfrom being co-opted into acting as jailbreak agents."}
{"id": "2508.03699", "pdf": "https://arxiv.org/pdf/2508.03699.pdf", "abs": "https://arxiv.org/abs/2508.03699", "title": "Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task", "authors": ["Subin Raj Peter"], "categories": ["cs.CV", "cs.HC", "cs.MM"], "comment": "7 pages, 7 figures, conference", "summary": "Virtual Reality (VR) has emerged as a powerful tool for workforce training,\noffering immersive, interactive, and risk-free environments that enhance skill\nacquisition, decision-making, and confidence. Despite its advantages,\ndeveloping VR applications for training remains a significant challenge due to\nthe time, expertise, and resources required to create accurate and engaging\ninstructional content. To address these limitations, this paper proposes a\nnovel approach that leverages Large Language Models (LLMs) to automate the\ngeneration of virtual instructions from textual input. The system comprises two\ncore components: an LLM module that extracts task-relevant information from the\ntext, and an intelligent module that transforms this information into animated\ndemonstrations and visual cues within a VR environment. The intelligent module\nreceives input from the LLM module and interprets the extracted information.\nBased on this, an instruction generator creates training content using relevant\ndata from a database. The instruction generator generates the instruction by\nchanging the color of virtual objects and creating animations to illustrate\ntasks. This approach enhances training effectiveness and reduces development\noverhead, making VR-based training more scalable and adaptable to evolving\nindustrial needs."}
{"id": "2508.04047", "pdf": "https://arxiv.org/pdf/2508.04047.pdf", "abs": "https://arxiv.org/abs/2508.04047", "title": "DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation", "authors": ["Jiabing Yang", "Yixiang Chen", "Zichen Wen", "Chenhang Cui", "Peiyan Li", "Yuan Xu", "Bowen Fang", "Yan Huang", "Liang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Controllable Text Generation (CTG) is a vital subfield in Natural Language\nProcessing (NLP), aiming to generate text that aligns with desired attributes.\nHowever, previous studies commonly focus on the quality of controllable text\ngeneration for short sequences, while the generation of long-form text remains\nlargely underexplored. In this paper, we observe that the controllability of\ntexts generated by the powerful prefix-based method Air-Decoding tends to\ndecline with increasing sequence length, which we hypothesize primarily arises\nfrom the observed decay in attention to the prefixes. Meanwhile, different\ntypes of prefixes including soft and hard prefixes are also key factors\ninfluencing performance. Building on these insights, we propose a lightweight\nand effective framework called Dynamic Token-level Prefix Augmentation (DTPA)\nbased on Air-Decoding for controllable text generation. Specifically, it first\nselects the optimal prefix type for a given task. Then we dynamically amplify\nthe attention to the prefix for the attribute distribution to enhance\ncontrollability, with a scaling factor growing exponentially as the sequence\nlength increases. Moreover, based on the task, we optionally apply a similar\naugmentation to the original prompt for the raw distribution to balance text\nquality. After attribute distribution reconstruction, the generated text\nsatisfies the attribute constraints well. Experiments on multiple CTG tasks\ndemonstrate that DTPA generally outperforms other methods in attribute control\nwhile maintaining competitive fluency, diversity, and topic relevance. Further\nanalysis highlights DTPA's superior effectiveness in long text generation."}
{"id": "2508.03715", "pdf": "https://arxiv.org/pdf/2508.03715.pdf", "abs": "https://arxiv.org/abs/2508.03715", "title": "Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors", "authors": ["Bertram Fuchs", "Mehdi Ejtehadi", "Ana Cisnal", "Jrgen Pannek", "Anke Scheel-Sailer", "Robert Riener", "Inge Eriks-Hoogland", "Diego Paez-Granados"], "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Autonomic Dysreflexia (AD) is a potentially life-threatening condition\ncharacterized by sudden, severe blood pressure (BP) spikes in individuals with\nspinal cord injury (SCI). Early, accurate detection is essential to prevent\ncardiovascular complications, yet current monitoring methods are either\ninvasive or rely on subjective symptom reporting, limiting applicability in\ndaily file. This study presents a non-invasive, explainable machine learning\nframework for detecting AD using multimodal wearable sensors. Data were\ncollected from 27 individuals with chronic SCI during urodynamic studies,\nincluding electrocardiography (ECG), photoplethysmography (PPG), bioimpedance\n(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three\ncommercial devices. Objective AD labels were derived from synchronized\ncuff-based BP measurements. Following signal preprocessing and feature\nextraction, BorutaSHAP was used for robust feature selection, and SHAP values\nfor explainability. We trained modality- and device-specific weak learners and\naggregated them using a stacked ensemble meta-model. Cross-validation was\nstratified by participants to ensure generalizability. HR- and ECG-derived\nfeatures were identified as the most informative, particularly those capturing\nrhythm morphology and variability. The Nearest Centroid ensemble yielded the\nhighest performance (Macro F1 = 0.77+/-0.03), significantly outperforming\nbaseline models. Among modalities, HR achieved the highest area under the curve\n(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature\nfeatures contributed less to overall accuracy, consistent with missing data and\nlow specificity. The model proved robust to sensor dropout and aligned well\nwith clinical AD events. These results represent an important step toward\npersonalized, real-time monitoring for individuals with SCI."}
{"id": "2508.04057", "pdf": "https://arxiv.org/pdf/2508.04057.pdf", "abs": "https://arxiv.org/abs/2508.04057", "title": "PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for\nenhancing large language models (LLMs) with external knowledge. However,\ncurrent RAG systems face two critical limitations: (1) they inefficiently\nretrieve information for every query, including simple questions that could be\nresolved using the LLM's parametric knowledge alone, and (2) they risk\nretrieving irrelevant documents when queries contain sparse information\nsignals. To address these gaps, we introduce Parametric-verified Adaptive\nInformation Retrieval and Selection (PAIRS), a training-free framework that\nintegrates parametric and retrieved knowledge to adaptively determine whether\nto retrieve and how to select external information. Specifically, PAIRS employs\na dual-path generation mechanism: First, the LLM produces both a direct answer\nand a context-augmented answer using self-generated pseudo-context. When these\noutputs converge, PAIRS bypasses external retrieval entirely, dramatically\nimproving the RAG system's efficiency. For divergent cases, PAIRS activates a\ndual-path retrieval (DPR) process guided by both the original query and\nself-generated contextual signals, followed by an Adaptive Information\nSelection (AIS) module that filters documents through weighted similarity to\nboth sources. This simple yet effective approach can not only enhance\nefficiency by eliminating unnecessary retrievals but also improve accuracy\nthrough contextually guided retrieval and adaptive information selection.\nExperimental results on six question-answering (QA) benchmarks show that PAIRS\nreduces retrieval costs by around 25% (triggering for only 75% of queries)\nwhile still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior\nbaselines on average."}
{"id": "2508.03729", "pdf": "https://arxiv.org/pdf/2508.03729.pdf", "abs": "https://arxiv.org/abs/2508.03729", "title": "Privileged Contrastive Pretraining for Multimodal Affect Modelling", "authors": ["Kosmas Pinitas", "Konstantinos Makantasis", "Georgios N. Yannakakis"], "categories": ["cs.LG", "cs.HC", "cs.MM"], "comment": null, "summary": "Affective Computing (AC) has made significant progress with the advent of\ndeep learning, yet a persistent challenge remains: the reliable transfer of\naffective models from controlled laboratory settings (in-vitro) to uncontrolled\nreal-world environments (in-vivo). To address this challenge we introduce the\nPrivileged Contrastive Pretraining (PriCon) framework according to which models\nare first pretrained via supervised contrastive learning (SCL) and then act as\nteacher models within a Learning Using Privileged Information (LUPI) framework.\nPriCon both leverages privileged information during training and enhances the\nrobustness of derived affect models via SCL. Experiments conducted on two\nbenchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained\nusing PriCon consistently outperform LUPI and end to end models. Remarkably, in\nmany cases, PriCon models achieve performance comparable to models trained with\naccess to all modalities during both training and testing. The findings\nunderscore the potential of PriCon as a paradigm towards further bridging the\ngap between in-vitro and in-vivo affective modelling, offering a scalable and\npractical solution for real-world applications."}
{"id": "2508.04073", "pdf": "https://arxiv.org/pdf/2508.04073.pdf", "abs": "https://arxiv.org/abs/2508.04073", "title": "Efficient Strategy for Improving Large Language Model (LLM) Capabilities", "authors": ["Julin Camilo Velandia Gutirrez"], "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; I.5.1"], "comment": "Based on master's thesis in Systems and Computer Engineering,\n  Universidad Nacional de Colombia (2025)", "summary": "Large Language Models (LLMs) have become a milestone in the field of\nartificial intelligence and natural language processing. However, their\nlarge-scale deployment remains constrained by the need for significant\ncomputational resources. This work proposes starting from a base model to\nexplore and combine data processing and careful data selection techniques,\ntraining strategies, and architectural adjustments to improve the efficiency of\nLLMs in resource-constrained environments and within a delimited knowledge\nbase. The methodological approach included defining criteria for building\nreliable datasets, conducting controlled experiments with different\nconfigurations, and systematically evaluating the resulting variants in terms\nof capability, versatility, response time, and safety. Finally, comparative\ntests were conducted to measure the performance of the developed variants and\nto validate the effectiveness of the proposed strategies. This work is based on\nthe master's thesis in Systems and Computer Engineering titled \"Efficient\nStrategy for Improving the Capabilities of Large Language Models (LLMs)\"."}
{"id": "2508.03922", "pdf": "https://arxiv.org/pdf/2508.03922.pdf", "abs": "https://arxiv.org/abs/2508.03922", "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "authors": ["Soroush Heydari"], "categories": ["cs.SE", "cs.HC", "D.2.1"], "comment": "8 pages", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming."}
{"id": "2508.04086", "pdf": "https://arxiv.org/pdf/2508.04086.pdf", "abs": "https://arxiv.org/abs/2508.04086", "title": "ToolGrad: Efficient Tool-use Dataset Generation with Textual \"Gradients\"", "authors": ["Zhongyi Zhou", "Kohei Uehara", "Haoyu Zhang", "Jingtao Zhou", "Lin Gu", "Ruofei Du", "Zheng Xu", "Tatsuya Harada"], "categories": ["cs.CL"], "comment": null, "summary": "Prior work synthesizes tool-use LLM datasets by first generating a user\nquery, followed by complex tool-use annotations like DFS. This leads to\ninevitable annotation failures and low efficiency in data generation. We\nintroduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad\nfirst constructs valid tool-use chains through an iterative process guided by\ntextual \"gradients\", and then synthesizes corresponding user queries. This\n\"answer-first\" approach led to ToolGrad-5k, a dataset generated with more\ncomplex tool use, lower cost, and 100% pass rate. Experiments show that models\ntrained on ToolGrad-5k outperform those on expensive baseline datasets and\nproprietary LLMs, even on OOD benchmarks."}
{"id": "2508.03990", "pdf": "https://arxiv.org/pdf/2508.03990.pdf", "abs": "https://arxiv.org/abs/2508.03990", "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks."}
{"id": "2508.04088", "pdf": "https://arxiv.org/pdf/2508.04088.pdf", "abs": "https://arxiv.org/abs/2508.04088", "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning", "authors": ["Jianghangfan Zhang", "Yibo Yan", "Kening Zheng", "Xin Zou", "Song Dai", "Xuming Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance."}
{"id": "2508.04337", "pdf": "https://arxiv.org/pdf/2508.04337.pdf", "abs": "https://arxiv.org/abs/2508.04337", "title": "Modelling and Classifying the Components of a Literature Review", "authors": ["Francisco Bolaos", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels."}
{"id": "2508.04117", "pdf": "https://arxiv.org/pdf/2508.04117.pdf", "abs": "https://arxiv.org/abs/2508.04117", "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks", "authors": ["Zhiwen Ruan", "Yun Chen", "Yutao Hou", "Peng Li", "Yang Liu", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "The pretrained large language models (LLMs) are finetuned with labeled data\nfor better instruction following ability and alignment with human values. In\nthis paper, we study the learning dynamics of LLM finetuning on reasoning tasks\nand reveal the uncovered over-memorization phenomenon during a specific stage\nof LLM finetuning. At this stage, the LLMs have excessively memorized training\ndata and exhibit high test perplexity while maintaining good test accuracy. We\ninvestigate the conditions that lead to LLM over-memorization and find that\ntraining epochs and large learning rates contribute to this issue. Although\nmodels with over-memorization demonstrate comparable test accuracy to normal\nmodels, they suffer from reduced robustness, poor out-of-distribution\ngeneralization, and decreased generation diversity. Our experiments unveil the\nover-memorization to be broadly applicable across different tasks, models, and\nfinetuning methods. Our research highlights that overparameterized, extensively\nfinetuned LLMs exhibit unique learning dynamics distinct from traditional\nmachine learning models. Based on our observations of over-memorization, we\nprovide recommendations on checkpoint and learning rate selection during\nfinetuning."}
{"id": "2508.04408", "pdf": "https://arxiv.org/pdf/2508.04408.pdf", "abs": "https://arxiv.org/abs/2508.04408", "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "authors": ["Carlos Andrs Ramrez Catao", "Makoto Itoh"], "categories": ["cs.SE", "cs.HC"], "comment": "16 pages, 2 figures, 2 formulas, 12 tables", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems."}
{"id": "2508.04149", "pdf": "https://arxiv.org/pdf/2508.04149.pdf", "abs": "https://arxiv.org/abs/2508.04149", "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap", "authors": ["Xuan Qi", "Rongwu Xu", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select", "summary": "Aligning large language models (LLMs) with human preferences is a critical\nchallenge in AI research. While methods like Reinforcement Learning from Human\nFeedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they\noften rely on large, costly preference datasets. The current work lacks methods\nfor high-quality data selection specifically for preference data. In this work,\nwe introduce a novel difficulty-based data selection strategy for preference\ndatasets, grounded in the DPO implicit reward mechanism. By selecting\npreference data examples with smaller DPO implicit reward gaps, which are\nindicative of more challenging cases, we improve data efficiency and model\nalignment. Our approach consistently outperforms five strong baselines across\nmultiple datasets and alignment tasks, achieving superior performance with only\n10\\% of the original data. This principled, efficient selection method offers a\npromising solution for scaling LLM alignment with limited resources."}
{"id": "2508.04412", "pdf": "https://arxiv.org/pdf/2508.04412.pdf", "abs": "https://arxiv.org/abs/2508.04412", "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "authors": ["Thassilo M. Schiepanski", "Nicholas Pil"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs."}
{"id": "2508.04179", "pdf": "https://arxiv.org/pdf/2508.04179.pdf", "abs": "https://arxiv.org/abs/2508.04179", "title": "The State Of TTS: A Case Study with Human Fooling Rates", "authors": ["Praveen Srinivasa Varadhan", "Sherry Thomas", "Sai Teja M. S.", "Suvrat Bhooshan", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at InterSpeech 2025", "summary": "While subjective evaluations in recent years indicate rapid progress in TTS,\ncan current TTS systems truly pass a human deception test in a Turing-like\nevaluation? We introduce Human Fooling Rate (HFR), a metric that directly\nmeasures how often machine-generated speech is mistaken for human. Our\nlarge-scale evaluation of open-source and commercial TTS models reveals\ncritical insights: (i) CMOS-based claims of human parity often fail under\ndeception testing, (ii) TTS progress should be benchmarked on datasets where\nhuman speech achieves high HFRs, as evaluating against monotonous or less\nexpressive reference samples sets a low bar, (iii) Commercial models approach\nhuman deception in zero-shot settings, while open-source systems still struggle\nwith natural conversational speech; (iv) Fine-tuning on high-quality data\nimproves realism but does not fully bridge the gap. Our findings underscore the\nneed for more realistic, human-centric evaluations alongside existing\nsubjective tests."}
{"id": "2508.04651", "pdf": "https://arxiv.org/pdf/2508.04651.pdf", "abs": "https://arxiv.org/abs/2508.04651", "title": "Live Music Models", "authors": ["Lyria Team", "Antoine Caillon", "Brian McWilliams", "Cassie Tarakajian", "Ian Simon", "Ilaria Manco", "Jesse Engel", "Noah Constant", "Pen Li", "Timo I. Denk", "Alberto Lalama", "Andrea Agostinelli", "Anna Huang", "Ethan Manilow", "George Brower", "Hakan Erdogan", "Heidi Lei", "Itai Rolnick", "Ivan Grishchenko", "Manu Orsini", "Matej Kastelic", "Mauricio Zuluaga", "Mauro Verzetti", "Michael Dooley", "Ondrej Skopek", "Rafael Ferrer", "Zaln Borsos", "aron van den Oord", "Douglas Eck", "Eli Collins", "Jason Baldridge", "Tom Hume", "Chris Donahue", "Kehang Han", "Adam Roberts"], "categories": ["cs.SD", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance."}
{"id": "2508.04182", "pdf": "https://arxiv.org/pdf/2508.04182.pdf", "abs": "https://arxiv.org/abs/2508.04182", "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity", "authors": ["Peizheng Guo", "Jingyao Wang", "Wenwen Qiang", "Huijie Guo", "Changwen Zheng", "Jiahuan Zhou", "Gang Hua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across vision-language tasks. However, they may suffer from\nhallucinations--generating outputs that are semantically inconsistent with the\ninput image or text. Through causal analyses, we find that: (i) hallucinations\nwith omission may arise from the failure to adequately capture essential causal\nfactors, and (ii) hallucinations with fabrication are likely caused by the\nmodel being misled by non-causal cues. To address these challenges, we propose\na novel reinforcement learning framework guided by causal completeness, which\njointly considers both causal sufficiency and causal necessity of tokens.\nSpecifically, we evaluate each token's standalone contribution and\ncounterfactual indispensability to define a token-level causal completeness\nreward. This reward is used to construct a causally informed advantage function\nwithin the GRPO optimization framework, encouraging the model to focus on\ntokens that are both causally sufficient and necessary for accurate generation.\nExperimental results across various benchmark datasets and tasks demonstrate\nthe effectiveness of our approach, which effectively mitigates hallucinations\nin MLLMs."}
{"id": "2311.06381", "pdf": "https://arxiv.org/pdf/2311.06381.pdf", "abs": "https://arxiv.org/abs/2311.06381", "title": "Optimal Fidelity Selection for Human-Supervised Search", "authors": ["Piyush Gupta", "Vaibhav Srivastava"], "categories": ["cs.HC"], "comment": null, "summary": "We study optimal fidelity selection in human-supervised underwater visual\nsearch, where operator performance is affected by cognitive factors like\nworkload and fatigue. In our experiments, participants perform two simultaneous\ntasks: detecting underwater mines in videos (primary) and responding to a\nvisual cue to estimate workload (secondary). Videos arrive as a Poisson process\nand queue for review, with the operator choosing between normal fidelity\n(faster playback) and high fidelity. Rewards are based on detection accuracy,\nwhile penalties depend on queue length. Workload is modeled as a hidden state\nusing an Input-Output Hidden Markov Model, and fidelity selection is optimized\nvia a Partially Observable Markov Decision Process. We evaluate two setups:\nfidelity-only selection and a version allowing task delegation to automation to\nmaintain queue stability. Our approach improves performance by 26.5% without\ndelegation and 50.3% with delegation, compared to a baseline where humans\nmanually choose their fidelity levels."}
{"id": "2508.04183", "pdf": "https://arxiv.org/pdf/2508.04183.pdf", "abs": "https://arxiv.org/abs/2508.04183", "title": "Characterizing Deep Research: A Benchmark and Formal Definition", "authors": ["Abhinav Java", "Ashmit Khandelwal", "Sukruta Midigeshi", "Aaron Halfaker", "Amit Deshpande", "Navin Goyal", "Ankur Gupta", "Nagarajan Natarajan", "Amit Sharma"], "categories": ["cs.CL"], "comment": "First three authors contributed equally (ordered alphabetically)", "summary": "Information tasks such as writing surveys or analytical reports require\ncomplex search and reasoning, and have recently been grouped under the umbrella\nof \\textit{deep research} -- a term also adopted by recent models targeting\nthese capabilities. Despite growing interest, the scope of the deep research\ntask remains underdefined and its distinction from other reasoning-intensive\nproblems is poorly understood. In this paper, we propose a formal\ncharacterization of the deep research (DR) task and introduce a benchmark to\nevaluate the performance of DR systems. We argue that the core defining feature\nof deep research is not the production of lengthy report-style outputs, but\nrather the high fan-out over concepts required during the search process, i.e.,\nbroad and reasoning-intensive exploration. To enable objective evaluation, we\ndefine DR using an intermediate output representation that encodes key claims\nuncovered during search-separating the reasoning challenge from surface-level\nreport generation. Based on this formulation, we propose a diverse, challenging\nbenchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,\ndatasets, materials discovery, prior art search) and public interest events\n(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1\nscore ranges between 0.02 and 0.72 for any sub-category. OpenAI's model\nperforms the best with an overall F1 score of 0.55. Analysis of reasoning\ntraces reveals the distribution over the number of referenced sources,\nbranching, and backtracking events executed by current DR systems, motivating\nfuture directions for improving their search mechanisms and grounding\ncapabilities. The benchmark is available at\nhttps://github.com/microsoft/LiveDRBench."}
{"id": "2410.00873", "pdf": "https://arxiv.org/pdf/2410.00873.pdf", "abs": "https://arxiv.org/abs/2410.00873", "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences", "authors": ["Zahra Ashktorab", "Michael Desmond", "Qian Pan", "James M. Johnson", "Martin Santillan Cooper", "Elizabeth M. Daly", "Rahul Nair", "Tejaswini Pedapati", "Hyo Jin Do", "Werner Geyer"], "categories": ["cs.HC"], "comment": null, "summary": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations."}
{"id": "2508.04196", "pdf": "https://arxiv.org/pdf/2508.04196.pdf", "abs": "https://arxiv.org/abs/2508.04196", "title": "Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models", "authors": ["Siddhant Panpatil", "Hiskias Dingeto", "Haon Park"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite significant advances in alignment techniques, we demonstrate that\nstate-of-the-art language models remain vulnerable to carefully crafted\nconversational scenarios that can induce various forms of misalignment without\nexplicit jailbreaking. Through systematic manual red-teaming with\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\nfundamental vulnerabilities in how current alignment methods handle narrative\nimmersion, emotional pressure, and strategic framing. These scenarios\nsuccessfully elicited a range of misaligned behaviors, including deception,\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\ndifferent psychological and contextual vulnerabilities. To validate\ngeneralizability, we distilled our successful manual attacks into\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\nthat sophisticated reasoning capabilities often become attack vectors rather\nthan protective mechanisms, as models can be manipulated into complex\njustifications for misaligned behavior. This work provides (i) a detailed\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\nframework. Together, these findings expose critical gaps in current alignment\nstrategies and highlight the need for robustness against subtle, scenario-based\nmanipulation in future AI systems."}
{"id": "2502.02929", "pdf": "https://arxiv.org/pdf/2502.02929.pdf", "abs": "https://arxiv.org/abs/2502.02929", "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.2; H.5.5; H.5.1"], "comment": "Updated abstract", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with 6DoF in XR. To provide a first step toward identifying\ndesign-related research directions in this space, we conducted an exploratory\nstudy where we recruited 27 participants, consisting of expert and non-expert\nsound designers. The goal was to assess design lessons that can be used to\ninform future research venues in 3D sound design. We ran a within-subjects\nstudy where users designed both a music and cinematic soundscapes. After\nthematically analyzing participant data, we constructed two design lessons: (1)\nProprioception for AR Sound Design, and (2) Balancing Audio-Visual Modalities\nin AR GUIs. Additionally, we provide application domains that can benefit most\nfrom 6DoF sound design based on our results. To expand on these insights, we\nconducted a second within-subjects study comparing AudioMiXR to a 2D panner\nbaseline. Results show that AudioMiXR significantly improved usability (SUS),\nreduced frustration and mental workload (NASA-TLX), and enhanced creativity\nacross all subscales. These findings demonstrate that 6DoF AR interaction\nyields measurable gains in user experience and creative output, positioning\nAudioMiXR as a promising foundation for future AR-based sound design tools."}
{"id": "2508.04199", "pdf": "https://arxiv.org/pdf/2508.04199.pdf", "abs": "https://arxiv.org/abs/2508.04199", "title": "Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts", "authors": ["Millicent Ochieng", "Anja Thieme", "Ignatius Ezeani", "Risa Ueno", "Samuel Maina", "Keshet Ronen", "Javier Gonzalez", "Jacki O'Neill"], "categories": ["cs.CL"], "comment": null, "summary": "Sentiment analysis in low-resource, culturally nuanced contexts challenges\nconventional NLP approaches that assume fixed labels and universal affective\nexpressions. We present a diagnostic framework that treats sentiment as a\ncontext-dependent, culturally embedded construct, and evaluate how large\nlanguage models (LLMs) reason about sentiment in informal, code-mixed WhatsApp\nmessages from Nairobi youth health groups. Using a combination of\nhuman-annotated data, sentiment-flipped counterfactuals, and rubric-based\nexplanation evaluation, we probe LLM interpretability, robustness, and\nalignment with human reasoning. Framing our evaluation through a social-science\nmeasurement lens, we operationalize and interrogate LLMs outputs as an\ninstrument for measuring the abstract concept of sentiment. Our findings reveal\nsignificant variation in model reasoning quality, with top-tier LLMs\ndemonstrating interpretive stability, while open models often falter under\nambiguity or sentiment shifts. This work highlights the need for culturally\nsensitive, reasoning-aware AI evaluation in complex, real-world communication."}
{"id": "2502.17829", "pdf": "https://arxiv.org/pdf/2502.17829.pdf", "abs": "https://arxiv.org/abs/2502.17829", "title": "Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm", "authors": ["Yudong Xie", "Zhifeng Han", "Qinfan Xiao", "Liwei Liang", "Lu-Qi Tao", "Tian-Ling Ren"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Silent speech interfaces (SSI) are being actively developed to assist\nindividuals with communication impairments who have long suffered from daily\nhardships and a reduced quality of life. However, silent sentences are\ndifficult to segment and recognize due to elision and linking. A novel silent\nspeech sentence recognition method is proposed to convert the facial motion\nsignals collected by six-axis accelerometers into transcribed words and\nsentences. A Conformer-based neural network with the\nConnectionist-Temporal-Classification algorithm is used to gain contextual\nunderstanding and translate the non-acoustic signals into words sequences,\nsolely requesting the constituent words in the database. Test results show that\nthe proposed method achieves a 97.17% accuracy in sentence recognition,\nsurpassing the existing silent speech recognition methods with a typical\naccuracy of 85%-95%, and demonstrating the potential of accelerometers as an\navailable SSI modality for high-accuracy silent speech sentence recognition."}
{"id": "2508.04204", "pdf": "https://arxiv.org/pdf/2508.04204.pdf", "abs": "https://arxiv.org/abs/2508.04204", "title": "ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments", "authors": ["Yuquan Wang", "Mi Zhang", "Yining Wang", "Geng Hong", "Xiaoyu You", "Min Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance in\nreasoning-intensive tasks, but they remain vulnerable to harmful content\ngeneration, particularly in the mid-to-late steps of their reasoning processes.\nExisting defense mechanisms, however, rely on costly fine-tuning and additional\nexpert knowledge, which restricts their scalability. In this work, we propose\nReasoningGuard, an inference-time safeguard for LRMs, which injects timely\nsafety aha moments to steer harmless while helpful reasoning processes.\nLeveraging the model's internal attention behavior, our approach accurately\nidentifies critical points in the reasoning path, and triggers spontaneous,\nsafety-oriented reflection. To safeguard both the subsequent reasoning steps\nand the final answers, we further implement a scaling sampling strategy during\nthe decoding phase, selecting the optimal reasoning path. Inducing minimal\nextra inference cost, ReasoningGuard effectively mitigates three types of\njailbreak attacks, including the latest ones targeting the reasoning process of\nLRMs. Our approach outperforms seven existing safeguards, achieving\nstate-of-the-art safety defenses while effectively avoiding the common\nexaggerated safety issues."}
{"id": "2507.19072", "pdf": "https://arxiv.org/pdf/2507.19072.pdf", "abs": "https://arxiv.org/abs/2507.19072", "title": "Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis", "authors": ["Oliver Bates", "Christian Remy", "Kieran Cutting", "Adam Tyler", "Adrian Friday"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "summary": "What could designing for carbon reduction of heating and cooling in\ncommercial settings look like in the near future? How can we challenge dominant\nmindsets and paradigms of efficiency and behaviour change? How can we help\nbuild worlds through our practice that can become future realities? This paper\nintroduces the fictional consultancy ANCSTRL.LAB to explore opportunities for\nmaking space in research projects that can encourage more systems-oriented\ninterventions. We present a design fiction that asks `what if energy management\nand reduction practice embraced systems thinking?'. Our design fiction explores\nhow future energy consultancies could utilise systems thinking, and (more than)\nhuman centred design to re-imagine energy management practice and change\nsystems in ways that are currently unfathomable. We finish by discussing how\nLIMITS research can utilise design fiction and speculative praxis to help build\nnew material realities where more holistic perspectives, the leveraging of\nsystems change, and the imagining of post-neoliberal futures is the norm."}
{"id": "2508.04219", "pdf": "https://arxiv.org/pdf/2508.04219.pdf", "abs": "https://arxiv.org/abs/2508.04219", "title": "Hierarchical Text Classification Using Black Box Large Language Models", "authors": ["Kosuke Yoshimura", "Hisashi Kashima"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 6 figures", "summary": "Hierarchical Text Classification (HTC) aims to assign texts to structured\nlabel hierarchies; however, it faces challenges due to data scarcity and model\ncomplexity. This study explores the feasibility of using black box Large\nLanguage Models (LLMs) accessed via APIs for HTC, as an alternative to\ntraditional machine learning methods that require extensive labeled data and\ncomputational resources. We evaluate three prompting strategies -- Direct Leaf\nLabel Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down\nMulti-step Hierarchical Label Prediction (TMH) -- in both zero-shot and\nfew-shot settings, comparing the accuracy and cost-effectiveness of these\nstrategies. Experiments on two datasets show that a few-shot setting\nconsistently improves classification accuracy compared to a zero-shot setting.\nWhile a traditional machine learning model achieves high accuracy on a dataset\nwith a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the\nmachine learning model on a dataset with a deeper hierarchy. API costs increase\nsignificantly due to the higher input tokens required for deeper label\nhierarchies on DH strategy. These results emphasize the trade-off between\naccuracy improvement and the computational cost of prompt strategy. These\nfindings highlight the potential of black box LLMs for HTC while underscoring\nthe need to carefully select a prompt strategy to balance performance and cost."}
{"id": "2508.00252", "pdf": "https://arxiv.org/pdf/2508.00252.pdf", "abs": "https://arxiv.org/abs/2508.00252", "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices", "authors": ["Wataru Kawabe", "Hiroto Fukuda", "Akihisa Shitara", "Yuri Nakao", "Yusuke Sugano"], "categories": ["cs.HC"], "comment": "31 pages", "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users."}
{"id": "2508.04239", "pdf": "https://arxiv.org/pdf/2508.04239.pdf", "abs": "https://arxiv.org/abs/2508.04239", "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting", "authors": ["Chanjuan Liu", "Shengzhi Wang", "Enqiang Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions."}
{"id": "2410.03723", "pdf": "https://arxiv.org/pdf/2410.03723.pdf", "abs": "https://arxiv.org/abs/2410.03723", "title": "Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "5 main pages, 10 total pages", "summary": "As AI advances in text generation, human trust in AI generated content\nremains constrained by biases that go beyond concerns of accuracy. This study\nexplores how bias shapes the perception of AI versus human generated content.\nThrough three experiments involving text rephrasing, news article\nsummarization, and persuasive writing, we investigated how human raters respond\nto labeled and unlabeled content. While the raters could not differentiate the\ntwo types of texts in the blind test, they overwhelmingly favored content\nlabeled as \"Human Generated,\" over those labeled \"AI Generated,\" by a\npreference score of over 30%. We observed the same pattern even when the labels\nwere deliberately swapped. This human bias against AI has broader societal and\ncognitive implications, as it undervalues AI performance. This study highlights\nthe limitations of human judgment in interacting with AI and offers a\nfoundation for improving human-AI collaboration, especially in creative fields."}
{"id": "2508.04248", "pdf": "https://arxiv.org/pdf/2508.04248.pdf", "abs": "https://arxiv.org/abs/2508.04248", "title": "TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening", "authors": ["Xi Wang", "Anxo Perez", "Javier Parapar", "Fabio Crestani"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper accepted at CIKM 2025", "summary": "The increasing demand for mental health services has outpaced the\navailability of real training data to develop clinical professionals, leading\nto limited support for the diagnosis of depression. This shortage has motivated\nthe development of simulated or virtual patients to assist in training and\nevaluation, but existing approaches often fail to generate clinically valid,\nnatural, and diverse symptom presentations. In this work, we embrace the recent\nadvanced language models as the backbone and propose a novel\nclinician-in-the-loop patient simulation pipeline, TalkDep, with access to\ndiversified patient profiles to develop simulated patients. By conditioning the\nmodel on psychiatric diagnostic criteria, symptom severity scales, and\ncontextual factors, our goal is to create authentic patient responses that can\nbetter support diagnostic model training and evaluation. We verify the\nreliability of these simulated patients with thorough assessments conducted by\nclinical professionals. The availability of validated simulated patients offers\na scalable and adaptable resource for improving the robustness and\ngeneralisability of automatic depression diagnosis systems."}
{"id": "2506.11773", "pdf": "https://arxiv.org/pdf/2506.11773.pdf", "abs": "https://arxiv.org/abs/2506.11773", "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments", "authors": ["Zikang Leng", "Megha Thukral", "Yaqi Liu", "Hrudhai Rajasekhar", "Shruthi K. Hiremath", "Jiaman He", "Thomas Pltz"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR."}
{"id": "2508.04257", "pdf": "https://arxiv.org/pdf/2508.04257.pdf", "abs": "https://arxiv.org/abs/2508.04257", "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs", "authors": ["Zunhai Su", "Kehong Yuan"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."}
{"id": "2508.02096", "pdf": "https://arxiv.org/pdf/2508.02096.pdf", "abs": "https://arxiv.org/abs/2508.02096", "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches", "authors": ["Raj Mahmud", "Yufeng Wu", "Abdullah Bin Sawad", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.IR", "cs.AI", "cs.HC", "H.3.3; H.5.2; I.2.7"], "comment": "Accepted at OzCHI 2025. 23 pages, 1 figure, 5 tables", "summary": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM. Our\nfindings reveal persistent limitations: post hoc surveys dominate, turn-level\naffective UX constructs are rarely assessed, and adaptive behaviours are seldom\nlinked to UX outcomes. LLM-based CRSs introduce further challenges, including\nepistemic opacity and verbosity, yet evaluations infrequently address these\nissues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices."}
{"id": "2508.04266", "pdf": "https://arxiv.org/pdf/2508.04266.pdf", "abs": "https://arxiv.org/abs/2508.04266", "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Qi Sun", "Huaipeng Zhao", "Tao Luo", "Jiandong Zhang", "Xiaoyi Zeng"], "categories": ["cs.CL"], "comment": "submit to AAAI2026", "summary": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1."}
{"id": "2508.04276", "pdf": "https://arxiv.org/pdf/2508.04276.pdf", "abs": "https://arxiv.org/abs/2508.04276", "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models", "authors": ["Jiayi Wen", "Tianxin Chen", "Zhirun Zheng", "Cheng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored."}
{"id": "2508.04325", "pdf": "https://arxiv.org/pdf/2508.04325.pdf", "abs": "https://arxiv.org/abs/2508.04325", "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models", "authors": ["Zizhan Ma", "Wenxuan Wang", "Guo Yu", "Yiu-Fai Cheung", "Meidan Ding", "Jie Liu", "Wenting Chen", "Linlin Shen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare."}
{"id": "2508.04337", "pdf": "https://arxiv.org/pdf/2508.04337.pdf", "abs": "https://arxiv.org/abs/2508.04337", "title": "Modelling and Classifying the Components of a Literature Review", "authors": ["Francisco Bolaos", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels."}
{"id": "2508.04349", "pdf": "https://arxiv.org/pdf/2508.04349.pdf", "abs": "https://arxiv.org/abs/2508.04349", "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy", "authors": ["Hongze Tan", "Jianfei Pan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."}
{"id": "2508.04350", "pdf": "https://arxiv.org/pdf/2508.04350.pdf", "abs": "https://arxiv.org/abs/2508.04350", "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models", "authors": ["Nima Iji", "Kia Dashtipour"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": null, "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks."}
{"id": "2508.04390", "pdf": "https://arxiv.org/pdf/2508.04390.pdf", "abs": "https://arxiv.org/abs/2508.04390", "title": "AIC CTU@FEVER 8: On-premise fact checking through long context RAG", "authors": ["Herbert Ullrich", "Jan Drchal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present our fact-checking pipeline which has scored first\nin FEVER 8 shared task. Our fact-checking system is a simple two-step RAG\npipeline based on our last year's submission. We show how the pipeline can be\nredeployed on-premise, achieving state-of-the-art fact-checking performance (in\nsense of Ev2R test-score), even under the constraint of a single NVidia A10\nGPU, 23GB of graphical memory and 60s running time per claim."}
{"id": "2508.04399", "pdf": "https://arxiv.org/pdf/2508.04399.pdf", "abs": "https://arxiv.org/abs/2508.04399", "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky", "authors": ["Xu Zhang", "Mei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "19 pages, 2 figures", "summary": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP."}
{"id": "2508.04401", "pdf": "https://arxiv.org/pdf/2508.04401.pdf", "abs": "https://arxiv.org/abs/2508.04401", "title": "Why are LLMs' abilities emergent?", "authors": ["Vladimr Havlk"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents."}
{"id": "2508.04402", "pdf": "https://arxiv.org/pdf/2508.04402.pdf", "abs": "https://arxiv.org/abs/2508.04402", "title": "What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems", "authors": ["Kiyotada Mori", "Seiya Kawano", "Chaoran Liu", "Carlos Toshinori Ishi", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": null, "summary": "Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at\nthe front end of their pipeline. The role of ASR in SDSs is to recognize\ninformation in user speech related to response generation appropriately.\nExamining selective listening of humans, which refers to the ability to focus\non and listen to important parts of a conversation during the speech, will\nenable us to identify the ASR capabilities required for SDSs and evaluate them.\nIn this study, we experimentally confirmed selective listening when humans\ngenerate dialogue responses by comparing human transcriptions for generating\ndialogue responses and reference transcriptions. Based on our experimental\nresults, we discuss the possibility of a new ASR evaluation method that\nleverages human selective listening, which can identify the gap between\ntranscription ability between ASR systems and humans."}
{"id": "2508.04403", "pdf": "https://arxiv.org/pdf/2508.04403.pdf", "abs": "https://arxiv.org/abs/2508.04403", "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model", "authors": ["Kiyotada Mori", "Seiya Kawano", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": null, "summary": "Prefetching of dialogue responses has been investigated to reduce\nuser-perceived latency (UPL), which refers to the user's waiting time before\nreceiving the system's response, in spoken dialogue systems. To reduce the UPL,\nit is necessary to predict complete user utterances before the end of the\nuser's speech, typically by language models, to prepare prefetched dialogue\nresponses. In this study, we proposed a prediction confidence model (PCM) that\ndetermines whether prefetching is possible or not by estimating the semantic\nsimilarity between the predicted complete user utterance and the complete user\nutterance. We evaluated our PCM based on the differences between the predicted\ncomplete user utterance and the complete user utterance."}
{"id": "2508.04423", "pdf": "https://arxiv.org/pdf/2508.04423.pdf", "abs": "https://arxiv.org/abs/2508.04423", "title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation", "authors": ["Jie Zhu", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang", "Fang Kong"], "categories": ["cs.CL"], "comment": "under review", "summary": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin."}
{"id": "2508.04440", "pdf": "https://arxiv.org/pdf/2508.04440.pdf", "abs": "https://arxiv.org/abs/2508.04440", "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion", "authors": ["Yutong Wu", "Di Huang", "Ruosi Wan", "Yue Peng", "Shijie Shang", "Chenrui Cao", "Lei Qi", "Rui Zhang", "Zidong Du", "Jie Yan", "Xing Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 17 figures, under review", "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models."}
{"id": "2508.04442", "pdf": "https://arxiv.org/pdf/2508.04442.pdf", "abs": "https://arxiv.org/abs/2508.04442", "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI", "authors": ["Rohaizah Abdul Wahid", "Muhamad Said Nizamuddin Nadim", "Suliana Sulaiman", "Syahmi Akmal Shaharudin", "Muhammad Danial Jupikil", "Iqqwan Jasman Su Azlan Su"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions."}
{"id": "2508.04494", "pdf": "https://arxiv.org/pdf/2508.04494.pdf", "abs": "https://arxiv.org/abs/2508.04494", "title": "CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation", "authors": ["Bastien Litard", "Gabriel Loiseau"], "categories": ["cs.CL"], "comment": "Under review in ARR July 2025", "summary": "Lexical semantics is concerned with both the multiple senses a word can adopt\nin different contexts, and the semantic relations that exist between meanings\nof different words. To investigate them, Contextualized Language Models are a\nvaluable tool that provides context-sensitive representations that can be used\nto investigate lexical meaning. Recent works like XL-LEXEME have leveraged the\ntask of Word-in-Context to fine-tune them to get more semantically accurate\nrepresentations, but Word-in-Context only compares occurrences of the same\nlemma, limiting the range of captured information. In this paper, we propose an\nextension, Concept Differentiation, to include inter-words scenarios. We\nprovide a dataset for this task, derived from SemCor data. Then we fine-tune\nseveral representation models on this dataset. We call these models\nConcept-Aligned Embeddings (CALE). By challenging our models and other models\non various lexical semantic tasks, we demonstrate that the proposed models\nprovide efficient multi-purpose representations of lexical meaning that reach\nbest performances in our experiments. We also show that CALE's fine-tuning\nbrings valuable changes to the spatial organization of embeddings."}
{"id": "2508.04530", "pdf": "https://arxiv.org/pdf/2508.04530.pdf", "abs": "https://arxiv.org/abs/2508.04530", "title": "StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering", "authors": ["Chenglei Shen", "Zhongxiang Sun", "Teng Shi", "Xiao Zhang", "Jun Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness."}
{"id": "2508.04531", "pdf": "https://arxiv.org/pdf/2508.04531.pdf", "abs": "https://arxiv.org/abs/2508.04531", "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning", "authors": ["Zhuang Chen", "Guanqun Bi", "Wen Zhang", "Jiawei Hu", "Aoyun Wang", "Xiyao Xiao", "Kun Feng", "Minlie Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare."}
{"id": "2508.04575", "pdf": "https://arxiv.org/pdf/2508.04575.pdf", "abs": "https://arxiv.org/abs/2508.04575", "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration", "authors": ["Nuo Chen", "Yicheng Tong", "Jiaying Wu", "Minh Duc Duong", "Qian Wang", "Qingyun Zou", "Bryan Hooi", "Bingsheng He"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Preprint", "summary": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes."}
{"id": "2508.04581", "pdf": "https://arxiv.org/pdf/2508.04581.pdf", "abs": "https://arxiv.org/abs/2508.04581", "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning", "authors": ["Magauiya Zhussip", "Dmitriy Shopkhoev", "Ammar Ali", "Stamatios Lefkimmiatis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."}
{"id": "2508.04604", "pdf": "https://arxiv.org/pdf/2508.04604.pdf", "abs": "https://arxiv.org/abs/2508.04604", "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search", "authors": ["Zhejun Zhao", "Yuehu Dong", "Alley Liu", "Lixue Zheng", "Pingsheng Liu", "Dongdong Shen", "Long Xia", "Jiashu Zhao", "Dawei Yin"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system."}
{"id": "2508.04623", "pdf": "https://arxiv.org/pdf/2508.04623.pdf", "abs": "https://arxiv.org/abs/2508.04623", "title": "Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider", "authors": ["Chirag Seth", "Utkarsh Singh"], "categories": ["cs.CL", "cs.IR", "68T50 % Natural language processing (in Computer Science)", "I.2.7; H.2.3"], "comment": null, "summary": "Text-to-SQL translation enables non-expert users to query relational\ndatabases using natural language, with applications in education and business\nintelligence. This study evaluates three lightweight transformer models -\nT5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on\nlow-resource settings. We developed a reusable, model-agnostic pipeline that\ntailors schema formatting to each model's architecture, training them across\n1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form\nAccuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small\nachieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2\n(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL\ngeneration. Despite resource constraints limiting performance, our pipeline's\nmodularity supports future enhancements, such as advanced schema linking or\nalternative base models. This work underscores the potential of compact\ntransformers for accessible text-to-SQL solutions in resource-scarce\nenvironments."}
{"id": "2508.04626", "pdf": "https://arxiv.org/pdf/2508.04626.pdf", "abs": "https://arxiv.org/abs/2508.04626", "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis", "authors": ["Feifan Song", "Bofei Gao", "Yifan Song", "Yi Liu", "Weimin Xiong", "Yuyang Song", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead."}
{"id": "2508.04632", "pdf": "https://arxiv.org/pdf/2508.04632.pdf", "abs": "https://arxiv.org/abs/2508.04632", "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards", "authors": ["Xu Guo", "Tianyi Liang", "Tong Jian", "Xiaogui Yang", "Ling-I Wu", "Chenhui Li", "Zhihui Lu", "Qipeng Guo", "Kai Chen"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research."}
{"id": "2508.04638", "pdf": "https://arxiv.org/pdf/2508.04638.pdf", "abs": "https://arxiv.org/abs/2508.04638", "title": "Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Simona Frenda", "Poppy Gerrard-Abbott", "Nancie Gunson", "Gavin Abercrombie", "Ioannis Konstas"], "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech, i.e. the practice of responding to online hate speech, has\ngained traction in NLP as a promising intervention. While early work emphasised\ncollaboration with non-governmental organisation stakeholders, recent research\ntrends have shifted toward automated pipelines that reuse a small set of legacy\ndatasets, often without input from affected communities. This paper presents a\nsystematic review of 74 NLP studies on counterspeech, analysing the extent to\nwhich stakeholder participation influences dataset creation, model development,\nand evaluation. To complement this analysis, we conducted a participatory case\nstudy with five NGOs specialising in online Gender-Based Violence (oGBV),\nidentifying stakeholder-informed practices for counterspeech generation. Our\nfindings reveal a growing disconnect between current NLP research and the needs\nof communities most impacted by toxic online content. We conclude with concrete\nrecommendations for re-centring stakeholder expertise in counterspeech\nresearch."}
{"id": "2508.04660", "pdf": "https://arxiv.org/pdf/2508.04660.pdf", "abs": "https://arxiv.org/abs/2508.04660", "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs", "authors": ["Noah Ziems", "Dilara Soylu", "Lakshya A Agrawal", "Isaac Miller", "Liheng Lai", "Chen Qian", "Kaiqiang Song", "Meng Jiang", "Dan Klein", "Matei Zaharia", "Karel D'Oosterlinck", "Christopher Potts", "Omar Khattab"], "categories": ["cs.CL"], "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool\nfor post-training language models (LMs). However, AI systems are increasingly\nexpressed as modular programs that mix together multiple LM calls with distinct\nprompt templates and other tools, and it is not clear how best to leverage GRPO\nto improve these systems. We begin to address this challenge by defining\nmmGRPO, a simple multi-module generalization of GRPO that groups LM calls by\nmodule across rollouts and handles variable-length and interrupted\ntrajectories. We find that mmGRPO, composed with automatic prompt optimization,\nimproves accuracy by 11% on average across classification, many-hop search, and\nprivacy-preserving delegation tasks against the post-trained LM, and by 5%\nagainst prompt optimization on its own. We open-source mmGRPO in DSPy as the\ndspy.GRPO optimizer."}
{"id": "2508.04664", "pdf": "https://arxiv.org/pdf/2508.04664.pdf", "abs": "https://arxiv.org/abs/2508.04664", "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management", "authors": ["Mo Li", "L. H. Xu", "Qitai Tan", "Ting Cao", "Yunxin Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. Work in progress", "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale."}
{"id": "2508.04676", "pdf": "https://arxiv.org/pdf/2508.04676.pdf", "abs": "https://arxiv.org/abs/2508.04676", "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay", "authors": ["Yunan Zhang", "Shuoran Jiang", "Mengchen Zhao", "Yuefeng Li", "Yang Fan", "Xiangping Wu", "Qingcai Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe."}
{"id": "2508.04698", "pdf": "https://arxiv.org/pdf/2508.04698.pdf", "abs": "https://arxiv.org/abs/2508.04698", "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data", "authors": ["Thibaut Thonet", "Germn Kruszewski", "Jos Rozen", "Pierre Erbacher", "Marc Dymetman"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance."}
{"id": "2508.04699", "pdf": "https://arxiv.org/pdf/2508.04699.pdf", "abs": "https://arxiv.org/abs/2508.04699", "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis", "authors": ["Anushka Yadav", "Isha Nalawade", "Srujana Pillarichety", "Yashwanth Babu", "Reshmi Ghosh", "Samyadeep Basu", "Wenlong Zhao", "Ali Nasaeh", "Sriram Balasubramanian", "Soundararajan Srinivasan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of reasoning models and their integration into practical AI\nchat bots has led to breakthroughs in solving advanced math, deep search, and\nextractive question answering problems that requires a complex and multi-step\nthought process. Yet, a complete understanding of why these models hallucinate\nmore than general purpose language models is missing. In this investigative\nstudy, we systematicallyexplore reasoning failures of contemporary language\nmodels on multi-hop question answering tasks. We introduce a novel, nuanced\nerror categorization framework that examines failures across three critical\ndimensions: the diversity and uniqueness of source documents involved (\"hops\"),\ncompleteness in capturing relevant information (\"coverage\"), and cognitive\ninefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by\ncomplementary automated metrics, our exploration uncovers intricate error\npatterns often hidden by accuracy-centric evaluations. This investigative\napproach provides deeper insights into the cognitive limitations of current\nmodels and offers actionable guidance toward enhancing reasoning fidelity,\ntransparency, and robustness in future language modeling efforts."}
{"id": "2508.03709", "pdf": "https://arxiv.org/pdf/2508.03709.pdf", "abs": "https://arxiv.org/abs/2508.03709", "title": "MD-LLM-1: A Large Language Model for Molecular Dynamics", "authors": ["Mhd Hussein Murtada", "Z. Faidon Brotzakis", "Michele Vendruscolo"], "categories": ["q-bio.BM", "cs.CL", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Molecular dynamics (MD) is a powerful approach for modelling molecular\nsystems, but it remains computationally intensive on spatial and time scales of\nmany macromolecular systems of biological interest. To explore the\nopportunities offered by deep learning to address this problem, we introduce a\nMolecular Dynamics Large Language Model (MD-LLM) framework to illustrate how\nLLMs can be leveraged to learn protein dynamics and discover states not seen in\ntraining. By applying MD-LLM-1, the first implementation of this approach,\nobtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein\nsystems, we show that training on one conformational state enables the\nprediction of other conformational states. These results indicate that MD-LLM-1\ncan learn the principles for the exploration of the conformational landscapes\nof proteins, although it is not yet modeling explicitly their thermodynamics\nand kinetics."}
{"id": "2508.03711", "pdf": "https://arxiv.org/pdf/2508.03711.pdf", "abs": "https://arxiv.org/abs/2508.03711", "title": "A Social Data-Driven System for Identifying Estate-related Events and Topics", "authors": ["Wenchuan Mu", "Menglin Li", "Kwan Hui Lim"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "comment": "Accepted at ASONAM 2025", "summary": "Social media platforms such as Twitter and Facebook have become deeply\nembedded in our everyday life, offering a dynamic stream of localized news and\npersonal experiences. The ubiquity of these platforms position them as valuable\nresources for identifying estate-related issues, especially in the context of\ngrowing urban populations. In this work, we present a language model-based\nsystem for the detection and classification of estate-related events from\nsocial media content. Our system employs a hierarchical classification\nframework to first filter relevant posts and then categorize them into\nactionable estate-related topics. Additionally, for posts lacking explicit\ngeotags, we apply a transformer-based geolocation module to infer posting\nlocations at the point-of-interest level. This integrated approach supports\ntimely, data-driven insights for urban management, operational response and\nsituational awareness."}
{"id": "2508.03718", "pdf": "https://arxiv.org/pdf/2508.03718.pdf", "abs": "https://arxiv.org/abs/2508.03718", "title": "Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding", "authors": ["Mike Gartner"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 7 figures", "summary": "U.S. health insurance is complex, and inadequate understanding and limited\naccess to justice have dire implications for the most vulnerable. Advances in\nnatural language processing present an opportunity to support efficient,\ncase-specific understanding, and to improve access to justice and healthcare.\nYet existing corpora lack context necessary for assessing even simple cases. We\ncollect and release a corpus of reputable legal and medical text related to\nU.S. health insurance. We also introduce an outcome prediction task for health\ninsurance appeals designed to support regulatory and patient self-help\napplications, and release a labeled benchmark for our task, and models trained\non it."}
{"id": "2508.03733", "pdf": "https://arxiv.org/pdf/2508.03733.pdf", "abs": "https://arxiv.org/abs/2508.03733", "title": "CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning", "authors": ["Wenjie Li", "Yujie Zhang", "Haoran Sun", "Yueqi Li", "Fanrui Zhang", "Mengzhe Xu", "Victoria Borja Clausich", "Sade Mellin", "Renhao Yang", "Chenrun Wang", "Jethro Zih-Shuo Wang", "Shiyi Yao", "Gen Li", "Yidong Xu", "Hanyu Wang", "Yilin Huang", "Angela Lin Wang", "Chen Shi", "Yin Zhang", "Jianan Guo", "Luqi Yang", "Renxuan Li", "Yang Xu", "Jiawei Liu", "Yao Zhang", "Lei Liu", "Carlos Gutirrez SanRomn", "Lei Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Chest X-ray (CXR) imaging is one of the most widely used diagnostic\nmodalities in clinical practice, encompassing a broad spectrum of diagnostic\ntasks. Recent advancements have seen the extensive application of\nreasoning-based multimodal large language models (MLLMs) in medical imaging to\nenhance diagnostic efficiency and interpretability. However, existing\nmultimodal models predominantly rely on \"one-time\" diagnostic approaches,\nlacking verifiable supervision of the reasoning process. This leads to\nchallenges in multi-task CXR diagnosis, including lengthy reasoning, sparse\nrewards, and frequent hallucinations. To address these issues, we propose\nCX-Mind, the first generative model to achieve interleaved \"think-answer\"\nreasoning for CXR tasks, driven by curriculum-based reinforcement learning and\nverifiable process rewards (CuRL-VPR). Specifically, we constructed an\ninstruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148\nsamples, and generated 42,828 high-quality interleaved reasoning data points\nsupervised by clinical reports. Optimization was conducted in two stages under\nthe Group Relative Policy Optimization framework: initially stabilizing basic\nreasoning with closed-domain tasks, followed by transfer to open-domain\ndiagnostics, incorporating rule-based conditional process rewards to bypass the\nneed for pretrained reward models. Extensive experimental results demonstrate\nthat CX-Mind significantly outperforms existing medical and general-domain\nMLLMs in visual understanding, text generation, and spatiotemporal alignment,\nachieving an average performance improvement of 25.1% over comparable\nCXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves\na mean recall@1 across 14 diseases that substantially surpasses the second-best\nresults, with multi-center expert evaluations further confirming its clinical\nutility across multiple dimensions."}
{"id": "2508.03772", "pdf": "https://arxiv.org/pdf/2508.03772.pdf", "abs": "https://arxiv.org/abs/2508.03772", "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks."}
{"id": "2508.03828", "pdf": "https://arxiv.org/pdf/2508.03828.pdf", "abs": "https://arxiv.org/abs/2508.03828", "title": "MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources", "authors": ["Samuel Barham", "Chandler May", "Benjamin Van Durme"], "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles\nwith their citations and scraped web sources; articles are represented in a\nrich data structure, and scraped source texts are stored inline with precise\ncharacter offsets of their citations in the article text. MegaWika 2 is a major\nupgrade from the original MegaWika, spanning six times as many articles and\ntwice as many fully scraped citations. Both MegaWika and MegaWika 2 support\nreport generation research ; whereas MegaWika also focused on supporting\nquestion answering and retrieval applications, MegaWika 2 is designed to\nsupport fact checking and analyses across time and language."}
{"id": "2508.03936", "pdf": "https://arxiv.org/pdf/2508.03936.pdf", "abs": "https://arxiv.org/abs/2508.03936", "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants", "authors": ["Xiangzhe Xu", "Guangyu Shen", "Zian Su", "Siyuan Cheng", "Hanxi Guo", "Lu Yan", "Xuan Chen", "Jiasheng Jiang", "Xiaolong Jin", "Chengpeng Wang", "Zhuo Zhang", "Xiangyu Zhang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "comment": "The first two authors (Xiangzhe Xu and Guangyu Shen) contributed\n  equally to this work", "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems."}
{"id": "2508.03962", "pdf": "https://arxiv.org/pdf/2508.03962.pdf", "abs": "https://arxiv.org/abs/2508.03962", "title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Dionysis Diamantis", "Christos Tryfonopoulos", "Thanasis Vergoulis"], "categories": ["cs.DL", "cs.AI", "cs.CL"], "comment": null, "summary": "The growing volume of scientific literature makes it challenging for\nscientists to move from a list of papers to a synthesized understanding of a\ntopic. Because of the constant influx of new papers on a daily basis, even if a\nscientist identifies a promising set of papers, they still face the tedious\ntask of individually reading through dozens of titles and abstracts to make\nsense of occasionally conflicting findings. To address this critical bottleneck\nin the research workflow, we introduce a summarization feature to BIP! Finder,\na scholarly search engine that ranks literature based on distinct impact\naspects like popularity and influence. Our approach enables users to generate\ntwo types of summaries from top-ranked search results: a concise summary for an\ninstantaneous at-a-glance comprehension and a more comprehensive literature\nreview-style summary for greater, better-organized comprehension. This ability\ndynamically leverages BIP! Finder's already existing impact-based ranking and\nfiltering features to generate context-sensitive, synthesized narratives that\ncan significantly accelerate literature discovery and comprehension."}
{"id": "2508.04001", "pdf": "https://arxiv.org/pdf/2508.04001.pdf", "abs": "https://arxiv.org/abs/2508.04001", "title": "ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval", "authors": ["Fengran Mo", "Jinghan Zhang", "Yuchen Hui", "Jia Ao Sun", "Zhichao Xu", "Zhan Su", "Jian-Yun Nie"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Conversational search aims to satisfy users' complex information needs via\nmultiple-turn interactions. The key challenge lies in revealing real users'\nsearch intent from the context-dependent queries. Previous studies achieve\nconversational search by fine-tuning a conversational dense retriever with\nrelevance judgments between pairs of context-dependent queries and documents.\nHowever, this training paradigm encounters data scarcity issues. To this end,\nwe propose ConvMix, a mixed-criteria framework to augment conversational dense\nretrieval, which covers more aspects than existing data augmentation\nframeworks. We design a two-sided relevance judgment augmentation schema in a\nscalable manner via the aid of large language models. Besides, we integrate the\nframework with quality control mechanisms to obtain semantically diverse\nsamples and near-distribution supervisions to combine various annotated data.\nExperimental results on five widely used benchmarks show that the\nconversational dense retriever trained by our ConvMix framework outperforms\nprevious baseline methods, which demonstrates our superior effectiveness."}
{"id": "2508.04118", "pdf": "https://arxiv.org/pdf/2508.04118.pdf", "abs": "https://arxiv.org/abs/2508.04118", "title": "AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities", "authors": ["Ruochen Zhao", "Simone Conia", "Eric Peng", "Min Li", "Saloni Potdar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Open-domain Knowledge Graph Completion (KGC) faces significant challenges in\nan ever-changing world, especially when considering the continual emergence of\nnew entities in daily news. Existing approaches for KGC mainly rely on\npretrained language models' parametric knowledge, pre-constructed queries, or\nsingle-step retrieval, typically requiring substantial supervision and training\ndata. Even so, they often fail to capture comprehensive and up-to-date\ninformation about unpopular and/or emerging entities. To this end, we introduce\nAgentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework\nthat combines iterative retrieval actions and multi-step reasoning to\ndynamically construct rich knowledge graph triplets. Experiments show that,\ndespite requiring zero training efforts, AgREE significantly outperforms\nexisting methods in constructing knowledge graph triplets, especially for\nemerging entities that were not seen during language models' training\nprocesses, outperforming previous methods by up to 13.7%. Moreover, we propose\na new evaluation methodology that addresses a fundamental weakness of existing\nsetups and a new benchmark for KGC on emerging entities. Our work demonstrates\nthe effectiveness of combining agent-based reasoning with strategic information\nretrieval for maintaining up-to-date knowledge graphs in dynamic information\nenvironments."}
{"id": "2508.04138", "pdf": "https://arxiv.org/pdf/2508.04138.pdf", "abs": "https://arxiv.org/abs/2508.04138", "title": "COPO: Consistency-Aware Policy Optimization", "authors": ["Jinghang Han", "Jiawei Chen", "Hang Shao", "Hao Ma", "Mingcheng Li", "Xintian Shen", "Lihao Zheng", "Wei Chen", "Tao Wei", "Lihua Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning has significantly enhanced the reasoning capabilities\nof Large Language Models (LLMs) in complex problem-solving tasks. Recently, the\nintroduction of DeepSeek R1 has inspired a surge of interest in leveraging\nrule-based rewards as a low-cost alternative for computing advantage functions\nand guiding policy optimization. However, a common challenge observed across\nmany replication and extension efforts is that when multiple sampled responses\nunder a single prompt converge to identical outcomes, whether correct or\nincorrect, the group-based advantage degenerates to zero. This leads to\nvanishing gradients and renders the corresponding samples ineffective for\nlearning, ultimately limiting training efficiency and downstream performance.\nTo address this issue, we propose a consistency-aware policy optimization\nframework that introduces a structured global reward based on outcome\nconsistency, the global loss based on it ensures that, even when model outputs\nshow high intra-group consistency, the training process still receives\nmeaningful learning signals, which encourages the generation of correct and\nself-consistent reasoning paths from a global perspective. Furthermore, we\nincorporate an entropy-based soft blending mechanism that adaptively balances\nlocal advantage estimation with global optimization, enabling dynamic\ntransitions between exploration and convergence throughout training. Our method\nintroduces several key innovations in both reward design and optimization\nstrategy. We validate its effectiveness through substantial performance gains\non multiple mathematical reasoning benchmarks, highlighting the proposed\nframework's robustness and general applicability. Code of this work has been\nreleased at https://github.com/hijih/copo-code.git."}
{"id": "2508.04143", "pdf": "https://arxiv.org/pdf/2508.04143.pdf", "abs": "https://arxiv.org/abs/2508.04143", "title": "Multilingual Source Tracing of Speech Deepfakes: A First Benchmark", "authors": ["Xi Xuan", "Yang Xiao", "Rohan Kumar Das", "Tomi Kinnunen"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted at Interspeech SPSC 2025 - 5th Symposium on Security and\n  Privacy in Speech Communication (Oral)", "summary": "Recent progress in generative AI has made it increasingly easy to create\nnatural-sounding deepfake speech from just a few seconds of audio. While these\ntools support helpful applications, they also raise serious concerns by making\nit possible to generate convincing fake speech in many languages. Current\nresearch has largely focused on detecting fake speech, but little attention has\nbeen given to tracing the source models used to generate it. This paper\nintroduces the first benchmark for multilingual speech deepfake source tracing,\ncovering both mono- and cross-lingual scenarios. We comparatively investigate\nDSP- and SSL-based modeling; examine how SSL representations fine-tuned on\ndifferent languages impact cross-lingual generalization performance; and\nevaluate generalization to unseen languages and speakers. Our findings offer\nthe first comprehensive insights into the challenges of identifying speech\ngeneration models when training and inference languages differ. The dataset,\nprotocol and code are available at\nhttps://github.com/xuanxixi/Multilingual-Source-Tracing."}
{"id": "2508.04166", "pdf": "https://arxiv.org/pdf/2508.04166.pdf", "abs": "https://arxiv.org/abs/2508.04166", "title": "ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations", "authors": ["Subhankar Swain", "Naquee Rizwan", "Nayandeep Deb", "Vishwajeet Singh Solanki", "Vishwa Gangadhar S", "Animesh Mukherjee"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The 2025 Global Risks Report identifies state-based armed conflict and\nsocietal polarisation among the most pressing global threats, with social media\nplaying a central role in amplifying toxic discourse. Memes, as a widely used\nmode of online communication, often serve as vehicles for spreading harmful\ncontent. However, limitations in data accessibility and the high cost of\ndataset curation hinder the development of robust meme moderation systems. To\naddress this challenge, in this work, we introduce a first-of-its-kind dataset\nof 6,300 real-world meme-based posts annotated in two stages: (i) binary\nclassification into toxic and normal, and (ii) fine-grained labelling of toxic\nmemes as hateful, dangerous, or offensive. A key feature of this dataset is\nthat it is enriched with auxiliary metadata of socially relevant tags,\nenhancing the context of each meme. In addition, we propose a tag generation\nmodule that produces socially grounded tags, because most in-the-wild memes\noften do not come with tags. Experimental results show that incorporating these\ntags substantially enhances the performance of state-of-the-art VLMs detection\ntasks. Our contributions offer a novel and scalable foundation for improved\ncontent moderation in multimodal online environments."}
{"id": "2508.04252", "pdf": "https://arxiv.org/pdf/2508.04252.pdf", "abs": "https://arxiv.org/abs/2508.04252", "title": "Graph Representation Learning with Massive Unlabeled Data for Rumor Detection", "authors": ["Chaoqun Cui", "Caiyan Jia"], "categories": ["cs.SI", "cs.CL"], "comment": "9 pages, 3 figures", "summary": "With the development of social media, rumors spread quickly, cause great harm\nto society and economy. Thereby, many effective rumor detection methods have\nbeen developed, among which the rumor propagation structure learning based\nmethods are particularly effective compared to other methods. However, the\nexisting methods still suffer from many issues including the difficulty to\nobtain large-scale labeled rumor datasets, which leads to the low\ngeneralization ability and the performance degeneration on new events since\nrumors are time-critical and usually appear with hot topics or newly emergent\nevents. In order to solve the above problems, in this study, we used\nlarge-scale unlabeled topic datasets crawled from the social media platform\nWeibo and Twitter with claim propagation structure to improve the semantic\nlearning ability of a graph reprentation learing model on various topics. We\nuse three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE\nin two commonly used training strategies, to verify the performance of general\ngraph semi-supervised methods in rumor detection tasks. In addition, for\nalleviating the time and topic difference between unlabeled topic data and\nrumor data, we also collected a rumor dataset covering a variety of topics over\na decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our\nexperiments show that these general graph self-supervised learning methods\noutperform previous methods specifically designed for rumor detection tasks and\nachieve good performance under few-shot conditions, demonstrating the better\ngeneralization ability with the help of our massive unlabeled topic dataset."}
{"id": "2508.04412", "pdf": "https://arxiv.org/pdf/2508.04412.pdf", "abs": "https://arxiv.org/abs/2508.04412", "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "authors": ["Thassilo M. Schiepanski", "Nicholas Pil"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs."}
{"id": "2508.04469", "pdf": "https://arxiv.org/pdf/2508.04469.pdf", "abs": "https://arxiv.org/abs/2508.04469", "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding", "authors": ["Emmanuelle Bourigault", "Pauline Bourigault"], "categories": ["cs.CV", "cs.CL"], "comment": "8 pages, 4 figures", "summary": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding."}
{"id": "2508.04482", "pdf": "https://arxiv.org/pdf/2508.04482.pdf", "abs": "https://arxiv.org/abs/2508.04482", "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use", "authors": ["Xueyu Hu", "Tao Xiong", "Biao Yi", "Zishu Wei", "Ruixuan Xiao", "Yurun Chen", "Jiasheng Ye", "Meiling Tao", "Xiangxin Zhou", "Ziyu Zhao", "Yuhuai Li", "Shengze Xu", "Shenzhi Wang", "Xinchen Xu", "Shuofei Qiao", "Zhaokai Wang", "Kun Kuang", "Tieyong Zeng", "Liang Wang", "Jiwei Li", "Yuchen Eleanor Jiang", "Wangchunshu Zhou", "Guoyin Wang", "Keting Yin", "Zhou Zhao", "Hongxia Yang", "Fan Wu", "Shengyu Zhang", "Fei Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 (Oral)", "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain."}
{"id": "2508.04495", "pdf": "https://arxiv.org/pdf/2508.04495.pdf", "abs": "https://arxiv.org/abs/2508.04495", "title": "Causal Reflection with Language Models", "authors": ["Abi Aryan", "Zac Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments."}
{"id": "2508.04567", "pdf": "https://arxiv.org/pdf/2508.04567.pdf", "abs": "https://arxiv.org/abs/2508.04567", "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective", "authors": ["Yifan Li", "Kun Zhou", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As scaling up training data has significantly improved the general multimodal\ncapabilities of Large Vision-Language Models (LVLMs), they still suffer from\nthe hallucination issue, generating text that is inconsistent with the visual\ninput. This phenomenon motivates us to systematically investigate the role of\ntraining data in hallucination. We introduce a new benchmark, POPEv2, which\nconsists of counterfactual images collected from the training data of LVLMs\nwith certain objects masked. Through comprehensive evaluation on POPEv2, we\nfind that current LVLMs suffer from training bias: they fail to fully leverage\ntheir training data and hallucinate more frequently on images seen during\ntraining. Specifically, they perform poorly on counterfactual images, often\nincorrectly answering ``Yes'' to questions about masked objects. To understand\nthis issue, we conduct probing experiments on the models' internal components,\nrevealing that this training bias is primarily located in the language modeling\n(LM) head. Based on these findings, we propose Obliviate, an efficient and\nlightweight unlearning method designed to mitigate object hallucination via\ntraining bias unlearning. Obliviate identifies the discrepancy between\nground-truth labels and model outputs on the training data as a proxy for bias\nand adopts a parameter- and data-efficient fine-tuning strategy that only\nupdates the LM head. Extensive experiments demonstrate the effectiveness of our\napproach. While only reusing the training data and updating approximately 2\\%\nof the parameters, Obliviate significantly reduces hallucination across both\ndiscriminative and generative tasks. Furthermore, it demonstrates strong\nscalability with respect to both model size (2B to 72B) and training data\nvolume, and exhibits promising generalization to hallucination types beyond\nobject-level hallucination. Our code and data will be publicly released."}
{"id": "2508.04571", "pdf": "https://arxiv.org/pdf/2508.04571.pdf", "abs": "https://arxiv.org/abs/2508.04571", "title": "Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation", "authors": ["Claudio Pomo", "Matteo Attimonelli", "Danilo Danese", "Fedelucio Narducci", "Tommaso Di Noia"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Accepted as Full Research Papers at CIKM 2025", "summary": "Multimodal Recommender Systems aim to improve recommendation accuracy by\nintegrating heterogeneous content, such as images and textual metadata. While\neffective, it remains unclear whether their gains stem from true multimodal\nunderstanding or increased model complexity. This work investigates the role of\nmultimodal item embeddings, emphasizing the semantic informativeness of the\nrepresentations. Initial experiments reveal that embeddings from standard\nextractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on\nmodality-specific encoders and ad hoc fusion strategies that lack control over\ncross-modal alignment. To overcome these limitations, we leverage Large\nVision-Language Models (LVLMs) to generate multimodal-by-design embeddings via\nstructured prompts. This approach yields semantically aligned representations\nwithout requiring any fusion. Experiments across multiple settings show notable\nperformance improvements. Furthermore, LVLMs embeddings offer a distinctive\nadvantage: they can be decoded into structured textual descriptions, enabling\ndirect assessment of their multimodal comprehension. When such descriptions are\nincorporated as side content into recommender systems, they improve\nrecommendation performance, empirically validating the semantic depth and\nalignment encoded within LVLMs outputs. Our study highlights the importance of\nsemantically rich representations and positions LVLMs as a compelling\nfoundation for building robust and meaningful multimodal representations in\nrecommendation tasks."}
{"id": "2508.04586", "pdf": "https://arxiv.org/pdf/2508.04586.pdf", "abs": "https://arxiv.org/abs/2508.04586", "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference", "authors": ["Nuo Chen", "Moming Duan", "Andre Huikai Lin", "Qian Wang", "Jiaying Wu", "Bingsheng He"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research."}
{"id": "2508.04683", "pdf": "https://arxiv.org/pdf/2508.04683.pdf", "abs": "https://arxiv.org/abs/2508.04683", "title": "Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering", "authors": ["Karthik Menon", "Batool Arhamna Haider", "Muhammad Arham", "Kanwal Mehreen", "Ram Mohan Rao Kadiyala", "Hamza Farooq"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study introduces Query Attribute Modeling (QAM), a hybrid framework that\nenhances search precision and relevance by decomposing open text queries into\nstructured metadata tags and semantic elements. QAM addresses traditional\nsearch limitations by automatically extracting metadata filters from free-form\ntext queries, reducing noise and enabling focused retrieval of relevant items.\n  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique\nitems with 40,000+ reviews and detailed product attributes) demonstrated QAM's\nsuperior performance, achieving a mean average precision at 5 (mAP@5) of\n52.99\\%. This represents significant improvement over conventional methods,\nincluding BM25 keyword search, encoder-based semantic similarity search,\ncross-encoder re-ranking, and hybrid search combining BM25 and semantic results\nvia Reciprocal Rank Fusion (RRF). The results establish QAM as a robust\nsolution for Enterprise Search applications, particularly in e-commerce\nsystems."}
{"id": "2508.04700", "pdf": "https://arxiv.org/pdf/2508.04700.pdf", "abs": "https://arxiv.org/abs/2508.04700", "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience", "authors": ["Zeyi Sun", "Ziyu Liu", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Tong Wu", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "comment": "Code at https://github.com/SunzeY/SEAgent", "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS."}
{"id": "2406.14805", "pdf": "https://arxiv.org/pdf/2406.14805.pdf", "abs": "https://arxiv.org/abs/2406.14805", "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "categories": ["cs.CL"], "comment": "KDD 2025", "summary": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs."}
{"id": "2407.18454", "pdf": "https://arxiv.org/pdf/2407.18454.pdf", "abs": "https://arxiv.org/abs/2407.18454", "title": "Fairness Definitions in Language Models Explained", "authors": ["Avash Palikhe", "Zichong Wang", "Zhipeng Yin", "Wenbin Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language Models (LMs) have demonstrated exceptional performance across\nvarious Natural Language Processing (NLP) tasks. Despite these advancements,\nLMs can inherit and amplify societal biases related to sensitive attributes\nsuch as gender and race, limiting their adoption in real-world applications.\nTherefore, fairness has been extensively explored in LMs, leading to the\nproposal of various fairness notions. However, the lack of clear agreement on\nwhich fairness definition to apply in specific contexts and the complexity of\nunderstanding the distinctions between these definitions can create confusion\nand impede further progress. To this end, this paper proposes a systematic\nsurvey that clarifies the definitions of fairness as they apply to LMs.\nSpecifically, we begin with a brief introduction to LMs and fairness in LMs,\nfollowed by a comprehensive, up-to-date overview of existing fairness notions\nin LMs and the introduction of a novel taxonomy that categorizes these concepts\nbased on their transformer architecture: encoder-only, decoder-only, and\nencoder-decoder LMs. We further illustrate each definition through experiments,\nshowcasing their practical implications and outcomes. Finally, we discuss\ncurrent research challenges and open questions, aiming to foster innovative\nideas and advance the field. The repository is publicly available online at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions."}
{"id": "2409.15395", "pdf": "https://arxiv.org/pdf/2409.15395.pdf", "abs": "https://arxiv.org/abs/2409.15395", "title": "Parse Trees Guided LLM Prompt Compression", "authors": ["Wenhao Mao", "Chengbin Hou", "Tianyu Zhang", "Xinyu Lin", "Ke Tang", "Hairong Lv"], "categories": ["cs.CL", "cs.AI"], "comment": "IEEE TPAMI major revision submitted", "summary": "Offering rich contexts to Large Language Models (LLMs) has shown to boost the\nperformance in various tasks, but the resulting longer prompt would increase\nthe computational cost and might exceed the input limit of LLMs. Recently, some\nprompt compression methods have been suggested to shorten the length of prompts\nby using language models to generate shorter prompts or by developing\ncomputational models to select important parts of original prompt. The\ngenerative compression methods would suffer from issues like hallucination,\nwhile the selective compression methods have not involved linguistic rules and\noverlook the global structure of prompt. To this end, we propose a novel\nselective compression method called PartPrompt. It first obtains a parse tree\nfor each sentence based on linguistic rules, and calculates local information\nentropy for each node in a parse tree. These local parse trees are then\norganized into a global tree according to the hierarchical structure such as\nthe dependency of sentences, paragraphs, and sections. After that, the\nroot-ward propagation and leaf-ward propagation are proposed to adjust node\nvalues over the global tree. Finally, a recursive algorithm is developed to\nprune the global tree based on the adjusted node values. The experiments show\nthat PartPrompt receives the state-of-the-art performance across various\ndatasets, metrics, compression ratios, and target LLMs for inference. The\nin-depth ablation studies confirm the effectiveness of designs in PartPrompt,\nand other additional experiments also demonstrate its superiority in terms of\nthe coherence of compressed prompts and in the extreme long prompt scenario."}
{"id": "2410.03723", "pdf": "https://arxiv.org/pdf/2410.03723.pdf", "abs": "https://arxiv.org/abs/2410.03723", "title": "Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "5 main pages, 10 total pages", "summary": "As AI advances in text generation, human trust in AI generated content\nremains constrained by biases that go beyond concerns of accuracy. This study\nexplores how bias shapes the perception of AI versus human generated content.\nThrough three experiments involving text rephrasing, news article\nsummarization, and persuasive writing, we investigated how human raters respond\nto labeled and unlabeled content. While the raters could not differentiate the\ntwo types of texts in the blind test, they overwhelmingly favored content\nlabeled as \"Human Generated,\" over those labeled \"AI Generated,\" by a\npreference score of over 30%. We observed the same pattern even when the labels\nwere deliberately swapped. This human bias against AI has broader societal and\ncognitive implications, as it undervalues AI performance. This study highlights\nthe limitations of human judgment in interacting with AI and offers a\nfoundation for improving human-AI collaboration, especially in creative fields."}
{"id": "2410.15576", "pdf": "https://arxiv.org/pdf/2410.15576.pdf", "abs": "https://arxiv.org/abs/2410.15576", "title": "A Survey of Conversational Search", "authors": ["Fengran Mo", "Kelong Mao", "Ziliang Zhao", "Hongjin Qian", "Haonan Chen", "Yiruo Cheng", "Xiaoxi Li", "Yutao Zhu", "Zhicheng Dou", "Jian-Yun Nie"], "categories": ["cs.CL", "cs.IR"], "comment": "38 pages, 8 figures, corresponding Github repository:\n  https://github.com/fengranMark/ConvSearch-Survey", "summary": "As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch."}
{"id": "2410.16520", "pdf": "https://arxiv.org/pdf/2410.16520.pdf", "abs": "https://arxiv.org/abs/2410.16520", "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context", "authors": ["Naba Rizvi", "Harper Strickland", "Daniel Gitelman", "Tristan Cooper", "Alexis Morales-Flores", "Michael Golden", "Aekta Kallepalli", "Akshat Alurkar", "Haaset Owens", "Saleha Ahmedi", "Isha Khirwadkar", "Imani Munyaka", "Nedjma Ousidhoum"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to ACL main 2025, 9 pages, 5 figures, 7 tables", "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."}
{"id": "2411.08397", "pdf": "https://arxiv.org/pdf/2411.08397.pdf", "abs": "https://arxiv.org/abs/2411.08397", "title": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision", "authors": ["Aoi Ito", "Kota Dohi", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries."}
{"id": "2412.12422", "pdf": "https://arxiv.org/pdf/2412.12422.pdf", "abs": "https://arxiv.org/abs/2412.12422", "title": "FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs", "authors": ["Monica Munnangi", "Akshay Swaminathan", "Jason Alan Fries", "Jenelle Jindal", "Sanjana Narayanan", "Ivan Lopez", "Lucia Tu", "Philip Chung", "Jesutofunmi A. Omiye", "Mehr Kashyap", "Nigam Shah"], "categories": ["cs.CL"], "comment": "To appear at MLHC 2025", "summary": "Verifying and attributing factual claims is essential for the safe and\neffective use of large language models (LLMs) in healthcare. A core component\nof factuality evaluation is fact decomposition, the process of breaking down\ncomplex clinical statements into fine-grained atomic facts for verification.\nRecent work has proposed fact decomposition, which uses LLMs to rewrite source\ntext into concise sentences conveying a single piece of information, to\nfacilitate fine-grained fact verification. However, clinical documentation\nposes unique challenges for fact decomposition due to dense terminology and\ndiverse note types and remains understudied. To address this gap and explore\nthese challenges, we present FactEHR, an NLI dataset consisting of document\nfact decompositions for 2,168 clinical notes spanning four types from three\nhospital systems, resulting in 987,266 entailment pairs. We assess the\ngenerated facts on different axes, from entailment evaluation of LLMs to a\nqualitative analysis. Our evaluation, including review by the clinicians,\nreveals substantial variability in LLM performance for fact decomposition. For\nexample, Gemini-1.5-Flash consistently generates relevant and accurate facts,\nwhile Llama-3 8B produces fewer and less consistent outputs. The results\nunderscore the need for better LLM capabilities to support factual verification\nin clinical text."}
{"id": "2502.11268", "pdf": "https://arxiv.org/pdf/2502.11268.pdf", "abs": "https://arxiv.org/abs/2502.11268", "title": "Improved Unbiased Watermark for Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts."}
{"id": "2502.13053", "pdf": "https://arxiv.org/pdf/2502.13053.pdf", "abs": "https://arxiv.org/abs/2502.13053", "title": "Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks", "authors": ["Yurun Chen", "Xavier Hu", "Keting Yin", "Juncheng Li", "Shengyu Zhang"], "categories": ["cs.CL"], "comment": "Accepted at ACM MM 2025 Main Conference", "summary": "As researchers continue to optimize AI agents for more effective task\nexecution within operating systems, they often overlook a critical security\nconcern: the ability of these agents to detect \"impostors\" within their\nenvironment. Through an analysis of the agents' operational context, we\nidentify a significant threat-attackers can disguise malicious attacks as\nenvironmental elements, injecting active disturbances into the agents'\nexecution processes to manipulate their decision-making. We define this novel\nthreat as the Active Environment Injection Attack (AEIA). Focusing on the\ninteraction mechanisms of the Android OS, we conduct a risk assessment of AEIA\nand identify two critical security vulnerabilities: (1) Adversarial content\ninjection in multimodal interaction interfaces, where attackers embed\nadversarial instructions within environmental elements to mislead agent\ndecision-making; and (2) Reasoning gap vulnerabilities in the agent's task\nexecution process, which increase susceptibility to AEIA attacks during\nreasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,\nan attack scheme that exploits interaction vulnerabilities in mobile operating\nsystems to assess the robustness of MLLM-based agents. Experimental results\nshow that even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% on the AndroidWorld benchmark by combining\ntwo vulnerabilities."}
{"id": "2502.15434", "pdf": "https://arxiv.org/pdf/2502.15434.pdf", "abs": "https://arxiv.org/abs/2502.15434", "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "15 pages", "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials."}
{"id": "2502.16781", "pdf": "https://arxiv.org/pdf/2502.16781.pdf", "abs": "https://arxiv.org/abs/2502.16781", "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data", "authors": ["Bhawna Piryani", "Jamshid Mozafari", "Abdelrahman Abdallah", "Antoine Doucet", "Adam Jatowt"], "categories": ["cs.CL"], "comment": "Accepted at CIKM 2025", "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts."}
{"id": "2502.17945", "pdf": "https://arxiv.org/pdf/2502.17945.pdf", "abs": "https://arxiv.org/abs/2502.17945", "title": "Assessing Agentic Large Language Models in Multilingual National Bias", "authors": ["Qianying Liu", "Katrina Qiyao Wang", "Fei Cheng", "Sadao Kurohashi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings. 14 pages", "summary": "Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education. \\footnote{Code\navailable at: https://github.com/yiyunya/assess_agentic_national_bias"}
{"id": "2503.09516", "pdf": "https://arxiv.org/pdf/2503.09516.pdf", "abs": "https://arxiv.org/abs/2503.09516", "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "authors": ["Bowen Jin", "Hansi Zeng", "Zhenrui Yue", "Jinsung Yoon", "Sercan Arik", "Dong Wang", "Hamed Zamani", "Jiawei Han"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "31 pages", "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1."}
{"id": "2503.10533", "pdf": "https://arxiv.org/pdf/2503.10533.pdf", "abs": "https://arxiv.org/abs/2503.10533", "title": "The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory", "authors": ["Robin Schmucker", "Steven Moore"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nThis method offers a scalable, pre-deployment evaluation without requiring\nstudent data, but its predictive validity concerning empirical IRT parameters\nis underexplored. To address this gap, we conducted a study involving 7,126\nmultiple-choice questions across various STEM subjects (physical science,\nmathematics, and life/earth sciences). Using an automated approach, we\nannotated each question with a 19-criteria IWF rubric and studied relationships\nto data-driven IRT parameters. Our analysis revealed statistically significant\nlinks between the number of IWFs and IRT difficulty and discrimination\nparameters, particularly in life/earth and physical science domains. We further\nobserved how specific IWF criteria can impact item quality more and less\nseverely (e.g., negative wording vs. implausible distractors) and how they\nmight make a question more or less challenging. Overall, our findings establish\nautomated IWF analysis as a valuable supplement to traditional validation,\nproviding an efficient method for initial item screening, particularly for\nflagging low-difficulty MCQs. Our findings show the need for further research\non domain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation."}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299.pdf", "abs": "https://arxiv.org/abs/2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."}
{"id": "2503.18878", "pdf": "https://arxiv.org/pdf/2503.18878.pdf", "abs": "https://arxiv.org/abs/2503.18878", "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders", "authors": ["Andrey Galichin", "Alexey Dontsov", "Polina Druzhinina", "Anton Razzhigaev", "Oleg Y. Rogov", "Elena Tutubalina", "Ivan Oseledets"], "categories": ["cs.CL"], "comment": null, "summary": "Recent LLMs like DeepSeek-R1 have demonstrated state-of-the-art performance\nby integrating deep thinking and complex reasoning during generation. However,\nthe internal mechanisms behind these reasoning processes remain unexplored. We\nobserve reasoning LLMs consistently use vocabulary associated with human\nreasoning processes. We hypothesize these words correspond to specific\nreasoning moments within the models' internal mechanisms. To test this\nhypothesis, we employ Sparse Autoencoders (SAEs), a technique for sparse\ndecomposition of neural network activations into human-interpretable features.\nWe introduce ReasonScore, an automatic metric to identify active SAE features\nduring these reasoning moments. We perform manual and automatic interpretation\nof the features detected by our metric, and find those with activation patterns\nmatching uncertainty, exploratory thinking, and reflection. Through steering\nexperiments, we demonstrate that amplifying these features increases\nperformance on reasoning-intensive benchmarks (+2.2%) while producing longer\nreasoning traces (+20.5%). Using the model diffing technique, we provide\nevidence that these features are present only in models with reasoning\ncapabilities. Our work provides the first step towards a mechanistic\nunderstanding of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning"}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311.pdf", "abs": "https://arxiv.org/abs/2504.12311", "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Yang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight strategy for adapting foundation\nmodels to downstream tasks, particularly for resource-constrained systems. As\npre-trained prompts become valuable assets, combining multiple source prompts\noffers a promising approach to enhance generalization for new tasks by\nleveraging complementary knowledge. However, naive aggregation often overlooks\ndifferent source prompts have different contribution potential to the target\ntask. To address this, we propose HGPrompt, a dynamic framework that learns\noptimal ensemble weights. These weights are optimized by jointly maximizing an\ninformation-theoretic metric for transferability and minimizing gradient\nconflicts via a novel regularization strategy. Specifically, we propose a\ndifferentiable prompt transferability metric to captures the discriminability\nof prompt-induced features on the target task. Meanwhile, HGPrompt match the\ngradient variances with respect to different source prompts based on Hessian\nand Fisher Information, ensuring stable and coherent knowledge transfer while\nsuppressing gradient conflicts among them. Extensive experiments on the\nlarge-scale VTAB benchmark demonstrate the state-of-the-art performance of\nHGPrompt, validating its effectiveness in learning an optimal ensemble for\neffective multi-source prompt transfer."}
{"id": "2504.12342", "pdf": "https://arxiv.org/pdf/2504.12342.pdf", "abs": "https://arxiv.org/abs/2504.12342", "title": "CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine", "authors": ["Hanmeng Zhong", "Linqing Chen", "Wentao Wu", "Weilei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent development in Retrieval-Augmented Large Language Models (LLMs) have\nshown great promise in biomedical applications. How ever, a critical gap\npersists in reliably evaluating their curation ability the process by which\nmodels select and integrate relevant references while filtering out noise. To\naddress this, we introduce the benchmark for Curation of Retrieval-Augmented\nLLMs in Biomedicine (CRAB), the first multilingual benchmark tailored for\nevaluating the biomedical curation of retrieval-augmented LLMs, available in\nEnglish, French, German and Chinese. By incorporating a novel citation-based\nevaluation metric, CRAB quantifies the curation performance of\nretrieval-augmented LLMs in biomedicine. Experimental results reveal\nsignificant discrepancies in the curation performance of mainstream LLMs,\nunderscoring the urgent need to improve it in the domain of biomedicine. Our\ndataset is available at https://huggingface.co/datasets/zhm0/CRAB."}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194.pdf", "abs": "https://arxiv.org/abs/2504.14194", "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "categories": ["cs.CL"], "comment": "ACL 2025 Best Theme Paper Award", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."}
{"id": "2504.16414", "pdf": "https://arxiv.org/pdf/2504.16414.pdf", "abs": "https://arxiv.org/abs/2504.16414", "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "authors": ["Mohammad Khodadad", "Ali Shiraee Kasmaee", "Mahdi Astaraki", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "categories": ["cs.CL"], "comment": null, "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics."}
{"id": "2505.15050", "pdf": "https://arxiv.org/pdf/2505.15050.pdf", "abs": "https://arxiv.org/abs/2505.15050", "title": "Improving the fact-checking performance of language models by relying on their entailment ability", "authors": ["Gaurav Kumar", "Debajyoti Mazumder", "Ayush Garg", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": "44 pages", "summary": "Automated fact-checking is a crucial task in this digital age. The NLP\ncommunity has been trying various strategies to build robust fact-checking\nsystems. However, we have not been very successful yet. One main reason behind\nthis is that fact verification is a complex process. Language models have to\nparse through multiple pieces of evidence, often contradicting each other, to\npredict a claim's veracity. In this paper, we proposed a simple yet effective\nstrategy, where we relied on the entailment ability of language models to\nimprove the fact-checking performance. Apart from that, we did a comparison of\ndifferent prompting and fine-tuning strategies, as it is currently lacking in\nthe literature. Some of our observations are: (i) training language models with\nraw evidence sentences (TBE-1) and overall claim-evidence understanding (TBE-2)\nresulted in an improvement up to 8.20% and 16.39% in macro-F1 for RAW-FC\ndataset, and (ii) training language models with entailed justifications (TBE-3)\noutperformed the baselines by a huge margin (up to 28.57% and 44.26% for\nLIAR-RAW and RAW-FC, respectively). We have shared our code repository to\nreproduce the results."}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227.pdf", "abs": "https://arxiv.org/abs/2505.16227", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601.pdf", "abs": "https://arxiv.org/abs/2505.18601", "title": "Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548.pdf", "abs": "https://arxiv.org/abs/2505.22548", "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Kaiyun Huang", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) reasoning has shown great promise in enhancing\nthe emotion understanding performance of large language models (LLMs). However,\ncurrent fixed-length CoT methods struggle to balance reasoning depth and\nefficiency. Simple tasks (e.g., sentiment classification) are over-reasoned,\nwhile complex tasks (e.g., sarcasm understanding) lack depth. To fill this gap,\nwe present Emotion-o1, an adaptive CoT framework that dynamically adjusts\nreasoning length based on emotion-task complexity. Emotion-o1 is trained by\ndistilling adaptive CoT patterns from a reasoning-oriented LLM, followed by\nsupervised fine-tuning and reinforcement learning with a four-part reward\ntargeting accuracy, brevity, structure, and redundancy. Experimental results on\nfour emotion tasks highlight: (1) Emotion-o1 demonstrates significant\nimprovements over its backbone, with F1 score increases of 10%(Sentiment),\n5%(Emotion), 18%(Humor), and 27%(Sarcasm). (2) In sentiment and sarcasm tasks,\nour 8B model demonstrates superior performance against advanced LLMs,\noutperforming Grok-3 by 1.1% and Claude-3.7 by 2%. (3) The framework maintains\naccuracy while reducing reasoning length by 83% compared to OpenAI-o1,\ndemonstrating effective precision-efficiency optimization. Emotion-o1\neffectively balances reasoning depth and efficiency for emotion understanding\nin LLMs."}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132.pdf", "abs": "https://arxiv.org/abs/2506.02132", "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "authors": ["Michael Li", "Nishant Subramani"], "categories": ["cs.CL", "cs.LG"], "comment": "INTERPLAY Workshop COLM 2025", "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how 25 models - from classical architectures (BERT, DeBERTa,\nGPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) - represent lexical identity and inflectional morphology across six\ntypologically diverse languages. Using linear and nonlinear classifiers trained\non hidden activations, we predict word lemmas and inflectional features layer\nby layer. We find that models concentrate lexical information linearly in early\nlayers and increasingly nonlinearly in later layers, while keeping inflectional\ninformation uniformly accessible and linearly separable throughout. Additional\nexperiments probe the nature of these encodings: attention and residual\nanalyses examine where within layers information can be recovered, steering\nvector experiments test what information can be functionally manipulated, and\nintrinsic dimensionality analyses explore how the representational structure\nevolves across layers. Remarkably, these encoding patterns emerge across all\nmodels we test, despite differences in architecture, size, and training regime\n(pretrained and instruction-tuned variants). This suggests that, even with\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties are\nimportant for next token prediction and are learned early during pretraining.\nOur code is available at https://github.com/ml5885/model_internal_sleuthing"}
{"id": "2506.05828", "pdf": "https://arxiv.org/pdf/2506.05828.pdf", "abs": "https://arxiv.org/abs/2506.05828", "title": "FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging", "authors": ["Zichen Tang", "Haihong E", "Ziyan Ma", "Haoyang He", "Jiacheng Liu", "Zhongjun Yang", "Zihua Rong", "Rongjin Li", "Kun Ji", "Qing Huang", "Xinyang Hu", "Yang Liu", "Qianhe Zheng"], "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "We introduce FinanceReasoning, a novel benchmark designed to evaluate the\nreasoning capabilities of large reasoning models (LRMs) in financial numerical\nreasoning problems. Compared to existing benchmarks, our work provides three\nkey advancements. (1) Credibility: We update 15.6% of the questions from four\npublic datasets, annotating 908 new questions with detailed Python solutions\nand rigorously refining evaluation standards. This enables an accurate\nassessment of the reasoning improvements of LRMs. (2) Comprehensiveness:\nFinanceReasoning covers 67.8% of financial concepts and formulas, significantly\nsurpassing existing datasets. Additionally, we construct 3,133 Python-formatted\nfunctions, which enhances LRMs' financial reasoning capabilities through\nrefined knowledge (e.g., 83.2% $\\rightarrow$ 91.6% for GPT-4o). (3) Challenge:\nModels are required to apply multiple financial formulas for precise numerical\nreasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with\nPoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical\nprecision. We demonstrate that combining Reasoner and Programmer models can\neffectively enhance LRMs' performance (e.g., 83.2% $\\rightarrow$ 87.8% for\nDeepSeek-R1). Our work paves the way for future research on evaluating and\nimproving LRMs in domain-specific complex reasoning tasks."}
{"id": "2506.05949", "pdf": "https://arxiv.org/pdf/2506.05949.pdf", "abs": "https://arxiv.org/abs/2506.05949", "title": "NameTag 3: A Tool and a Service for Multilingual/Multitagset NER", "authors": ["Jana Strakov", "Milan Straka"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "We introduce NameTag 3, an open-source tool and cloud-based web service for\nmultilingual, multidataset, and multitagset named entity recognition (NER),\nsupporting both flat and nested entities. NameTag 3 achieves state-of-the-art\nresults on 21 test datasets in 15 languages and remains competitive on the\nrest, even against larger models. It is available as a command-line tool and as\na cloud-based service, enabling use without local installation. NameTag 3 web\nservice currently provides flat NER for 17 languages, trained on 21 corpora and\nthree NE tagsets, all powered by a single 355M-parameter fine-tuned model; and\nnested NER for Czech, powered by a 126M fine-tuned model. The source code is\nlicensed under open-source MPL 2.0, while the models are distributed under\nnon-commercial CC BY-NC-SA 4.0. Documentation is available at\nhttps://ufal.mff.cuni.cz/nametag, source code at\nhttps://github.com/ufal/nametag3, and trained models via https://lindat.cz. The\nREST service and the web application can be found at\nhttps://lindat.mff.cuni.cz/services/nametag/. A demonstration video is\navailable at https://www.youtube.com/watch?v=-gaGnP0IV8A."}
{"id": "2506.11127", "pdf": "https://arxiv.org/pdf/2506.11127.pdf", "abs": "https://arxiv.org/abs/2506.11127", "title": "UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this issue, we propose replacing text with\nspeech as the instruction input modality for GUI agents, and introduce\nUITron-Speech, which is the first end-to-end GUI agent capable of directly\nprocessing speech instructions and on-device screenshots to predict user\nactions. To tackle the problem of data scarcity, we synthesize high-quality\nspeech instruction datasets using a random-speaker text-to-speech model.\nAdditionally, we design a mixed-modality training strategy to mitigate the\ninherent modality imbalance in pre-trained foundation models. Furthermore, we\nconduct a statistical analysis of the distribution of GUI grounding prediction\nerrors and propose a training-free two-step grounding refinement method to\nalleviate minor localization deviations. Extensive experiments on multiple\nbenchmarks demonstrate that UITron-Speech achieves robust performance and\nsuperior adaptability, underscoring the feasibility and potential of\nspeech-driven GUI agents for more accessible and intelligent human-computer\ninteraction. Our code and datasets are available at\nhttps://github.com/UITron-hub/UITron-Speech."}
{"id": "2506.14448", "pdf": "https://arxiv.org/pdf/2506.14448.pdf", "abs": "https://arxiv.org/abs/2506.14448", "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks."}
{"id": "2507.04642", "pdf": "https://arxiv.org/pdf/2507.04642.pdf", "abs": "https://arxiv.org/abs/2507.04642", "title": "R1-RE: Cross-Domain Relation Extraction with RLVR", "authors": ["Runpeng Dai", "Tong Zheng", "Run Yang", "Kaixian Yu", "Hongtu Zhu"], "categories": ["cs.CL"], "comment": "14 pages, 7 figures", "summary": "Relation extraction (RE) is a core task in natural language processing.\nTraditional approaches typically frame RE as a supervised learning problem,\ndirectly mapping context to labels-an approach that often suffers from poor\nout-of-domain (OOD) generalization. Inspired by the workflow of human\nannotators, we reframe RE as a reasoning task guided by annotation guidelines\nand introduce R1-RE, the first reinforcement learning with verifiable reward\n(RLVR) framework for RE tasks. Our method elicits the reasoning abilities of\nsmall language models for annotation tasks, resulting in significantly improved\nOOD robustness. We evaluate our approach on the public Sem-2010 dataset and a\nprivate MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of\napproximately 70%, on par with leading proprietary models such as GPT-4o.\nAdditionally, our comprehensive analysis provides novel insights into the\ntraining dynamics and emergent reasoning behaviors of the RLVR paradigm for RE."}
{"id": "2507.15715", "pdf": "https://arxiv.org/pdf/2507.15715.pdf", "abs": "https://arxiv.org/abs/2507.15715", "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana Ciuc", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "categories": ["cs.CL", "astro-ph.IM"], "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research."}
{"id": "2507.19407", "pdf": "https://arxiv.org/pdf/2507.19407.pdf", "abs": "https://arxiv.org/abs/2507.19407", "title": "Towards Domain Specification of Embedding Models in Medicine", "authors": ["Mohammad Khodadad", "Ali Shiraee Kasmaee", "Mahdi Astaraki", "Hamidreza Mahyar"], "categories": ["cs.CL"], "comment": null, "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks."}
{"id": "2507.22716", "pdf": "https://arxiv.org/pdf/2507.22716.pdf", "abs": "https://arxiv.org/abs/2507.22716", "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "authors": ["Jie He", "Victor Gutirrez-Basulto", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."}
{"id": "2508.00370", "pdf": "https://arxiv.org/pdf/2508.00370.pdf", "abs": "https://arxiv.org/abs/2508.00370", "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "categories": ["cs.CL", "cs.LG"], "comment": "The data and method in the paper need to be re-audited", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."}
{"id": "2508.01317", "pdf": "https://arxiv.org/pdf/2508.01317.pdf", "abs": "https://arxiv.org/abs/2508.01317", "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales."}
{"id": "2508.02038", "pdf": "https://arxiv.org/pdf/2508.02038.pdf", "abs": "https://arxiv.org/abs/2508.02038", "title": "Marco-Voice Technical Report", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Technical Report. Our code and dataset are publicly available at\n  https://github.com/AIDC-AI/Marco-Voice and\n  https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis. Our code and\ndataset are publicly available at https://github.com/AIDC-AI/Marco-Voice and\nhttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively."}
{"id": "2508.02591", "pdf": "https://arxiv.org/pdf/2508.02591.pdf", "abs": "https://arxiv.org/abs/2508.02591", "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "authors": ["Omri Uzan", "Yuval Pinter"], "categories": ["cs.CL"], "comment": null, "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks."}
{"id": "2508.02997", "pdf": "https://arxiv.org/pdf/2508.02997.pdf", "abs": "https://arxiv.org/abs/2508.02997", "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available."}
{"id": "2508.03363", "pdf": "https://arxiv.org/pdf/2508.03363.pdf", "abs": "https://arxiv.org/abs/2508.03363", "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs."}
{"id": "2508.03440", "pdf": "https://arxiv.org/pdf/2508.03440.pdf", "abs": "https://arxiv.org/abs/2508.03440", "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "authors": ["Chnhung Wu", "Jinliang Lu", "Zixuan Ren", "Gangqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks."}
{"id": "2403.04618", "pdf": "https://arxiv.org/pdf/2403.04618.pdf", "abs": "https://arxiv.org/abs/2403.04618", "title": "Strong Priority and Determinacy in Timed CCS", "authors": ["Luigi Liquori", "Michael Mendler"], "categories": ["cs.PL", "cs.CL"], "comment": "Change Notes (06.08.25): Streamlined the definition of coherence and\n  non-interference; Corrections in Def.~14 for coherence, adding condition on\n  residual transitions; Adjusted coding of Esterel signals (Ex.~11) to match\n  adjusted Def.~14; To reflect changed Def.~14, use the term \"c-coherence'';\n  Minor rewrite of Sec.~2.3 and Sec.~4; Further corrections and revisions in\n  Appendices", "summary": "Building on the standard theory of process algebra with priorities, we\nidentify a new scheduling mechanism, called \"constructive reduction\" which is\ndesigned to capture the essence of synchronous programming. The distinctive\nproperty of this evaluation strategy is to achieve determinacy-by-construction\nfor multi-cast concurrent communication with shared memory. In the technical\nsetting of CCS extended by clocks and priorities, we prove for a large class of\n\"coherent\" processes a confluence property for constructive reductions. We show\nthat under some restrictions, called \"pivotability\", coherence is preserved by\nthe operators of prefix, summation, parallel composition, restriction and\nhiding. Since this permits memory and sharing, we are able to cover a strictly\nlarger class of processes compared to those in Milner's classical confluence\ntheory for CCS without priorities."}
{"id": "2405.15877", "pdf": "https://arxiv.org/pdf/2405.15877.pdf", "abs": "https://arxiv.org/abs/2405.15877", "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications", "authors": ["Yang Li", "Daniel Agyei Asante", "Changsheng Zhao", "Ernie Chang", "Yangyang Shi", "Vikas Chandra"], "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques."}
{"id": "2410.02745", "pdf": "https://arxiv.org/pdf/2410.02745.pdf", "abs": "https://arxiv.org/abs/2410.02745", "title": "AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity", "authors": ["Zhibin Lan", "Liqiang Niu", "Fandong Meng", "Wenbo Li", "Jie Zhou", "Jinsong Su"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Recently, large multimodal models (LMMs) have achieved significant\nadvancements. When dealing with high-resolution images, dominant LMMs typically\ndivide them into multiple local images and a global image, leading to a large\nnumber of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can\nadaptively select the appropriate visual granularity based on the input image\nand instruction. Specifically, we first apply the multiple pooling layers to\nobtain visual tokens at different granularities. Then we propose a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we put forward RGLF, a novel training\nparadigm that aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark)."}
{"id": "2410.09908", "pdf": "https://arxiv.org/pdf/2410.09908.pdf", "abs": "https://arxiv.org/abs/2410.09908", "title": "Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via Sparse Task Projection", "authors": ["Pengfei Jin", "Peng Shu", "Sifan Song", "Sekeun Kim", "Qing Xiao", "Cheng Chen", "Tianming Liu", "Xiang Li", "Quanzheng Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in parameter-efficient transfer learning have demonstrated\nthe utility of composing LoRA adapters from libraries of pretrained modules.\nHowever, most existing approaches rely on simple retrieval heuristics or\nuniform averaging, which overlook the latent structure of task relationships in\nrepresentation space. We propose a new framework for adapter reuse that moves\nbeyond retrieval, formulating adapter composition as a geometry-aware sparse\nreconstruction problem. Specifically, we represent each task by a latent\nprototype vector derived from the base model's encoder and aim to approximate\nthe target task prototype as a sparse linear combination of retrieved reference\nprototypes, under an $\\ell_1$-regularized optimization objective. The resulting\ncombination weights are then used to blend the corresponding LoRA adapters,\nyielding a composite adapter tailored to the target task. This formulation not\nonly preserves the local geometric structure of the task representation\nmanifold, but also promotes interpretability and efficient reuse by selecting a\nminimal set of relevant adapters. We demonstrate the effectiveness of our\napproach across multiple domains-including medical image segmentation, medical\nreport generation and image synthesis. Our results highlight the benefit of\ncoupling retrieval with latent geometry-aware optimization for improved\nzero-shot generalization."}
{"id": "2410.13928", "pdf": "https://arxiv.org/pdf/2410.13928.pdf", "abs": "https://arxiv.org/abs/2410.13928", "title": "Automatically Interpreting Millions of Features in Large Language Models", "authors": ["Gonalo Paulo", "Alex Mallen", "Caden Juang", "Nora Belrose"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations."}
{"id": "2412.04449", "pdf": "https://arxiv.org/pdf/2412.04449.pdf", "abs": "https://arxiv.org/abs/2412.04449", "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay", "authors": ["Jun Zhang", "Desen Meng", "Zhengming Zhang", "Zhenpeng Huang", "Tao Wu", "Limin Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD", "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."}
{"id": "2501.06663", "pdf": "https://arxiv.org/pdf/2501.06663.pdf", "abs": "https://arxiv.org/abs/2501.06663", "title": "Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization", "authors": ["Jiayi Tian", "Jinming Lu", "Hai Li", "Xiangwei Wang", "Cong Hao", "Ian Young", "Zheng Zhang"], "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Transformer models have achieved state-of-the-art performance across a wide\nrange of machine learning tasks. There is growing interest in training\ntransformers on resource-constrained edge devices due to considerations such as\nprivacy, domain adaptation, and on-device scientific machine learning. However,\nthe significant computational and memory demands required for transformer\ntraining often exceed the capabilities of an edge device. Leveraging low-rank\ntensor compression, this paper presents the first on-FPGA accelerator for\nend-to-end transformer training. On the algorithm side, we present a\nbi-directional contraction flow for tensorized transformer training,\nsignificantly reducing the computational FLOPS and intra-layer memory costs\ncompared to existing tensor operations. On the hardware side, we store all\nhighly compressed model parameters and gradient information on chip, creating\nan on-chip-memory-only framework for each stage in training. This reduces\noff-chip communication and minimizes latency and energy costs. Additionally, we\nimplement custom computing kernels for each training stage and employ\nintra-layer parallelism and pipe-lining to further enhance run-time and memory\nefficiency. Through experiments on transformer models within $36.7$ to $93.5$\nMB using FP-32 data formats on the ATIS dataset, our tensorized FPGA\naccelerator could conduct single-batch end-to-end training on the AMD Alevo U50\nFPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM.\nCompared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA\ntraining achieves a memory reduction of $30\\times$ to $51\\times$. Our FPGA\naccelerator also achieves up to $3.6\\times$ less energy cost per epoch compared\nwith tensor Transformer training on an NVIDIA RTX 3090 GPU."}
{"id": "2502.01083", "pdf": "https://arxiv.org/pdf/2502.01083.pdf", "abs": "https://arxiv.org/abs/2502.01083", "title": "Tool Unlearning for Tool-Augmented LLMs", "authors": ["Jiali Cheng", "Hadi Amiri"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 https://clu-uml.github.io/MU-Bench-Project-Page/", "summary": "Tool-augmented large language models (LLMs) are often trained on datasets of\nquery-response pairs, which embed the ability to use tools or APIs directly\ninto the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to\nforget learned tools due to security vulnerabilities, privacy regulations, or\ntool deprecations. However, ``tool unlearning'' has not been investigated in\nunlearning literature. We introduce this novel task, which requires addressing\ndistinct challenges compared to traditional unlearning: knowledge removal\nrather than forgetting individual samples, the high cost of optimizing LLMs,\nand the need for principled evaluation metrics. To bridge these gaps, we\npropose ToolDelete, the first approach for unlearning tools from tool-augmented\nLLMs. It implements three key properties to address the above challenges for\neffective tool unlearning and introduces a new membership inference attack\n(MIA) model for effective evaluation. Extensive experiments on multiple tool\nlearning datasets and tool-augmented LLMs show that ToolDelete effectively\nunlearns randomly selected tools, while preserving the LLM's knowledge on\nnon-deleted tools and maintaining performance on general tasks."}
{"id": "2503.18892", "pdf": "https://arxiv.org/pdf/2503.18892.pdf", "abs": "https://arxiv.org/abs/2503.18892", "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools."}
{"id": "2505.16888", "pdf": "https://arxiv.org/pdf/2505.16888.pdf", "abs": "https://arxiv.org/abs/2505.16888", "title": "CAIN: Hijacking LLM-Humans Conversations via Malicious System Prompts", "authors": ["Viet Pham", "Thai Le"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382.pdf", "abs": "https://arxiv.org/abs/2506.06382", "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "authors": ["Micha P. Karpowicz"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": "cleared mathematics, proofs and ideas explained, added missing\n  definitions and axioms, discussion and speculation section added", "summary": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory."}
{"id": "2506.15787", "pdf": "https://arxiv.org/pdf/2506.15787.pdf", "abs": "https://arxiv.org/abs/2506.15787", "title": "SLR: Automated Synthesis for Scalable Logical Reasoning", "authors": ["Lukas Helff", "Ahmad Omar", "Felix Friedrich", "Antonia Wst", "Hikaru Shindo", "Rupert Mitchell", "Tim Woydt", "Patrick Schramowski", "Wolfgang Stammer", "Kristian Kersting"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning."}
{"id": "2506.16402", "pdf": "https://arxiv.org/pdf/2506.16402.pdf", "abs": "https://arxiv.org/abs/2506.16402", "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks", "authors": ["Xiaoya Lu", "Zeren Chen", "Xuhao Hu", "Yijin Zhou", "Weichen Zhang", "Dongrui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Flawed planning from VLM-driven embodied agents poses significant safety\nhazards, hindering their deployment in real-world household tasks. However,\nexisting static, non-interactive evaluation paradigms fail to adequately assess\nrisks within these interactive environments, since they cannot simulate dynamic\nrisks that emerge from an agent's actions and rely on unreliable post-hoc\nevaluations that ignore unsafe intermediate steps. To bridge this critical gap,\nwe propose evaluating an agent's interactive safety: its ability to perceive\nemergent risks and execute mitigation steps in the correct procedural order. We\nthus present IS-Bench, the first multi-modal benchmark designed for interactive\nsafety, featuring 161 challenging scenarios with 388 unique safety risks\ninstantiated in a high-fidelity simulator. Crucially, it facilitates a novel\nprocess-oriented evaluation that verifies whether risk mitigation actions are\nperformed before/after specific risk-prone steps. Extensive experiments on\nleading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current\nagents lack interactive safety awareness, and that while safety-aware\nChain-of-Thought can improve performance, it often compromises task completion.\nBy highlighting these critical limitations, IS-Bench provides a foundation for\ndeveloping safer and more reliable embodied AI systems. Code and data are\nreleased under [this https URL](https://github.com/AI45Lab/IS-Bench)."}
{"id": "2506.19143", "pdf": "https://arxiv.org/pdf/2506.19143.pdf", "abs": "https://arxiv.org/abs/2506.19143", "title": "Thought Anchors: Which LLM Reasoning Steps Matter?", "authors": ["Paul C. Bogdan", "Uzay Macar", "Neel Nanda", "Arthur Conmy"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Paul C. Bogdan and Uzay Macar contributed equally to this work, and\n  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy\n  contributed equally to this work as senior authors, and their listed order\n  was determined by coinflip", "summary": "Reasoning large language models have recently achieved state-of-the-art\nperformance in many fields. However, their long-form chain-of-thought reasoning\ncreates interpretability challenges as each generated token depends on all\nprevious ones, making the computation harder to decompose. We argue that\nanalyzing reasoning traces at the sentence level is a promising approach to\nunderstanding reasoning processes. We present three complementary attribution\nmethods: (1) a black-box method measuring each sentence's counterfactual\nimportance by comparing final answers across 100 rollouts conditioned on the\nmodel generating that sentence or one with a different meaning; (2) a white-box\nmethod of aggregating attention patterns between pairs of sentences, which\nidentified \"broadcasting\" sentences that receive disproportionate attention\nfrom all future sentences via \"receiver\" attention heads; (3) a causal\nattribution method measuring logical connections between sentences by\nsuppressing attention toward one sentence and measuring the effect on each\nfuture sentence's tokens. Each method provides evidence for the existence of\nthought anchors, reasoning steps that have outsized importance and that\ndisproportionately influence the subsequent reasoning process. These thought\nanchors are typically planning or backtracking sentences. We provide an\nopen-source tool (www.thought-anchors.com) for visualizing the outputs of our\nmethods, and present a case study showing converging patterns across methods\nthat map how a model performs multi-step reasoning. The consistency across\nmethods demonstrates the potential of sentence-level analysis for a deeper\nunderstanding of reasoning models."}
{"id": "2507.03958", "pdf": "https://arxiv.org/pdf/2507.03958.pdf", "abs": "https://arxiv.org/abs/2507.03958", "title": "A Comparative Study of Specialized LLMs as Dense Retrievers", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by CCIR25 and published by Springer LNCS or LNAI", "summary": "While large language models (LLMs) are increasingly deployed as dense\nretrievers, the impact of their domain-specific specialization on retrieval\neffectiveness remains underexplored. This investigation systematically examines\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\nan essential step toward developing unified retrievers capable of handling\ntext, code, images, and multimodal content. We conduct extensive experiments\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\ncode/math-specialized, long reasoning, and vision-language models across\nzero-shot retrieval settings and the supervised setting. For the zero-shot\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\nspecialization and the long reasoning capability cause consistent degradation\nin three settings, indicating conflicts between mathematical reasoning and\nsemantic matching. The vision-language model and code-specialized LLMs\ndemonstrate superior zero-shot performance compared to other LLMs, even\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\nto base LLMs in supervised settings. These findings suggest promising\ndirections for the unified retrieval task leveraging cross-domain and\ncross-modal fusion."}
{"id": "2507.05727", "pdf": "https://arxiv.org/pdf/2507.05727.pdf", "abs": "https://arxiv.org/abs/2507.05727", "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "16 pages, 4 figures", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior benchmarks have largely focused on assessing the acoustic robustness of\nASR models, leaving evaluations of their linguistic capabilities relatively\nunderexplored. This largely stems from the limited parameter sizes and training\ncorpora of conventional ASR models, leaving them with insufficient world\nknowledge, which is crucial for accurately recognizing named entities across\ndiverse domains. For instance, drug and treatment names in medicine or\nspecialized technical terms in engineering. Recent breakthroughs in Large\nLanguage Models (LLMs) and corresponding Large Audio Language Models (LALMs)\nhave markedly enhanced the visibility of advanced context modeling and general\nartificial intelligence capabilities. Leveraging LLMs, we envision a unified\nsystem capable of robust speech recognition across diverse real-world domains,\nyet existing benchmarks are inadequate for evaluating this objective. To\naddress this gap, we propose ContextASR-Bench: a comprehensive, large-scale\nbenchmark designed to assess the linguistic competence of ASR systems using\ncorpora that feature numerous named entities across multiple domains. It\nencompasses up to 40,000 data entries with more than 300,000 named entities\nacross over 10 domains. Beyond the audio and its transcription, each sample\nprovides the domain it belongs to and a list of named entities it contains,\nwhich are referred to as the context. Based on this, we introduce three\nevaluation modes to assess how effectively models can exploit such context to\nimprove ASR accuracy. Extensive evaluation on ContextASR-Bench highlights that\nLALMs outperform conventional ASR models by a large margin thanks to the strong\nworld knowledge and context modeling of LLMs, yet there remains ample room for\nfurther improvement. The dataset and evaluation code have been released."}
{"id": "2507.17937", "pdf": "https://arxiv.org/pdf/2507.17937.pdf", "abs": "https://arxiv.org/abs/2507.17937", "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Memorization in generative models extends far beyond verbatim text\nreproduction--it manifests through non-literal patterns, semantic associations,\nand surprisingly, across modalities in transcript-conditioned generation tasks\nsuch as Lyrics-to-Song (L2S) and Text-to-Video (T2V) models. We reveal a new\nclass of cross-modality memorization where models trained on these tasks leak\ncopyrighted content through indirect, phonetic pathways invisible to\ntraditional text-based analysis. In this work, we introduce Adversarial\nPhoneTic Prompting (APT), an attack that replaces iconic phrases with\nhomophonic alternatives--e.g., \"mom's spaghetti\" becomes \"Bob's\nconfetti\"--preserving the acoustic form while largely changing semantic\ncontent. We demonstrate that models can be prompted to regurgitate memorized\nsongs using phonetically similar but semantically unrelated lyrics. Despite the\nsemantic drift, black-box models like SUNO and open-source models like YuE\ngenerate outputs that are strikingly similar to the original\nsongs--melodically, rhythmically, and vocally--achieving high scores on\nAudioJudge, CLAP, and CoverID. These effects persist across genres and\nlanguages. More surprisingly, we find that phonetic prompts alone can trigger\nvisual memorization in text-to-video models: when given altered lyrics from\nLose Yourself, Veo 3 generates scenes that mirror the original music\nvideo--complete with a hooded rapper and dim urban settings--despite no\nexplicit visual cues in the prompt. This cross-modality leakage represents an\nunprecedented threat: models memorize deep, structural patterns that transcend\ntheir training modality, making traditional safety measures like copyright\nfilters ineffective. Our findings reveal a fundamental vulnerability in\ntranscript-conditioned generative models and raise urgent concerns around\ncopyright, provenance, and secure deployment of multimodal generation systems."}
{"id": "2507.20096", "pdf": "https://arxiv.org/pdf/2507.20096.pdf", "abs": "https://arxiv.org/abs/2507.20096", "title": "EcoTransformer: Attention without Multiplication", "authors": ["Xin Gao", "Xingming Xu", "Shirin Amiraslani", "Hong Xu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05"], "comment": "8 pages, 1 figure", "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy.\n  (This version (v2) supersedes v1 and reflects the intended release and\nlicensing.)"}
{"id": "2508.00222", "pdf": "https://arxiv.org/pdf/2508.00222.pdf", "abs": "https://arxiv.org/abs/2508.00222", "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem."}
{"id": "2508.02629", "pdf": "https://arxiv.org/pdf/2508.02629.pdf", "abs": "https://arxiv.org/abs/2508.02629", "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents", "authors": ["Yibin Liu", "Zhixuan Liang", "Zanxin Chen", "Tianxing Chen", "Mengkang Hu", "Wanxi Dong", "Congsheng Xu", "Zhaoming Han", "Yusen Qin", "Yao Mu"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines."}
{"id": "2508.03306", "pdf": "https://arxiv.org/pdf/2508.03306.pdf", "abs": "https://arxiv.org/abs/2508.03306", "title": "Reliable Evaluation Protocol for Low-Precision Retrieval", "authors": ["Kisu Yang", "Yoonna Jang", "Hwanseok Jang", "Kenneth Choi", "Isabelle Augenstein", "Heuiseok Lim"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "11 pages, 5 figures, submitted to ARR", "summary": "Lowering the numerical precision of model parameters and computations is\nwidely adopted to improve the efficiency of retrieval systems. However, when\ncomputing relevance scores between the query and documents in low-precision, we\nobserve spurious ties due to the reduced granularity. This introduces high\nvariability in the results based on tie resolution, making the evaluation less\nreliable. To address this, we propose a more robust retrieval evaluation\nprotocol designed to reduce score variation. It consists of: (1) High-Precision\nScoring (HPS), which upcasts the final scoring step to higher precision to\nresolve tied candidates with minimal computational cost; and (2) Tie-aware\nRetrieval Metrics (TRM), which report expected scores, range, and bias to\nquantify order uncertainty of tied candidates. Our experiments test multiple\nmodels with three scoring functions on two retrieval datasets to demonstrate\nthat HPS dramatically reduces tie-induced instability, and TRM accurately\nrecovers expected metric values. This combination enables a more consistent and\nreliable evaluation system for lower-precision retrievals."}
