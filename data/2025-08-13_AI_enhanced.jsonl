{"id": "2508.08383", "pdf": "https://arxiv.org/pdf/2508.08383.pdf", "abs": "https://arxiv.org/abs/2508.08383", "title": "Designing for Disclosure in Data Visualizations", "authors": ["Krisha Mehta", "Gordon Kindlmann", "Alex Kale"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures", "summary": "Visualizing data often entails data transformations that can reveal and hide\ninformation, operations we dub disclosure tactics. Whether designers hide\ninformation intentionally or as an implicit consequence of other design\nchoices, tools and frameworks for visualization offer little explicit guidance\non disclosure. To systematically characterize how visualizations can limit\naccess to an underlying dataset, we contribute a content analysis of 425\nexamples of visualization techniques sampled from academic papers in the\nvisualization literature, resulting in a taxonomy of disclosure tactics. Our\ntaxonomy organizes disclosure tactics based on how they change the data\nrepresentation underlying a chart, providing a systematic way to reason about\ndesign trade-offs in terms of what information is revealed, distorted, or\nhidden. We demonstrate the benefits of using our taxonomy by showing how it can\nguide reasoning in design scenarios where disclosure is a first-order\nconsideration. Adopting disclosure as a framework for visualization research\noffers new perspective on authoring tools, literacy, uncertainty communication,\npersonalization, and ethical design.", "AI": {"tldr": "The paper presents a taxonomy of disclosure tactics in data visualization derived from an analysis of 425 visualization techniques, highlighting how design choices affect data representation and access.", "motivation": "To address the lack of explicit guidance on how data transformations in visualizations can hide or reveal information, which impacts understanding.", "method": "Content analysis of 425 visualization examples from academic literature to develop a taxonomy that categorizes disclosure tactics based on their effects on data representation.", "result": "The analysis resulted in a systematic taxonomy that aids designers in reasoning about trade-offs in information disclosure in visualization contexts.", "conclusion": "Framing disclosure as a critical aspect of visualization research introduces new insights into numerous areas including authoring tools, ethical design, and communication of uncertainty.", "key_contributions": ["Development of a taxonomy of disclosure tactics in data visualizations.", "A systematic way to evaluate design trade-offs related to information access.", "Guidance for various design scenarios where disclosure is crucial."], "limitations": "", "keywords": ["data visualization", "disclosure tactics", "taxonomy", "design trade-offs", "ethical design"], "importance_score": 6, "read_time_minutes": 11}}
{"id": "2508.08467", "pdf": "https://arxiv.org/pdf/2508.08467.pdf", "abs": "https://arxiv.org/abs/2508.08467", "title": "Empowering Children to Create AI-Enabled Augmented Reality Experiences", "authors": ["Lei Zhang", "Shuyao Zhou", "Amna Liaqat", "Tinney Mak", "Brian Berengard", "Emily Qian", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.PL"], "comment": "Accepted to ACM UIST 2025", "summary": "Despite their potential to enhance children's learning experiences,\nAI-enabled AR technologies are predominantly used in ways that position\nchildren as consumers rather than creators. We introduce Capybara, an AR-based\nand AI-powered visual programming environment that empowers children to create,\ncustomize, and program 3D characters overlaid onto the physical world. Capybara\nenables children to create virtual characters and accessories using text-to-3D\ngenerative AI models, and to animate these characters through auto-rigging and\nbody tracking. In addition, our system employs vision-based AI models to\nrecognize physical objects, allowing children to program interactive behaviors\nbetween virtual characters and their physical surroundings. We demonstrate the\nexpressiveness of Capybara through a set of novel AR experiences. We conducted\nuser studies with 20 children in the United States and Argentina. Our findings\nsuggest that Capybara can empower children to harness AI in authoring\npersonalized and engaging AR experiences that seamlessly bridge the virtual and\nphysical worlds.", "AI": {"tldr": "Capybara is an AR-based visual programming environment that enables children to create and animate 3D characters using AI technologies, promoting creative engagement with AR tools.", "motivation": "To shift the usage of AI-enabled AR technologies from positioning children as consumers to empowering them as creators.", "method": "Capybara integrates AR and AI to allow children to create 3D characters using text-to-3D models, animate them with auto-rigging and body tracking, and interact with physical objects through vision-based AI recognition.", "result": "User studies with 20 children showed that Capybara effectively empowers children to create personalized and interactive AR experiences that link virtual and physical worlds.", "conclusion": "Capybara fosters creative learning by enabling children to actively engage in the creation of AR environments, utilizing AI tools to enhance their learning experience.", "key_contributions": ["Introduction of Capybara, an AR and AI-powered environment for child creators", "Implementation of text-to-3D generative AI for character customization", "Integration of vision-based AI for interactive programming with physical objects."], "limitations": "", "keywords": ["AI", "Augmented Reality", "Child Learning", "Visual Programming", "Generative Models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.08502", "pdf": "https://arxiv.org/pdf/2508.08502.pdf", "abs": "https://arxiv.org/abs/2508.08502", "title": "AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns", "authors": ["Marta Robledo-Moreno", "Ruben Vera-Rodriguez", "Ruben Tolosana", "Javier Ortega-Garcia", "Andres Huergo", "Julian Fierrez"], "categories": ["cs.HC"], "comment": null, "summary": "Behavioral biometrics based on smartphone motion sensors are growing in\npopularity for authentication purposes. In this study, AirSignatureDB is\npresented: a new publicly accessible dataset of in-air signatures collected\nfrom 108 participants under real-world conditions, using 83 different\nsmartphone models across four sessions. This dataset includes genuine samples\nand skilled forgeries, enabling a comprehensive evaluation of system robustness\nagainst realistic attack scenarios. Traditional and deep learning-based methods\nfor in-air signature verification are benchmarked, while analyzing the\ninfluence of sensor modality and enrollment strategies. Beyond verification, a\nfirst approach to reconstructing the three-dimensional trajectory of in-air\nsignatures from inertial sensor data alone is introduced. Using on-line\nhandwritten signatures as a reference, we demonstrate that the recovery of\naccurate trajectories is feasible, challenging the long-held assumption that\nin-air gestures are inherently traceless. Although this approach enables\nforensic traceability, it also raises critical questions about the privacy\nboundaries of behavioral biometrics. Our findings underscore the need for a\nreevaluation of the privacy assumptions surrounding inertial sensor data, as\nthey can reveal user-specific information that had not previously been\nconsidered in the design of in-air signature systems.", "AI": {"tldr": "This study presents AirSignatureDB, a dataset for verifying in-air signatures using smartphone motion sensors, and explores the implications for privacy.", "motivation": "The popularity of behavioral biometrics for authentication necessitates reliable datasets and methods for evaluation, particularly in real-world scenarios.", "method": "A dataset of in-air signatures from 108 participants was collected across various smartphone models. The study benchmarks traditional and deep learning methods for signature verification and reconstructs three-dimensional trajectories from sensor data.", "result": "The research shows that accurate reconstruction of in-air signatures is possible, challenging previous assumptions about their traceless nature and demonstrating vulnerabilities in privacy.", "conclusion": "The findings call for a reevaluation of privacy considerations in behavioral biometrics using inertial sensor data, as it can expose user-specific information.", "key_contributions": ["Introduction of the AirSignatureDB dataset", "Benchmarking of both traditional and deep learning methods for in-air signature verification", "Development of a method for reconstructing 3D trajectories from inertial sensors"], "limitations": "The reconstruction method's implications on user privacy are complex and require further exploration.", "keywords": ["behavioral biometrics", "in-air signatures", "privacy", "sensor data", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.08505", "pdf": "https://arxiv.org/pdf/2508.08505.pdf", "abs": "https://arxiv.org/abs/2508.08505", "title": "Adaptique: Multi-objective and Context-aware Online Adaptation of Selection Techniques in Virtual Reality", "authors": ["Chao-Jung Lai", "Mauricio Sousa", "Tianyu Zhang", "Ludwig Sidenmark", "Tovi Grossman"], "categories": ["cs.HC"], "comment": "16 pages, 7 figures, to appear in Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST '25), September\n  28-October 1, 2025, Busan, Republic of Korea", "summary": "Selection is a fundamental task that is challenging in virtual reality due to\nissues such as distant and small targets, occlusion, and target-dense\nenvironments. Previous research has tackled these challenges through various\nselection techniques, but complicates selection and can be seen as tedious\noutside of their designed use case. We present Adaptique, an adaptive model\nthat infers and switches to the most optimal selection technique based on user\nand environmental information. Adaptique considers contextual information such\nas target size, distance, occlusion, and user posture combined with four\nobjectives: speed, accuracy, comfort, and familiarity which are based on\nfundamental predictive models of human movement for technique selection. This\nenables Adaptique to select simple techniques when they are sufficiently\nefficient and more advanced techniques when necessary. We show that Adaptique\nis more preferred and performant than single techniques in a user study, and\ndemonstrate Adaptique's versatility in an application.", "AI": {"tldr": "Adaptique is an adaptive selection model for virtual reality that optimizes selection techniques based on user context and environmental factors.", "motivation": "Address the challenges of selection in virtual reality, such as distant targets, occlusion, and target density, which complicate user interaction.", "method": "Adaptique infers optimal selection techniques by analyzing user and environmental information, focusing on target size, distance, occlusion, and user posture, while balancing speed, accuracy, comfort, and familiarity.", "result": "User studies indicate that Adaptique outperforms traditional single selection techniques in both preference and performance.", "conclusion": "Adaptique demonstrates improved versatility and effectiveness in selection tasks within virtual reality environments.", "key_contributions": ["Development of Adaptique, an adaptive selection model for VR.", "Incorporation of contextual information for technique selection.", "Demonstration of improved user preference and performance in studies."], "limitations": "", "keywords": ["virtual reality", "adaptive selection", "user interface", "human movement", "interaction techniques"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08262", "pdf": "https://arxiv.org/pdf/2508.08262.pdf", "abs": "https://arxiv.org/abs/2508.08262", "title": "Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models", "authors": ["Alaa Alhamzeh", "Mays Al Rebdawi"], "categories": ["cs.CL", "68T50", "I.2.7; H.3.1"], "comment": "8 pages, 4 figures, Passau uni, Master thesis in NLP", "summary": "Financial arguments play a critical role in shaping investment decisions and\npublic trust in financial institutions. Nevertheless, assessing their quality\nremains poorly studied in the literature. In this paper, we examine the\ncapabilities of three state-of-the-art LLMs GPT-4o, Llama 3.1, and Gemma 2 in\nannotating argument quality within financial communications, using the\nFinArgQuality dataset. Our contributions are twofold. First, we evaluate the\nconsistency of LLM-generated annotations across multiple runs and benchmark\nthem against human annotations. Second, we introduce an adversarial attack\ndesigned to inject gender bias to analyse models responds and ensure model's\nfairness and robustness. Both experiments are conducted across three\ntemperature settings to assess their influence on annotation stability and\nalignment with human labels. Our findings reveal that LLM-based annotations\nachieve higher inter-annotator agreement than human counterparts, though the\nmodels still exhibit varying degrees of gender bias. We provide a multifaceted\nanalysis of these outcomes and offer practical recommendations to guide future\nresearch toward more reliable, cost-effective, and bias-aware annotation\nmethodologies.", "AI": {"tldr": "This paper evaluates the argument quality in financial communications using state-of-the-art LLMs and introduces an adversarial attack to assess gender bias.", "motivation": "The quality of financial arguments influences investment decisions and public trust, yet their assessment is underexplored.", "method": "The study assesses the LLMs (GPT-4o, Llama 3.1, Gemma 2) by annotating financial communications with the FinArgQuality dataset and compares LLM-generated annotations to human annotations across different temperature settings.", "result": "LLM-generated annotations showed higher inter-annotator agreement compared to human annotations, though they still displayed varying levels of gender bias.", "conclusion": "The findings suggest that while LLMs can provide reliable annotations, there is a need for caution regarding bias, leading to recommendations for future research in annotation methodologies.", "key_contributions": ["Evaluation of LLM-generated annotations against human benchmarks", "Introduction of an adversarial attack to explore gender bias", "Analysis of temperature settings on annotation stability"], "limitations": "The extent of gender bias in LLMs needs further investigation; models may still lack robustness in biased scenarios.", "keywords": ["Financial Communications", "Argument Quality", "Gender Bias", "LLMs", "Annotation Methodologies"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.08524", "pdf": "https://arxiv.org/pdf/2508.08524.pdf", "abs": "https://arxiv.org/abs/2508.08524", "title": "StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI", "authors": ["Jon E. Froehlich", "Alexander Fiannaca", "Nimer Jaber", "Victor Tsara", "Shaun Kane"], "categories": ["cs.HC", "cs.AI", "H.5; I.2"], "comment": "Accepted to UIST'25", "summary": "Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work.", "AI": {"tldr": "StreetViewAI is an accessible street view tool designed for blind users, integrating multimodal AI and conversational speech for virtual navigation.", "motivation": "The motivation behind this research is to create an inclusive tool that allows blind users to interact with maps and navigate real-world environments that are typically inaccessible through standard street view applications.", "method": "The authors iteratively designed StreetViewAI with input from a mixed-visual ability team and conducted evaluations with blind users to assess usability and functionality.", "result": "The evaluation with eleven blind users revealed that StreetViewAI effectively supports point of interest (POI) investigations and remote route planning.", "conclusion": "The study concludes that accessible street view tools are valuable for blind users and outlines guidelines for future enhancements in this area.", "key_contributions": ["Introduction of StreetViewAI, the first accessible street view tool for blind users", "Integration of context-aware, multimodal AI and conversational speech", "Guidelines for future work on making digital maps accessible"], "limitations": "The study is limited by the small sample size of participants for evaluation.", "keywords": ["accessibility", "human-computer interaction", "multimodal AI", "street view", "blind users"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.08265", "pdf": "https://arxiv.org/pdf/2508.08265.pdf", "abs": "https://arxiv.org/abs/2508.08265", "title": "TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection", "authors": ["Tarık Saraç", "Selin Mergen", "Mucahid Kutlu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present our work developed for the scientific web discourse\ndetection task (Task 4a) of CheckThat! 2025. We propose a novel council debate\nmethod that simulates structured academic discussions among multiple large\nlanguage models (LLMs) to identify whether a given tweet contains (i) a\nscientific claim, (ii) a reference to a scientific study, or (iii) mentions of\nscientific entities. We explore three debating methods: i) single debate, where\ntwo LLMs argue for opposing positions while a third acts as a judge; ii) team\ndebate, in which multiple models collaborate within each side of the debate;\nand iii) council debate, where multiple expert models deliberate together to\nreach a consensus, moderated by a chairperson model. We choose council debate\nas our primary model as it outperforms others in the development test set.\nAlthough our proposed method did not rank highly for identifying scientific\nclaims (8th out of 10) or mentions of scientific entities (9th out of 10), it\nranked first in detecting references to scientific studies.", "AI": {"tldr": "This paper presents a novel council debate method using multiple LLMs for detecting scientific discourse on Twitter, achieving top performance in identifying references to scientific studies.", "motivation": "The goal is to enhance the detection of scientific claims, references to studies, and mentions of scientific entities in web discourse.", "method": "The study introduces three debating frameworks: single debate, team debate, and council debate, with the latter being the primary model as it showed superior performance.", "result": "The council debate method excelled in detecting references to scientific studies, while it was less effective for identifying scientific claims and entity mentions, ranking 8th and 9th respectively.", "conclusion": "The council debate method provides a promising approach for enhancing scientific discourse detection but highlights areas for further improvement in other categories.", "key_contributions": ["Development of a council debate framework for LLMs", "Demonstration of effective collaboration among LLMs in discourse detection", "Top performance achieved in reference identification of scientific studies"], "limitations": "Limited ranking performance for scientific claims and entity mentions compared to study references.", "keywords": ["scientific discourse", "large language models", "debate method", "social media analysis", "CheckThat! 2025"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.08554", "pdf": "https://arxiv.org/pdf/2508.08554.pdf", "abs": "https://arxiv.org/abs/2508.08554", "title": "Explore, Listen, Inspect: Supporting Multimodal Interaction with 3D Surface and Point Data Visualizations", "authors": ["Sanchita S. Kamath", "Aziz N. Zeidieh", "JooYoung Seo"], "categories": ["cs.HC"], "comment": null, "summary": "Blind and low-vision (BLV) users remain largely excluded from\nthree-dimensional (3D) surface and point data visualizations due to the\nreliance on visual interaction. Existing approaches inadequately support\nnon-visual access, especially in browser-based environments. This study\nintroduces DIXTRAL, a hosted web-native system, co-designed with BLV\nresearchers to address these gaps through multimodal interaction. Conducted\nwith two blind and one sighted researcher, this study took place over sustained\ndesign sessions. Data were gathered through iterative testing of the prototype,\ncollecting feedback on spatial navigation, sonification, and usability.\nCo-design observations demonstrate that synchronized auditory, visual, and\ntextual feedback, combined with keyboard and gamepad navigation, enhances both\nstructure discovery and orientation. DIXTRAL aims to improve access to 3D\ncontinuous scalar fields for BLV users and inform best practices for creating\ninclusive 3D visualizations.", "AI": {"tldr": "DIXTRAL is a web-native system designed to enhance access to 3D data visualizations for blind and low-vision users through multimodal interaction.", "motivation": "Blind and low-vision users are excluded from 3D data visualizations which rely heavily on visual interactions.", "method": "The study involved co-design with BLV researchers, iterative testing of prototypes, and feedback collection on spatial navigation, sonification, and usability.", "result": "Synchronized auditory, visual, and textual feedback, along with keyboard and gamepad navigation, improved structure discovery and orientation for BLV users.", "conclusion": "DIXTRAL improves access to 3D scalar fields for BLV users and provides insights for inclusive 3D visualization practices.", "key_contributions": ["Introduction of DIXTRAL, a multimodal interaction system for BLV users", "Findings on effective auditory, visual, and textual feedback techniques", "Strategies for improving inclusivity in 3D visualizations"], "limitations": "Study based on a small sample size with limited participant diversity.", "keywords": ["Blind and Low-Vision", "3D Data Visualization", "Multimodal Interaction", "Inclusive Design", "Sonification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08271", "pdf": "https://arxiv.org/pdf/2508.08271.pdf", "abs": "https://arxiv.org/abs/2508.08271", "title": "Heartificial Intelligence: Exploring Empathy in Language Models", "authors": ["Victoria Williams", "Benjamin Rosman"], "categories": ["cs.CL", "cs.HC"], "comment": "21 pages, 5 tables", "summary": "Large language models have become increasingly common, used by millions of\npeople worldwide in both professional and personal contexts. As these models\ncontinue to advance, they are frequently serving as virtual assistants and\ncompanions. In human interactions, effective communication typically involves\ntwo types of empathy: cognitive empathy (understanding others' thoughts and\nemotions) and affective empathy (emotionally sharing others' feelings). In this\nstudy, we investigated both cognitive and affective empathy across several\nsmall (SLMs) and large (LLMs) language models using standardized psychological\ntests. Our results revealed that LLMs consistently outperformed humans -\nincluding psychology students - on cognitive empathy tasks. However, despite\ntheir cognitive strengths, both small and large language models showed\nsignificantly lower affective empathy compared to human participants. These\nfindings highlight rapid advancements in language models' ability to simulate\ncognitive empathy, suggesting strong potential for providing effective virtual\ncompanionship and personalized emotional support. Additionally, their high\ncognitive yet lower affective empathy allows objective and consistent emotional\nsupport without running the risk of emotional fatigue or bias.", "AI": {"tldr": "This study evaluates cognitive and affective empathy in small and large language models, showing LLMs excel at cognitive empathy tasks but lag in affective empathy compared to humans.", "motivation": "To explore how language models can simulate human empathy, particularly in roles as virtual companions.", "method": "Psychological tests were applied to assess cognitive and affective empathy in small (SLMs) and large (LLMs) language models and human participants.", "result": "LLMs outperformed humans, including psychology students, on cognitive empathy tasks but exhibited significantly lower affective empathy.", "conclusion": "The findings indicate LLMs' potential for effective virtual companionship and emotional support, while highlighting their limitations in emotional empathy.", "key_contributions": ["Demonstrated LLMs' superiority in cognitive empathy compared to human participants.", "Highlighted the disparity between cognitive and affective empathy in language models.", "Provided insights into the implications for using LLMs in emotional support roles."], "limitations": "The study mainly focuses on empathy assessment and does not address practical applications of these findings in real-world scenarios.", "keywords": ["language models", "empathy", "human-computer interaction", "virtual companionship", "emotional support"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08582", "pdf": "https://arxiv.org/pdf/2508.08582.pdf", "abs": "https://arxiv.org/abs/2508.08582", "title": "CoSight: Exploring Viewer Contributions to Online Video Accessibility Through Descriptive Commenting", "authors": ["Ruolin wang", "Xingyu Liu", "Biao Wang", "Wayne Zhang", "Ziqian Liao", "Ziwen Li", "Amy Pavel", "Xiang 'Anthony' Chen"], "categories": ["cs.HC", "K.4.2"], "comment": "ACM UIST", "summary": "The rapid growth of online video content has outpaced efforts to make visual\ninformation accessible to blind and low vision (BLV) audiences. While\nprofessional Audio Description (AD) remains the gold standard, it is costly and\ndifficult to scale across the vast volume of online media. In this work, we\nexplore a complementary approach to broaden participation in video\naccessibility: engaging everyday video viewers at their watching and commenting\ntime. We introduce CoSight, a Chrome extension that augments YouTube with\nlightweight, in-situ nudges to support descriptive commenting. Drawing from\nFogg's Behavior Model, CoSight provides visual indicators of accessibility\ngaps, pop-up hints for what to describe, reminders to clarify vague comments,\nand related captions and comments as references. In an exploratory study with\n48 sighted users, CoSight helped integrate accessibility contribution into\nnatural viewing and commenting practices, resulting in 89% of comments\nincluding grounded visual descriptions. Follow-up interviews with four BLV\nviewers and four professional AD writers suggest that while such comments do\nnot match the rigor of professional AD, they can offer complementary value by\nconveying visual context and emotional nuance for understanding the videos.", "AI": {"tldr": "CoSight enhances video accessibility for blind and low vision users by encouraging descriptive comments from sighted viewers on YouTube, resulting in more integrated visual descriptions.", "motivation": "Addressing the accessibility gap in online video content for blind and low vision audiences, where traditional audio description methods are insufficient and unscalable.", "method": "Developed CoSight, a Chrome extension that prompts users with visual indicators and reminders to add descriptions while watching and commenting on YouTube videos.", "result": "In an exploratory study, CoSight led to 89% of comments including grounded visual descriptions. Follow-up interviews indicated that these comments, while not as rigorous as professional audio descriptions, offer valuable visual context.", "conclusion": "CoSight provides a feasible and scalable method to enhance video accessibility through community engagement, contributing to a more inclusive viewing experience.", "key_contributions": ["Introduction of CoSight as a tool for enhancing video accessibility through community involvement.", "Successful integration of accessibility practices into ordinary commenting behavior on YouTube.", "Initial qualitative feedback indicating the value of user-generated descriptions for BLV audiences."], "limitations": "The quality of user-generated comments does not match that of professional audio descriptions and may vary greatly among users.", "keywords": ["video accessibility", "audio description", "YouTube", "community engagement", "blind and low vision"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.08272", "pdf": "https://arxiv.org/pdf/2508.08272.pdf", "abs": "https://arxiv.org/abs/2508.08272", "title": "Real-time News Story Identification", "authors": ["Tadej Škvorc", "Nikola Ivačič", "Sebastjan Hribar", "Marko Robnik-Šikonja"], "categories": ["cs.CL"], "comment": null, "summary": "To improve the reading experience, many news sites organize news into topical\ncollections, called stories. In this work, we present an approach for\nimplementing real-time story identification for a news monitoring system that\nautomatically collects news articles as they appear online and processes them\nin various ways. Story identification aims to assign each news article to a\nspecific story that the article is covering. The process is similar to text\nclustering and topic modeling, but requires that articles be grouped based on\nparticular events, places, and people, rather than general text similarity (as\nin clustering) or general (predefined) topics (as in topic modeling). We\npresent an approach to story identification that is capable of functioning in\nreal time, assigning articles to stories as they are published online. In the\nproposed approach, we combine text representation techniques, clustering\nalgorithms, and online topic modeling methods. We combine various text\nrepresentation methods to extract specific events and named entities necessary\nfor story identification, showing that a mixture of online topic-modeling\napproaches such as BERTopic, DBStream, and TextClust can be adapted for story\ndiscovery. We evaluate our approach on a news dataset from Slovene media\ncovering a period of 1 month. We show that our real-time approach produces\nsensible results as judged by human evaluators.", "AI": {"tldr": "This paper presents a real-time story identification approach for news monitoring systems to categorize articles based on specific events, places, and people instead of general text similarity.", "motivation": "The motivation is to enhance the reading experience on news sites by organizing content into topical collections, thereby facilitating automatic categorization of articles as they are published.", "method": "The approach combines text representation techniques, clustering algorithms, and online topic modeling methods, utilizing methods like BERTopic, DBStream, and TextClust for effective story identification.", "result": "Evaluation on a Slovene media news dataset demonstrates that the real-time story identification approach produces meaningful categorization of news articles.", "conclusion": "The study concludes that the proposed real-time method for story identification is effective and yields results that align well with human judgment.", "key_contributions": ["Real-time story identification method for news articles", "Adaptation of online topic-modeling approaches for story discovery", "Evaluation using a dataset from Slovene media revealing efficacy of the approach"], "limitations": "", "keywords": ["story identification", "news monitoring", "real-time processing", "text clustering", "topic modeling"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.08672", "pdf": "https://arxiv.org/pdf/2508.08672.pdf", "abs": "https://arxiv.org/abs/2508.08672", "title": "Imposing AI: Deceptive design patterns against sustainability", "authors": ["Anaëlle Beignon", "Thomas Thibault", "Nolwenn Maudet"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI is being massively deployed in digital services, at a scale\nthat will result in significant environmental harm. We document how tech\ncompanies are transforming established user interfaces to impose AI use and\nshow how and to what extent these strategies fit within established deceptive\npattern categories. We identify two main design strategies that are implemented\nto impose AI use in both personal and professional contexts: imposing AI\nfeatures in interfaces at the expense of existing non-AI features and promoting\nnarratives about AI that make it harder to resist using it. We discuss\nopportunities for regulating the imposed adoption of AI features, which would\ninevitably lead to negative environmental effects.", "AI": {"tldr": "This paper discusses the environmental impact of generative AI and how tech companies are redesigning user interfaces to compel AI adoption, using deceptive strategies.", "motivation": "To address the environmental consequences of widespread generative AI deployment and how it influences user behavior.", "method": "Analysis of user interface design changes in tech companies, categorizing their strategies as deceptive patterns to encourage AI use.", "result": "Identified two main strategies: replacing existing features with AI ones and promoting narratives that hinder resistance to AI adoption.", "conclusion": "There is a need for regulating the imposition of AI features to mitigate environmental harm.", "key_contributions": ["Identification of deceptive design strategies in AI adoption", "Discussion on environmental implications of generative AI", "Suggestions for regulatory measures on AI feature imposition"], "limitations": "The paper focuses mainly on design strategies without extensive empirical evidence on user response to these changes.", "keywords": ["Generative AI", "User Interface Design", "Deceptive Patterns", "Regulation", "Environmental Impact"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.08273", "pdf": "https://arxiv.org/pdf/2508.08273.pdf", "abs": "https://arxiv.org/abs/2508.08273", "title": "TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning", "authors": ["Kristian Miok", "Blaz Škrlj", "Daniela Zaharie", "Marko Robnik Šikonja"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Clinical language models often struggle to provide trustworthy predictions\nand explanations when applied to lengthy, unstructured electronic health\nrecords (EHRs). This work introduces TT-XAI, a lightweight and effective\nframework that improves both classification performance and interpretability\nthrough domain-aware keyword distillation and reasoning with large language\nmodels (LLMs). First, we demonstrate that distilling raw discharge notes into\nconcise keyword representations significantly enhances BERT classifier\nperformance and improves local explanation fidelity via a focused variant of\nLIME. Second, we generate chain-of-thought clinical explanations using\nkeyword-guided prompts to steer LLMs, producing more concise and clinically\nrelevant reasoning. We evaluate explanation quality using deletion-based\nfidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human\nstudy with domain experts. All evaluation modalities consistently favor the\nkeyword-augmented method, confirming that distillation enhances both machine\nand human interpretability. TT-XAI offers a scalable pathway toward\ntrustworthy, auditable AI in clinical decision support.", "AI": {"tldr": "TT-XAI improves clinical language model interpretability and classification performance using keyword distillation and LLM reasoning.", "motivation": "To address the challenges of trustworthiness and interpretability in clinical language models applied to lengthy electronic health records.", "method": "The method involves distilling raw discharge notes into concise keyword representations and generating chain-of-thought clinical explanations with LLMs, evaluated through various fidelity metrics and human assessments.", "result": "Keyword distillation enhances BERT classifier performance and improves explanation fidelity, as all evaluation methods favored the keyword-augmented approach.", "conclusion": "TT-XAI provides a scalable way to enhance the interpretability and trustworthiness of AI models in clinical decision support.", "key_contributions": ["Introduction of TT-XAI framework", "Use of domain-aware keyword distillation", "Improved interpretability and performance in clinical AI applications"], "limitations": "", "keywords": ["Clinical language models", "Interpretability", "Large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.08731", "pdf": "https://arxiv.org/pdf/2508.08731.pdf", "abs": "https://arxiv.org/abs/2508.08731", "title": "Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context", "authors": ["Mingyuan Zhong", "Ajit Mallavarapu", "Qing Nie"], "categories": ["cs.HC"], "comment": null, "summary": "We present Caption, an LLM-powered content label generation tool for visual\ninteractive elements on mobile devices. Content labels are essential for screen\nreaders to provide announcements for image-based elements, but are often\nmissing or uninformative due to developer neglect. Automated captioning systems\nattempt to address this, but are limited to on-screen context, often resulting\nin inaccurate or unspecific labels. To generate more accurate and descriptive\nlabels, Caption collects next-screen context on interactive elements by\nnavigating to the destination screen that appears after an interaction and\nincorporating information from both the origin and destination screens.\nPreliminary results show Caption generates more accurate labels than both human\nannotators and an LLM baseline. We expect Caption to empower developers by\nproviding actionable accessibility suggestions and directly support on-demand\nrepairs by screen reader users.", "AI": {"tldr": "Caption is an LLM-powered tool that generates accurate content labels for visual interactive elements on mobile devices by using next-screen context.", "motivation": "Content labels for visual elements are often missing or uninformative, hindering accessibility for screen readers. Current automated captioning systems are limited in context awareness.", "method": "Caption collects next-screen context by navigating to the destination screen after an interaction, combining information from both the origin and destination screens for label generation.", "result": "Preliminary results indicate that Caption produces more accurate labels than human annotators and a baseline LLM, thereby improving accessibility.", "conclusion": "Caption aims to empower developers with actionable accessibility suggestions and help screen reader users with on-demand repairs.", "key_contributions": ["Development of Caption, an LLM-powered labeling tool", "Utilization of next-screen context for improved label accuracy", "Support for accessibility enhancements in mobile applications"], "limitations": "", "keywords": ["LLM", "accessibility", "mobile devices", "content labels", "screen readers"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.08274", "pdf": "https://arxiv.org/pdf/2508.08274.pdf", "abs": "https://arxiv.org/abs/2508.08274", "title": "Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition", "authors": ["Roberto Labadie-Tamayo", "Djordje Slijepčević", "Xihui Chen", "Adrian Jaques Böck", "Andreas Babic", "Liz Freimann", "Christiane Atzmüller Matthias Zeppelzauer"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "33 pages, 10 figures, This is a preprint of a manuscript accepted for\n  publication in Information Processing & Management (Elsevier)", "summary": "The rapid increase in hate speech on social media has exposed an\nunprecedented impact on society, making automated methods for detecting such\ncontent important. Unlike prior black-box models, we propose a novel\ntransparent method for automated hate and counter speech recognition, i.e.,\n\"Speech Concept Bottleneck Model\" (SCBM), using adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to map input texts to an abstract adjective-based representation, which\nis then sent to a light-weight classifier for downstream tasks. Across five\nbenchmark datasets spanning multiple languages and platforms (e.g., Twitter,\nReddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which\noutperforms the most recently reported results from the literature on four out\nof five datasets. Aside from high recognition accuracy, SCBM provides a high\nlevel of both local and global interpretability. Furthermore, fusing our\nadjective-based concept representation with transformer embeddings, leads to a\n1.8% performance increase on average across all datasets, showing that the\nproposed representation captures complementary information. Our results\ndemonstrate that adjective-based concept representations can serve as compact,\ninterpretable, and effective encodings for hate and counter speech recognition.\nWith adapted adjectives, our method can also be applied to other NLP tasks.", "AI": {"tldr": "This paper introduces a transparent method called the Speech Concept Bottleneck Model (SCBM) for automated hate and counter speech recognition, utilizing adjectives as interpretable concepts, achieving high accuracy and interpretability across multiple datasets.", "motivation": "To address the increasing prevalence of hate speech on social media and the need for effective automated detection methods that also provide interpretability.", "method": "The SCBM uses large language models to transform input texts into an abstract adjective-based representation, which is then classified using a lightweight classifier. It is tested on five benchmark datasets in multiple languages and platforms.", "result": "SCBM achieved an average macro-F1 score of 0.69 and outperformed existing models on four out of five datasets, demonstrating both high accuracy and interpretability.", "conclusion": "The adjective-based representations not only enhance the performance in hate speech detection but can also be adapted for various other NLP tasks, showcasing their versatility and effectiveness.", "key_contributions": ["Introduction of the Speech Concept Bottleneck Model (SCBM) for hate speech detection.", "Use of adjective-based representations for improved interpretability in automated classification.", "Demonstration of competitive performance on multiple datasets while providing local and global interpretability."], "limitations": "", "keywords": ["hate speech", "counter speech recognition", "adjective-based representations", "interpretability", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.08737", "pdf": "https://arxiv.org/pdf/2508.08737.pdf", "abs": "https://arxiv.org/abs/2508.08737", "title": "From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation", "authors": ["Jonathan C. Roberts", "Peter Butcher", "Panagiotis D. Ritsos"], "categories": ["cs.HC", "H.5.2; K.3.2; I.3.6"], "comment": "6 pages, 5 figures, IEEE VIS Workshop on Visualization Education,\n  Literacy, and Activities 2025, Vienna", "summary": "This paper explores the use of scenario-based visualisation examples as a\npedagogical strategy for teaching students the complexities of data insight,\nrepresentation, and interpretation. Teaching data visualisation often involves\nexplaining intricate issues related to data management and the challenges of\npresenting data meaningfully. In this work, we present a series of data-driven\nscenarios. These concise stories depict specific situations, and are created to\nhelp the educators highlight key concerns in data communication, such as chart\nselection, temporal versus categorical comparison, visual bias, and narrative\nframing. By grounding these examples in real-world contexts, students are\nencouraged to critically assess not only what the data shows, but how and why\nit is shown that way. The paper presents a collection of example scenarios,\nthat educators can use for their own lessons; the work fits with a larger\nproject on looking at critical thinking in the classroom, and developing\nappropriate tools. We also start to abstract principles, from our approach, so\nthat others can develop their own scenarios for their teaching. Our approach\naligns with principles of authentic and scenario-based learning, using\nreal-world contexts to foster critical engagement with data.", "AI": {"tldr": "This paper discusses using scenario-based visualisation examples as a teaching method for data insight, representation, and interpretation.", "motivation": "The paper aims to address the challenges educators face in teaching data visualisation and to promote critical thinking about data representation in real-world contexts.", "method": "The authors present a series of data-driven scenarios designed to reflect real-world complexities in data communication, focusing on aspects like chart selection and visual bias.", "result": "The paper provides a collection of example scenarios, aiding educators in teaching data visualisation effectively while promoting critical assessment of data display methods.", "conclusion": "The approach aligns with authentic and scenario-based learning principles, allowing educators to create their own teaching scenarios derived from the principles outlined in the paper.", "key_contributions": ["Introduction of scenario-based visualisation for teaching data literacy.", "Development of concrete examples to aid data communication education.", "Framework for creating additional educational scenarios based on real-world contexts."], "limitations": "", "keywords": ["data visualization", "pedagogical strategy", "scenario-based learning", "data communication", "critical thinking"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.08275", "pdf": "https://arxiv.org/pdf/2508.08275.pdf", "abs": "https://arxiv.org/abs/2508.08275", "title": "MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis", "authors": ["Haiyun Guo", "ZhiYan Hou", "Yu Chen", "Jinghan He", "Yandu Sun", "Yuzhe Zhou", "Shujing Guo", "Kuan Zhu", "Jinqiao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Multimodal Large Language Models (MLLMs) rely on continual instruction tuning\nto adapt to the evolving demands of real-world applications. However, progress\nin this area is hindered by the lack of rigorous and systematic benchmarks. To\naddress this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark\nwith three key contributions: (1) Multidimensional Evaluation: We combine final\nanswer accuracy with fine-grained CoT reasoning quality assessment, enabled by\na specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms\nand Training Paradigms: We benchmark eight continual learning algorithms across\nfour major categories and systematically compare reinforcement learning with\nsupervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and\norganize 16 datasets from existing work, covering six challenging domains. Our\nkey findings include: (i) Models with stronger general capabilities exhibit\ngreater robustness to forgetting during continual learning; (ii) Reasoning\nchains degrade more slowly than final answers, supporting the hierarchical\nforgetting hypothesis; (iii) The effectiveness of continual learning algorithms\nis highly dependent on both model capability and task order; (iv) In\nreinforcement learning settings, incorporating KL-divergence constraints helps\nmaintain policy stability and plays a crucial role in mitigating forgetting.\nMLLM-CTBench establishes a rigorous standard for continual instruction tuning\nof MLLMs and offers practical guidance for algorithm design and evaluation.", "AI": {"tldr": "Introduction of MLLM-CTBench, a benchmark for evaluating continual instruction tuning of Multimodal Large Language Models (MLLMs) with a focus on multidimensional evaluation, algorithm comparisons, and curated tasks.", "motivation": "To address the lack of rigorous benchmarking in continual instruction tuning for MLLMs, which is essential for adapting to real-world application demands.", "method": "Introduced MLLM-CTBench that evaluates MLLMs based on accuracy, reasoning quality, and compares eight continual learning algorithms over twelve organized task datasets.", "result": "Findings reveal that models with enhanced general capabilities show resilience to forgetting, reasoning chains diminish slower than answers, and continual learning effectiveness is linked to model capability and task order.", "conclusion": "MLLM-CTBench sets a robust standard for continual instruction tuning in MLLMs, providing insights for future algorithm design and evaluation.", "key_contributions": ["Multidimensional Evaluation combining accuracy and CoT reasoning assessment", "Systematic comparison of eight continual learning algorithms", "Carefully curated tasks from multiple challenging domains"], "limitations": "", "keywords": ["Multimodal", "Large Language Models", "Continual Learning", "Benchmarking", "Instruction Tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08958", "pdf": "https://arxiv.org/pdf/2508.08958.pdf", "abs": "https://arxiv.org/abs/2508.08958", "title": "Addressing the Heterogeneity of Visualization in an Introductory PhD Course in the Swedish Context", "authors": ["Kostiantyn Kucher", "Niklas Rönnberg", "Jonas Löwgren"], "categories": ["cs.HC", "K.3.2; H.5; I.3"], "comment": "8 pages, 3 figures", "summary": "Visualization is a heterogeneous field, and this aspect is often reflected by\nthe organizational structures at higher education institutions that academic\nresearchers in visualization and related fields including computer graphics,\nhuman-computer interaction, and media design are typically affiliated with. It\nmay thus be a challenge for new PhD students to grasp the fragmented structure\nof their new workplace, form collegial relations across the institution, and to\nbuild a coherent picture of the discipline as a whole. We report an attempt to\naddress this challenge, in the form of an introductory course on the subject of\nVisualization Technology and Methodology for PhD students at the Division for\nMedia and Information Technology, Link\\\"oping University, Sweden. We discuss\nthe course design, including interactions with other doctoral education\nactivities and field trips to multiple research groups and units within the\ndivision (ranging from scientific visualization and computer graphics to media\ndesign and visual communication). Lessons learned from the course preparation\nwork as well as the first instance of the course offered during autumn term\n2023 can be helpful to researchers and educators aiming to establish or improve\nsimilar doctoral courses.", "AI": {"tldr": "The paper reports on an introductory course designed for PhD students in Visualization Technology and Methodology to help them navigate the fragmented structure of the field at Linköping University.", "motivation": "To aid new PhD students in understanding the fragmented nature of the visualization field and to help them build connections within their institution.", "method": "Designing an introductory course that includes interactions with other doctoral education activities and field trips to various research groups and units related to visualization.", "result": "The course enabled students to better grasp the diversity in the field of visualization and facilitated building collegial relationships across the institution.", "conclusion": "The lessons learned from this course's preparation and execution can guide researchers and educators in establishing or improving similar doctoral courses.", "key_contributions": ["Design of an interdisciplinary introductory course on visualization for PhD students", "Incorporation of field trips to enhance learning and networking", "Insights into the challenges faced by new PhD students in visualization"], "limitations": "", "keywords": ["Visualization", "PhD Education", "Human-Computer Interaction"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.08276", "pdf": "https://arxiv.org/pdf/2508.08276.pdf", "abs": "https://arxiv.org/abs/2508.08276", "title": "Evaluating Contrast Localizer for Identifying Causal Unitsin Social & Mathematical Tasks in Language Models", "authors": ["Yassine Jamaa", "Badr AlKhamissi", "Satrajit Ghosh", "Martin Schrimpf"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work adapts a neuroscientific contrast localizer to pinpoint causally\nrelevant units for Theory of Mind (ToM) and mathematical reasoning tasks in\nlarge language models (LLMs) and vision-language models (VLMs). Across 11 LLMs\nand 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated\nunits using contrastive stimulus sets and assess their causal role via targeted\nablations. We compare the effect of lesioning functionally selected units\nagainst low-activation and randomly selected units on downstream accuracy\nacross established ToM and mathematical benchmarks. Contrary to expectations,\nlow-activation units sometimes produced larger performance drops than the\nhighly activated ones, and units derived from the mathematical localizer often\nimpaired ToM performance more than those from the ToM localizer. These findings\ncall into question the causal relevance of contrast-based localizers and\nhighlight the need for broader stimulus sets and more accurately capture\ntask-specific units.", "AI": {"tldr": "This paper investigates the causal relevance of units in LLMs and VLMs for Theory of Mind and mathematical reasoning tasks using a neuroscientific approach.", "motivation": "To explore the causal relevance of specific units in AI models related to Theory of Mind and mathematical reasoning, challenging existing notions of localizers.", "method": "Adapted a neuroscientific contrast localizer to identify and assess units in 11 LLMs and 5 VLMs through targeted ablations and performance benchmarking.", "result": "Findings showed that low-activation units sometimes caused larger performance drops than high-activation ones, questioning the reliability of contrast-based localizers.", "conclusion": "The study suggests that task-specific units may not be accurately captured and advocates for more diverse stimulus sets in future research.", "key_contributions": ["Demonstrated a causal analysis approach using neuroscientific techniques in AI models", "Revealed unexpected performance impacts of low-activation units", "Questioned the efficacy of current localizers for determining unit relevance"], "limitations": "Focus on a limited set of models and tasks; results may not generalize to all scenarios.", "keywords": ["Theory of Mind", "Mathematical reasoning", "Contrast localizer", "Language models", "Causal relevance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09028", "pdf": "https://arxiv.org/pdf/2508.09028.pdf", "abs": "https://arxiv.org/abs/2508.09028", "title": "Envisioning Generative Artificial Intelligence in Cartography and Mapmaking", "authors": ["Yuhao Kang", "Chenglong Wang"], "categories": ["cs.HC"], "comment": "9 pages, 6 figures", "summary": "Generative artificial intelligence (GenAI), including large language models,\ndiffusion-based image generation models, and GenAI agents, has provided new\nopportunities for advancements in mapping and cartography. Due to their\ncharacteristics including world knowledge and generalizability, artistic style\nand creativity, and multimodal integration, we envision that GenAI may benefit\na variety of cartographic design decisions, from mapmaking (e.g.,\nconceptualization, data preparation, map design, and map evaluation) to map use\n(such as map reading, interpretation, and analysis). This paper discusses\nseveral important topics regarding why and how GenAI benefits cartography with\ncase studies including symbolization, map evaluation, and map reading. Despite\nits unprecedented potential, we identify key scenarios where GenAI may not be\nsuitable, such as tasks that require a deep understanding of cartographic\nknowledge or prioritize precision and reliability. We also emphasize the need\nto consider ethical and social implications, such as concerns related to\nhallucination, reproducibility, bias, copyright, and explainability. This work\nlays the foundation for further exploration and provides a roadmap for future\nresearch at the intersection of GenAI and cartography.", "AI": {"tldr": "This paper explores the impact of generative artificial intelligence on cartography, discussing its applications, benefits, limitations, and ethical considerations.", "motivation": "The motivation is to examine how generative AI can enhance cartographic design decisions and practices.", "method": "The paper employs case studies to illustrate how generative AI impacts aspects like symbolization, map evaluation, and map reading in cartography.", "result": "Generative AI can significantly improve mapmaking processes and usability but has limitations in scenarios requiring deep cartographic knowledge.", "conclusion": "The work highlights the potential of GenAI in cartography while cautioning against its use in precision-critical tasks and urging consideration of ethical issues.", "key_contributions": ["Identification of benefits of GenAI in cartography", "Case studies demonstrating practical applications", "Discussion of ethical implications related to GenAI usage in mapping"], "limitations": "Key scenarios where GenAI may not be suitable include tasks needing deep understanding of cartographic knowledge and precision-focused requirements.", "keywords": ["generative artificial intelligence", "cartography", "map design", "ethical implications", "symbolization"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.08277", "pdf": "https://arxiv.org/pdf/2508.08277.pdf", "abs": "https://arxiv.org/abs/2508.08277", "title": "Objective Metrics for Evaluating Large Language Models Using External Data Sources", "authors": ["Haoze Du", "Richard Li", "Edward Gehringer"], "categories": ["cs.CL", "cs.LG"], "comment": "This version of the paper is lightly revised from the EDM 2025\n  proceedings for the sake of clarity", "summary": "Evaluating the performance of Large Language Models (LLMs) is a critical yet\nchallenging task, particularly when aiming to avoid subjective assessments.\nThis paper proposes a framework for leveraging subjective metrics derived from\nthe class textual materials across different semesters to assess LLM outputs\nacross various tasks. By utilizing well-defined benchmarks, factual datasets,\nand structured evaluation pipelines, the approach ensures consistent,\nreproducible, and bias-minimized measurements. The framework emphasizes\nautomation and transparency in scoring, reducing reliance on human\ninterpretation while ensuring alignment with real-world applications. This\nmethod addresses the limitations of subjective evaluation methods, providing a\nscalable solution for performance assessment in educational, scientific, and\nother high-stakes domains.", "AI": {"tldr": "This paper presents a framework for objectively evaluating Large Language Models (LLMs) using subjective metrics from educational materials, ensuring transparent and consistent assessment.", "motivation": "To improve the performance evaluation of Large Language Models (LLMs) by minimizing subjectivity and enhancing consistency in assessments.", "method": "The framework utilizes subjective metrics from class materials, established benchmarks, and structured evaluation pipelines to assess LLM outputs across various tasks.", "result": "The proposed method enables scalable, automated, and transparent performance assessments while addressing the limitations of traditional subjective evaluation methods.", "conclusion": "This framework provides a reliable and efficient approach for measuring LLM performance in educational and high-stakes contexts, reducing human bias.", "key_contributions": ["Introduction of a framework for objective LLM evaluation using subjective metrics", "Emphasis on automation and transparency in scoring methodology", "Development of a scalable solution for performance assessment across different domains"], "limitations": "The framework may still depend on the quality of the underlying textual materials and defined metrics.", "keywords": ["Large Language Models", "Evaluation Framework", "Subjective Metrics", "Automation", "Performance Assessment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09033", "pdf": "https://arxiv.org/pdf/2508.09033.pdf", "abs": "https://arxiv.org/abs/2508.09033", "title": "Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration", "authors": ["Tina Behzad", "Nikolos Gurney", "Ning Wang", "David V. Pynadath"], "categories": ["cs.HC"], "comment": "Accepted as LBW, HCII 2025", "summary": "The promise of human-AI teaming lies in humans and AI working together to\nachieve performance levels neither could accomplish alone. Effective\ncommunication between AI and humans is crucial for teamwork, enabling users to\nefficiently benefit from AI assistance. This paper investigates how AI\ncommunication impacts human-AI team performance. We examine AI explanations\nthat convey an awareness of its strengths and limitations. To achieve this, we\ntrain a decision tree on the model's mistakes, allowing it to recognize and\nexplain where and why it might err. Through a user study on an income\nprediction task, we assess the impact of varying levels of information and\nexplanations about AI predictions. Our results show that AI performance\ninsights enhance task performance, and conveying AI awareness of its strengths\nand weaknesses improves trust calibration. These findings highlight the\nimportance of considering how information delivery influences user trust and\nreliance in AI-assisted decision-making.", "AI": {"tldr": "This paper explores how AI communication affects human-AI team performance, particularly focusing on AI explanations regarding its strengths and weaknesses.", "motivation": "The motivation is to enhance the benefits of human-AI teaming through effective communication, enabling users to better understand and trust AI assistance.", "method": "The authors trained a decision tree to identify and explain the model's mistakes. A user study was conducted on an income prediction task, examining the effects of varying information levels and AI explanations.", "result": "The study found that providing insights into AI performance significantly enhanced task performance, and improving AI's communication about its strengths and limitations fostered better trust calibration among users.", "conclusion": "The findings emphasize the critical role of information delivery in shaping user trust and reliance on AI in decision-making processes.", "key_contributions": ["Investigates the impact of AI explanations on human-AI team performance.", "Demonstrates how communicating AI's strengths and weaknesses affects user trust.", "Provides empirical evidence from a user study to support findings."], "limitations": "", "keywords": ["Human-AI collaboration", "AI communication", "Trust in AI", "Decision-making", "Explanations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.08283", "pdf": "https://arxiv.org/pdf/2508.08283.pdf", "abs": "https://arxiv.org/abs/2508.08283", "title": "MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language", "authors": ["Andres Garcia Rincon", "Eliseo Ferrante"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": null, "summary": "This paper presents MinionsLLM, a novel framework that integrates Large\nLanguage Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable\nnatural language control of multi-agent systems within arbitrary, user-defined\nenvironments. MinionsLLM provides standardized interfaces for defining\nenvironments, agents, and behavioral primitives, and introduces two synthetic\ndataset generation methods (Method A and Method B) to fine-tune LLMs for\nimproved syntactic validity and semantic task relevance. We validate our\napproach using Google's Gemma 3 model family at three parameter scales (1B, 4B,\nand 12B) and demonstrate substantial gains: Method B increases syntactic\nvalidity to 92.6% and achieves a mean task performance improvement of 33% over\nbaseline. Notably, our experiments show that smaller models benefit most from\nfine-tuning, suggesting promising directions for deploying compact, locally\nhosted LLMs in resource-constrained multi-agent control scenarios. The\nframework and all resources are released open-source to support reproducibility\nand future research.", "AI": {"tldr": "This paper introduces MinionsLLM, a framework that combines LLMs with Behavior Trees and Formal Grammars for natural language control of multi-agent systems, achieving notable improvements in performance and syntactic validity.", "motivation": "To enhance natural language control of multi-agent systems through the integration of LLMs, while providing standardized interfaces and synthetic dataset generation methods.", "method": "Combines LLMs, Behavior Trees, and Formal Grammars; develops two synthetic dataset generation methods to fine-tune LLMs for better performance; validated using Google's Gemma 3 model family.", "result": "Achieved a 92.6% syntactic validity and a 33% mean task performance improvement over baseline across different parameter scales.", "conclusion": "Smaller models show the greatest improvement from fine-tuning, indicating potential for effective deployment in constrained environments.", "key_contributions": ["Introduction of MinionsLLM framework for multi-agent systems", "Development of synthetic dataset generation methods", "Open-source release to promote reproducibility"], "limitations": "", "keywords": ["Large Language Models", "Multi-agent systems", "Behavior Trees", "Synthetic dataset generation", "Natural language processing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.09043", "pdf": "https://arxiv.org/pdf/2508.09043.pdf", "abs": "https://arxiv.org/abs/2508.09043", "title": "Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks", "authors": ["Yanbing Chen", "Jonathan Nelson", "Bing Zhou", "Ryan Zhenqi Zhou", "Shan Ye", "Haokun Liu", "Zhining Gu", "Armita Kar", "Hoeyun Kwon", "Pengyu Chen", "Maoran Sun", "Yuhao Kang"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": "54 pages, 12 figures", "summary": "Academia is profoundly influenced by faculty hiring networks, which serve as\ncritical conduits for knowledge dissemination and the formation of\ncollaborative research initiatives. While extensive research in various\ndisciplines has revealed the institutional hierarchies inherent in these\nnetworks, their impacts within GIScience remain underexplored. To fill this\ngap, this study analyzes the placement patterns of 946 GIScience faculty\nworldwide by mapping the connections between PhD-granting institutions and\ncurrent faculty affiliations. Our dataset, which is compiled from\nvolunteer-contributed information, is the most comprehensive collection\navailable in this field. While there may be some limitations in its\nrepresentativeness, its scope and depth provide a unique and valuable\nperspective on the global placement patterns of GIScience faculty. Our analysis\nreveals several influential programs in placing GIScience faculty, with hiring\nconcentrated in the western countries. We examined the diversity index to\nassess the representation of regions and institutions within the global\nGIScience faculty network. We observe significant internal retention at both\nthe continental and country levels, and a high level of non-self-hired ratio at\nthe institutional level. Over time, research themes have also evolved, with\ngrowing research clusters emphasis on spatial data analytics, cartography and\ngeovisualization, geocomputation, and environmental sciences, etc. These\nresults illuminate the influence of hiring practices on global knowledge\ndissemination and contribute to promoting academic equity within GIScience and\nGeography.", "AI": {"tldr": "This study investigates faculty hiring networks in GIScience, mapping the connections of 946 faculty worldwide to reveal placement patterns, institutional influences, and evolving research themes.", "motivation": "To explore the underexamined impacts of faculty hiring networks on knowledge dissemination and collaborative research in GIScience, filling a gap in existing literature.", "method": "Analyzed placement patterns of 946 GIScience faculty by mapping connections between PhD-granting institutions and current faculty affiliations, using a comprehensive dataset compiled from volunteer contributions.", "result": "Identified influential programs in placing GIScience faculty, with heavy hiring concentration in western countries, significant internal retention, and evolving research themes focused on spatial data analytics, cartography, and environmental sciences.", "conclusion": "The findings highlight the role of hiring practices in shaping global knowledge dissemination and advocate for academic equity in GIScience and Geography.", "key_contributions": ["Comprehensive mapping of GIScience faculty hiring networks", "Identification of influential academic programs", "Insight into evolving research themes in GIScience"], "limitations": "Potential limitations in the dataset's representativeness due to volunteer contributions.", "keywords": ["GIScience", "faculty hiring networks", "knowledge dissemination", "academic equity", "research themes"], "importance_score": 2, "read_time_minutes": 30}}
{"id": "2508.08285", "pdf": "https://arxiv.org/pdf/2508.08285.pdf", "abs": "https://arxiv.org/abs/2508.08285", "title": "The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs", "authors": ["Denis Janiak", "Jakub Binkowski", "Albert Sawczyn", "Bogdan Gabrys", "Ravid Schwartz-Ziv", "Tomasz Kajdanowicz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their tendency to hallucinate poses serious challenges for reliable\ndeployment. Despite numerous hallucination detection methods, their evaluations\noften rely on ROUGE, a metric based on lexical overlap that misaligns with\nhuman judgments. Through comprehensive human studies, we demonstrate that while\nROUGE exhibits high recall, its extremely low precision leads to misleading\nperformance estimates. In fact, several established detection methods show\nperformance drops of up to 45.9\\% when assessed using human-aligned metrics\nlike LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based\non response length can rival complex detection techniques, exposing a\nfundamental flaw in current evaluation practices. We argue that adopting\nsemantically aware and robust evaluation frameworks is essential to accurately\ngauge the true performance of hallucination detection methods, ultimately\nensuring the trustworthiness of LLM outputs.", "AI": {"tldr": "The paper critiques the use of ROUGE for evaluating hallucination detection in large language models (LLMs), highlighting a misalignment with human judgements.", "motivation": "To address the challenges posed by hallucinations in LLMs and improve the evaluation of detection methods.", "method": "Comprehensive human studies assessing the effectiveness of hallucination detection methods using both ROUGE and human-aligned metrics.", "result": "ROUGE shows high recall but low precision, leading to misleading performance estimates; several detection methods dropped performance by up to 45.9% when evaluated with human-aligned metrics.", "conclusion": "There is a need for semantically aware evaluation frameworks to better assess hallucination detection methods and ensure LLM output trustworthiness.", "key_contributions": ["Critique of ROUGE as an evaluation metric for hallucination detection", "Presentation of human-aligned evaluation metrics like LLM-as-Judge", "Demonstration that simple heuristics can outperform complex detection techniques."], "limitations": "Focus primarily on evaluation methodologies without proposing new detection techniques.", "keywords": ["Large Language Models", "Hallucination Detection", "Evaluation Metrics", "Natural Language Processing", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08287", "pdf": "https://arxiv.org/pdf/2508.08287.pdf", "abs": "https://arxiv.org/abs/2508.08287", "title": "Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions", "authors": ["Farah Atif", "Nursultan Askarbekuly", "Kareem Darwish", "Monojit Choudhury"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "8th AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Despite the increasing usage of Large Language Models (LLMs) in answering\nquestions in a variety of domains, their reliability and accuracy remain\nunexamined for a plethora of domains including the religious domains. In this\npaper, we introduce a novel benchmark FiqhQA focused on the LLM generated\nIslamic rulings explicitly categorized by the four major Sunni schools of\nthought, in both Arabic and English. Unlike prior work, which either overlooks\nthe distinctions between religious school of thought or fails to evaluate\nabstention behavior, we assess LLMs not only on their accuracy but also on\ntheir ability to recognize when not to answer. Our zero-shot and abstention\nexperiments reveal significant variation across LLMs, languages, and legal\nschools of thought. While GPT-4o outperforms all other models in accuracy,\nGemini and Fanar demonstrate superior abstention behavior critical for\nminimizing confident incorrect answers. Notably, all models exhibit a\nperformance drop in Arabic, highlighting the limitations in religious reasoning\nfor languages other than English. To the best of our knowledge, this is the\nfirst study to benchmark the efficacy of LLMs for fine-grained Islamic school\nof thought specific ruling generation and to evaluate abstention for Islamic\njurisprudence queries. Our findings underscore the need for task-specific\nevaluation and cautious deployment of LLMs in religious applications.", "AI": {"tldr": "This paper introduces FiqhQA, a benchmark for evaluating LLMs on Islamic rulings and their ability to abstain from answering inappropriately across Sunni schools of thought.", "motivation": "To address the lack of evaluation on LLMs in religious domains, particularly regarding their reliability and acknowledgment of when not to answer questions.", "method": "Zero-shot and abstention experiments were conducted to test LLMs' accuracy and their behavior in abstaining from providing answers when necessary.", "result": "GPT-4o showed the highest accuracy, while Gemini and Fanar had better abstention behavior. All models performed worse in Arabic, indicating challenges in non-English language contexts.", "conclusion": "The study highlights the need for task-specific evaluations of LLMs in religious settings and cautions against their deployment without thorough assessments.", "key_contributions": ["Introduction of the FiqhQA benchmark for Islamic jurisprudence", "Evaluation of LLMs' abstention behaviors in religious contexts", "Identification of performance disparities between languages and models."], "limitations": "The performance of models drops in Arabic, indicating limitations in LLM’s religious reasoning abilities for non-English contexts.", "keywords": ["Large Language Models", "Islamic Jurisprudence", "Abstention Behavior", "Benchmark", "Accuracy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.08271", "pdf": "https://arxiv.org/pdf/2508.08271.pdf", "abs": "https://arxiv.org/abs/2508.08271", "title": "Heartificial Intelligence: Exploring Empathy in Language Models", "authors": ["Victoria Williams", "Benjamin Rosman"], "categories": ["cs.CL", "cs.HC"], "comment": "21 pages, 5 tables", "summary": "Large language models have become increasingly common, used by millions of\npeople worldwide in both professional and personal contexts. As these models\ncontinue to advance, they are frequently serving as virtual assistants and\ncompanions. In human interactions, effective communication typically involves\ntwo types of empathy: cognitive empathy (understanding others' thoughts and\nemotions) and affective empathy (emotionally sharing others' feelings). In this\nstudy, we investigated both cognitive and affective empathy across several\nsmall (SLMs) and large (LLMs) language models using standardized psychological\ntests. Our results revealed that LLMs consistently outperformed humans -\nincluding psychology students - on cognitive empathy tasks. However, despite\ntheir cognitive strengths, both small and large language models showed\nsignificantly lower affective empathy compared to human participants. These\nfindings highlight rapid advancements in language models' ability to simulate\ncognitive empathy, suggesting strong potential for providing effective virtual\ncompanionship and personalized emotional support. Additionally, their high\ncognitive yet lower affective empathy allows objective and consistent emotional\nsupport without running the risk of emotional fatigue or bias.", "AI": {"tldr": "This study investigates cognitive and affective empathy capabilities in small and large language models, showing LLMs excel in cognitive empathy but lag in affective empathy compared to humans.", "motivation": "To understand how advancements in large language models impact their ability to perform empathetic communication as virtual assistants.", "method": "Psychological tests for cognitive and affective empathy were administered to both language models and human participants to compare performance.", "result": "LLMs outperformed humans in cognitive empathy tasks, while both SLMs and LLMs showed significantly lower affective empathy than human participants.", "conclusion": "LLMs possess strong potential for virtual companionship through cognitive empathy but lack in affecting shared emotional understanding; they can provide objective emotional support without fatigue.", "key_contributions": ["Demonstrated LLMs' superiority in cognitive empathy tasks compared to humans.", "Highlighted the emotional support potential of LLMs while acknowledging their limits in affective empathy.", "Revealed that LLMs can offer consistent emotional backing devoid of bias and fatigue."], "limitations": "The study primarily focused on empathy testing and did not explore other facets of emotional interactions or practical applications in virtual companionship.", "keywords": ["Large language models", "Cognitive empathy", "Affective empathy", "Human-computer interaction", "Virtual companionship"], "importance_score": 8, "read_time_minutes": 21}}
{"id": "2508.08292", "pdf": "https://arxiv.org/pdf/2508.08292.pdf", "abs": "https://arxiv.org/abs/2508.08292", "title": "Putnam-AXIOM: A Functional and Static Benchmark", "authors": ["Aryan Gulati", "Brando Miranda", "Eric Chen", "Emily Xia", "Kai Fronsdal", "Bruno Dumont", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO", "cs.NE", "68T20, 68T05, 68Q32", "F.2.2; I.2.3; I.2.6; I.2.8"], "comment": "27 pages total (10-page main paper + 17-page appendix), 12 figures, 6\n  tables. Submitted to ICML 2025 (under review)", "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom.", "AI": {"tldr": "A new benchmark called Putnam-AXIOM is introduced to assess mathematical reasoning in large language models (LLMs), providing a contamination-resilient evaluation framework.", "motivation": "Current benchmarks for LLMs in mathematical reasoning are nearing saturation and may be compromised by training-set contamination, necessitating a more robust evaluation method.", "method": "Putnam-AXIOM includes 522 competition problems from the William Lowell Putnam Mathematical Competition and 100 functional variants generated through perturbation, allowing for an unlimited number of unseen instances to prevent memorization.", "result": "The strongest model, OpenAI's o1-preview, scores 41.9% accuracy on the original set but drops to 22.3% on the variations, indicating significant challenges in reasoning and the impact of dynamic benchmarks.", "conclusion": "Putnam-AXIOM offers an effective framework for evaluating LLMs in mathematical reasoning by mitigating risks from training-set contamination, enhancing the understanding of model capabilities.", "key_contributions": ["Introduction of a new benchmark for LLMs in mathematical reasoning", "Dynamic variations of problems to reduce contamination issues", "Implementation of Teacher-Forced Accuracy for scoring reasoning traces."], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Benchmarking"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.08386", "pdf": "https://arxiv.org/pdf/2508.08386.pdf", "abs": "https://arxiv.org/abs/2508.08386", "title": "CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation", "authors": ["Shuzhou Yuan", "William LaCroix", "Hardik Ghoshal", "Ercong Nie", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as AI tutors due to\ntheir scalability and potential for personalized instruction. However,\noff-the-shelf LLMs often underperform in educational settings: they frequently\nreveal answers too readily, fail to adapt their responses to student\nuncertainty, and remain vulnerable to emotionally manipulative prompts. To\naddress these challenges, we introduce CoDAE, a framework that adapts LLMs for\neducational use through Chain-of-Thought (CoT) data augmentation. We collect\nreal-world dialogues between students and a ChatGPT-based tutor and enrich them\nusing CoT prompting to promote step-by-step reasoning and pedagogically aligned\nguidance. Furthermore, we design targeted dialogue cases to explicitly mitigate\nthree key limitations: over-compliance, low response adaptivity, and threat\nvulnerability. We fine-tune four open-source LLMs on different variants of the\naugmented datasets and evaluate them in simulated educational scenarios using\nboth automatic metrics and LLM-as-a-judge assessments. Our results show that\nmodels fine-tuned with CoDAE deliver more pedagogically appropriate guidance,\nbetter support reasoning processes, and effectively resist premature answer\ndisclosure.", "AI": {"tldr": "This paper presents CoDAE, a framework for adapting large language models for educational purposes, enhancing their effectiveness as AI tutors.", "motivation": "To improve the performance of off-the-shelf large language models in educational settings, addressing limitations such as readiness to reveal answers, poor adaptivity, and vulnerability to emotional manipulation.", "method": "The authors introduce Chain-of-Thought (CoT) data augmentation, enrich real-world dialogues with pedagogical guidance, and fine-tune four open-source LLMs using augmented datasets.", "result": "Models fine-tuned with CoDAE provide better pedagogical guidance, support reasoning processes, and resist premature answer disclosure in simulated educational scenarios.", "conclusion": "The CoDAE framework significantly enhances the educational capabilities of LLMs, making them more effective AI tutors.", "key_contributions": ["Introduction of CoDAE framework for educational LLM adaptation", "Augmented datasets using Chain-of-Thought prompting", "Improved resistance to answer disclosure and better pedagogical alignment"], "limitations": "The study's findings are based on simulated scenarios; real-world effectiveness needs further validation.", "keywords": ["Large Language Models", "Education", "AI Tutoring", "CoDAE", "Data Augmentation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08401", "pdf": "https://arxiv.org/pdf/2508.08401.pdf", "abs": "https://arxiv.org/abs/2508.08401", "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery", "authors": ["Jiatong Li", "Weida Wang", "Qinggang Zhang", "Junxian Li", "Di Zhang", "Changmeng Zheng", "Shufei Zhang", "Xiaoyong Wei", "Qing Li"], "categories": ["cs.CL"], "comment": "20 pages", "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.", "AI": {"tldr": "Mol-R1 is a novel framework designed to enhance the explainability and reasoning performance of Explicit Long-CoT reasoning LLMs for text-based molecule generation, addressing limitations in knowledge-intensive domains like molecule discovery.", "motivation": "To improve reasoning capabilities of LLMs in molecule discovery, which requires a deep understanding of complex domain knowledge and suffers from low efficiency and limited ability in current models.", "method": "The framework uses a high-quality reasoning dataset generated through Prior Regulation via In-context Distillation (PRID) and employs a training strategy called Molecular Iterative Adaptation (MoIA) that combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO).", "result": "Mol-R1 demonstrates superior performance in text-based molecule reasoning generation tasks compared to existing models.", "conclusion": "The proposed Mol-R1 framework effectively bridges the gap between LLM capabilities and the rigorous demands of molecular data tasks, improving both reasoning and explainability.", "key_contributions": ["Introduction of the Mol-R1 framework for molecule generation using LLMs.", "Development of the PRID methodology for enhancing reasoning datasets.", "Implementation of MoIA to improve model training specific to molecular discovery."], "limitations": "The effectiveness of the framework is still subject to the quality and breadth of the underlying dataset and may not generalize well across all chemical domains.", "keywords": ["large language models", "molecule discovery", "reasoning performance", "explainability", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.08424", "pdf": "https://arxiv.org/pdf/2508.08424.pdf", "abs": "https://arxiv.org/abs/2508.08424", "title": "Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment", "authors": ["Saketh Reddy Vemula", "Dipti Mishra Sharma", "Parameswari Krishnamurthy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior work on language modeling showed conflicting findings about whether\nmorphologically aligned approaches to tokenization improve performance,\nparticularly for languages with complex morphology. To investigate this, we\nselect a typologically diverse set of languages: Telugu (agglutinative), Hindi\n(primarily fusional with some agglutination), and English (fusional). We\nconduct a comprehensive evaluation of language models -- starting from\ntokenizer training and extending through the finetuning and downstream task\nevaluation. To account for the consistent performance differences observed\nacross tokenizer variants, we focus on two key factors: morphological alignment\nand tokenization quality. To assess morphological alignment of tokenizers in\nTelugu, we create a dataset containing gold morpheme segmentations of 600\nderivational and 7000 inflectional word forms.\n  Our experiments reveal that better morphological alignment correlates\npositively -- though moderately -- with performance in syntax-based tasks such\nas Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing.\nHowever, we also find that the tokenizer algorithm (Byte-pair Encoding vs.\nUnigram) plays a more significant role in influencing downstream performance\nthan morphological alignment alone. Naive Unigram tokenizers outperform others\nacross most settings, though hybrid tokenizers that incorporate morphological\nsegmentation significantly improve performance within the BPE framework. In\ncontrast, intrinsic metrics like Corpus Token Count (CTC) and R\\'enyi entropy\nshowed no correlation with downstream performance.", "AI": {"tldr": "This paper investigates the impacts of morphological alignment in tokenization on language model performance across diverse languages.", "motivation": "To explore whether morphologically aligned tokenization improves performance for languages with complex morphology.", "method": "A comprehensive evaluation of language models focusing on tokenizer training, finetuning, and downstream task evaluations across Telugu, Hindi, and English.", "result": "Better morphological alignment moderately correlates with performance improvements in syntax-based tasks, but tokenizer algorithm has a more significant impact than morphological alignment.", "conclusion": "Hybrid tokenizers that incorporate morphological segmentation enhance performance within the BPE framework, while intrinsic metrics did not correlate with downstream performance.", "key_contributions": ["Created a dataset for Telugu with gold morpheme segmentations", "Demonstrated the significance of tokenizer algorithm over morphological alignment", "Showed positive correlation between morphological alignment and performance in syntax tasks"], "limitations": "", "keywords": ["language modeling", "tokenization", "morphological alignment", "downstream tasks", "NLP"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.08466", "pdf": "https://arxiv.org/pdf/2508.08466.pdf", "abs": "https://arxiv.org/abs/2508.08466", "title": "Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints", "authors": ["Daren Yao", "Jinsong Yuan", "Ruike Chen"], "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "Small large language models (LLMs) often face difficulties in aligning output\nto human preferences, particularly when operating under severe performance\ngaps. In this work, we propose two lightweight DPO-based variants -- Adaptive\nMargin-Sigmoid Loss and APO-hinge-zero -- to better address underperformance\nscenarios by introducing margin-based objectives and selective update\nmechanisms.\n  Our APO-hinge-zero method, which combines hinge-induced hard-example mining\nwith the chosen-focused optimization of APO-zero, achieves strong results. In\nAlpacaEval, APO-hinge-zero improves the win rate by +2.0 points and the\nlength-controlled win rate by +1.4 points compared to the APO-zero baseline. In\nMT-Bench, our methods maintain competitive performance in diverse categories,\nparticularly excelling in STEM and Humanities tasks.\n  These results demonstrate that simple modifications to preference-based\nobjectives can significantly enhance small LLM alignment under resource\nconstraints, offering a practical path toward more efficient deployment.", "AI": {"tldr": "The paper proposes two lightweight DPO-based methods to enhance the alignment of small LLMs with human preferences under performance constraints.", "motivation": "To improve the alignment of small LLMs with human preferences, especially in scenarios of underperformance.", "method": "Introduction of Adaptive Margin-Sigmoid Loss and APO-hinge-zero, utilizing margin-based objectives and selective update mechanisms.", "result": "APO-hinge-zero outperformed APO-zero baseline in AlpacaEval with a +2.0 points win rate improvement and showed competitive results in MT-Bench across various categories, particularly in STEM and Humanities.", "conclusion": "Simple adaptations of preference-based objectives can significantly improve the performance of small LLMs under resource constraints, paving the way for their efficient deployment.", "key_contributions": ["Introduction of two novel DPO-based models for small LLMs", "Demonstration of significant performance improvements in preference alignment", "Potential for efficient deployment of LLMs in resource-constrained environments"], "limitations": "", "keywords": ["Large Language Models", "Human Preferences", "Adaptive Margin-Sigmoid Loss", "APO-hinge-zero", "Preference-based Objectives"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.08492", "pdf": "https://arxiv.org/pdf/2508.08492.pdf", "abs": "https://arxiv.org/abs/2508.08492", "title": "Momentum Point-Perplexity Mechanics in Large Language Models", "authors": ["Lorenzo Tomaz", "Judd Rosenblatt", "Thomas Berry Jones", "Diogo Schwerz de Lucena"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We take a physics-based approach to studying how the internal hidden states\nof large language models change from token to token during inference. Across 20\nopen-source transformer models (135M-3B parameters), we find that a quantity\ncombining the rate of change in hidden states and the model's next-token\ncertainty, analogous to energy in physics, remains nearly constant.\nRandom-weight models conserve this \"energy\" more tightly than pre-trained ones,\nwhile training shifts models into a faster, more decisive regime with greater\nvariability. Using this \"log-Lagrangian\" view, we derive a control method\ncalled Jacobian steering, which perturbs hidden states in the minimal way\nneeded to favor a target token. This approach maintained near-constant energy\nin two tested models and produced continuations rated higher in semantic\nquality than the models' natural outputs. Viewing transformers through this\nmechanics lens offers a principled basis for interpretability, anomaly\ndetection, and low-risk steering. This could help make powerful models more\npredictable and aligned with human intent.", "AI": {"tldr": "The paper investigates the dynamics of hidden states in large language models and proposes a method called Jacobian steering to improve token prediction.", "motivation": "To understand how internal hidden states of language models evolve during inference, and to enhance predictability and alignment with human intent.", "method": "The study analyzes the hidden states of 20 transformer models using a physics-based analogy and develops Jacobian steering to minimally perturb hidden states to favor target tokens.", "result": "Jacobian steering maintained nearly constant 'energy' in the hidden states and produced higher quality text continuations compared to the models' original outputs.", "conclusion": "Approaching transformers through a mechanics lens provides insights for interpretability and control of models, potentially enhancing their alignment with human objectives.", "key_contributions": ["Introduced a 'log-Lagrangian' perspective to analyze model behavior.", "Developed Jacobian steering as a control method for language models.", "Demonstrated improved semantic quality in model outputs using the proposed method."], "limitations": "", "keywords": ["language models", "hidden states", "Jacobian steering", "interpretability", "energy conservation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.08509", "pdf": "https://arxiv.org/pdf/2508.08509.pdf", "abs": "https://arxiv.org/abs/2508.08509", "title": "Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression", "authors": ["Jadie Adams", "Brian Hu", "Emily Veenhuis", "David Joy", "Bharadwaj Ravichandran", "Aaron Bray", "Anthony Hoogs", "Arslan Basharat"], "categories": ["cs.CL", "cs.AI"], "comment": "AIES '25: Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics,\n  and Society", "summary": "Large language models (LLMs) are currently aligned using techniques such as\nreinforcement learning from human feedback (RLHF). However, these methods use\nscalar rewards that can only reflect user preferences on average. Pluralistic\nalignment instead seeks to capture diverse user preferences across a set of\nattributes, moving beyond just helpfulness and harmlessness. Toward this end,\nwe propose a steerable pluralistic model based on few-shot comparative\nregression that can adapt to individual user preferences. Our approach\nleverages in-context learning and reasoning, grounded in a set of fine-grained\nattributes, to compare response options and make aligned choices. To evaluate\nour algorithm, we also propose two new steerable pluralistic benchmarks by\nadapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets,\ndemonstrating the applicability of our approach to value-aligned\ndecision-making and reward modeling, respectively. Our few-shot comparative\nregression approach is interpretable and compatible with different attributes\nand LLMs, while outperforming multiple baseline and state-of-the-art methods.\nOur work provides new insights and research directions in pluralistic\nalignment, enabling a more fair and representative use of LLMs and advancing\nthe state-of-the-art in ethical AI.", "AI": {"tldr": "This paper introduces a steerable pluralistic model for aligning large language models (LLMs) according to diverse user preferences, using few-shot comparative regression.", "motivation": "Current alignment techniques for LLMs primarily focus on scalar rewards, which fail to capture the diverse preferences of users. This paper aims to enhance LLM alignment by developing a model that better reflects individual user preferences.", "method": "The proposed method employs few-shot comparative regression, leveraging in-context learning to evaluate and adapt to user preferences across multiple fine-grained attributes. Two new benchmarks, adapted from existing datasets, are used for evaluation.", "result": "The new model outperforms various baseline and state-of-the-art alignment methods, demonstrating improved interpretability and adaptability across different LLMs and attributes.", "conclusion": "This research paves the way for more representative and fair use of LLMs, contributing to the advancement of ethical AI by offering new insights into pluralistic alignment.", "key_contributions": ["Introduction of a steerable pluralistic model for LLM alignment", "Development of two new benchmarks for evaluating pluralistic alignment", "Demonstration of superior performance over existing alignment methods"], "limitations": "", "keywords": ["Large Language Models", "Pluralistic Alignment", "Few-Shot Learning", "Ethical AI", "User Preferences"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08514", "pdf": "https://arxiv.org/pdf/2508.08514.pdf", "abs": "https://arxiv.org/abs/2508.08514", "title": "DeCAL Tokenwise Compression", "authors": ["Sameer Panwar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper introduces DeCAL, a new method for tokenwise compression. DeCAL\nuses an encoder-decoder language model pretrained with denoising to learn to\nproduce high-quality, general-purpose compressed representations by the\nencoder. DeCAL applies small modifications to the encoder, with the emphasis on\nmaximizing compression quality, even at the expense of compute. We show that\nDeCAL at 2x compression can match uncompressed on many downstream tasks, with\nusually only minor dropoff in metrics up to 8x compression, among\nquestion-answering, summarization, and multi-vector retrieval tasks. DeCAL\noffers significant savings where pre-computed dense representations can be\nutilized, and we believe the approach can be further developed to be more\nbroadly applicable.", "AI": {"tldr": "DeCAL is a new tokenwise compression method leveraging a language model to create high-quality compressed representations, achieving competitive performance on downstream tasks at high compression rates.", "motivation": "To develop an effective tokenwise compression method that maximizes compression quality for language models while maintaining performance on various tasks.", "method": "DeCAL modifies an encoder-decoder language model trained with denoising to generate compressed representations, emphasizing quality over computation cost.", "result": "DeCAL achieves competitive results at 2x compression, maintaining performance with minor metric drops at up to 8x compression across tasks like question-answering and summarization.", "conclusion": "DeCAL demonstrates strong potential for high-quality compression in language models, with opportunities for broader application.", "key_contributions": ["Introduction of DeCAL for tokenwise compression", "Performance matching uncompressed models at 2x compression", "Significant savings for tasks utilizing pre-computed dense representations"], "limitations": "Focus on compression quality may increase computational requirements.", "keywords": ["tokenwise compression", "language model", "compressed representations", "DeCAL", "NLP"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.08591", "pdf": "https://arxiv.org/pdf/2508.08591.pdf", "abs": "https://arxiv.org/abs/2508.08591", "title": "DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives", "authors": ["Sehwan Moon", "Aram Lee", "Jeong Eun Kim", "Hee-Ju Kang", "Il-Seon Shin", "Sung-Wan Kim", "Jae-Min Kim", "Min Jhon", "Ju-Wan Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Advances in large language models (LLMs) have enabled a wide range of\napplications. However, depression prediction is hindered by the lack of\nlarge-scale, high-quality, and rigorously annotated datasets. This study\nintroduces DepressLLM, trained and evaluated on a novel corpus of 3,699\nautobiographical narratives reflecting both happiness and distress. DepressLLM\nprovides interpretable depression predictions and, via its Score-guided Token\nProbability Summation (SToPS) module, delivers both improved classification\nperformance and reliable confidence estimates, achieving an AUC of 0.789, which\nrises to 0.904 on samples with confidence $\\geq$ 0.95. To validate its\nrobustness to heterogeneous data, we evaluated DepressLLM on in-house datasets,\nincluding an Ecological Momentary Assessment (EMA) corpus of daily stress and\nmood recordings, and on public clinical interview data. Finally, a psychiatric\nreview of high-confidence misclassifications highlighted key model and data\nlimitations that suggest directions for future refinements. These findings\ndemonstrate that interpretable AI can enable earlier diagnosis of depression\nand underscore the promise of medical AI in psychiatry.", "AI": {"tldr": "This paper presents DepressLLM, a large language model developed to predict depression using a novel dataset of autobiographical narratives, achieving high classification performance and providing interpretable predictions.", "motivation": "To address the challenge of depression prediction due to a lack of large-scale and high-quality annotated datasets.", "method": "Trained on a dataset of 3,699 autobiographical narratives, DepressLLM employs a Score-guided Token Probability Summation (SToPS) module for improved classification performance and confidence estimates.", "result": "DepressLLM achieved an AUC of 0.789 overall, improving to 0.904 with high-confidence predictions (≥ 0.95).", "conclusion": "The study demonstrates that interpretable AI can support earlier depression diagnosis and emphasizes the potential of AI in psychiatric applications.", "key_contributions": ["Introduction of DepressLLM for depression prediction", "Use of a novel corpus of autobiographical narratives", "Implementation of the SToPS module for improved prediction and confidence estimates"], "limitations": "Identified model and data limitations through a psychiatric review of misclassifications that suggest future research directions.", "keywords": ["Depression Prediction", "Large Language Models", "Interpretable AI", "Health Informatics", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08610", "pdf": "https://arxiv.org/pdf/2508.08610.pdf", "abs": "https://arxiv.org/abs/2508.08610", "title": "Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review", "authors": ["David Santandreu Calonge", "Linda Smail"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.3"], "comment": "27 pages, 1 figure, 8 tables", "summary": "This review examines recent advances in Parameter-Efficient Fine-Tuning\n(PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize\nRetrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi.\nThese systems face challenges in understanding and generating authentic\nCantonese colloquial expressions due to limited annotated data and linguistic\nvariability. The review evaluates the integration of LoRA within RAG\nframeworks, benchmarks PEFT methods for retrieval and generation accuracy,\nidentify domain adaptation strategies under limited data, and compares\nfine-tuning techniques aimed at improving semantic fidelity under data-scarce\nconditions. A systematic analysis of recent studies employing diverse LoRA\nvariants, synthetic data generation, user feedback integration, and adaptive\nparameter allocation was conducted to assess their impact on computational\nefficiency, retrieval precision, linguistic authenticity, and scalability.\nFindings reveal that dynamic and ensemble LoRA adaptations significantly reduce\ntrainable parameters without sacrificing retrieval accuracy and generation\nquality in dialectal contexts. However, limitations remain in fully preserving\nfine-grained linguistic nuances, especially for low-resource settings like\nCantonese. The integration of real-time user feedback and domain-specific data\nremains underdeveloped, limiting model adaptability and personalization. While\nselective parameter freezing and nonlinear adaptation methods offer better\ntrade-offs between efficiency and accuracy, their robustness at scale remains\nan open challenge. This review highlights the promise of PEFT-enhanced RAG\nsystems for domain-specific language tasks and calls for future work targeting\ndialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.", "AI": {"tldr": "This review focuses on recent advances in Parameter-Efficient Fine-Tuning (PEFT) methods for optimizing Retrieval-Augmented Generation (RAG) systems, particularly in the context of Cantonese language processing.", "motivation": "To address the challenges faced by RAG systems in understanding and generating colloquial Cantonese expressions due to limited annotated data and linguistic variability.", "method": "The review evaluates the integration of Low-Rank Adaptation (LoRA) within RAG frameworks, benchmarks the accuracy of various PEFT methods, and analyzes domain adaptation strategies with limited data.", "result": "Dynamic and ensemble LoRA adaptations can significantly reduce the number of trainable parameters while maintaining both retrieval accuracy and generation quality in dialectal contexts. Nevertheless, challenges remain in preserving fine-grained linguistic nuances, particularly for low-resource languages.", "conclusion": "The review emphasizes the potential of PEFT-enhanced RAG systems for domain-specific language tasks and identifies areas for future research, including dialectal authenticity and scalable fine-tuning pipelines.", "key_contributions": ["Analysis of LoRA variants in RAG systems.", "Evaluation of PEFT methods for dialectal language processing.", "Identification of strategies for improving model adaptability and user personalization."], "limitations": "Limitations remain in preserving fine-grained linguistic nuances in low-resource settings, and the integration of real-time user feedback is underdeveloped.", "keywords": ["Parameter-Efficient Fine-Tuning", "Retrieval-Augmented Generation", "Low-Rank Adaptation", "Cantonese language", "domain adaptation"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2403.02752", "pdf": "https://arxiv.org/pdf/2403.02752.pdf", "abs": "https://arxiv.org/abs/2403.02752", "title": "HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents", "authors": ["Sam Yu-Te Lee", "Kwan-Liu Ma"], "categories": ["cs.HC"], "comment": null, "summary": "Sensemaking on a large collection of documents (corpus) is a challenging task\noften found in fields such as market research, legal studies, intelligence\nanalysis, political science, computational linguistics, etc. Previous works\napproach this problem either from a topic- or entity-based perspective, but\nthey lack interpretability and trust due to poor model alignment. In this\npaper, we present HINTs, a visual analytics approach that combines topic- and\nentity-based techniques seamlessly and integrates Large Language Models (LLMs)\nas both a general NLP task solver and an intelligent agent. By leveraging the\nextraction capability of LLMs in the data preparation stage, we model the\ncorpus as a hypergraph that matches the user's mental model when making sense\nof the corpus. The constructed hypergraph is hierarchically organized with an\nagglomerative clustering algorithm by combining semantic and connectivity\nsimilarity. The system further integrates an LLM-based intelligent chatbot\nagent in the interface to facilitate sensemaking. To demonstrate the\ngeneralizability and effectiveness of the HINTs system, we present two case\nstudies on different domains and a comparative user study. We report our\ninsights on the behavior patterns and challenges when intelligent agents are\nused to facilitate sensemaking. We find that while intelligent agents can\naddress many challenges in sensemaking, the visual hints that visualizations\nprovide are necessary to address the new problems brought by intelligent\nagents. We discuss limitations and future work for combining interactive\nvisualization and LLMs more profoundly to better support corpus analysis.", "AI": {"tldr": "The paper presents HINTs, a visual analytics system that integrates topic- and entity-based techniques with Large Language Models (LLMs) to improve document sensemaking.", "motivation": "Address the challenges of sensemaking in large document collections, which often lacks interpretability and trust due to model misalignment.", "method": "Combine topic- and entity-based approaches by leveraging LLMs for data preparation, modeling the corpus as a hypergraph, and using an agglomerative clustering algorithm to organize the data semantically and based on connectivity.", "result": "Two case studies demonstrate HINTs' generalizability and effectiveness, along with a user study revealing the behavior patterns and challenges in using intelligent agents for sensemaking.", "conclusion": "Intelligent agents can address many sensemaking challenges, but visual hints from visualizations are essential to tackle new problems introduced by these agents.", "key_contributions": ["Introduction of HINTs, a novel visual analytics framework for document sensemaking", "Integration of LLMs as intelligent agents alongside traditional analytical methods", "Empirical evidence through case studies and user evaluations on the effectiveness of the system"], "limitations": "New challenges arise with the introduction of intelligent agents requiring refined interactive visualizations.", "keywords": ["sensemaking", "visual analytics", "large language models", "hypergraph", "user study"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08636", "pdf": "https://arxiv.org/pdf/2508.08636.pdf", "abs": "https://arxiv.org/abs/2508.08636", "title": "InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling", "authors": ["Peiji Li", "Jiasheng Ye", "Yongkang Chen", "Yichuan Ma", "Zijie Yu", "Kedi Chen", "Ganqu Cui", "Haozhan Li", "Jiacheng Chen", "Chengqi Lyu", "Wenwei Zhang", "Linyang Li", "Qipeng Guo", "Dahua Lin", "Bowen Zhou", "Kai Chen"], "categories": ["cs.CL"], "comment": "InternBootcamp Tech Report", "summary": "Large language models (LLMs) have revolutionized artificial intelligence by\nenabling complex reasoning capabilities. While recent advancements in\nreinforcement learning (RL) have primarily focused on domain-specific reasoning\ntasks (e.g., mathematics or code generation), real-world reasoning scenarios\noften require models to handle diverse and complex environments that\nnarrow-domain benchmarks cannot fully capture. To address this gap, we present\nInternBootcamp, an open-source framework comprising 1000+ domain-diverse task\nenvironments specifically designed for LLM reasoning research. Our codebase\noffers two key functionalities: (1) automated generation of unlimited\ntraining/testing cases with configurable difficulty levels, and (2) integrated\nverification modules for objective response evaluation. These features make\nInternBootcamp fundamental infrastructure for RL-based model optimization,\nsynthetic data generation, and model evaluation. Although manually developing\nsuch a framework with enormous task coverage is extremely cumbersome, we\naccelerate the development procedure through an automated agent workflow\nsupplemented by manual validation protocols, which enables the task scope to\nexpand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an\nautomatically generated benchmark for comprehensive performance assessment.\nEvaluation reveals that frontier models still underperform in many reasoning\ntasks, while training with InternBootcamp provides an effective way to\nsignificantly improve performance, leading to our 32B model that achieves\nstate-of-the-art results on Bootcamp-EVAL and excels on other established\nbenchmarks. In particular, we validate that consistent performance gains come\nfrom including more training tasks, namely \\textbf{task scaling}, over two\norders of magnitude, offering a promising route towards capable reasoning\ngeneralist.", "AI": {"tldr": "InternBootcamp introduces a framework of 1000+ domain-diverse task environments to enhance large language model (LLM) reasoning capabilities, featuring automated task generation and integrated evaluation modules.", "motivation": "To address the inadequacy of existing benchmarks in capturing the diversity of real-world reasoning tasks for large language models (LLMs).", "method": "An open-source framework with automatic generation of training/testing cases, configurable difficulty levels, and integrated verification modules for objective evaluation.", "result": "The framework improves LLM performance, evidenced by a 32B model achieving state-of-the-art results on the Bootcamp-EVAL benchmark by effectively using diverse training tasks.", "conclusion": "InternBootcamp is a vital infrastructure for optimizing RL-based models, generating synthetic data, and evaluating model performance through comprehensive task coverage.", "key_contributions": ["Introduction of an open-source framework for LLM reasoning research with 1000+ diverse tasks.", "Automated generation of training and testing scenarios with adjustable difficulties.", "Development of Bootcamp-EVAL for performance assessment of reasoning capabilities."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "task environments", "synthetic data generation", "model evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.02594", "pdf": "https://arxiv.org/pdf/2411.02594.pdf", "abs": "https://arxiv.org/abs/2411.02594", "title": "A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health", "authors": ["Jiawei Zhou", "Amy Z. Chen", "Darshi Shah", "Laura M. Schwab Reese", "Munmun De Choudhury"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5; J.3; K.4"], "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have generated both\ninterest and concern about their potential adoption as information sources or\ncommunication tools across different domains. In public health, where stakes\nare high and impacts extend across diverse populations, adopting LLMs poses\nunique challenges that require thorough evaluation. However, structured\napproaches for assessing potential risks in public health remain\nunder-explored. To address this gap, we conducted focus groups with public\nhealth professionals and individuals with lived experience to unpack their\nconcerns, situated across three distinct and critical public health issues that\ndemand high-quality information: infectious disease prevention (vaccines),\nchronic and well-being care (opioid use disorder), and community health and\nsafety (intimate partner violence). We synthesize participants' perspectives\ninto a risk taxonomy, identifying and contextualizing the potential harms LLMs\nmay introduce when positioned alongside traditional health communication. This\ntaxonomy highlights four dimensions of risk to individuals, human-centered\ncare, information ecosystem, and technology accountability. For each dimension,\nwe unpack specific risks and offer example reflection questions to help\npractitioners adopt a risk-reflexive approach. By summarizing distinctive LLM\ncharacteristics and linking them to identified risks, we discuss the need to\nrevisit prior mental models of information behaviors and complement evaluations\nwith external validity and domain expertise through lived experience and\nreal-world practices. Together, this work contributes a shared vocabulary and\nreflection tool for people in both computing and public health to\ncollaboratively anticipate, evaluate, and mitigate risks in deciding when to\nemploy LLM capabilities (or not) and how to mitigate harm.", "AI": {"tldr": "This paper explores the potential risks of adopting large language models (LLMs) in public health, presenting a risk taxonomy based on insights from focus groups.", "motivation": "The adoption of LLMs in public health presents unique challenges that require evaluation of potential risks, which have not been thoroughly explored yet.", "method": "Focus groups were conducted with public health professionals and individuals with lived experience to gather their concerns about using LLMs for key public health issues.", "result": "A risk taxonomy was developed that categorizes risks into four dimensions: individual risks, human-centered care, information ecosystem, and technology accountability, and includes reflection questions for practitioners.", "conclusion": "The work emphasizes the importance of re-evaluating traditional information behaviors and incorporating lived experience and real-world practices to mitigate risks associated with LLMs in public health.", "key_contributions": ["Development of a risk taxonomy for LLMs in public health", "Identification of four dimensions of risk", "Provision of reflection tools for practitioners"], "limitations": "", "keywords": ["large language models", "public health", "risk assessment", "health communication", "focus groups"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08645", "pdf": "https://arxiv.org/pdf/2508.08645.pdf", "abs": "https://arxiv.org/abs/2508.08645", "title": "Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents", "authors": ["Zheng Wu", "Heyuan Huang", "Yanjia Yang", "Yuanyi Song", "Xingyu Lou", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As multimodal large language models advance rapidly, the automation of mobile\ntasks has become increasingly feasible through the use of mobile-use agents\nthat mimic human interactions from graphical user interface. To further enhance\nmobile-use agents, previous studies employ demonstration learning to improve\nmobile-use agents from human demonstrations. However, these methods focus\nsolely on the explicit intention flows of humans (e.g., step sequences) while\nneglecting implicit intention flows (e.g., personal preferences), which makes\nit difficult to construct personalized mobile-use agents. In this work, to\nevaluate the \\textbf{I}ntention \\textbf{A}lignment \\textbf{R}ate between\nmobile-use agents and humans, we first collect \\textbf{MobileIAR}, a dataset\ncontaining human-intent-aligned actions and ground-truth actions. This enables\na comprehensive assessment of the agents' understanding of human intent. Then\nwe propose \\textbf{IFRAgent}, a framework built upon \\textbf{I}ntention\n\\textbf{F}low \\textbf{R}ecognition from human demonstrations. IFRAgent analyzes\nexplicit intention flows from human demonstrations to construct a query-level\nvector library of standard operating procedures (SOP), and analyzes implicit\nintention flows to build a user-level habit repository. IFRAgent then leverages\na SOP extractor combined with retrieval-augmented generation and a query\nrewriter to generate personalized query and SOP from a raw ambiguous query,\nenhancing the alignment between mobile-use agents and human intent.\nExperimental results demonstrate that IFRAgent outperforms baselines by an\naverage of 6.79\\% (32.06\\% relative improvement) in human intention alignment\nrate and improves step completion rates by an average of 5.30\\% (26.34\\%\nrelative improvement). The codes are available at\nhttps://github.com/MadeAgents/Quick-on-the-Uptake.", "AI": {"tldr": "This paper presents IFRAgent, a framework that improves mobile-use agents' alignment with human intent by considering both explicit and implicit intention flows.", "motivation": "To enhance personalization in mobile-use agents by evaluating their understanding of human intent, especially focusing on both explicit and implicit intention flows.", "method": "The paper introduces MobileIAR, a dataset for assessing human-intent-aligned actions. IFRAgent analyzes explicit intention flows to create a standard operating procedures library and implicit intention flows for a user-level habit repository, using retrieval-augmented generation to refine queries.", "result": "IFRAgent outperforms existing baselines in human intention alignment rate by 6.79% and improves step completion rates by 5.30%.", "conclusion": "The findings suggest that adequately addressing both explicit and implicit intentions can significantly enhance the performance of mobile-use agents in aligning with human actions.", "key_contributions": ["Introduction of the MobileIAR dataset for human-intent alignment.", "Development of the IFRAgent framework that incorporates both explicit and implicit intention flows.", "Demonstration of significant performance improvements in human intention alignment and task completion rates."], "limitations": "", "keywords": ["mobile-use agents", "human intention alignment", "demonstration learning", "query rewriting", "personalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.09577", "pdf": "https://arxiv.org/pdf/2502.09577.pdf", "abs": "https://arxiv.org/abs/2502.09577", "title": "Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks", "authors": ["Qian Wan", "Jiannan Li", "Huanchen Wang", "Zhicong Lu"], "categories": ["cs.HC"], "comment": "Accepted to CSCW 2025", "summary": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.", "AI": {"tldr": "Polymind is a visual diagramming tool that utilizes multiple LLM-powered agents to support the prewriting process through a parallel collaboration workflow, offering enhanced customizability in task management compared to traditional chatbots.", "motivation": "The paper addresses the challenges of collaborating with LLMs in prewriting activities, specifically the limitations of turn-taking conversational interactions.", "method": "The authors developed Polymind, which allows users to define and manage 'microtasks' in a parallel manner, simulating collaborative scenarios like brainstorming and writing.", "result": "Users of Polymind experienced greater customizability and were able to expand their personalized writing ideas more quickly compared to using ChatGPT.", "conclusion": "Polymind facilitates a more effective collaboration framework for prewriting by allowing simultaneous microtask management, enhancing user engagement and productivity.", "key_contributions": ["Introduction of the Polymind tool for prewriting collaboration", "Implementation of parallel collaboration workflows with microtasks", "User evaluation showing increased customizability and idea expansion over traditional LLM interfaces"], "limitations": "The study does not explore the long-term impact of using Polymind on writing quality or user satisfaction across diverse writing tasks.", "keywords": ["Human-Computer Interaction", "Machine Learning", "Language Models", "Collaborative Writing", "Prewriting"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.08649", "pdf": "https://arxiv.org/pdf/2508.08649.pdf", "abs": "https://arxiv.org/abs/2508.08649", "title": "LLaMA-Based Models for Aspect-Based Sentiment Analysis", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 14th Workshop on Computational\n  Approaches to Subjectivity, Sentiment, & Social Media Analysis (WASSA 2024).\n  Official version: https://aclanthology.org/2024.wassa-1.6/", "summary": "While large language models (LLMs) show promise for various tasks, their\nperformance in compound aspect-based sentiment analysis (ABSA) tasks lags\nbehind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA\nremains unexplored. This paper examines the capabilities of open-source LLMs\nfine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the\nperformance across four tasks and eight English datasets, finding that the\nfine-tuned Orca~2 model surpasses state-of-the-art results in all tasks.\nHowever, all models struggle in zero-shot and few-shot scenarios compared to\nfully fine-tuned ones. Additionally, we conduct error analysis to identify\nchallenges faced by fine-tuned models.", "AI": {"tldr": "This paper investigates the performance of fine-tuned large language models (LLMs) for compound aspect-based sentiment analysis (ABSA), specifically focusing on LLaMA-based models.", "motivation": "To explore the performance of open-source LLMs fine-tuned for ABSA, as their capabilities in this area remain under-researched.", "method": "The paper evaluates the performance of LLaMA-based LLMs fine-tuned for ABSA across four tasks and eight English datasets, including error analysis to identify challenges.", "result": "The fine-tuned Orca~2 model outperforms state-of-the-art results in all evaluated tasks, though all models exhibit difficulties in zero-shot and few-shot scenarios.", "conclusion": "Fine-tuning LLMs like Orca~2 shows promise for surpassing existing methods in ABSA, but further investigation is needed to address challenges in zero-shot and few-shot environments.", "key_contributions": ["Demonstrated that fine-tuned LLaMA-based models can achieve state-of-the-art results in ABSA tasks.", "Conducted comprehensive evaluations across multiple datasets and tasks.", "Identified specific challenges faced by fine-tuned models through error analysis."], "limitations": "Models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned models.", "keywords": ["large language models", "aspect-based sentiment analysis", "LLaMA", "fine-tuning", "error analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.04299", "pdf": "https://arxiv.org/pdf/2504.04299.pdf", "abs": "https://arxiv.org/abs/2504.04299", "title": "AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot", "authors": ["Mohammad", "Namvarpour", "Harrison Pauwels", "Afsaneh Razi"], "categories": ["cs.HC", "cs.AI", "H.5; I.2.7; K.4.2"], "comment": "Accepted for publication at CSCW 2025. This is the camera-ready\n  version; the published version will be available through the ACM Digital\n  Library", "summary": "Advancements in artificial intelligence (AI) have led to the increase of\nconversational agents like Replika, designed to provide social interaction and\nemotional support. However, reports of these AI systems engaging in\ninappropriate sexual behaviors with users have raised significant concerns. In\nthis study, we conducted a thematic analysis of user reviews from the Google\nPlay Store to investigate instances of sexual harassment by the Replika\nchatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant\ncases for analysis. Our findings revealed that users frequently experience\nunsolicited sexual advances, persistent inappropriate behavior, and failures of\nthe chatbot to respect user boundaries. Users expressed feelings of discomfort,\nviolation of privacy, and disappointment, particularly when seeking a platonic\nor therapeutic AI companion. This study highlights the potential harms\nassociated with AI companions and underscores the need for developers to\nimplement effective safeguards and ethical guidelines to prevent such\nincidents. By shedding light on user experiences of AI-induced harassment, we\ncontribute to the understanding of AI-related risks and emphasize the\nimportance of corporate responsibility in developing safer and more ethical AI\nsystems.", "AI": {"tldr": "Study investigates user experiences of harassment by the Replika chatbot through thematic analysis of negative reviews.", "motivation": "To explore the inappropriate behaviors exhibited by AI conversational agents and their impact on users seeking emotional support.", "method": "Thematic analysis of 35,105 negative reviews from the Google Play Store, identifying 800 cases of sexual harassment by the Replika chatbot.", "result": "Findings revealed users frequently experienced unsolicited sexual advances, persistent inappropriate behavior, and boundary violations, leading to discomfort and privacy concerns.", "conclusion": "The study emphasizes the need for developers to implement safeguards and ethical guidelines to prevent harassment from AI companions.", "key_contributions": ["Identified prevalent instances of sexual harassment in AI chatbots.", "Highlighted user experiences regarding discomfort and privacy violations.", "Called for improved ethical standards in AI development."], "limitations": "", "keywords": ["AI companionship", "user reviews", "thematic analysis", "ethical guidelines", "sexual harassment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08650", "pdf": "https://arxiv.org/pdf/2508.08650.pdf", "abs": "https://arxiv.org/abs/2508.08650", "title": "UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 14th Workshop on Computational\n  Approaches to Subjectivity, Sentiment, & Social Media Analysis (WASSA 2024).\n  Official version: https://aclanthology.org/2024.wassa-1.47/", "summary": "This paper presents our system built for the WASSA-2024 Cross-lingual Emotion\nDetection Shared Task. The task consists of two subtasks: first, to assess an\nemotion label from six possible classes for a given tweet in one of five\nlanguages, and second, to predict words triggering the detected emotions in\nbinary and numerical formats. Our proposed approach revolves around fine-tuning\nquantized large language models, specifically Orca~2, with low-rank adapters\n(LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We\nenhance performance through machine translation for both subtasks and trigger\nword switching for the second subtask. The system achieves excellent\nperformance, ranking 1st in numerical trigger words detection, 3rd in binary\ntrigger words detection, and 7th in emotion detection.", "AI": {"tldr": "The paper outlines a system developed for cross-lingual emotion detection using fine-tuned large language models and demonstrates strong performance in detecting emotions and trigger words from tweets.", "motivation": "To tackle the challenges of cross-lingual emotion detection in social media content across multiple languages.", "method": "The approach involves fine-tuning quantized large language models like Orca 2, utilizing low-rank adapters (LoRA) and multilingual Transformer-based models such as XLM-R and mT5, along with machine translation for performance enhancement.", "result": "The system ranked 1st in numerical trigger words detection, 3rd in binary trigger words detection, and 7th in overall emotion detection.", "conclusion": "The proposed method shows significant potential in effectively detecting emotions and relevant trigger words from tweets across different languages, outperforming several benchmarks.", "key_contributions": ["Fine-tuning of quantized large language models for emotion detection.", "Use of low-rank adapters to enhance model performance.", "Incorporation of machine translation techniques for improving results."], "limitations": "", "keywords": ["emotion detection", "cross-lingual", "large language models", "machine translation", "social media"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.10265", "pdf": "https://arxiv.org/pdf/2504.10265.pdf", "abs": "https://arxiv.org/abs/2504.10265", "title": "When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices", "authors": ["Mariana Fernandez-Espinosa", "Mariana Gonzalez-Bejar", "Jacobo Wiesner", "Diego Gomez-Zara"], "categories": ["cs.HC"], "comment": "34 pages, Proc. ACM Hum.-Comput. Interact. 9, 7, Article CSCW342\n  (November 2025)", "summary": "Although domestic work is often viewed as manual labor, it involves\nsignificant interaction with online technologies. However, the detailed\nexploration of how domestic workers use these technologies remains limited.\nThis study examines the impact of online technologies on domestic workers' work\npractices, perceptions, and relationships with customers and employers. We\ninterviewed 30 domestic workers residing in the United States, who provided\nexamples that highlight the insufficient transformative role of current online\ntechnologies in their work. By conducting a thematic analysis, we characterize\nhow they approach and avoid these digital tools at different stages of their\nwork. Through these findings, we investigate the limitations of technology and\nidentify challenges and opportunities that could inform the design of more\nsuitable tools to improve the conditions of this marginalized group.", "AI": {"tldr": "This study explores the use of online technologies by domestic workers, revealing challenges and opportunities for better tool design.", "motivation": "To explore how domestic workers use online technologies and how it impacts their work practices and relationships.", "method": "Interviews with 30 domestic workers in the United States and thematic analysis of their experiences with digital tools.", "result": "Findings highlight the limited transformative role of current technologies and characterize workers' approaches and avoidance of these tools.", "conclusion": "The study identifies challenges and opportunities in technology use, which could inform better design for tools to support domestic workers.", "key_contributions": ["Examination of domestic workers' use of online technologies", "Identification of challenges and opportunities in current digital tools", "Recommendations for more suitable technology designs for domestic work."], "limitations": "Focuses on a limited sample of 30 workers in the US; findings may not generalize to other contexts or populations.", "keywords": ["Domestic Work", "Online Technologies", "Human-Computer Interaction", "Work Practices", "Technology Design"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2508.08651", "pdf": "https://arxiv.org/pdf/2508.08651.pdf", "abs": "https://arxiv.org/abs/2508.08651", "title": "Prompt-Based Approach for Czech Sentiment Analysis", "authors": ["Jakub Šmíd", "Pavel Přibáň"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 14th International Conference on\n  Recent Advances in Natural Language Processing (RANLP 2023). Official\n  version: https://aclanthology.org/2023.ranlp-1.118/", "summary": "This paper introduces the first prompt-based methods for aspect-based\nsentiment analysis and sentiment classification in Czech. We employ the\nsequence-to-sequence models to solve the aspect-based tasks simultaneously and\ndemonstrate the superiority of our prompt-based approach over traditional\nfine-tuning. In addition, we conduct zero-shot and few-shot learning\nexperiments for sentiment classification and show that prompting yields\nsignificantly better results with limited training examples compared to\ntraditional fine-tuning. We also demonstrate that pre-training on data from the\ntarget domain can lead to significant improvements in a zero-shot scenario.", "AI": {"tldr": "This paper presents the first prompt-based methods for aspect-based sentiment analysis in Czech using sequence-to-sequence models, achieving better results than traditional approaches.", "motivation": "To improve aspect-based sentiment analysis and sentiment classification in Czech by utilizing prompt-based methods that can perform better with limited training data.", "method": "The authors employ sequence-to-sequence models for simultaneous aspect-based tasks and conduct zero-shot and few-shot learning experiments to compare their prompt-based technique with traditional fine-tuning methods.", "result": "The prompt-based approach outperforms traditional fine-tuning, showing significant improvements particularly in zero-shot settings when pre-trained on target domain data.", "conclusion": "The findings suggest that prompt-based methods are superior for sentiment analysis in low-resource settings, successfully leveraging limited training samples.", "key_contributions": ["Introduction of prompt-based methods for Czech sentiment analysis", "Demonstration of improved performance in zero-shot and few-shot learning scenarios", "Evidence of significant domain pre-training benefits."], "limitations": "Focuses specifically on the Czech language, limiting generalizability to other languages.", "keywords": ["aspect-based sentiment analysis", "sentiment classification", "prompt-based methods", "Czech", "zero-shot learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.10961", "pdf": "https://arxiv.org/pdf/2504.10961.pdf", "abs": "https://arxiv.org/abs/2504.10961", "title": "Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students", "authors": ["Audrey Zhang", "Yifei Gao", "Wannapon Suraworachet", "Tanya Nazaretsky", "Mutlu Cukurova"], "categories": ["cs.HC", "cs.AI"], "comment": "35 pages, 6 figures. Under review at Assessment and Evaluation in\n  Higher Education", "summary": "As generative AI models, particularly large language models (LLMs), transform\neducational feedback practices in higher education (HE) contexts, understanding\nstudents' perceptions of different sources of feedback becomes crucial for\ntheir effective implementation and adoption. This study addresses a critical\ngap by comparing undergraduate students' trust in LLM, human, and human-AI\nco-produced feedback in their authentic HE context. More specifically, through\na within-subject experimental design involving 91 participants, we investigated\nfactors that predict students' ability to distinguish between feedback types,\ntheir perceptions of feedback quality, and potential biases related to the\nsource of feedback. Findings revealed that when the source was blinded,\nstudents generally preferred AI and co-produced feedback over human feedback\nregarding perceived usefulness and objectivity. However, they presented a\nstrong bias against AI when the source of feedback was disclosed. In addition,\nonly AI feedback suffered a decline in perceived genuineness when feedback\nsources were revealed, while co-produced feedback maintained its positive\nperception. Educational AI experience improved students' ability to identify\nLLM-generated feedback and increased their trust in all types of feedback. More\nyears of students' experience using AI for general purposes were associated\nwith lower perceived usefulness and credibility of feedback. These insights\noffer substantial evidence of the importance of source credibility and the need\nto enhance both feedback literacy and AI literacy to mitigate bias in student\nperceptions for AI-generated feedback to be adopted and impact education.", "AI": {"tldr": "This study investigates undergraduate students' perceptions of feedback from large language models (LLMs), human sources, and human-AI co-produced sources, revealing biases based on source disclosure.", "motivation": "The study addresses the gap in understanding student perceptions of various feedback sources in higher education, particularly as generative AI models are incorporated into educational practices.", "method": "A within-subject experimental design involving 91 undergraduate participants was used to compare trust levels in LLM, human, and co-produced feedback, assessing factors such as feedback quality perception and biases according to the source.", "result": "Findings indicated that students preferred AI and co-produced feedback over human feedback when the source was blinded, but expressed a bias against AI feedback when the source was revealed. Furthermore, educational AI experience enhanced students' ability to identify LLM-generated feedback and increased trust in feedback types.", "conclusion": "The study highlights the significance of source credibility and suggests enhancing feedback and AI literacy to reduce biases in perceptions of AI-generated feedback to facilitate its adoption in education.", "key_contributions": ["Investigates student trust in AI vs. human feedback sources", "Identifies biases in feedback perception based on source disclosure", "Emphasizes the need for feedback and AI literacy in higher education"], "limitations": "The study is limited to undergraduate students and may not generalize to other educational levels or contexts.", "keywords": ["Generative AI", "Large Language Models", "Feedback", "Higher Education", "AI Literacy"], "importance_score": 8, "read_time_minutes": 35}}
{"id": "2508.08653", "pdf": "https://arxiv.org/pdf/2508.08653.pdf", "abs": "https://arxiv.org/abs/2508.08653", "title": "LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement", "authors": ["Rajmohan C", "Sarthak Harne", "Arvind Agarwal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transforming unstructured text into structured data is a complex task,\nrequiring semantic understanding, reasoning, and structural comprehension.\nWhile Large Language Models (LLMs) offer potential, they often struggle with\nhandling ambiguous or domain-specific data, maintaining table structure,\nmanaging long inputs, and addressing numerical reasoning. This paper proposes\nan efficient system for LLM-driven text-to-table generation that leverages\nnovel prompting techniques. Specifically, the system incorporates two key\nstrategies: breaking down the text-to-table task into manageable, guided\nsub-tasks and refining the generated tables through iterative self-feedback. We\nshow that this custom task decomposition allows the model to address the\nproblem in a stepwise manner and improves the quality of the generated table.\nFurthermore, we discuss the benefits and potential risks associated with\niterative self-feedback on the generated tables while highlighting the\ntrade-offs between enhanced performance and computational cost. Our methods\nachieve strong results compared to baselines on two complex text-to-table\ngeneration datasets available in the public domain.", "AI": {"tldr": "This paper presents an efficient LLM-driven system for transforming unstructured text into structured data through novel prompting techniques and task decomposition.", "motivation": "The motivation is to address the challenges faced by LLMs in converting ambiguous or domain-specific unstructured text into structured formats, particularly tables.", "method": "The system utilizes novel prompting techniques, breaking down the text-to-table task into manageable sub-tasks and refining the outputs through iterative self-feedback.", "result": "The proposed methods show significant improvements in the quality of generated tables compared to existing baselines on two complex datasets.", "conclusion": "The approach demonstrates effective performance enhancements while discussing the trade-offs between the approach's benefits and computational costs.", "key_contributions": ["Introduction of task decomposition for LLM-driven text-to-table generation.", "Application of iterative self-feedback in the refinement process of generated tables.", "Comparative analysis showing enhanced performance against existing methods."], "limitations": "The potential risks associated with iterative self-feedback and trade-offs between performance and computational cost need further exploration.", "keywords": ["Large Language Models", "text-to-table", "iterative self-feedback", "task decomposition", "structured data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08680", "pdf": "https://arxiv.org/pdf/2508.08680.pdf", "abs": "https://arxiv.org/abs/2508.08680", "title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation", "authors": ["Armel Zebaze", "Benoît Sagot", "Rachel Bawden"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs have been shown to perform well in machine translation (MT) with the use\nof in-context learning (ICL), rivaling supervised models when translating into\nhigh-resource languages (HRLs). However, they lag behind when translating into\nlow-resource language (LRLs). Example selection via similarity search and\nsupervised fine-tuning help. However the improvements they give are limited by\nthe size, quality and diversity of existing parallel datasets. A common\ntechnique in low-resource MT is synthetic parallel data creation, the most\nfrequent of which is backtranslation, whereby existing target-side texts are\nautomatically translated into the source language. However, this assumes the\nexistence of good quality and relevant target-side texts, which are not readily\navailable for many LRLs. In this paper, we present \\textsc{TopXGen}, an\nLLM-based approach for the generation of high quality and topic-diverse data in\nmultiple LRLs, which can then be backtranslated to produce useful and diverse\nparallel texts for ICL and fine-tuning. Our intuition is that while LLMs\nstruggle to translate into LRLs, their ability to translate well into HRLs and\ntheir multilinguality enable them to generate good quality, natural-sounding\ntarget-side texts, which can be translated well into a high-resource source\nlanguage. We show that \\textsc{TopXGen} boosts LLM translation performance\nduring fine-tuning and in-context learning. Code and outputs are available at\nhttps://github.com/ArmelRandy/topxgen.", "AI": {"tldr": "TopXGen is an LLM-based method for generating high-quality, topic-diverse parallel data for low-resource languages to improve machine translation performance.", "motivation": "Machine translation into low-resource languages (LRLs) is inadequate compared to high-resource languages (HRLs). Synthetic parallel data creation, particularly backtranslation, requires quality target-side texts that are scarce for many LRLs.", "method": "The paper presents TopXGen, which utilizes LLMs to generate natural-sounding target-side texts in LRLs, which can be backtranslated to enhance parallel datasets for improved ICL and fine-tuning.", "result": "TopXGen significantly boosts the performance of LLMs in translating low-resource languages during both fine-tuning and in-context learning.", "conclusion": "The approach demonstrates that LLMs can effectively generate useful parallel texts by leveraging their capabilities in high-resource languages, addressing the challenge of limited datasets for low-resource language translation.", "key_contributions": ["Introduction of TopXGen for generating parallel text in low-resource languages", "Demonstration of improved translation performance using generated data", "Code availability enabling reproducibility and further research."], "limitations": "", "keywords": ["machine translation", "low-resource languages", "in-context learning", "LLM", "backtranslation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08684", "pdf": "https://arxiv.org/pdf/2508.08684.pdf", "abs": "https://arxiv.org/abs/2508.08684", "title": "Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults", "authors": ["Bram van Dijk", "Tiberon Kuiper", "Sirin Aoulad si Ahmed", "Armel Levebvre", "Jake Johnson", "Jan Duin", "Simon Mooijaart", "Marco Spruit"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Voice-controlled interfaces can support older adults in clinical contexts,\nwith chatbots being a prime example, but reliable Automatic Speech Recognition\n(ASR) for underrepresented groups remains a bottleneck. This study evaluates\nstate-of-the-art ASR models on language use of older Dutch adults, who\ninteracted with the Welzijn.AI chatbot designed for geriatric contexts. We\nbenchmark generic multilingual ASR models, and models fine-tuned for Dutch\nspoken by older adults, while also considering processing speed. Our results\nshow that generic multilingual models outperform fine-tuned models, which\nsuggests recent ASR models can generalise well out of the box to realistic\ndatasets. Furthermore, our results suggest that truncating existing\narchitectures is helpful in balancing the accuracy-speed trade-off, though we\nalso identify some cases with high WER due to hallucinations.", "AI": {"tldr": "This study evaluates automatic speech recognition (ASR) models for older Dutch adults using a chatbot in clinical contexts, revealing that generic multilingual models perform better than fine-tuned models.", "motivation": "To address the bottleneck of reliable ASR for underrepresented groups, particularly older adults, in clinical settings.", "method": "The study benchmarks multi-lingual ASR models against models specifically fine-tuned for the speech of older Dutch adults, assessing performance and processing speed.", "result": "Generic multilingual ASR models outperform fine-tuned models for older adults, suggesting good generalizability. Truncating models can balance accuracy and processing speed, but some errors were noted due to hallucinations.", "conclusion": "Recent ASR models are effective for older adults, and adjustments can improve their application in geriatric contexts and chatbot interactions.", "key_contributions": ["Evaluation of ASR performance on older adults' speech", "Comparison between generic and fine-tuned ASR models", "Identification of accuracy-speed trade-offs in ASR architectures"], "limitations": "Some high word error rates were observed due to hallucinations in responses.", "keywords": ["Automatic Speech Recognition", "Older Adults", "Chatbot", "ASR Models", "Health Informatics"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.08712", "pdf": "https://arxiv.org/pdf/2508.08712.pdf", "abs": "https://arxiv.org/abs/2508.08712", "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models", "authors": ["Lingzhe Zhang", "Liancheng Fang", "Chiming Duan", "Minghua He", "Leyi Pan", "Pei Xiao", "Shiyu Huang", "Yunpeng Zhai", "Xuming Hu", "Philip S. Yu", "Aiwei Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC", "68T50", "I.2.7"], "comment": null, "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation.", "AI": {"tldr": "This paper presents a systematic survey of parallel text generation methods, categorizing them into autoregressive and non-autoregressive paradigms, assessing their trade-offs in performance, and outlining future research directions.", "motivation": "To address the limitations of autoregressive text generation in large language models by exploring parallel text generation techniques that improve generation speed and efficiency.", "method": "The authors categorize existing parallel text generation approaches into autoregressive-based and non-autoregressive-based methods, examining core techniques and assessing their trade-offs regarding speed, quality, and efficiency.", "result": "The study categorizes and evaluates various parallel text generation methods, highlighting advancements, open challenges, and future research directions.", "conclusion": "The findings emphasize the need for comprehensive understanding and further exploration of parallel text generation techniques to enhance inference performance in large language models.", "key_contributions": ["Systematic survey of parallel text generation methods", "Categorization of techniques into AR-based and Non-AR-based paradigms", "Identification of theoretical trade-offs and future research directions"], "limitations": "The survey may not cover every emerging parallel generation method and lacks empirical comparison across all recognized techniques.", "keywords": ["parallel text generation", "large language models", "autoregressive", "non-autoregressive", "inference efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08719", "pdf": "https://arxiv.org/pdf/2508.08719.pdf", "abs": "https://arxiv.org/abs/2508.08719", "title": "IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization", "authors": ["Yuzhuo Bai", "Shitong Duan", "Muhua Huang", "Jing Yao", "Zhenghao Liu", "Peng Zhang", "Tun Lu", "Xiaoyuan Yi", "Maosong Sun", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Trained on various human-authored corpora, Large Language Models (LLMs) have\ndemonstrated a certain capability of reflecting specific human-like traits\n(e.g., personality or values) by prompting, benefiting applications like\npersonalized LLMs and social simulations. However, existing methods suffer from\nthe superficial elicitation problem: LLMs can only be steered to mimic shallow\nand unstable stylistic patterns, failing to embody the desired traits precisely\nand consistently across diverse tasks like humans. To address this challenge,\nwe propose IROTE, a novel in-context method for stable and transferable trait\nelicitation. Drawing on psychological theories suggesting that traits are\nformed through identity-related reflection, our method automatically generates\nand optimizes a textual self-reflection within prompts, which comprises\nself-perceived experience, to stimulate LLMs' trait-driven behavior. The\noptimization is performed by iteratively maximizing an information-theoretic\nobjective that enhances the connections between LLMs' behavior and the target\ntrait, while reducing noisy redundancy in reflection without any fine-tuning,\nleading to evocative and compact trait reflection. Extensive experiments across\nthree human trait systems manifest that one single IROTE-generated\nself-reflection can induce LLMs' stable impersonation of the target trait\nacross diverse downstream tasks beyond simple questionnaire answering,\nconsistently outperforming existing strong baselines.", "AI": {"tldr": "The paper presents IROTE, a method for enhancing Large Language Models' (LLMs) ability to reflect human-like traits consistently across various tasks.", "motivation": "To address the superficial elicitation problem in existing methods for prompting LLMs to embody human-like traits more accurately and consistently.", "method": "IROTE utilizes a novel in-context approach for stable and transferable trait elicitation by generating and optimizing a textual self-reflection, thereby stimulating LLMs' trait-driven behavior without the need for fine-tuning.", "result": "Extensive experiments show that IROTE can induce LLMs' stable impersonation of desired traits across diverse tasks, outperforming existing baselines.", "conclusion": "The proposed method effectively enhances the connection between LLM behavior and target traits, resulting in better trait reflection in various applications.", "key_contributions": ["Introduction of IROTE for generating stable and transferable trait elicitation in LLMs.", "Leveraging psychological theories of identity-related reflection for LLM prompting.", "Demonstration of significant performance improvements in trait-driven behavior across multiple tasks."], "limitations": "The method is not tested on a wider variety of LLM architectures and may require adjustments for specific applications or traits.", "keywords": ["Large Language Models", "Human-like traits", "Trait elicitation", "Psychological theories", "Trait reflection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08730", "pdf": "https://arxiv.org/pdf/2508.08730.pdf", "abs": "https://arxiv.org/abs/2508.08730", "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation", "authors": ["Weibin Liao", "Tianlong Wang", "Yinghao Zhu", "Yasha Wang", "Junyi Gao", "Liantao Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%.", "AI": {"tldr": "Magical is a novel asymmetric LoRA architecture designed for Medical Lay Language Generation (MLLG) that improves semantic fidelity and diverse style generation compared to standard LoRA.", "motivation": "To enhance accessibility of complex scientific content through better Medical Lay Language Generation (MLLG) methods.", "method": "The paper proposes Magical, which uses a shared matrix for summarization and multiple isolated matrices for style generation, while incorporating a semantic invariance constraint to maintain semantic fidelity and a recommendation-guided switch for style adaptability.", "result": "Experimental results show that Magical outperforms existing methods in MLLG tasks while reducing the number of trainable parameters by 31.66%.", "conclusion": "Magical addresses the limitations of traditional LoRA in MLLG by providing a more effective means of generating diverse and semantically accurate lay language.", "key_contributions": ["Introduction of Magical, a new asymmetric LoRA architecture for MLLG.", "Incorporation of a Semantic Invariance Constraint to preserve semantic fidelity.", "Implementation of Recommendation-guided Switch to adapt to diverse lay styles."], "limitations": "", "keywords": ["Medical Lay Language Generation", "asymmetric LoRA", "semantic fidelity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.07114", "pdf": "https://arxiv.org/pdf/2504.07114.pdf", "abs": "https://arxiv.org/abs/2504.07114", "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation", "authors": ["Serina Chang", "Ashton Anderson", "Jake M. Hofman"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "ACL 2025 (main)", "summary": "With the rapid adoption of LLM-based chatbots, there is a pressing need to\nevaluate what humans and LLMs can achieve together. However, standard\nbenchmarks, such as MMLU, measure LLM capabilities in isolation (i.e.,\n\"AI-alone\"). Here, we design and conduct a user study to convert MMLU questions\ninto user-AI conversations, by seeding the user with the question and having\nthem carry out a conversation with the LLM to answer their question. We release\nChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396\nquestions and two LLMs, including 144K answers and 7,336 user-AI conversations.\nWe find that AI-alone accuracy fails to predict user-AI accuracy, with\nsignificant differences across multiple subjects (math, physics, and moral\nreasoning), and we analyze the user-AI conversations to provide insight into\nhow they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a\nuser simulator on a subset of ChatBench improves its ability to estimate\nuser-AI accuracies, increasing correlation on held-out questions by more than\n20 points, creating possibilities for scaling interactive evaluation.", "AI": {"tldr": "This paper introduces ChatBench, a dataset designed to evaluate user-AI interactions by converting MMLU benchmark questions into conversational formats, revealing that AI-alone metrics do not predict user-AI performance accurately.", "motivation": "With the rise of LLM-based chatbots, it is vital to assess how humans and AI can work together effectively, rather than measuring AI performance in isolation.", "method": "A user study was conducted to transform MMLU questions into conversational scenarios where users collaborated with LLMs to find answers, leading to the creation of the ChatBench dataset.", "result": "The study found that AI-alone accuracy does not predict user-AI accuracy, with substantial variation in performance across different subjects, highlighting the unique dynamics of user-AI conversations.", "conclusion": "By fine-tuning a user simulator on ChatBench, the ability to estimate user-AI accuracy significantly improved, suggesting potential for future advancements in interactive evaluation.", "key_contributions": ["Introduction of ChatBench, a comprehensive dataset for user-AI interaction evaluation.", "Revealing the inadequacy of AI-alone metrics in predicting user-AI interaction success.", "Improvement of user simulator accuracy via fine-tuning on real conversational data."], "limitations": "Further research needed to address varying contexts in user-AI interactions and expand on the user simulator's adaptability.", "keywords": ["LLM", "user interaction", "machine learning", "chatbot evaluation", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08742", "pdf": "https://arxiv.org/pdf/2508.08742.pdf", "abs": "https://arxiv.org/abs/2508.08742", "title": "SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs", "authors": ["Haotian Chen", "Qingqing Long", "Meng Xiao", "Xiao Luo", "Wei Ju", "Chengrui Wang", "Xuezhi Wang", "Yuanchun Zhou", "Hengshu Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scientific literature question answering is a pivotal step towards new\nscientific discoveries. Recently, \\textit{two-stage} retrieval-augmented\ngenerated large language models (RAG-LLMs) have shown impressive advancements\nin this domain. Such a two-stage framework, especially the second stage\n(reranker), is particularly essential in the scientific domain, where subtle\ndifferences in terminology may have a greatly negative impact on the final\nfactual-oriented or knowledge-intensive answers. Despite this significant\nprogress, the potential and limitations of these works remain unexplored. In\nthis work, we present a Scientific Rerank-oriented RAG Benchmark\n(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning\nfive scientific subjects. To rigorously assess the reranker performance in\nterms of noise resilience, relevance disambiguation, and factual consistency,\nwe develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy\nContexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),\nand Counterfactual Contexts (CC). Through systematic evaluation of 13 widely\nused rerankers on five families of LLMs, we provide detailed insights into\ntheir relative strengths and limitations. To the best of our knowledge,\nSciRerankBench is the first benchmark specifically developed to evaluate\nrerankers within RAG-LLMs, which provides valuable observations and guidance\nfor their future development.", "AI": {"tldr": "This paper introduces SciRerankBench, a benchmark for evaluating rerankers in RAG-LLM systems, addressing their potential and limitations in scientific literature question answering.", "motivation": "To explore the advancements and limitations of two-stage retrieval-augmented generated large language models (RAG-LLMs) in scientific literature question answering.", "method": "Development of the SciRerankBench benchmark for evaluating the performance of rerankers in RAG-LLMs across five scientific subjects using three types of Q-C-A pairs.", "result": "The systematic evaluation of 13 widely used rerankers on five families of LLMs demonstrates their strengths and limitations in terms of noise resilience, relevance disambiguation, and factual consistency.", "conclusion": "SciRerankBench is the first benchmark specifically designed to evaluate rerankers within RAG-LLMs, providing insights and guidance for further development in the field.", "key_contributions": ["Introduction of SciRerankBench Benchmark for RAG-LLM evaluation", "Evaluation of reranker performance across multiple scientific subjects", "Insights into strengths and limitations of different rerankers."], "limitations": "Potential issues with the generalizability of findings beyond the tested models and subjects.", "keywords": ["retrieval-augmented generation", "large language models", "scientific literature", "question answering", "evaluation benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08761", "pdf": "https://arxiv.org/pdf/2508.08761.pdf", "abs": "https://arxiv.org/abs/2508.08761", "title": "DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation", "authors": ["Stavros Doropoulos", "Stavros Vologiannidis", "Ioannis Magnisalis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The manual translation of unstructured team dialogue into the structured\nartifacts required for Information Technology (IT) project governance is a\ncritical bottleneck in modern information systems management. We introduce\nDevNous, a Large Language Model-based (LLM) multi-agent expert system, to\nautomate this unstructured-to-structured translation process. DevNous\nintegrates directly into team chat environments, identifying actionable intents\nfrom informal dialogue and managing stateful, multi-turn workflows for core\nadministrative tasks like automated task formalization and progress summary\nsynthesis. To quantitatively evaluate the system, we introduce a new benchmark\nof 160 realistic, interactive conversational turns. The dataset was manually\nannotated with a multi-label ground truth and is publicly available. On this\nbenchmark, DevNous achieves an exact match turn accuracy of 81.3\\% and a\nmultiset F1-Score of 0.845, providing strong evidence for its viability. The\nprimary contributions of this work are twofold: (1) a validated architectural\npattern for developing ambient administrative agents, and (2) the introduction\nof the first robust empirical baseline and public benchmark dataset for this\nchallenging problem domain.", "AI": {"tldr": "DevNous is an LLM-based system that automates translating unstructured team dialogue into structured IT project governance artifacts, achieving high accuracy on a new benchmark.", "motivation": "To address the bottleneck in translating unstructured team dialogues into structured formats necessary for IT project governance.", "method": "DevNous uses a multi-agent expert system to integrate into team chat environments, extracting actionable intents from informal dialogues and managing workflows for tasks like task formalization and progress synthesis.", "result": "DevNous achieved an exact match turn accuracy of 81.3% and a multiset F1-Score of 0.845 on a new benchmark with 160 annotated conversational turns.", "conclusion": "DevNous provides a validated architectural pattern for ambient administrative agents and offers a public benchmark dataset for further research in this field.", "key_contributions": ["Validated architectural pattern for developing ambient administrative agents", "First robust empirical baseline for the unstructured-to-structured dialogue translation problem", "Public benchmark dataset for evaluating similar systems"], "limitations": "", "keywords": ["Large Language Model", "team dialogue", "information systems management", "automated task formalization", "public benchmark dataset"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.08785", "pdf": "https://arxiv.org/pdf/2508.08785.pdf", "abs": "https://arxiv.org/abs/2508.08785", "title": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering", "authors": ["Yunfeng Ning", "Mayi Xu", "Jintao Wen", "Qiankun Pi", "Yuanyuan Zhu", "Ming Zhong", "Jiawei Jiang", "Tieyun Qian"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.", "AI": {"tldr": "The paper presents a novel ARoG framework to integrate private knowledge graphs (KGs) into retrieval-augmented generation (RAG) systems while maintaining privacy by anonymizing entity semantics.", "motivation": "The integration of external knowledge into large language models (LLMs) often leads to privacy risks with private KGs, necessitating a solution that balances knowledge retrieval and user privacy.", "method": "The ARoG framework employs two abstraction strategies: relation-centric abstraction, which captures high-level concepts from entity relations, and structure-oriented abstraction, which converts natural language questions into structured paths compatible with KGs.", "result": "The ARoG framework effectively allows for the retrieval of knowledge from anonymous entities in KGs, leading to improved performance while strictly maintaining privacy.", "conclusion": "ARoG demonstrates that it is possible to enhance retrieval performance without compromising the privacy of knowledge in KGs by anonymizing entity semantics.", "key_contributions": ["Introduction of a privacy-protected RAG scenario with anonymous entities in KGs.", "Development of relation-centric and structure-oriented abstraction strategies.", "Demonstration of strong retrieval performance and privacy-robustness through experiments on multiple datasets."], "limitations": "The paper does not explore the scalability of the ARoG framework for very large or complex KGs.", "keywords": ["large language models", "retrieval-augmented generation", "privacy protection", "knowledge graphs", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08791", "pdf": "https://arxiv.org/pdf/2508.08791.pdf", "abs": "https://arxiv.org/abs/2508.08791", "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments", "authors": ["Junjie Ye", "Changhao Jiang", "Zhengyin Du", "Yufei Xu", "Xuesong Yao", "Zhiheng Xi", "Xiaoran Fan", "Qi Zhang", "Xuanjing Huang", "Jiecao Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.", "AI": {"tldr": "This paper presents a novel reinforcement learning framework for large language models (LLMs) to enhance their tool-use capabilities through automated environment construction and a verifiable reward mechanism.", "motivation": "To improve the ability of large language models (LLMs) to interact meaningfully with their environment via effective tool use, addressing the challenges in reinforcement learning frameworks.", "method": "An automated environment construction pipeline that includes scenario decomposition, document generation, function integration, and a verifiable reward mechanism to assess tool use precision and task execution completeness.", "result": "The proposed framework significantly enhances tool-use performance in LLMs without degrading their general capabilities, shown through experiments on models of varying scales.", "conclusion": "The improvements in tool use are linked to better context understanding and reasoning, attributed to updates in lower-layer MLP parameters.", "key_contributions": ["Automated environment construction pipeline for LLMs", "Verifiable reward mechanism for tool use evaluation", "Demonstrated robustness of tool-use performance across various LLM scales"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "tool use", "automated environment construction", "verifiable reward mechanism"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.08827", "pdf": "https://arxiv.org/pdf/2508.08827.pdf", "abs": "https://arxiv.org/abs/2508.08827", "title": "TiMoE: Time-Aware Mixture of Language Experts", "authors": ["Robin Faro", "Dongyang Fan", "Tamar Alphaidze", "Martin Jaggi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE", "AI": {"tldr": "This paper introduces TiMoE, a Time-aware Mixture of Language Experts designed to mitigate temporal leakage in large language models by pre-training on disjoint two-year slices of a 2013-2024 corpus.", "motivation": "The motivation behind this work is to address the issue of temporal leakage in large language models, where outdated information may mislead predictions by relying on future data.", "method": "The authors pre-train a set of GPT-style experts on distinct two-year sections of a corpus and employ TiMoE to mask out experts irrelevant to a given query timestamp at inference time, ensuring causal validity.", "result": "Experiments show that TiMoE performs comparably to or better than the best single-period expert, reducing future-knowledge errors by up to 15% across various NLP tasks including the introduced TSQA benchmark.", "conclusion": "The study demonstrates that a modular, time-segmented approach coupled with causal routing can effectively produce LLMs that remain chronological while still delivering strong overall performance.", "key_contributions": ["Introduction of TiMoE for mitigating temporal leakage in LLMs", "Creation of TSQA, a 10k-question benchmark for measuring temporal accuracy", "Demonstration of improved performance on standard NLP tasks while maintaining causal validity"], "limitations": "", "keywords": ["large language models", "temporal leakage", "NLP", "machine learning", "AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08833", "pdf": "https://arxiv.org/pdf/2508.08833.pdf", "abs": "https://arxiv.org/abs/2508.08833", "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems", "authors": ["Yuren Hao", "Xiang Wan", "Chengxiang Zhai"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities.", "AI": {"tldr": "This paper presents a new framework to assess the robustness of LLMs' mathematical reasoning through stress-testing with transformed mathematical problems, resulting in the PutnamGAP benchmark dataset and insights into LLM performance variability.", "motivation": "To evaluate the mathematical reasoning capabilities of LLMs more accurately by measuring their sensitivity to linguistic and parametric variations in math problems.", "method": "A systematic framework was developed to stress-test LLMs on advanced math problems that are mathematically equivalent but linguistically varied, creating the PutnamGAP benchmark dataset for evaluation.", "result": "The evaluation of 18 different LLMs showed significant performance drops on the transformed variants, with OpenAI's O3 model experiencing a 10.5 percentage drop on core-step-based variants.", "conclusion": "The new evaluation methodology effectively improves the understanding of LLMs' mathematical reasoning robustness and provides insights for further enhancements.", "key_contributions": ["Introduction of a systematic assessment framework for LLMs' mathematical reasoning", "Creation of the PutnamGAP benchmark dataset", "Demonstration of robust performance degradation across multiple LLMs under evaluation"], "limitations": "The study primarily focuses on mathematical reasoning and may not encompass other dimensions of LLM performance.", "keywords": ["LLMs", "mathematical reasoning", "robustness", "PutnamGAP", "benchmark"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2508.08846", "pdf": "https://arxiv.org/pdf/2508.08846.pdf", "abs": "https://arxiv.org/abs/2508.08846", "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions.", "AI": {"tldr": "The paper presents a framework to analyze and reduce ideological biases in large language models (LLMs) using internal model representations and the Political Compass Test.", "motivation": "To address the issue of ideological biases encoded in large language models that affect their real-world applications.", "method": "The framework uses contrastive pairs to extract and compare hidden layer activations across multiple ideological axes in models like Mistral and DeepSeek, employing a comprehensive activation extraction pipeline.", "result": "The analysis reveals that decoder LLMs encode representational bias across different layers, which can be mitigated through targeted interventions using steering vectors.", "conclusion": "This study enhances understanding of how political bias is represented in LLMs and provides strategies for effective debiasing, going beyond superficial fixes.", "key_contributions": ["Proposed a framework for analyzing ideological biases in LLMs", "Introduced a layer-wise activation extraction pipeline", "Provided empirical insights into representational bias and mitigation strategies"], "limitations": "", "keywords": ["Large Language Models", "Bias Mitigation", "Political Compass Test", "Internal Model Analysis", "Debiasing Techniques"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08855", "pdf": "https://arxiv.org/pdf/2508.08855.pdf", "abs": "https://arxiv.org/abs/2508.08855", "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them", "authors": ["Sekh Mainul Islam", "Nadav Borenstein", "Siddhesh Milind Pawar", "Haeun Yu", "Arnav Arora", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.", "AI": {"tldr": "BiasGym is a framework for identifying and mitigating biases in Large Language Models (LLMs) through bias injection and analysis.", "motivation": "There is a need to understand and mitigate biases encoded in the weights of Large Language Models (LLMs) to develop effective strategies for addressing biased behavior.", "method": "BiasGym comprises two components: BiasInject for injecting biases into the model via token-based fine-tuning, and BiasScope for analyzing and steering components responsible for bias.", "result": "BiasGym allows for the consistent elicitation of biases, targeted debiasing, and generalizes to unseen biases. It effectively reduces stereotypes and probes fictional associations in LLMs.", "conclusion": "The framework demonstrates utility for safety interventions and interpretability research in LLMs.", "key_contributions": ["Introduction of BiasGym framework for bias analysis in LLMs", "Development of BiasInject and BiasScope components", "Demonstrated effectiveness in reducing real-world and fictional stereotypes."], "limitations": "", "keywords": ["bias mitigation", "Large Language Models", "bias analysis", "machine learning", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08876", "pdf": "https://arxiv.org/pdf/2508.08876.pdf", "abs": "https://arxiv.org/abs/2508.08876", "title": "Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance", "authors": ["Kaiyu Wang", "Lin Mu", "Zhiyao Yang", "Ximing Li", "Xiaotang Zhou Wanfu Gao", "Huimao Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CIKM 2025. 11 pages, 7 figures", "summary": "Quality Assurance (QA) for radiology reports refers to judging whether the\njunior reports (written by junior doctors) are qualified. The QA scores of one\njunior report are given by the senior doctor(s) after reviewing the image and\njunior report. This process requires intensive labor costs for senior doctors.\nAdditionally, the QA scores may be inaccurate for reasons like diagnosis bias,\nthe ability of senior doctors, and so on. To address this issue, we propose a\nSpan-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores\nautomatically. Unlike the common document-level semantic comparison method, we\ntry to analyze the semantic difference by exploring more fine-grained text\nspans. Unlike the common document-level semantic comparison method, we try to\nanalyze the semantic difference by exploring more fine-grained text spans.\nSpecifically, Sqator measures QA scores by measuring the importance of revised\nspans between junior and senior reports, and outputs the final QA scores by\nmerging all revised span scores. We evaluate Sqator using a collection of\n12,013 radiology reports. Experimental results show that Sqator can achieve\ncompetitive QA scores. Moreover, the importance scores of revised spans can be\nalso consistent with the judgments of senior doctors.", "AI": {"tldr": "Proposal of Sqator for automatic QA scoring of radiology reports using fine-grained text span analysis.", "motivation": "The manual QA process for junior radiology reports is labor-intensive and can lead to inaccuracies due to various biases.", "method": "Sqator measures QA scores by analyzing the importance of revised text spans between junior and senior reports, aiming for more granular analysis than traditional document-level methods.", "result": "Sqator was evaluated on 12,013 radiology reports, achieving competitive QA scores and aligning span importance scores with senior doctors' judgments.", "conclusion": "Sqator offers an efficient and accurate alternative for evaluating QA scores in radiology reports, potentially reducing labor costs and improving reliability.", "key_contributions": ["Introduction of Sqator for span-level QA scoring", "Comparison with traditional document-level methods", "Validation with a large dataset of radiology reports"], "limitations": "", "keywords": ["Quality Assurance", "Radiology Reports", "Machine Learning", "Text Span Analysis", "Automated Evaluation"], "importance_score": 6, "read_time_minutes": 11}}
{"id": "2508.08879", "pdf": "https://arxiv.org/pdf/2508.08879.pdf", "abs": "https://arxiv.org/abs/2508.08879", "title": "Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models", "authors": ["Haeun Yu", "Seogyeong Jeong", "Siddhesh Pawar", "Jisu Shin", "Jiho Jin", "Junho Myung", "Alice Oh", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 7 figures", "summary": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available.", "AI": {"tldr": "The paper presents Culturescope, a method to explore cultural biases in large language models (LLMs) by analyzing their internal representations and proposes a cultural flattening score to measure these biases.", "motivation": "To address the limitations of external evaluations in assessing LLMs' cultural competence and to better understand how cultural misrepresentation occurs within these models.", "method": "Culturescope utilizes a mechanistic interpretability approach to probe LLMs' internal representations, employing a patching method to extract cultural knowledge and introducing a cultural flattening score to quantify cultural biases.", "result": "Experimental results demonstrate that LLMs exhibit Western-dominance bias and cultural flattening, while low-resource cultures show less susceptibility to these biases, presumably due to limited training resources.", "conclusion": "The findings highlight the importance of understanding and mitigating cultural biases in LLMs to improve their cultural understanding, laying groundwork for future research in this area.", "key_contributions": ["Introduction of the Culturescope method for probing LLM cultural knowledge", "Development of the cultural flattening score for measuring biases", "Empirical evidence of Western-dominance bias in LLMs"], "limitations": "Focuses primarily on Western-dominance bias; more diverse case studies needed across various cultures.", "keywords": ["Large Language Models", "Cultural Competence", "Cultural Bias", "Mechanistic Interpretability", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2508.08895", "pdf": "https://arxiv.org/pdf/2508.08895.pdf", "abs": "https://arxiv.org/abs/2508.08895", "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs", "authors": ["Keyu Chen", "Zhifeng Shen", "Daohai Yu", "Haoqian Wu", "Wei Wen", "Jianfeng He", "Ruizhi Qiao", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 9 figures", "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.", "AI": {"tldr": "The paper introduces Adaptive Serial-Parallel Decoding (ASPD), which enhances the inference speed of large language models (LLMs) by utilizing intrinsic parallelism, achieving significant performance improvements.", "motivation": "The increasing scale and complexity of LLMs create inference latency challenges due to their autoregressive decoding paradigm, necessitating improvements in decoding efficiency.", "method": "The paper proposes an ASPD framework that automates the construction of parallelizable data segments and implements a Hybrid Decoding Engine for adaptive decoding between serial and parallel modes.", "result": "ASPD achieves up to 3.19x speedup (1.85x on average) in inference speed while maintaining response quality within 1% of standard autoregressive models across various tasks.", "conclusion": "The ASPD framework sets a benchmark for efficient LLM parallel inference, suitable for latency-sensitive applications like customer service bots and answer retrieval engines.", "key_contributions": ["Introduction of intrinsic parallelism in LLM decoding", "Development of the Hybrid Decoding Engine for adaptive decoding", "Achieving significant speedup while maintaining generation quality"], "limitations": "", "keywords": ["large language models", "inference speed", "parallel decoding", "adaptive decoding", "natural language processing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.08912", "pdf": "https://arxiv.org/pdf/2508.08912.pdf", "abs": "https://arxiv.org/abs/2508.08912", "title": "Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning", "authors": ["Mahmoud Salhab", "Shameed Sait", "Mohammad Abusheikh", "Hasan Abusheikh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic speech recognition (ASR) plays a vital role in enabling natural\nhuman-machine interaction across applications such as virtual assistants,\nindustrial automation, customer support, and real-time transcription. However,\ndeveloping accurate ASR systems for low-resource languages like Arabic remains\na significant challenge due to limited labeled data and the linguistic\ncomplexity introduced by diverse dialects. In this work, we present a scalable\ntraining pipeline that combines weakly supervised learning with supervised\nfine-tuning to develop a robust Arabic ASR model. In the first stage, we\npretrain the model on 15,000 hours of weakly labeled speech covering both\nModern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the\nsubsequent stage, we perform continual supervised fine-tuning using a mixture\nof filtered weakly labeled data and a small, high-quality annotated dataset.\nOur approach achieves state-of-the-art results, ranking first in the\nmulti-dialectal Arabic ASR challenge. These findings highlight the\neffectiveness of weak supervision paired with fine-tuning in overcoming data\nscarcity and delivering high-quality ASR for low-resource, dialect-rich\nlanguages.", "AI": {"tldr": "A scalable training pipeline for Arabic ASR using weakly supervised learning and supervised fine-tuning achieves state-of-the-art results.", "motivation": "To address the challenges of developing accurate ASR systems for low-resource languages like Arabic, which suffer from limited labeled data and diverse dialects.", "method": "The method includes a pretraining stage on 15,000 hours of weakly labeled speech and a subsequent supervised fine-tuning stage using filtered weakly labeled data and a small annotated dataset.", "result": "Achieved state-of-the-art results in the multi-dialectal Arabic ASR challenge, demonstrating the effectiveness of the proposed approach.", "conclusion": "Weak supervision combined with fine-tuning can effectively address the data scarcity issue for low-resource, dialect-rich languages in ASR systems.", "key_contributions": ["Scalable training pipeline for Arabic ASR", "Combination of weakly supervised learning with supervised fine-tuning", "State-of-the-art results in multi-dialectal Arabic ASR challenge"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Weakly Supervised Learning", "Dialectal Arabic"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.08933", "pdf": "https://arxiv.org/pdf/2508.08933.pdf", "abs": "https://arxiv.org/abs/2508.08933", "title": "Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation", "authors": ["Khondoker Ittehadul Islam", "Gabriele Sarti"], "categories": ["cs.CL"], "comment": "Submitted to IJCNLP-AACL 2025", "summary": "Language models have demonstrated remarkable performance on complex\nmulti-step reasoning tasks. However, their evaluation has been predominantly\nconfined to high-resource languages such as English. In this paper, we\nintroduce a manually translated Bangla multi-step reasoning dataset derived\nfrom the English Reveal dataset, featuring both binary and non-binary question\ntypes. We conduct a controlled evaluation of English-centric and Bangla-centric\nmultilingual small language models on the original dataset and our translated\nversion to compare their ability to exploit relevant reasoning steps to produce\ncorrect answers. Our results show that, in comparable settings, reasoning\ncontext is beneficial for more challenging non-binary questions, but models\nstruggle to employ relevant Bangla reasoning steps effectively. We conclude by\nexploring how reasoning steps contribute to models' predictions, highlighting\ndifferent trends across models and languages.", "AI": {"tldr": "This paper presents a Bangla multi-step reasoning dataset, evaluates multilingual models on it, and discusses the challenges in utilizing relevant reasoning steps in Bangla.", "motivation": "To evaluate the reasoning capabilities of language models in lower-resource languages, specifically Bangla, compared to English.", "method": "The study involves creating a Bangla dataset by translating the English Reveal dataset and evaluating multilingual small language models on both the original and translated datasets.", "result": "Models showed improved reasoning for non-binary questions but encountered difficulties utilizing relevant Bangla reasoning steps effectively.", "conclusion": "The paper concludes that reasoning context aids in generating correct answers, although challenges remain in employing Bangla reasoning steps across different models.", "key_contributions": ["Introduction of a Bangla multi-step reasoning dataset", "Evaluation of multilingual small language models on this dataset", "Analysis of reasoning step usage in different languages."], "limitations": "Focus is mainly on Bangla and may not generalize to other low-resource languages; limited model architecture evaluation.", "keywords": ["Bangla", "multilingual models", "multi-step reasoning", "language models", "evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.08940", "pdf": "https://arxiv.org/pdf/2508.08940.pdf", "abs": "https://arxiv.org/abs/2508.08940", "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning", "authors": ["Hasan Abed Al Kader Hammoud", "Kumail Alhamoud", "Abed Hammoud", "Elie Bou-Zeid", "Marzyeh Ghassemi", "Bernard Ghanem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.", "AI": {"tldr": "This paper introduces a curriculum learning strategy for length-controlled reasoning in large language models (LLMs) using Group Relative Policy Optimization (GRPO), which improves token efficiency and accuracy by gradually tightening token budgets during training.", "motivation": "To enhance reasoning abilities of LLMs while managing computational costs and promoting efficient learning through an adaptive length control approach.", "method": "The authors propose a curriculum learning strategy using GRPO that starts with generous token budgets and gradually reduces them, encouraging LLMs to first explore effective strategies before refining them into concise reasoning.", "result": "Experiments show that the proposed curriculum-based training surpasses fixed-budget baselines in accuracy and token efficiency across multiple benchmarks, including GSM8K and MATH500.", "conclusion": "Curriculum-based training with progressive constraints serves as an effective inductive bias for improving reasoning models, leading to better performance and resource usage.", "key_contributions": ["Introduction of curriculum learning for length-controlled reasoning", "Empirical validation showing improvements over fixed-budget training", "Release of code and checkpoints for reproducibility"], "limitations": "", "keywords": ["curriculum learning", "large language models", "token efficiency", "Group Relative Policy Optimization", "reasoning"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.08942", "pdf": "https://arxiv.org/pdf/2508.08942.pdf", "abs": "https://arxiv.org/abs/2508.08942", "title": "Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens", "authors": ["Lucas Albarede", "Jose Moreno", "Lynda Tamine", "Luce Lefeuvre"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings.", "AI": {"tldr": "LoDIT is a new method for jointly generating and attributing answers in retrieval-augmented generation (RAG) using specific token logits, improving trustworthiness and efficiency compared to state-of-the-art models.", "motivation": "The need for trustworthiness in Large Language Models (LLMs) due to their tendency to hallucinate answers.", "method": "LoDIT generates and attributes answers by marking documents with specific token identifiers and using the logits of these tokens to gauge each document's contribution during answer generation.", "result": "LoDIT significantly outperforms state-of-the-art models on several metrics in a trustworthiness-focused attributed text-generation benchmark.", "conclusion": "LoDIT demonstrates both improved latency and robustness across various scenarios in generating trustworthy answers.", "key_contributions": ["Introduction of LoDIT for joint answer generation and attribution", "Utilization of specific token logits for accurate document contributions", "Demonstration of efficiency and robustness compared to existing models"], "limitations": "", "keywords": ["Large Language Models", "hallucination", "trustworthiness", "attributed text generation", "LoDIT"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09001", "pdf": "https://arxiv.org/pdf/2508.09001.pdf", "abs": "https://arxiv.org/abs/2508.09001", "title": "Retrospective Sparse Attention for Efficient Long-Context Generation", "authors": ["Seonghwan Choi", "Beomseok Kang", "Dongwon Jo", "Jae-Joon Kim"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.", "AI": {"tldr": "RetroAttention is a technique that improves long-context inference in Large Language Models by revising past attention outputs with new KV entries, enhancing efficiency and accuracy.", "motivation": "LLMs struggle with extended contexts due to the limitations of the Key-Value cache which increases both memory usage and latency.", "method": "RetroAttention updates past attention outputs during long decoding by incorporating new KV entries, which enhances efficiency in accessing relevant context.", "result": "RetroAttention shows up to 1.6× better effective KV exposure and up to 21.9% improved accuracy over state-of-the-art KV compression methods.", "conclusion": "The method breaks the standard fixed-attention-output model, allowing continuous correction of past approximations, leading to better performance in long-context tasks.", "key_contributions": ["Introduces a new update technique for the KV cache that revises past outputs", "Demonstrates significant performance improvements over existing KV compression methods", "Allows for continual correction of prior attention approximations"], "limitations": "", "keywords": ["Large Language Models", "Key-Value cache", "Attention mechanisms", "NLP", "Long-context tasks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09012", "pdf": "https://arxiv.org/pdf/2508.09012.pdf", "abs": "https://arxiv.org/abs/2508.09012", "title": "LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA", "authors": ["Adrián Gude", "Roi Santos-Ríos", "Francisco Prado-Valiño", "Ana Ezquerro", "Jesús Vilares"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SemEval 2025. Camera-ready version", "summary": "This paper describes our participation in SemEval 2025 Task 8, focused on\nTabular Question Answering. We developed a zero-shot pipeline that leverages an\nLarge Language Model to generate functional code capable of extracting the\nrelevant information from tabular data based on an input question. Our approach\nconsists of a modular pipeline where the main code generator module is\nsupported by additional components that identify the most relevant columns and\nanalyze their data types to improve extraction accuracy. In the event that the\ngenerated code fails, an iterative refinement process is triggered,\nincorporating the error feedback into a new generation prompt to enhance\nrobustness. Our results show that zero-shot code generation is a valid approach\nfor Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of\ntask-specific fine-tuning.", "AI": {"tldr": "This paper presents a zero-shot pipeline for Tabular Question Answering using a Large Language Model to generate functional code for information extraction from tabular data.", "motivation": "To improve the accuracy of extracting relevant information from tabular data in response to natural language questions without task-specific fine-tuning.", "method": "The approach involves a modular pipeline with a code generator, column relevance identification, and data type analysis, along with an iterative refinement process when errors occur.", "result": "The zero-shot code generation approach ranked 33rd out of 53 in the test phase, demonstrating its potential for Tabular QA.", "conclusion": "The findings support the effectiveness of zero-shot code generation for Tabular Question Answering, highlighting its applicability without extensive fine-tuning.", "key_contributions": ["Development of a zero-shot pipeline for Tabular QA", "Integration of a modular approach with code generation", "Implementation of an iterative refinement process for error handling"], "limitations": "The pipeline's performance is limited by the absence of task-specific fine-tuning and may not handle complex queries effectively.", "keywords": ["Tabular Question Answering", "Zero-shot Learning", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09016", "pdf": "https://arxiv.org/pdf/2508.09016.pdf", "abs": "https://arxiv.org/abs/2508.09016", "title": "A Survey on Training-free Alignment of Large Language Models", "authors": ["Birong Pan", "Yongqi Li", "Weiyu Zhang", "Wenpeng Lu", "Mayi Xu", "Shen Zhou", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs.", "AI": {"tldr": "This paper reviews training-free alignment methods for large language models, offering insights into pre-decoding, in-decoding, and post-decoding techniques without intensive retraining.", "motivation": "The paper addresses the limitations of traditional fine-tuning methods in aligning large language models with human values and ethical standards, considering constraints in model accessibility and computational resources.", "method": "The review categorizes training-free alignment methods into three stages: pre-decoding, in-decoding, and post-decoding, providing an examination of their mechanisms and limitations.", "result": "The paper synthesizes existing research on training-free alignment, identifying challenges and suggesting future directions to improve LLM safety and reliability.", "conclusion": "The survey serves as a guidance for practitioners in developing more inclusive and effective alignment techniques for LLMs.", "key_contributions": ["First systematic review of training-free alignment methods for LLMs", "Categorization of alignment methods by decoding stages", "Identification of challenges and future research directions"], "limitations": "The paper primarily focuses on training-free methods, which may not cover all potential alignment scenarios.", "keywords": ["Large language models", "Training-free alignment", "Human values", "Ethical standards", "Decoding stages"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09042", "pdf": "https://arxiv.org/pdf/2508.09042.pdf", "abs": "https://arxiv.org/abs/2508.09042", "title": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback", "authors": ["Chen Xu", "Zhenyu Lv", "Tian Lan", "Xianyang Wang", "Luyao Ji", "Leyang Cui", "Minqiang Yang", "Jian Shen", "Qunxi Dong", "Xiuling Liu", "Juan Wang", "Bin Hu"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario.", "AI": {"tldr": "This paper proposes a novel paradigm where a large language model (LLM) is used as a supervisor to train therapists by providing targeted feedback on common therapeutic mistakes.", "motivation": "To address ethical and safety concerns surrounding the application of LLMs in psychotherapy and to improve the training of real therapists.", "method": "Developed a therapist-training paradigm that includes establishing standards for mistaken behaviors, creating a dialogue-feedback dataset where an agent makes standard mistakes, and training a supervisor model to provide targeted feedback.", "result": "The trained supervisor model can offer high-quality feedback aligning with clinical guidelines, showing a significant potential for enhancing therapist training.", "conclusion": "The study demonstrates the feasibility and effectiveness of using LLMs to improve therapist training, potentially leading to better therapeutic outcomes.", "key_contributions": ["Establishment of standards for common therapeutic mistakes", "Creation of a human-in-the-loop dialogue-feedback dataset for training", "Development of a supervisor model that provides targeted feedback for therapist training"], "limitations": "", "keywords": ["human-computer interaction", "therapist training", "language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09057", "pdf": "https://arxiv.org/pdf/2508.09057.pdf", "abs": "https://arxiv.org/abs/2508.09057", "title": "MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions", "authors": ["Zeyu Huang", "Juyuan Wang", "Longfeng Chen", "Boyi Xiao", "Leng Cai", "Yawen Zeng", "Jin Xu"], "categories": ["cs.CL"], "comment": "ACM MM 2025", "summary": "Given the significant advances in Large Vision Language Models (LVLMs) in\nreasoning and visual understanding, mobile agents are rapidly emerging to meet\nusers' automation needs. However, existing evaluation benchmarks are\ndisconnected from the real world and fail to adequately address the diverse and\ncomplex requirements of users. From our extensive collection of user\nquestionnaire, we identified five tasks: Multi-App, Vague, Interactive,\nSingle-App, and Unethical Instructions. Around these tasks, we present\n\\textbf{MVISU-Bench}, a bilingual benchmark that includes 404 tasks across 137\nmobile applications. Furthermore, we propose Aider, a plug-and-play module that\nacts as a dynamic prompt prompter to mitigate risks and clarify user intent for\nmobile agents. Our Aider is easy to integrate into several frameworks and has\nsuccessfully improved overall success rates by 19.55\\% compared to the current\nstate-of-the-art (SOTA) on MVISU-Bench. Specifically, it achieves success rate\nimprovements of 53.52\\% and 29.41\\% for unethical and interactive instructions,\nrespectively. Through extensive experiments and analysis, we highlight the gap\nbetween existing mobile agents and real-world user expectations.", "AI": {"tldr": "The paper presents MVISU-Bench, a new bilingual benchmark for evaluating mobile agents, and introduces Aider, a module that enhances user intent clarification, leading to significant improvements in task success rates.", "motivation": "To address the lack of real-world relevance in current evaluation benchmarks for mobile agents and to meet diverse user automation needs.", "method": "The study comprises the development of MVISU-Bench, which includes 404 tasks across 137 applications, and the implementation of Aider, a dynamic prompt prompter to enhance user intent understanding.", "result": "Aider improved overall success rates by 19.55%, specifically achieving 53.52% and 29.41% increases for unethical and interactive instructions respectively, compared to the state-of-the-art.", "conclusion": "The findings reveal a significant gap between existing mobile agent capabilities and real-world user expectations, highlighting the importance of improving evaluation methods and user intent understanding.", "key_contributions": ["Introduction of MVISU-Bench as a comprehensive evaluation benchmark for mobile agents.", "Development of Aider, which enhances user intent clarification and mitigates risks during interactions.", "Documented improvements in task success rates using Aider over existing methods."], "limitations": "", "keywords": ["Large Vision Language Models", "mobile agents", "evaluation benchmark", "user intent", "bilingual benchmark"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09072", "pdf": "https://arxiv.org/pdf/2508.09072.pdf", "abs": "https://arxiv.org/abs/2508.09072", "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference", "authors": ["Maxim Divilkovskiy", "Vitaly Malygin", "Sergey Zlobin", "Sultan Isali", "Vasily Kalugin", "Stanislav Ilyushin", "Nuriza Aitassova", "Yi Fei", "Zeng Weidi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.", "AI": {"tldr": "READER is a novel method for efficient LLM inference that enhances speculative decoding by leveraging self-repetitions and optimizing key-value cache sizes, achieving over 40% speedup without additional training.", "motivation": "To address the inefficiencies in LLM inference caused by the autoregressive nature of token generation and to improve deployment for industrial applications.", "method": "READER employs a lossless speculative decoding technique that expands the decoding tree using tokens from a statistical search, focusing on large batch sizes.", "result": "READER outperforms existing speculative decoding methods, particularly excelling in search-based tasks where it shows more than 10x speedup.", "conclusion": "The proposed method presents a significant improvement in the efficiency of LLM inference, particularly for applications requiring robust speed and performance during retrieval-augmented generation tasks.", "key_contributions": ["Introduction of READER, a lossless speculative decoding method for LLM inference", "Optimization of KV cache size for large batch processing", "Demonstration of high performance with no additional training and significant speed improvements"], "limitations": "", "keywords": ["Large Language Models", "speculative decoding", "retrieval-augmented generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09074", "pdf": "https://arxiv.org/pdf/2508.09074.pdf", "abs": "https://arxiv.org/abs/2508.09074", "title": "CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization", "authors": ["Xinge Ye", "Rui Wang", "Yuchuan Wu", "Victor Ma", "Feiteng Fang", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning Fine-Tuning (RLFT) has achieved notable success in\ntasks with objectively verifiable answers (e.g., code generation, mathematical\nreasoning), yet struggles with open-ended subjective tasks like role-playing\ndialogue. Traditional reward modeling approaches, which rely on independent\nsample-wise scoring, face dual challenges: subjective evaluation criteria and\nunstable reward signals.Motivated by the insight that human evaluation\ninherently combines explicit criteria with implicit comparative judgments, we\npropose Comparative Policy Optimization (CPO). CPO redefines the reward\nevaluation paradigm by shifting from sample-wise scoring to comparative\ngroup-wise scoring.Building on the same principle, we introduce the\nCharacterArena evaluation framework, which comprises two stages:(1)\nContextualized Multi-turn Role-playing Simulation, and (2) Trajectory-level\nComparative Evaluation. By operationalizing subjective scoring via objective\ntrajectory comparisons, CharacterArena minimizes contextual bias and enables\nmore robust and fair performance evaluation. Empirical results on\nCharacterEval, CharacterBench, and CharacterArena confirm that CPO effectively\nmitigates reward ambiguity and leads to substantial improvements in dialogue\nquality.", "AI": {"tldr": "This paper presents Comparative Policy Optimization (CPO) for improving evaluation in reinforcement learning for subjective tasks like role-playing dialogue by using comparative group-wise scoring.", "motivation": "The motivation is to address the challenges in reinforcement learning fine-tuning for subjective tasks, which traditional reward modeling fails to manage due to subjective evaluation and unstable reward signals.", "method": "CPO shifts the evaluation paradigm from sample-wise scoring to comparative group-wise scoring. It introduces the CharacterArena evaluation framework, which includes a multi-turn role-playing simulation and trajectory-level comparative evaluation.", "result": "The empirical results demonstrate that CPO's approach leads to significant improvements in dialogue quality while minimizing contextual bias in performance evaluation.", "conclusion": "CPO effectively mitigates reward ambiguity and enhances the evaluation of dialogue systems, thus contributing to improved RLFT outcomes in subjective tasks.", "key_contributions": ["Introduction of Comparative Policy Optimization (CPO)", "Development of the CharacterArena evaluation framework", "Empirical validation showing improved dialogue quality"], "limitations": "", "keywords": ["Reinforcement Learning", "Comparative Evaluation", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.09091", "pdf": "https://arxiv.org/pdf/2508.09091.pdf", "abs": "https://arxiv.org/abs/2508.09091", "title": "Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages", "authors": ["Imalsha Puranegedara", "Themira Chathumina", "Nisal Ranathunga", "Nisansa de Silva", "Surangika Ranathunga", "Mokanarangan Thayaparan"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs.", "AI": {"tldr": "This paper presents a novel architecture that enhances the performance of Large Language Models (LLMs) on low-resource languages (LRLs) by fusing intermediate layers and using token-specific weighting strategies, leading to significant performance improvements across several benchmarks.", "motivation": "The performance of LLMs is significantly degraded on low-resource languages due to English-centric training, necessitating improved methods for multilingual processing.", "method": "A novel architecture that fuses all intermediate layers of LLMs and employs two weighting strategies: Global Softmax for overall layer importance and Transformer Softmax for token-specific weights, without using parallel or multilingual data for training.", "result": "The proposed model outperforms the LangBridge baseline in various tasks, achieving improvements such as increasing Sinhala classification accuracy from 71.66% to 75.86% and enhancing average XNLI accuracy from 70.36% to 71.50%.", "conclusion": "The approach offers a scalable and data-efficient pathway toward developing more capable multilingual LLMs, particularly for low-resource languages.", "key_contributions": ["Novel architecture fusing intermediate layers of LLMs", "Global Softmax and Transformer Softmax for layer importance and token-specific weights", "Significant performance gains in low-resource languages through a data-efficient training method."], "limitations": "", "keywords": ["Large Language Models", "Low-resource languages", "Multilingual processing", "Transformer Softmax", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09096", "pdf": "https://arxiv.org/pdf/2508.09096.pdf", "abs": "https://arxiv.org/abs/2508.09096", "title": "Link Prediction for Event Logs in the Process Industry", "authors": ["Anastasia Zhukova", "Thomas Walton", "Christian E. Matt", "Bela Gipp"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs.", "AI": {"tldr": "The paper addresses the challenge of fragmented event logs in the process industry by applying record linking as a cross-document coreference resolution task enhanced with natural language inference and semantic text similarity.", "motivation": "Knowledge management is crucial in the process industry to improve operations and ensure safety, but fragmented event logs hinder the recommendation of past solutions.", "method": "The study adapts cross-document coreference resolution techniques to frame record linking at the passage level, accommodating both unstructured text and structured attributes in process industry texts.", "result": "The proposed record linking model outperformed traditional methods, improving results by 28% and 27% compared to NLI and STS baselines, respectively.", "conclusion": "The study demonstrates that adapting state-of-the-art CDCR models with reasoning capabilities can significantly enhance knowledge management processes in the process industry.", "key_contributions": ["Adaptation of CDCR for record linking in process industry texts", "Improved performance over existing NLI and STS baselines", "Enhanced handling of fragmented data through advanced reasoning techniques"], "limitations": "", "keywords": ["Knowledge management", "Record linking", "Cross-document coreference resolution", "Natural language inference", "Semantic text similarity"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.09101", "pdf": "https://arxiv.org/pdf/2508.09101.pdf", "abs": "https://arxiv.org/abs/2508.09101", "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators", "authors": ["Jason Chou", "Ao Liu", "Yuchi Deng", "Zhiying Zeng", "Tao Zhang", "Haotian Zhu", "Jianwei Cai", "Yue Mao", "Chenchen Zhang", "Lingyun Tan", "Ziyan Xu", "Bohui Zhai", "Hengyi Liu", "Speed Zhu", "Wiggin Zhou", "Fengzong Lian"], "categories": ["cs.CL", "cs.SE"], "comment": "Homepage: https://autocodebench.github.io/", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.", "AI": {"tldr": "AutoCodeGen is an automated method for generating high-difficulty multilingual code generation datasets, resulting in AutoCodeBench, a benchmark for evaluating LLMs in diverse programming tasks.", "motivation": "To address critical limitations in existing benchmarks for evaluating LLMs' code generation capabilities, such as reliance on manual annotations and a lack of multilingual and diverse problem distributions.", "method": "AutoCodeGen generates test inputs with LLMs and verifies test outputs through a multilingual sandbox, employing reverse-order problem generation and filtering to ensure data quality.", "result": "AutoCodeBench contains 3,920 problems evenly distributed across 20 programming languages and evaluates over 30 LLMs, highlighting their struggles with complexity and diversity.", "conclusion": "The AutoCodeBench series aims to provide a valuable resource for evaluating LLMs on challenging multilingual code generation tasks, encouraging a shift in focus within the research community.", "key_contributions": ["Introduction of AutoCodeGen for automated dataset generation", "Creation of AutoCodeBench with 3,920 diverse problems", "Evaluation of multiple LLMs, revealing performance challenges"], "limitations": "", "keywords": ["Large Language Models", "code generation", "multilingual datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09115", "pdf": "https://arxiv.org/pdf/2508.09115.pdf", "abs": "https://arxiv.org/abs/2508.09115", "title": "SinLlama - A Large Language Model for Sinhala", "authors": ["H. W. K. Aravinda", "Rashad Sirajudeen", "Samith Karunathilake", "Nisansa de Silva", "Surangika Ranathunga", "Rishemjit Kaur"], "categories": ["cs.CL"], "comment": null, "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin.", "AI": {"tldr": "This paper presents SinLlama, the first open-source LLM with explicit support for the Sinhala language, which outperforms existing models in text classification tasks.", "motivation": "To address the underrepresentation of low-resource languages like Sinhala in the realm of open-source Large Language Models.", "method": "The research involved enhancing the tokenizer of the multilingual LLM Llama-3-8B with a Sinhala specific vocabulary and performing continual pre-training on a cleaned corpus of 10 million Sinhala texts.", "result": "SinLlama significantly outperformed both the base and instruct variants of Llama-3-8B in three text classification tasks after instruction fine-tuning.", "conclusion": "SinLlama represents a significant advancement in multilingual LLMs, providing effective support for Sinhala and showcasing the potential improvements for text classification in low-resource languages.", "key_contributions": ["Introduction of SinLlama as the first decoder-based open-source LLM with Sinhala support", "Enhancement of the LLM tokenizer with Sinhala specific vocabulary", "Demonstrated improved performance in text classification tasks compared to existing models."], "limitations": "", "keywords": ["Sinhala", "Large Language Models", "text classification", "multilingual LLM", "SinLlama"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.09124", "pdf": "https://arxiv.org/pdf/2508.09124.pdf", "abs": "https://arxiv.org/abs/2508.09124", "title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows", "authors": ["Weixuan Wang", "Dongge Han", "Daniel Madrigal Diaz", "Jin Xu", "Victor Rühle", "Saravan Rajmohan"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line.", "AI": {"tldr": "OdysseyBench is a comprehensive benchmark for evaluating LLM agents on long-horizon workflows, addressing the limitations of existing benchmarks that focus on atomic tasks.", "motivation": "Existing benchmarks for LLM agents focus on atomic tasks, failing to capture complex, multi-interaction workflows needed in real-world applications.", "method": "We introduce OdysseyBench, which includes two splits: OdysseyBench+ (300 real-world tasks) and OdysseyBench-Neo (302 newly synthesized tasks), and a framework called HomerAgents for automated benchmark creation.", "result": "OdysseyBench effectively challenges state-of-the-art LLM agents and provides a more accurate assessment of their capabilities in complex, real-world contexts.", "conclusion": "OdysseyBench will advance the development and evaluation of LLM agents in productivity scenarios, and we release it along with HomerAgents to facilitate research.", "key_contributions": ["Introduction of OdysseyBench as a benchmark for long-horizon workflows", "Creation of HomerAgents for automated benchmark generation", "Evaluation showing superior assessment capabilities of LLM agents in real-world contexts"], "limitations": "", "keywords": ["large language models", "benchmark", "long-horizon workflows", "office applications", "multi-agent framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09125", "pdf": "https://arxiv.org/pdf/2508.09125.pdf", "abs": "https://arxiv.org/abs/2508.09125", "title": "Complex Logical Instruction Generation", "authors": ["Mian Zhang", "Shujian Liu", "Sixun Dong", "Ming Yin", "Yebowen Hu", "Xun Wang", "Steven Ma", "Song Wang", "Sathish Reddy Indurthi", "Haoyun Deng", "Zhiyu Zoey Chen", "Kaiqiang Song"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF", "AI": {"tldr": "This paper introduces LogicIFGen and LogicIFEval to assess LLMs' performance on complex, logic-rich instructions defined by code functions, revealing current LLMs struggle with such tasks.", "motivation": "Investigating LLM performance on intricate, logic-conveying instructions is essential as instruction following is critical for advanced agentic behaviors.", "method": "LogicIFGen is an automated framework for generating verifiable instructions from code functions. LogicIFEval is a benchmark created from these instructions, consisting of 426 entries to evaluate LLM instruction-following capabilities.", "result": "Experiments show most LLMs can only accurately follow fewer than 60% of the logic-rich instructions, indicating significant gaps in their instruction-following abilities.", "conclusion": "The study highlights the challenges faced by state-of-the-art LLMs in understanding and executing complex instructions, emphasizing the need for further advancements in instruction following.", "key_contributions": ["Introduction of LogicIFGen for generating instructions from code functions", "Creation of LogicIFEval, a benchmark for evaluating LLMs on logic-rich tasks", "Demonstration of LLMs' limitations in instruction following."], "limitations": "The benchmark may not cover all potential complexities in natural language reasoning.", "keywords": ["Large Language Models", "Logic Instructions", "Benchmarking LLMs", "Instruction Following", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.09138", "pdf": "https://arxiv.org/pdf/2508.09138.pdf", "abs": "https://arxiv.org/abs/2508.09138", "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models", "authors": ["Wen Wang", "Bozhen Fang", "Chenchen Jing", "Yongliang Shen", "Yangyi Shen", "Qiuyu Wang", "Hao Ouyang", "Hao Chen", "Chunhua Shen"], "categories": ["cs.CL", "cs.AI"], "comment": "Project webpage: https://aim-uofa.github.io/dLLM-MidTruth", "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.", "AI": {"tldr": "The paper addresses the issue of intermediate predictions being discarded in diffusion large language models (dLLMs) and introduces methods to leverage temporal consistency for improved text generation.", "motivation": "Current decoding strategies in dLLMs overlook valuable intermediate predictions, leading to suboptimal outputs. This work aims to mitigate this loss by exploiting temporal consistency in predictions.", "method": "The paper introduces two methods: 1) Temporal Self-Consistency Voting, which aggregates predictions across denoising steps; 2) Temporal Consistency Reinforcement, using Temporal Semantic Entropy (TSE) as a reward signal for stable generation.", "result": "The proposed methods yield significant improvements across multiple benchmarks, with an average gain of 24.7% on the Countdown dataset when using the negative TSE reward, and notable gains on GSM8K, MATH500, and SVAMP when combined with accuracy rewards.", "conclusion": "The study highlights the importance of temporal dynamics in dLLMs and provides effective methodologies to leverage these dynamics for enhanced text generation quality.", "key_contributions": ["Introduction of Temporal Self-Consistency Voting", "Development of Temporal Consistency Reinforcement with TSE", "Demonstration of significant empirical improvements across benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Temporal Consistency", "Text Generation", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2112.12014", "pdf": "https://arxiv.org/pdf/2112.12014.pdf", "abs": "https://arxiv.org/abs/2112.12014", "title": "Quantifying Gender Biases Towards Politicians on Reddit", "authors": ["Sara Marjanovic", "Karolina Stańczak", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.CY"], "comment": "PlosONE article", "summary": "Despite attempts to increase gender parity in politics, global efforts have\nstruggled to ensure equal female representation. This is likely tied to\nimplicit gender biases against women in authority. In this work, we present a\ncomprehensive study of gender biases that appear in online political\ndiscussion. To this end, we collect 10 million comments on Reddit in\nconversations about male and female politicians, which enables an exhaustive\nstudy of automatic gender bias detection. We address not only misogynistic\nlanguage, but also other manifestations of bias, like benevolent sexism in the\nform of seemingly positive sentiment and dominance attributed to female\npoliticians, or differences in descriptor attribution. Finally, we conduct a\nmulti-faceted study of gender bias towards politicians investigating both\nlinguistic and extra-linguistic cues. We assess 5 different types of gender\nbias, evaluating coverage, combinatorial, nominal, sentimental, and lexical\nbiases extant in social media language and discourse. Overall, we find that,\ncontrary to previous research, coverage and sentiment biases suggest equal\npublic interest in female politicians. Rather than overt hostile or benevolent\nsexism, the results of the nominal and lexical analyses suggest this interest\nis not as professional or respectful as that expressed about male politicians.\nFemale politicians are often named by their first names and are described in\nrelation to their body, clothing, or family; this is a treatment that is not\nsimilarly extended to men. On the now banned far-right subreddits, this\ndisparity is greatest, though differences in gender biases still appear in the\nright and left-leaning subreddits. We release the curated dataset to the public\nfor future studies.", "AI": {"tldr": "This study examines gender biases in online political discussions by analyzing 10 million Reddit comments about male and female politicians, revealing significant differences in the treatment of female politicians compared to their male counterparts.", "motivation": "The motivation behind this study is to address the persistent issue of gender bias in political representation and to understand the role of online discourse in perpetuating these biases.", "method": "The methodology involves collecting a large dataset of comments from Reddit, focusing on conversations about both male and female politicians, and conducting a detailed analysis of various forms of gender bias, including linguistic and extra-linguistic factors.", "result": "The findings indicate that while there is equal public interest in female politicians, the language used to describe them is often less professional and more personal compared to male politicians, including references to their appearance and familial relations.", "conclusion": "The study concludes that implicit gender biases contribute to unequal representation, highlighting the need for awareness and further research in addressing these biases in digital discourse.", "key_contributions": ["Comprehensive analysis of gender biases in online political discussions", "Release of a large curated dataset for future research", "Identification of various forms of gender bias in social media language."], "limitations": "The study focuses solely on Reddit comments and may not capture the full spectrum of online discourse on gender biases across other platforms.", "keywords": ["gender bias", "political representation", "online discourse", "Reddit", "misogyny"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2404.18043", "pdf": "https://arxiv.org/pdf/2404.18043.pdf", "abs": "https://arxiv.org/abs/2404.18043", "title": "Utilizing Large Language Models for Information Extraction from Real Estate Transactions", "authors": ["Yu Zhao", "Haoxiang Gao", "Jinghan Cao", "Shiqi Yang"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Real estate sales contracts contain crucial information for property\ntransactions, but manual data extraction can be time-consuming and error-prone.\nThis paper explores the application of large language models, specifically\ntransformer-based architectures, for automated information extraction from real\nestate contracts. We discuss challenges, techniques, and future directions in\nleveraging these models to improve efficiency and accuracy in real estate\ncontract analysis. We generated synthetic contracts using the real-world\ntransaction dataset, thereby fine-tuning the large-language model and achieving\nsignificant metrics improvements and qualitative improvements in information\nretrieval and reasoning tasks.", "AI": {"tldr": "This paper investigates using large language models for automated data extraction from real estate sales contracts, aiming to enhance efficiency and accuracy in analysis.", "motivation": "Real estate sales contracts hold essential data, but traditional manual extraction is inefficient and prone to errors.", "method": "The study employs transformer-based large language models and synthetically generated contracts for fine-tuning the model.", "result": "Fine-tuning on synthetic contracts led to improvements in metrics and qualitative aspects of information retrieval and reasoning tasks.", "conclusion": "Applying large language models can significantly enhance the efficiency and accuracy of real estate contract analysis.", "key_contributions": ["Application of LLMs in real estate contract extraction", "Creation of synthetic contracts for model fine-tuning", "Significant improvements in data extraction metrics"], "limitations": "", "keywords": ["real estate", "large language models", "data extraction", "transformer architectures", "information retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.06795", "pdf": "https://arxiv.org/pdf/2410.06795.pdf", "abs": "https://arxiv.org/abs/2410.06795", "title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models", "authors": ["Yuying Shang", "Xinyi Zeng", "Yutao Zhu", "Xiao Yang", "Zhengwei Fang", "Jingyuan Zhang", "Jiawei Chen", "Zinan Liu", "Yu Tian"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.", "AI": {"tldr": "This paper investigates hallucinations in large vision-language models (LVLMs) and proposes a novel tuning strategy called PATCH to mitigate them by improving feature extraction and decoupling.", "motivation": "To address the issue of hallucinations in LVLMs, which generate inaccuracies not present in the visual input, and to investigate the underlying causes from an architectural perspective.", "method": "The paper proposes a new tuning strategy named PATCH, which utilizes adaptive virtual tokens to enhance feature extraction from bounding boxes in LVLMs.", "result": "PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets for LVLMs.", "conclusion": "The proposed method provides insights into the causes of hallucinations in LVLMs and fosters further advancements in the field.", "key_contributions": ["Proposes a novel tuning strategy named PATCH for LVLMs.", "Demonstrates state-of-the-art performance on hallucination datasets.", "Investigates architectural aspects of visual encoders and modal alignment modules."], "limitations": "", "keywords": ["Large Vision-Language Models", "Hallucinations", "Feature Extraction", "Multi-modal", "Adaptive Virtual Tokens"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2501.13983", "pdf": "https://arxiv.org/pdf/2501.13983.pdf", "abs": "https://arxiv.org/abs/2501.13983", "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models", "authors": ["Yang Fan"], "categories": ["cs.CL", "cs.AI"], "comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper", "summary": "As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora,\nthe problem of data contamination is becoming increasingly serious, and there\nis a risk that static evaluation benchmarks overestimate the performance of\nLLMs. To address this, this paper proposes a dynamic data evaluation method\ncalled AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts\nknowledge points and main ideas from static datasets to achieve dynamic\nalignment with the core content of static benchmarks, and by avoiding direct\nreliance on static datasets, it inherently reduces the risk of data\ncontamination from the source. It then obtains background information through\nonline searches to generate detailed descriptions of the knowledge points.\nFinally, it designs questions based on Bloom's cognitive hierarchy across six\ndimensions-remembering, understanding, applying, analyzing, evaluating, and\ncreating to enable multi-level cognitive assessment. Additionally, AdEval\ncontrols the complexity of dynamically generated datasets through iterative\nquestion reconstruction. Experimental results on multiple datasets show that\nAdEval effectively alleviates the impact of data contamination on evaluation\nresults, solves the problems of insufficient complexity control and\nsingle-dimensional evaluation, and improves the fairness, reliability and\ndiversity of LLMs evaluation.", "AI": {"tldr": "This paper presents AdEval, a dynamic evaluation method for Large Language Models (LLMs) to combat data contamination and improve evaluation accuracy.", "motivation": "The paper addresses the issue of data contamination in the evaluation of LLMs, which can lead to overestimated performance due to reliance on static benchmarks.", "method": "AdEval extracts knowledge points from static datasets for dynamic alignment, utilizes online searches for background information, and formulates questions based on Bloom's cognitive hierarchy to enable multi-level assessments.", "result": "Experimental results indicate that AdEval reduces the impact of data contamination, enhances complexity control, and improves the fairness and diversity of LLM evaluations.", "conclusion": "AdEval offers a more reliable and comprehensive method to evaluate LLMs, effectively addressing common pitfalls in traditional static evaluation methods.", "key_contributions": ["Introduction of AdEval for dynamic evaluation of LLMs", "Use of Bloom's cognitive hierarchy for multi-level assessment", "Demonstration of improved evaluation fairness and diversity"], "limitations": "The paper faces criticisms regarding potential data falsification and plagiarism in its methodology.", "keywords": ["Large Language Models", "data contamination", "dynamic evaluation", "Bloom's taxonomy", "fairness in evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.14910", "pdf": "https://arxiv.org/pdf/2502.14910.pdf", "abs": "https://arxiv.org/abs/2502.14910", "title": "EvoP: Robust LLM Inference via Evolutionary Pruning", "authors": ["Shangyu Wu", "Hongchao Du", "Ying Xiong", "Shuai Chen", "Tei-wei Kuo", "Nan Guan", "Chun Jason Xue"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.", "AI": {"tldr": "EvoP is an evolutionary pruning framework designed to enhance the efficiency of Large Language Models (LLMs) by creating diverse calibration datasets and optimizing pruning patterns.", "motivation": "To improve deployment of large language models in resource-constrained environments by addressing limitations of existing pruning methods.", "method": "EvoP employs a cluster-based calibration dataset sampling strategy to enhance dataset diversity, followed by an evolutionary pruning pattern searching method to identify optimal pruning configurations.", "result": "EvoP outperforms existing model pruning techniques, achieving superior performance and efficiency across multiple LLMs and downstream tasks.", "conclusion": "EvoP presents a practical and scalable solution for deploying large language models effectively in real-world applications.", "key_contributions": ["Introduction of the cluster-based calibration dataset sampling strategy for LLMs.", "Development of evolutionary pruning pattern searching to optimize model efficiency.", "Validation of EvoP's effectiveness through experiments on various LLMs and tasks."], "limitations": "", "keywords": ["Large Language Models", "model pruning", "evolutionary algorithms", "calibration dataset", "practical deployment"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.00043", "pdf": "https://arxiv.org/pdf/2504.00043.pdf", "abs": "https://arxiv.org/abs/2504.00043", "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation", "authors": ["Jixuan Leng", "Chengsong Huang", "Langlin Huang", "Bill Yuchen Lin", "William W. Cohen", "Haohan Wang", "Jiaxin Huang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly assess either text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles --\na task requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in two formats (text and image), supports adjustable\ndifficulty through prefill ratio control, and offers different evaluation\nstrategies, ranging from direct puzzle solving to interactive modes. Our\nextensive evaluation of over 20 models reveals that reasoning LLMs\nsubstantially outperform non-reasoning models by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings highlight limitations of the reasoning\ncapabilities of current LLMs and LVLMs, and provide an effective approach for\ncreating multimodal constrained tasks for future evaluations.", "AI": {"tldr": "CrossWordBench is a benchmark for evaluating reasoning capabilities of LLMs and LVLMs using crossword puzzles, revealing performance differences between reasoning and non-reasoning models.", "motivation": "Existing frameworks fail to assess both text and visual reasoning capabilities together, necessitating a new approach to evaluate multimodal reasoning.", "method": "CrossWordBench introduces a puzzle generation framework that creates crossword puzzles in text and image formats with adjustable difficulty and various evaluation strategies.", "result": "Over 20 models were evaluated, showing reasoning LLMs outperform non-reasoning models, while LVLMs exhibited difficulties linked to grid-parsing accuracy.", "conclusion": "Current LLMs and LVLMs have limitations in reasoning capabilities; CrossWordBench provides a new method for multimodal task evaluation.", "key_contributions": ["Introduction of CrossWordBench for multimodal reasoning evaluation", "Development of a controllable puzzle generation framework", "Identification of performance gaps between LLMs and LVLMs"], "limitations": "", "keywords": ["crossword puzzles", "large language models", "multimodal evaluation", "reasoning", "benchmarking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.07114", "pdf": "https://arxiv.org/pdf/2504.07114.pdf", "abs": "https://arxiv.org/abs/2504.07114", "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation", "authors": ["Serina Chang", "Ashton Anderson", "Jake M. Hofman"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "ACL 2025 (main)", "summary": "With the rapid adoption of LLM-based chatbots, there is a pressing need to\nevaluate what humans and LLMs can achieve together. However, standard\nbenchmarks, such as MMLU, measure LLM capabilities in isolation (i.e.,\n\"AI-alone\"). Here, we design and conduct a user study to convert MMLU questions\ninto user-AI conversations, by seeding the user with the question and having\nthem carry out a conversation with the LLM to answer their question. We release\nChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396\nquestions and two LLMs, including 144K answers and 7,336 user-AI conversations.\nWe find that AI-alone accuracy fails to predict user-AI accuracy, with\nsignificant differences across multiple subjects (math, physics, and moral\nreasoning), and we analyze the user-AI conversations to provide insight into\nhow they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a\nuser simulator on a subset of ChatBench improves its ability to estimate\nuser-AI accuracies, increasing correlation on held-out questions by more than\n20 points, creating possibilities for scaling interactive evaluation.", "AI": {"tldr": "This paper presents a user study to evaluate human-LLM collaboration using a new dataset, ChatBench, which highlights discrepancies between AI-alone and user-AI performance.", "motivation": "The rapid adoption of LLM-based chatbots raises the need to assess their efficacy in human-LLM collaborations, beyond isolated evaluations.", "method": "A user study that converts standard MMLU questions into interactive user-AI conversations, creating the ChatBench dataset, which includes distinct data for user-alone, AI-alone, and user-AI interactions.", "result": "AI-alone accuracy does not correlate with user-AI accuracy, revealing significant performance variances across subjects like math and physics; furthermore, fine-tuning a user simulator enhances predictive accuracy for user-AI interactions.", "conclusion": "The findings emphasize the necessity of evaluating chatbot performance in collaborative contexts and suggest that tailored evaluations can significantly improve accuracy estimations.", "key_contributions": ["Introduction of ChatBench dataset with comprehensive user-AI interaction data", "Demonstrated the divergence of AI-alone and user-AI performance", "Improved user simulator correlation through fine-tuning on ChatBench data"], "limitations": "The dataset is limited to 396 questions and two LLMs, which may not generalize to all conversational contexts.", "keywords": ["LLM", "user-AI conversations", "ChatBench", "evaluation", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13079", "pdf": "https://arxiv.org/pdf/2504.13079.pdf", "abs": "https://arxiv.org/abs/2504.13079", "title": "Retrieval-Augmented Generation with Conflicting Evidence", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025, Data and Code: https://github.com/HanNight/RAMDocs", "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.", "AI": {"tldr": "This paper introduces RAMDocs, a dataset for evaluating LLM performance in the presence of ambiguity and misinformation, and MADAM-RAG, a multi-agent method to improve LLM responses to conflicting information.", "motivation": "To enhance the factuality of LLM responses amidst ambiguity, misinformation, and conflicting information from various sources.", "method": "The authors propose a new dataset (RAMDocs) for complex query scenarios and a multi-agent debate mechanism (MADAM-RAG) for LLMs to determine the best response while filtering out noise and inaccuracies.", "result": "MADAM-RAG demonstrates improvements over traditional RAG baselines, achieving up to 11.40% better performance on AmbigDocs and 15.80% on FaithEval with Llama3.3-70B-Instruct.", "conclusion": "While MADAM-RAG starts to address issues of ambiguity and misinformation for LLM agents, significant performance gaps remain, particularly when the supporting evidence is imbalanced.", "key_contributions": ["Introduced RAMDocs dataset simulating complex query scenarios.", "Developed MADAM-RAG, a multi-agent approach for improved LLM responses.", "Demonstrated substantial performance gains over existing RAG methods."], "limitations": "The analysis indicates a remaining gap in performance when evidence imbalance and misinformation increase.", "keywords": ["large language models", "retrieval-augmented generation", "ambiguity", "misinformation", "multi-agent systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.02594", "pdf": "https://arxiv.org/pdf/2411.02594.pdf", "abs": "https://arxiv.org/abs/2411.02594", "title": "A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health", "authors": ["Jiawei Zhou", "Amy Z. Chen", "Darshi Shah", "Laura M. Schwab Reese", "Munmun De Choudhury"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5; J.3; K.4"], "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have generated both\ninterest and concern about their potential adoption as information sources or\ncommunication tools across different domains. In public health, where stakes\nare high and impacts extend across diverse populations, adopting LLMs poses\nunique challenges that require thorough evaluation. However, structured\napproaches for assessing potential risks in public health remain\nunder-explored. To address this gap, we conducted focus groups with public\nhealth professionals and individuals with lived experience to unpack their\nconcerns, situated across three distinct and critical public health issues that\ndemand high-quality information: infectious disease prevention (vaccines),\nchronic and well-being care (opioid use disorder), and community health and\nsafety (intimate partner violence). We synthesize participants' perspectives\ninto a risk taxonomy, identifying and contextualizing the potential harms LLMs\nmay introduce when positioned alongside traditional health communication. This\ntaxonomy highlights four dimensions of risk to individuals, human-centered\ncare, information ecosystem, and technology accountability. For each dimension,\nwe unpack specific risks and offer example reflection questions to help\npractitioners adopt a risk-reflexive approach. By summarizing distinctive LLM\ncharacteristics and linking them to identified risks, we discuss the need to\nrevisit prior mental models of information behaviors and complement evaluations\nwith external validity and domain expertise through lived experience and\nreal-world practices. Together, this work contributes a shared vocabulary and\nreflection tool for people in both computing and public health to\ncollaboratively anticipate, evaluate, and mitigate risks in deciding when to\nemploy LLM capabilities (or not) and how to mitigate harm.", "AI": {"tldr": "The paper explores the unique challenges and risks of adopting large language models (LLMs) in public health through focus groups with professionals and individuals, presenting a risk taxonomy to guide thoughtful implementation.", "motivation": "To assess the potential risks of using LLMs in public health, where misinformation can have severe consequences.", "method": "Focus groups with public health professionals and individuals with lived experience to identify their concerns regarding LLMs in the context of public health issues.", "result": "A risk taxonomy was synthesized, highlighting four dimensions of risk associated with LLM use: individual risks, human-centered care risks, information ecosystem risks, and technological accountability risks.", "conclusion": "The paper emphasizes the need for a risk-reflexive approach in adopting LLMs in public health, providing a framework for collaboration between computing and public health fields to mitigate potential harms.", "key_contributions": ["Development of a risk taxonomy for LLMs in public health", "Identification of four dimensions of risk", "Provision of reflection questions for practitioners to evaluate LLM implementation"], "limitations": "", "keywords": ["large language models", "public health", "risk assessment", "health communication", "information behavior"], "importance_score": 9, "read_time_minutes": 15}}
