{"id": "2509.13323", "pdf": "https://arxiv.org/pdf/2509.13323.pdf", "abs": "https://arxiv.org/abs/2509.13323", "title": "AI Behavioral Science", "authors": ["Matthew O. Jackson", "Qiaozhu Me", "Stephanie W. Wang", "Yutong Xie", "Walter Yuan", "Seth Benzell", "Erik Brynjolfsson", "Colin F. Camerer", "James Evans", "Brian Jabarian", "Jon Kleinberg", "Juanjuan Meng", "Sendhil Mullainathan", "Asuman Ozdaglar", "Thomas Pfeiffer", "Moshe Tennenholtz", "Robb Willer", "Diyi Yang", "Teng Ye"], "categories": ["cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "We discuss the three main areas comprising the new and emerging field of \"AI\nBehavioral Science\". This includes not only how AI can enhance research in the\nbehavioral sciences, but also how the behavioral sciences can be used to study\nand better design AI and to understand how the world will change as AI and\nhumans interact in increasingly layered and complex ways.", "AI": {"tldr": "The paper explores the intersection of AI and behavioral sciences, discussing AI's potential to enhance research and how behavioral sciences can inform AI design and human-AI interactions.", "motivation": "To understand how AI can contribute to behavioral science research and how behavioral sciences can inform the design of AI systems.", "method": "", "result": "", "conclusion": "", "key_contributions": ["Exploration of AI's role in enhancing behavioral science research", "Discussion on using behavioral science principles to improve AI design", "Insights on the evolving interaction between AI and humans"], "limitations": "", "keywords": ["AI", "Behavioral Science", "Human-AI Interaction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.13324", "pdf": "https://arxiv.org/pdf/2509.13324.pdf", "abs": "https://arxiv.org/abs/2509.13324", "title": "Designing Psychometric Bias Measures for ChatBots: An Application to Racial Bias Measurement", "authors": ["Mouhacine Benosman"], "categories": ["cs.HC"], "comment": "7 pages, 1 figure", "summary": "Artificial intelligence (AI), particularly in the form of large language\nmodels (LLMs) or chatbots, has become increasingly integrated into our daily\nlives. In the past five years, several LLMs have been introduced, including\nChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. These\nmodels have the potential to be employed across a wide range of human-machine\ninteraction applications, such as chatbots for information retrieval,\nassistance in corporate hiring decisions, college admissions, financial loan\napprovals, parole determinations, and even in medical fields like psychotherapy\ndelivered through chatbots. The key question is whether these chatbots will\ninteract with humans in a bias-free manner or if they will further reinforce\nthe existing pathological biases present in human-to-human interactions. If the\nlatter is true, then how to rigorously measure these biases? We aim to address\nthis challenge by proposing a principled framework for designing psychometric\nmeasures to evaluate chatbot biases.", "AI": {"tldr": "This paper proposes a framework for measuring biases in AI chatbots, addressing the impact of such biases in various human-machine interaction applications.", "motivation": "The integration of LLMs in daily life raises concerns about whether these systems will perpetuate existing human biases in their interactions.", "method": "The authors propose a principled framework for designing psychometric measures to evaluate biases in chatbots, aiming to rigorously assess their interactions with users.", "result": "The framework is designed to evaluate bias in various applications, including corporate hiring, loan approvals, and psychotherapy, indicating a need for careful measurement in these contexts.", "conclusion": "Addressing chatbot biases is crucial to ensure they do not reinforce existing societal biases in their applications.", "key_contributions": ["Proposed a framework for designing psychometric measures for chatbot bias evaluation", "Identified various domains of application for chatbots where bias measurement is critical", "Highlighted the importance of addressing biases in AI interactions to avoid societal reinforcement."], "limitations": "", "keywords": ["artificial intelligence", "large language models", "chatbots", "bias measurement", "human-machine interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13326", "pdf": "https://arxiv.org/pdf/2509.13326.pdf", "abs": "https://arxiv.org/abs/2509.13326", "title": "LLM Chatbot-Creation Approaches", "authors": ["Hemil Mehta", "Tanvi Raut", "Kohav Yadav", "Edward F. Gehringer"], "categories": ["cs.HC", "cs.LG"], "comment": "Forthcoming in Frontiers in Education (FIE 2025), Nashville,\n  Tennessee, USA, Nov 2-5, 2025", "summary": "This full research-to-practice paper explores approaches for developing\ncourse chatbots by comparing low-code platforms and custom-coded solutions in\neducational contexts. With the rise of Large Language Models (LLMs) like GPT-4\nand LLaMA, LLM-based chatbots are being integrated into teaching workflows to\nautomate tasks, provide assistance, and offer scalable support. However,\nselecting the optimal development strategy requires balancing ease of use,\ncustomization, data privacy, and scalability. This study compares two\ndevelopment approaches: low-code platforms like AnythingLLM and Botpress, with\ncustom-coded solutions using LangChain, FAISS, and FastAPI. The research uses\nPrompt engineering, Retrieval-augmented generation (RAG), and personalization\nto evaluate chatbot prototypes across technical performance, scalability, and\nuser experience. Findings indicate that while low-code platforms enable rapid\nprototyping, they face limitations in customization and scaling, while\ncustom-coded systems offer more control but require significant technical\nexpertise. Both approaches successfully implement key research principles such\nas adaptive feedback loops and conversational continuity. The study provides a\nframework for selecting the appropriate development strategy based on\ninstitutional goals and resources. Future work will focus on hybrid solutions\nthat combine low-code accessibility with modular customization and incorporate\nmultimodal input for intelligent tutoring systems.", "AI": {"tldr": "This paper explores the comparison of low-code platforms and custom-coded solutions for developing educational chatbots, emphasizing the integration of LLMs and their implications on scalability and customization.", "motivation": "The rise of LLMs in educational contexts necessitates effective chatbot development strategies that balance ease of use with technical demands.", "method": "The study evaluates chatbot prototypes built with low-code platforms like AnythingLLM and Botpress against custom-coded solutions using LangChain and FastAPI, employing prompt engineering and retrieval-augmented generation.", "result": "Low-code platforms allow for quick prototyping but lack deep customization and scalability, whereas custom-coded solutions provide control at the cost of requiring greater technical skills. Both methods maintain effective research principles for user engagement.", "conclusion": "The paper proposes a framework to guide the selection of development strategies based on institutional goals, highlighting potential future innovations in hybrid solutions for chatbots.", "key_contributions": ["Comparison of low-code vs custom-coded chatbot development", "Framework for selecting development strategy", "Evaluation of chatbot prototypes for educational use"], "limitations": "Low-code platforms are limited in customization and scalability; custom-coded solutions demand significant technical expertise.", "keywords": ["chatbots", "LLMs", "low-code", "education", "custom-coded"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13444", "pdf": "https://arxiv.org/pdf/2509.13444.pdf", "abs": "https://arxiv.org/abs/2509.13444", "title": "DuetUI: A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces", "authors": ["Yuan Xu", "Shaowen Xiang", "Yizhi Song", "Ruoting Sun", "Xin Tong"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models are reshaping task automation, yet remain limited in\ncomplex, multi-step real-world tasks that require aligning with vague user\nintent and enabling dynamic user override. From a formative study with 12\nparticipants, we found that end-users actively seek to shape generative\ninterfaces rather than relying on one-shot outputs. To address this, we\nintroduce the human-agent co-generation paradigm, materialized in DuetUI. This\nLLM-empowered system unfolds alongside task progress through a bidirectional\ncontext loop--the agent scaffolds the interface by decomposing the task, while\nthe user's direct manipulations implicitly steer the agent's next generation\nstep. In a user study with 24 participants, DuetUI significantly improved task\nefficiency and interface usability compared to a baseline, fostering seamless\nhuman-agent collaboration. Our contributions include the proposal and\nvalidation of this novel paradigm, the design of the DuetUI prototype embodying\nit, and empirical insights into how this bidirectional loop better aligns\nagents with human intent.", "AI": {"tldr": "This paper introduces DuetUI, an LLM-empowered system that enhances human-agent collaboration in complex task automation through a human-agent co-generation paradigm.", "motivation": "Large Language Models face challenges in automating complex tasks that require understanding and aligning with user intent.", "method": "Based on a formative study with 12 participants, a new paradigm called human-agent co-generation was developed and implemented in the DuetUI prototype. The system operates through a bidirectional context loop that allows for dynamic user interaction and agent response.", "result": "In a user study with 24 participants, DuetUI significantly improved task efficiency and interface usability compared to a baseline, indicating success in enhancing human-agent collaboration.", "conclusion": "The study validates the human-agent co-generation paradigm and demonstrates that the bidirectional loop effectively aligns agents with user intent, providing empirical insights into its benefits.", "key_contributions": ["Introduction of the human-agent co-generation paradigm", "Development of the DuetUI prototype", "Empirical insights into user-agent interaction dynamics"], "limitations": "", "keywords": ["Human-Agent Collaboration", "Large Language Models", "Task Automation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13480", "pdf": "https://arxiv.org/pdf/2509.13480.pdf", "abs": "https://arxiv.org/abs/2509.13480", "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "categories": ["cs.CL"], "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "This paper evaluates large language models for gender-neutral rewriting (GNR) in Italian, introducing a framework to measure fairness and semantics.", "motivation": "To eliminate unnecessary gender specifications in Italian text, enhancing inclusivity without losing meaning.", "method": "Systematic evaluation of state-of-the-art LLMs for GNR with few-shot prompting, model fine-tuning, and targeted data cleaning.", "result": "Open-weight LLMs outperform existing models for Italian GNR, with fine-tuned models matching or exceeding performance at a reduced size.", "conclusion": "Optimizing training data presents trade-offs between neutrality and meaning preservation in GNR tasks.", "key_contributions": ["Introduced a novel framework for evaluating GNR in Italian", "Showcased the superior performance of open-weight LLMs over dedicated GNR models", "Demonstrated the effectiveness of fine-tuning for specific GNR tasks"], "limitations": "", "keywords": ["gender-neutral rewriting", "large language models", "Italian", "semantic fidelity", "neutrality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13466", "pdf": "https://arxiv.org/pdf/2509.13466.pdf", "abs": "https://arxiv.org/abs/2509.13466", "title": "Do We Need Subsidiarity in Software?", "authors": ["Louisa Conwill", "Megan Levis Scheirer", "Walter Scheirer"], "categories": ["cs.HC"], "comment": null, "summary": "Subsidiarity is a principle of social organization that promotes human\ndignity and resists over-centralization by balancing personal autonomy with\nintervention from higher authorities only when necessary. Thus it is a\nrelevant, but not previously explored, critical lens for discerning the\ntradeoffs between complete user control of software and surrendering control to\n\"big tech\" for convenience, as is common in surveillance capitalism. Our study\nexplores data privacy through the lens of subsidiarity: we employ a\nmulti-method approach of data flow monitoring and user interviews to determine\nthe level of control different everyday technologies currently operate at, and\nthe level of control everyday computer users think is necessary. We found that\nchat platforms like Slack and Discord violate subsidiarity the most. Our work\nprovides insight into when users are willing to surrender privacy for\nconvenience and demonstrates how subsidiarity can inform designs that promote\nhuman flourishing.", "AI": {"tldr": "The study explores data privacy through the lens of subsidiarity, examining user control versus convenience in everyday technologies.", "motivation": "To analyze the trade-offs between user control of software and surrendering control to larger tech companies in the context of data privacy.", "method": "A multi-method approach involving data flow monitoring and user interviews to assess user control levels in everyday technologies.", "result": "Chat platforms like Slack and Discord were identified as violating the principle of subsidiarity the most; users are willing to trade privacy for convenience.", "conclusion": "Subsidiarity can guide the design of technologies that enhance human dignity and promote better user autonomy in the digital landscape.", "key_contributions": ["Introduction of subsidiarity as a lens for analyzing data privacy", "Insights into user willingness to compromise privacy for convenience", "Evaluation of control levels in popular chat platforms"], "limitations": "", "keywords": ["subsidiarity", "data privacy", "user control", "surveillance capitalism", "human-computer interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.13539", "pdf": "https://arxiv.org/pdf/2509.13539.pdf", "abs": "https://arxiv.org/abs/2509.13539", "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "categories": ["cs.CL"], "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "A dataset called Op-Fed is released, containing 1044 annotated sentences from FOMC transcripts, addressing challenges in opinion and stance classification related to monetary policy.", "motivation": "To enhance understanding and classification of sentiments in monetary policy discussions that affect public financial decisions.", "method": "Developed a five-stage hierarchical schema to tackle imbalanced classes and inter-sentence dependence, employing active learning to improve positive instance annotation.", "result": "The dataset allowed a closed-weight LLM to achieve 0.80 zero-shot accuracy in opinion classification, but only 0.61 in stance classification, significantly below the human baseline of 0.89.", "conclusion": "Op-Fed can aid in model training, calibrating confidence, and serve as a foundation for future sentiment annotation tasks in monetary policy.", "key_contributions": ["Release of a comprehensive dataset for sentiment analysis in monetary policy discussions", "Development of a hierarchical schema addressing classification challenges", "Demonstration of LLM performance against human baselines in a new domain"], "limitations": "The dataset is limited to the FOMC transcripts and may not generalize to other types of texts or contexts.", "keywords": ["dataset", "monetary policy", "opinion classification", "active learning", "language model"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2509.13468", "pdf": "https://arxiv.org/pdf/2509.13468.pdf", "abs": "https://arxiv.org/abs/2509.13468", "title": "AR-TMT: Investigating the Impact of Distraction Types on Attention and Behavior in AR-based Trail Making Test", "authors": ["Sihun Baek", "Zhehan Qu", "Maria Gorlatova"], "categories": ["cs.HC"], "comment": "Accepted at VRST 2025. 9 pages, 8 figures", "summary": "Despite the growing use of AR in safety-critical domains, the field lacks a\nsystematic understanding of how different types of distraction affect user\nbehavior in AR environments. To address this gap, we present AR-TMT, an AR\nadaptation of the Trail Making Test that spatially renders targets for\nsequential selection on the Magic Leap 2. We implemented distractions in three\ncategories: top-down, bottom-up, and spatial distraction based on Wolfe's\nGuided Search model, and captured performance, gaze, motor behavior, and\nsubjective load measures to analyze user attention and behavior. A user study\nwith 34 participants revealed that top-down distraction degraded performance\nthrough semantic interference, while bottom-up distraction disrupted initial\nattentional engagement. Spatial distraction destabilized gaze behavior, leading\nto more scattered and less structured visual scanning patterns. We also found\nthat performance was correlated with attention control ($R^2 = .20$--$.35$)\nunder object-based distraction conditions, where distractors possessed\ntask-relevant features. The study offers insights into distraction mechanisms\nand their impact on users, providing opportunities for generalization to\necologically relevant AR tasks while underscoring the need to address the\nunique demands of AR environments.", "AI": {"tldr": "The paper presents AR-TMT, an AR adaptation of the Trail Making Test, exploring user behavior under different types of distraction in augmented reality environments.", "motivation": "To systematically understand how different types of distraction affect user behavior in AR environments, especially in safety-critical domains.", "method": "Implemented AR-TMT on Magic Leap 2 with distractions categorized as top-down, bottom-up, and spatial; performance metrics, gaze, motor behavior, and subjective load measures were captured.", "result": "A study with 34 participants revealed that top-down distraction led to performance degradation, bottom-up distraction affected initial attention, and spatial distraction caused unstable gaze patterns. Performance correlated with attention control in object-based distraction scenarios.", "conclusion": "The study provides insights into distraction mechanisms in AR, suggesting the need to address unique demands of AR tasks, with potential generalization to ecologically relevant AR applications.", "key_contributions": ["Development of AR-TMT for studying distraction in AR environments", "Identification of the effects of top-down, bottom-up, and spatial distractions on user performance", "Correlation between attention control and performance under object-based distractions"], "limitations": "", "keywords": ["Augmented Reality", "Distraction", "User Behavior", "Eye Tracking", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2509.13569", "pdf": "https://arxiv.org/pdf/2509.13569.pdf", "abs": "https://arxiv.org/abs/2509.13569", "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "authors": ["John Mendonça", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "João Sedoc"], "categories": ["cs.CL"], "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "This paper reviews the DSTC12 Track 1 on dialogue system evaluation, focusing on dimensionality, language, culture, and safety in dialogue systems.", "motivation": "The paper addresses the inadequacies of traditional dialogue evaluation metrics and highlights the increasing importance of culturally aware and safe dialogue systems due to the growth of LLMs.", "method": "The evaluation encompasses two tasks: 1) Dialogue-level, Multi-dimensional Automatic Evaluation Metrics, and 2) Multilingual and Multicultural Safety Detection, with emphasis on the performance of a Llama-3-8B baseline against various metrics.", "result": "The Llama-3-8B baseline achieved a Spearman's correlation of 0.1681 in Task 1, indicating room for improvement, while in Task 2, teams outperformed a Llama-Guard-3-1B baseline with a top ROC-AUC of 0.9648 for the multilingual safety subset.", "conclusion": "Results underline the need for better dialogue evaluation methods and culturally aware safety measures, with significant variation in baseline performances across tasks.", "key_contributions": ["Overview of dialogue system evaluation methodologies", "Results from multi-dimensional evaluation metrics", "Insights on the need for culturally-aware safety detection"], "limitations": "The evaluation metrics still showed substantial room for improvement, particularly in dialogue-level metrics.", "keywords": ["dialogue systems", "evaluation metrics", "cultural safety", "multilingual", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13532", "pdf": "https://arxiv.org/pdf/2509.13532.pdf", "abs": "https://arxiv.org/abs/2509.13532", "title": "Py maidr: Bridging Visual and Non-Visual Data Experiences Through a Unified Python Framework", "authors": ["JooYoung Seo", "Saairam Venkatesh", "Daksh Pokar", "Sanchita Kamath", "Krishna Anandan Ganesan"], "categories": ["cs.HC"], "comment": null, "summary": "Although recent efforts have developed accessible data visualization tools\nfor blind and low-vision (BLV) users, most follow a \"design for them\" approach\nthat creates an unintentional divide between sighted creators and BLV\nconsumers. This unidirectional paradigm perpetuates a power dynamic where\nsighted creators produce non-visual content boundaries for BLV consumers to\naccess. This paper proposes a bidirectional approach, \"design for us,\" where\nboth sighted and BLV collaborators can employ the same tool to create,\ninterpret, and communicate data visualizations for each other. We introduce Py\nmaidr, a Python package that seamlessly encodes multimodal (e.g., tactile,\nauditory, conversational) data representations into visual plots generated by\nMatplotlib and Seaborn. By simply importing the maidr package and invoking the\nmaidr.show() method, users can generate accessible plots with minimal changes\nto their existing codebase regardless of their visual dis/abilities. Our\ntechnical case studies demonstrate how this tool is scalable and can be\nintegrated into interactive computing (e.g., Jupyter Notebook, Google Colab),\nreproducible and literate programming (e.g., Quarto), and reactive dashboards\n(e.g., Shiny, Streamlit). Our performance benchmarks demonstrate that Py maidr\nintroduces minimal and consistent overhead during the rendering and export of\nplots against Matplotlib and Seaborn baselines. This work significantly\ncontributes to narrowing the accessibility gap in data visualization by\nproviding a unified framework that fosters collaboration and communication\nbetween sighted and BLV individuals.", "AI": {"tldr": "This paper introduces Py maidr, a Python package designed to create accessible data visualizations for both blind and low-vision (BLV) users and sighted individuals, promoting collaboration and reducing accessibility gaps.", "motivation": "To address the unidirectional design approach that perpetuates a divide between sighted creators and BLV consumers in data visualization tools.", "method": "The paper presents the Py maidr package that enables the encoding of multimodal data representations into visual plots using existing libraries like Matplotlib and Seaborn, facilitating the creation of accessible visualizations by diverse users.", "result": "The benchmarks show that Py maidr adds minimal overhead to plot rendering and can easily integrate with platforms like Jupyter Notebook and Google Colab, enhancing accessibility without sacrificing performance.", "conclusion": "Py maidr significantly narrows the accessibility gap in data visualization and fosters collaboration between sighted and BLV users by providing a unified tool for creating and interpreting data visualizations.", "key_contributions": ["Introduction of a bidirectional approach to data visualization design that includes both sighted and BLV users.", "Development of the Py maidr package that integrates with existing Python libraries for accessible visualizations.", "Demonstration of minimal performance overhead and integration with interactive platforms."], "limitations": "", "keywords": ["accessible visualization", "blind and low-vision users", "data communication", "Python package", "collaborative design"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.13624", "pdf": "https://arxiv.org/pdf/2509.13624.pdf", "abs": "https://arxiv.org/abs/2509.13624", "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "This paper presents an analysis framework for transfer learning in large language models (LLMs), uncovering hidden factors that influence performance across diverse tasks.", "motivation": "The need to adapt large language models to a wide range of tasks without having comprehensive training data for each one.", "method": "An analysis framework utilizing a transfer learning matrix and dimensionality reduction to evaluate cross-task interactions in LLMs, along with training and analyzing 10 models to identify latent abilities.", "result": "The findings demonstrate that performance improvements cannot be solely explained by dataset similarity or quality; instead, underlying statistical factors like class distribution and linguistic features play a crucial role.", "conclusion": "The study offers valuable insights into the dynamics of transfer learning, enhancing LLM adaptability across various tasks.", "key_contributions": ["Developed an analysis framework for LLM transfer learning", "Identified latent abilities of models", "Revealed hidden statistical factors affecting performance"], "limitations": "", "keywords": ["transfer learning", "large language models", "cross-task interactions", "latent abilities", "statistical factors"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13646", "pdf": "https://arxiv.org/pdf/2509.13646.pdf", "abs": "https://arxiv.org/abs/2509.13646", "title": "Vistoria: A Multimodal System to Support Fictional Story Writing through Instrumental Text-Image Co-Editing", "authors": ["Kexue Fu", "Jingfei Huang", "Long Ling", "Sumin Hong", "Yihang Zuo", "Ray LC", "Toby Jia-jun Li"], "categories": ["cs.HC"], "comment": "Paper is under the review", "summary": "Humans think visually-we remember in images, dream in pictures, and use\nvisual metaphors to communicate. Yet, most creative writing tools remain\ntext-centric, limiting how authors plan and translate ideas. We present\nVistoria, a system for synchronized text-image co-editing in fictional story\nwriting that treats visuals and text as coequal narrative materials. A\nformative Wizard-of-Oz co-design study with 10 story writers revealed how\nsketches, images, and annotations serve as essential instruments for ideation\nand organization. Drawing on theories of Instrumental Interaction and\nStructural Mapping, Vistoria introduces multimodal operations-lasso, collage,\nfilters, and perspective shifts that enable seamless narrative exploration\nacross modalities. A controlled study with 12 participants shows that\nco-editing enhances expressiveness, immersion, and collaboration, enabling\nwriters to explore divergent directions, embrace serendipitous randomness, and\ntrace evolving storylines. While multimodality increased cognitive demand,\nparticipants reported stronger senses of authorship and agency. These findings\ndemonstrate how multimodal co-editing expands creative potential by balancing\nabstraction and concreteness in narrative development.", "AI": {"tldr": "Vistoria is a text-image co-editing system for creative writing that enhances expressiveness and collaboration among authors by integrating visual and textual elements.", "motivation": "Creative writing tools currently focus on text, which limits authors' ability to visually plan and organize their stories. The paper aims to address this gap by integrating visual elements into the writing process.", "method": "The authors conducted a formative Wizard-of-Oz co-design study with 10 story writers to understand how visuals aid in writing, followed by a controlled study with 12 participants testing the co-editing system's effectiveness.", "result": "The study found that co-editing with visuals improves expressiveness, immersion, and collaboration, helping writers to explore new narrative directions and feel a stronger sense of authorship.", "conclusion": "Multimodal co-editing can enhance creative potential in narrative development by balancing abstraction and concreteness despite increasing cognitive demands.", "key_contributions": ["Introduction of the Vistoria system for synchronized text-image co-editing", "Identification of multimodal operations that facilitate narrative exploration", "Empirical evidence supporting the benefits of multimodal co-editing for writers"], "limitations": "Increased cognitive demand was reported by participants when utilizing multimodal features.", "keywords": ["multimodal interaction", "creative writing", "co-editing", "visual metaphors", "narrative exploration"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.13664", "pdf": "https://arxiv.org/pdf/2509.13664.pdf", "abs": "https://arxiv.org/abs/2509.13664", "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "The paper investigates how large language models (LLMs) encode ambiguity in questions and how this encoding can be detected and controlled at the neuron level.", "motivation": "Understanding how LLMs handle ambiguity in natural language questions is crucial for improving their interpretability and control in applications.", "method": "The authors identify Ambiguity-Encoding Neurons (AENs) in LLMs that encode question ambiguity, and investigate their detection and manipulation through layerwise analysis.", "result": "Probes trained on these AENs effectively detect ambiguity and outperform existing methods in various contexts.", "conclusion": "LLMs contain compact internal representations of question ambiguity, allowing for interpretable and controllable responses to ambiguous queries.", "key_contributions": ["Identification of Ambiguity-Encoding Neurons in LLMs", "Development of effective probes for ambiguity detection", "Demonstration of behavior control in LLMs through manipulation of AENs"], "limitations": "", "keywords": ["Large Language Models", "Ambiguity Detection", "Neural Networks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13671", "pdf": "https://arxiv.org/pdf/2509.13671.pdf", "abs": "https://arxiv.org/abs/2509.13671", "title": "I, Robot? Socio-Technical Implications of Ultra-Personalized AI-Powered AAC; an Autoethnographic Account", "authors": ["Tobias Weinberg", "Ricardo E. Gonzalez Penuela", "Stephanie Valencia", "Thijs Roumen"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 9 figures", "summary": "Generic AI auto-complete for message composition often fails to capture the\nnuance of personal identity, requiring significant editing. While harmless in\nlow-stakes settings, for users of Augmentative and Alternative Communication\n(AAC) devices, who rely on such systems for everyday communication, this\nediting burden is particularly acute. Intuitively, the need for edits would be\nlower if language models were personalized to the communication of the specific\nuser. While technically feasible, such personalization raises socio-technical\nquestions: what are the implications of logging one's own conversations, and\nhow does personalization affect privacy, authorship, and control? We explore\nthese questions through an autoethnographic study in three phases: (1) seven\nmonths of collecting all the lead author's AAC communication data, (2)\nfine-tuning a model on this dataset, and (3) three months of daily use of\npersonalized AI suggestions. We reflect on these phases through continuous\ndiary entries and interaction logs. Our findings highlight the value of\npersonalization as well as implications on privacy, authorship, and blurring\nthe boundaries of self-expression.", "AI": {"tldr": "This paper examines the personalization of AI auto-complete systems for users of Augmentative and Alternative Communication (AAC) devices by reflecting on a study that involved data collection, model fine-tuning, and daily usage, while addressing privacy and self-expression issues.", "motivation": "The need for personalized AI systems for AAC users to reduce editing burdens and improve communication effectiveness while considering socio-technical implications.", "method": "An autoethnographic study consisting of data collection over seven months, fine-tuning a language model on this dataset, and three months of using personalized AI suggestions, supplemented by continuous diary entries and interaction logs.", "result": "Personalization of AI suggestions significantly reduced the need for edits in communication, revealing both advantages and challenges regarding privacy, authorship, and self-expression.", "conclusion": "The study highlights the dual role of personalization in enhancing communication for AAC users while raising important socio-technical questions.", "key_contributions": ["Exploration of personalization effects on AAC communication", "Insights into privacy and authorship implications", "A comprehensive autoethnographic approach to studying AI personalization"], "limitations": "The study is based on the lead author's experience, which may not be generalizable to all AAC users.", "keywords": ["Augmentative and Alternative Communication", "personalization", "AI auto-complete", "privacy", "self-expression"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2509.13672", "pdf": "https://arxiv.org/pdf/2509.13672.pdf", "abs": "https://arxiv.org/abs/2509.13672", "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "The paper introduces CL$^2$GEC, the first Continual Learning benchmark for Chinese Grammatical Error Correction, designed to evaluate error correction across multiple academic disciplines with distinct linguistic styles.", "motivation": "There is a demand for automated writing assistance tailored to diverse academic domains, but existing systems lack benchmarks for multi-disciplinary applications, particularly in handling linguistic variation through continual learning.", "method": "The authors developed a benchmark that includes 10,000 human-annotated sentences across 10 disciplines, simulating sequential exposure to diverse styles in a continual learning setting. Large language models are evaluated using standard and continual learning metrics, including tuning and adaptation methods.", "result": "Experimental results demonstrated that regularization-based methods are more effective at preventing forgetting compared to other methods, providing insight into the adaptability of error correction systems.", "conclusion": "The CL$^2$GEC benchmark lays the groundwork for advancing research in adaptive grammatical error correction across various academic fields, enabling better writing assistance tools.", "key_contributions": ["Introduction of the CL$^2$GEC benchmark for CGEC in multi-disciplinary contexts.", "Evaluation of large language models using continual learning metrics.", "Demonstration of the effectiveness of regularization methods in mitigating forgetting."], "limitations": "", "keywords": ["CGEC", "continual learning", "grammatical error correction", "large language models", "benchmark"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.13679", "pdf": "https://arxiv.org/pdf/2509.13679.pdf", "abs": "https://arxiv.org/abs/2509.13679", "title": "From Prompts to Reflection: Designing Reflective Play for GenAI Literacy", "authors": ["Qianou Ma", "Megan Chai", "Yike Tan", "Jihun Choi", "Jini Kim", "Erik Harpstead", "Geoff Kauffman", "Tongshuang Wu"], "categories": ["cs.HC"], "comment": null, "summary": "The wide adoption of Generative AI (GenAI) in everyday life highlights the\nneed for greater literacy around its evolving capabilities, biases, and\nlimitations. While many AI literacy efforts focus on children through\ngame-based learning, few interventions support adults in developing a nuanced,\nreflective understanding of GenAI via playful exploration. To address the gap,\nwe introduce ImaginAItion, a multiplayer party game inspired by Drawful and\ngrounded in the reflective play framework to surface model defaults, biases,\nand human-AI perception gaps through prompting and discussion. From ten\nsessions (n=30), we show how gameplay helped adults recognize systematic biases\nin GenAI, reflect on humans and AI interpretation differences, and adapt their\nprompting strategies. We also found that group dynamics and composition, such\nas expertise and diversity, amplified or muted reflection. Our work provides a\nstarting point to scale critical GenAI literacy through playful, social\ninterventions resilient to rapidly evolving technologies.", "AI": {"tldr": "The paper presents ImaginAItion, a multiplayer game designed to enhance adults' understanding of Generative AI through reflective play.", "motivation": "There is a need for effective AI literacy interventions for adults, as many current efforts focus on children and do not address adult understanding of Generative AI capabilities and biases.", "method": "The study involved ten gameplay sessions with 30 adult participants, where the game facilitated discussions about biases in Generative AI and differences in human-AI interpretation.", "result": "Participants recognized systematic biases in Generative AI and improved their prompting strategies through gameplay reflection. Group dynamics influenced the depth of reflection.", "conclusion": "ImaginAItion serves as a valuable tool for fostering critical literacy about Generative AI in adults, with potential for scaling such interventions.", "key_contributions": ["Introduction of a multiplayer party game for AI literacy among adults.", "Empirical evidence showing enhanced recognition of biases and prompting strategies through gameplay.", "Insights on how group dynamics affect learning outcomes."], "limitations": "The study is limited to a small sample size and specific group settings, which may affect the generalizability of results.", "keywords": ["Generative AI", "AI literacy", "reflective play", "game-based learning", "adult education"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.13677", "pdf": "https://arxiv.org/pdf/2509.13677.pdf", "abs": "https://arxiv.org/abs/2509.13677", "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG is a novel framework for Controlled Text Generation that improves precise control in text generation through multi-agent workflows and introduces an auto-prompt module, achieving state-of-the-art results.", "motivation": "Despite advancements in NLP, Controlled Text Generation faces challenges in achieving fine control and scalability, necessitating a scalable solution for better text generation.", "method": "The paper introduces AgentCTG, which leverages multi-agent workflows for simulation of control mechanisms and an auto-prompt module to enhance generation effectiveness.", "result": "AgentCTG achieves state-of-the-art performance on several public datasets and significantly improves online navigation experiences in role-playing applications.", "conclusion": "The proposed framework enhances text generation control, improves user engagement, and offers a personalized experience in online communities.", "key_contributions": ["Introduction of AgentCTG framework for improved text generation control", "Implementation of multi-agent workflows for enhanced collaboration", "Development of a Character-Driven Rewriting task."], "limitations": "", "keywords": ["Controlled Text Generation", "Multi-Agent Workflows", "Natural Language Processing", "Character-Driven Rewriting", "Text Generation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.13742", "pdf": "https://arxiv.org/pdf/2509.13742.pdf", "abs": "https://arxiv.org/abs/2509.13742", "title": "Spatial Balancing: Harnessing Spatial Reasoning to Balance Scientific Exposition and Narrative Engagement in LLM-assisted Science Communication Writing", "authors": ["Kexue Fu", "Jiaye Leng", "Yawen Zhang", "Jingfei Huang", "Yihang Zuo", "Runze Cai", "Zijian Ding", "Ray LC", "Shengdong Zhao", "Qinyuan Lei"], "categories": ["cs.HC"], "comment": null, "summary": "Balancing scientific exposition and narrative engagement is a central\nchallenge in science communication. To examine how to achieve balance, we\nconducted a formative study with four science communicators and a literature\nreview of science communication practices, focusing on their workflows and\nstrategies. These insights revealed how creators iteratively shift between\nexposition and engagement but often lack structured support. Building on this,\nwe developed SpatialBalancing, a co-writing system that connects human spatial\nreasoning with the linguistic intelligence of large language models. The system\nvisualizes revision trade-offs in a dual-axis space, where users select\nstrategy-based labels to generate, compare, and refine versions during the\nrevision process. This spatial externalization transforms revision into spatial\nnavigation, enabling intentional iterations that balance scientific rigor with\nnarrative appeal. In a within-subjects study (N=16), SpatialBalancing enhanced\nmetacognitive reflection, flexibility, and creative exploration, demonstrating\nhow coupling spatial reasoning with linguistic generation fosters monitoring in\niterative science communication writing.", "AI": {"tldr": "The paper presents SpatialBalancing, a co-writing system designed to enhance science communication by balancing scientific exposition and narrative engagement through spatial reasoning and large language models.", "motivation": "The study aims to address the challenge of achieving a balance between scientific exposition and narrative engagement in science communication.", "method": "A formative study with science communicators and a literature review to understand workflows, followed by the development of SpatialBalancing, which visualizes revision trade-offs in a dual-axis space.", "result": "SpatialBalancing significantly enhanced participants' metacognitive reflection, flexibility, and creative exploration during the science communication writing process.", "conclusion": "Combining spatial reasoning with linguistic generation in SpatialBalancing enables intentional iterations that improve the quality of science communication by balancing rigor and appeal.", "key_contributions": ["Development of SpatialBalancing, a novel co-writing system.", "Introduction of a dual-axis visualization for revision trade-offs.", "Demonstration of enhanced metacognitive reflection and creative exploration through the system."], "limitations": "", "keywords": ["science communication", "large language models", "SpatialBalancing", "revision strategies", "metacognitive reflection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13683", "pdf": "https://arxiv.org/pdf/2509.13683.pdf", "abs": "https://arxiv.org/abs/2509.13683", "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE is a novel framework that enhances LLM context fidelity by integrating in-context evidence during reasoning without requiring extensive labeled data.", "motivation": "To improve the consistency and accuracy of LLM responses by enabling them to effectively use provided context during reasoning.", "method": "CARE teaches LLMs to utilize their own retrieval capabilities to integrate in-context evidence directly within their reasoning processes.", "result": "CARE significantly enhances retrieval accuracy and answer generation performance, outpacing traditional methods and supervised fine-tuning on various QA benchmarks.", "conclusion": "The CARE framework marks a significant improvement in making large language models more accurate, reliable, and efficient for knowledge-intensive applications.", "key_contributions": ["Proposes a framework (CARE) for enhanced context integration in LLMs", "Shows significant improvements over existing fine-tuning and retrieval methods", "Demonstrates effectiveness across real-world and counterfactual QA benchmarks"], "limitations": "", "keywords": ["large language models", "context fidelity", "retrieval-augmented reasoning", "evidence integration", "QA benchmarks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13892", "pdf": "https://arxiv.org/pdf/2509.13892.pdf", "abs": "https://arxiv.org/abs/2509.13892", "title": "Synthetic Data Generation for Screen Time and App Usage", "authors": ["Gustavo Kruger", "Nikhil Sachdeva", "Michael Sobolev"], "categories": ["cs.HC", "cs.AI", "I.2; J.4"], "comment": "14 pages", "summary": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models.", "AI": {"tldr": "This paper explores using large language models (LLMs) to generate synthetic smartphone usage datasets to overcome challenges in real-world data collection.", "motivation": "Smartphone usage data is crucial for understanding human-computer interaction and behavior, but gathering such data is hindered by cost, privacy, and sample biases.", "method": "The authors conducted a case study comparing four prompt strategies for generating smartphone usage data using an LLM, evaluating the impact of prompt detail and seed data inclusion.", "result": "The study found that detailed prompts significantly improved the plausibility and structure of synthetic smartphone usage datasets, though capturing diverse human behavior remains a challenge.", "conclusion": "While LLMs can generate usable synthetic datasets, careful prompt design and evaluation metrics specific to use cases are essential for balancing data fidelity and diversity.", "key_contributions": ["Exploration of LLMs for synthetic smartphone usage data generation", "Analysis of prompt strategies affecting data quality", "Insights into trade-offs between data fidelity and diversity"], "limitations": "Challenges in capturing diverse human behavioral patterns and evaluating data quality remain.", "keywords": ["smartphone usage", "synthetic data", "large language models", "human behavior", "data generation"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2509.13695", "pdf": "https://arxiv.org/pdf/2509.13695.pdf", "abs": "https://arxiv.org/abs/2509.13695", "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "This paper explores the performance of Large Language Models (LLMs) in Natural Language Inference (NLI) with a focus on comparatives in Japanese, highlighting their challenges and the impact of prompt formats.", "motivation": "To evaluate the robustness of LLMs in handling NLI tasks involving numerical and logical expressions, particularly in non-dominant languages like Japanese.", "method": "The authors constructed a Japanese NLI dataset centering on comparatives and assessed various LLMs in zero-shot and few-shot scenarios.", "result": "The findings indicate that model performance is highly sensitive to prompt formats in zero-shot conditions and affected by gold labels in few-shot contexts. LLMs had difficulty with linguistic phenomena unique to Japanese, though prompts with logical semantic representations improved performance.", "conclusion": "The study highlights significant challenges faced by LLMs in NLI tasks for Japanese and suggests that prompt design can enhance their inferential capabilities.", "key_contributions": ["Creation of a Japanese NLI dataset focusing on comparatives", "Demonstration of LLMs' sensitivity to prompt formats and few-shot examples", "Identification of unique linguistic challenges in Japanese NLI."], "limitations": "The study primarily focuses on Japanese, limiting its applicability to other languages. Further exploration of different prompt formats and their effects is needed.", "keywords": ["Natural Language Inference", "Large Language Models", "Comparatives", "Japanese", "Zero-shot Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.13899", "pdf": "https://arxiv.org/pdf/2509.13899.pdf", "abs": "https://arxiv.org/abs/2509.13899", "title": "AI as a teaching tool and learning partner", "authors": ["Steven Watterson", "Sarah Atkinson", "Elaine Murray", "Andrew McDowell"], "categories": ["cs.HC"], "comment": "6 Pages, 1 Figure", "summary": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour.", "AI": {"tldr": "This paper explores the impact of LLM-based tools in undergraduate and postgraduate teaching programs, evaluating a chatbot and audio podcasts for enhancing learning.", "motivation": "To determine how to effectively integrate LLMs into education and assess their impact on teaching and learning.", "method": "Implemented a LLM-powered chatbot and AI-generated audio podcasts in two courses, then surveyed students on their experiences and attitudes.", "result": "Students positively perceived the chatbot, reporting it enhanced their learning experience, while the audio podcasts had mixed reception, with only localized interest among users.", "conclusion": "Integrating LLMs into education tools can be beneficial, particularly chatbots, but the effectiveness of other formats like podcasts may vary among students.", "key_contributions": ["Demonstrated positive student reactions to an LLM-based chatbot in education.", "Identified mixed responses to AI-generated audio content in teaching.", "Provided insights on the potential of AI tools in enhancing learning experiences."], "limitations": "Limited to biological courses and small class sizes, which may not generalize to other subjects or larger populations.", "keywords": ["Large Language Models", "education", "AI tools", "chatbot", "podcasts"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2509.13696", "pdf": "https://arxiv.org/pdf/2509.13696.pdf", "abs": "https://arxiv.org/abs/2509.13696", "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "categories": ["cs.CL"], "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "This work explores the use of instruction-tuned LLMs for clinical classification tasks, integrating clinical notes and structured EHR data.", "motivation": "To investigate the potential of LLMs in handling clinical classification tasks with structured data, an area that has been largely unexplored.", "method": "We implemented DSPy-based prompt optimization to adapt instruction-tuned LLMs for processing clinical notes along with structured Electronic Health Record (EHR) inputs.", "result": "The adapted LLMs achieved performance comparable to specialized multimodal systems, while maintaining lower complexity and enhanced adaptability across various tasks.", "conclusion": "The findings suggest that LLMs can effectively support clinical classification tasks with both qualitative and quantitative data.", "key_contributions": ["Adopting instruction-tuned LLMs to address structured data in clinical settings", "Demonstrating comparable performance to multimodal systems with lower complexity", "Implementing DSPy-based prompt optimization for improved adaptability"], "limitations": "", "keywords": ["large language models", "clinical classification", "structured data", "EHR", "prompt optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.14050", "pdf": "https://arxiv.org/pdf/2509.14050.pdf", "abs": "https://arxiv.org/abs/2509.14050", "title": "AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart Devices Enhances Privacy Protection", "authors": ["Wael Albayaydh", "Ivan Flechais", "Rui Zhao", "Jood Albayaydh"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Privacy concerns and fears of unauthorized access in smart home devices often\nstem from misunderstandings about how data is collected, used, and protected.\nThis study explores how AI-powered tools can offer innovative privacy\nprotections through clear, personalized, and contextual support to users.\nThrough 23 in-depth interviews with users, AI developers, designers, and\nregulators, and using Grounded Theory analysis, we identified two key themes:\nAspirations for AI-Enhanced Privacy - how users perceive AI's potential to\nempower them, address power imbalances, and improve ease of use- and AI\nEthical, Security, and Regulatory Considerations-challenges in strengthening\ndata security, ensuring regulatory compliance, and promoting ethical AI\npractices. Our findings contribute to the field by uncovering user aspirations\nfor AI-driven privacy solutions, identifying key security and ethical\nchallenges, and providing actionable recommendations for all stakeholders,\nparticularly targeting smart device designers and AI developers, to guide the\nco-design of AI tools that enhance privacy protection in smart home devices. By\nbridging the gap between user expectations, AI capabilities, and regulatory\nframeworks, this work offers practical insights for shaping the future of\nprivacy-conscious AI integration in smart homes.", "AI": {"tldr": "This study examines how AI can enhance privacy in smart home devices through user-centered design and addresses privacy concerns.", "motivation": "Address misunderstandings around data privacy in smart home devices and explore AI’s potential to enhance user privacy.", "method": "Conducted 23 in-depth interviews with users, AI developers, designers, and regulators; analyzed data using Grounded Theory.", "result": "Identified themes of user aspirations for AI-enhanced privacy and ethical/security challenges; provided recommendations for stakeholders.", "conclusion": "The study offers insights for co-designing AI tools that enhance privacy in smart homes, bridging user expectations with AI capabilities.", "key_contributions": ["Identified user aspirations for AI-driven privacy solutions", "Outlined key ethical and security challenges", "Provided actionable recommendations for smart device designers and AI developers"], "limitations": "", "keywords": ["AI", "privacy", "smart homes", "user-centered design", "ethics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13702", "pdf": "https://arxiv.org/pdf/2509.13702.pdf", "abs": "https://arxiv.org/abs/2509.13702", "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "authors": ["Xiao Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "The paper introduces DSCC-HS, a new approach for suppressing hallucinations in LLMs by dynamically calibrating model outputs during decoding.", "motivation": "Hallucinations in large language models hinder their reliable use in applications, prompting the need for more proactive solutions.", "method": "The authors developed Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS), which utilizes a compact proxy model to adjust outputs in real-time based on adversarial training.", "result": "DSCC-HS achieved state-of-the-art performance on both TruthfulQA (99.2% Factual Consistency Rate) and BioGEN (highest FActScore of 46.50).", "conclusion": "The approach effectively improves the factual accuracy of large language models without altering their architecture.", "key_contributions": ["Introduction of a proactive framework for hallucination suppression in LLMs.", "Utilization of dual-process cognitive theory for real-time output adjustment.", "Achievement of state-of-the-art results in benchmarks like TruthfulQA and BioGEN."], "limitations": "", "keywords": ["Large Language Models", "Hallucination Suppression", "Dynamic Calibration", "Factual Consistency", "Machine Learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.14056", "pdf": "https://arxiv.org/pdf/2509.14056.pdf", "abs": "https://arxiv.org/abs/2509.14056", "title": "EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation", "authors": ["Jiahui An", "Bingjie Cheng", "Dmitriy Rudyka", "Elisa Donati", "Sara Fabrikant"], "categories": ["cs.HC", "F.2.2, I.2.7"], "comment": "11 pages, 7 figures", "summary": "Brain computer interfaces enable real-time monitoring of cognitive load, but\ntheir effectiveness in dynamic navigation contexts is not well established.\nUsing an existing VR navigation dataset, we examined whether EEG signals can\nclassify cognitive load during map-based wayfinding and whether classification\naccuracy depends more on task complexity or on individual traits. EEG\nrecordings from forty-six participants navigating routes with 3, 5, or 7 map\nlandmarks were analyzed with a nested cross-validation framework across\nmultiple machine learning models. Classification achieved mean accuracies up to\n90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class\nproblem, both well above chance. Demographic and cognitive variables (age,\ngender, spatial ability, working memory) showed no significant influence. These\nfindings demonstrate that task demands outweigh individual differences in\nshaping classification performance, highlighting the potential for\ntask-adaptive navigation systems that dynamically adjust map complexity in\nresponse to real-time cognitive states.", "AI": {"tldr": "The study explores the effectiveness of EEG signals in classifying cognitive load during VR navigation tasks with varying map complexity.", "motivation": "To investigate how well EEG signals can classify cognitive load in dynamic navigation contexts, and to understand the influence of task complexity versus individual traits.", "method": "EEG signals from 46 participants were analyzed as they navigated routes with different numbers of map landmarks (3, 5, 7), using a nested cross-validation framework across various machine learning models.", "result": "Mean classification accuracies reached 90.8% for binary contrasts and 78.7% for the three-class problem, indicating that task demands were more significant than individual differences in shaping results.", "conclusion": "The study concludes that task demands strongly influence cognitive load classification, suggesting potential for developing navigation systems that adapt map complexity based on real-time cognitive load.", "key_contributions": ["Demonstrated high classification accuracy of cognitive load using EEG during navigation tasks", "Identified that task complexity impacts cognitive load assessment more than individual traits", "Proposed potential for adaptive navigation systems based on cognitive states"], "limitations": "", "keywords": ["cognitive load", "EEG", "navigation", "machine learning", "brain-computer interfaces"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2509.13706", "pdf": "https://arxiv.org/pdf/2509.13706.pdf", "abs": "https://arxiv.org/abs/2509.13706", "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "The paper presents an NLP screening tool for detecting high-severity incident reports in radiation oncology, showing that models like BlueBERT can achieve performance similar to human reviewers.", "motivation": "To improve the efficiency of reviewing incident reports in healthcare, which is currently a manual and time-intensive process requiring expert knowledge.", "method": "Two types of NLP models were trained and evaluated: support vector machines (SVM) and BlueBERT, using incident report datasets from one institution and the IAEA SAFRON database.", "result": "The SVM and BlueBERT models achieved AUROC scores of 0.82 and 0.81 respectively on the institution's test set, while the BlueBERT_TRANSFER enhanced cross-institution performance to an AUROC of 0.78.", "conclusion": "Cross-institution NLP models developed for incident report analysis can detect high-severity reports with performance comparable to human experts.", "key_contributions": ["Development of NLP models for incident report analysis in radiation oncology.", "Introduction of a transfer learning approach with BlueBERT for improved performance across institutions.", "Demonstration of NLP model effectiveness in comparison to human performance on curated datasets."], "limitations": "Performance on external datasets without transfer learning was limited, indicating a need for further refinement.", "keywords": ["NLP", "incident reports", "radiation oncology", "BlueBERT", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.14132", "pdf": "https://arxiv.org/pdf/2509.14132.pdf", "abs": "https://arxiv.org/abs/2509.14132", "title": "When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training", "authors": ["Julia S. Dollis", "Iago A. Brito", "Fernanda B. Färber", "Pedro S. F. B. Ribeiro", "Rafael T. Sousa", "Arlindo R. Galvão Filho"], "categories": ["cs.HC", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.", "AI": {"tldr": "This paper presents a framework that integrates large language models into virtual reality to train interpersonal skills in medical education.", "motivation": "The effectiveness of virtual reality for training interpersonal skills is hampered by a lack of psychologically plausible virtual humans, particularly in high-stakes fields like medical education.", "method": "A mixed-method, within-subjects study was conducted with licensed physicians using simulated consultations with a new VR system that integrates LLMs to create coherent virtual patients.", "result": "The approach is feasible and perceived as rewarding and effective by physicians, highlighting important design principles for future training environments.", "conclusion": "The validated framework offers insights into creating socially intelligent VR training systems and emphasizes the importance of perceived authenticity in challenges.", "key_contributions": ["Integration of LLMs into VR for medical training", "Identification of design principles for virtual patients", "Validation of the effectiveness in a physician training context"], "limitations": "", "keywords": ["Virtual Reality", "Large Language Models", "Medical Education", "Interpersonal Skills", "Training Enhancement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13723", "pdf": "https://arxiv.org/pdf/2509.13723.pdf", "abs": "https://arxiv.org/abs/2509.13723", "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "The paper presents Dual-Stage Progressive Compression (DSPC), a training-free method for compressing prompts in large language models to reduce computational costs without sacrificing performance.", "motivation": "To tackle the issue of prompt inflation in large language models which increases computational costs while trying to improve output accuracy.", "method": "DSPC employs a two-stage approach: the coarse-grained stage filters low-value sentences based on TF-IDF, and the fine-grained stage prunes low-utility tokens based on attention contribution, loss difference, and positional importance.", "result": "DSPC demonstrates consistent improvements on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo, achieving a performance score of 49.17 on the FewShot task of the Longbench dataset using 3x fewer tokens compared to existing methods.", "conclusion": "The DSPC method effectively reduces token usage while maintaining or improving the performance of language models, thus addressing prompt inflation without additional training costs.", "key_contributions": ["Introduction of a training-free prompt compression method (DSPC)", "Dual-stage approach: semantic filtering and token pruning", "Significant performance improvements on standard benchmarks with reduced token requirements."], "limitations": "", "keywords": ["large language models", "prompt compression", "natural language processing", "DSPC", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13734", "pdf": "https://arxiv.org/pdf/2509.13734.pdf", "abs": "https://arxiv.org/abs/2509.13734", "title": "Implementing a Logical Inference System for Japanese Comparatives", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "This paper presents ccg-jcomp, a logical inference system designed for Natural Language Inference (NLI) involving Japanese comparatives, evaluating its performance against existing LLMs.", "motivation": "To address the challenges in Natural Language Inference (NLI) with Japanese comparatives, which differ significantly from English comparatives.", "method": "The study proposes a logic-based approach using compositional semantics to create ccg-jcomp, specifically designed for handling Japanese comparative expressions.", "result": "The proposed ccg-jcomp system demonstrates superior accuracy in NLI tasks involving Japanese comparatives when compared to existing Large Language Models (LLMs).", "conclusion": "ccg-jcomp effectively addresses the limitations of applying existing logical inference systems designed for English to Japanese, providing a robust solution for NLI in Japanese.", "key_contributions": ["Development of a novel logical inference system (ccg-jcomp) for Japanese comparatives.", "Evaluation against existing LLMs to highlight effectiveness and accuracy.", "Addressing morphological and semantic differences between Japanese and English comparatives."], "limitations": "The focus is solely on Japanese comparatives, limiting applicability to other languages or comparative forms.", "keywords": ["Natural Language Inference", "Japanese", "comparatives", "large language models", "compositional semantics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.13775", "pdf": "https://arxiv.org/pdf/2509.13775.pdf", "abs": "https://arxiv.org/abs/2509.13775", "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "categories": ["cs.CL", "cs.AI"], "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "This paper explores data-efficient and parameter-efficient strategies for Arabic Dialect Identification (ADI) using Large Language Models (LLMs), focusing on soft-prompting and few-shot inferences.", "motivation": "The motivation behind this research is to improve the identification of Arabic dialects using LLMs through data-efficient and parameter-efficient methodologies, which is crucial given the nuanced nature of Arabic dialects.", "method": "The researchers investigated various soft-prompting strategies (prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2), as well as LoRA reparameterizations, and conducted experiments using Arabic-specific encoder models on major datasets while analyzing n-shot inferences on different multilingual models.", "result": "The results indicated that LLMs face challenges in distinguishing dialectal nuances in few-shot or zero-shot scenarios, but soft-prompted encoder variants showed improved performance, with LoRA-based fine-tuned models achieving the best results, even outperforming full fine-tuning.", "conclusion": "The study concludes that while LLMs struggle with Arabic dialect identification under limited data conditions, the use of efficient prompting techniques and architectures can enhance performance significantly.", "key_contributions": ["Investigation of various soft-prompting strategies for Arabic dialect identification", "Comparison of performance between hard prompting and soft prompting methodologies", "Demonstration of superiority of LoRA-based models over traditional fine-tuning approaches"], "limitations": "The models still exhibit struggles in distinguishing dialects in few-shot and zero-shot conditions, indicating a need for further improvements.", "keywords": ["Arabic Dialect Identification", "Large Language Models", "soft prompting strategies", "parameter-efficient tuning", "LoRA"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.13790", "pdf": "https://arxiv.org/pdf/2509.13790.pdf", "abs": "https://arxiv.org/abs/2509.13790", "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS is a novel framework for dynamic instruction tuning of LLMs, addressing the limitations of static curriculum methods by adapting to evolving model capabilities.", "motivation": "To enhance the performance of large language models in instruction tuning, addressing the shortcomings of static curriculum tuning methods that do not adapt to model progress.", "method": "CAMPUS introduces a dynamic selection of sub-curriculum, enables competency-aware adjustments to the curriculum schedule, and implements multiple difficulty-based scheduling techniques.", "result": "Extensive experiments demonstrate that CAMPUS outperforms existing state-of-the-art methods in instruction tuning efficiency.", "conclusion": "CAMPUS provides a more adaptive and effective approach for instruction tuning, surpassing traditional static methods in performance.", "key_contributions": ["Dynamic selection for sub-curriculum", "Competency-aware curriculum schedule adjustments", "Multiple difficulty-based scheduling"], "limitations": "", "keywords": ["Instruction tuning", "Curriculum learning", "Large language models", "Adaptive learning", "Competency-aware tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13803", "pdf": "https://arxiv.org/pdf/2509.13803.pdf", "abs": "https://arxiv.org/abs/2509.13803", "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "authors": ["Laura García-Sardiña", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "categories": ["cs.CL"], "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "This paper investigates how grammatical gender in job titles impacts automatic job ranking systems, proposing a methodology to evaluate and compare gender bias.", "motivation": "To address gender bias in automatic job ranking systems caused by grammatical gender assignment in job titles.", "method": "The paper introduces metrics for ranking comparison controlling for gender, specifically using RBO (Rank-Biased Overlap), and generates test sets for a job title matching task across four grammatical gender languages.", "result": "All evaluated multilingual models exhibited varying degrees of gender bias in job title ranking.", "conclusion": "The findings highlight the importance of considering grammatical gender in job ranking systems to mitigate bias.", "key_contributions": ["Introduction of RBO for gender bias evaluation in job titles", "Creation of test sets for job title matching in multiple languages", "Empirical evaluation of existing multilingual models for gender bias"], "limitations": "The study focuses primarily on grammatical gender and may not consider other forms of bias.", "keywords": ["gender bias", "job ranking", "RBO", "multilingual models", "grammatical gender"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.13813", "pdf": "https://arxiv.org/pdf/2509.13813.pdf", "abs": "https://arxiv.org/abs/2509.13813", "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "This paper presents a geometric framework for uncertainty quantification in large language models that improves hallucination detection by providing both global and local uncertainty estimates using only black-box model access.", "motivation": "To address the hallucination problem in large language models, which generate plausible but incorrect answers, by providing a robust black-box method for uncertainty quantification.", "method": "The proposed framework uses archetypal analysis to derive global and local uncertainty estimates; it introduces Geometric Volume for global uncertainty and Geometric Suspicion for ranking response reliability.", "result": "The framework shows competitive performance on short answer datasets and demonstrates superior performance on medical datasets, which are critical due to the risks associated with hallucinations.", "conclusion": "The framework provides valuable insights into reliability attribution for individual responses and is theoretically justified by linking convex hull volume to entropy.", "key_contributions": ["Geometric Volume for global uncertainty estimation", "Geometric Suspicion for ranking individual response reliability", "A black-box approach for uncertainty quantification without needing internal model access."], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "hallucination detection", "archetypal analysis", "medical datasets"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13814", "pdf": "https://arxiv.org/pdf/2509.13814.pdf", "abs": "https://arxiv.org/abs/2509.13814", "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "categories": ["cs.CL"], "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "This paper discusses AutoMin 2025, focusing on automatic meeting summarization and question answering based on meeting transcripts.", "motivation": "The AutoMin shared task aims to advance the field of automatic meeting summarization and evaluate the capabilities of large language models (LLMs) in this context.", "method": "The task included minuting structured meeting minutes in English and Czech, as well as a QA component based on meeting transcripts, with evaluations involving baseline systems and LLMs.", "result": "Limited participation was noted in 2025, with only one team in the minuting task and two in QA, but baseline evaluations were included for comprehensive analysis.", "conclusion": "Despite reduced participation, the inclusion of baseline systems aims to provide insights into the effectiveness of current LLMs in summarizing and understanding meeting content.", "key_contributions": ["Introduction of a new QA task based on meeting transcripts", "Evaluation of LLMs using multiple baseline systems", "Coverage of two languages and domains for the minuting task"], "limitations": "Limited participant engagement reduced the variety of approaches in evaluation.", "keywords": ["automatic meeting summarization", "question answering", "large language models", "multilingual", "conference proceedings"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.13677", "pdf": "https://arxiv.org/pdf/2509.13677.pdf", "abs": "https://arxiv.org/abs/2509.13677", "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "The paper presents AgentCTG, a novel framework for Controlled Text Generation that enhances fine-grained control over text creation by simulating multi-agent workflows and introducing an auto-prompt module.", "motivation": "To address the challenges in Controlled Text Generation (CTG), including cost, scalability, and the need for domain knowledge learning.", "method": "The paper introduces AgentCTG, which utilizes collaboration methods among agents and an auto-prompt module to enhance text generation effectiveness. It includes a new Character-Driven Rewriting task.", "result": "AgentCTG achieves state-of-the-art results on multiple public datasets and significantly improves the driving experience in role-playing online navigation applications.", "conclusion": "The framework enables more immersive interactions by optimizing contextual text generation, enhancing personalization and user engagement in online platforms.", "key_contributions": ["Introduction of AgentCTG framework for enhanced control in CTG", "Development of the auto-prompt module", "Creation of the Character-Driven Rewriting task", "Demonstration of significant improvements in practical applications."], "limitations": "", "keywords": ["Controlled Text Generation", "Multi-agent workflows", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13835", "pdf": "https://arxiv.org/pdf/2509.13835.pdf", "abs": "https://arxiv.org/abs/2509.13835", "title": "Large Language Models Discriminate Against Speakers of German Dialects", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "The paper investigates biases in large language models (LLMs) regarding dialect perception in German, revealing significant negative stereotypes against dialect speakers.", "motivation": "To explore societal stereotypes faced by dialect speakers and their reflection in the behaviors of large language models (LLMs).", "method": "The study employs two tasks (an association task and a decision task) and constructs a novel evaluation corpus comparing dialect sentences with standard German ones.", "result": "The findings indicate that all evaluated LLMs show significant bias against German dialect speakers, with negative associations and decision-making patterns.", "conclusion": "Labeling linguistic demographics amplifies bias more than implicit mentions, highlighting the need for awareness in LLM behaviors.", "key_contributions": ["Introduction of a novel evaluation corpus for dialect bias assessment", "Identification of significant biases in LLMs against German dialect speakers", "Demonstration that explicit demographic labeling intensifies bias in LLMs"], "limitations": "", "keywords": ["Dialect Bias", "Large Language Models", "Sociolinguistics", "German Dialects", "AI Ethics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.13869", "pdf": "https://arxiv.org/pdf/2509.13869.pdf", "abs": "https://arxiv.org/abs/2509.13869", "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "authors": ["Yang Liu", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "This study analyzes the alignment of large language models (LLMs) with human values regarding social biases across different bias scenarios, revealing inconsistencies and preferences among models.", "motivation": "To investigate the differences in alignment of LLMs with human values in various bias scenarios, focusing on social biases.", "method": "The study analyzes 12 LLMs from four model families across four datasets to assess misalignment rates and preferences for different scenario types, along with understanding capacity evaluated through generated explanations.", "result": "LLMs with larger parameter scales do not show lower misalignment rates; they exhibit alignment preferences for certain scenario types, with consistency in judgment among the same model family but no significant differences in understanding of social biases across LLMs.", "conclusion": "Fine-tuned smaller language models can provide more readable explanations of social biases, albeit with lower agreement to model norms; different LLMs have unique strengths and weaknesses in misalignment.", "key_contributions": ["Analysis of LLM alignment with human values in social bias scenarios", "Comparison of explanation generation between small and large models", "Insights on judgment consistency within model families"], "limitations": "Study may not cover all variations of biases or LLM architectures, potential observer bias in scenario selection.", "keywords": ["large language models", "human values", "social biases", "alignment", "explanation generation"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2509.14023", "pdf": "https://arxiv.org/pdf/2509.14023.pdf", "abs": "https://arxiv.org/abs/2509.14023", "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "This study compares audio-based evaluations with traditional text-only assessments in machine translation, highlighting the importance of incorporating speech in future evaluation frameworks.", "motivation": "The study addresses the limitations of traditional text-centric assessments in machine translation by proposing a more natural evaluation method through speech.", "method": "The research compares text-only and audio-based evaluations of 10 machine translation systems using crowd-sourced judgments from Amazon Mechanical Turk, along with statistical significance testing.", "result": "The results show that crowd-sourced audio assessments yield rankings consistent with text-only evaluations but also highlight significant differences in some cases between systems, indicating the value of speech-based assessments.", "conclusion": "The authors suggest that incorporating audio-based evaluations into machine translation assessment frameworks could enhance quality evaluations and reflect real-world usage more effectively.", "key_contributions": ["Proposed a novel audio-based evaluation method for machine translation quality assessment.", "Demonstrated statistical consistency between audio and text-only evaluations while uncovering significant differences among systems.", "Argued for the inclusion of speech as a critical component in future MT evaluation methodologies."], "limitations": "Limited to evaluation of 10 MT systems; findings may not generalize to other MT applications; reliance on crowd-sourced judgments could introduce variability.", "keywords": ["Machine Translation", "Speech Evaluation", "Crowd-sourced Assessment", "Multimodal Evaluation", "Statistical Testing"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2509.13879", "pdf": "https://arxiv.org/pdf/2509.13879.pdf", "abs": "https://arxiv.org/abs/2509.13879", "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "CER is a novel biomedical fact-checking framework integrating evidence retrieval with large language model reasoning, mitigating misinformation risks in healthcare.", "motivation": "Address the risks posed by misinformation in healthcare and enhance public trust in medical systems through automated fact-checking.", "method": "The CER framework combines scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction to validate biomedical claims.", "result": "Evaluations on expert-annotated datasets show CER achieves state-of-the-art performance and demonstrates promising cross-dataset generalization.", "conclusion": "The integration of advanced retrieval techniques and large language models in CER effectively mitigates the risk of misinformation, providing grounded evidence-based outputs.", "key_contributions": ["Introduction of the CER framework for biomedical fact-checking.", "Integration of scientific evidence retrieval and large language model reasoning.", "Demonstration of state-of-the-art performance on expert-annotated datasets."], "limitations": "", "keywords": ["Misinformation", "Biomedical Fact-Checking", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.11824", "pdf": "https://arxiv.org/pdf/2408.11824.pdf", "abs": "https://arxiv.org/abs/2408.11824", "title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions", "authors": ["Yanda Li", "Chi Zhang", "Wenjia Jiang", "Wanqi Yang", "Bin Fu", "Pei Cheng", "Xin Chen", "Ling Chen", "Yunchao Wei"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon.", "AI": {"tldr": "This paper presents a novel framework for LLM-driven multimodal agents on mobile devices, enhancing user interface interactions through adaptable task execution.", "motivation": "The advancement of Multimodal Large Language Models has led to the need for more sophisticated LLM-driven agents that can improve interactions with software interfaces.", "method": "The framework includes an exploration phase for documenting UI functionalities and a deployment phase utilizing RAG technology for efficient task execution.", "result": "The experimental results show superior performance of the agent across various benchmarks, demonstrating effectiveness in real-world applications.", "conclusion": "The framework successfully adapts to various applications while performing complex task workflows, showing promise for future mobile applications.", "key_contributions": ["Introduced a novel LLM-based multimodal agent framework for mobile devices.", "Demonstrated adaptability in handling customized workflows across multiple applications.", "Showcased superior performance through experimental benchmarks."], "limitations": "", "keywords": ["Multimodal Large Language Models", "LLM-driven agents", "Mobile applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13888", "pdf": "https://arxiv.org/pdf/2509.13888.pdf", "abs": "https://arxiv.org/abs/2509.13888", "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "Introducing CER, a framework for biomedical fact-checking that combines evidence retrieval, reasoning via LLMs, and supervised veracity prediction to enhance the accuracy of health-related claims.", "motivation": "Address misinformation in healthcare which poses risks to public health and trust in medical systems, highlighting the unique challenges in validating biomedical claims.", "method": "CER integrates scientific evidence retrieval, reasoning with large language models, and uses supervised learning for veracity prediction.", "result": "Evaluations show CER achieves state-of-the-art performance on expert-annotated datasets and demonstrates promising generalization across different datasets.", "conclusion": "The integration of LLM capabilities with advanced retrieval techniques effectively reduces the risk of hallucinations in health-related claims, promoting evidence-based outputs.", "key_contributions": ["Introduction of CER framework for biomedical fact-checking", "Integration of large language models with evidence retrieval", "Demonstration of state-of-the-art performance on benchmark datasets"], "limitations": "", "keywords": ["biomedical fact-checking", "machine learning", "healthcare misinformation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.05347", "pdf": "https://arxiv.org/pdf/2502.05347.pdf", "abs": "https://arxiv.org/abs/2502.05347", "title": "The Role of Human Creativity in the Presence of AI Creativity Tools at Work: A Case Study on AI-Driven Content Transformation in Journalism", "authors": ["Sitong Wang", "Jocelyn McKinnon-Crowley", "Tao Long", "Kian Loong Lua", "Keren Henderson", "Kevin Crowston", "Jeffrey V. Nickerson", "Mark Hansen", "Lydia B. Chilton"], "categories": ["cs.HC"], "comment": null, "summary": "As AI becomes more capable, it is unclear how human creativity will remain\nessential in jobs that incorporate AI. We conducted a 14-week study of a\nstudent newsroom using an AI tool to convert web articles into social media\nvideos. Most creators treated the tool as a creative springboard, not as a\ncompletion mechanism. They edited the AI outputs. The tool enabled the team to\npublish successful content that received over 500,000 views. Human creativity\nremained essential: after AI produced templated outputs, creators took\nownership of the task, injecting their own creativity, especially when AI\nfailed to create appropriate content. AI was initially seen as an authority,\ndue to creators' lack of experience, but they ultimately learned to assert\ntheir own authority.", "AI": {"tldr": "A study on how human creativity persists in jobs using AI, showing that creators viewed AI as a tool for inspiration rather than a replacement.", "motivation": "To explore the role of human creativity in jobs augmented by AI tools in a newsroom setting.", "method": "A 14-week study involving a student newsroom using an AI tool to convert web articles into social media videos, observing creator interactions with the tool.", "result": "The AI tool allowed creators to produce content that achieved over 500,000 views, highlighting the importance of human creativity in refining AI outputs.", "conclusion": "Human creativity remains crucial even when AI tools are utilized, as creators learn to combine AI outputs with their own creative insights.", "key_contributions": ["Demonstrated the role of AI as a creative springboard in content creation.", "Showed how human authority evolves when working with AI tools.", "Highlighted the challenges and successes of using AI in a collaborative environment."], "limitations": "The study is context-specific to a student newsroom and may not generalize to all industries relying on AI tools.", "keywords": ["AI in creativity", "human-computer interaction", "content creation", "social media", "student newsroom"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.13905", "pdf": "https://arxiv.org/pdf/2509.13905.pdf", "abs": "https://arxiv.org/abs/2509.13905", "title": "Do Large Language Models Understand Word Senses?", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "This paper evaluates the Word Sense Disambiguation (WSD) capabilities of instruction-tuned Large Language Models (LLMs) and their ability to understand word meanings in various generative contexts.", "motivation": "The paper addresses the underexplored area of whether LLMs truly comprehend word senses in context, an important aspect of natural language understanding.", "method": "The authors evaluate the WSD capabilities of instruction-tuned LLMs and compare them against specialized WSD systems. They assess two top-performing LLMs in three generative tasks: definition generation, free-form explanation, and example generation.", "result": "Top-performing LLMs such as GPT-4o and DeepSeek-V3 demonstrate WSD performance comparable to specialized systems, with 98% accuracy in explaining word meanings in context, particularly excelling in free-form explanations.", "conclusion": "The findings suggest that instruction-tuned LLMs not only perform well in WSD tasks but also exhibit strong generative capabilities in providing contextual word meanings.", "key_contributions": ["Evaluated LLMs against state-of-the-art WSD systems.", "Demonstrated LLMs' performance in generative tasks related to word meaning understanding.", "Highlighted the robustness of LLMs across domains and difficulty levels."], "limitations": "", "keywords": ["Word Sense Disambiguation", "Large Language Models", "natural language understanding"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.06696", "pdf": "https://arxiv.org/pdf/2502.06696.pdf", "abs": "https://arxiv.org/abs/2502.06696", "title": "Social Media Should Feel Like Minecraft, Not Instagram: 3D Gamer Youth Visions for Meaningful Social Connections through Fictional Inquiry", "authors": ["JaeWon Kim", "Hyunsung Cho", "Fannie Liu", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate youth visions for ideal remote social interactions, drawing on\nco-design interviews with 23 participants (aged 15-24) experienced with 3D\ngaming environments. Using a Fictional Inquiry (FI) method set in the Harry\nPotter universe, this research reveals that young people desire social media\nthat functions more like immersive, navigable shared social spaces. Across\nthese interviews, participants identified six key priorities for meaningful\nsocial connection over social media: intuitive social navigation, shared\ncollaborative experiences, communal environments fostering close relationships,\nflexible self-presentation, intentional engagement, and playful social\nmechanics. We introduce the \\textit{spatial integrity} framework, a set of four\ninterrelated design principles: spatial presence, spatial composition, spatial\nconfiguration, and spatial depth. Together, these principles outline how online\nspaces can be designed to feel more like meaningful environments, spaces where\nrelationships can grow through shared presence, movement, and intentional\ninteraction. Participants also described the FI process itself as meaningful,\nnot only for generating new ideas but for empowering them to imagine and shape\nthe future of social media.", "AI": {"tldr": "This research explores youths' visions for ideal remote social interactions through co-design interviews in a 3D gaming context, highlighting their priorities for social media design.", "motivation": "To understand young people's expectations for remote social interactions and their vision of ideal social media experiences.", "method": "Co-design interviews with 23 participants aged 15-24 using a Fictional Inquiry method set in the Harry Potter universe.", "result": "Participants identified six priorities for social media design, and a spatial integrity framework was introduced to guide the creation of more immersive and meaningful online spaces.", "conclusion": "The research emphasizes the importance of intentional design in social media to facilitate real connections and empower users to shape their experiences.", "key_contributions": ["Identification of six key priorities for social media design based on user input.", "Introduction of the spatial integrity framework for designing immersive online environments.", "Highlighting the meaningfulness of the co-design process for participants."], "limitations": "", "keywords": ["social interactions", "co-design", "spatial integrity", "social media", "youth perspectives"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.13930", "pdf": "https://arxiv.org/pdf/2509.13930.pdf", "abs": "https://arxiv.org/abs/2509.13930", "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "categories": ["cs.CL"], "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "This paper investigates how different document languages impact citation and generation in multilingual retrieval-augmented generation (mRAG) systems, revealing biases in citation choices.", "motivation": "To explore the impact of different document languages on the citation behavior of multilingual retrieval-augmented generation (mRAG) systems.", "method": "A controlled methodology utilizing model internals to measure language preference while keeping document relevance constant across eight languages and six open-weight models was employed.", "result": "Models tend to cite English sources preferentially when queries are in English, a bias that is amplified for lower-resource languages and affects citation choices, indicating a trade-off between relevance and language preference.", "conclusion": "Understanding how language models utilize multilingual context can inform the design of more equitable mRAG systems by addressing citation biases.", "key_contributions": ["Introduces a methodology to study language preference in mRAG systems.", "Demonstrates bias towards English citation in multilingual contexts.", "Highlights trade-offs between document relevance and language preference."], "limitations": "Limited to the investigation of eight languages and six open-weight models; may not generalize across all multilingual settings.", "keywords": ["multilingual", "retrieval-augmented generation", "citation bias", "language models", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13980", "pdf": "https://arxiv.org/pdf/2509.13980.pdf", "abs": "https://arxiv.org/abs/2509.13980", "title": "Long-context Reference-based MT Quality Estimation", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "This paper presents a submission to WMT25 focusing on automated translation quality evaluation using the COMET framework and segment-level Error Span Annotation scores with long-context data.", "motivation": "The motivation is to enhance the accuracy of automated translation quality evaluation by leveraging long-context data to better reflect human judgments.", "method": "The authors concatenated in-domain human-annotated sentences and computed weighted averages of their Error Span Annotation scores. They integrated multiple human judgment datasets and trained multilingual regression models to predict quality scores from various translations.", "result": "The experimental results indicate that models using long-context information outperform those based solely on short segments in terms of correlation with human judgment.", "conclusion": "Incorporating long-context data significantly enhances automated translation quality evaluation by aligning more closely with human assessments.", "key_contributions": ["Utilization of long-context data for quality score predictions.", "Integration of multiple human judgment datasets for improved training.", "Demonstration of improved correlations between automated scores and human judgments."], "limitations": "", "keywords": ["Machine Translation", "Automated Evaluation", "COMET Framework", "Long-context Data", "Quality Assessment"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.13990", "pdf": "https://arxiv.org/pdf/2509.13990.pdf", "abs": "https://arxiv.org/abs/2509.13990", "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "This paper presents Slim-SC, an efficient variant of the Self-Consistency technique for Test-Time Scaling in LLMs, which reduces computational overhead while maintaining accuracy.", "motivation": "The motivation behind this research is to address the high computational overhead of Self-Consistency in LLMs during test time, which limits its wider applicability.", "method": "The paper proposes Slim-SC, a pruning strategy that identifies and eliminates redundant reasoning chains by analyzing inter-chain similarity at the thought level.", "result": "Experiments demonstrate that Slim-SC decreases inference latency by up to 45% and KVC usage by 26% on three STEM reasoning datasets, while also maintaining or enhancing accuracy.", "conclusion": "Slim-SC provides an efficient alternative to Self-Consistency in LLM reasoning, making Test-Time Scaling more accessible.", "key_contributions": ["Introduction of Slim-SC as an efficient alternative to Self-Consistency.", "Theoretical and empirical analysis of inefficiencies in Self-Consistency.", "Demonstration of significant reductions in computational resources with maintained accuracy."], "limitations": "", "keywords": ["Test-Time Scaling", "Self-Consistency", "LLMs", "Inference Latency", "Pruning Strategy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.14004", "pdf": "https://arxiv.org/pdf/2509.14004.pdf", "abs": "https://arxiv.org/abs/2509.14004", "title": "Early Stopping Chain-of-thoughts in Large Language Models", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "This study introduces ES-CoT, a method to shorten chain-of-thought generation in large language models by detecting answer convergence, achieving a 41% reduction in inference tokens while maintaining accuracy.", "motivation": "To improve the efficiency of large language models (LLMs) in generating chain-of-thought (CoT) responses while minimizing inference costs.", "method": "The proposed ES-CoT method tracks the run length of consecutive identical step answers to determine answer convergence and stops generation early when convergence is detected.", "result": "ES-CoT reduces the number of inference tokens by approximately 41% on average across five reasoning datasets from three different LLMs, while maintaining accuracy comparable to standard CoT methods.", "conclusion": "ES-CoT is a practical and effective approach that enables efficient reasoning in LLMs without sacrificing performance, integrating well with existing prompting techniques.", "key_contributions": ["Introduction of ES-CoT for efficient CoT generation.", "Demonstration of significant reduction in inference costs with maintained accuracy.", "Empirical validation across multiple reasoning datasets."], "limitations": "", "keywords": ["large language models", "chain-of-thought", "inference cost", "answer convergence", "ES-CoT"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.14008", "pdf": "https://arxiv.org/pdf/2509.14008.pdf", "abs": "https://arxiv.org/abs/2509.14008", "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "This paper presents Hala, a family of Arabic-centric instruction and translation models that achieve state-of-the-art results in Arabic NLP tasks by leveraging a novel translate-and-tune pipeline.", "motivation": "The motivation behind Hala is to enhance Arabic instruction following and translation capabilities while maintaining high throughput without quality loss.", "method": "The approach includes compressing a strong AR↔EN teacher model to FP8 and using it to create bilingual supervision, followed by fine-tuning a lightweight language model LFM2-1.2B on this data to translate English instructions into Arabic.", "result": "Hala models have been trained at various sizes (350M, 700M, 1.2B, and 9B parameters) and have achieved state-of-the-art results on Arabic-centric benchmarks, outperforming their base models in both 'nano' and 'small' categories.", "conclusion": "The paper concludes that the Hala models provide significant advancements in Arabic NLP and encourages further research with the released models, data, and evaluation strategies.", "key_contributions": ["Introduction of the translate-and-tune pipeline for Arabic-centric models.", "State-of-the-art performance on Arabic NLP tasks.", "Release of models and data to foster Arabic NLP research."], "limitations": "", "keywords": ["Arabic NLP", "translation models", "instruction following"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.09252", "pdf": "https://arxiv.org/pdf/2410.09252.pdf", "abs": "https://arxiv.org/abs/2410.09252", "title": "DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue", "authors": ["Minh Pham Dinh", "Munira Syed", "Michael G Yankoski", "Trenton W. Ford"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Designing a generalist scientific agent capable of performing tasks in\nlaboratory settings to assist researchers has become a key goal in recent\nArtificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks\nare inherently more delicate and complex, requiring agents to possess a higher\nlevel of reasoning ability, structured and temporal understanding of their\nenvironment, and a strong emphasis on safety. Existing approaches often fail to\naddress these multifaceted requirements. To tackle these challenges, we present\nDAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches,\nDAVIS incorporates structured and temporal memory, which enables model-based\nplanning. Additionally, DAVIS implements an agentic, multi-turn retrieval\nsystem, similar to a human's inner monologue, allowing for a greater degree of\nreasoning over past experiences. DAVIS demonstrates substantially improved\nperformance on the ScienceWorld benchmark comparing to previous approaches on 8\nout of 9 elementary science subjects. In addition, DAVIS's World Model\ndemonstrates competitive performance on the famous HotpotQA and MusiqueQA\ndataset for multi-hop question answering. To the best of our knowledge, DAVIS\nis the first RAG agent to employ an interactive retrieval method in a RAG\npipeline.", "AI": {"tldr": "A novel scientific agent, DAVIS, improves AI task performance in laboratory settings by integrating structured and temporal memory and agentic multi-turn retrieval.", "motivation": "To create a generalist scientific agent that can assist researchers in laboratory settings by addressing the complexities of scientific tasks with advanced reasoning and safety.", "method": "DAVIS employs structured and temporal memory for model-based planning, alongside an agentic multi-turn retrieval system that mimics human reasoning, enhancing its ability to perform in complex environments.", "result": "DAVIS outperforms previous models on the ScienceWorld benchmark across 8 of 9 science subjects and also shows competitive performance on multi-hop question answering tasks in HotpotQA and MusiqueQA datasets.", "conclusion": "DAVIS represents a significant advancement in RAG agents, being the first to use an interactive retrieval method within a RAG framework, enhancing scientific task performance in AI.", "key_contributions": ["Introduction of structured and temporal memory in a RAG agent", "First implementation of an agentic multi-turn retrieval system for AI tasks", "Demonstrated improved performance on multiple benchmarks relevant to science tasks."], "limitations": "", "keywords": ["Artificial Intelligence", "Science", "Multi-turn Retrieval", "Model-based Planning", "Human-like Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.14023", "pdf": "https://arxiv.org/pdf/2509.14023.pdf", "abs": "https://arxiv.org/abs/2509.14023", "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "This study evaluates Machine Translation (MT) systems using both text-only and audio-based assessments, finding that audio evaluations can reveal significant differences in translation quality.", "motivation": "Despite advancements in Machine Translation, quality assessments are still mainly text-centric, which does not reflect real-world applications where translations are often spoken.", "method": "The study compares evaluations of 10 MT systems using crowd-sourced judgments from Amazon Mechanical Turk, performing statistical tests to assess the reliability of audio-based evaluations versus text-only evaluations.", "result": "Audio-based assessments yield rankings consistent with text-only evaluations, but also highlight significant differences in some cases due to the richer modality of speech.", "conclusion": "Incorporating speech-based assessments into MT evaluation frameworks is proposed to enhance the quality and relevance of evaluations.", "key_contributions": ["Proposes audio-based evaluations as a natural alternative to traditional text-centric assessments.", "Demonstrates that audio evaluations can reveal significant differences between MT systems not apparent in text-only assessments.", "Provides statistical evidence for the reliability and consistency of crowd-sourced audio evaluations."], "limitations": "Limited to the evaluation of 10 MT systems and reliance on crowd-sourced judgments which may introduce variability.", "keywords": ["Machine Translation", "Speech Translation", "Multimodal Assessment"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2412.12478", "pdf": "https://arxiv.org/pdf/2412.12478.pdf", "abs": "https://arxiv.org/abs/2412.12478", "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script", "authors": ["Xi Cao", "Yuan Sun", "Jiajun Li", "Quzong Gesang", "Nuo Qun", "Tashi Nyima"], "categories": ["cs.CL", "cs.CR", "cs.HC"], "comment": null, "summary": "DNN-based language models excel across various NLP tasks but remain highly\nvulnerable to textual adversarial attacks. While adversarial text generation is\ncrucial for NLP security, explainability, evaluation, and data augmentation,\nrelated work remains overwhelmingly English-centric, leaving the problem of\nconstructing high-quality and sustainable adversarial robustness benchmarks for\nlower-resourced languages both difficult and understudied. First, method\ncustomization for lower-resourced languages is complicated due to linguistic\ndifferences and limited resources. Second, automated attacks are prone to\ngenerating invalid or ambiguous adversarial texts. Last but not least, language\nmodels continuously evolve and may be immune to parts of previously generated\nadversarial texts. To address these challenges, we introduce HITL-GAT, an\ninteractive system based on a general approach to human-in-the-loop generation\nof adversarial texts. Additionally, we demonstrate the utility of HITL-GAT\nthrough a case study on Tibetan script, employing three customized adversarial\ntext generation methods and establishing its first adversarial robustness\nbenchmark, providing a valuable reference for other lower-resourced languages.", "AI": {"tldr": "This paper presents HITL-GAT, an interactive system for generating adversarial texts aimed at addressing the challenges in crafting robust benchmarks for lower-resourced languages like Tibetan.", "motivation": "To tackle the lack of high-quality adversarial robustness benchmarks for lower-resourced languages, which are often overlooked in adversarial text generation studies.", "method": "The study introduces HITL-GAT, an interactive system that utilizes a human-in-the-loop approach for generating adversarial texts tailored to lower-resourced languages, demonstrated through a case study on Tibetan script.", "result": "The development and demonstration of HITL-GAT led to the first validated benchmark for adversarial robustness in Tibetan, showcasing effective methods for generating adversarial texts.", "conclusion": "HITL-GAT proves to be a valuable tool in advancing adversarial text generation for lower-resourced languages and sets a precedent for future research in this area.", "key_contributions": ["Development of HITL-GAT for human-in-the-loop adversarial text generation", "First adversarial robustness benchmark for Tibetan script", "Customized adversarial text generation methods for lower-resourced languages"], "limitations": "The research focuses on Tibetan and may not generalize over all lower-resourced languages; the complexity of linguistic differences remains a challenge.", "keywords": ["adversarial text generation", "lower-resourced languages", "HITL-GAT"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.14031", "pdf": "https://arxiv.org/pdf/2509.14031.pdf", "abs": "https://arxiv.org/abs/2509.14031", "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "authors": ["Paweł Mąka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "This paper investigates the impact of contextually rich training data on machine translation performance, finding that increased context utilization significantly enhances accuracy.", "motivation": "To address the challenges in achieving human-level translations, especially in handling contextual phenomena such as pronoun disambiguation, and to explore the impact of training data sparsity.", "method": "The authors constructed training datasets with controlled proportions of contextually relevant examples to systematically validate the association between data sparsity and model performance across single- and multilingual settings.", "result": "The results confirm a strong correlation between training data sparsity and model performance, with accuracy improvements of up to 6 and 8 percentage points observed in single- and multilingual evaluations, respectively.", "conclusion": "The study highlights the importance of contextually rich training data in translation models and proposes effective strategies to enhance context utilization, although improvements in one contextual area do not generalize to others.", "key_contributions": ["Validated the impact of contextually rich data on translation performance.", "Revealed limited cross-lingual transfer within language sub-families.", "Proposed and evaluated training strategies that enhance context utilization."], "limitations": "Improvements in context utilization are specific to certain phenomena and do not generalize across all areas of translation.", "keywords": ["machine translation", "context utilization", "data sparsity", "multilingual settings", "training strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.14034", "pdf": "https://arxiv.org/pdf/2509.14034.pdf", "abs": "https://arxiv.org/abs/2509.14034", "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "authors": ["Zijie Lin", "Bryan Hooi"], "categories": ["cs.CL"], "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "The paper introduces ConfMAD, a Multi-Agent Debate framework that incorporates confidence expression in Generative Large Language Models to improve debate effectiveness and system performance.", "motivation": "To address the challenges of inappropriate confidence expression in Multi-Agent Debate systems, which leads to suboptimal reasoning and belief maintenance by LLMs during debates.", "method": "The authors develop the ConfMAD framework, integrating mechanisms for LLMs to express their confidence levels throughout the debate process.", "result": "Experimental results indicate that incorporating confidence expression improves the effectiveness of debates and influences debate dynamics positively.", "conclusion": "The study highlights the importance of confidence expression in MAD systems, suggesting that it can lead to better task performance by enabling clearer communication of LLMs' knowledge and reasoning strengths.", "key_contributions": ["Introduction of the ConfMAD framework for confidence expression in MAD systems.", "Demonstration of improved debate effectiveness through experimental validation.", "Insights into how confidence levels affect debate dynamics and outcomes."], "limitations": "", "keywords": ["Generative Large Language Models", "Multi-Agent Debate", "Confidence Expression"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.14036", "pdf": "https://arxiv.org/pdf/2509.14036.pdf", "abs": "https://arxiv.org/abs/2509.14036", "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "This paper introduces Question-based Sign Language Translation (QB-SLT), which enhances sign language translation by integrating dialogue context through a novel Self-supervised Learning method.", "motivation": "To improve sign language translation by incorporating dialogue context for better performance than traditional gloss annotations.", "method": "The authors propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) method that utilizes contrastive learning to align multimodality features for translation.", "result": "The proposed approach achieved state-of-the-art performance on the CSL-Daily-QA and PHOENIX-2014T-QA datasets, effectively demonstrating that dialogue assistance can improve translation quality. Visualization results support the use of dialogue for enhancement.", "conclusion": "Incorporating dialogue context in Sign Language Translation significantly improves translation quality, surpassing traditional gloss methods.", "key_contributions": ["Introduction of Question-based Sign Language Translation (QB-SLT)", "Development of SSL-SSAW for effective dialogue integration", "Demonstration of superior performance with easily accessible question assistance over gloss assistance"], "limitations": "", "keywords": ["Sign Language Translation", "Question-based Translation", "Self-supervised Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.14128", "pdf": "https://arxiv.org/pdf/2509.14128.pdf", "abs": "https://arxiv.org/abs/2509.14128", "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS"], "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "Canary-1B-v2 is a fast multilingual ASR and AST model, evaluated against existing models with promising results.", "motivation": "The need for efficient and accurate multilingual ASR and AST systems to handle diverse language inputs, particularly for applications involving European languages.", "method": "Canary-1B-v2 employs a FastConformer encoder with a Transformer decoder, trained on 1.7M hours of data, implementing a two-stage pre-training and fine-tuning process with dynamic data balancing.", "result": "Canary-1B-v2 outperforms Whisper-large-v3 in English ASR, performing 10x faster, and remains competitive with larger models like Seamless-M4T-v2-large for multilingual performance.", "conclusion": "The introduction of both Canary-1B-v2 and its successor, Parakeet-TDT-0.6B-v3, represents significant advancements in multilingual ASR and AST capabilities.", "key_contributions": ["Introduction of a robust multilingual ASR and AST model", "Demonstration of FastConformer and nGPT with competitive speed and performance", "Release of Parakeet-TDT-0.6B-v3 providing efficient ASR across 25 languages"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Speech-to-Text Translation", "FastConformer", "multilingual", "nGPT"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.14161", "pdf": "https://arxiv.org/pdf/2509.14161.pdf", "abs": "https://arxiv.org/abs/2509.14161", "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS is a new dataset to advance code-switched speech recognition and translation for low-resourced languages.", "motivation": "The dataset aims to support research in code-switched speech systems, especially for languages with fewer resources.", "method": "CS-FLEURS includes four test sets covering 113 unique language pairs and a training set with 128 hours of data across various setups.", "result": "The dataset consists of diverse pairs, including real voices, generative text-to-speech, and lower-resourced language pairs.", "conclusion": "CS-FLEURS aims to broaden the research scope for code-switched speech, offering materials for various language pairs and resources.", "key_contributions": ["Introduction of a dataset covering 113 code-switched language pairs across 52 languages.", "Inclusion of various speech generation methods including generative text-to-speech.", "Provision of both test and training sets focused on low-resource languages."], "limitations": "", "keywords": ["code-switching", "speech recognition", "low-resourced languages", "dataset", "speech translation"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2509.14171", "pdf": "https://arxiv.org/pdf/2509.14171.pdf", "abs": "https://arxiv.org/abs/2509.14171", "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "The paper presents AssoCiAm, a novel benchmark to evaluate the associative ability of multimodal large language models (MLLMs), addressing ambiguity issues and demonstrating a correlation between cognition and association.", "motivation": "To improve the evaluation of the associative ability of MLLMs, as existing frameworks overlook ambiguity inherent in association tasks, leading to unreliable assessments.", "method": "The authors decompose ambiguity into internal and external types and propose the AssoCiAm benchmark, employing a hybrid computational method to evaluate MLLMs while minimizing ambiguity.", "result": "Experiments show a strong positive correlation between cognition and association in MLLMs. Ambiguity in evaluations correlates with increased randomness in MLLM behavior.", "conclusion": "The AssoCiAm benchmark effectively enhances the reliability of associative ability evaluations in MLLMs, helping to clarify the relationship between cognition and association.", "key_contributions": ["Introduction of AssoCiAm benchmark for evaluating associative ability in MLLMs", "Decomposition of ambiguity into internal and external types", "Demonstration of correlation between cognition and association in MLLMs"], "limitations": "The study may not comprehensively address all forms of ambiguity in association tasks beyond the explored types.", "keywords": ["multimodal large language models", "associative ability", "ambiguity", "artificial general intelligence", "benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.14180", "pdf": "https://arxiv.org/pdf/2509.14180.pdf", "abs": "https://arxiv.org/abs/2509.14180", "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "authors": ["Akhil Theerthala"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "The paper introduces a novel framework for personalized financial advice, integrating behavioral finance with a dataset for fine-tuning a smaller LLM model, yielding competitive performance at lower costs.", "motivation": "To enhance personalized financial advice by integrating user goals, constraints, and behavioral finance to reduce the maintenance costs and improve financial returns.", "method": "The authors developed a reproducible framework that combines financial context and behavioral insights to construct supervision data and fine-tune the Qwen-3-8B model on a 19k sample reasoning dataset.", "result": "The fine-tuned 8B model demonstrates performance comparable to larger models (14-32B parameters) across various metrics while incurring significantly lower costs (80% savings).", "conclusion": "Carefully curated data and integration of behavioral finance enable smaller models to perform competitively, making personalized financial advice more accessible and cost-effective.", "key_contributions": ["Introduced a novel framework for constructing supervision data for financial advisors.", "Created a large reasoning dataset for training scenarios in personal finance.", "Achieved competitive performance of a smaller model compared to larger models at reduced costs."], "limitations": "The paper does not address the generalizability of the model beyond the specific financial contexts studied.", "keywords": ["personal finance", "LLM", "behavioral finance", "dataset", "financial advice"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.14197", "pdf": "https://arxiv.org/pdf/2509.14197.pdf", "abs": "https://arxiv.org/abs/2509.14197", "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "A computational analysis of migration discourse in UK and US legislative debates using LLMs, revealing changes in attitudes over time.", "motivation": "To examine the evolution of migration-related discourse in UK parliamentary debates compared to US congressional discourse over 75 years.", "method": "Large-scale computational analysis using open-weight LLMs to annotate statements and track stances and tones, alongside a semi-automated framework for extracting narrative frames.", "result": "The study found increasing polarization in US discourse while UK attitudes remained aligned across parties, with a notable shift towards securitized narratives and a decline in integration-focused discussions.", "conclusion": "LLMs can effectively facilitate scalable and detailed discourse analysis, providing insights into political and historical contexts.", "key_contributions": ["Development of a framework for fine-grained narrative frame extraction", "Insights into the ideological gap between UK political parties", "Demonstrated the utility of LLMs in discourse trends analysis over time"], "limitations": "", "keywords": ["Migration Discourse", "LLMs", "Political Analysis", "Narrative Frames", "UK Parliament"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2509.14233", "pdf": "https://arxiv.org/pdf/2509.14233.pdf", "abs": "https://arxiv.org/abs/2509.14233", "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "authors": ["Alejandro Hernández-Cano", "Alexander Hägele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank Ďurech", "Ido Hakimi", "Juan García Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabolčec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian Bösch", "Maximilian Böther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "María Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike Lübeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendoncça", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "Léo Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tramèr", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus is an open suite of large language models aimed at improving data compliance and multilingual representation by using openly available data and enhancing support for non-English languages.", "motivation": "To address systemic shortcomings in the open model ecosystem regarding data compliance and multilingual representation.", "method": "Pretrained on openly available data while respecting content-owner rights and using the Goldfish objective to suppress verbatim data recall.", "result": "Apertus achieves state-of-the-art results among fully open models on multilingual benchmarks, with a strong focus on non-English content.", "conclusion": "Apertus provides a comprehensive suite of scientific artifacts for transparency, enabling further research and development in the open model space.", "key_contributions": ["Fully open models with respect for data compliance", "Enhanced multilingual representation with a significant portion of non-English data", "Release of all development artifacts for transparency and extension"], "limitations": "", "keywords": ["large language models", "multilingual representation", "data compliance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.14132", "pdf": "https://arxiv.org/pdf/2509.14132.pdf", "abs": "https://arxiv.org/abs/2509.14132", "title": "When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training", "authors": ["Julia S. Dollis", "Iago A. Brito", "Fernanda B. Färber", "Pedro S. F. B. Ribeiro", "Rafael T. Sousa", "Arlindo R. Galvão Filho"], "categories": ["cs.HC", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.", "AI": {"tldr": "A framework integrating LLMs into VR for training interpersonal skills, particularly in medical education, by creating coherent virtual patients with distinct personalities.", "motivation": "The need for psychologically plausible virtual humans in VR for training complex interpersonal skills, especially in high-stakes fields like medical education, where communication is essential.", "method": "The study involved a mixed-method, within-subjects evaluation with licensed physicians participating in simulated consultations using the developed system.", "result": "The approach is feasible and perceived as a rewarding training enhancement by physicians.", "conclusion": "The framework provides valuable insights and design principles for creating socially intelligent VR training environments.", "key_contributions": ["Introduction of a modular architecture for VR virtual patients", "Validation of feasibility and perceived effectiveness among physicians", "Identification of key design principles like the 'realism-verbosity paradox'"], "limitations": "", "keywords": ["virtual reality", "large language models", "medical education", "interpersonal skills", "socially intelligent systems"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2308.07107", "pdf": "https://arxiv.org/pdf/2308.07107.pdf", "abs": "https://arxiv.org/abs/2308.07107", "title": "Large Language Models for Information Retrieval: A Survey", "authors": ["Yutao Zhu", "Huaying Yuan", "Shuting Wang", "Jiongnan Liu", "Wenhan Liu", "Chenlong Deng", "Haonan Chen", "Zheng Liu", "Zhicheng Dou", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.IR"], "comment": "Updated to version 4; Accepted by ACM TOIS", "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.", "AI": {"tldr": "This survey examines the integration of large language models (LLMs) with information retrieval (IR) systems, highlighting advancements and challenges.", "motivation": "To consolidate existing methodologies and provide insights into how LLMs can enhance IR systems amidst their evolving landscape.", "method": "The survey reviews the confluence of traditional IR methods with modern neural architectures, focusing on LLM applications in query rewriting, retrieval, ranking, and reading tasks.", "result": "Identifies challenges such as data scarcity and interpretability while showcasing advancements in LLM-driven IR methods, along with potential future directions like search agents.", "conclusion": "An emphasis on integrating both traditional and modern approaches is crucial for overcoming existing challenges in IR systems.", "key_contributions": ["Comprehensive overview of LLMs in IR systems", "Identifies key challenges and suggests future directions", "Highlights the integration of traditional and neural methods"], "limitations": "The survey may not cover the very latest advancements post-publication and is limited to existing literature.", "keywords": ["Information Retrieval", "Large Language Models", "Neural Models", "Natural Language Processing", "Search Systems"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2405.13541", "pdf": "https://arxiv.org/pdf/2405.13541.pdf", "abs": "https://arxiv.org/abs/2405.13541", "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts", "authors": ["Yuu Jinnai", "Ukyo Honda"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP Findings, 2025", "summary": "Preference optimization is a standard approach to fine-tuning large language\nmodels to align with human preferences. The quantity, diversity, and\nrepresentativeness of the preference dataset are critical to the effectiveness\nof preference optimization. However, obtaining a large amount of preference\nannotations is difficult in many applications. This raises the question of how\nto use the limited annotation budget to create an effective preference dataset.\nTo this end, we propose Annotation-Efficient Preference Optimization (AEPO).\nInstead of exhaustively annotating preference over all available response\ntexts, AEPO selects a subset of responses that maximizes diversity and\nrepresentativeness from the available responses and then annotates preference\nover the selected ones. In this way, AEPO focuses the annotation budget on\nlabeling preferences over a smaller but informative subset of responses. We\nevaluate the performance of preference learning using AEPO on three datasets\nand show that it outperforms the baselines with the same annotation budget. Our\ncode is available at https://github.com/CyberAgentAILab/annotation-efficient-po", "AI": {"tldr": "Proposes Annotation-Efficient Preference Optimization (AEPO) to enhance preference dataset effectiveness despite limited annotations for fine-tuning large language models.", "motivation": "Addresses challenges in obtaining large preference datasets necessary for effective fine-tuning of language models according to human preferences.", "method": "AEPO strategically selects a small, diverse, and representative subset of responses from a larger pool for preference annotation, optimizing the use of limited annotation resources.", "result": "AEPO consistently outperforms baseline methods on three datasets while using the same amount of annotation budget, demonstrating its effectiveness in preference learning.", "conclusion": "The proposed method provides a means to efficiently compile preference data, improving the alignment of language models with human user preferences under budget constraints.", "key_contributions": ["Introduces AEPO for efficient preference dataset creation.", "Demonstrates AEPO's superiority in preference learning with limited annotations.", "Provides an open-source implementation for reproducibility."], "limitations": "", "keywords": ["preference optimization", "annotation efficiency", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.16013", "pdf": "https://arxiv.org/pdf/2406.16013.pdf", "abs": "https://arxiv.org/abs/2406.16013", "title": "Database-Augmented Query Representation for Information Retrieval", "authors": ["Soyeong Jeong", "Jinheon Baek", "Sukmin Cho", "Sung Ju Hwang", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "EMNLP 2025", "summary": "Information retrieval models that aim to search for documents relevant to a\nquery have shown multiple successes, which have been applied to diverse tasks.\nYet, the query from the user is oftentimes short, which challenges the\nretrievers to correctly fetch relevant documents. To tackle this, previous\nstudies have proposed expanding the query with a couple of additional\n(user-related) features related to it. However, they may be suboptimal to\neffectively augment the query, and there is plenty of other information\navailable to augment it in a relational database. Motivated by this fact, we\npresent a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with the graph-based set-encoding strategy, which considers\nhierarchies of features in the database without order. We validate our DAQu in\ndiverse retrieval scenarios, demonstrating that it significantly enhances\noverall retrieval performance over relevant baselines. Our code is available at\n\\href{https://github.com/starsuzi/DAQu}{this https URL}.", "AI": {"tldr": "This paper presents DAQu, a novel retrieval framework that enhances information retrieval by augmenting user queries with relevant metadata from relational databases.", "motivation": "To improve document retrieval accuracy for short user queries by utilizing additional metadata from relational databases, addressing limitations in existing query expansion methods.", "method": "DAQu augments the original query with metadata across multiple tables in a relational database and utilizes a graph-based set-encoding strategy to encode this metadata without order.", "result": "DAQu significantly enhances retrieval performance across diverse scenarios compared to relevant baselines.", "conclusion": "The proposed framework demonstrates that leveraging relational database metadata can substantially improve document retrieval accuracy, with code availability for further research.", "key_contributions": ["Introduction of the Database-Augmented Query framework (DAQu)", "Application of graph-based set-encoding for metadata features", "Validation of DAQu in various retrieval scenarios showing increased performance."], "limitations": "", "keywords": ["Information Retrieval", "Query Expansion", "Relational Databases", "Metadata", "Graph-based Encoding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2409.12147", "pdf": "https://arxiv.org/pdf/2409.12147.pdf", "abs": "https://arxiv.org/abs/2409.12147", "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning", "authors": ["Justin Chih-Yao Chen", "Archiki Prasad", "Swarnadeep Saha", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Camera-Ready)", "summary": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication.", "AI": {"tldr": "MAgICoRe enhances Large Language Model reasoning through targeted multi-agent refinement, improving performance on math datasets while addressing common pitfalls of excessive, insufficient, and error-prone refinements.", "motivation": "To improve the reasoning of Large Language Models (LLMs) using test-time aggregation strategies and refine solutions with mechanisms that address common challenges in LLM performance.", "method": "MAgICoRe categorizes problem difficulty and utilizes a multi-agent loop involving a Solver, Reviewer, and Refiner to optimize feedback and iterative refinement based on external reward model scores.", "result": "MAgICoRe outperforms existing methods, showing a 3.4% improvement over Self-Consistency and greater efficacy in multi-agent learning, continuing to improve with more iterations.", "conclusion": "MAgICoRe effectively enhances LLM reasoning through a structured approach to problem difficulty and iterative refinement, outperforming traditional methods across multiple math datasets.", "key_contributions": ["Introduction of MAgICoRe for effective LLM refinement", "Use of multi-agent systems for enhanced error localization", "Proven effectiveness over existing aggregation strategies in LLM performance"], "limitations": "", "keywords": ["Large Language Models", "refinement", "multi-agent systems", "error localization", "machine learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.09252", "pdf": "https://arxiv.org/pdf/2410.09252.pdf", "abs": "https://arxiv.org/abs/2410.09252", "title": "DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue", "authors": ["Minh Pham Dinh", "Munira Syed", "Michael G Yankoski", "Trenton W. Ford"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Designing a generalist scientific agent capable of performing tasks in\nlaboratory settings to assist researchers has become a key goal in recent\nArtificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks\nare inherently more delicate and complex, requiring agents to possess a higher\nlevel of reasoning ability, structured and temporal understanding of their\nenvironment, and a strong emphasis on safety. Existing approaches often fail to\naddress these multifaceted requirements. To tackle these challenges, we present\nDAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches,\nDAVIS incorporates structured and temporal memory, which enables model-based\nplanning. Additionally, DAVIS implements an agentic, multi-turn retrieval\nsystem, similar to a human's inner monologue, allowing for a greater degree of\nreasoning over past experiences. DAVIS demonstrates substantially improved\nperformance on the ScienceWorld benchmark comparing to previous approaches on 8\nout of 9 elementary science subjects. In addition, DAVIS's World Model\ndemonstrates competitive performance on the famous HotpotQA and MusiqueQA\ndataset for multi-hop question answering. To the best of our knowledge, DAVIS\nis the first RAG agent to employ an interactive retrieval method in a RAG\npipeline.", "AI": {"tldr": "The paper presents DAVIS, a scientific AI agent that integrates structured and temporal memory for enhanced reasoning and task performance in laboratory settings, achieving significant improvements over existing methods.", "motivation": "The goal is to design a generalist scientific agent capable of assisting researchers in complex laboratory tasks, which require advanced reasoning and safety considerations.", "method": "DAVIS combines structured and temporal memory with model-based planning, and incorporates a multi-turn retrieval system to facilitate reasoning akin to human inner monologue.", "result": "DAVIS shows substantial performance improvements on the ScienceWorld benchmark in 8 out of 9 science subjects and competitive results on HotpotQA and MusiqueQA datasets.", "conclusion": "DAVIS represents a novel approach to retrieval-augmented generation by utilizing interactive retrieval within a RAG pipeline, enhancing the agent’s reasoning capabilities.", "key_contributions": ["Introduction of structured and temporal memory in RAG systems", "Implementation of a multi-turn retrieval system for enhanced reasoning", "First RAG agent to use interactive retrieval in its pipeline"], "limitations": "", "keywords": ["AI", "Human-Computer Interaction", "RAG", "Machine Learning", "Scientific Agents"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.10857", "pdf": "https://arxiv.org/pdf/2410.10857.pdf", "abs": "https://arxiv.org/abs/2410.10857", "title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "authors": ["Siyuan Huang", "Zhiyuan Ma", "Jintao Du", "Changhua Meng", "Weiqiang Wang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2024 Findings", "summary": "Self-Consistency, a widely-used decoding strategy, significantly boosts the\nreasoning capabilities of Large Language Models (LLMs). However, it depends on\nthe plurality voting rule, which focuses on the most frequent answer while\noverlooking all other minority responses. These inconsistent minority views\noften illuminate areas of uncertainty within the model's generation process. To\naddress this limitation, we present Mirror-Consistency, an enhancement of the\nstandard Self-Consistency approach. Our method incorporates a 'reflective\nmirror' into the self-ensemble decoding process and enables LLMs to critically\nexamine inconsistencies among multiple generations. Additionally, just as\nhumans use the mirror to better understand themselves, we propose using\nMirror-Consistency to enhance the sample-based confidence calibration methods,\nwhich helps to mitigate issues of overconfidence. Our experimental results\ndemonstrate that Mirror-Consistency yields superior performance in both\nreasoning accuracy and confidence calibration compared to Self-Consistency.", "AI": {"tldr": "Presentation of Mirror-Consistency, an improved decoding strategy for LLMs that enhances reasoning capabilities and confidence calibration by addressing the limitations of traditional Self-Consistency.", "motivation": "The motivation behind this paper is to overcome the limitations of the Self-Consistency decoding strategy used in Large Language Models, which ignores minority responses that could reveal important areas of uncertainty.", "method": "The proposed Mirror-Consistency method integrates a reflective mirror into the self-ensemble decoding process, allowing LLMs to analyze and incorporate inconsistencies from multiple generations for better decision-making.", "result": "Experimental results show that Mirror-Consistency outperforms traditional Self-Consistency in both reasoning accuracy and confidence calibration.", "conclusion": "The proposed method offers a new approach to enhance LLM performance by addressing overconfidence and improving the understanding of model generation uncertainties.", "key_contributions": ["Introduction of Mirror-Consistency as a decoding enhancement for LLMs.", "Demonstration of improved reasoning accuracy compared to Self-Consistency.", "Proposed method for better confidence calibration of LLM outputs."], "limitations": "", "keywords": ["Large Language Models", "Self-Consistency", "Mirror-Consistency", "Confidence Calibration", "Reasoning Accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.13456", "pdf": "https://arxiv.org/pdf/2410.13456.pdf", "abs": "https://arxiv.org/abs/2410.13456", "title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland", "authors": ["Luca Rolshoven", "Vishvaksenan Rasiah", "Srinanda Brügger Bose", "Sarah Hostettler", "Lara Burkhalter", "Matthias Stürmer", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals", "AI": {"tldr": "Automated headnote creation can enhance legal research efficiency by summarizing court rulings.", "motivation": "Improving accessibility of legal cases through automated summarization to assist legal professionals in Switzerland.", "method": "Introduced the Swiss Leading Decision Summarization (SLDS) dataset comprising 18K court rulings in multiple languages and fine-tuned various mT5 models for summarization tasks.", "result": "Proprietary models excel in zero-shot settings, while fine-tuned smaller models offer competitive performance for summarization of legal documents.", "conclusion": "The dataset is publicly available to encourage further research and the development of legal assistive technologies.", "key_contributions": ["Creation of the SLDS dataset with multilingual legal data", "Evaluation of mT5 models for legal summarization", "Insights into zero-shot vs. fine-tuned model performance in legal contexts"], "limitations": "", "keywords": ["legal research", "automated summarization", "court rulings", "cross-lingual", "dataset"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2411.06207", "pdf": "https://arxiv.org/pdf/2411.06207.pdf", "abs": "https://arxiv.org/abs/2411.06207", "title": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models", "authors": ["Zhen Zhang", "Xinyu Wang", "Yong Jiang", "Zile Qiao", "Zhuo Chen", "Guangyu Li", "Feiteng Mu", "Mengting Hu", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in.", "AI": {"tldr": "This paper introduces a Knowledge Boundary Model (KBM) to optimize Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) by determining when RAG is necessary, thereby improving computational efficiency.", "motivation": "LLMs struggle with keeping up-to-date with dynamic knowledge and managing unknown static information, requiring a more efficient RAG approach.", "method": "The authors propose the KBM to identify the known and unknown components of a question, enabling selective triggering of RAG processes.", "result": "Experiments show that KBM effectively reduces unnecessary retrievals while maintaining or improving LLM performance across multiple datasets in English and Chinese.", "conclusion": "The KBM model significantly enhances performance by decreasing retrievals needed, demonstrating its utility in dynamic knowledge handling and as an LLM plug-in.", "key_contributions": ["Proposed the Knowledge Boundary Model (KBM) for LLMs", "Demonstrated efficiency in reducing computational costs during text generation", "Evaluated KBM in diverse contexts including dynamic and long-tail knowledge scenarios."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Knowledge Boundary Model", "dynamic knowledge", "long-tail static knowledge"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.12478", "pdf": "https://arxiv.org/pdf/2412.12478.pdf", "abs": "https://arxiv.org/abs/2412.12478", "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script", "authors": ["Xi Cao", "Yuan Sun", "Jiajun Li", "Quzong Gesang", "Nuo Qun", "Tashi Nyima"], "categories": ["cs.CL", "cs.CR", "cs.HC"], "comment": null, "summary": "DNN-based language models excel across various NLP tasks but remain highly\nvulnerable to textual adversarial attacks. While adversarial text generation is\ncrucial for NLP security, explainability, evaluation, and data augmentation,\nrelated work remains overwhelmingly English-centric, leaving the problem of\nconstructing high-quality and sustainable adversarial robustness benchmarks for\nlower-resourced languages both difficult and understudied. First, method\ncustomization for lower-resourced languages is complicated due to linguistic\ndifferences and limited resources. Second, automated attacks are prone to\ngenerating invalid or ambiguous adversarial texts. Last but not least, language\nmodels continuously evolve and may be immune to parts of previously generated\nadversarial texts. To address these challenges, we introduce HITL-GAT, an\ninteractive system based on a general approach to human-in-the-loop generation\nof adversarial texts. Additionally, we demonstrate the utility of HITL-GAT\nthrough a case study on Tibetan script, employing three customized adversarial\ntext generation methods and establishing its first adversarial robustness\nbenchmark, providing a valuable reference for other lower-resourced languages.", "AI": {"tldr": "HITL-GAT is an interactive system for generating adversarial texts to improve NLP security, specifically focusing on lower-resourced languages like Tibetan.", "motivation": "To tackle the vulnerability of DNN-based language models to adversarial attacks, especially for lower-resourced languages that lack established benchmarks.", "method": "An interactive system called HITL-GAT is introduced, utilizing human-in-the-loop techniques to generate adversarial texts tailored to linguistic and resource challenges of lower-resourced languages.", "result": "HITL-GAT was applied to develop adversarial text generation methods for Tibetan script and established a first adversarial robustness benchmark for this language.", "conclusion": "The proposed system offers significant advancements for assessing adversarial robustness in lower-resourced languages, providing foundational benchmarks and methods for future research.", "key_contributions": ["Introduction of HITL-GAT for adversarial text generation", "Developed adversarial methods tailored for Tibetan script", "Established the first adversarial robustness benchmark for lower-resourced languages"], "limitations": "The effectiveness of the methods may vary with other languages and continuous evolution of language models may affect past adversarial texts.", "keywords": ["adversarial attacks", "NLP security", "lower-resourced languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.09765", "pdf": "https://arxiv.org/pdf/2501.09765.pdf", "abs": "https://arxiv.org/abs/2501.09765", "title": "Enhancing the De-identification of Personally Identifiable Information in Educational Data", "authors": ["Zilyu Ji", "Yuntian Shen", "Jionghao Lin", "Kenneth R. Koedinger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https://github.com/AnonJD/PrivacyAI", "AI": {"tldr": "The study investigates the GPT-4o-mini model for PII detection in educational data, assessing its fine-tuning versus established frameworks and demonstrating superior performance.", "motivation": "To protect Personally Identifiable Information (PII) in educational technologies while maintaining data utility.", "method": "The research employs prompting and fine-tuning approaches to evaluate and compare the performance of the GPT-4o-mini model against established frameworks.", "result": "The fine-tuned GPT-4o-mini model achieved a recall of 0.9589 on the CRAPII dataset, showing improved precision and reduced computational costs compared to Azure AI Language.", "conclusion": "Fine-tuned GPT-4o-mini proves to be a cost-effective and accurate tool for PII detection, ensuring privacy protection and enhancing data utility for educational purposes.", "key_contributions": ["Investigation of GPT-4o-mini model for PII detection tasks in educational data.", "Comparison of fine-tuning and prompting approaches against established frameworks.", "Demonstration of the model's cost-effectiveness and improved performance metrics."], "limitations": "", "keywords": ["PII detection", "GPT-4o-mini", "educational data", "privacy protection", "fine-tuning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.19301", "pdf": "https://arxiv.org/pdf/2501.19301.pdf", "abs": "https://arxiv.org/abs/2501.19301", "title": "Beyond checkmate: exploring the creative chokepoints in AI text", "authors": ["Nafis Irtiza Tripto", "Saranya Venkatraman", "Mahjabin Nahar", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at 30th Conference on Empirical Methods in Natural Language\n  Processing (EMNLP'25 Main conference). 9 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction.", "AI": {"tldr": "This study investigates the distinctions between human and AI-generated texts across various segments to enhance detection strategies for LLM-generated content.", "motivation": "With the rise of Large Language Models, concerns about the misuse of text generation have grown, necessitating effective detection methods for LLM-generated text.", "method": "The research analyzes human and AI texts through segment-specific patterns, drawing an analogy to chess games, to uncover differences in writing styles across segments like introduction, body, and conclusion.", "result": "AI texts, while closely resembling human writing in the body section, show significant differences in features such as linguistic flow, with human texts displaying greater stylistic variation across segments.", "conclusion": "The study provides insights into the differences between human and AI texts that can inform better detection techniques and understanding of LLM capabilities.", "key_contributions": ["Introduces segment-specific analysis to differentiate human and AI-generated texts", "Reveals that the body segment, despite its length, is less unique compared to other segments", "Highlights human stylistic variation as a key factor in distinguishing between text types."], "limitations": "", "keywords": ["Large Language Models", "Text Generation", "Human-AI Text Distinction", "Natural Language Processing", "Detection Strategies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.07445", "pdf": "https://arxiv.org/pdf/2502.07445.pdf", "abs": "https://arxiv.org/abs/2502.07445", "title": "Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon", "authors": ["Nurit Cohen-Inger", "Yehonatan Elisha", "Bracha Shapira", "Lior Rokach", "Seffi Cohen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings, indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.", "AI": {"tldr": "The paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to evaluate overfitting in large language models (LLMs) by distorting benchmark prompts and assessing true language understanding beyond surface cues.", "motivation": "To address concerns that high benchmark scores of LLMs may not reflect genuine language understanding but rather an overreliance on surface patterns in the training data.", "method": "C-BOD introduces a framework that systematically distorts benchmark prompts while preserving semantic content to evaluate LLMs' susceptibility to memorized patterns.", "result": "The evaluation revealed a 2.15% average performance decrease across 26 LLMs, with 20 models showing significant performance differences under perturbation, indicating overfitting tendencies, especially in higher accuracy models.", "conclusion": "C-BOD provides a method to improve LLM training processes by promoting models that exhibit resilience and generalization instead of simply high leaderboard scores.", "key_contributions": ["Introduction of C-BOD for detecting overfitting in LLMs", "Revealing performance biases in LLMs based on prompt rephrasing", "Highlighting the importance of robustness and generalization in LLM evaluations"], "limitations": "", "keywords": ["large language models", "benchmark overfitting", "C-BOD", "model evaluation", "language understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.19749", "pdf": "https://arxiv.org/pdf/2502.19749.pdf", "abs": "https://arxiv.org/abs/2502.19749", "title": "What's Not Said Still Hurts: A Description-Based Evaluation Framework for Measuring Social Bias in LLMs", "authors": ["Jinhao Pan", "Chahat Raj", "Ziyu Yao", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": "EMNLP Findings 2025", "summary": "Large Language Models (LLMs) often exhibit social biases inherited from their\ntraining data. While existing benchmarks evaluate bias by term-based mode\nthrough direct term associations between demographic terms and bias terms, LLMs\nhave become increasingly adept at avoiding biased responses, leading to\nseemingly low levels of bias. However, biases persist in subtler, contextually\nhidden forms that traditional benchmarks fail to capture. We introduce the\nDescription-based Bias Benchmark (DBB), a novel dataset designed to assess bias\nat the semantic level that bias concepts are hidden within naturalistic, subtly\nframed contexts in real-world scenarios rather than superficial terms. We\nanalyze six state-of-the-art LLMs, revealing that while models reduce bias in\nresponse at the term level, they continue to reinforce biases in nuanced\nsettings. Data, code, and results are available at\nhttps://github.com/JP-25/Description-based-Bias-Benchmark.", "AI": {"tldr": "This paper introduces a new benchmark for evaluating social bias in Large Language Models (LLMs) by focusing on semantic-level biases hidden in context rather than term-based associations.", "motivation": "To address the shortcomings of existing bias benchmarks that primarily focus on direct term associations, which fail to identify more subtle, contextually hidden biases in LLMs.", "method": "The authors present the Description-based Bias Benchmark (DBB), a dataset that assesses LLMs for biases at the semantic level in real-world contexts.", "result": "Analysis of six state-of-the-art LLMs shows that while they reduce bias at the term level, they continue to reflect biases in more nuanced and subtle contexts.", "conclusion": "DBB provides a more accurate measure of bias in LLMs, revealing persistent issues that traditional benchmarks miss; the dataset and results are freely available for further research.", "key_contributions": ["Introduction of the Description-based Bias Benchmark (DBB) for evaluating biases in LLMs.", "Demonstration of subtle biases in LLMs that are not captured by traditional metrics.", "Release of data, code, and results to promote further evaluation of bias in LLMs."], "limitations": "", "keywords": ["Large Language Models", "social bias", "benchmarks", "natural language processing", "semantics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23759", "pdf": "https://arxiv.org/pdf/2505.23759.pdf", "abs": "https://arxiv.org/abs/2505.23759", "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", "authors": ["Heekyung Lee", "Jiaxin Ge", "Tsung-Han Wu", "Minwoo Kang", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.", "AI": {"tldr": "The paper evaluates vision-language models (VLMs) in solving rebus puzzles, highlighting their strengths in simple visual decoding and weaknesses in abstract reasoning.", "motivation": "To assess how current VLMs interpret and solve rebus puzzles, which require complex reasoning and cultural understanding beyond standard tasks.", "method": "The authors created a hand-generated and annotated benchmark of English rebus puzzles and analyzed the performance of various VLMs on these tasks.", "result": "VLMs show some ability to decode simple visual clues but struggle significantly with abstract reasoning and understanding visual metaphors.", "conclusion": "The evaluation reveals both the potential and limitations of VLMs in tackling tasks that require multi-modal abstraction.", "key_contributions": ["Creation of a benchmark for rebus puzzles for VLM assessment", "Analysis of VLM capabilities in solving complex, culturally nuanced puzzles", "Identification of specific limitations in abstract reasoning and metaphor understanding in VLMs"], "limitations": "The puzzles constructed may not encompass all possible linguistic and visual challenges present in rebus puzzles.", "keywords": ["vision-language models", "rebus puzzles", "multi-modal reasoning", "symbolic reasoning", "cultural understanding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032.pdf", "abs": "https://arxiv.org/abs/2506.07032", "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/.", "AI": {"tldr": "Introducing ViMUL-Bench, a benchmark to evaluate multilingual video large multimodal models (LMMs) across 14 languages; it includes a dataset and a model designed for inclusivity in cultural representation.", "motivation": "The need for cultural and linguistic inclusivity in video large multimodal models (LMMs) beyond the English language has not been adequately addressed in previous research.", "method": "Introduction of ViMUL-Bench to evaluate Video LMMs across 14 languages with a specific focus on low- and high-resource languages; creation of a machine translated multilingual video training set.", "result": "ViMUL-Bench includes 8,000 manually verified samples across various categories and a multilingual video LMM named ViMUL that improves understanding for both high- and low-resource languages.", "conclusion": "ViMUL-Bench and the corresponding multilingual video model and training set aim to facilitate research towards developing more inclusive multilingual video LMMs and are publicly available.", "key_contributions": ["ViMUL-Bench for evaluating multilingual video LMM capabilities across diverse languages", "Creation of a large-scale multilingual video training dataset", "Development of ViMUL, a multilingual LMM that offers better tradeoff for language resources"], "limitations": "", "keywords": ["multimodal models", "multilingual evaluation", "video analysis", "cultural inclusivity", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
