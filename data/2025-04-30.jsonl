{"id": "2504.20049", "pdf": "https://arxiv.org/pdf/2504.20049.pdf", "abs": "https://arxiv.org/abs/2504.20049", "title": "It's the same but not the same: Do LLMs distinguish Spanish varieties?", "authors": ["Marina Mayor-Rocher", "Cristina Pozo", "Nina Melero", "Gonzalo Martínez", "María Grandury", "Pedro Reviriego"], "categories": ["cs.CL"], "comment": "in Spanish language", "summary": "In recent years, large language models (LLMs) have demonstrated a high\ncapacity for understanding and generating text in Spanish. However, with five\nhundred million native speakers, Spanish is not a homogeneous language but\nrather one rich in diatopic variations spanning both sides of the Atlantic. For\nthis reason, in this study, we evaluate the ability of nine language models to\nidentify and distinguish the morphosyntactic and lexical peculiarities of seven\nvarieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,\nPeninsular, Mexican and Central American and Rioplatense) through a\nmultiple-choice test. The results indicate that the Peninsular Spanish variety\nis the best identified by all models and that, among them, GPT-4o is the only\nmodel capable of recognizing the variability of the Spanish language.\n  --\n  En los \\'ultimos a\\~nos, los grandes modelos de lenguaje (LLMs, por sus\nsiglas en ingl\\'es) han demostrado una alta capacidad para comprender y generar\ntexto en espa\\~nol. Sin embargo, con quinientos millones de hablantes nativos,\nla espa\\~nola no es una lengua homog\\'enea, sino rica en variedades\ndiat\\'opicas que se extienden a ambos lados del Atl\\'antico. Por todo ello,\nevaluamos en este trabajo la capacidad de nueve modelos de lenguaje de\nidentificar y discernir las peculiaridades morfosint\\'acticas y l\\'exicas de\nsiete variedades de espa\\~nol (andino, antillano, caribe\\~no continental,\nchileno, espa\\~nol peninsular, mexicano y centroamericano y rioplatense)\nmediante un test de respuesta m\\'ultiple. Los resultados obtenidos indican que\nla variedad de espa\\~nol peninsular es la mejor identificada por todos los\nmodelos y que, de entre todos, GPT-4o es el \\'unico modelo capaz de identificar\nla variabilidad de la lengua espa\\~nola."}
{"id": "2504.20051", "pdf": "https://arxiv.org/pdf/2504.20051.pdf", "abs": "https://arxiv.org/abs/2504.20051", "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "authors": ["Frances Laureano De Leon", "Harish Tayyar Madabushi", "Mark G. Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Multiword expressions, characterised by non-compositional meanings and\nsyntactic irregularities, are an example of nuanced language. These expressions\ncan be used literally or idiomatically, leading to significant changes in\nmeaning. While large language models have demonstrated strong performance\nacross many tasks, their ability to handle such linguistic subtleties remains\nuncertain. Therefore, this study evaluates how state-of-the-art language models\nprocess the ambiguity of potentially idiomatic multiword expressions,\nparticularly in contexts that are less frequent, where models are less likely\nto rely on memorisation. By evaluating models across in Portuguese and\nGalician, in addition to English, and using a novel code-switched dataset and a\nnovel task, we find that large language models, despite their strengths,\nstruggle with nuanced language. In particular, we find that the latest models,\nincluding GPT-4, fail to outperform the xlm-roBERTa-base baselines in both\ndetection and semantic tasks, with especially poor performance on the novel\ntasks we introduce, despite its similarity to existing tasks. Overall, our\nresults demonstrate that multiword expressions, especially those which are\nambiguous, continue to be a challenge to models."}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086.pdf", "abs": "https://arxiv.org/abs/2504.20086", "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss."}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157.pdf", "abs": "https://arxiv.org/abs/2504.20157", "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared."}
{"id": "2504.20308", "pdf": "https://arxiv.org/pdf/2504.20308.pdf", "abs": "https://arxiv.org/abs/2504.20308", "title": "Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South", "authors": ["Ozioma C. Oguine", "Oghenemaro Anuyah", "Zainab Agha", "Iris Melgarez", "Adriana Alvarado Garcia", "Karla Badillo-Urquiola"], "categories": ["cs.HC", "cs.CY"], "comment": "30 pages, 1 figure", "summary": "Youth online safety research in HCI has historically centered on perspectives\nfrom the Global North, often overlooking the unique particularities and\ncultural contexts of regions in the Global South. This paper presents a\nsystematic review of 66 youth online safety studies published between 2014 and\n2024, specifically focusing on regions in the Global South. Our findings reveal\na concentrated research focus in Asian countries and predominance of\nquantitative methods. We also found limited research on marginalized youth\npopulations and a primary focus on risks related to cyberbullying. Our analysis\nunderscores the critical role of cultural factors in shaping online safety,\nhighlighting the need for educational approaches that integrate social dynamics\nand awareness. We propose methodological recommendations and a future research\nagenda that encourages the adoption of situated, culturally sensitive\nmethodologies and youth-centered approaches to researching youth online safety\nregions in the Global South. This paper advocates for greater inclusivity in\nyouth online safety research, emphasizing the importance of addressing varied\nsociocultural contexts to better understand and meet the online safety needs of\nyouth in the Global South."}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168.pdf", "abs": "https://arxiv.org/abs/2504.20168", "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats."}
{"id": "2504.20320", "pdf": "https://arxiv.org/pdf/2504.20320.pdf", "abs": "https://arxiv.org/abs/2504.20320", "title": "\"I've talked to ChatGPT about my issues last night.\": Examining Mental Health Conversations with Large Language Models through Reddit Analysis", "authors": ["Kyuha Jung", "Gyuho Lee", "Yuanhui Huang", "Yunan Chen"], "categories": ["cs.HC"], "comment": "Forthcoming at the 28th ACM SIGCHI Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW '25) on October 18-22, 2025", "summary": "We investigate the role of large language models (LLMs) in supporting mental\nhealth by analyzing Reddit posts and comments about mental health conversations\nwith ChatGPT. Our findings reveal that users value ChatGPT as a safe,\nnon-judgmental space, often favoring it over human support due to its\naccessibility, availability, and knowledgeable responses. ChatGPT provides a\nrange of support, including actionable advice, emotional support, and\nvalidation, while helping users better understand their mental states.\nAdditionally, we found that ChatGPT offers innovative support for individuals\nfacing mental health challenges, such as assistance in navigating difficult\nconversations, preparing for therapy sessions, and exploring therapeutic\ninterventions. However, users also voiced potential risks, including the spread\nof incorrect health advice, ChatGPT's overly validating nature, and privacy\nconcerns. We discuss the implications of LLMs as tools for mental health\nsupport in both everyday health and clinical therapy settings and suggest\nstrategies to mitigate risks in LLM-powered interactions."}
{"id": "2504.20220", "pdf": "https://arxiv.org/pdf/2504.20220.pdf", "abs": "https://arxiv.org/abs/2504.20220", "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports", "authors": ["Henning Schäfer", "Cynthia S. Schmidt", "Johannes Wutzkowsky", "Kamil Lorek", "Lea Reinartz", "Johannes Rückert", "Christian Temme", "Britta Böckmann", "Peter A. Horn", "Christoph M. Friedrich"], "categories": ["cs.CL", "cs.CV", "68T07", "I.7.5; I.4.7; I.2.7; H.3.3; J.3"], "comment": null, "summary": "Despite the growing adoption of electronic health records, many processes\nstill rely on paper documents, reflecting the heterogeneous real-world\nconditions in which healthcare is delivered. The manual transcription process\nis time-consuming and prone to errors when transferring paper-based data to\ndigital formats. To streamline this workflow, this study presents an\nopen-source pipeline that extracts and categorizes checkbox data from scanned\ndocuments. Demonstrated on transfusion reaction reports, the design supports\nadaptation to other checkbox-rich document types. The proposed method\nintegrates checkbox detection, multilingual optical character recognition (OCR)\nand multilingual vision-language models (VLMs). The pipeline achieves high\nprecision and recall compared against annually compiled gold-standards from\n2017 to 2024. The result is a reduction in administrative workload and accurate\nregulatory reporting. The open-source availability of this pipeline encourages\nself-hosted parsing of checkbox forms."}
{"id": "2504.20342", "pdf": "https://arxiv.org/pdf/2504.20342.pdf", "abs": "https://arxiv.org/abs/2504.20342", "title": "Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI", "authors": ["Shou-Tzu Han"], "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2; H.1.2"], "comment": "10 pages, 5 figures, preliminary results, early-stage work intended\n  for future conference submission", "summary": "Reflexion is an AI-powered platform designed to enable structured emotional\nself-reflection at scale. By integrating real-time emotion detection, layered\nreflective prompting, and metaphorical storytelling generation, Reflexion\nempowers users to engage in autonomous emotional exploration beyond basic\nsentiment categorization. Grounded in theories of expressive writing, cognitive\nrestructuring, self-determination, and critical consciousness development, the\nsystem scaffolds a progressive journey from surface-level emotional recognition\ntoward value-aligned action planning. Initial pilot studies with diverse\nparticipants demonstrate positive outcomes in emotional articulation, cognitive\nreframing, and perceived psychological resilience. Reflexion represents a\npromising direction for scalable, theory-informed affective computing\ninterventions aimed at fostering emotional literacy and psychological growth\nacross educational, therapeutic, and public health contexts."}
{"id": "2504.20251", "pdf": "https://arxiv.org/pdf/2504.20251.pdf", "abs": "https://arxiv.org/abs/2504.20251", "title": "A Platform for Generating Educational Activities to Teach English as a Second Language", "authors": ["Aiala Rosá", "Santiago Góngora", "Juan Pablo Filevich", "Ignacio Sastre", "Laura Musto", "Brian Carpenter", "Luis Chiruzzo"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Unpublished report written in 2023", "summary": "We present a platform for the generation of educational activities oriented\nto teaching English as a foreign language. The different activities -- games\nand language practice exercises -- are strongly based on Natural Language\nProcessing techniques. The platform offers the possibility of playing\nout-of-the-box games, generated from resources created semi-automatically and\nthen manually curated. It can also generate games or exercises of greater\ncomplexity from texts entered by teachers, providing a stage of review and\nedition of the generated content before use. As a way of expanding the variety\nof activities in the platform, we are currently experimenting with image and\ntext generation. In order to integrate them and improve the performance of\nother neural tools already integrated, we are working on migrating the platform\nto a more powerful server. In this paper we describe the development of our\nplatform and its deployment for end users, discussing the challenges faced and\nhow we overcame them, and also detail our future work plans."}
{"id": "2504.20365", "pdf": "https://arxiv.org/pdf/2504.20365.pdf", "abs": "https://arxiv.org/abs/2504.20365", "title": "Thoughtful, Confused, or Untrustworthy: How Text Presentation Influences Perceptions of AI Writing Tools", "authors": ["David Zhou", "John R. Gallagher", "Sarah Sterman"], "categories": ["cs.HC"], "comment": "17 pages, 3 figures, ACM Creativity and Cognition 2025", "summary": "AI writing tools have been shown to dramatically change the way people write,\nyet the effects of AI text presentation are not well understood nor always\nintentionally designed. Although text presentation in existing large language\nmodel interfaces is linked to the speed of the underlying model, text\npresentation speed can impact perceptions of AI systems, potentially\ninfluencing whether AI suggestions are accepted or rejected. In this paper, we\nanalyze the effects of varying text generation speed in creative and\nprofessional writing scenarios on an online platform (n=297). We find that\nspeed is correlated with perceived humanness and trustworthiness of the AI\ntool, as well as the perceived quality of the generated text. We discuss its\nimplications on creative and writing processes, along with future steps in the\nintentional design of AI writing tool interfaces."}
{"id": "2504.20276", "pdf": "https://arxiv.org/pdf/2504.20276.pdf", "abs": "https://arxiv.org/abs/2504.20276", "title": "Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi", "authors": ["Dandan Chen Kaptur", "Yue Huang", "Xuejun Ryan Ji", "Yanhui Guo", "Bradley Kaptur"], "categories": ["cs.CL", "stat.AP"], "comment": "13 pages, Paper presented at the National Council on Measurement in\n  Education (NCME) Conference, Denver, Colorado, in April 2025", "summary": "This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),\nfor systematic reviews. We evaluated their performance by comparing\nLLM-generated codes with human-generated codes from a peer-reviewed systematic\nreview on assessment. Our findings suggested that the performance of LLMs\nfluctuates by data volume and question complexity for systematic reviews."}
{"id": "2504.20369", "pdf": "https://arxiv.org/pdf/2504.20369.pdf", "abs": "https://arxiv.org/abs/2504.20369", "title": "Perception-aware Sampling for Scatterplot Visualizations", "authors": ["Zafeiria Moumoulidou", "Hamza Elhamdadi", "Ke Yang", "Subrata Mitra", "Cindy Xiong Bearfield", "Alexandra Meliou"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Visualizing data is often a crucial first step in data analytics workflows,\nbut growing data sizes pose challenges due to computational and visual\nperception limitations. As a result, data analysts commonly down-sample their\ndata and work with subsets. Deriving representative samples, however, remains a\nchallenge. This paper focuses on scatterplots, a widely-used visualization\ntype, and introduces a novel sampling objective -- perception-awareness --\naiming to improve sample efficacy by targeting humans' perception of a\nvisualization.\n  We make the following contributions: (1) We propose perception-augmented\ndatabases and design PAwS: a novel perception-aware sampling method for\nscatterplots that leverages saliency maps -- a computer vision tool for\npredicting areas of attention focus in visualizations -- and models\nperception-awareness via saliency, density, and coverage objectives. (2) We\ndesign ApproPAwS: a fast, perception-aware method for approximate\nvisualizations, which exploits the fact that small visual perturbations are\noften imperceptible to humans. (3) We introduce the concept of perceptual\nsimilarity as a metric for sample quality, and present a novel method that\ncompares saliency maps to measure it. (4) Our extensive experimental evaluation\nshows that our methods consistently outperform prior art in producing samples\nwith high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups\nwith minimal loss in visual fidelity. Our user study shows that PAwS is often\npreferred by humans, validating our quantitative findings."}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304.pdf", "abs": "https://arxiv.org/abs/2504.20304", "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research."}
{"id": "2504.20567", "pdf": "https://arxiv.org/pdf/2504.20567.pdf", "abs": "https://arxiv.org/abs/2504.20567", "title": "Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks", "authors": ["Tanmay Chakraborty", "Marion Koelle", "Jörg Schlötterer", "Nadine Schlicker", "Christian Wirth", "Christin Seifert"], "categories": ["cs.HC"], "comment": null, "summary": "Bayesian Optimisation (BO) is a family of methods for finding optimal\nparameters when the underlying function to be optimised is unknown. BO is used,\nfor example, for hyperparameter tuning in machine learning and as an expert\nsupport tool for tuning cyberphysical systems. For settings where humans are\ninvolved in the tuning task, methods have been developed to explain BO\n(Explainable Bayesian Optimization, XBO). However, there is little guidance on\nhow to present XBO results to humans so that they can tune the system\neffectively and efficiently. In this paper, we investigate how the XBO\nexplanation format affects users' task performance, task load, understanding\nand trust in XBO. We chose a task that is accessible to a wide range of users.\nSpecifically, we set up an egg cooking scenario with 6 parameters that\nparticipants had to adjust to achieve a perfect soft-boiled egg. We compared\nthree different explanation formats: a bar chart, a list of rules and a textual\nexplanation in a between-subjects online study with 213 participants. Our\nresults show that adding any type of explanation increases task success,\nreduces the number of trials needed to achieve success, and improves\ncomprehension and confidence. While explanations add more information for\nparticipants to process, we found no increase in user task load. We also found\nthat the aforementioned results were independent of the explanation format; all\nformats had a similar effect.This is an interesting finding for practical\napplications, as it suggests that explanations can be added to BO tuning tasks\nwithout the burden of designing or selecting specific explanation formats. In\nthe future, it would be interesting to investigate scenarios of prolonged use\nof the explanation formats and whether they have different effects on users'\nmental models of the underlying system."}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323.pdf", "abs": "https://arxiv.org/abs/2504.20323", "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases."}
{"id": "2504.20741", "pdf": "https://arxiv.org/pdf/2504.20741.pdf", "abs": "https://arxiv.org/abs/2504.20741", "title": "In defence of post-hoc explanations in medical AI", "authors": ["Joshua Hatherley", "Lauritz Munch", "Jens Christian Bjerring"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Since the early days of the Explainable AI movement, post-hoc explanations\nhave been praised for their potential to improve user understanding, promote\ntrust, and reduce patient safety risks in black box medical AI systems.\nRecently, however, critics have argued that the benefits of post-hoc\nexplanations are greatly exaggerated since they merely approximate, rather than\nreplicate, the actual reasoning processes that black box systems take to arrive\nat their outputs. In this article, we aim to defend the value of post-hoc\nexplanations against this recent critique. We argue that even if post-hoc\nexplanations do not replicate the exact reasoning processes of black box\nsystems, they can still improve users' functional understanding of black box\nsystems, increase the accuracy of clinician-AI teams, and assist clinicians in\njustifying their AI-informed decisions. While post-hoc explanations are not a\n\"silver bullet\" solution to the black box problem in medical AI, we conclude\nthat they remain a useful strategy for addressing the black box problem in\nmedical AI."}
{"id": "2504.20355", "pdf": "https://arxiv.org/pdf/2504.20355.pdf", "abs": "https://arxiv.org/abs/2504.20355", "title": "Local Prompt Optimization", "authors": ["Yash Jain", "Vishal Chowdhary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as Oral at NAACL 2025 (Main Conference)", "summary": "In recent years, the use of prompts to guide the output of Large Language\nModels have increased dramatically. However, even the best of experts struggle\nto choose the correct words to stitch up a prompt for the desired task. To\nsolve this, LLM driven prompt optimization emerged as an important problem.\nExisting prompt optimization methods optimize a prompt globally, where in all\nthe prompt tokens have to be optimized over a large vocabulary while solving a\ncomplex task. The large optimization space (tokens) leads to insufficient\nguidance for a better prompt. In this work, we introduce Local Prompt\nOptimization (LPO) that integrates with any general automatic prompt\nengineering method. We identify the optimization tokens in a prompt and nudge\nthe LLM to focus only on those tokens in its optimization step. We observe\nremarkable performance improvements on Math Reasoning (GSM8k and MultiArith)\nand BIG-bench Hard benchmarks across various automatic prompt engineering\nmethods. Further, we show that LPO converges to the optimal prompt faster than\nglobal methods."}
{"id": "2504.20782", "pdf": "https://arxiv.org/pdf/2504.20782.pdf", "abs": "https://arxiv.org/abs/2504.20782", "title": "Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces", "authors": ["Daniel Gaspar-Figueiredo", "Marta Fernández-Diego", "Silvia Abrahão", "Emilio Insfran"], "categories": ["cs.HC", "cs.SE"], "comment": "Accepted for publication at the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025)", "summary": "Adaptive User Interfaces (AUI) play a crucial role in modern software\napplications by dynamically adjusting interface elements to accommodate users'\ndiverse and evolving needs. However, existing adaptation strategies often lack\nreal-time responsiveness. Reinforcement Learning (RL) has emerged as a\npromising approach for addressing complex, sequential adaptation challenges,\nenabling adaptive systems to learn optimal policies based on previous\nadaptation experiences. Although RL has been applied to AUIs,integrating RL\nagents effectively within user interactions remains a challenge.\n  In this paper, we enhance a RL-based Adaptive User Interface adaption\nframework by incorporating personalized human feedback directly into the\nleaning process. Unlike prior approaches that rely on a single pre-trained RL\nmodel, our approach trains a unique RL agent for each user, allowing\nindividuals to actively shape their personal RL agent's policy, potentially\nleading to more personalized and responsive UI adaptations. To evaluate this\napproach, we conducted an empirical study to assess the impact of integrating\nhuman feedback into the RL-based Adaptive User Interface adaption framework and\nits effect on User Experience (UX). The study involved 33 participants\ninteracting with AUIs incorporating human feedback and non-adaptive user\ninterfaces in two domains: an e-learning platform and a trip-planning\napplication. The results suggest that incorporating human feedback into\nRL-driven adaptations significantly enhances UX, offering promising directions\nfor advancing adaptive capabilities and user-centered design in AUIs."}
{"id": "2504.20356", "pdf": "https://arxiv.org/pdf/2504.20356.pdf", "abs": "https://arxiv.org/abs/2504.20356", "title": "What Causes Knowledge Loss in Multilingual Language Models?", "authors": ["Maria Khelli", "Samuel Cahyawijaya", "Ayu Purwarianti", "Genta Indra Winata"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer in natural language processing (NLP) models enhances\nmultilingual performance by leveraging shared linguistic knowledge. However,\ntraditional methods that process all data simultaneously often fail to mimic\nreal-world scenarios, leading to challenges like catastrophic forgetting, where\nfine-tuning on new tasks degrades performance on previously learned ones. Our\nstudy explores this issue in multilingual contexts, focusing on linguistic\ndifferences affecting representational learning rather than just model\nparameters. We experiment with 52 languages using LoRA adapters of varying\nranks to evaluate non-shared, partially shared, and fully shared parameters.\nOur aim is to see if parameter sharing through adapters can mitigate forgetting\nwhile preserving prior knowledge. We find that languages using non-Latin\nscripts are more susceptible to catastrophic forgetting, whereas those written\nin Latin script facilitate more effective cross-lingual transfer."}
{"id": "2504.20844", "pdf": "https://arxiv.org/pdf/2504.20844.pdf", "abs": "https://arxiv.org/abs/2504.20844", "title": "Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations", "authors": ["Angelika Kothe", "Volker Hohmann", "Giso Grimm"], "categories": ["cs.HC", "cs.SD"], "comment": null, "summary": "Interactive communication in virtual reality can be used in experimental\nparadigms to increase the ecological validity of hearing device evaluations.\nThis requires the virtual environment to elicit natural communication behaviour\nin listeners. This study evaluates the effect of virtual animated characters'\nhead movements on participants' communication behaviour and experience.\n  Triadic conversations were conducted between a test participant and two\nconfederates. To facilitate the manipulation of head movements, the\nconversation was conducted in telepresence using a system that transmitted\naudio, head movement data and video with low delay. The confederates were\nrepresented by virtual animated characters (avatars) with different levels of\nanimation: Static heads, automated head movement animations based on speech\nlevel onsets, and animated head movements based on the transmitted head\nmovements of the interlocutors. A condition was also included in which the\nvideos of the interlocutors' heads were embedded in the visual scene.\n  The results show significant effects of animation level on the participants'\nspeech and head movement behaviour as recorded by physical sensors, as well as\non the subjective sense of presence and the success of the conversation. The\nlargest effects were found for the range of head orientation during speech and\nthe perceived realism of avatars. Participants reported that they were spoken\nto in a more helpful way when the avatars showed head movements transmitted\nfrom the interlocutors than when the avatars' heads were static.\n  We therefore conclude that the representation of interlocutors must include\nsufficiently realistic head movements in order to elicit natural communication\nbehaviour."}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371.pdf", "abs": "https://arxiv.org/abs/2504.20371", "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Yufeng Chen", "Jinan Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory; the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompting templates, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompting strategies on multiple\nstate-of-the-art LLMs. Our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs."}
{"id": "2504.20886", "pdf": "https://arxiv.org/pdf/2504.20886.pdf", "abs": "https://arxiv.org/abs/2504.20886", "title": "Mapping a Movement: Exploring a Proposed Police Training Facility in Atlanta and the Stop Cop City Movement through Online Maps", "authors": ["Camille Harris", "Clio Andris"], "categories": ["cs.HC"], "comment": "Supplementary material available at\n  https://doi.org/10.7910/DVN/PCQ294", "summary": "In 2021, the City of Atlanta and Atlanta Police Foundation launched plans to\nbuild a large police training facility in the South River Forest in\nunincorporated DeKalb County, GA. Residents of Atlanta and DeKalb County,\nenvironmental activists, police and prison abolitionists, and other activists\nand concerned individuals formed the movement in opposition to the facility,\nknown as the Stop Cop City / Defend the Atlanta Forest movement. Social media\nand digital maps became common tools for communicating information about the\nfacility and the movement. Here, we examine online maps about the facility and\nthe opposition movement, originating from grassroots organizations, the City of\nAtlanta, news media outlets, the Atlanta Police Foundation, and individuals. We\ngather and examine 32 publicly available maps collected through the Google\nSearch API, Twitter (now X), Instagram and reddit. Using a framework of\ncritical cartography, we conduct a content analysis of these maps to identify\nthe mapping technologies and techniques (data, cartographic elements, styles)\nused by different stakeholders and roles that maps and mapping technologies can\nplay in social movements. We examine the extent to which these maps provide\ndata to confirm or contradict concerns raised by grassroots organizations and\nlocal residents about the facility. We find that stakeholders and mapmakers use\ngeospatial tools in different ways and likely have varied access to mapping\ntechnologies. We argue that documenting the use of maps to communicate\ninformation about a contentious project can help enumerate community positions\nand perspectives, and we advocate for accessible mapmaking tools. We conclude\nby discussing the implications of accessibility of mapping technology and\nposting maps to social media, and share example map images that extend the\ngeographic information systems (GIS) techniques seen in the retrieved maps."}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444.pdf", "abs": "https://arxiv.org/abs/2504.20444", "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "authors": ["Mika Hämäläinen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst."}
{"id": "2504.20976", "pdf": "https://arxiv.org/pdf/2504.20976.pdf", "abs": "https://arxiv.org/abs/2504.20976", "title": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users", "authors": ["Dabbrata Das", "Argho Deb Das", "Farhan Sadaf"], "categories": ["cs.HC"], "comment": null, "summary": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience."}
{"id": "2504.20451", "pdf": "https://arxiv.org/pdf/2504.20451.pdf", "abs": "https://arxiv.org/abs/2504.20451", "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs", "authors": ["Daniel Lee", "Harsh Sharma", "Jieun Han", "Sunny Jeong", "Alice Oh", "Vered Shwartz"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SemEval-2025 Workshop (ACL 2025)", "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation."}
{"id": "2406.11357", "pdf": "https://arxiv.org/pdf/2406.11357.pdf", "abs": "https://arxiv.org/abs/2406.11357", "title": "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "authors": ["Zhonghao Li", "Xuming Hu", "Aiwei Liu", "Kening Zheng", "Sirui Huang", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.MA"], "comment": "8 pages", "summary": "Large Language Models (LLMs) are limited by their parametric knowledge,\nleading to hallucinations in knowledge-extensive tasks. To address this,\nRetrieval-Augmented Generation (RAG) incorporates external document chunks to\nexpand LLM knowledge. Furthermore, compressing information from document chunks\nthrough extraction or summarization can improve LLM performance. Nonetheless,\nLLMs still struggle to notice and utilize scattered key information, a problem\nknown as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to\nrestructure the content for LLM to recognize the key information. We propose\n$\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that\noperates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a\nsingle decoder-only LLM to adaptively extract query-relevant contents verbatim\nalong with the necessary context, and section them based on their\ninterconnectedness, thereby highlights information distinction, and aligns\ndownstream LLMs with the original context effectively. Experiments show that a\ntrained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to\ndownstream LLM in improving answer accuracy, and outperforms other\nstate-of-the-art advanced RAG and concurrent compressing approaches in various\nsingle-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5%\ntokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared\nto the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that\ncan be seamlessly integrated with RAG systems, facilitating its application\nacross diverse open-source frameworks."}
{"id": "2504.20469", "pdf": "https://arxiv.org/pdf/2504.20469.pdf", "abs": "https://arxiv.org/abs/2504.20469", "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "authors": ["Enfa Fane", "Mihai Surdeanu", "Eduardo Blanco", "Steven R. Corman"], "categories": ["cs.CL", "cs.CY", "I.2.7"], "comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)", "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing."}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094.pdf", "abs": "https://arxiv.org/abs/2504.20094", "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions."}
{"id": "2504.20484", "pdf": "https://arxiv.org/pdf/2504.20484.pdf", "abs": "https://arxiv.org/abs/2504.20484", "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "authors": ["Linjuan Wu", "Haoran Wei", "Huan Lin", "Tianhao Li", "Baosong Yang", "Weiming Lu"], "categories": ["cs.CL"], "comment": "12 pages, 6 figures, Under Review", "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation."}
{"id": "2504.20114", "pdf": "https://arxiv.org/pdf/2504.20114.pdf", "abs": "https://arxiv.org/abs/2504.20114", "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering", "authors": ["Zhonghao Li", "Kunpeng Zhang", "Jinghuai Ou", "Shuliang Liu", "Xuming Hu"], "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop."}
{"id": "2504.20500", "pdf": "https://arxiv.org/pdf/2504.20500.pdf", "abs": "https://arxiv.org/abs/2504.20500", "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation", "authors": ["Huimin Lu", "Masaru Isonuma", "Junichiro Mori", "Ichiro Sakata"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICLR 2025 (poster)", "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs."}
{"id": "2504.20196", "pdf": "https://arxiv.org/pdf/2504.20196.pdf", "abs": "https://arxiv.org/abs/2504.20196", "title": "Prompting LLMs for Code Editing: Struggles and Remedies", "authors": ["Daye Nam", "Ahmed Omran", "Ambar Murillo", "Saksham Thakur", "Abner Araujo", "Marcel Blistein", "Alexander Frömmgen", "Vincent Hellendoorn", "Satish Chandra"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming software engineering,\nwith coding assistants embedded in an IDE becoming increasingly prevalent.\nWhile research has focused on improving the tools and understanding developer\nperceptions, a critical gap exists in understanding how developers actually use\nthese tools in their daily workflows, and, crucially, where they struggle. This\npaper addresses part of this gap through a multi-phased investigation of\ndeveloper interactions with an LLM-powered code editing and transformation\nfeature, Transform Code, in an IDE widely used at Google. First, we analyze\ntelemetry logs of the feature usage, revealing that frequent re-prompting can\nbe an indicator of developer struggles with using Transform Code. Second, we\nconduct a qualitative analysis of unsatisfactory requests, identifying five key\ncategories of information often missing from developer prompts. Finally, based\non these findings, we propose and evaluate a tool, AutoPrompter, for\nautomatically improving prompts by inferring missing information from the\nsurrounding code context, leading to a 27% improvement in edit correctness on\nour test set."}
{"id": "2504.20547", "pdf": "https://arxiv.org/pdf/2504.20547.pdf", "abs": "https://arxiv.org/abs/2504.20547", "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "authors": ["Jesus Lovon", "Thouria Ben-Haddi", "Jules Di Scala", "Jose G. Moreno", "Lynda Tamine"], "categories": ["cs.CL"], "comment": null, "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement."}
{"id": "2504.20215", "pdf": "https://arxiv.org/pdf/2504.20215.pdf", "abs": "https://arxiv.org/abs/2504.20215", "title": "Exploring AI-powered Digital Innovations from A Transnational Governance Perspective: Implications for Market Acceptance and Digital Accountability Accountability", "authors": ["Claire Li", "David Peter Wallis Freeborn"], "categories": ["cs.CY", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This study explores the application of the Technology Acceptance Model (TAM)\nto AI-powered digital innovations within a transnational governance framework.\nBy integrating Latourian actor-network theory (ANT), this study examines how\ninstitutional motivations, regulatory compliance, and ethical and cultural\nacceptance drive organisations to develop and adopt AI innovations, enhancing\ntheir market acceptance and transnational accountability. We extend the TAM\nframework by incorporating regulatory, ethical, and socio-technical\nconsiderations as key social pressures shaping AI adoption. Recognizing that AI\nis embedded within complex actor-networks, we argue that accountability is\nco-constructed among organisations, regulators, and societal actors rather than\nbeing confined to individual developers or adopters. To address these\nchallenges, we propose two key solutions: (1) internal resource\nreconfiguration, where organisations restructure their governance and\ncompliance mechanisms to align with global standards; and (2) reshaping\norganisational boundaries through actor-network management, fostering\nengagement with external stakeholders, regulatory bodies, and transnational\ngovernance institutions. These approaches allow organisations to enhance AI\naccountability, foster ethical and regulatory alignment, and improve market\nacceptance on a global scale."}
{"id": "2504.20552", "pdf": "https://arxiv.org/pdf/2504.20552.pdf", "abs": "https://arxiv.org/abs/2504.20552", "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters", "authors": ["Baz Roland", "Kristina Malyseva", "Anna Pappa", "Tristan Cazenave"], "categories": ["cs.CL"], "comment": null, "summary": "This project introduces BrAIcht, an AI conversational agent that creates\ndialogues in the distinctive style of the famous German playwright Bertolt\nBrecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7\nbillion parameters and a modified version of the base Llama2 suitable for\nGerman language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of\nother German plays that are stylistically similar to Bertolt Brecht are used to\nform a more di-erse dataset. Due to the limited memory capacity, a\nparameterefficient fine-tuning technique called QLoRA is implemented to train\nthe large language model. The results, based on BLEU score and perplexity, show\nvery promising performance of BrAIcht in generating dialogues in the style of\nBertolt Brecht."}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294.pdf", "abs": "https://arxiv.org/abs/2504.20294", "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets."}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581.pdf", "abs": "https://arxiv.org/abs/2504.20581", "title": "ClonEval: An Open Voice Cloning Benchmark", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard."}
{"id": "2504.20329", "pdf": "https://arxiv.org/pdf/2504.20329.pdf", "abs": "https://arxiv.org/abs/2504.20329", "title": "AI in Software Engineering: Perceived Roles and Their Impact on Adoption", "authors": ["Ilya Zakharov", "Ekaterina Koshchenko", "Agnia Sergeyuk"], "categories": ["cs.SE", "cs.HC"], "comment": "Accepted at 33rd ACM International Conference on the Foundations of\n  Software Engineering (FSE Companion '25), June 23-28, 2025, Trondheim, Norway", "summary": "This paper investigates how developers conceptualize AI-powered Development\nTools and how these role attributions influence technology acceptance. Through\nqualitative analysis of 38 interviews and a quantitative survey with 102\nparticipants, we identify two primary Mental Models: AI as an inanimate tool\nand AI as a human-like teammate. Factor analysis further groups AI roles into\nSupport Roles (e.g., assistant, reference guide) and Expert Roles (e.g.,\nadvisor, problem solver). We find that assigning multiple roles to AI\ncorrelates positively with Perceived Usefulness and Perceived Ease of Use,\nindicating that diverse conceptualizations enhance AI adoption. These insights\nsuggest that AI4SE tools should accommodate varying user expectations through\nadaptive design strategies that align with different Mental Models."}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605.pdf", "abs": "https://arxiv.org/abs/2504.20605", "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models."}
{"id": "2504.20340", "pdf": "https://arxiv.org/pdf/2504.20340.pdf", "abs": "https://arxiv.org/abs/2504.20340", "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks", "authors": ["Khoi Trinh", "Scott Seidenberger", "Raveen Wijewickrama", "Murtuza Jadliwala", "Anindya Maiti"], "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "With AI-generated content becoming ubiquitous across the web, social media,\nand other digital platforms, it is vital to examine how such content are\ninspired and generated. The creation of AI-generated images often involves\nrefining the input prompt iteratively to achieve desired visual outcomes. This\nstudy focuses on the relatively underexplored concept of image regeneration\nusing AI, in which a human operator attempts to closely recreate a specific\ntarget image by iteratively refining their prompt. Image regeneration is\ndistinct from normal image generation, which lacks any predefined visual\nreference. A separate challenge lies in determining whether existing image\nsimilarity metrics (ISMs) can provide reliable, objective feedback in iterative\nworkflows, given that we do not fully understand if subjective human judgments\nof similarity align with these metrics. Consequently, we must first validate\ntheir alignment with human perception before assessing their potential as a\nfeedback mechanism in the iterative prompt refinement process. To address these\nresearch gaps, we present a structured user study evaluating how iterative\nprompt refinement affects the similarity of regenerated images relative to\ntheir targets, while also examining whether ISMs capture the same improvements\nperceived by human observers. Our findings suggest that incremental prompt\nadjustments substantially improve alignment, verified through both subjective\nevaluations and quantitative measures, underscoring the broader potential of\niterative workflows to enhance generative AI content creation across various\napplication domains."}
{"id": "2504.20609", "pdf": "https://arxiv.org/pdf/2504.20609.pdf", "abs": "https://arxiv.org/abs/2504.20609", "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks", "authors": ["Xinyu Yao", "Mengdi Wang", "Bo Chen", "Xiaobing Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing."}
{"id": "2504.20442", "pdf": "https://arxiv.org/pdf/2504.20442.pdf", "abs": "https://arxiv.org/abs/2504.20442", "title": "Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework", "authors": ["Yuchen Wang", "Pengfei Jia", "Zhitao Shu", "Keyan Liu", "Abdul Rashid Mohamed Shariff"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "With the intensification of global climate change, accurate prediction of\nweather indicators is of great significance in disaster prevention and\nmitigation, agricultural production, and transportation. Precipitation, as one\nof the key meteorological indicators, plays a crucial role in water resource\nmanagement, agricultural production, and urban flood control. This study\nproposes a multidimensional precipitation index prediction model based on a\nCNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation\nforecasts. The dataset is sourced from Pune, Maharashtra, India, covering\nmonthly mean precipitation data from 1972 to 2002. This dataset includes nearly\n31 years (1972-2002) of monthly average precipitation, reflecting the long-term\nfluctuations and seasonal variations of precipitation in the region. By\nanalyzing these time series data, the CNN-LSTM model effectively captures local\nfeatures and long-term dependencies. Experimental results show that the model\nachieves a root mean square error (RMSE) of 6.752, which demonstrates a\nsignificant advantage over traditional time series prediction methods in terms\nof prediction accuracy and generalization ability. Furthermore, this study\nprovides new research ideas for precipitation prediction. However, the model\nrequires high computational resources when dealing with large-scale datasets,\nand its predictive ability for multidimensional precipitation data still needs\nimprovement. Future research could extend the model to support and predict\nmultidimensional precipitation data, thereby promoting the development of more\naccurate and efficient meteorological prediction technologies."}
{"id": "2504.20643", "pdf": "https://arxiv.org/pdf/2504.20643.pdf", "abs": "https://arxiv.org/abs/2504.20643", "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations", "authors": ["Moran Mizrahi", "Chen Shani", "Gabriel Stanovsky", "Dan Jurafsky", "Dafna Shahaf"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI."}
{"id": "2504.20519", "pdf": "https://arxiv.org/pdf/2504.20519.pdf", "abs": "https://arxiv.org/abs/2504.20519", "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging", "authors": ["Neil K. R. Sehgal", "Sunny Rai", "Manuel Tonneau", "Anish K. Agarwal", "Joseph Cappella", "Melanie Kornides", "Lyle Ungar", "Alison Buttenheim", "Sharath Chandra Guntuku"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies."}
{"id": "2504.20668", "pdf": "https://arxiv.org/pdf/2504.20668.pdf", "abs": "https://arxiv.org/abs/2504.20668", "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages", "authors": ["Ivan Vykopal", "Martin Hyben", "Robert Moro", "Michal Gregor", "Jakub Simko"], "categories": ["cs.CL"], "comment": null, "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process."}
{"id": "2504.20656", "pdf": "https://arxiv.org/pdf/2504.20656.pdf", "abs": "https://arxiv.org/abs/2504.20656", "title": "Federated learning, ethics, and the double black box problem in medical AI", "authors": ["Joshua Hatherley", "Anders Søgaard", "Angela Ballantyne", "Ruben Pauwels"], "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Federated learning (FL) is a machine learning approach that allows multiple\ndevices or institutions to collaboratively train a model without sharing their\nlocal data with a third-party. FL is considered a promising way to address\npatient privacy concerns in medical artificial intelligence. The ethical risks\nof medical FL systems themselves, however, have thus far been underexamined.\nThis paper aims to address this gap. We argue that medical FL presents a new\nvariety of opacity -- federation opacity -- that, in turn, generates a\ndistinctive double black box problem in healthcare AI. We highlight several\ninstances in which the anticipated benefits of medical FL may be exaggerated,\nand conclude by highlighting key challenges that must be overcome to make FL\nethically feasible in medicine."}
{"id": "2504.20678", "pdf": "https://arxiv.org/pdf/2504.20678.pdf", "abs": "https://arxiv.org/abs/2504.20678", "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)", "authors": ["Yaroslav Getman", "Tamás Grósz", "Mikko Kurimo", "Giampiero Salvi"], "categories": ["cs.CL", "eess.AS"], "comment": "First draft of the baseline paper for the NOCASA competition\n  (https://teflon.aalto.fi/nocasa-2025/), 5 pages", "summary": "This paper presents the \"Non-native Children's Automatic Speech Assessment\"\n(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA\nchallenges participants to develop new systems that can assess single-word\npronunciations of young second language (L2) learners as part of a gamified\npronunciation training app. To achieve this, several issues must be addressed,\nmost notably the limited nature of available training data and the highly\nunbalanced distribution among the pronunciation level categories. To expedite\nthe development, we provide a pseudo-anonymized training data (TeflonNorL2),\ncontaining 10,334 recordings from 44 speakers attempting to pronounce 205\ndistinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that\nshould be given in the game). In addition to the data, two already trained\nsystems are released as official baselines: an SVM classifier trained on the\nComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter\nachieves the best performance on the challenge test set, with an unweighted\naverage recall (UAR) of 36.37%."}
{"id": "2504.20685", "pdf": "https://arxiv.org/pdf/2504.20685.pdf", "abs": "https://arxiv.org/abs/2504.20685", "title": "Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion", "authors": ["Zesheng Wang", "Alexandre Bruckert", "Patrick Le Callet", "Guangtao Zhai"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating realistic listener facial motions in dyadic conversations remains\nchallenging due to the high-dimensional action space and temporal dependency\nrequirements. Existing approaches usually consider extracting 3D Morphable\nModel (3DMM) coefficients and modeling in the 3DMM space. However, this makes\nthe computational speed of the 3DMM a bottleneck, making it difficult to\nachieve real-time interactive responses. To tackle this problem, we propose\nFacial Action Diffusion (FAD), which introduces the diffusion methods from the\nfield of image generation to achieve efficient facial action generation. We\nfurther build the Efficient Listener Network (ELNet) specially designed to\naccommodate both the visual and audio information of the speaker as input.\nConsidering of FAD and ELNet, the proposed method learns effective listener\nfacial motion representations and leads to improvements of performance over the\nstate-of-the-art methods while reducing 99% computational time."}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679.pdf", "abs": "https://arxiv.org/abs/2504.20679", "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science."}
{"id": "2504.20761", "pdf": "https://arxiv.org/pdf/2504.20761.pdf", "abs": "https://arxiv.org/abs/2504.20761", "title": "Confidence-based Intent Prediction for Teleoperation in Bimanual Robotic Suturing", "authors": ["Zhaoyang Jacopo Hu", "Haozheng Xu", "Sion Kim", "Yanan Li", "Ferdinando Rodriguez y Baena", "Etienne Burdet"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Robotic-assisted procedures offer enhanced precision, but while fully\nautonomous systems are limited in task knowledge, difficulties in modeling\nunstructured environments, and generalisation abilities, fully manual\nteleoperated systems also face challenges such as delay, stability, and reduced\nsensory information. To address these, we developed an interactive control\nstrategy that assists the human operator by predicting their motion plan at\nboth high and low levels. At the high level, a surgeme recognition system is\nemployed through a Transformer-based real-time gesture classification model to\ndynamically adapt to the operator's actions, while at the low level, a\nConfidence-based Intention Assimilation Controller adjusts robot actions based\non user intent and shared control paradigms. The system is built around a\nrobotic suturing task, supported by sensors that capture the kinematics of the\nrobot and task dynamics. Experiments across users with varying skill levels\ndemonstrated the effectiveness of the proposed approach, showing statistically\nsignificant improvements in task completion time and user satisfaction compared\nto traditional teleoperation."}
{"id": "2504.20699", "pdf": "https://arxiv.org/pdf/2504.20699.pdf", "abs": "https://arxiv.org/abs/2504.20699", "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?", "authors": ["Evangelia Gogoulou", "Shorouq Zahra", "Liane Guillou", "Luise Dürlich", "Joakim Nivre"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task."}
{"id": "2504.20792", "pdf": "https://arxiv.org/pdf/2504.20792.pdf", "abs": "https://arxiv.org/abs/2504.20792", "title": "RecGaze: The First Eye Tracking and User Interaction Dataset for Carousel Interfaces", "authors": ["Santiago de Leon-Martinez", "Jingwei Kang", "Robert Moro", "Maarten de Rijke", "Branislav Kveton", "Harrie Oosterhuis", "Maria Bielikova"], "categories": ["cs.IR", "cs.HC"], "comment": "Accepted to Resource & Reproducibility Track SIGIR '25", "summary": "Carousel interfaces are widely used in e-commerce and streaming services, but\nlittle research has been devoted to them. Previous studies of interfaces for\npresenting search and recommendation results have focused on single ranked\nlists, but it appears their results cannot be extrapolated to carousels due to\nthe added complexity. Eye tracking is a highly informative approach to\nunderstanding how users click, yet there are no eye tracking studies concerning\ncarousels. There are very few interaction datasets on recommenders with\ncarousel interfaces and none that contain gaze data.\n  We introduce the RecGaze dataset: the first comprehensive feedback dataset on\ncarousels that includes eye tracking results, clicks, cursor movements, and\nselection explanations. The dataset comprises of interactions from 3 movie\nselection tasks with 40 different carousel interfaces per user. In total, 87\nusers and 3,477 interactions are logged. In addition to the dataset, its\ndescription and possible use cases, we provide results of a survey on carousel\ndesign and the first analysis of gaze data on carousels, which reveals a golden\ntriangle or F-pattern browsing behavior.\n  Our work seeks to advance the field of carousel interfaces by providing the\nfirst dataset with eye tracking results on carousels. In this manner, we\nprovide and encourage an empirical understanding of interactions with carousel\ninterfaces, for building better recommender systems through gaze information,\nand also encourage the development of gaze-based recommenders."}
{"id": "2504.20703", "pdf": "https://arxiv.org/pdf/2504.20703.pdf", "abs": "https://arxiv.org/abs/2504.20703", "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification", "authors": ["Foteini Papadopoulou", "Osman Mutlu", "Neris Özen", "Bas H. M. van der Velden", "Iris Hendrickx", "Ali Hürriyetoğlu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system developed for the SemEval-2025 Task 9: The\nFood Hazard Detection Challenge. The shared task's objective is to evaluate\nexplainable classification systems for classifying hazards and products in two\nlevels of granularity from food recall incident reports. In this work, we\npropose text augmentation techniques as a way to improve poor performance on\nminority classes and compare their effect for each category on various\ntransformer and machine learning models. We explore three word-level data\naugmentation techniques, namely synonym replacement, random word swapping, and\ncontextual word insertion. The results show that transformer models tend to\nhave a better overall performance. None of the three augmentation techniques\nconsistently improved overall performance for classifying hazards and products.\nWe observed a statistically significant improvement (P < 0.05) in the\nfine-grained categories when using the BERT model to compare the baseline with\neach augmented model. Compared to the baseline, the contextual words insertion\naugmentation improved the accuracy of predictions for the minority hazard\nclasses by 6%. This suggests that targeted augmentation of minority classes can\nimprove the performance of transformer models."}
{"id": "2504.20903", "pdf": "https://arxiv.org/pdf/2504.20903.pdf", "abs": "https://arxiv.org/abs/2504.20903", "title": "Modeling AI-Human Collaboration as a Multi-Agent Adaptation", "authors": ["Prothit Sen", "Sai Mihir Jakkaraju"], "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": "Manuscript under review for the Special Issue: 'Can AI Do Strategy?'\n  at Strategy Science (May 1, 2025)", "summary": "We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings."}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708.pdf", "abs": "https://arxiv.org/abs/2504.20708", "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner."}
{"id": "2504.20910", "pdf": "https://arxiv.org/pdf/2504.20910.pdf", "abs": "https://arxiv.org/abs/2504.20910", "title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines", "authors": ["Sachin R. Pendse", "Darren Gergle", "Rachel Kornfield", "Jonah Meyerhoff", "David Mohr", "Jina Suh", "Annie Wescott", "Casey Williams", "Jessica Schleider"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT 2025)", "summary": "Red-teaming is a core part of the infrastructure that ensures that AI models\ndo not produce harmful content. Unlike past technologies, the black box nature\nof generative AI systems necessitates a uniquely interactional mode of testing,\none in which individuals on red teams actively interact with the system,\nleveraging natural language to simulate malicious actors and solicit harmful\noutputs. This interactional labor done by red teams can result in mental health\nharms that are uniquely tied to the adversarial engagement strategies necessary\nto effectively red team. The importance of ensuring that generative AI models\ndo not propagate societal or individual harm is widely recognized -- one less\nvisible foundation of end-to-end AI safety is also the protection of the mental\nhealth and wellbeing of those who work to keep model outputs safe. In this\npaper, we argue that the unmet mental health needs of AI red-teamers is a\ncritical workplace safety concern. Through analyzing the unique mental health\nimpacts associated with the labor done by red teams, we propose potential\nindividual and organizational strategies that could be used to meet these\nneeds, and safeguard the mental health of red-teamers. We develop our proposed\nstrategies through drawing parallels between common red-teaming practices and\ninteractional labor common to other professions (including actors, mental\nhealth professionals, conflict photographers, and content moderators),\ndescribing how individuals and organizations within these professional spaces\nsafeguard their mental health given similar psychological demands. Drawing on\nthese protective practices, we describe how safeguards could be adapted for the\ndistinct mental health challenges experienced by red teaming organizations as\nthey mitigate emerging technological risks on the new digital frontlines."}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734.pdf", "abs": "https://arxiv.org/abs/2504.20734", "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines."}
{"id": "2312.04470", "pdf": "https://arxiv.org/pdf/2312.04470.pdf", "abs": "https://arxiv.org/abs/2312.04470", "title": "GaitGuard: Towards Private Gait in Mixed Reality", "authors": ["Diana Romero", "Ruchi Jagdish Patel", "Athina Markopoulou", "Salma Elmalaki"], "categories": ["cs.HC", "cs.CR"], "comment": "22 pages, 12 figures, added scalability experiment", "summary": "Augmented/Mixed Reality (AR/MR) technologies usher in a new era of immersive,\ncollective experiences, differentiating them from traditional mobile systems.\nAs these technologies evolve, prioritizing privacy and security is critical.\nThis paper focuses on gait privacy, where gait, the way a person walks, can\nreveal sensitive information such as age, ethnicity, or disorders. We present\nGaitGuard, a real-time system that protects gait privacy against video-based\ngait extraction attacks in MR environments. GaitGuard leverages a\nmulti-threaded framework to efficiently process video frames, incorporating\ndedicated modules for stream capture, body detection and tracking, and privacy\nleak mitigation. We compare and combine multiple mitigation techniques,\noffering guidance to navigate the privacy-utility tradeoff. Through extensive\nexperiments covering 248 settings across mitigation regions, types, and tunable\nparameters, we assess the impact of these techniques on privacy, video quality,\nand system performance. GaitGuard reduces the confidence of video-based gait\nextraction attacks by introducing a substantial distribution shift\n(Jensen-Shannon Divergence of 0.63, indicating highly altered gait features)\nand a decrease in identification risks by up to 68%, while maintaining 29 FPS\nand preserving video clarity. GaitGuard provides a practical real-time solution\nfor privacy-preserving MR applications without affecting the MR user experience\nbased on 20 subjective user surveys."}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752.pdf", "abs": "https://arxiv.org/abs/2504.20752", "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."}
{"id": "2405.03844", "pdf": "https://arxiv.org/pdf/2405.03844.pdf", "abs": "https://arxiv.org/abs/2405.03844", "title": "Perception in Pixels: Effects of Avatar Representation in Video-Mediated Collaborative Interactions", "authors": ["Pitch Sinlapanuntakul", "Mark Zachry"], "categories": ["cs.HC", "H.5.1; H.5.3; J.4"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Interactive collaborative video is now a common part of remote work. Despite\nits prevalence, traditional video conferencing can be challenging, sometimes\ncausing social discomforts that undermine process and outcomes. Avatars on 2D\ndisplays offer a promising alternative for enhancing self-representation,\nbridging the gap between virtual reality (VR) and traditional non-immersive\nvideo. However, the use of such avatars in activity-oriented group settings\nremains underexplored. To address this gap, we conducted a mixed-methods,\nwithin-subject study investigating the impacts of avatar-mediated versus\ntraditional video representations on collaboration satisfaction and\nself-esteem. 32 participants (8 groups of 4 with pre-established relationships)\nengaged in goal-directed activities, followed by group interviews. Results\nindicate that avatars significantly enhance self-esteem and collaboration\nsatisfaction, while qualitative insights reveal the dynamic perceptions and\nexperiences of avatars, including benefits, challenges, and factors influencing\nadoption likelihood. Our study contributes to understanding and implications of\navatars as a camera-driven representation in video-mediated collaborative\ninteractions."}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769.pdf", "abs": "https://arxiv.org/abs/2504.20769", "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%."}
{"id": "2409.14634", "pdf": "https://arxiv.org/pdf/2409.14634.pdf", "abs": "https://arxiv.org/abs/2409.14634", "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination", "authors": ["Marissa Radensky", "Simra Shahid", "Raymond Fok", "Pao Siangliulue", "Tom Hope", "Daniel S. Weld"], "categories": ["cs.HC", "cs.AI", "H.5.2, I.2"], "comment": "Updated with new and improved user study", "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas, and facet-based ideation is an established\nframework for idea generation. To see how large language models (LLMs) might\nassist in this process, we contribute a novel mixed-initiative ideation tool\ncalled Scideator. Starting from a user-provided set of scientific papers,\nScideator extracts key facets -- purposes, mechanisms, and evaluations -- from\nthese and related papers, allowing users to explore the idea space by\ninteractively recombining facets to synthesize inventive ideas. Scideator also\nhelps users gauge idea originality by searching the literature for overlaps,\nassessing idea novelty and providing explanations. To support these tasks,\nScideator introduces three LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty\nChecker. In a within-subjects user study (N=22) with computer-science\nresearchers comparing Scideator to a strong baseline, our tool provided\nsignificantly more creativity support, particularly with respect to\nexploration, which participants considered the most important factor for idea\ngeneration."}
{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771.pdf", "abs": "https://arxiv.org/abs/2504.20771", "title": "Turing Machine Evaluation for Large Language Model", "authors": ["Haitao Wu", "Zongbo Han", "Huaxi Huang", "Changqing Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench."}
{"id": "2409.16732", "pdf": "https://arxiv.org/pdf/2409.16732.pdf", "abs": "https://arxiv.org/abs/2409.16732", "title": "Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions", "authors": ["Ananya Bhattacharjee", "Sarah Yi Xu", "Pranav Rao", "Yuchen Zeng", "Jonah Meyerhoff", "Syed Ishtiaque Ahmed", "David C Mohr", "Michael Liut", "Alex Mariakakis", "Rachel Kornfield", "Joseph Jay Williams"], "categories": ["cs.HC"], "comment": null, "summary": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that personalized\nLLM-enhanced stories were perceived to be better than human-written ones in\nconveying key takeaways, promoting reflection, and reducing belief in negative\nthoughts. These stories were not only seen as more relatable but also similarly\nauthentic to human-written ones, highlighting the potential of LLMs in helping\nyoung adults manage their struggles. The findings of this work provide crucial\ndesign considerations for future narrative-based digital mental health\ninterventions, such as the need to maintain relatability without veering into\nimplausibility and refining the wording and tone of AI-enhanced content."}
{"id": "2504.20839", "pdf": "https://arxiv.org/pdf/2504.20839.pdf", "abs": "https://arxiv.org/abs/2504.20839", "title": "Universal language model with the intervention of quantum theory", "authors": ["D. -F. Qin"], "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "This paper examines language modeling based on the theory of quantum\nmechanics. It focuses on the introduction of quantum mechanics into the\nsymbol-meaning pairs of language in order to build a representation model of\nnatural language. At the same time, it is realized that word embedding, which\nis widely used as a basic technique for statistical language modeling, can be\nexplained and improved by the mathematical framework of quantum mechanics. On\nthis basis, this paper continues to try to use quantum statistics and other\nrelated theories to study the mathematical representation, natural evolution\nand statistical properties of natural language. It is also assumed that the\nsource of such quantum properties is the physicality of information. The\nfeasibility of using quantum theory to model natural language is pointed out\nthrough the construction of a experimental code. The paper discusses, in terms\nof applications, the possible help of the theory in constructing generative\nmodels that are popular nowadays. A preliminary discussion of future\napplications of the theory to quantum computers is also presented."}
{"id": "2501.01397", "pdf": "https://arxiv.org/pdf/2501.01397.pdf", "abs": "https://arxiv.org/abs/2501.01397", "title": "WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI", "authors": ["Wesley Hanwen Deng", "Wang Claire", "Howard Ziyu Han", "Jason I. Hong", "Kenneth Holstein", "Motahhare Eslami"], "categories": ["cs.HC"], "comment": null, "summary": "There has been growing interest from both practitioners and researchers in\nengaging end users in AI auditing, to draw upon users' unique knowledge and\nlived experiences. However, we know little about how to effectively scaffold\nend users in auditing in ways that can generate actionable insights for AI\npractitioners. Through formative studies with both users and AI practitioners,\nwe first identified a set of design goals to support user-engaged AI auditing.\nWe then developed WeAudit, a workflow and system that supports end users in\nauditing AI both individually and collectively. We evaluated WeAudit through a\nthree-week user study with user auditors and interviews with industry\nGenerative AI practitioners. Our findings offer insights into how WeAudit\nsupports users in noticing and reflecting upon potential AI harms and in\narticulating their findings in ways that industry practitioners can act upon.\nBased on our observations and feedback from both users and practitioners, we\nidentify several opportunities to better support user engagement in AI auditing\nprocesses. We discuss implications for future research to support effective and\nresponsible user engagement in AI auditing and red-teaming."}
{"id": "2504.20849", "pdf": "https://arxiv.org/pdf/2504.20849.pdf", "abs": "https://arxiv.org/abs/2504.20849", "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry", "authors": ["Anum Afzal", "Alexandre Mercier", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent."}
{"id": "2504.17960", "pdf": "https://arxiv.org/pdf/2504.17960.pdf", "abs": "https://arxiv.org/abs/2504.17960", "title": "VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics", "authors": ["Kazi Shahrukh Omar", "Shuaijie Wang", "Ridhuparan Kungumaraju", "Tanvi Bhatt", "Fabio Miranda"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted for publication in the IEEE Transactions on Visualization\n  and Computer Graphics. VIGMA is available at https://github.com/komar41/VIGMA", "summary": "Gait disorders are commonly observed in older adults, who frequently\nexperience various issues related to walking. Additionally, researchers and\nclinicians extensively investigate mobility related to gait in typically and\natypically developing children, athletes, and individuals with orthopedic and\nneurological disorders. Effective gait analysis enables the understanding of\nthe causal mechanisms of mobility and balance control of patients, the\ndevelopment of tailored treatment plans to improve mobility, the reduction of\nfall risk, and the tracking of rehabilitation progress. However, analyzing gait\ndata is a complex task due to the multivariate nature of the data, the large\nvolume of information to be interpreted, and the technical skills required.\nExisting tools for gait analysis are often limited to specific patient groups\n(e.g., cerebral palsy), only handle a specific subset of tasks in the entire\nworkflow, and are not openly accessible. To address these shortcomings, we\nconducted a requirements assessment with gait practitioners (e.g., researchers,\nclinicians) via surveys and identified key components of the workflow,\nincluding (1) data processing and (2) data analysis and visualization. Based on\nthe findings, we designed VIGMA, an open-access visual analytics framework\nintegrated with computational notebooks and a Python library, to meet the\nidentified requirements. Notably, the framework supports analytical\ncapabilities for assessing disease progression and for comparing multiple\npatient groups. We validated the framework through usage scenarios with experts\nspecializing in gait and mobility rehabilitation. VIGMA is available at\nhttps://github.com/komar41/VIGMA."}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922.pdf", "abs": "https://arxiv.org/abs/2504.20922", "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs."}
{"id": "2504.19423", "pdf": "https://arxiv.org/pdf/2504.19423.pdf", "abs": "https://arxiv.org/abs/2504.19423", "title": "MER 2025: When Affective Computing Meets Large Language Models", "authors": ["Zheng Lian", "Rui Liu", "Kele Xu", "Bin Liu", "Xuefei Liu", "Yazhou Zhang", "Xin Liu", "Yong Li", "Zebang Cheng", "Haolin Zuo", "Ziyang Ma", "Xiaojiang Peng", "Xie Chen", "Ya Li", "Erik Cambria", "Guoying Zhao", "Björn W. Schuller", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub."}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946.pdf", "abs": "https://arxiv.org/abs/2504.20946", "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models", "authors": ["Tyler McDonald", "Ali Emami"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications."}
{"id": "2504.20035", "pdf": "https://arxiv.org/pdf/2504.20035.pdf", "abs": "https://arxiv.org/abs/2504.20035", "title": "Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality", "authors": ["Brandon Woodard", "Melvin He", "Mose Sakashita", "Jing Qian", "Zainab Iftikhar", "Joseph LaViola Jr"], "categories": ["cs.HC", "H.5.2; H.5.1; H.5.0; H.1.2"], "comment": null, "summary": "Off-the-shelf smartphone-based AR systems typically use a single front-facing\nor rear-facing camera, which restricts user interactions to a narrow field of\nview and small screen size, thus reducing their practicality. We present\nCam-2-Cam, an interaction concept implemented in three smartphone-based AR\napplications with interactions that span both cameras. Results from our\nqualitative analysis conducted on 30 participants presented two major design\nlessons that explore the interaction space of smartphone AR while maintaining\ncritical AR interface attributes like embodiment and immersion: (1) Balancing\nContextual Relevance and Feedback Quality serves to outline a delicate balance\nbetween implementing familiar interactions people do in the real world and the\nquality of multimodal AR responses and (2) Preventing Disorientation using\nSimultaneous Capture and Alternating Cameras which details how to prevent\ndisorientation during AR interactions using the two distinct camera techniques\nwe implemented in the paper. Additionally, we consider observed user\nassumptions or natural tendencies to inform future implementations of\ndual-camera setups for smartphone-based AR. We envision our design lessons as\nan initial pioneering step toward expanding the interaction space of\nsmartphone-based AR, potentially driving broader adoption and overcoming\nlimitations of single-camera AR."}
{"id": "2504.20951", "pdf": "https://arxiv.org/pdf/2504.20951.pdf", "abs": "https://arxiv.org/abs/2504.20951", "title": "Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models", "authors": ["Maryna Vyshnyvetska"], "categories": ["cs.CL"], "comment": "12 pages, 1 figure", "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity."}
{"id": "2403.14562", "pdf": "https://arxiv.org/pdf/2403.14562.pdf", "abs": "https://arxiv.org/abs/2403.14562", "title": "Agentic AI: The Era of Semantic Decoding", "authors": ["Maxime Peyrard", "Martin Josifoski", "Robert West"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": "25 pages, 3 figures", "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation."}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964.pdf", "abs": "https://arxiv.org/abs/2504.20964", "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench."}
{"id": "2408.07461", "pdf": "https://arxiv.org/pdf/2408.07461.pdf", "abs": "https://arxiv.org/abs/2408.07461", "title": "Problem Solving Through Human-AI Preference-Based Cooperation", "authors": ["Subhabrata Dutta", "Timo Kaufmann", "Goran Glavaš", "Ivan Habernal", "Kristian Kersting", "Frauke Kreuter", "Mira Mezini", "Iryna Gurevych", "Eyke Hüllermeier", "Hinrich Schuetze"], "categories": ["cs.AI", "cs.HC"], "comment": "20 pages", "summary": "While there is a widespread belief that artificial general intelligence (AGI)\n-- or even superhuman AI -- is imminent, complex problems in expert domains are\nfar from being solved. We argue that such problems require human-AI cooperation\nand that the current state of the art in generative AI is unable to play the\nrole of a reliable partner due to a multitude of shortcomings, including\ndifficulty to keep track of a complex solution artifact (e.g., a software\nprogram), limited support for versatile human preference expression and lack of\nadapting to human preference in an interactive setting. To address these\nchallenges, we propose HAICo2, a novel human-AI co-construction framework. We\ntake first steps towards a formalization of HAICo2 and discuss the difficult\nopen research problems that it faces."}
{"id": "2504.20972", "pdf": "https://arxiv.org/pdf/2504.20972.pdf", "abs": "https://arxiv.org/abs/2504.20972", "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap", "authors": ["Yifan Wei", "Xiaoyan Yu", "Ran Song", "Hao Peng", "Angsheng Li"], "categories": ["cs.CL"], "comment": "The CR version will be updated subsequently", "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark."}
{"id": "2410.05452", "pdf": "https://arxiv.org/pdf/2410.05452.pdf", "abs": "https://arxiv.org/abs/2410.05452", "title": "WearableMil: An End-to-End Framework for Military Activity Recognition and Performance Monitoring", "authors": ["Barak Gahtan", "Shany Funk", "Einat Kodesh", "Itay Ketko", "Tsvi Kuflik", "Alex M. Bronstein"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Musculoskeletal injuries during military training significantly impact\nreadiness, making prevention through activity monitoring crucial. While Human\nActivity Recognition (HAR) using wearable devices offers promising solutions,\nit faces challenges in processing continuous data streams and recognizing\ndiverse activities without predefined sessions. This paper introduces an\nend-to-end framework for preprocessing, analyzing, and recognizing activities\nfrom wearable data in military training contexts. Using data from 135 soldiers\nwearing \\textit{Garmin--55} smartwatches over six months with over 15 million\nminutes. We develop a hierarchical deep learning approach that achieves 93.8%\naccuracy in temporal splits and 83.8% in cross-user evaluation. Our framework\naddresses missing data through physiologically-informed methods, reducing\nunknown sleep states from 40.38% to 3.66%. We demonstrate that while longer\ntime windows (45-60 minutes) improve basic state classification, they present\ntrade-offs in detecting fine-grained activities. Additionally, we introduce an\nintuitive visualization system that enables real-time comparison of individual\nperformance against group metrics across multiple physiological indicators.\nThis approach to activity recognition and performance monitoring provides\nmilitary trainers with actionable insights for optimizing training programs and\npreventing injuries."}
{"id": "2504.20059", "pdf": "https://arxiv.org/pdf/2504.20059.pdf", "abs": "https://arxiv.org/abs/2504.20059", "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence", "authors": ["Joey Chan", "Qiao Jin", "Nicholas Wan", "Charalampos S. Floudas", "Elisabetta Xue", "Zhiyong Lu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "10 pages with 2 figures and 2 tables", "summary": "Clinical trials are crucial for assessing new treatments; however,\nrecruitment challenges - such as limited awareness, complex eligibility\ncriteria, and referral barriers - hinder their success. With the growth of\nonline platforms, patients increasingly turn to social media and health\ncommunities for support, research, and advocacy, expanding recruitment pools\nand established enrollment pathways. Recognizing this potential, we utilized\nTrialGPT, a framework that leverages a large language model (LLM) as its\nbackbone, to match 50 online patient cases (collected from published case\nreports and a social media website) to clinical trials and evaluate performance\nagainst traditional keyword-based searches. Our results show that TrialGPT\noutperforms traditional methods by 46% in identifying eligible trials, with\neach patient, on average, being eligible for around 7 trials. Additionally, our\noutreach efforts to case authors and trial organizers regarding these\npatient-trial matches yielded highly positive feedback, which we present from\nboth perspectives."}
{"id": "2501.18045", "pdf": "https://arxiv.org/pdf/2501.18045.pdf", "abs": "https://arxiv.org/abs/2501.18045", "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors", "authors": ["Myra Cheng", "Angela Y. Lee", "Kristina Rapuano", "Kate Niederhoffer", "Alex Liebscher", "Jeffrey Hancock"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025", "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development."}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073.pdf", "abs": "https://arxiv.org/abs/2504.20073", "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN."}
{"id": "2502.07608", "pdf": "https://arxiv.org/pdf/2502.07608.pdf", "abs": "https://arxiv.org/abs/2502.07608", "title": "Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting", "authors": ["Arvind Pillai", "Dimitris Spathis", "Subigya Nepal", "Amanda C Collins", "Daniel M Mackin", "Michael V Heinz", "Tess Z Griffin", "Nicholas C Jacobson", "Andrew Campbell"], "categories": ["cs.LG", "cs.HC"], "comment": "Accepted to CHIL 2025. Code and models:\n  https://github.com/arvind1609/time2lang", "summary": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks."}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084.pdf", "abs": "https://arxiv.org/abs/2504.20084", "title": "AI Awareness", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094.pdf", "abs": "https://arxiv.org/abs/2504.20094", "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions."}
{"id": "2504.20117", "pdf": "https://arxiv.org/pdf/2504.20117.pdf", "abs": "https://arxiv.org/abs/2504.20117", "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies", "authors": ["Shubham Gandhi", "Dhruv Shah", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system\nleveraging large language models (LLMs) agents to automate the codification of\nresearch methodologies described in machine learning literature. The system\nbridges the gap between high-level research concepts and their practical\nimplementation, allowing researchers auto-generating code of existing research\npapers for benchmarking or building on top-of existing methods specified in the\nliterature with availability of partial or complete starter code.\nResearchCodeAgent employs a flexible agent architecture with a comprehensive\naction suite, enabling context-aware interactions with the research\nenvironment. The system incorporates a dynamic planning mechanism, utilizing\nboth short and long-term memory to adapt its approach iteratively. We evaluate\nResearchCodeAgent on three distinct machine learning tasks with distinct task\ncomplexity and representing different parts of the ML pipeline: data\naugmentation, optimization, and data batching. Our results demonstrate the\nsystem's effectiveness and generalizability, with 46.9% of generated code being\nhigh-quality and error-free, and 25% showing performance improvements over\nbaseline implementations. Empirical analysis shows an average reduction of\n57.9% in coding time compared to manual implementation. We observe higher gains\nfor more complex tasks. ResearchCodeAgent represents a significant step towards\nautomating the research implementation process, potentially accelerating the\npace of machine learning research."}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199.pdf", "abs": "https://arxiv.org/abs/2504.20199", "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios."}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294.pdf", "abs": "https://arxiv.org/abs/2504.20294", "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets."}
{"id": "2504.20456", "pdf": "https://arxiv.org/pdf/2504.20456.pdf", "abs": "https://arxiv.org/abs/2504.20456", "title": "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding", "authors": ["Gabe Guo", "Stefano Ermon"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In arbitrary-order language models, it is an open question how to sample\ntokens in parallel from the correct joint distribution. With discrete diffusion\nmodels, the more tokens they generate in parallel, the less their predicted\ndistributions adhere to the originally learned data distribution, as they rely\non a conditional independence assumption that only works with infinitesimally\nsmall timesteps. We find that a different class of models, any-subset\nautoregressive models (AS-ARMs), holds the solution. As implied by the name,\nAS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs\nsupport parallelized joint probability density estimation, allowing them to\ncorrect their own parallel-generated token distributions, via our Any-Subset\nSpeculative Decoding (ASSD) algorithm. ASSD provably enables generation of\ntokens from the correct joint distribution, with the number of neural network\ncalls upper bounded by the number of tokens predicted. We empirically verify\nthat ASSD speeds up language generation, without sacrificing quality.\nFurthermore, we provide a mathematically justified scheme for training AS-ARMs\nfor generation, and show that AS-ARMs achieve state-of-the-art performance\namong sub-200M parameter models on infilling benchmark tasks, and nearly match\nthe performance of models 50X larger on code generation. Our theoretical and\nempirical results indicate that the once-forgotten AS-ARMs are a promising\ndirection of language modeling."}
{"id": "2504.20458", "pdf": "https://arxiv.org/pdf/2504.20458.pdf", "abs": "https://arxiv.org/abs/2504.20458", "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User", "authors": ["Xiaolei Wang", "Chunxuan Xia", "Junyi Li", "Fanzhe Meng", "Lei Huang", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations. A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations. User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge. While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience.\n  To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs. The simulated user\nprovides feedback to the items recommended by CRSs, enabling them to better\ncapture intricate user preferences through multi-turn interaction. Inspired by\ngenerative reward models, we design two types of feedback actions for the\nsimulated user: i.e., generative item scoring, which offers coarse-grained\nfeedback, and attribute-based item critique, which provides fine-grained\nfeedback. To ensure seamless integration, these feedback actions are unified\ninto an instruction-based format, allowing the development of a unified\nsimulated user via instruction tuning on synthesized data. With this simulated\nuser, automatic multi-turn interaction with CRSs can be effectively conducted.\nFurthermore, to strike a balance between effectiveness and efficiency, we draw\ninspiration from the paradigm of reward-guided search in complex reasoning\ntasks and employ beam search for the interaction process. On top of this, we\npropose an efficient candidate ranking method to improve the recommendation\nresults derived from interaction. Extensive experiments on public datasets\ndemonstrate the effectiveness, efficiency, and transferability of our approach."}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571.pdf", "abs": "https://arxiv.org/abs/2504.20571", "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR"}
{"id": "2504.20595", "pdf": "https://arxiv.org/pdf/2504.20595.pdf", "abs": "https://arxiv.org/abs/2504.20595", "title": "ReasonIR: Training Retrievers for Reasoning Tasks", "authors": ["Rulin Shao", "Rui Qiao", "Varsha Kishore", "Niklas Muennighoff", "Xi Victoria Lin", "Daniela Rus", "Bryan Kian Hsiang Low", "Sewon Min", "Wen-tau Yih", "Pang Wei Koh", "Luke Zettlemoyer"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}", "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model."}
{"id": "2504.20859", "pdf": "https://arxiv.org/pdf/2504.20859.pdf", "abs": "https://arxiv.org/abs/2504.20859", "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "authors": ["Guy Hadad", "Haggai Roitman", "Yotam Eshel", "Bracha Shapira", "Lior Rokach"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in SIGIR '25", "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments."}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879.pdf", "abs": "https://arxiv.org/abs/2504.20879", "title": "The Leaderboard Illusion", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930.pdf", "abs": "https://arxiv.org/abs/2504.20930", "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs."}
{"id": "2504.20938", "pdf": "https://arxiv.org/pdf/2504.20938.pdf", "abs": "https://arxiv.org/abs/2504.20938", "title": "Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition", "authors": ["Zhengfu He", "Junxuan Wang", "Rui Lin", "Xuyang Ge", "Wentao Shu", "Qiong Tang", "Junping Zhang", "Xipeng Qiu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\nTransformer attention layers to disentangle original Multi Head Self Attention\n(MHSA) into individually comprehensible components. Lorsa is designed to\naddress the challenge of attention superposition to understand\nattention-mediated interaction between features in different token positions.\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\ndiscovered MHSA behaviors like induction heads, successor heads and attention\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\ndifferent Transformer components, and lead to consistent findings in many ways.\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\ninterpretability while Lorsa exhibits superior circuit discovery properties,\nespecially for features computed collectively by multiple MHSA heads. We also\nconduct extensive experiments on architectural design ablation, Lorsa scaling\nlaw and error analysis."}
{"id": "2308.09138", "pdf": "https://arxiv.org/pdf/2308.09138.pdf", "abs": "https://arxiv.org/abs/2308.09138", "title": "Semantic Consistency for Assuring Reliability of Large Language Models", "authors": ["Harsh Raj", "Vipul Gupta", "Domenic Rosati", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "An updated version of this preprint is available at arXiv:2502.15924,\n  and has been accepted at the Transactions on Machine Learning Research", "summary": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold."}
{"id": "2310.03903", "pdf": "https://arxiv.org/pdf/2310.03903.pdf", "abs": "https://arxiv.org/abs/2310.03903", "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models", "authors": ["Saaket Agashe", "Yue Fan", "Anthony Reyna", "Xin Eric Wang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated emergent common-sense\nreasoning and Theory of Mind (ToM) capabilities, making them promising\ncandidates for developing coordination agents. This study introduces the\nLLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context\nof Pure Coordination Settings, where agents must cooperate to maximize gains.\nOur benchmark evaluates LLMs through two distinct tasks. The first is Agentic\nCoordination, where LLMs act as proactive participants in four pure\ncoordination games. The second is Coordination Question Answering (CoordQA),\nwhich tests LLMs on 198 multiple-choice questions across these games to\nevaluate three key abilities: Environment Comprehension, ToM Reasoning, and\nJoint Planning. Results from Agentic Coordination experiments reveal that\nLLM-Agents excel in multi-agent coordination settings where decision-making\nprimarily relies on environmental variables but face challenges in scenarios\nrequiring active consideration of partners' beliefs and intentions. The CoordQA\nexperiments further highlight significant room for improvement in LLMs' Theory\nof Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC)\nexperiments in the Agentic Coordination setting demonstrate that LLM agents,\nunlike RL methods, exhibit robustness to unseen partners. These findings\nindicate the potential of LLMs as Agents in pure coordination setups and\nunderscore areas for improvement. Code Available at\nhttps://github.com/eric-ai-lab/llm_coordination."}
{"id": "2310.12059", "pdf": "https://arxiv.org/pdf/2310.12059.pdf", "abs": "https://arxiv.org/abs/2310.12059", "title": "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education", "authors": ["Duc-Vu Nguyen", "Quoc-Nam Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at SoICT 2023", "summary": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only."}
{"id": "2403.14562", "pdf": "https://arxiv.org/pdf/2403.14562.pdf", "abs": "https://arxiv.org/abs/2403.14562", "title": "Agentic AI: The Era of Semantic Decoding", "authors": ["Maxime Peyrard", "Martin Josifoski", "Robert West"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": "25 pages, 3 figures", "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation."}
{"id": "2407.15229", "pdf": "https://arxiv.org/pdf/2407.15229.pdf", "abs": "https://arxiv.org/abs/2407.15229", "title": "A Practical Analysis of Human Alignment with *PO", "authors": ["Kian Ahrabian", "Xihui Lin", "Barun Patra", "Vishrav Chaudhary", "Alon Benhaim", "Jay Pujara", "Xia Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 findings papers. 9 pages, 7 figures, 3 tables", "summary": "At the forefront of state-of-the-art human alignment methods are preference\noptimization methods (*PO). Prior research has often concentrated on\nidentifying the best-performing method, typically involving a grid search over\nhyperparameters, which can be impractical for general practitioners. In this\npaper, we examine the robustness of existing state-of-the-art methods to\nvarying hyperparameters in a realistic out-of-distribution (OOD) scenario that\nmirrors real-world applications of human alignment. Our goal is to empirically\nfind the method that increases the likelihood of achieving better results\nthrough the lens of various metrics, such as KL divergence and response length.\nWe also introduce LN-DPO, a simple length-normalized version of DPO that is\nmore stable across hyperparameters, effectively reduces the average response\nlength, and improves performance. Our analysis of state-of-the-art\nreference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO)\nmethods reveals that they perform similarly at their peak (i.e., best possible\nscenario). However, we uncover that the pattern of change in performance\ngreatly varies as we move away from the best possible scenario."}
{"id": "2408.00137", "pdf": "https://arxiv.org/pdf/2408.00137.pdf", "abs": "https://arxiv.org/abs/2408.00137", "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "authors": ["Sangwon Yu", "Jongyoon Song", "Bongkyu Hwang", "Hoyoung Kang", "Sooah Cho", "Junhwa Choi", "Seongho Joe", "Taehee Lee", "Youngjune L. Gwon", "Sungroh Yoon"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025 Oral", "summary": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities."}
{"id": "2409.07394", "pdf": "https://arxiv.org/pdf/2409.07394.pdf", "abs": "https://arxiv.org/abs/2409.07394", "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL"], "comment": "NAACL 2025 (main conference), Code:\n  https://github.com/HanNight/AdaCAD", "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Across four LLMs, six question-answering (QA) and three\nsummarization datasets, we demonstrate that ADACAD consistently outperforms\nother decoding baselines with average QA accuracy gains of 14.21% (absolute)\nover a static contrastive baseline, and improves the factuality of summaries by\n6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt\nperformance when conflict is absent, ADACAD mitigates these losses, making it\nmore applicable to real-world datasets in which some examples have conflict and\nothers do not."}
{"id": "2410.02102", "pdf": "https://arxiv.org/pdf/2410.02102.pdf", "abs": "https://arxiv.org/abs/2410.02102", "title": "Racing Thoughts: Explaining Contextualization Errors in Large Language Models", "authors": ["Michael A. Lepori", "Michael C. Mozer", "Asma Ghandeharioun"], "categories": ["cs.CL"], "comment": null, "summary": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it."}
{"id": "2410.07103", "pdf": "https://arxiv.org/pdf/2410.07103.pdf", "abs": "https://arxiv.org/abs/2410.07103", "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Jongyoon Song", "Saehyung Lee", "Junsung Park", "Sungroh Yoon"], "categories": ["cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the absolute position\nof supporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order,\nrelative position, in which the supporting documents are presented. We refer to\nthis as the misordered context problem. To address this issue, based on the\ntheoretical approach, we propose a simple yet effective method called context\nrepetition (CoRe), which involves prompting the model by repeatedly presenting\nthe context. This ensures that certain contiguous reasoning segments within\nsupporting documents are presented in the optimal order, effectively guiding\nthe model's reasoning in the appropriate direction. Applying CoRe, we improve\nthe F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to\n70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."}
{"id": "2410.23463", "pdf": "https://arxiv.org/pdf/2410.23463.pdf", "abs": "https://arxiv.org/abs/2410.23463", "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following", "authors": ["Gabrielle Kaili-May Liu", "Bowen Shi", "Avi Caciularu", "Idan Szpektor", "Arman Cohan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nunique difficulties, including management of inter-document dependencies,\nredundancy, and incoherent structures. To address this challenge, we introduce\nMDCure, a scalable and effective instruction data generation framework to\nenhance the MD capabilities of LLMs without the computational cost of\npre-training or reliance on human-annotated data. MDCure generates high-quality\nsynthetic MD instruction data over sets of articles via targeted prompts. We\nalso introduce MDCureRM, a cost-effective, MD-specific reward model to score\nand filter generated data based on their training utility for MD settings.\nMDCure is compatible with open- and closed-source models in addition to policy\noptimization methods such as PPO, enabling even small open-source models to\nsurpass proprietary LLMs as strong generators of high-quality MD instruction\ndata without further data filtering. With MDCure, we fine-tune a wide variety\nof LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model\nfamilies. Extensive evaluations on a wide range of MD and long-context\nbenchmarks spanning various tasks and domains show MDCure consistently improves\nperformance over pre-trained baselines and base models by up to 75.1%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure."}
{"id": "2410.24175", "pdf": "https://arxiv.org/pdf/2410.24175.pdf", "abs": "https://arxiv.org/abs/2410.24175", "title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."}
{"id": "2411.07127", "pdf": "https://arxiv.org/pdf/2411.07127.pdf", "abs": "https://arxiv.org/abs/2411.07127", "title": "Benchmarking LLMs' Judgments with No Gold Standard", "authors": ["Shengwei Xu", "Yuxuan Lu", "Grant Schoenebeck", "Yuqing Kong"], "categories": ["cs.CL", "cs.LG"], "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)", "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset."}
{"id": "2411.08243", "pdf": "https://arxiv.org/pdf/2411.08243.pdf", "abs": "https://arxiv.org/abs/2411.08243", "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset", "authors": ["Khaoula Chehbouni", "Jonathan Colaço Carr", "Yash More", "Jackie CK Cheung", "Golnoosh Farnadi"], "categories": ["cs.CL", "cs.CY"], "comment": "Prepared for conference submission", "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs."}
{"id": "2411.09694", "pdf": "https://arxiv.org/pdf/2411.09694.pdf", "abs": "https://arxiv.org/abs/2411.09694", "title": "A Bayesian Optimization Approach to Machine Translation Reranking", "authors": ["Julius Cheng", "Maike Züfle", "Vilém Zouhar", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": "NAACL 2025 camera ready", "summary": "Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers."}
{"id": "2502.01563", "pdf": "https://arxiv.org/pdf/2502.01563.pdf", "abs": "https://arxiv.org/abs/2502.01563", "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding", "authors": ["Mingyu Jin", "Kai Mei", "Wujiang Xu", "Mingjie Sun", "Ruixiang Tang", "Mengnan Du", "Zirui Liu", "Yongfeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in contextual\nknowledge understanding. In this paper, we show that these concentrated massive\nvalues consistently emerge in specific regions of attention queries (Q) and\nkeys (K) while not having such patterns in values (V) in various modern\ntransformer-based LLMs (Q, K, and V mean the representations output by the\nquery, key, and value layers respectively). Through extensive experiments, we\nfurther demonstrate that these massive values play a critical role in\ninterpreting contextual knowledge (knowledge obtained from the current context\nwindow) rather than in retrieving parametric knowledge stored within the\nmodel's parameters. Our further investigation of quantization strategies\nreveals that ignoring these massive values leads to a pronounced drop in\nperformance on tasks requiring rich contextual understanding, aligning with our\nanalysis. Finally, we trace the emergence of concentrated massive values and\nfind that such concentration is caused by Rotary Positional Encoding (RoPE),\nwhich has appeared since the first layers. These findings shed new light on how\nQ and K operate in LLMs and offer practical insights for model design and\noptimization. The Code is Available at\nhttps://github.com/MingyuJ666/Rope_with_LLM."}
{"id": "2502.12836", "pdf": "https://arxiv.org/pdf/2502.12836.pdf", "abs": "https://arxiv.org/abs/2502.12836", "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation", "authors": ["Mohammad Feli", "Iman Azimi", "Pasi Liljeberg", "Amir M. Rahmani"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub."}
{"id": "2502.17163", "pdf": "https://arxiv.org/pdf/2502.17163.pdf", "abs": "https://arxiv.org/abs/2502.17163", "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "authors": ["María Andrea Cruz Blandón", "Jayasimha Talur", "Bruno Charron", "Dong Liu", "Saab Mansour", "Marcello Federico"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG"}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358.pdf", "abs": "https://arxiv.org/abs/2503.15358", "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Marco Idiart"], "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.19878", "pdf": "https://arxiv.org/pdf/2503.19878.pdf", "abs": "https://arxiv.org/abs/2503.19878", "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "authors": ["Nengbo Wang", "Xiaotian Han", "Jagdip Singh", "Jing Ma", "Vipin Chaudhary"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239.pdf", "abs": "https://arxiv.org/abs/2504.05239", "title": "LLM-based Automated Grading with Human-in-the-Loop", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation."}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072.pdf", "abs": "https://arxiv.org/abs/2504.07072", "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Setayesh Heydari", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "categories": ["cs.CL", "cs.CV"], "comment": "v2: corrected the author list", "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks."}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914.pdf", "abs": "https://arxiv.org/abs/2504.13914", "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning", "authors": ["ByteDance Seed", ":", "Jiaze Chen", "Tiantian Fan", "Xin Liu", "Lingjun Liu", "Zhiqi Lin", "Mingxuan Wang", "Chengyi Wang", "Xiangpeng Wei", "Wenyuan Xu", "Yufeng Yuan", "Yu Yue", "Lin Yan", "Qiying Yu", "Xiaochen Zuo", "Chi Zhang", "Ruofei Zhu", "Zhecheng An", "Zhihao Bai", "Yu Bao", "Xingyan Bin", "Jiangjie Chen", "Feng Chen", "Hongmin Chen", "Riwei Chen", "Liangqiang Chen", "Zixin Chen", "Jinsong Chen", "Siyan Chen", "Kaiyuan Chen", "Zhi Chen", "Jin Chen", "Jiecao Chen", "Jinxin Chi", "Weinan Dai", "Ning Dai", "Jiahui Dai", "Shihan Dou", "Yantao Du", "Zhengyin Du", "Jianhui Duan", "Chen Dun", "Ting-Han Fan", "Jiazhan Feng", "Junda Feng", "Ziyuan Feng", "Yuwei Fu", "Wenqi Fu", "Hanjie Fu", "Hao Ge", "Hongyi Guo", "Mingji Han", "Li Han", "Wenhao Hao", "Xintong Hao", "Qianyu He", "Jerry He", "Feng He", "Wen Heng", "Zehua Hong", "Qi Hou", "Liang Hu", "Shengding Hu", "Nan Hu", "Kai Hua", "Qi Huang", "Ziyue Huang", "Hongzhi Huang", "Zihao Huang", "Ting Huang", "Wenhao Huang", "Wei Jia", "Bin Jia", "Xiaoying Jia", "Yuhua Jiang", "Haobin Jiang", "Ziheng Jiang", "Kaihua Jiang", "Chengquan Jiang", "Jianpeng Jiao", "Xiaoran Jin", "Xing Jin", "Xunhao Lai", "Zheng Li", "Xiang Li", "Liyi Li", "Hongkai Li", "Zheng Li", "Shengxian Wan", "Ya Wang", "Yunshui Li", "Chenggang Li", "Niuniu Li", "Siyu Li", "Xi Li", "Xiao Li", "Aoyan Li", "Yuntao Li", "Nianning Liang", "Xinnian Liang", "Haibin Lin", "Weijian Lin", "Ye Lin", "Zhicheng Liu", "Guanlin Liu", "Guanlin Liu", "Chenxiao Liu", "Yan Liu", "Gaohong Liu", "Juncai Liu", "Chundian Liu", "Deyi Liu", "Kaibo Liu", "Siyao Liu", "Qi Liu", "Yongfei Liu", "Kang Liu", "Gan Liu", "Boyi Liu", "Rui Long", "Weiqiang Lou", "Chenwei Lou", "Xiang Luo", "Yao Luo", "Caiping Lv", "Heyang Lv", "Bole Ma", "Qianli Ma", "Hongzhi Ma", "Yiyuan Ma", "Jin Ma", "Wenchang Ma", "Tingting Ma", "Chen Mao", "Qiyang Min", "Zhe Nan", "Guanghan Ning", "Jinxiang Ou", "Haojie Pan", "Renming Pang", "Yanghua Peng", "Tao Peng", "Lihua Qian", "Lihua Qian", "Mu Qiao", "Meng Qu", "Cheng Ren", "Hongbin Ren", "Yong Shan", "Wei Shen", "Ke Shen", "Kai Shen", "Guangming Sheng", "Jinlong Shi", "Wenlei Shi", "Guang Shi", "Shuai Shuai Cao", "Yuxin Song", "Zuquan Song", "Jing Su", "Yifan Sun", "Tao Sun", "Zewei Sun", "Borui Wan", "Zihan Wang", "Xiaohui Wang", "Xi Wang", "Shuguang Wang", "Jun Wang", "Qinlong Wang", "Chenyuan Wang", "Shuai Wang", "Zihan Wang", "Changbao Wang", "Jiaqiang Wang", "Shihang Wang", "Xuwu Wang", "Zaiyuan Wang", "Yuxuan Wang", "Wenqi Wang", "Taiqing Wang", "Chengzhi Wei", "Houmin Wei", "Ziyun Wei", "Shufa Wei", "Zheng Wu", "Yonghui Wu", "Yangjun Wu", "Bohong Wu", "Shuang Wu", "Jingqiao Wu", "Ning Wu", "Shuangzhi Wu", "Jianmin Wu", "Chenguang Xi", "Fan Xia", "Yuqiao Xian", "Liang Xiang", "Boren Xiang", "Bowen Xiao", "Zhen Xiao", "Xia Xiao", "Yongsheng Xiao", "Chao Xin", "Shulin Xin", "Yuwen Xiong", "Jingjing Xu", "Ziwen Xu", "Chenyin Xu", "Jiayi Xu", "Yifan Xu", "Wei Xu", "Yufei Xu", "Shikun Xu", "Shipeng Yan", "Shen Yan", "Qingping Yang", "Xi Yang", "Tianhao Yang", "Yuehang Yang", "Yuan Yang", "Ximing Yang", "Zeyu Yang", "Guang Yang", "Yifan Yang", "Xuesong Yao", "Bairen Yi", "Fan Yin", "Jianian Yin", "Ziqiang Ying", "Xiangyu Yu", "Hongli Yu", "Song Yu", "Menghan Yu", "Huan Yu", "Siyu Yuan", "Jun Yuan", "Yutao Zeng", "Tianyang Zhan", "Zheng Zhang", "Yun Zhang", "Mofan Zhang", "Wang Zhang", "Ru Zhang", "Zhi Zhang", "Tianqi Zhang", "Xinyi Zhang", "Zhexi Zhang", "Sijun Zhang", "Wenqiang Zhang", "Xiangxiang Zhang", "Yongtao Zhang", "Yuyu Zhang", "Ge Zhang", "He Zhang", "Yue Zhang", "Renjie Zheng", "Ningxin Zheng", "Zhuolin Zheng", "Yaowei Zheng", "Chen Zheng", "Xiaoyun Zhi", "Wanjun Zhong", "Cheng Zhong", "Zheng Zhong", "Baoquan Zhong", "Xun Zhou", "Na Zhou", "Huan Zhou", "Hang Zhu", "Defa Zhu", "Wenjia Zhu", "Lei Zuo"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed1.5-Thinking, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research. Model trial link:\nhttps://www.volcengine.com/experience/ark."}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642.pdf", "abs": "https://arxiv.org/abs/2504.15642", "title": "Computational Typology", "authors": ["Gerhard Jäger"], "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology."}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900.pdf", "abs": "https://arxiv.org/abs/2504.15900", "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406.pdf", "abs": "https://arxiv.org/abs/2504.18406", "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333.pdf", "abs": "https://arxiv.org/abs/2504.19333", "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406.pdf", "abs": "https://arxiv.org/abs/2504.19406", "title": "Context Selection and Rewriting for Video-based Educational Question Generation", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013.pdf", "abs": "https://arxiv.org/abs/2504.20013", "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2303.01903", "pdf": "https://arxiv.org/pdf/2303.01903.pdf", "abs": "https://arxiv.org/abs/2303.01903", "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering", "authors": ["Zhou Yu", "Xuecheng Ouyang", "Zhenwei Shao", "Meng Wang", "Jun Yu"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "An extended journal version of our CVPR 2023 paper, which has been\n  accepted at IEEE T-PAMI 2025. The original conference version can be referred\n  to as the v1 version", "summary": "Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the \\emph{blind} LLM\nas the provided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. The two types\nof answer heuristics are jointly encoded into a formatted prompt to facilitate\nthe LLM's understanding of both the image and question, thus generating a more\naccurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet\nsignificantly outperforms existing state-of-the-art methods on four challenging\nknowledge-based VQA datasets. Prophet is general that can be instantiated with\nthe combinations of different VQA models (i.e., both discriminative and\ngenerative ones) and different LLMs (i.e., both commercial and open-source\nones). Moreover, Prophet can also be integrated with modern large multimodal\nmodels in different stages, which is named Prophet++, to further improve the\ncapabilities on knowledge-based VQA tasks."}
{"id": "2410.13675", "pdf": "https://arxiv.org/pdf/2410.13675.pdf", "abs": "https://arxiv.org/abs/2410.13675", "title": "Pose-Based Sign Language Appearance Transfer", "authors": ["Amit Moryossef", "Gerard Sant", "Zifan Jiang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\nhttps://github.com/sign-language-processing/pose-anonymization."}
{"id": "2412.02467", "pdf": "https://arxiv.org/pdf/2412.02467.pdf", "abs": "https://arxiv.org/abs/2412.02467", "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators", "authors": ["Tejumade Afonja", "Hui-Po Wang", "Raouf Kerkouche", "Mario Fritz"], "categories": ["cs.LG", "cs.CL", "cs.CR", "D.4.6; G.3; I.2.7"], "comment": null, "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."}
{"id": "2412.06832", "pdf": "https://arxiv.org/pdf/2412.06832.pdf", "abs": "https://arxiv.org/abs/2412.06832", "title": "SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering", "authors": ["Michael Iannelli", "Sneha Kuchipudi", "Vera Dvorak"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DC", "H.3.4; H.3.3; I.2.7; I.2.11; C.2.4"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngeneralize to new information by decoupling reasoning capabilities from static\nknowledge bases. Traditional RAG enhancements have explored vertical\nscaling-assigning subtasks to specialized modules-and horizontal\nscaling-replicating tasks across multiple agents-to improve performance.\nHowever, real-world applications impose diverse Service Level Agreements (SLAs)\nand Quality of Service (QoS) requirements, involving trade-offs among\nobjectives such as reducing cost, ensuring answer quality, and adhering to\nspecific operational constraints.\n  In this work, we present a systems-oriented approach to multi-agent RAG\ntailored for real-world Question Answering (QA) applications. By integrating\ntask-specific non-functional requirements-such as answer quality, cost, and\nlatency-into the system, we enable dynamic reconfiguration to meet diverse\nSLAs. Our method maps these Service Level Objectives (SLOs) to system-level\nparameters, allowing the generation of optimal results within specified\nresource constraints.\n  We conduct a case study in the QA domain, demonstrating how dynamic\nre-orchestration of a multi-agent RAG system can effectively manage the\ntrade-off between answer quality and cost. By adjusting the system based on\nquery intent and operational conditions, we systematically balance performance\nand resource utilization. This approach allows the system to meet SLOs for\nvarious query types, showcasing its practicality for real-world applications."}
{"id": "2501.12433", "pdf": "https://arxiv.org/pdf/2501.12433.pdf", "abs": "https://arxiv.org/abs/2501.12433", "title": "Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models", "authors": ["Tabinda Aman", "Mohammad Nadeem", "Shahab Saquib Sohail", "Mohammad Anas", "Erik Cambria"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Animal stereotypes are deeply embedded in human culture and language. They\noften shape our perceptions and expectations of various species. Our study\ninvestigates how animal stereotypes manifest in vision-language models during\nthe task of image generation. Through targeted prompts, we explore whether\nDALL-E perpetuates stereotypical representations of animals, such as \"owls as\nwise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped\ninstances where the model consistently generates images aligned with cultural\nbiases. The current work is the first of its kind to examine animal\nstereotyping in vision-language models systematically and to highlight a\ncritical yet underexplored dimension of bias in AI-generated visual content."}
{"id": "2501.18045", "pdf": "https://arxiv.org/pdf/2501.18045.pdf", "abs": "https://arxiv.org/abs/2501.18045", "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors", "authors": ["Myra Cheng", "Angela Y. Lee", "Kristina Rapuano", "Kate Niederhoffer", "Alex Liebscher", "Jeffrey Hancock"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025", "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development."}
{"id": "2502.03629", "pdf": "https://arxiv.org/pdf/2502.03629.pdf", "abs": "https://arxiv.org/abs/2502.03629", "title": "REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations", "authors": ["Peter Sushko", "Ayana Bharadwaj", "Zhi Yang Lim", "Vasily Ilin", "Ben Caffee", "Dongping Chen", "Mohammadreza Salehi", "Cheng-Yu Hsieh", "Ranjay Krishna"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Published at CVPR 2025", "summary": "Existing image editing models struggle to meet real-world demands. Despite\nexcelling in academic benchmarks, they have yet to be widely adopted for real\nuser needs. Datasets that power these models use artificial edits, lacking the\nscale and ecological validity necessary to address the true diversity of user\nrequests. We introduce REALEDIT, a large-scale image editing dataset with\nauthentic user requests and human-made edits sourced from Reddit. REALEDIT\nincludes a test set of 9300 examples to evaluate models on real user requests.\nOur results show that existing models fall short on these tasks, highlighting\nthe need for realistic training data. To address this, we introduce 48K\ntraining examples and train our REALEDIT model, achieving substantial gains -\noutperforming competitors by up to 165 Elo points in human judgment and 92\npercent relative improvement on the automated VIEScore metric. We deploy our\nmodel on Reddit, testing it on new requests, and receive positive feedback.\nBeyond image editing, we explore REALEDIT's potential in detecting edited\nimages by partnering with a deepfake detection non-profit. Finetuning their\nmodel on REALEDIT data improves its F1-score by 14 percentage points,\nunderscoring the dataset's value for broad applications."}
{"id": "2503.04606", "pdf": "https://arxiv.org/pdf/2503.04606.pdf", "abs": "https://arxiv.org/abs/2503.04606", "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation", "authors": ["Aoxiong Yin", "Kai Shen", "Yichong Leng", "Xu Tan", "Xinyu Zhou", "Juncheng Li", "Siliang Tang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Our code is available at https://github.com/LanDiff/LanDiff", "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Kling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/."}
{"id": "2503.04992", "pdf": "https://arxiv.org/pdf/2503.04992.pdf", "abs": "https://arxiv.org/abs/2503.04992", "title": "Wanda++: Pruning Large Language Models via Regional Gradients", "authors": ["Yifan Yang", "Kai Zhen", "Bhavana Ganesh", "Aram Galstyan", "Goeric Huybrechts", "Markus Müller", "Jonas M. Kübler", "Rupak Vignesh Swaminathan", "Athanasios Mouchtaris", "Sravan Babu Bodapati", "Nathan Susanj", "Zheng Zhang", "Jack FitzGerald", "Abhishek Kumar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU."}
{"id": "2503.09089", "pdf": "https://arxiv.org/pdf/2503.09089.pdf", "abs": "https://arxiv.org/abs/2503.09089", "title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "authors": ["Zhaoling Chen", "Xiangru Tang", "Gangda Deng", "Fang Wu", "Jialong Wu", "Zhiwei Jiang", "Viktor Prasanna", "Arman Cohan", "Xingyao Wang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120.pdf", "abs": "https://arxiv.org/abs/2504.13120", "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs' outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs."}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365.pdf", "abs": "https://arxiv.org/abs/2504.17365", "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062.pdf", "abs": "https://arxiv.org/abs/2504.19062", "title": "Versatile Framework for Song Generation with Prompt-based Control", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://aaronz345.github.io/VersBandDemo."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458.pdf", "abs": "https://arxiv.org/abs/2504.19458", "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
