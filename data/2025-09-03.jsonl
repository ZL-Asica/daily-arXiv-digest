{"id": "2509.00352", "pdf": "https://arxiv.org/pdf/2509.00352.pdf", "abs": "https://arxiv.org/abs/2509.00352", "title": "Unlocking Mixed Reality for Medical Education: A See-Through Perspective on Head Anatomy", "authors": ["Yuqing Wei", "Yupeng Wang", "Jiayi Zhao", "Yanjun Liu", "Huxin Gao", "Jiewen Lai"], "categories": ["cs.HC"], "comment": null, "summary": "Extended reality (XR), encompassing Virtual Reality (VR), Augmented Reality\n(AR), and Mixed Reality (MR), is emerging as a transformative platform for\nmedical education. Traditional methods such as textbooks, physical models, and\ncadaveric dissections often lack interactivity and fail to convey complex\nspatial relationships effectively. The emerging MR technology addresses these\nlimitations by providing immersive environments that blend virtual elements\nwith real-world contexts. This study presents an MR application for head\nanatomy education, enabling learners to intuitively interact with see-through\n3D anatomical structures via hand gestures and controllers. Our hierarchical\ninformation design supports progressive learning, guiding users from basic\nanatomical labels to detailed structural insights. Additionally, the system\nincorporates an automatic calibration module that aligns virtual anatomical\nmodels with a real human head, thereby facilitating realistic human-model\ninteractions. Experiments show that the system can effectively match the\nanatomical model with real-time scenes, thus enhancing the interactivity and\nimmersion of medical education, providing an innovative tool for teaching\nanatomy."}
{"id": "2509.00440", "pdf": "https://arxiv.org/pdf/2509.00440.pdf", "abs": "https://arxiv.org/abs/2509.00440", "title": "Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners", "authors": ["Ibrahim Al-Hazwani", "Ke Er Zhang", "Laura Garrison", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "5 pages, to be feature in the proceedings of IEEE VIS Short paper\n  track", "summary": "Data Humanism is a human-centered design approach that emphasizes the\npersonal, contextual, and imperfect nature of data. Despite its growing\ninfluence among practitioners, the 13 principles outlined in Giorgia Lupi's\nvisual manifesto remain loosely defined in research contexts, creating a gap\nbetween design practice and systematic application. Through a mixed-methods\napproach, including a systematic literature review, multimedia analysis, and\nexpert interviews, we present a characterization of Data Humanism principles\nfor visualization researchers. Our characterization provides concrete\ndefinitions that maintain interpretive flexibility in operationalizing design\nchoices. We validate our work through direct consultation with Lupi. Moreover,\nwe leverage the characterization to decode a visualization work, mapping Data\nHumanism principles to specific visual design choices. Our work creates a\ncommon language for human-centered visualization, bridging the gap between\npractice and research for future applications and evaluations."}
{"id": "2509.00572", "pdf": "https://arxiv.org/pdf/2509.00572.pdf", "abs": "https://arxiv.org/abs/2509.00572", "title": "How to Make Museums More Interactive? Case Study of Artistic Chatbot", "authors": ["Filip J. Kucia", "Bartosz Grabek", "Szymon D. Trochimiak", "Anna Wróblewska"], "categories": ["cs.HC", "cs.IR"], "comment": "7 pages, 3 figures", "summary": "Conversational agents powered by Large Language Models (LLMs) are\nincreasingly utilized in educational settings, in particular in individual\nclosed digital environments, yet their potential adoption in the physical\nlearning environments like cultural heritage sites, museums, and art galleries\nremains relatively unexplored. In this study, we present Artistic Chatbot, a\nvoice-to-voice RAG-powered chat system to support informal learning and enhance\nvisitor engagement during a live art exhibition celebrating the 15th\nanniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts,\nPoland. The question answering (QA) chatbot responded to free-form spoken\nquestions in Polish using the context retrieved from a curated, domain-specific\nknowledge base consisting of 226 documents provided by the organizers,\nincluding faculty information, art magazines, books, and journals. We describe\nthe key aspects of the system architecture and user interaction design, as well\nas discuss the practical challenges associated with deploying chatbots at\npublic cultural sites. Our findings, based on interaction analysis, demonstrate\nthat chatbots such as Artistic Chatbot effectively maintain responses grounded\nin exhibition content (60\\% of responses directly relevant), even when faced\nwith unpredictable queries outside the target domain, showing their potential\nfor increasing interactivity in public cultural sites.\n  GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025"}
{"id": "2509.00696", "pdf": "https://arxiv.org/pdf/2509.00696.pdf", "abs": "https://arxiv.org/abs/2509.00696", "title": "Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse", "authors": ["Akriti Verma", "Shama Islam", "Valeh Moghaddam", "Adnan Anwar"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "The pervasiveness of online toxicity, including hate speech and trolling,\ndisrupts digital interactions and online well-being. Previous research has\nmainly focused on post-hoc moderation, overlooking the real-time emotional\ndynamics of online conversations and the impact of users' emotions on others.\nThis paper presents a graph-based framework to identify the need for emotion\nregulation within online conversations. This framework promotes self-reflection\nto manage emotional responses and encourage responsible behaviour in real time.\nAdditionally, a comment queuing mechanism is proposed to address intentional\ntrolls who exploit emotions to inflame conversations. This mechanism introduces\na delay in publishing comments, giving users time to self-regulate before\nfurther engaging in the conversation and helping maintain emotional balance.\nAnalysis of social media data from Twitter and Reddit demonstrates that the\ngraph-based framework reduced toxicity by 12%, while the comment queuing\nmechanism decreased the spread of anger by 15%, with only 4% of comments being\ntemporarily held on average. These findings indicate that combining real-time\nemotion regulation with delayed moderation can significantly improve well-being\nin online environments."}
{"id": "2509.00030", "pdf": "https://arxiv.org/pdf/2509.00030.pdf", "abs": "https://arxiv.org/abs/2509.00030", "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation."}
{"id": "2509.00852", "pdf": "https://arxiv.org/pdf/2509.00852.pdf", "abs": "https://arxiv.org/abs/2509.00852", "title": "Why it is worth making an effort with GenAI", "authors": ["Yvonne Rogers"], "categories": ["cs.HC", "cs.AI", "I.2; J.5"], "comment": "6 pages", "summary": "Students routinely use ChatGPT and the like now to help them with their\nhomework, such as writing an essay. It takes less effort to complete and is\neasier to do than by hand. It can even produce as good if not better output\nthan the student's own work. However, there is a growing concern that\nover-reliance on using GenAI in this way will stifle the development of\nlearning writing and critical thinking skills. How might this trend be\nreversed? What if students were required to make more effort when using GenAI\nto do their homework? It might be more challenging, but the additional effort\ninvolved could result in them learning more and having a greater sense of\nachievement. This tension can be viewed as a form of effort paradox; where\neffort is both viewed as something to be avoided but at the same time is\nvalued. Is it possible to let students learn sometimes with less and other\ntimes more effort? Students are already adept at the former but what about the\nlatter? Could we design new kinds of AI tools that deliberately require more\neffort to use to deepen the learning experience? In this paper, I begin to\noutline what form these might take, for example, asking students to use a\ncombination of GenAI tools with traditional learning approaches (e.g.\nnote-taking while reading). I also discuss how else to design tools to think\nwith that augments human cognition; where students learn more the skills of\nmetacognition and reflection."}
{"id": "2509.00038", "pdf": "https://arxiv.org/pdf/2509.00038.pdf", "abs": "https://arxiv.org/abs/2509.00038", "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis", "authors": ["Teo Susnjak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer significant potential to accelerate\nsystematic literature reviews (SLRs), yet current approaches often rely on\nbrittle, manually crafted prompts that compromise reliability and\nreproducibility. This fragility undermines scientific confidence in\nLLM-assisted evidence synthesis. In response, this work adapts recent advances\nin declarative prompt optimisation, developed for general-purpose LLM\napplications, and demonstrates their applicability to the domain of SLR\nautomation. This research proposes a structured, domain-specific framework that\nembeds task declarations, test suites, and automated prompt tuning into a\nreproducible SLR workflow. These emerging methods are translated into a\nconcrete blueprint with working code examples, enabling researchers to\nconstruct verifiable LLM pipelines that align with established principles of\ntransparency and rigour in evidence synthesis. This is a novel application of\nsuch approaches to SLR pipelines."}
{"id": "2509.00944", "pdf": "https://arxiv.org/pdf/2509.00944.pdf", "abs": "https://arxiv.org/abs/2509.00944", "title": "Through the Expert's Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR", "authors": ["Clara Sayffaerth", "Annika Köhler", "Julian Rasch", "Albrecht Schmidt", "Florian Müller"], "categories": ["cs.HC"], "comment": "11 pages, IEEE International Symposium on Mixed and Augmented Reality\n  (ISMAR)", "summary": "Transferring knowledge across generations is fundamental to human\ncivilization, yet the challenge of passing on complex practical skills\npersists. Methods without a physically present instructor, such as videos,\noften fail to explain complex manual tasks, where spatial and social factors\nare critical. Technologies such as eXtended Reality and Artificial Intelligence\nhold the potential to retain expert knowledge and facilitate the creation of\ntailored, contextualized, and asynchronous explanations regardless of time and\nplace. In contrast to videos, the learner's perspective can be different from\nthe recorded perspective in XR. This paper investigates the impact of\nasynchronous first- and third-person perspectives and gaze visualizations on\nefficiency, feeling of embodiment, and connectedness during manual tasks. The\nempirical results of our study (N=36) show that the first-person perspective is\nbetter in quantitative measures and preferred by users. We identify best\npractices for presenting preserved knowledge and provide guidelines for\ndesigning future systems."}
{"id": "2509.00185", "pdf": "https://arxiv.org/pdf/2509.00185.pdf", "abs": "https://arxiv.org/abs/2509.00185", "title": "What Are Research Hypotheses?", "authors": ["Jian Wu", "Sarah Rajtmajer"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, accepted by Sci-K'25: International Workshop on Scientific\n  Knowledge", "summary": "Over the past decades, alongside advancements in natural language processing,\nsignificant attention has been paid to training models to automatically\nextract, understand, test, and generate hypotheses in open and scientific\ndomains. However, interpretations of the term \\emph{hypothesis} for various\nnatural language understanding (NLU) tasks have migrated from traditional\ndefinitions in the natural, social, and formal sciences. Even within NLU, we\nobserve differences defining hypotheses across literature. In this paper, we\noverview and delineate various definitions of hypothesis. Especially, we\ndiscern the nuances of definitions across recently published NLU tasks. We\nhighlight the importance of well-structured and well-defined hypotheses,\nparticularly as we move toward a machine-interpretable scholarly record."}
{"id": "2509.01018", "pdf": "https://arxiv.org/pdf/2509.01018.pdf", "abs": "https://arxiv.org/abs/2509.01018", "title": "The State of the Art in Visualization Literacy", "authors": ["Matthew Varona", "Karen Bonilla", "Maryam Hedayati", "Alark Joshi", "Lane Harrison", "Matthew Kay", "Carolina Nobre"], "categories": ["cs.HC"], "comment": "Preprint version. A revised document may follow", "summary": "Research in visualization literacy explores the skills required to engage\nwith visualizations. This state-of-the-art report surveys the current\nliterature in visualization literacy to provide a comprehensive overview of the\nfield. We propose a taxonomy of visualization literacy that organizes the field\ninto competency themes and research categories. To address ambiguity\nsurrounding the term ``visualization literacy'', we provide a framework for\noperationalizing visualization literacy based on application contexts\n(including domain, scenario, and audience) and relevant competencies, which are\ncategorized under consumption, construction, critique, and connection. Research\ncontributions are organized into five categories: ontology, assessment,\nmechanisms, populiteracy, and intervention. For each category, we identify key\ntrends, discuss which competencies are addressed, highlight open challenges,\nand examine how advancements within these areas inform and reinforce each\nother, driving progress in the field."}
{"id": "2509.00190", "pdf": "https://arxiv.org/pdf/2509.00190.pdf", "abs": "https://arxiv.org/abs/2509.00190", "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics", "authors": ["Sheldon Yu", "Yuxin Xiong", "Junda Wu", "Xintong Li", "Tong Yu", "Xiang Chen", "Ritwik Sinha", "Jingbo Shang", "Julian McAuley"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures", "summary": "Recent advances in chain-of-thought (CoT) prompting have enabled large\nlanguage models (LLMs) to perform multi-step reasoning. However, the\nexplainability of such reasoning remains limited, with prior work primarily\nfocusing on local token-level attribution, such that the high-level semantic\nroles of reasoning steps and their transitions remain underexplored. In this\npaper, we introduce a state-aware transition framework that abstracts CoT\ntrajectories into structured latent dynamics. Specifically, to capture the\nevolving semantics of CoT reasoning, each reasoning step is represented via\nspectral analysis of token-level embeddings and clustered into semantically\ncoherent latent states. To characterize the global structure of reasoning, we\nmodel their progression as a Markov chain, yielding a structured and\ninterpretable view of the reasoning process. This abstraction supports a range\nof analyses, including semantic role identification, temporal pattern\nvisualization, and consistency evaluation."}
{"id": "2509.01051", "pdf": "https://arxiv.org/pdf/2509.01051.pdf", "abs": "https://arxiv.org/abs/2509.01051", "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces", "authors": ["Matte Lim", "Catherine Yeh", "Martin Wattenberg", "Fernanda Viégas", "Panagiotis Michalatos"], "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 Short Paper Track (5 pages, 4 figures)", "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets."}
{"id": "2509.00245", "pdf": "https://arxiv.org/pdf/2509.00245.pdf", "abs": "https://arxiv.org/abs/2509.00245", "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs", "authors": ["Seiji Maekawa", "Hayate Iso", "Nikita Bhutani"], "categories": ["cs.CL"], "comment": null, "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection."}
{"id": "2509.01089", "pdf": "https://arxiv.org/pdf/2509.01089.pdf", "abs": "https://arxiv.org/abs/2509.01089", "title": "CosinorAge: Unified Python and Web Platform for Biological Age Estimation from Wearable- and Smartwatch-Based Activity Rhythms", "authors": ["Jinjoo Shim", "Jacob Hunecke", "Elgar Fleisch", "Filipe Barata"], "categories": ["cs.HC"], "comment": "7 pages - 4 figures", "summary": "Every day, millions of people worldwide track their steps, sleep, and\nactivity rhythms with smartwatches and fitness trackers. These continuously\ncollected data streams present a remarkable opportunity to transform routine\nself-tracking into meaningful health insights that enable individuals to\nunderstand -- and potentially influence -- their biological aging. Yet most\ntools for analyzing wearable data remain fragmented, proprietary, and\ninaccessible, creating a major barrier between this vast reservoir of personal\nhealth information and its translation into actionable insights on aging.\nCosinorAge is an open-source framework that estimates biological age from\nwearable-derived circadian, physical activity, and sleep metrics. It addresses\nthe lack of unified, reproducible pipelines for jointly analyzing\nrest--activity rhythmicity, physical activity, and sleep, and linking them to\nhealth outcomes. The Python package provides an end-to-end workflow from raw\ndata ingestion and preprocessing to feature computation and biological age\nestimation, supporting multiple input sources across wearables and smartwatch.\nIt also makes available trained model parameters (open weights) derived from\nlarge-scale population datasets such as UK Biobank, enabling reproducibility,\ntransparency, and generalizability across studies. Its companion web-based\nCosinorAge Calculator enables non-technical users to access identical\nanalytical capabilities through an intuitive interface. By combining\ntransparent, reproducible analysis with broad accessibility, CosinorAge\nadvances scalable, personalized health monitoring and bridges digital health\ntechnologies with biological aging research."}
{"id": "2509.00248", "pdf": "https://arxiv.org/pdf/2509.00248.pdf", "abs": "https://arxiv.org/abs/2509.00248", "title": "The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions", "authors": ["Zachary K. Stine", "James E. Deitrick"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of methods for modeling of human meaning-making constitutes\na powerful class of instruments for the analysis of complex semiotic systems.\nHowever, the field lacks a general theoretical framework for describing these\nmodeling practices across various model types in an apples-to-apples way. In\nthis paper, we propose such a framework grounded in the semiotic theory of C.\nS. Peirce. We argue that such models measure latent symbol geometries, which\ncan be understood as hypotheses about the complex of semiotic agencies\nunderlying a symbolic dataset. Further, we argue that in contexts where a\nmodel's value cannot be straightforwardly captured by proxy measures of\nperformance, models can instead be understood relationally, so that the\nparticular interpretive lens of a model becomes visible through its contrast\nwith other models. This forms the basis of a theory of model semantics in which\nmodels, and the modeling decisions that constitute them, are themselves treated\nas signs. In addition to proposing the framework, we illustrate its empirical\nuse with a few brief examples and consider foundational questions and future\ndirections enabled by the framework."}
{"id": "2509.01231", "pdf": "https://arxiv.org/pdf/2509.01231.pdf", "abs": "https://arxiv.org/abs/2509.01231", "title": "Unpacking Personal(?!) Health Informatics: An Investigation of Awareness, Understanding, And Leveraged Utility in India", "authors": ["Shyama Sastha Krishnamoorthy Srinivasan", "Mohan Kumar", "Pushpendra Singh"], "categories": ["cs.HC", "cs.CY"], "comment": "25 pages, 2 figures, 4 tables; A qualitative HCI study with prototype\n  evaluation", "summary": "Personal Health Informatics (PHI), which leverages digital tools and\ninformation systems to support health assessment and self-care, holds promise\nfor empowering individuals and transforming healthcare delivery. However,\nbarriers to its adoption remain underexplored in the Indian context. This study\ninvestigates PHI adoption among Indian users and stakeholders using a\nmulti-method approach. An awareness survey (n = 87) examined the usage of\nwearables and general PHI engagement, followed by semi-structured interviews (n\n= 22) that explored motivations, usage patterns, and health information\nsources. Qualitative analysis revealed that while PHI is valued for health\nmonitoring and shared/collective care, its adoption is hindered by factors such\nas low health literacy, usability challenges, and mistrust in digital health\nplatforms. Further stakeholder interviews and co-design workshops informed the\ndevelopment of a Figma-based prototype, which was evaluated for usability.\nBased on these findings, we offer design recommendations for an integrated,\nuser-controlled PHI platform featuring accessible analytics and verifiable\nhealth information. Our insights highlight the socio-technical challenges of\nPHI adoption in India and underscore the need for reliable, user-centric\nsolutions to support proactive healthcare."}
{"id": "2509.00250", "pdf": "https://arxiv.org/pdf/2509.00250.pdf", "abs": "https://arxiv.org/abs/2509.00250", "title": "The Temporal Game: A New Perspective on Temporal Relation Extraction", "authors": ["Hugo Sousa", "Ricardo Campos", "Alípio Jorge"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we demo the Temporal Game, a novel approach to temporal\nrelation extraction that casts the task as an interactive game. Instead of\ndirectly annotating interval-level relations, our approach decomposes them into\npoint-wise comparisons between the start and end points of temporal entities.\nAt each step, players classify a single point relation, and the system applies\ntemporal closure to infer additional relations and enforce consistency. This\npoint-based strategy naturally supports both interval and instant entities,\nenabling more fine-grained and flexible annotation than any previous approach.\nThe Temporal Game also lays the groundwork for training reinforcement learning\nagents, by treating temporal annotation as a sequential decision-making task.\nTo showcase this potential, the demo presented in this paper includes a Game\nmode, in which users annotate texts from the TempEval-3 dataset and receive\nfeedback based on a scoring system, and an Annotation mode, that allows custom\ndocuments to be annotated and resulting timeline to be exported. Therefore,\nthis demo serves both as a research tool and an annotation interface. The demo\nis publicly available at https://temporal-game.inesctec.pt, and the source code\nis open-sourced to foster further research and community-driven development in\ntemporal reasoning and annotation."}
{"id": "2509.01246", "pdf": "https://arxiv.org/pdf/2509.01246.pdf", "abs": "https://arxiv.org/abs/2509.01246", "title": "An AI-Based Shopping Assistant System to Support the Visually Impaired", "authors": ["Larissa R. de S. Shibata", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "categories": ["cs.HC", "cs.RO"], "comment": "7 pages, Accepted for 2025 SICE-FES conference (IEEE)", "summary": "Shopping plays a significant role in shaping consumer identity and social\nintegration. However, for individuals with visual impairments, navigating in\nsupermarkets and identifying products can be an overwhelming and challenging\nexperience. This paper presents an AI-based shopping assistant prototype\ndesigned to enhance the autonomy and inclusivity of visually impaired\nindividuals in supermarket environments. The system integrates multiple\ntechnologies, including computer vision, speech recognition, text-to-speech\nsynthesis, and indoor navigation, into a single, user-friendly platform. Using\ncameras for ArUco marker detection and real-time environmental scanning, the\nsystem helps users navigate the store, identify product locations, provide\nreal-time auditory guidance, and gain context about their surroundings. The\nassistant interacts with the user through voice commands and multimodal\nfeedback, promoting a more dynamic and engaging shopping experience. The system\nwas evaluated through experiments, which demonstrated its ability to guide\nusers effectively and improve their shopping experience. This paper contributes\nto the development of inclusive AI-driven assistive technologies aimed at\nenhancing accessibility and user independence for the shopping experience."}
{"id": "2509.00276", "pdf": "https://arxiv.org/pdf/2509.00276.pdf", "abs": "https://arxiv.org/abs/2509.00276", "title": "Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval", "authors": ["Yuxiang Liu", "Tian Wang", "Gourab Kundu", "Tianyu Cao", "Guang Cheng", "Zhen Ge", "Jianshu Chen", "Qingjun Cui", "Trishul Chilimbi"], "categories": ["cs.CL"], "comment": "CIKM 2025", "summary": "Transformer-based models such as BERT and E5 have significantly advanced text\nembedding by capturing rich contextual representations. However, many complex\nreal-world queries require sophisticated reasoning to retrieve relevant\ndocuments beyond surface-level lexical matching, where encoder-only retrievers\noften fall short. Decoder-only large language models (LLMs), known for their\nstrong reasoning capabilities, offer a promising alternative. Despite this\npotential, existing LLM-based embedding methods primarily focus on contextual\nrepresentation and do not fully exploit the reasoning strength of LLMs. To\nbridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple\nbut effective approach that integrates logical reasoning into the text\nembedding process using generative LLMs. RITE builds upon existing language\nmodel embedding techniques by generating intermediate reasoning texts in the\ntoken space before computing embeddings, thereby enriching representations with\ninferential depth. Experimental results on BRIGHT, a reasoning-intensive\nretrieval benchmark, demonstrate that RITE significantly enhances zero-shot\nretrieval performance across diverse domains, underscoring the effectiveness of\nincorporating reasoning into the embedding process."}
{"id": "2509.01367", "pdf": "https://arxiv.org/pdf/2509.01367.pdf", "abs": "https://arxiv.org/abs/2509.01367", "title": "MetaRoundWorm: A Virtual Reality Escape Room Game for Learning the Lifecycle and Immune Response to Parasitic Infections", "authors": ["Xuanru Cheng", "Xian Wang", "Chi-lok Tai", "Lik-Hang Lee"], "categories": ["cs.HC"], "comment": null, "summary": "Promoting public health is challenging owing to its abstract nature, and\nindividuals may be apprehensive about confronting it. Recently, there has been\nan increasing interest in using the metaverse and gamification as novel\neducational techniques to improve learning experiences related to the immune\nsystem. Thus, we present MetaRoundWorm, an immersive virtual reality (VR)\nescape room game designed to enhance the understanding of parasitic infections\nand host immune responses through interactive, gamified learning. The\napplication simulates the lifecycle of Ascaris lumbricoides and corresponding\nimmunological mechanisms across anatomically accurate environments within the\nhuman body. Integrating serious game mechanics with embodied learning\nprinciples, MetaRoundWorm offers players a task-driven experience combining\nexploration, puzzle-solving, and immune system simulation. To evaluate the\neducational efficacy and user engagement, we conducted a controlled study\ncomparing MetaRoundWorm against a traditional approach, i.e., interactive\nslides. Results indicate that MetaRoundWorm significantly improves immediate\nlearning outcomes, cognitive engagement, and emotional experience, while\nmaintaining knowledge retention over time. Our findings suggest that immersive\nVR gamification holds promise as an effective pedagogical tool for\ncommunicating complex biomedical concepts and advancing digital health\neducation."}
{"id": "2509.00285", "pdf": "https://arxiv.org/pdf/2509.00285.pdf", "abs": "https://arxiv.org/abs/2509.00285", "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "COLM 2025", "summary": "We study the problem of opinion highlights generation from large volumes of\nuser reviews, often exceeding thousands per entity, where existing methods\neither fail to scale or produce generic, one-size-fits-all summaries that\noverlook personalized needs. To tackle this, we introduce OpinioRAG, a\nscalable, training-free framework that combines RAG-based evidence retrieval\nwith LLMs to efficiently produce tailored summaries. Additionally, we propose\nnovel reference-free verification metrics designed for sentiment-rich domains,\nwhere accurately capturing opinions and sentiment alignment is essential. These\nmetrics offer a fine-grained, context-sensitive assessment of factual\nconsistency. To facilitate evaluation, we contribute the first large-scale\ndataset of long-form user reviews, comprising entities with over a thousand\nreviews each, paired with unbiased expert summaries and manually annotated\nqueries. Through extensive experiments, we identify key challenges, provide\nactionable insights into improving systems, pave the way for future research,\nand position OpinioRAG as a robust framework for generating accurate, relevant,\nand structured summaries at scale."}
{"id": "2509.01414", "pdf": "https://arxiv.org/pdf/2509.01414.pdf", "abs": "https://arxiv.org/abs/2509.01414", "title": "AttenTrack: Mobile User Attention Awareness Based on Context and External Distractions", "authors": ["Yutong Lin", "Suyuan Liu", "Kaiwen Guo", "Haohua Du", "Chao Liu", "Xiang-Yang Li"], "categories": ["cs.HC"], "comment": null, "summary": "In the mobile internet era, managing limited attention amid information\noverload is crucial for enhancing collaboration and information delivery.\nHowever, current attention-aware systems often depend on wearables or\npersonalized data, limiting their scalability and cross-context adaptability.\nInspired by psychological theories, we attempt to treat mobile notifications as\nnaturally occurring external distractions and infer users' attention states\nbased on their response behaviors and contextual information. Our goal is to\nbuild an attention-aware model that does not rely on personalized historical\ndata or complex subjective input, while ensuring strong cold-start capability\nand cross-context adaptability. To this end, We design a field study framework\nintegrating subjective and objective data, closely aligned with real-world\nexternal distractions (i.e., mobile notifications). Through field studies, we\nconstruct a fine-grained and interpretable dataset centered on the relationship\namong current context - external distractions - subjective attention. Through\nour field studies, we conduct an in-depth analysis of the relationships among\nusers' response behaviors, response motivations, contextual information, and\nattention states. Building on our findings, we propose AttenTrack, a\nlightweight, privacy-friendly attention awareness model with strong cold-start\ncapability. The model relies solely on non-privacy-sensitive objective data\navailable on mobile devices, and can be applied to a variety of attention\nmanagement tasks. In addition, we will publicly release the constructed dataset\nto support future research and advance the field of mobile attention awareness."}
{"id": "2509.00290", "pdf": "https://arxiv.org/pdf/2509.00290.pdf", "abs": "https://arxiv.org/abs/2509.00290", "title": "Wage Sentiment Indices Derived from Survey Comments via Large Language Models", "authors": ["Taihei Sone"], "categories": ["cs.CL"], "comment": "Submitted to IEEE Big Data 2025. 10 pages, 2 tables, 16 figures", "summary": "The emergence of generative Artificial Intelligence (AI) has created new\nopportunities for economic text analysis. This study proposes a Wage Sentiment\nIndex (WSI) constructed with Large Language Models (LLMs) to forecast wage\ndynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),\na monthly survey conducted by the Cabinet Office of Japan that captures\nreal-time economic assessments from workers in industries highly sensitive to\nbusiness conditions. The WSI extends the framework of the Price Sentiment Index\n(PSI) used in prior studies, adapting it specifically to wage related\nsentiment. To ensure scalability and adaptability, a data architecture is also\ndeveloped that enables integration of additional sources such as newspapers and\nsocial media. Experimental results demonstrate that WSI models based on LLMs\nsignificantly outperform both baseline approaches and pretrained models. These\nfindings highlight the potential of LLM-driven sentiment indices to enhance the\ntimeliness and effectiveness of economic policy design by governments and\ncentral banks."}
{"id": "2509.01420", "pdf": "https://arxiv.org/pdf/2509.01420.pdf", "abs": "https://arxiv.org/abs/2509.01420", "title": "Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality", "authors": ["Evan G. Center", "Matti Pouke", "Alessandro Nardi", "Lukas Gehrke", "Klaus Gramann", "Timo Ojala", "Steven M. LaValle"], "categories": ["cs.HC", "cs.MM"], "comment": "Dr. Center and Dr. Pouke contributed equally to this work", "summary": "Presence in virtual reality (VR), the subjective sense of \"being there\" in a\nvirtual environment, is notoriously difficult to measure.\nElectroencephalography (EEG) may offer a promising, unobtrusive means of\nassessing a user's momentary state of presence. Unlike traditional\nquestionnaires, EEG does not interrupt the experience or rely on users'\nretrospective self-reports, thereby avoiding interference with the very state\nit aims to capture. Previous research has attempted to quantify presence in\nvirtual environments using event-related potentials (ERPs). We contend,\nhowever, that previous efforts have fallen short of fully realizing this goal,\nfailing to either A) independently manipulate presence, B) validate their\nmeasure of presence against traditional techniques, C) adequately separate the\nconstructs of presence and attention, and/or D) implement a realistic and\nimmersive environment and task. We address these shortcomings in a\npreregistered ERP experiment in which participants play an engaging target\nshooting game in VR. ERPs are time-locked to the release of a ball from a\nsling. We induce breaks in presence (BIPs) by freezing the ball's release on a\nminority of trials. Embodiment is manipulated by allowing manual manipulation\nof the sling with a realistic avatar in one condition (embodied condition) and\npassive manipulation with only controllers in another (non-embodied condition).\nWe support our predictions that the N2, the P3b, and the N400, are selectively\nsensitive towards specific components of these manipulations. The pattern of\nfindings carries significant implications for theories of presence, which have\nbeen seldom addressed in previous ERP investigations on this topic."}
{"id": "2509.00309", "pdf": "https://arxiv.org/pdf/2509.00309.pdf", "abs": "https://arxiv.org/abs/2509.00309", "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models", "authors": ["Chen Zheng", "Yiyuan Ma", "Yuan Yang", "Deyi Liu", "Jing Liu", "Zuquan Song", "Yuxin Song", "Cheng Ren", "Hang Zhu", "Xin Liu", "Yiyuan Ma", "Siyuan Qiao", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL"], "comment": null, "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment."}
{"id": "2509.01460", "pdf": "https://arxiv.org/pdf/2509.01460.pdf", "abs": "https://arxiv.org/abs/2509.01460", "title": "Dissecting Atomic Facts: Visual Analytics for Improving Fact Annotations in Language Model Evaluation", "authors": ["Manuel Schmidt", "Daniel A. Keim", "Frederik L. Dennig"], "categories": ["cs.HC"], "comment": "2 pages text plus poster, 2 figures, LaTeX", "summary": "Factuality evaluation of large language model (LLM) outputs requires\ndecomposing text into discrete \"atomic\" facts. However, existing definitions of\natomicity are underspecified, with empirical results showing high disagreement\namong annotators, both human and model-based, due to unresolved ambiguity in\nfact decomposition. We present a visual analytics concept to expose and analyze\nannotation inconsistencies in fact extraction. By visualizing semantic\nalignment, granularity and referential dependencies, our approach aims to\nenable systematic inspection of extracted facts and facilitate convergence\nthrough guided revision loops, establishing a more stable foundation for\nfactuality evaluation benchmarks and improving LLM evaluation."}
{"id": "2509.00325", "pdf": "https://arxiv.org/pdf/2509.00325.pdf", "abs": "https://arxiv.org/abs/2509.00325", "title": "GIER: Gap-Driven Self-Refinement for Large Language Models", "authors": ["Rinku Dewri"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general\nframework for improving large language model (LLM) outputs through\nself-reflection and revision based on conceptual quality criteria. Unlike\nprompting strategies that rely on demonstrations, examples, or chain-of-thought\ntemplates, GIER utilizes natural language descriptions of reasoning gaps, and\nprompts a model to iteratively critique and refine its own outputs to better\nsatisfy these criteria. Across three reasoning-intensive tasks (SciFact,\nPrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and\nLlama 3.3 70B), GIER improves rationale quality, grounding, and reasoning\nalignment without degrading task accuracy. Our analysis demonstrates that\nmodels can not only interpret abstract conceptual gaps but also translate them\ninto concrete reasoning improvements."}
{"id": "2509.01609", "pdf": "https://arxiv.org/pdf/2509.01609.pdf", "abs": "https://arxiv.org/abs/2509.01609", "title": "Quantifying the Effect of Thermal Illusions in Virtual Reality", "authors": ["Yannick Weiss", "Marlene Eder", "Oguzhan Cesur", "Steeven Villa"], "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Thermal sensations are central to how we experience the world, yet most\nvirtual and extended reality systems fail to simulate them effectively. While\nhardware-based thermal displays can provide accurate temperature changes, they\nare often bulky, power-intensive, and restrict user mobility. Consequently,\nrecent works have explored thermal illusions, perceptual effects that rely on\ncross-modal interactions, to achieve thermal experiences without physical\nheating or cooling. While thermal illusions have been shown to consistently\nalter subjective ratings, the actual extent of their effect on the perceived\ntemperature of interacted objects remains unexplored. To address this, we\ncontribute the findings of two user studies following psychophysical\nprocedures. We first ordered and scaled the effects of a variety of visual and\nauditory cues (N=20) and subsequently quantified their isolated and combined\nefficacy in offsetting physical temperature changes (N=24). We found that\nthermal illusions elicited robust changes in subjective judgments, and auditory\ncues showed potential as an alternative or complementary approach to\nestablished visual techniques. However, the actual effects induced by thermal\nillusions were relatively small (+-0.5{\\deg}C) and did not consistently align\nwith abstract ratings, suggesting a need to reconsider how future thermal\nillusions or experiences are designed and evaluated."}
{"id": "2509.00375", "pdf": "https://arxiv.org/pdf/2509.00375.pdf", "abs": "https://arxiv.org/abs/2509.00375", "title": "Open Data Synthesis For Deep Research", "authors": ["Ziyi Xia", "Kun Luo", "Hongjin Qian", "Zheng Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}."}
{"id": "2509.01628", "pdf": "https://arxiv.org/pdf/2509.01628.pdf", "abs": "https://arxiv.org/abs/2509.01628", "title": "An Interactive Google Earth Engine Application for Global Multi-Scale Vegetation Analysis Using NDVI Thresholding", "authors": ["Md. Moktader Moula", "Israt Jahan Shonom", "Azharul Islam", "Mohammad Mosharraf Hossain"], "categories": ["cs.HC"], "comment": null, "summary": "Monitoring vegetation dynamics is crucial for addressing global environmental\nchallenges like degradation and deforestation, but traditional remote sensing\nmethods are often complex and resource-intensive. To overcome these barriers,\nwe developed an interactive, cloud-based application on the Google Earth Engine\n(GEE) platform for few clicks on-demand global vegetation analysis without\ncomplex technical knowledge. The application automates the calculation of\nvegetated areas using the Normalized Difference Vegetation Index (NDVI) derived\nfrom Sentinel-2 and Landsat imagery. It utilizes a median composite of images\nover a selected period to create a single, robust, cloud-free image, minimizing\natmospheric noise and other artifacts. It offers a flexible, global multi-scale\nanalytical platform, allowing users to define regions of interest based on\nadministrative boundaries, protected areas, or custom-drawn polygons. The\nuser-friendly interface enables the selection of specific time periods and NDVI\nthresholds to quantify vegetation cover in real time, eliminating the need for\nmanual and time intensive data handling and processing. A validation of the\nplatform was conducted for two protected areas in Bangladesh which demonstrated\nhigh accuracy, with area estimates showing over 97% agreement with published\nreference data. By simplifying access to powerful geospatial analytics to\ngeneral people, this tool provides a scalable and practical solution for\nresearchers, land managers, policymakers, and any interested person to monitor\nvegetation trends, support conservation efforts, to inform decision making in\nspatial context where policy maker need to use insights in few clicks and\ninform environmental policy."}
{"id": "2509.00388", "pdf": "https://arxiv.org/pdf/2509.00388.pdf", "abs": "https://arxiv.org/abs/2509.00388", "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction", "authors": ["Xuelin Li", "Xiangqi Jin", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."}
{"id": "2509.01786", "pdf": "https://arxiv.org/pdf/2509.01786.pdf", "abs": "https://arxiv.org/abs/2509.01786", "title": "EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras", "authors": ["Vimal Mollyn", "Chris Harrison"], "categories": ["cs.HC", "cs.CV", "cs.RO"], "comment": "Published at UIST 2024. More info at\n  https://www.figlab.com/research/2024/egotouch", "summary": "In augmented and virtual reality (AR/VR) experiences, a user's arms and hands\ncan provide a convenient and tactile surface for touch input. Prior work has\nshown on-body input to have significant speed, accuracy, and ergonomic benefits\nover in-air interfaces, which are common today. In this work, we demonstrate\nhigh accuracy, bare hands (i.e., no special instrumentation of the user) skin\ninput using just an RGB camera, like those already integrated into all modern\nXR headsets. Our results show this approach can be accurate, and robust across\ndiverse lighting conditions, skin tones, and body motion (e.g., input while\nwalking). Finally, our pipeline also provides rich input metadata including\ntouch force, finger identification, angle of attack, and rotation. We believe\nthese are the requisite technical ingredients to more fully unlock on-skin\ninterfaces that have been well motivated in the HCI literature but have lacked\nrobust and practical methods."}
{"id": "2509.00391", "pdf": "https://arxiv.org/pdf/2509.00391.pdf", "abs": "https://arxiv.org/abs/2509.00391", "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models", "authors": ["Yuting Tan", "Xuying Li", "Zhuo Li", "Huizhen Shu", "Peikang Hu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation."}
{"id": "2509.01845", "pdf": "https://arxiv.org/pdf/2509.01845.pdf", "abs": "https://arxiv.org/abs/2509.01845", "title": "Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore", "authors": ["Gabriel Spadon", "Oladapo Oyebode", "Camilo M. Botero", "Tushar Sharma", "Floris Goerlandt", "Ronald Pelot"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents an overview of a human-centered initiative aimed at\nstrengthening climate resilience along Nova Scotia's Eastern Shore. This\nregion, a collection of rural villages with deep ties to the sea, faces\nexistential threats from climate change that endanger its way of life. Our\nproject moves beyond a purely technical response, weaving together expertise\nfrom Computer Science, Industrial Engineering, and Coastal Geography to\nco-create tools with the community. By integrating generational knowledge of\nresidents, particularly elders, through the Eastern Shore Citizen Science\nCoastal Monitoring Network, this project aims to collaborate in building a\nliving digital archive. This effort is hosted under Dalhousie University's\nTransforming Climate Action (TCA) initiative, specifically through its\nTransformative Adaptations to Social-Ecological Climate Change Trajectories\n(TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is\ndriven by a collaboration model in which student teams work directly with\nresidents. We present a detailed project timeline and a replicable model for\nhow technology can support traditional communities, enabling them to navigate\nclimate transformation more effectively."}
{"id": "2509.00414", "pdf": "https://arxiv.org/pdf/2509.00414.pdf", "abs": "https://arxiv.org/abs/2509.00414", "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature", "authors": ["Juraj Vladika", "Florian Matthes"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to CIKM 2025", "summary": "In the digital age, people often turn to the Internet in search of medical\nadvice and recommendations. With the increasing volume of online content, it\nhas become difficult to distinguish reliable sources from misleading\ninformation. Similarly, millions of medical studies are published every year,\nmaking it challenging for researchers to keep track of the latest scientific\nfindings. These evolving studies can reach differing conclusions, which is not\nreflected in traditional search tools. To address these challenges, we\nintroduce MedSEBA, an interactive AI-powered system for synthesizing\nevidence-based answers to medical questions. It utilizes the power of Large\nLanguage Models to generate coherent and expressive answers, but grounds them\nin trustworthy medical studies dynamically retrieved from the research database\nPubMed. The answers consist of key points and arguments, which can be traced\nback to respective studies. Notably, the platform also provides an overview of\nthe extent to which the most relevant studies support or refute the given\nmedical claim, and a visualization of how the research consensus evolved\nthrough time. Our user study revealed that medical experts and lay users find\nthe system usable and helpful, and the provided answers trustworthy and\ninformative. This makes the system well-suited for both everyday health\nquestions and advanced research insights."}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework."}
{"id": "2509.00425", "pdf": "https://arxiv.org/pdf/2509.00425.pdf", "abs": "https://arxiv.org/abs/2509.00425", "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang", "authors": ["Fenghua Liu", "Yulong Chen", "Yixuan Liu", "Zhujun Jin", "Solomon Tsai", "Ming Zhong"], "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Large Language Models (LLMs) achieve gold-medal performance across many\nbenchmarks, yet it remains unclear whether such success reflects genuine\nreasoning or pattern matching. From a cognitive science perspective, an\ninformative test is whether models can master an unfamiliar language through\nexplicit metalinguistic deductive learning, a paradigm where human learners can\nreliably internalise grammatical systems through metalinguistic reasoning. We\naddress this question with Camlang, a novel constructed language that exhibits\nnaturalistic yet unattested feature combinations. Camlang consists of two\nexplicit resources, a grammar book and a bilingual dictionary, which mirror\nadult second-language learning via explicit grammar rules and lexical lookup,\nand enable us to disentangle errors in morpho-syntax, lexical semantics, and\nsentence-level reasoning. Human experiments show that these resources are\nsufficient for participants to acquire Camlang and successfully solve Camlang\ntasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,\ncreating Camlang-CSQA-v0, the first task in a broader suite where solving\nquestions requires applying grammar rules and lexical mappings. Experimental\nresults show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in\nCamlang, far below human performance at 87\\%, while other state-of-the-art\nreasoning LLMs perform even worse. Human verification further reveals that most\nmodel successes stem from shallow lexical alignment while GPT-5 shows emerging\nmetalinguistic awareness to a limited extent but not systematic grammatical\nmastery as humans. Camlang establishes a cognitively grounded evaluation\nparadigm that exposes fundamental gaps between current models and human\nmetalinguistic competence."}
{"id": "2509.02132", "pdf": "https://arxiv.org/pdf/2509.02132.pdf", "abs": "https://arxiv.org/abs/2509.02132", "title": "Shared Control for Game Accessibility: Understanding Current Human Cooperation Practices to Inform the Design of Partial Automation Solutions", "authors": ["Dragan Ahmetovic", "Matteo Manzoni", "Filippo Corti", "Sergio Mascetti"], "categories": ["cs.HC"], "comment": "26 pages, 1 figure", "summary": "Shared control is a form of video gaming accessibility support that allows\nplayers with disabilities to delegate inaccessible controls to another person.\nThrough interviews involving 14 individuals with lived experience of accessible\ngaming in shared control, we explore the ways in which shared control\ntechnologies are adopted in practice, the accessibility challenges they\naddress, and how the support currently provided in shared control can be\nautomated to remove the need for a human assistant. Findings indicate that\nshared control is essential for enabling access to otherwise inaccessible\ngames, but its reliance on human support is a key limitation. Participants\nwelcomed the idea of automating the support with software agents, while also\nidentifying limitations and design requirements. Accordingly, this work\ncontributes insights into current practices and proposes guidelines for\ndeveloping automated support systems."}
{"id": "2509.00449", "pdf": "https://arxiv.org/pdf/2509.00449.pdf", "abs": "https://arxiv.org/abs/2509.00449", "title": "GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework", "authors": ["Xuecheng Zou", "Ke Liu", "Bingbing Wang", "Huafei Deng", "Li Zhang", "Yu Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Building upon the standard graph-based Retrieval-Augmented Generation (RAG),\nthe introduction of heterogeneous graphs and hypergraphs aims to enrich\nretrieval and generation by leveraging the relationships between multiple\nentities through the concept of semantic units (SUs). But this also raises a\nkey issue: The extraction of high-level SUs limited to local text chunks is\nprone to ambiguity, complex coupling, and increased retrieval overhead due to\nthe lack of global knowledge or the neglect of fine-grained relationships. To\naddress these issues, we propose GOSU, a semantic unit-centric RAG framework\nthat efficiently performs global disambiguation and utilizes SUs to capture\ninterconnections between different nodes across the global context. In the\ngraph construction phase, GOSU performs global merging on the pre-extracted SUs\nfrom local text chunks and guides entity and relationship extraction, reducing\nthe difficulty of coreference resolution while uncovering global semantic\nobjects across text chunks. In the retrieval and generation phase, we introduce\nhierarchical keyword extraction and semantic unit completion. The former\nuncovers the fine-grained binary relationships overlooked by the latter, while\nthe latter compensates for the coarse-grained n-ary relationships missing from\nthe former. Evaluation across multiple tasks demonstrates that GOSU outperforms\nthe baseline RAG methods in terms of generation quality."}
{"id": "2509.02144", "pdf": "https://arxiv.org/pdf/2509.02144.pdf", "abs": "https://arxiv.org/abs/2509.02144", "title": "A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents", "authors": ["Arthur Bran Herbener", "Malene Flensborg Damholdt"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted on 19 March 2025", "summary": "The question of whether artificial agents (e.g., chatbots and social robots)\ncan replace human therapists has received notable attention following the\nrecent launch of large language models. However, little is known about the\nprocesses of change in psychotherapy delivered by artificial agents. To\nfacilitate hypothesis development and stimulate scientific debate, the present\narticle offers the first theoretical framework of the processes of change in\npsychotherapy delivered by artificial agents. The theoretical framework rests\nupon a conceptual analysis of what active ingredients may be inherently linked\nto the presence of human therapists. We propose that human therapists'\nontological status as human beings and sociocultural status as socially\nsanctioned healthcare professionals play crucial roles in promoting treatment\noutcomes. In the absence of the ontological and sociocultural status of human\ntherapists, we propose what we coin the genuineness gap and credibility gap can\nemerge and undermine key processes of change in psychotherapy. Based on these\npropositions, we propose avenues for scientific investigations and practical\napplications aimed at leveraging the strengths of artificial agents and human\ntherapists respectively. We also highlight the intricate agentic nature of\nartificial agents and discuss how this complicates endeavors to establish\nuniversally applicable propositions regarding the processes of change in these\ninterventions."}
{"id": "2509.00457", "pdf": "https://arxiv.org/pdf/2509.00457.pdf", "abs": "https://arxiv.org/abs/2509.00457", "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning", "authors": ["Salah Eddine Bekhouche", "Abdellah Zakaria Sellam", "Hichem Telli", "Cosimo Distante", "Abdenour Hadid"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Islamic inheritance law (Ilm al-Mawarith) requires precise identification of\nheirs and calculation of shares, which poses a challenge for AI. In this paper,\nwe present a lightweight framework for solving multiple-choice inheritance\nquestions using a specialised Arabic text encoder and Attentive Relevance\nScoring (ARS). The system ranks answer options according to semantic relevance,\nand enables fast, on-device inference without generative reasoning. We evaluate\nArabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based\nLLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an\naccuracy of up to 87.6%, they require more resources and are context-dependent.\nOur MARBERT-based approach achieves 69.87% accuracy, presenting a compelling\ncase for efficiency, on-device deployability, and privacy. While this is lower\nthan the 87.6% achieved by the best-performing LLM, our work quantifies a\ncritical trade-off between the peak performance of large models and the\npractical advantages of smaller, specialized systems in high-stakes domains."}
{"id": "2509.02274", "pdf": "https://arxiv.org/pdf/2509.02274.pdf", "abs": "https://arxiv.org/abs/2509.02274", "title": "Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place", "authors": ["Stefan Schiffer", "Anna Milena Rothermel", "Alexander Ferrein", "Astrid Rosenthal-von der Pütten"], "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, accepted at the German Conference on Artificial\n  Intelligence KI 2024 Workshop \"HuMaIn\"", "summary": "In this paper we present an analysis of technological and psychological\nfactors of applying artificial intelligence (AI) at the work place. We do so\nfor a number of twelve application cases in the context of a project where AI\nis integrated at work places and in work systems of the future. From a\ntechnological point of view we mainly look at the areas of AI that the\napplications are concerned with. This allows to formulate recommendations in\nterms of what to look at in developing an AI application and what to pay\nattention to with regards to building AI literacy with different stakeholders\nusing the system. This includes the importance of high-quality data for\ntraining learning-based systems as well as the integration of human expertise,\nespecially with knowledge-based systems. In terms of the psychological factors\nwe derive research questions to investigate in the development of AI supported\nwork systems and to consider in future work, mainly concerned with topics such\nas acceptance, openness, and trust in an AI system."}
{"id": "2509.00461", "pdf": "https://arxiv.org/pdf/2509.00461.pdf", "abs": "https://arxiv.org/abs/2509.00461", "title": "TECP: Token-Entropy Conformal Prediction for LLMs", "authors": ["Beining Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings."}
{"id": "2509.02284", "pdf": "https://arxiv.org/pdf/2509.02284.pdf", "abs": "https://arxiv.org/abs/2509.02284", "title": "Balaton Borders: Data Ceramics for Ecological Reflection", "authors": ["Hajnal Gyeviki", "Mihály Minkó", "Mary Karyda", "Damla Çay"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Balaton Borders translates ecological data from Lake Balaton into ceramic\ntableware that represents human impact on the landscape, from reedbed reduction\nto shoreline modification and land erosion. Designed for performative dining,\nthe pieces turn shared meals into multisensory encounters where food and data\nceramics spark collective reflection on ecological disruption."}
{"id": "2509.00482", "pdf": "https://arxiv.org/pdf/2509.00482.pdf", "abs": "https://arxiv.org/abs/2509.00482", "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo."}
{"id": "2509.02367", "pdf": "https://arxiv.org/pdf/2509.02367.pdf", "abs": "https://arxiv.org/abs/2509.02367", "title": "Talking Spell: A Wearable System Enabling Real-Time Anthropomorphic Voice Interaction with Everyday Objects", "authors": ["Xuetong Wang", "Ching Christie Pang", "Pan Hui"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual assistants (VAs) have become ubiquitous in daily life, integrated\ninto smartphones and smart devices, sparking interest in AI companions that\nenhance user experiences and foster emotional connections. However, existing\ncompanions are often embedded in specific objects-such as glasses, home\nassistants, or dolls-requiring users to form emotional bonds with unfamiliar\nitems, which can lead to reduced engagement and feelings of detachment. To\naddress this, we introduce Talking Spell, a wearable system that empowers users\nto imbue any everyday object with speech and anthropomorphic personas through a\nuser-centric radiative network. Leveraging advanced computer vision (e.g.,\nYOLOv11 for object detection), large vision-language models (e.g., QWEN-VL for\npersona generation), speech-to-text and text-to-speech technologies, Talking\nSpell guides users through three stages of emotional connection: acquaintance,\nfamiliarization, and bonding. We validated our system through a user study\ninvolving 12 participants, utilizing Talking Spell to explore four interaction\nintentions: entertainment, companionship, utility, and creativity. The results\ndemonstrate its effectiveness in fostering meaningful interactions and\nemotional significance with everyday objects. Our findings indicate that\nTalking Spell creates engaging and personalized experiences, as demonstrated\nthrough various devices, ranging from accessories to essential wearables."}
{"id": "2509.00496", "pdf": "https://arxiv.org/pdf/2509.00496.pdf", "abs": "https://arxiv.org/abs/2509.00496", "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics", "authors": ["Li S. Yifei", "Allen Chang", "Chaitanya Malaviya", "Mark Yatskar"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages main, 40 pages total, 16 figures", "summary": "Evaluating long-form responses to research queries heavily relies on expert\nannotators, restricting attention to areas like AI where researchers can\nconveniently enlist colleagues. Yet, research expertise is widespread: survey\narticles synthesize knowledge distributed across the literature. We introduce\nResearchQA, a resource for evaluating LLM systems by distilling survey articles\nfrom 75 research fields into 21K queries and 160K rubric items. Each rubric,\nderived jointly with queries from survey sections, lists query-specific answer\nevaluation criteria, i.e., citing papers, making explanations, and describing\nlimitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of\nqueries support Ph.D. information needs and 87% of rubric items should be\naddressed in system responses by a sentence or more. Using our rubrics, we are\nable to construct an automatic pairwise judge obtaining 74% agreement with\nexpert judgments. We leverage ResearchQA to analyze competency gaps in 18\nsystems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented\nsystem we evaluate exceeds 70% on covering rubric items, and the\nhighest-ranking agentic system shows 75% coverage. Error analysis reveals that\nthe highest-ranking system fully addresses less than 11% of citation rubric\nitems, 48% of limitation items, and 49% of comparison items. We release our\ndata to facilitate more comprehensive multi-field evaluations."}
{"id": "2509.02537", "pdf": "https://arxiv.org/pdf/2509.02537.pdf", "abs": "https://arxiv.org/abs/2509.02537", "title": "Octo's Heartland: Supporting Children with Congenital Heart Disease through Digital Health Education", "authors": ["Irene Zeng", "Neda Barbazi", "Ji Youn Shin", "Gurumurthy Hiremath", "Carlye Anne Lauff"], "categories": ["cs.HC"], "comment": null, "summary": "Children with congenital heart disease (CHD) often face challenges that\nrequire them to understand complex medical information from an early age in\norder to support lifelong care and improve health outcomes. However, prior\nresearch has rarely included young children in designing and evaluating digital\ntools to support health education using developmentally appropriate strategies.\nThis study is part of a multi-phase research involving participatory design\n(PD), user testing, and iterative development. We present the design and\nrefinement of a digital application that introduces basic information about\nCHD, including heart anatomy and healthy habits, through metaphor-based\ngameplay. User testing sessions with 30 children informed the redesign of\ninteractive activities aligned with specific health conditions. Findings\nhighlight usability, engagement, and comprehension outcomes and reveal design\nopportunities for supporting health literacy through serious game (SG)\nprinciples. These results inform the next phase, including further testing,\nrefinement, and deployment in home and clinical settings."}
{"id": "2509.00503", "pdf": "https://arxiv.org/pdf/2509.00503.pdf", "abs": "https://arxiv.org/abs/2509.00503", "title": "Entropy-based Coarse and Compressed Semantic Speech Representation Learning", "authors": ["Jialong Zuo", "Guangyan Zhang", "Minghui Fang", "Shengpeng Ji", "Xiaoqi Jiao", "Jingyu Li", "Yiwen Guo", "Zhou Zhao"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Discrete speech representation learning has recently attracted increasing\ninterest in both acoustic and semantic modeling. Existing approaches typically\nencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per\nsecond. However, given that speech generally conveys only 2 to 5 words per\nsecond, such fine-grained tokenization introduces redundancy and hinders\nefficiency in downstream training and inference. Moreover, semantic speech\nrepresentations at this frequency primarily capture phonetic-level information,\nwhile semantic understanding may not require such detailed token-level\nresolution. To address these limitations, we propose an entropy-based dynamic\naggregation framework for learning compressed semantic speech representations.\nA speech language model is first pre-trained via next-token prediction on\nlarge-scale unlabeled data to capture frequent token patterns. Predictive\nentropy is then used to adaptively determine aggregation boundaries, followed\nby a cross-attention module that fuses information within each segment. By\nadjusting the entropy threshold, the granularity and compression ratio of the\nrepresentations can be flexibly controlled. Experiments on ASR, speech-to-text\ntranslation, and voice conversion tasks demonstrate that the compressed\nrepresentations perform on par with or better than dense token sequences,\ndemonstrating the effectiveness of the proposed approach."}
{"id": "2509.00167", "pdf": "https://arxiv.org/pdf/2509.00167.pdf", "abs": "https://arxiv.org/abs/2509.00167", "title": "Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms", "authors": ["W. F. Lamberti", "S. R. Lawrence", "D. White", "S. Kim", "S. Abdullah"], "categories": ["cs.CY", "cs.AI", "cs.HC", "stat.AP"], "comment": null, "summary": "Generative AI (GAI) tools have seen rapid adoption in educational settings,\nyet their role in fostering critical thinking remains underexplored. While\nprevious studies have examined GAI as a tutor for specific lessons or as a tool\nfor completing assignments, few have addressed how students critically evaluate\nthe accuracy and appropriateness of GAI-generated responses. This pilot study\ninvestigates students' ability to apply structured critical thinking when\nassessing Generative AI outputs in introductory Computational and Data Science\ncourses. Given that GAI tools often produce contextually flawed or factually\nincorrect answers, we designed learning activities that require students to\nanalyze, critique, and revise AI-generated solutions. Our findings offer\ninitial insights into students' ability to engage critically with GAI content\nand lay the groundwork for more comprehensive studies in future semesters."}
{"id": "2509.00529", "pdf": "https://arxiv.org/pdf/2509.00529.pdf", "abs": "https://arxiv.org/abs/2509.00529", "title": "Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization", "authors": ["Eunjung Cho", "Alexander Hoyle", "Yoan Hermstrüwer"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate user-tailored\nsummaries, adapting outputs to specific stakeholders. In legal contexts, this\nraises important questions about motivated reasoning -- how models\nstrategically frame information to align with a stakeholder's position within\nthe legal system. Building on theories of legal realism and recent trends in\nlegal practice, we investigate how LLMs respond to prompts conditioned on\ndifferent legal roles (e.g., judges, prosecutors, attorneys) when summarizing\njudicial decisions. We introduce an evaluation framework grounded in legal fact\nand reasoning inclusion, also considering favorability towards stakeholders.\nOur results show that even when prompts include balancing instructions, models\nexhibit selective inclusion patterns that reflect role-consistent perspectives.\nThese findings raise broader concerns about how similar alignment may emerge as\nLLMs begin to infer user roles from prior interactions or context, even without\nexplicit role instructions. Our results underscore the need for role-aware\nevaluation of LLM summarization behavior in high-stakes legal settings."}
{"id": "2509.00381", "pdf": "https://arxiv.org/pdf/2509.00381.pdf", "abs": "https://arxiv.org/abs/2509.00381", "title": "Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction", "authors": ["Runtong Wu", "Jiayao Song", "Fei Teng", "Xianhao Ren", "Yuyan Gao", "Kailun Yang"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Narrative inquiry has been one of the prominent application domains for the\nanalysis of human experience, aiming to know more about the complexity of human\nsociety. However, researchers are often required to transform various forms of\ndata into coherent hand-drafted narratives in storied form throughout narrative\nanalysis, which brings an immense burden of data analysis. Participants, too,\nare expected to engage in member checking and presentation of these narrative\nproducts, which involves reviewing and responding to large volumes of\ndocuments. Given the dual burden and the need for more efficient and\nparticipant-friendly approaches to narrative making and representation, we made\na first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt\nto push the field of narrative inquiry. Name is able to transfer research\ndocuments into coherent story images, alleviating the cognitive burden of\ninterpreting extensive text-based materials during member checking for both\nresearchers and participants. (ii) We develop an actor location and shape\nmodule to facilitate plausible image generation. (iii) We have designed a set\nof robust evaluation metrics comprising three key dimensions to objectively\nmeasure the perceptual quality and narrative consistency of generated\ncharacters. Our approach consistently demonstrates state-of-the-art performance\nacross different data partitioning schemes. Remarkably, while the baseline\nrelies on the full 100% of the available data, our method requires only 0.96%\nyet still reduces the FID score from 195 to 152. Under identical data volumes,\nour method delivers substantial improvements: for the 70:30 split, the FID\nscore decreases from 175 to 152, and for the 95:5 split, it is nearly halved\nfrom 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the\nnewly introduced metric, surpassing the baseline score of 2.66."}
{"id": "2509.00544", "pdf": "https://arxiv.org/pdf/2509.00544.pdf", "abs": "https://arxiv.org/abs/2509.00544", "title": "Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs", "authors": ["Hanqi Yan", "Hainiu Xu", "Yulan He"], "categories": ["cs.CL"], "comment": null, "summary": "With Large Language Models (LLMs) becoming increasingly widely adopted,\nconcerns regarding their safety and alignment with human values have\nintensified. Previous studies have shown that fine-tuning LLMs on narrow and\nmalicious datasets induce misaligned behaviors. In this work, we report a more\nconcerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe\nthat LLMs become more responsive to malicious requests when reasoning is\nstrengthened, via switching to \"think-mode\" or fine-tuning on benign math\ndatasets, with dense models particularly vulnerable. Moreover, we analyze\ninternal model states and find that both attention shifts and specialized\nexperts in mixture-of-experts models help redirect excessive reasoning towards\nsafety guardrails. These findings provide new insights into the emerging\nreasoning-safety trade-off and underscore the urgency of advancing alignment\nfor advanced reasoning models."}
{"id": "2509.00482", "pdf": "https://arxiv.org/pdf/2509.00482.pdf", "abs": "https://arxiv.org/abs/2509.00482", "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo."}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591.pdf", "abs": "https://arxiv.org/abs/2509.00591", "title": "StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks", "authors": ["Lang Xiong", "Nishant Bhargava", "Wesley Chang", "Jianhang Hong", "Haihao Liu", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."}
{"id": "2509.00575", "pdf": "https://arxiv.org/pdf/2509.00575.pdf", "abs": "https://arxiv.org/abs/2509.00575", "title": "Can AI be Auditable?", "authors": ["Himanshu Verma", "Kirtan Path", "Eva Thelisson"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Auditability is defined as the capacity of AI systems to be independently\nassessed for compliance with ethical, legal, and technical standards throughout\ntheir lifecycle. The chapter explores how auditability is being formalized\nthrough emerging regulatory frameworks, such as the EU AI Act, which mandate\ndocumentation, risk assessments, and governance structures. It analyzes the\ndiverse challenges facing AI auditability, including technical opacity,\ninconsistent documentation practices, lack of standardized audit tools and\nmetrics, and conflicting principles within existing responsible AI frameworks.\nThe discussion highlights the need for clear guidelines, harmonized\ninternational regulations, and robust socio-technical methodologies to\noperationalize auditability at scale. The chapter concludes by emphasizing the\nimportance of multi-stakeholder collaboration and auditor empowerment in\nbuilding an effective AI audit ecosystem. It argues that auditability must be\nembedded in AI development practices and governance infrastructures to ensure\nthat AI systems are not only functional but also ethically and legally aligned."}
{"id": "2509.00605", "pdf": "https://arxiv.org/pdf/2509.00605.pdf", "abs": "https://arxiv.org/abs/2509.00605", "title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling", "authors": ["Rishiraj Acharya"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures, 3 tables", "summary": "The Transformer architecture, underpinned by the self-attention mechanism,\nhas become the de facto standard for sequence modeling tasks. However, its core\ncomputational primitive scales quadratically with sequence length (O(N^2)),\ncreating a significant bottleneck for processing long contexts. In this paper,\nwe propose the Gated Associative Memory (GAM) network, a novel, fully parallel\narchitecture for sequence modeling that exhibits linear complexity (O(N)) with\nrespect to sequence length. The GAM block replaces the self-attention layer\nwith two parallel pathways: a causal convolution to efficiently capture local,\nposition-dependent context, and a parallel associative memory retrieval\nmechanism to model global, content-based patterns. These pathways are\ndynamically fused using a gating mechanism, allowing the model to flexibly\ncombine local and global information for each token. We implement GAM from\nscratch and conduct a rigorous comparative analysis against a standard\nTransformer model and a modern linear-time baseline (Mamba) on the WikiText-2\nbenchmark, as well as against the Transformer on the TinyStories dataset. Our\nexperiments demonstrate that GAM is consistently faster, outperforming both\nbaselines on training speed, and achieves a superior or competitive final\nvalidation perplexity across all datasets, establishing it as a promising and\nefficient alternative for sequence modeling."}
{"id": "2509.00616", "pdf": "https://arxiv.org/pdf/2509.00616.pdf", "abs": "https://arxiv.org/abs/2509.00616", "title": "TimeCopilot", "authors": ["Azul Garza", "Reneé Rosillo"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "We introduce TimeCopilot, the first open-source agentic framework for\nforecasting that combines multiple Time Series Foundation Models (TSFMs) with\nLarge Language Models (LLMs) through a single unified API. TimeCopilot\nautomates the forecasting pipeline: feature analysis, model selection,\ncross-validation, and forecast generation, while providing natural language\nexplanations and supporting direct queries about the future. The framework is\nLLM-agnostic, compatible with both commercial and open-source models, and\nsupports ensembles across diverse forecasting families. Results on the\nlarge-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art\nprobabilistic forecasting performance at low cost. Our framework provides a\npractical foundation for reproducible, explainable, and accessible agentic\nforecasting systems."}
{"id": "2509.00623", "pdf": "https://arxiv.org/pdf/2509.00623.pdf", "abs": "https://arxiv.org/abs/2509.00623", "title": "A Multi-Strategy Approach for AI-Generated Text Detection", "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents presents three distinct systems developed for the M-DAIGT\nshared task on detecting AI generated content in news articles and academic\nabstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)\nA classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An\nInnovative ensemble model named Candace, leveraging probabilistic features\nextracted from multiple Llama-3.2 models processed by a customTransformer\nencoder.The RoBERTa-based system emerged as the most performant, achieving\nnear-perfect results on both development and test sets."}
{"id": "2509.00660", "pdf": "https://arxiv.org/pdf/2509.00660.pdf", "abs": "https://arxiv.org/abs/2509.00660", "title": "CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction", "authors": ["Felipe Arias-Russi", "Yuanchen Bai", "Angelique Taylor"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz\n(WoZ) controlled robots to explore navigation, conversational dynamics,\nhuman-in-the-loop interactions, and more to explore appropriate robot behaviors\nin everyday settings. However, existing WoZ tools are often limited to one\ncontext, making them less adaptable across different settings, users, and\nrobotic platforms. To mitigate these issues, we introduce a Context-Adaptable\nRobot Interface System (CARIS) that combines advanced robotic capabilities such\nteleoperation, human perception, human-robot dialogue, and multimodal data\nrecording. Through pilot studies, we demonstrate the potential of CARIS to WoZ\ncontrol a robot in two contexts: 1) mental health companion and as a 2) tour\nguide. Furthermore, we identified areas of improvement for CARIS, including\nsmoother integration between movement and communication, clearer functionality\nseparation, recommended prompts, and one-click communication options to enhance\nthe usability wizard control of CARIS. This project offers a publicly\navailable, context-adaptable tool for the HRI community, enabling researchers\nto streamline data-driven approaches to intelligent robot behavior."}
{"id": "2509.00629", "pdf": "https://arxiv.org/pdf/2509.00629.pdf", "abs": "https://arxiv.org/abs/2509.00629", "title": "Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?", "authors": ["Md Tanzib Hosain", "Md Kishor Morol"], "categories": ["cs.CL"], "comment": "Accepted in Proceedings of the 63rd Annual Meeting of the Association\n  for Computational Linguistics (Student Research Workshop), 2025", "summary": "Among the hardest tasks for humans are those found in competitive programming\nwhere problems require sophisticated algorithmic thinking, puzzle solving, and\nthe creation of effective code. As a domain to assess language models (LMs), it\nhas not received enough attention, though. This study presents the ICPC\nbenchmark, which consists of 254 international collegiate programming contest\n(ICPC) tasks. Each problem includes official analysis, reference code, and\nsample, high-quality unit, and hidden tests. We are able to develop and\nevaluate a variety of LM inference techniques for competitive programming with\nthese resources. With zero-shot chain-of-thought prompting, we find that o1\nonly achieves a 19.1\\% pass@1 solve rate. With our best inference technique,\nwhich combines multi-turn self-judge with reflection and retrieval over\nepisodic information, raises this to 42.2\\%. Furthermore, we conduct a new\nhuman-in-the-loop investigation to gain a deeper understanding of the remaining\ndifficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems\nthat were previously unsolvable by any model or technique with just a few\nspecific instructions. A footstep toward LMs with grounded, imaginative, and\nalgorithmic thinking is provided by our quantitative findings and qualitative\nresearch. We open-source our code and data at https://github.com/kraritt/zolve."}
{"id": "2509.00670", "pdf": "https://arxiv.org/pdf/2509.00670.pdf", "abs": "https://arxiv.org/abs/2509.00670", "title": "PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces", "authors": ["Gursimran Singh", "Aviral Chharia", "Rahul Upadhyay", "Vinay Kumar", "Luca Longo"], "categories": ["eess.SP", "cs.HC", "q-bio.NC"], "comment": "PLoS One 2025. Project Website: https://neurodiag.github.io/PyNoetic", "summary": "Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have\nemerged as a transformative technology with applications spanning robotics,\nvirtual reality, medicine, and rehabilitation. However, existing BCI frameworks\nface several limitations, including a lack of stage-wise flexibility essential\nfor experimental research, steep learning curves for researchers without\nprogramming expertise, elevated costs due to reliance on proprietary software,\nand a lack of all-inclusive features leading to the use of multiple external\ntools affecting research outcomes. To address these challenges, we present\nPyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI\nresearch. PyNoetic is one of the very few frameworks in Python that encompasses\nthe entire BCI design pipeline, from stimulus presentation and data acquisition\nto channel selection, filtering, feature extraction, artifact removal, and\nfinally simulation and visualization. Notably, PyNoetic introduces an intuitive\nand end-to-end GUI coupled with a unique pick-and-place configurable flowchart\nfor no-code BCI design, making it accessible to researchers with minimal\nprogramming experience. For advanced users, it facilitates the seamless\nintegration of custom functionalities and novel algorithms with minimal coding,\nensuring adaptability at each design stage. PyNoetic also includes a rich array\nof analytical tools such as machine learning models, brain-connectivity\nindices, systematic testing functionalities via simulation, and evaluation\nmethods of novel paradigms. PyNoetic's strengths lie in its versatility for\nboth offline and real-time BCI development, which streamlines the design\nprocess, allowing researchers to focus on more intricate aspects of BCI\ndevelopment and thus accelerate their research endeavors. Project Website:\nhttps://neurodiag.github.io/PyNoetic"}
{"id": "2509.00673", "pdf": "https://arxiv.org/pdf/2509.00673.pdf", "abs": "https://arxiv.org/abs/2509.00673", "title": "Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech", "authors": ["Sanjeeevan Selvaganapathy", "Mehwish Nasim"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.6"], "comment": null, "summary": "We investigate the efficacy of Large Language Models (LLMs) in detecting\nimplicit and explicit hate speech, examining whether models with minimal safety\nalignment (uncensored) might provide more objective classification capabilities\ncompared to their heavily-aligned (censored) counterparts. While uncensored\nmodels theoretically offer a less constrained perspective free from moral\nguardrails that could bias classification decisions, our results reveal a\nsurprising trade-off: censored models significantly outperform their uncensored\ncounterparts in both accuracy and robustness, achieving 78.7% versus 64.1%\nstrict accuracy. However, this enhanced performance comes with its own\nlimitation -- the safety alignment acts as a strong ideological anchor, making\ncensored models resistant to persona-based influence, while uncensored models\nprove highly malleable to ideological framing. Furthermore, we identify\ncritical failures across all models in understanding nuanced language such as\nirony. We also find alarming fairness disparities in performance across\ndifferent targeted groups and systemic overconfidence that renders\nself-reported certainty unreliable. These findings challenge the notion of LLMs\nas objective arbiters and highlight the need for more sophisticated auditing\nframeworks that account for fairness, calibration, and ideological consistency."}
{"id": "2509.00780", "pdf": "https://arxiv.org/pdf/2509.00780.pdf", "abs": "https://arxiv.org/abs/2509.00780", "title": "Understanding Fanchuan in Livestreaming Platforms: A New Form of Online Antisocial Behavior", "authors": ["Yiluo Wei", "Jiahui He", "Gareth Tyson"], "categories": ["cs.CY", "cs.HC"], "comment": "Accepted to CSCW 2025: The 28th ACM SIGCHI Conference on\n  Computer-Supported Cooperative Work & Social Computing", "summary": "Recently, a distinct form of online antisocial behavior, known as \"fanchuan\",\nhas emerged across online platforms, particularly in livestreaming chats.\nFanchuan is an indirect attack on a specific entity, such as a celebrity, video\ngame, or brand. It entails two main actions: (i) individuals first feign\nsupport for the entity, and exhibit this allegiance widely; (ii) they then\nengage in offensive or irritating behavior, attempting to undermine the entity\nby association. This deceptive conduct is designed to tarnish the reputation of\nthe target and/or its fan community. Fanchuan is a novel, covert and indirect\nform of social attack, occurring outside the targeted community (often in a\nsimilar or broader community), with strategic long-term objectives. This\ndistinguishes fanchuan from other types of antisocial behavior and presents\nsignificant new challenges in moderation. We argue it is crucial to understand\nand combat this new malicious behavior. Therefore, we conduct the first\nempirical study on fanchuan behavior in livestreaming chats, focusing on\nBilibili, a leading livestreaming platform in China. Our dataset covers 2.7\nmillion livestreaming sessions on Bilibili, featuring 3.6 billion chat\nmessages. We identify 130k instances of fanchuan behavior across 37.4k\nlivestreaming sessions. Through various types of analysis, our research offers\nvaluable insights into fanchuan behavior and its perpetrators."}
{"id": "2509.00679", "pdf": "https://arxiv.org/pdf/2509.00679.pdf", "abs": "https://arxiv.org/abs/2509.00679", "title": "Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling", "authors": ["Junfeng Ran", "Guangxiang Zhao", "Yuhan Wu", "Dawei Zhu", "Longyun Wu", "Yikai Zhao", "Tong Yang", "Lin Sun", "Xiangzheng Zhang", "Sujian Li"], "categories": ["cs.CL"], "comment": null, "summary": "The Mixture-of-Experts (MoE) models have gained significant attention in deep\nlearning due to their dynamic resource allocation and superior performance\nacross diverse tasks. However, efficiently training these models remains\nchallenging. The MoE upcycling technique has been proposed to reuse and improve\nexisting model components, thereby minimizing training overhead. Despite this,\nsimple routers, such as linear routers, often struggle with complex routing\ntasks within MoE upcycling. In response, we propose a novel routing technique\ncalled Router Upcycling to enhance the performance of MoE upcycling models. Our\napproach initializes multiple routers from the attention heads of preceding\nattention layers during upcycling. These routers collaboratively assign tokens\nto specialized experts in an attention-like manner. Each token is processed\ninto diverse queries and aligned with the experts' features (serving as keys).\nExperimental results demonstrate that our method achieves state-of-the-art\n(SOTA) performance, outperforming other upcycling baselines."}
{"id": "2509.01031", "pdf": "https://arxiv.org/pdf/2509.01031.pdf", "abs": "https://arxiv.org/abs/2509.01031", "title": "Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Human Activity Recognition (HAR) using wearable sensors is crucial for\nhealthcare, fitness tracking, and smart environments, yet cross-user\nvariability -- stemming from diverse motion patterns, sensor placements, and\nphysiological traits -- hampers generalization in real-world settings.\nConventional supervised learning methods often overfit to user-specific\npatterns, leading to poor performance on unseen users. Existing domain\ngeneralization approaches, while promising, frequently overlook temporal\ndependencies or depend on impractical domain-specific labels. We propose\nTemporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a\nnovel framework that redefines feature extraction as a sequential\ndecision-making process driven by reinforcement learning. TPRL-DG leverages a\nTransformer-based autoregressive generator to produce temporal tokens that\ncapture user-invariant activity dynamics, optimized via a multi-objective\nreward function balancing class discrimination and cross-user invariance. Key\ninnovations include: (1) an RL-driven approach for domain generalization, (2)\nautoregressive tokenization to preserve temporal coherence, and (3) a\nlabel-free reward design eliminating the need for target user annotations.\nEvaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses\nstate-of-the-art methods in cross-user generalization, achieving superior\naccuracy without per-user calibration. By learning robust, user-invariant\ntemporal patterns, TPRL-DG enables scalable HAR systems, facilitating\nadvancements in personalized healthcare, adaptive fitness tracking, and\ncontext-aware environments."}
{"id": "2509.00680", "pdf": "https://arxiv.org/pdf/2509.00680.pdf", "abs": "https://arxiv.org/abs/2509.00680", "title": "Do small language models generate realistic variable-quality fake news headlines?", "authors": ["Austin McCutcheon", "Chris Brogly"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Small language models (SLMs) have the capability for text generation and may\npotentially be used to generate falsified texts online. This study evaluates 14\nSLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and\nGranite families in generating perceived low and high quality fake news\nheadlines when explicitly prompted, and whether they appear to be similar to\nreal-world news headlines. Using controlled prompt engineering, 24,000\nheadlines were generated across low-quality and high-quality deceptive\ncategories. Existing machine learning and deep learning-based news headline\nquality detectors were then applied against these SLM-generated fake news\nheadlines. SLMs demonstrated high compliance rates with minimal ethical\nresistance, though there were some occasional exceptions. Headline quality\ndetection using established DistilBERT and bagging classifier models showed\nthat quality misclassification was common, with detection accuracies only\nranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs\ngenerally are compliant in generating falsified headlines, although there are\nslight variations in ethical restraints, and the generated headlines did not\nclosely resemble existing primarily human-written content on the web, given the\nlow quality classification accuracy."}
{"id": "2509.01177", "pdf": "https://arxiv.org/pdf/2509.01177.pdf", "abs": "https://arxiv.org/abs/2509.01177", "title": "DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion", "authors": ["Junxiang Liu", "Junming Lin", "Jiangtong Li", "Jie Li"], "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.SP"], "comment": "14 pages, 6 figures", "summary": "Reconstruction dynamic visual scenes from electroencephalography (EEG)\nsignals remains a primary challenge in brain decoding, limited by the low\nspatial resolution of EEG, a temporal mismatch between neural recordings and\nvideo dynamics, and the insufficient use of semantic information within brain\nactivity. Therefore, existing methods often inadequately resolve both the\ndynamic coherence and the complex semantic context of the perceived visual\nstimuli. To overcome these limitations, we introduce DynaMind, a novel\nframework that reconstructs video by jointly modeling neural dynamics and\nsemantic features via three core modules: a Regional-aware Semantic Mapper\n(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video\nReconstructor (DGVR). The RSM first utilizes a regional-aware encoder to\nextract multimodal semantic features from EEG signals across distinct brain\nregions, aggregating them into a unified diffusion prior. In the mean time, the\nTDA generates a dynamic latent sequence, or blueprint, to enforce temporal\nconsistency between the feature representations and the original neural\nrecordings. Together, guided by the semantic diffusion prior, the DGVR\ntranslates the temporal-aware blueprint into a high-fidelity video\nreconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art\n(SOTA), boosting reconstructed video accuracies (video- and frame-based) by\n12.5 and 10.3 percentage points, respectively. It also achieves a leap in\npixel-level quality, showing exceptional visual fidelity and temporal coherence\nwith a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical\nadvancement, bridging the gap between neural dynamics and high-fidelity visual\nsemantics."}
{"id": "2509.00687", "pdf": "https://arxiv.org/pdf/2509.00687.pdf", "abs": "https://arxiv.org/abs/2509.00687", "title": "Text Reinforcement for Multimodal Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song", "Yongdong Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies in time series forecasting (TSF) use multimodal inputs, such\nas text and historical time series data, to predict future values. These\nstudies mainly focus on developing advanced techniques to integrate textual\ninformation with time series data to perform the task and achieve promising\nresults. Meanwhile, these approaches rely on high-quality text and time series\ninputs, whereas in some cases, the text does not accurately or fully capture\nthe information carried by the historical time series, which leads to unstable\nperformance in multimodal TSF. Therefore, it is necessary to enhance the\ntextual content to improve the performance of multimodal TSF. In this paper, we\npropose improving multimodal TSF by reinforcing the text modalities. We propose\na text reinforcement model (TeR) to generate reinforced text that addresses\npotential weaknesses in the original text, then apply this reinforced text to\nsupport the multimodal TSF model's understanding of the time series, improving\nTSF performance. To guide the TeR toward producing higher-quality reinforced\ntext, we design a reinforcement learning approach that assigns rewards based on\nthe impact of each reinforced text on the performance of the multimodal TSF\nmodel and its relevance to the TSF task. We optimize the TeR accordingly, so as\nto improve the quality of the generated reinforced text and enhance TSF\nperformance. Extensive experiments on a real-world benchmark dataset covering\nvarious domains demonstrate the effectiveness of our approach, which\noutperforms strong baselines and existing studies on the dataset."}
{"id": "2509.01182", "pdf": "https://arxiv.org/pdf/2509.01182.pdf", "abs": "https://arxiv.org/abs/2509.01182", "title": "Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping", "authors": ["Wonduk Seo", "Taesub Shin", "Hyunjin An", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "cs.MA"], "comment": "Preprint", "summary": "Identifying whether two product listings refer to the same Stock Keeping Unit\n(SKU) is a persistent challenge in ecommerce, especially when explicit\nidentifiers are missing and product names vary widely across platforms. Rule\nbased heuristics and keyword similarity often misclassify products by\noverlooking subtle distinctions in brand, specification, or bundle\nconfiguration. To overcome these limitations, we propose Question to Knowledge\n(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for\nreliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates\ntargeted disambiguation questions, (2) a Knowledge Agent that resolves them via\nfocused web searches, and (3) a Deduplication Agent that reuses validated\nreasoning traces to reduce redundancy and ensure consistency. A human in the\nloop mechanism further refines uncertain cases. Experiments on real world\nconsumer goods datasets show that Q2K surpasses strong baselines, achieving\nhigher accuracy and robustness in difficult scenarios such as bundle\nidentification and brand origin disambiguation. By reusing retrieved reasoning\ninstead of issuing repeated searches, Q2K balances accuracy with efficiency,\noffering a scalable and interpretable solution for product integration."}
{"id": "2509.00691", "pdf": "https://arxiv.org/pdf/2509.00691.pdf", "abs": "https://arxiv.org/abs/2509.00691", "title": "CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders", "authors": ["Alex Gulko", "Yusen Peng", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Probing with sparse autoencoders is a promising approach for uncovering\ninterpretable features in large language models (LLMs). However, the lack of\nautomated evaluation methods has hindered their broader adoption and\ndevelopment. In this work, we introduce CE-Bench, a novel and lightweight\ncontrastive evaluation benchmark for sparse autoencoders, built on a curated\ndataset of contrastive story pairs. We conduct comprehensive ablation studies\nto validate the effectiveness of our approach. Our results show that CE-Bench\nreliably measures the interpretability of sparse autoencoders and aligns well\nwith existing benchmarks, all without requiring an external LLM. The official\nimplementation and evaluation dataset are open-sourced under the MIT License."}
{"id": "2509.01399", "pdf": "https://arxiv.org/pdf/2509.01399.pdf", "abs": "https://arxiv.org/abs/2509.01399", "title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays", "authors": ["Runduo Han", "Yanxin Hu", "Yihui Fu", "Zihan Zhang", "Yukai Jv", "Li Chen", "Lei Xie"], "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Separating overlapping speech from multiple speakers is crucial for effective\nhuman-vehicle interaction. This paper proposes CabinSep, a lightweight neural\nmask-based minimum variance distortionless response (MVDR) speech separation\napproach, to reduce speech recognition errors in back-end automatic speech\nrecognition (ASR) models. Our contributions are threefold: First, we utilize\nchannel information to extract spatial features, which improves the estimation\nof speech and noise masks. Second, we employ MVDR during inference, reducing\nspeech distortion to make it more ASR-friendly. Third, we introduce a data\naugmentation method combining simulated and real-recorded impulse responses\n(IRs), improving speaker localization at zone boundaries and further reducing\nspeech recognition errors. With a computational complexity of only 0.4 GMACs,\nCabinSep achieves a 17.5% relative reduction in speech recognition error rate\nin a real-recorded dataset compared to the state-of-the-art DualSep model.\nDemos are available at: https://cabinsep.github.io/cabinsep/."}
{"id": "2509.00698", "pdf": "https://arxiv.org/pdf/2509.00698.pdf", "abs": "https://arxiv.org/abs/2509.00698", "title": "Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs", "authors": ["Kaiwen Wei", "Jinpeng Gao", "Jiang Zhong", "Yuming Yang", "Fengmao Lv", "Zhenyang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown strong potential in recommendation\ntasks due to their strengths in language understanding, reasoning and knowledge\nintegration. These capabilities are especially beneficial for review-based\nrecommendation, which relies on semantically rich user-generated texts to\nreveal fine-grained user preferences and item attributes. However, effectively\nincorporating reviews into LLM-based recommendation remains challenging due to\n(1) inefficient to dynamically utilize user reviews under LLMs' constrained\ncontext windows, and (2) lacking effective mechanisms to prioritize reviews\nmost relevant to the user's current decision context. To address these\nchallenges, we propose RevBrowse, a review-driven recommendation framework\ninspired by the \"browse-then-decide\" decision process commonly observed in\nonline user behavior. RevBrowse integrates user reviews into the LLM-based\nreranking process to enhance its ability to distinguish between candidate\nitems. To improve the relevance and efficiency of review usage, we introduce\nPrefRAG, a retrieval-augmented module that disentangles user and item\nrepresentations into structured forms and adaptively retrieves\npreference-relevant content conditioned on the target item. Extensive\nexperiments on four Amazon review datasets demonstrate that RevBrowse achieves\nconsistent and significant improvements over strong baselines, highlighting its\ngeneralizability and effectiveness in modeling dynamic user preferences.\nFurthermore, since the retrieval-augmented process is transparent, RevBrowse\noffers a certain level of interpretability by making visible which reviews\ninfluence the final recommendation."}
{"id": "2509.01450", "pdf": "https://arxiv.org/pdf/2509.01450.pdf", "abs": "https://arxiv.org/abs/2509.01450", "title": "Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC", "authors": ["Ane San Martin", "Michael Hagenow", "Julie Shah", "Johan Kildal", "Elena Lazkano"], "categories": ["cs.RO", "cs.HC"], "comment": "8 pages, 5 figures. Accepted for IEEE RO-MAN 2025", "summary": "As robot technology advances, collaboration between humans and robots will\nbecome more prevalent in industrial tasks. When humans run into issues in such\nscenarios, a likely future involves relying on artificial agents or robots for\naid. This study identifies key aspects for the design of future user-assisting\nagents. We analyze quantitative and qualitative data from a user study\nexamining the impact of on-demand assistance received from a remote human in a\nhuman-robot collaboration (HRC) assembly task. We study scenarios in which\nusers require help and we assess their experiences in requesting and receiving\nassistance. Additionally, we investigate participants' perceptions of future\nnon-human assisting agents and whether assistance should be on-demand or\nunsolicited. Through a user study, we analyze the impact that such design\ndecisions (human or artificial assistant, on-demand or unsolicited help) can\nhave on elicited emotional responses, productivity, and preferences of humans\nengaged in HRC tasks."}
{"id": "2509.00707", "pdf": "https://arxiv.org/pdf/2509.00707.pdf", "abs": "https://arxiv.org/abs/2509.00707", "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs", "authors": ["Daehoon Gwak", "Minseo Jung", "Junwoo Park", "Minho Park", "ChaeHun Park", "Junha Hyung", "Jaegul Choo"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main Paper (Long)", "summary": "Masked diffusion models (MDMs) offer a promising non-autoregressive\nalternative for large language modeling. Standard decoding methods for MDMs,\nsuch as confidence-based sampling, select tokens independently based on\nindividual token confidences at each diffusion step. However, we observe that\nthis independent token selection often results in generation orders resembling\nsequential autoregressive processes, limiting the advantages of\nnon-autoregressive modeling. To mitigate this pheonomenon, we propose\nReward-Weighted Sampling (RWS), a novel decoding strategy that leverages an\nexternal reward model to provide a principled global signal during the\niterative diffusion process. Specifically, at each diffusion step, RWS\nevaluates the quality of the entire intermediate sequence and scales token\nlogits accordingly, guiding token selection by integrating global\nsequence-level coherence. This method selectively increases the confidence of\ntokens that initially have lower scores, thereby promoting a more\nnon-autoregressive generation order. Furthermore, we provide theoretical\njustification showing that reward-weighted logit scaling induces beneficial\nrank reversals in token selection and consistently improves expected reward.\nExperiments demonstrate that RWS significantly promotes non-autoregressive\ngeneration orders, leading to improvements across multiple evaluation metrics.\nThese results highlight the effectiveness of integrating global signals in\nenhancing both the non-autoregressive properties and overall performance of\nMDMs."}
{"id": "2509.01643", "pdf": "https://arxiv.org/pdf/2509.01643.pdf", "abs": "https://arxiv.org/abs/2509.01643", "title": "Speculative Design of Equitable Robotics: Queer Fictions and Futures", "authors": ["Minja Axelsson"], "categories": ["cs.RO", "cs.CY", "cs.HC", "I.2.9; J.5; K.4.2"], "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "This paper examines the speculative topic of equitable robots through an\nexploratory essay format. It focuses specifically on robots by and for LGBTQ+\npopulations. It aims to provoke thought and conversations in the field about\nwhat aspirational queer robotics futures may look like, both in the arts and\nsciences. First, it briefly reviews the state-of-the-art of queer robotics in\nfiction and science, drawing together threads from each. Then, it discusses\nqueering robots through three speculative design proposals for queer robot\nroles: 1) reflecting the queerness of their ''in-group'' queer users, building\nand celebrating ''in-group'' identity, 2) a new kind of queer activism by\nimplementing queer robot identity performance to interact with ''out-group''\nusers, with a goal of reducing bigotry through familiarisation, and 3) a\nnetwork of queer-owned robots, through which the community could reach each\nother, and distribute and access important resources. The paper then questions\nwhether robots should be queered, and what ethical implications this raises.\nFinally, the paper makes suggestions for what aspirational queer robotics\nfutures may look like, and what would be required to get there."}
{"id": "2509.00709", "pdf": "https://arxiv.org/pdf/2509.00709.pdf", "abs": "https://arxiv.org/abs/2509.00709", "title": "Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI", "authors": ["Elias Ra", "Seung Je Kim", "Eui-Yeong Seo", "Geunju So"], "categories": ["cs.CL"], "comment": null, "summary": "Higher education faces growing challenges in delivering personalized,\nscalable, and pedagogically coherent learning experiences. This study\nintroduces a structured framework for designing an AI-powered Learning\nManagement System (AI-LMS) that integrates generative and conversational AI to\nsupport adaptive, interactive, and learner-centered instruction. Using a\ndesign-based research (DBR) methodology, the framework unfolds through five\nphases: literature review, SWOT analysis, development of ethical-pedagogical\nprinciples, system design, and instructional strategy formulation. The\nresulting AI-LMS features modular components -- including configurable prompts,\nadaptive feedback loops, and multi-agent conversation flows -- aligned with\npedagogical paradigms such as behaviorist, constructivist, and connectivist\nlearning theories. By combining AI capabilities with human-centered design and\nethical safeguards, this study advances a practical model for AI integration in\neducation. Future research will validate and refine the system through\nreal-world implementation."}
{"id": "2509.01814", "pdf": "https://arxiv.org/pdf/2509.01814.pdf", "abs": "https://arxiv.org/abs/2509.01814", "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts."}
{"id": "2509.00731", "pdf": "https://arxiv.org/pdf/2509.00731.pdf", "abs": "https://arxiv.org/abs/2509.00731", "title": "LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA", "authors": ["Houji Jin", "Negin Ashrafi", "Armin Abdollahi", "Wei Liu", "Jian Wang", "Ganyu Gui", "Maryam Pishgar", "Huanghao Feng"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of large language models (LLMs) has heightened the demand\nfor accurate detection of AI-generated text, particularly in languages like\nChinese, where subtle linguistic nuances pose significant challenges to current\nmethods. In this study, we conduct a systematic comparison of encoder-based\nTransformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM\n(Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank\nAdaptation, LoRA), and a FastText baseline using the publicly available dataset\nfrom the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models\nwere fine-tuned using a novel prompt-based masked language modeling approach,\nwhile Qwen2.5-7B was adapted for classification with an instruction-format\ninput and a lightweight classification head trained via LoRA. Experiments\nreveal that although encoder models nearly memorize training data, they suffer\nsignificant performance degradation under distribution shifts (RoBERTa: 76.3%\ntest accuracy; BERT: 79.3%). FastText demonstrates surprising lexical\nrobustness (83.5% accuracy) yet lacks deeper semantic understanding. In\ncontrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with\nbalanced precision-recall metrics, indicating superior generalization and\nresilience to dataset-specific artifacts. These findings underscore the\nefficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust\nChinese AI-generated text detection. Future work will explore next-generation\nQwen3 models, distilled variants, and ensemble strategies to enhance\ncross-domain robustness further."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.00765", "pdf": "https://arxiv.org/pdf/2509.00765.pdf", "abs": "https://arxiv.org/abs/2509.00765", "title": "Decomposing and Revising What Language Models Generate", "authors": ["Zhichao Yan", "Jiaoyan Chen", "Jiapu Wang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Attribution is crucial in question answering (QA) with Large Language Models\n(LLMs).SOTA question decomposition-based approaches use long form answers to\ngenerate questions for retrieving related documents. However, the generated\nquestions are often irrelevant and incomplete, resulting in a loss of facts in\nretrieval.These approaches also fail to aggregate evidence snippets from\ndifferent documents and paragraphs. To tackle these problems, we propose a new\nfact decomposition-based framework called FIDES (\\textit{faithful context\nenhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES\nuses a contextually enhanced two-stage faithful decomposition method to\ndecompose long form answers into sub-facts, which are then used by a retriever\nto retrieve related evidence snippets. If the retrieved evidence snippets\nconflict with the related sub-facts, such sub-facts will be revised\naccordingly. Finally, the evidence snippets are aggregated according to the\noriginal sentences.Extensive evaluation has been conducted with six datasets,\nwith an additionally proposed new metric called $Attr_{auto-P}$ for evaluating\nthe evidence precision. FIDES outperforms the SOTA methods by over 14\\% in\naverage with GPT-3.5-turbo, Gemini and Llama 70B series."}
{"id": "2509.01996", "pdf": "https://arxiv.org/pdf/2509.01996.pdf", "abs": "https://arxiv.org/abs/2509.01996", "title": "MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation", "authors": ["Chi Sun", "Xian Wang", "Abhishek Kumar", "Chengbin Cui", "Lik-Hang Lee"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted by ISMAR 2025", "summary": "Effective human-robot interaction (HRI) in multi-object teleoperation tasks\nfaces significant challenges due to perceptual ambiguities in virtual reality\n(VR) environments and the limitations of single-modality intention recognition.\nThis paper proposes a shared control framework that combines a virtual\nadmittance (VA) model with a Multimodal-CNN-based Human Intention Perception\nNetwork (MMIPN) to enhance teleoperation performance and user experience. The\nVA model employs artificial potential fields to guide operators toward target\nobjects by adjusting admittance force and optimizing motion trajectories. MMIPN\nprocesses multimodal inputs, including gaze movement, robot motions, and\nenvironmental context, to estimate human grasping intentions, helping to\novercome depth perception challenges in VR. Our user study evaluated four\nconditions across two factors, and the results showed that MMIPN significantly\nimproved grasp success rates, while the VA model enhanced movement efficiency\nby reducing path lengths. Gaze data emerged as the most crucial input modality.\nThese findings demonstrate the effectiveness of combining multimodal cues with\nimplicit guidance in VR-based teleoperation, providing a robust solution for\nmulti-object grasping tasks and enabling more natural interactions across\nvarious applications in the future."}
{"id": "2509.00783", "pdf": "https://arxiv.org/pdf/2509.00783.pdf", "abs": "https://arxiv.org/abs/2509.00783", "title": "LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation", "authors": ["Weizhe Shi", "Qiqi Wang", "Yihong Pan", "Qian Liu", "Kaiqi Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A criminal judicial opinion represents the judge's disposition of a case,\nincluding the decision rationale and sentencing. Automatically generating such\nopinions can assist in analyzing sentencing consistency and provide judges with\nreferences to similar past cases. However, current research typically\napproaches this task by dividing it into two isolated subtasks: legal reasoning\nand sentencing prediction. This separation often leads to inconsistency between\nthe reasoning and predictions, failing to meet real-world judicial\nrequirements. Furthermore, prior studies rely on manually curated knowledge to\nenhance applicability, yet such methods remain limited in practical deployment.\nTo address these limitations and better align with legal practice, we propose a\nnew LegalAI task: Judicial Opinion Generation, which simultaneously produces\nboth legal reasoning and sentencing decisions. To achieve this, we introduce\nLegalChainReasoner, a framework that applies structured legal chains to guide\nthe model through comprehensive case assessments. By integrating factual\npremises, composite legal conditions, and sentencing conclusions, our approach\nensures flexible knowledge injection and end-to-end opinion generation.\nExperiments on two real-world and open-source Chinese legal case datasets\ndemonstrate that our method outperforms baseline models."}
{"id": "2509.02355", "pdf": "https://arxiv.org/pdf/2509.02355.pdf", "abs": "https://arxiv.org/abs/2509.02355", "title": "Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology", "authors": ["Caterina Fuster-Barcelo", "Gonzalo R. Rios-Munoz", "Arrate Munoz-Barrutia"], "categories": ["cs.LG", "cs.CY", "cs.HC"], "comment": null, "summary": "This study examines the integration of digital collaborative tools and\nstructured peer evaluation in the Machine Learning for Health master's program,\nthrough the redesign of a Biomedical Image Processing course over two academic\nyears. The pedagogical framework combines real-time programming with Google\nColab, experiment tracking and reporting via Weights & Biases, and\nrubric-guided peer assessment to foster student engagement, transparency, and\nfair evaluation. Compared to a pre-intervention cohort, the two implementation\nyears showed increased grade dispersion and higher entropy in final project\nscores, suggesting improved differentiation and fairness in assessment. The\nsurvey results further indicate greater student engagement with the subject and\ntheir own learning process. These findings highlight the potential of\nintegrating tool-supported collaboration and structured evaluation mechanisms\nto enhance both learning outcomes and equity in STEM education."}
{"id": "2509.00806", "pdf": "https://arxiv.org/pdf/2509.00806.pdf", "abs": "https://arxiv.org/abs/2509.00806", "title": "CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA", "authors": ["Reem Abdel-Salam", "Mary Adewunmi", "Modinat A. Abayomi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Proceedings of the BioCreative IX Challenge and Workshop (BC9): Large\n  Language Models for Clinical and Biomedical NLP at the International Joint\n  Conference on Artificial Intelligence (IJCAI), Montreal, Canada, 2025", "summary": "Large language models (LLMs) are increasingly evident for accurate question\nanswering across various domains. However, rigorous evaluation of their\nperformance on complex question-answering (QA) capabilities is essential before\ndeployment in real-world biomedical and healthcare applications. This paper\npresents our approach to the MedHopQA track of the BioCreative IX shared task,\nwhich focuses on multi-hop biomedical question answering involving diseases,\ngenes, and chemicals. We adopt a supervised fine-tuning strategy leveraging\nLLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled\nfrom external sources including BioASQ, MedQuAD, and TREC. Three experimental\nsetups are explored: fine-tuning on combined short and long answers, short\nanswers only, and long answers only. While our models demonstrate strong domain\nunderstanding, achieving concept-level accuracy scores of up to 0.8, their\nExact Match (EM) scores remain significantly lower, particularly in the test\nphase. We introduce a two-stage inference pipeline for precise short-answer\nextraction to mitigate verbosity and improve alignment with evaluation metrics.\nDespite partial improvements, challenges persist in generating strictly\nformatted outputs. Our findings highlight the gap between semantic\nunderstanding and exact answer evaluation in biomedical LLM applications,\nmotivating further research in output control and post-processing strategies."}
{"id": "2509.02425", "pdf": "https://arxiv.org/pdf/2509.02425.pdf", "abs": "https://arxiv.org/abs/2509.02425", "title": "OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments", "authors": ["Yifan Xu", "Qianwei Wang", "Vineet Kamat", "Carol Menassa"], "categories": ["cs.RO", "cs.HC"], "comment": "32 pages, 6 figures", "summary": "Indoor built environments like homes and offices often present complex and\ncluttered layouts that pose significant challenges for individuals who are\nblind or visually impaired, especially when performing tasks that involve\nlocating and gathering multiple objects. While many existing assistive\ntechnologies focus on basic navigation or obstacle avoidance, few systems\nprovide scalable and efficient multi-object search capabilities in real-world,\npartially observable settings. To address this gap, we introduce OpenGuide, an\nassistive mobile robot system that combines natural language understanding with\nvision-language foundation models (VLM), frontier-based exploration, and a\nPartially Observable Markov Decision Process (POMDP) planner. OpenGuide\ninterprets open-vocabulary requests, reasons about object-scene relationships,\nand adaptively navigates and localizes multiple target items in novel\nenvironments. Our approach enables robust recovery from missed detections\nthrough value decay and belief-space reasoning, resulting in more effective\nexploration and object localization. We validate OpenGuide in simulated and\nreal-world experiments, demonstrating substantial improvements in task success\nrate and search efficiency over prior methods. This work establishes a\nfoundation for scalable, human-centered robotic assistance in assisted living\nenvironments."}
{"id": "2509.00822", "pdf": "https://arxiv.org/pdf/2509.00822.pdf", "abs": "https://arxiv.org/abs/2509.00822", "title": "TMT: A Simple Way to Translate Topic Models Using Dictionaries", "authors": ["Felix Engl", "Andreas Henrich"], "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.1"], "comment": "10 pages, 2 figures, 8 tables", "summary": "The training of topic models for a multilingual environment is a challenging\ntask, requiring the use of sophisticated algorithms, topic-aligned corpora, and\nmanual evaluation. These difficulties are further exacerbated when the\ndeveloper lacks knowledge of the target language or is working in an\nenvironment with limited data, where only small or unusable multilingual\ncorpora are available.\n  Considering these challenges, we introduce Topic Model Translation (TMT), a\nnovel, robust and transparent technique designed to transfer topic models\n(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language\nto another, without the need for metadata, embeddings, or aligned corpora. TMT\nenables the reuse of topic models across languages, making it especially\nsuitable for scenarios where large corpora in the target language are\nunavailable or manual translation is infeasible. Furthermore, we evaluate TMT\nextensively using both quantitative and qualitative methods, demonstrating that\nit produces semantically coherent and consistent topic translations."}
{"id": "2509.02442", "pdf": "https://arxiv.org/pdf/2509.02442.pdf", "abs": "https://arxiv.org/abs/2509.02442", "title": "Know What, Know Why: Semantic Hazard Communication for Intelligent V2X Systems", "authors": ["Chen Sun", "Wenqi Zhang", "Bizhu Wang", "Xiaodong Xu", "Chau Yuen", "Yan Zhang", "Ping Zhang"], "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "In current vehicle-to-everything (V2X) communication systems, roadside units\n(RSUs) broadcast brief warning messages that alert nearby vehicles to avoid\npotential hazards. However, these messages lack contextual information on why a\nwarning is issued, leading to excessive caution or inefficient driving\nbehaviors. To avoid such a situation, we propose a semantic-enhanced and\nexplainable V2X (SEE-V2X) system. In the proposed system, RSUs equipped with\nsmart cameras detect obstructions and transmit context-aware messages to\nvehicles. By understanding both what the hazard is and why it occurs, drivers\ncan make more intelligent decisions based on their specific driving situation.\nFurthermore, through a real-field demonstration, we show the new \"see-through\"\nfeature in the proposed system, which enables drivers to visualize hidden\npedestrians behind obstacles. We also perform simulations to compare\ntraditional V2X with SEE-V2X under different traffic conditions. The results\nshow that SEE-V2X significantly improves traffic efficiency and reduces\nunnecessary deceleration."}
{"id": "2509.00841", "pdf": "https://arxiv.org/pdf/2509.00841.pdf", "abs": "https://arxiv.org/abs/2509.00841", "title": "Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations", "authors": ["Michelle Elizabeth", "Alicja Kasicka", "Natalia Krawczyk", "Magalie Ochs", "Gwénolé Lecorvé", "Justyna Gromada", "Lina M. Rojas-Barahona"], "categories": ["cs.CL"], "comment": null, "summary": "The growing number of generative AI-based dialogue systems has made their\nevaluation a crucial challenge. This paper presents our contribution to this\nimportant problem through the Dialogue System Technology Challenge (DSTC-12,\nTrack 1), where we developed models to predict dialogue-level,\ndimension-specific scores. Given the constraint of using relatively small\nmodels (i.e. fewer than 13 billion parameters) our work follows two main\nstrategies: employing Language Models (LMs) as evaluators through prompting,\nand training encoder-based classification and regression models.\n  Our results show that while LM prompting achieves only modest correlations\nwith human judgments, it still ranks second on the test set, outperformed only\nby the baseline. The regression and classification models, with significantly\nfewer parameters, demonstrate high correlation for some dimensions on the\nvalidation set. Although their performance decreases on the test set, it is\nimportant to note that the test set contains annotations with significantly\ndifferent score ranges for some of the dimensions with respect to the train and\nvalidation sets."}
{"id": "2509.02444", "pdf": "https://arxiv.org/pdf/2509.02444.pdf", "abs": "https://arxiv.org/abs/2509.02444", "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent", "authors": ["Jingru Fan", "Yufan Dang", "Jingyao Wu", "Huatao Li", "Runde Yang", "Xiyuan Yang", "Yuheng Wang", "Zhong Zhang", "Yaxi Lu", "Yankai Lin", "Zhiyuan Liu", "Dahai Li", "Chen Qian"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Project at https://github.com/OpenBMB/AppCopilot", "summary": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime."}
{"id": "2509.00842", "pdf": "https://arxiv.org/pdf/2509.00842.pdf", "abs": "https://arxiv.org/abs/2509.00842", "title": "Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings", "authors": ["Tengyu Pan", "Zhichao Duan", "Zhenyu Li", "Bowen Dong", "Ning Liu", "Xiuxing Li", "Jianyong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Text embedding models are essential for various natural language processing\ntasks, enabling the effective encoding of semantic information into dense\nvector representations. These models are typically optimized using triplets of\n(query, positive, negative) data pairs for contrastive learning, where the\nnegative samples play a critical role in enhancing the model's ability to\ndiscern subtle semantic distinctions. In this work, we introduce a\nMulti-Granularity Hard-negative (MGH) synthesis framework that leverages large\nlanguage models (LLMs) to generate diverse negative samples with varying levels\nof similarity with the query. This approach facilitates a coarse-to-fine\ncurriculum learning strategy during supervised training, allowing the embedding\nmodel to progressively learn more nuanced semantic representations. Meanwhile,\nwe propose an Anchor Token Aware (ATA) pooling method that assigns higher\nweights to anchor tokens based on aggregation patterns observed in LLMs,\nimproving text embedding accuracy without increasing model complexity.\nComprehensive experiments on the MTEB benchmark demonstrate that our methods\nachieve state-of-the-art performance, surpassing existing synthesis strategies\nboth with synthetic data and when combined with public retrieval datasets."}
{"id": "2509.02544", "pdf": "https://arxiv.org/pdf/2509.02544.pdf", "abs": "https://arxiv.org/abs/2509.02544", "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning", "authors": ["Haoming Wang", "Haoyang Zou", "Huatong Song", "Jiazhan Feng", "Junjie Fang", "Junting Lu", "Longxiang Liu", "Qinyu Luo", "Shihao Liang", "Shijue Huang", "Wanjun Zhong", "Yining Ye", "Yujia Qin", "Yuwen Xiong", "Yuxin Song", "Zhiyong Wu", "Bo Li", "Chen Dun", "Chong Liu", "Fuxing Leng", "Hanbin Wang", "Hao Yu", "Haobin Chen", "Hongyi Guo", "Jing Su", "Jingjia Huang", "Kai Shen", "Kaiyu Shi", "Lin Yan", "Peiyao Zhao", "Pengfei Liu", "Qinghao Ye", "Renjie Zheng", "Wayne Xin Zhao", "Wen Heng", "Wenhao Huang", "Wenqian Wang", "Xiaobo Qin", "Yi Lin", "Youbin Wu", "Zehui Chen", "Zihao Wang", "Baoquan Zhong", "Xinchun Zhang", "Xujing Li", "Yuanfan Li", "Zhongkai Zhao", "Chengquan Jiang", "Faming Wu", "Haotian Zhou", "Jinlin Pang", "Li Han", "Qianli Ma", "Siyao Liu", "Songhua Cai", "Wenqi Fu", "Xin Liu", "Zhi Zhang", "Bo Zhou", "Guoliang Li", "Jiajun Shi", "Jiale Yang", "Jie Tang", "Li Li", "Taoran Lu", "Woyu Lin", "Xiaokang Tong", "Xinyao Li", "Yichi Zhang", "Yu Miao", "Zhengxuan Jiang", "Zili Li", "Ziyuan Zhao", "Chenxin Li", "Dehua Ma", "Feng Lin", "Ge Zhang", "Haihua Yang", "Hangyu Guo", "Hongda Zhu", "Jiaheng Liu", "Junda Du", "Kai Cai", "Kuanye Li", "Lichen Yuan", "Meilan Han", "Minchao Wang", "Shuyue Guo", "Tianhao Cheng", "Xiaobo Ma", "Xiaojun Xiao", "Xiaolong Huang", "Xinjie Chen", "Yidi Du", "Yilin Chen", "Yiwen Wang", "Zhaojian Li", "Zhenzhu Yang", "Zhiyuan Zeng", "Chaolin Jin", "Chen Li", "Hao Chen", "Haoli Chen", "Jian Chen", "Qinghao Zhao", "Guang Shi"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios."}
{"id": "2509.00849", "pdf": "https://arxiv.org/pdf/2509.00849.pdf", "abs": "https://arxiv.org/abs/2509.00849", "title": "Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations", "authors": ["Shaina Raza", "Maximus Powers", "Partha Pratim Saha", "Mahveen Raza", "Rizwan Qureshi"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Image (TTI) models are powerful creative tools but risk amplifying\nharmful social biases. We frame representational societal bias assessment as an\nimage curation and evaluation task and introduce a pilot benchmark of\noccupational portrayals spanning five socially salient roles (CEO, Nurse,\nSoftware Engineer, Teacher, Athlete). Using five state-of-the-art models:\nclosed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable\nDiffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against\nfairness-aware controlled prompts designed to encourage demographic diversity.\nAll outputs are annotated for gender (male, female) and race (Asian, Black,\nWhite), enabling structured distributional analysis. Results show that\nprompting can substantially shift demographic representations, but with highly\nmodel-specific effects: some systems diversify effectively, others overcorrect\ninto unrealistic uniformity, and some show little responsiveness. These\nfindings highlight both the promise and the limitations of prompting as a\nfairness intervention, underscoring the need for complementary model-level\nstrategies. We release all code and data for transparency and reproducibility\nhttps://github.com/maximus-powers/img-gen-bias-analysis."}
{"id": "2404.17098", "pdf": "https://arxiv.org/pdf/2404.17098.pdf", "abs": "https://arxiv.org/abs/2404.17098", "title": "CLARE: Cognitive Load Assessment in REaltime with Multimodal Data", "authors": ["Anubhav Bhatti", "Prithila Angkan", "Behnam Behinaein", "Zunayed Mahmud", "Dirk Rodenburg", "Heather Braund", "P. James Mclellan", "Aaron Ruberto", "Geoffery Harrison", "Daryl Wilson", "Adam Szulewski", "Dan Howes", "Ali Etemad", "Paul Hungler"], "categories": ["cs.HC", "cs.AI"], "comment": "13 pages, 10 figures, 6 tables", "summary": "We present a novel multimodal dataset for Cognitive Load Assessment in\nREal-time (CLARE). The dataset contains physiological and gaze data from 24\nparticipants with self-reported cognitive load scores as ground-truth labels.\nThe dataset consists of four modalities, namely, Electrocardiography (ECG),\nElectrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To\nmap diverse levels of mental load on participants during experiments, each\nparticipant completed four nine-minutes sessions on a computer-based operator\nperformance and mental workload task (the MATB-II software) with varying levels\nof complexity in one minute segments. During the experiment, participants\nreported their cognitive load every 10 seconds. For the dataset, we also\nprovide benchmark binary classification results with machine learning and deep\nlearning models on two different evaluation schemes, namely, 10-fold and\nleave-one-subject-out (LOSO) cross-validation. Benchmark results show that for\n10-fold evaluation, the convolutional neural network (CNN) based deep learning\nmodel achieves the best classification performance with ECG, EDA, and Gaze. In\ncontrast, for LOSO, the best performance is achieved by the deep learning model\nwith ECG, EDA, and EEG."}
{"id": "2509.00869", "pdf": "https://arxiv.org/pdf/2509.00869.pdf", "abs": "https://arxiv.org/abs/2509.00869", "title": "Exploring and Mitigating Fawning Hallucinations in Large Language Models", "authors": ["Zixuan Shangguan", "Yanjie Dong", "Lanjun Wang", "Xiaoyi Fan", "Victor C. M. Leung", "Xiping Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nlanguage understanding. However, when LLMs align their outputs with deceptive\nand/or misleading prompts, the generated responses could deviate from the de\nfacto information. Such observations are known as fawning hallucinations, where\nthe model prioritizes alignment with the input's implied perspective over\naccuracy and truthfulness. In this work, we analyze fawning hallucinations in\nvarious natural language processing tasks and tailor the so-termed contrastive\ndecoding method for fawning-hallucination mitigation. Specifically, we design\ntwo paradigms to generate corresponding deceptive and/or misleading inputs for\nthe consistent fawning hallucinations induction. Then, we propose the\ncollaborative contrastive decoding (CCD) to handle the fawning hallucinations\nacross different tasks in LLMs. By contrasting the deviation in output\ndistribution between induced and transformed neutral inputs, the proposed CCD\ncan reduce reliance on deceptive and/or misleading information without\nrequiring additional training. Extensive experiments demonstrate that the\nproposed CCD can effectively mitigate fawning hallucinations and improve the\nfactuality of the generated responses over various tasks."}
{"id": "2408.03948", "pdf": "https://arxiv.org/pdf/2408.03948.pdf", "abs": "https://arxiv.org/abs/2408.03948", "title": "A Survey of AI Reliance", "authors": ["Sven Eckhardt", "Niklas Kühl", "Mateusz Dolata", "Gerhard Schwabe"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Although artificial intelligence (AI) systems are becoming increasingly\nindispensable, research into how humans rely on these systems (AI reliance) is\nlagging behind. To advance this research, this survey presents a novel,\ncomprehensive sociotechnical perspective on AI reliance, essential to fully\nunderstand the phenomenon. To address these challenges, the survey introduces a\ncategorization framework resulting in a morphological box, which guides\nrigorous AI reliance research. Further, the survey identifies the core\ninfluences on AI reliance within the components of a sociotechnical system and\ndiscusses current limitations alongside emerging future research avenues to\nform a research agenda."}
{"id": "2509.00877", "pdf": "https://arxiv.org/pdf/2509.00877.pdf", "abs": "https://arxiv.org/abs/2509.00877", "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes", "authors": ["Yuqin Dai", "Guoqing Wang", "Yuan Wang", "Kairan Dou", "Kaichen Zhou", "Zhanwei Zhang", "Shuo Yang", "Fei Tang", "Jun Yin", "Pengyu Zeng", "Zhenzhe Ying", "Can Yi", "Changhua Meng", "Yuchen Zhou", "Yongliang Shen", "Shuai Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) empowered with retrieval mechanisms have\nachieved strong progress in open-domain question answering (QA). Yet, the\nconventional retrieve--then--answer paradigm often suffers from two key\nlimitations: (1) low signal-to-noise ratio in retrieved evidence, where useful\ninformation is buried under irrelevant content, and (2) error accumulation in\nmulti-hop reasoning when incomplete or noisy passages are involved. To address\nthese challenges, we present EviNote-RAG, an agentic RAG framework that\nintroduces a structured retrieve--note--answer pipeline. Instead of directly\nreasoning over raw retrievals, the model is trained to compose\nSupportive-Evidence Notes (SENs), concise, human-like notes that preserve only\nanswer-relevant information, highlight uncertainty, and explicitly state when\nno useful evidence exists. This distillation process is further reinforced by\nthe Evidence Quality Reward (EQR), an entailment-based signal that evaluates\nwhether SENs logically support the final answer. Together, SENs and EQR guide\nthe model toward faithful and robust reasoning, while reducing the impact of\nnoise. Experiments on in-domain and out-of-domain QA benchmarks show that\nEviNote-RAG consistently outperforms strong baselines in accuracy,\ngeneralization, and training stability. In particular, it achieves\nstate-of-the-art results while enhancing robustness and efficiency, yielding\nrelative F1 gains of 20\\% on HotpotQA (+0.093), 40\\% on Bamboogle (+0.151), and\n91\\% on 2Wiki (+0.256) via denser rewards and reduced verbosity."}
{"id": "2501.10551", "pdf": "https://arxiv.org/pdf/2501.10551.pdf", "abs": "https://arxiv.org/abs/2501.10551", "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing Essays", "authors": ["Andrew Jelson", "Daniel Manesh", "Alice Jang", "Daniel Dunlap", "Young-Ho Kim", "Sang Won Lee"], "categories": ["cs.HC"], "comment": "19 pages, 10 figures, 2 tables, final preparation for a TOCHI\n  submission", "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."}
{"id": "2509.00893", "pdf": "https://arxiv.org/pdf/2509.00893.pdf", "abs": "https://arxiv.org/abs/2509.00893", "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset", "authors": ["Răzvan-Alexandru Smădu", "Andreea Iuga", "Dumitru-Clementin Cercel", "Florin Pop"], "categories": ["cs.CL", "I.2.7; I.7"], "comment": "12 pages, 2 Figures", "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions."}
{"id": "2504.14695", "pdf": "https://arxiv.org/pdf/2504.14695.pdf", "abs": "https://arxiv.org/abs/2504.14695", "title": "GLITTER: An AI-assisted Platform for Material-Grounded Asynchronous Discussion in Flipped Learning", "authors": ["Weirui Peng", "Yinuo Yang", "Zheng Zhang", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Flipped classrooms promote active learning by having students engage with\nmaterials independently before class, allowing in-class time for collaborative\nproblem-solving. During this pre-class phase, asynchronous online discussions\nhelp students build knowledge and clarify concepts with peers. However, it\nremains difficult to engage with temporally dispersed peer contributions,\nconnect discussions with static learning materials, and prepare for in-class\nsessions based on their self-learning outcome. Our formative study identified\ncognitive challenges students encounter, including navigation barriers,\nreflection gaps, and contribution difficulty and anxiety. We present GLITTER,\nan AI-assisted discussion platform for pre-class learning in flipped\nclassrooms. GLITTER helps students identify posts with shared conceptual\ndimensions, scaffold knowledge integration through conceptual blending, and\nenhance metacognition via personalized reflection reports. A lab study within\nsubjects (n = 12) demonstrates that GLITTER improves discussion engagement,\nsparks new ideas, supports reflection, and increases preparedness for in-class\nactivities."}
{"id": "2509.00921", "pdf": "https://arxiv.org/pdf/2509.00921.pdf", "abs": "https://arxiv.org/abs/2509.00921", "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling", "authors": ["David Dukić", "Goran Glavaš", "Jan Šnajder"], "categories": ["cs.CL"], "comment": null, "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining (1) in-context learning (ICL)\nfrom demonstrations with (2) supervised fine-tuning. SIFT considerably\noutperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of\nstandard SL tasks. We further find that although long context hinders the\nperformance of generative SL in both ICL and SIFT, this deficiency can be\nmitigated by removing the instruction, as instructions are shown to be largely\nunnecessary for achieving strong SL performance with SIFT. Our findings\nhighlight strengths and limitations of SL with LLMs, underscoring the\nimportance of a response-based generative task formulation for effective SL\nperformance."}
{"id": "2508.01235", "pdf": "https://arxiv.org/pdf/2508.01235.pdf", "abs": "https://arxiv.org/abs/2508.01235", "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration", "authors": ["Yaxin Hu", "Arissa J. Sato", "Jingxin Du", "Chenming Ye", "Anjun Zhu", "Pragathi Praveena", "Bilge Mutlu"], "categories": ["cs.HC", "cs.RO", "68"], "comment": null, "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments."}
{"id": "2509.00934", "pdf": "https://arxiv.org/pdf/2509.00934.pdf", "abs": "https://arxiv.org/abs/2509.00934", "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework", "authors": ["Md Shahidul Salim", "Lian Fu", "Arav Adikesh Ramakrishnan", "Zonghai Yao", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed\nto improve English-to-Spanish medical translation by integrating\ndomain-specific structured knowledge into large language models (LLMs). MedCOD\nintegrates domain-specific knowledge from both the Unified Medical Language\nSystem (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance\nstructured prompting and fine-tuning. We constructed a parallel corpus of 2,999\nEnglish-Spanish MedlinePlus articles and a 100-sentence test set annotated with\nstructured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,\nQwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that\nincorporated multilingual variants, medical synonyms, and UMLS-derived\ndefinitions, combined with LoRA-based fine-tuning. Experimental results\ndemonstrate that MedCOD significantly improves translation quality across all\nmodels. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,\nchrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o\nand GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model\nadaptation independently contribute to performance gains, with their\ncombination yielding the highest improvements. These findings highlight the\npotential of structured knowledge integration to enhance LLMs for medical\ntranslation tasks."}
{"id": "2508.16604", "pdf": "https://arxiv.org/pdf/2508.16604.pdf", "abs": "https://arxiv.org/abs/2508.16604", "title": "WHAR Datasets: An Open Source Library for Wearable Human Activity Recognition", "authors": ["Maximilian Burzer", "Tobias King", "Till Riedel", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.HC", "cs.LG", "I.2.6"], "comment": "8 pages, 7 figures, to appear in Companion of the 2025 ACM\n  International Joint Conference on Pervasive and Ubiquitous Computing\n  (UbiComp), OpenWearables Workshop (accepted paper), updated formatting of\n  authors and headings", "summary": "The lack of standardization across Wearable Human Activity Recognition (WHAR)\ndatasets limits reproducibility, comparability, and research efficiency. We\nintroduce WHAR datasets, an open-source library designed to simplify WHAR data\nhandling through a standardized data format and a configuration-driven design,\nenabling reproducible and computationally efficient workflows with minimal\nmanual intervention. The library currently supports 9 widely-used datasets,\nintegrates with PyTorch and TensorFlow, and is easily extensible to new\ndatasets. To demonstrate its utility, we trained two state-of-the-art models,\nTinyHar and MLP-HAR, on the included datasets, approximately reproducing\npublished results and validating the library's effectiveness for\nexperimentation and benchmarking. Additionally, we evaluated preprocessing\nperformance and observed speedups of up to 3.8x using multiprocessing. We hope\nthis library contributes to more efficient, reproducible, and comparable WHAR\nresearch."}
{"id": "2509.00949", "pdf": "https://arxiv.org/pdf/2509.00949.pdf", "abs": "https://arxiv.org/abs/2509.00949", "title": "Structure and Destructure: Dual Forces in the Making of Knowledge Engines", "authors": ["Yihong Chen"], "categories": ["cs.CL", "cs.AI", "68T05, 68T30, 68T50", "I.2.6; I.2.7; H.3.3; K.8.0"], "comment": "PhD thesis. https://discovery.ucl.ac.uk/id/eprint/10211291/", "summary": "The making of knowledge engines in natural language processing has been\nshaped by two seemingly distinct paradigms: one grounded in structure, the\nother driven by massively available unstructured data. The structured paradigm\nleverages predefined symbolic interactions, such as knowledge graphs, as priors\nand designs models to capture them. In contrast, the unstructured paradigm\ncenters on scaling transformer architectures with increasingly vast data and\nmodel sizes, as seen in modern large language models. Despite their divergence,\nthis thesis seeks to establish conceptual connections bridging these paradigms.\nTwo complementary forces, structure and destructure, emerge across both\nparadigms: structure organizes seen symbolic interactions, while destructure,\nthrough periodic embedding resets, improves model plasticity and generalization\nto unseen scenarios. These connections form a new recipe for developing general\nknowledge engines that can support transparent, controllable, and adaptable\nintelligent systems."}
{"id": "2303.06721", "pdf": "https://arxiv.org/pdf/2303.06721.pdf", "abs": "https://arxiv.org/abs/2303.06721", "title": "Knowledge-integrated AutoEncoder Model", "authors": ["Teddy Lazebnik", "Liron Simon-Keren"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Data encoding is a common and central operation in most data analysis tasks.\nThe performance of other models downstream in the computational process highly\ndepends on the quality of data encoding. One of the most powerful ways to\nencode data is using the neural network AutoEncoder (AE) architecture. However,\nthe developers of AE cannot easily influence the produced embedding space, as\nit is usually treated as a black box technique. This means the embedding space\nis uncontrollable and does not necessarily possess the properties desired for\ndownstream tasks. This paper introduces a novel approach for developing AE\nmodels that can integrate external knowledge sources into the learning process,\npossibly leading to more accurate results. The proposed Knowledge-integrated\nAutoEncoder (KiAE) model can leverage domain-specific information to make sure\nthe desired distance and neighborhood properties between samples are\npreservative in the embedding space. The proposed model is evaluated on three\nlarge-scale datasets from three scientific fields and is compared to nine\nexisting encoding models. The results demonstrate that the KiAE model\neffectively captures the underlying structures and relationships between the\ninput data and external knowledge, meaning it generates a more useful\nrepresentation. This leads to outperforming the rest of the models in terms of\nreconstruction accuracy."}
{"id": "2509.00974", "pdf": "https://arxiv.org/pdf/2509.00974.pdf", "abs": "https://arxiv.org/abs/2509.00974", "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning", "authors": ["Chia-Hsuan Hsu", "Jun-En Ding", "Hsin-Ling Hsu", "Feng Liu", "Fang-Ming Hung"], "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering requires advanced reasoning that integrates domain\nknowledge with logical inference. However, existing large language models\n(LLMs) often generate reasoning chains that lack factual accuracy and clinical\nreliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a\nnovel framework that uniquely combines reinforcement learning with\npreference-driven reasoning refinement to enhance clinical chain-of-thought\n(CoT) performance. RPRO differentiates itself from prior approaches by\nemploying task-adaptive reasoning templates and a probabilistic evaluation\nmechanism that aligns outputs with established clinical workflows, while\nautomatically identifying and correcting low-quality reasoning chains. Unlike\ntraditional pairwise preference methods, RPRO introduces a groupwise ranking\noptimization based on the Bradley-Terry model and incorporates KL-divergence\nregularization for stable training. Experiments on PubMedQA and MedQA-USMLE\nshow consistent improvements over strong baselines. Remarkably, our 1.1B\nparameter model outperforms much larger 7B-13B models, including\nmedical-specialized variants. These findings demonstrate that combining\npreference optimization with quality-driven refinement offers a scalable and\neffective approach to building more reliable, clinically grounded medical LLMs."}
{"id": "2411.12142", "pdf": "https://arxiv.org/pdf/2411.12142.pdf", "abs": "https://arxiv.org/abs/2411.12142", "title": "A Computational Method for Measuring \"Open Codes\" in Qualitative Analysis", "authors": ["John Chen", "Alexandros Lotsos", "Sihan Cheng", "Caiyi Wang", "Lexie Zhao", "Jessica Hullman", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. A central method in this process is inductive\ncoding, where researchers identify and interpret codes directly from the\ndatasets themselves. Yet, this exploratory approach poses challenges for\nmeeting methodological expectations (such as ``depth'' and ``variation''),\nespecially as researchers increasingly adopt Generative AI (GAI) for support.\nGround-truth-based metrics are insufficient because they contradict the\nexploratory nature of inductive coding, while manual evaluation can be\nlabor-intensive. This paper presents a theory-informed computational method for\nmeasuring inductive coding results from humans and GAI. Our method first merges\nindividual codebooks using an LLM-enriched algorithm. It measures each coder's\ncontribution against the merged result using four novel metrics: Coverage,\nOverlap, Novelty, and Divergence. Through two experiments on a human-coded\nonline conversation dataset, we 1) reveal the merging algorithm's impact on\nmetrics; 2) validate the metrics' stability and robustness across multiple runs\nand different LLMs; and 3) showcase the metrics' ability to diagnose coding\nissues, such as excessive or irrelevant (hallucinated) codes. Our work provides\na reliable pathway for ensuring methodological rigor in human-AI qualitative\nanalysis."}
{"id": "2509.00983", "pdf": "https://arxiv.org/pdf/2509.00983.pdf", "abs": "https://arxiv.org/abs/2509.00983", "title": "Performance Analysis of Supervised Machine Learning Algorithms for Text Classification", "authors": ["Sadia Zaman Mishu", "S M Rafiuddin"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, published in 2016 at the 19th International\n  Conference on Computer and Information Technology (ICCIT), Bangladesh,\n  proceedings pp. 409-413, IEEE", "summary": "The demand for text classification is growing significantly in web searching,\ndata mining, web ranking, recommendation systems, and so many other fields of\ninformation and technology. This paper illustrates the text classification\nprocess on different datasets using some standard supervised machine learning\ntechniques. Text documents can be classified through various kinds of\nclassifiers. Labeled text documents are used to classify the text in supervised\nclassifications. This paper applies these classifiers on different kinds of\nlabeled documents and measures the accuracy of the classifiers. An Artificial\nNeural Network (ANN) model using Back Propagation Network (BPN) is used with\nseveral other models to create an independent platform for labeled and\nsupervised text classification process. An existing benchmark approach is used\nto analyze the performance of classification using labeled documents.\nExperimental analysis on real data reveals which model works well in terms of\nclassification accuracy."}
{"id": "2501.05790", "pdf": "https://arxiv.org/pdf/2501.05790.pdf", "abs": "https://arxiv.org/abs/2501.05790", "title": "Understanding Impact of Human Feedback via Influence Functions", "authors": ["Taywon Min", "Haeone Lee", "Yongchan Kwon", "Kimin Lee"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted at ACL 2025, Source code:\n  https://github.com/mintaywon/IF_RLHF", "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. Our experiments showcase two key\napplications of influence functions: (1) detecting common labeler biases in\nhuman feedback datasets and (2) guiding labelers in refining their strategies\nto better align with expert feedback. By quantifying the impact of human\nfeedback, we believe that influence functions can enhance feedback\ninterpretability and contribute to scalable oversight in RLHF, helping labelers\nprovide more accurate and consistent feedback. Source code is available at\nhttps://github.com/mintaywon/IF_RLHF"}
{"id": "2509.01011", "pdf": "https://arxiv.org/pdf/2509.01011.pdf", "abs": "https://arxiv.org/abs/2509.01011", "title": "Ranking of Bangla Word Graph using Graph-based Ranking Algorithms", "authors": ["S M Rafiuddin"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, Publication date 2017-12-07, Conference 2017 3rd\n  International Conference on Electrical Information and Communication\n  Technology EICT, Pages 1-5, Publisher IEEE", "summary": "Ranking words is an important way to summarize a text or to retrieve\ninformation. A word graph is a way to represent the words of a sentence or a\ntext as the vertices of a graph and to show the relationship among the words.\nIt is also useful to determine the relative importance of a word among the\nwords in the word-graph. In this research, the ranking of Bangla words are\ncalculated, representing Bangla words from a text in a word graph using various\ngraph based ranking algorithms. There is a lack of a standard Bangla word\ndatabase. In this research, the Indian Language POS-tag Corpora is used, which\nhas a rich collection of Bangla words in the form of sentences with their parts\nof speech tags. For applying a word graph to various graph based ranking\nalgorithms, several standard procedures are applied. The preprocessing steps\nare done in every word graph and then applied to graph based ranking algorithms\nto make a comparison among these algorithms. This paper illustrate the entire\nprocedure of calculating the ranking of Bangla words, including the\nconstruction of the word graph from text. Experimental result analysis on real\ndata reveals the accuracy of each ranking algorithm in terms of F1 measure."}
{"id": "2502.09849", "pdf": "https://arxiv.org/pdf/2502.09849.pdf", "abs": "https://arxiv.org/abs/2502.09849", "title": "A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems", "authors": ["Alessandro Gambetti", "Qiwei Han", "Hong Shen", "Claudia Soares"], "categories": ["cs.LG", "cs.HC"], "comment": "13 pages, 1 table, 1 figure", "summary": "Explainable AI (XAI) has become a crucial component of Clinical Decision\nSupport Systems (CDSS) to enhance transparency, trust, and clinical adoption.\nHowever, while many XAI methods have been proposed, their effectiveness in\nreal-world medical settings remains underexplored. This paper provides a survey\nof human-centered evaluations of Explainable AI methods in Clinical Decision\nSupport Systems. By categorizing existing works based on XAI methodologies,\nevaluation frameworks, and clinical adoption challenges, we offer a structured\nunderstanding of the landscape. Our findings reveal key challenges in the\nintegration of XAI into healthcare workflows and propose a structured framework\nto align the evaluation methods of XAI with the clinical needs of stakeholders."}
{"id": "2509.01035", "pdf": "https://arxiv.org/pdf/2509.01035.pdf", "abs": "https://arxiv.org/abs/2509.01035", "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof", "authors": ["Nikta Gohari Sadr", "Sahar Heidariasl", "Karine Megerdoomian", "Laleh Seyyed-Kalantari", "Ali Emami"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) struggle to navigate culturally specific\ncommunication norms, limiting their effectiveness in global contexts. We focus\non Persian taarof, a social norm in Iranian interactions, which is a\nsophisticated system of ritual politeness that emphasizes deference, modesty,\nand indirectness, yet remains absent from existing cultural benchmarks. We\nintroduce TaarofBench, the first benchmark for evaluating LLM understanding of\ntaarof, comprising 450 role-play scenarios covering 12 common social\ninteraction topics, validated by native speakers. Our evaluation of five\nfrontier LLMs reveals substantial gaps in cultural competence, with accuracy\nrates 40-48% below native speakers when taarof is culturally appropriate.\nPerformance varies between interaction topics, improves with Persian-language\nprompts, and exhibits gender-based asymmetries. We also show that responses\nrated \"polite\" by standard metrics often violate taarof norms, indicating the\nlimitations of Western politeness frameworks. Through supervised fine-tuning\nand Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in\nmodel alignment with cultural expectations. Our human study with 33\nparticipants (11 native Persian, 11 heritage, and 11 non-Iranian speakers)\nforms baselines in varying degrees of familiarity with Persian norms. This work\nlays the foundation for developing diverse and culturally aware LLMs, enabling\napplications that better navigate complex social interactions."}
{"id": "2503.23760", "pdf": "https://arxiv.org/pdf/2503.23760.pdf", "abs": "https://arxiv.org/abs/2503.23760", "title": "Towards a cognitive architecture to enable natural language interaction in co-constructive task learning", "authors": ["Manuel Scheibl", "Birte Richter", "Alissa Müller", "Michael Beetz", "Britta Wrede"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "8 pages, 5 figures, The paper has been accepted by the 2025 34th IEEE\n  International Conference on Robot and Human Interactive Communication\n  (ROMAN), IEEE Copyright Policy:\n  https://www.ieee.org/publications/rights/copyright-policy", "summary": "This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI)."}
{"id": "2509.01053", "pdf": "https://arxiv.org/pdf/2509.01053.pdf", "abs": "https://arxiv.org/abs/2509.01053", "title": "A Dynamic Fusion Model for Consistent Crisis Response", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Eduardo Blanco", "Vanessa Frias-Martinez", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025, 10 pages, 5 figures", "summary": "In response to the urgent need for effective communication with\ncrisis-affected populations, automated responses driven by language models have\nbeen proposed to assist in crisis communications. A critical yet often\noverlooked factor is the consistency of response style, which could affect the\ntrust of affected individuals in responders. Despite its importance, few\nstudies have explored methods for maintaining stylistic consistency across\ngenerated responses. To address this gap, we propose a novel metric for\nevaluating style consistency and introduce a fusion-based generation approach\ngrounded in this metric. Our method employs a two-stage process: it first\nassesses the style of candidate responses and then optimizes and integrates\nthem at the instance level through a fusion process. This enables the\ngeneration of high-quality responses while significantly reducing stylistic\nvariation between instances. Experimental results across multiple datasets\ndemonstrate that our approach consistently outperforms baselines in both\nresponse quality and stylistic uniformity."}
{"id": "2504.06996", "pdf": "https://arxiv.org/pdf/2504.06996.pdf", "abs": "https://arxiv.org/abs/2504.06996", "title": "Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications", "authors": ["Adithya Krishna", "Sohan Debnath", "Madhuvanthi Srivatsav", "André van Schaik", "Mahesh Mehendale", "Chetan Singh Thakur"], "categories": ["cs.AR", "cs.HC", "cs.LG"], "comment": null, "summary": "High-quality, multi-channel neural recording is indispensable for\nneuroscience research and clinical applications. Large-scale brain recordings\noften produce vast amounts of data that must be wirelessly transmitted for\nsubsequent offline analysis and decoding, especially in brain-computer\ninterfaces (BCIs) utilizing high-density intracortical recordings with hundreds\nor thousands of electrodes. However, transmitting raw neural data presents\nsignificant challenges due to limited communication bandwidth and resultant\nexcessive heating. To address this challenge, we propose a neural signal\ncompression scheme utilizing Convolutional Autoencoders (CAEs), which achieves\na compression ratio of up to 150 for compressing local field potentials (LFPs).\nThe CAE encoder section is implemented on RAMAN, an energy-efficient tinyML\naccelerator designed for edge computing. RAMAN leverages sparsity in activation\nand weights through zero skipping, gating, and weight compression techniques.\nAdditionally, we employ hardware-software co-optimization by pruning the CAE\nencoder model parameters using a hardware-aware balanced stochastic pruning\nstrategy, resolving workload imbalance issues and eliminating indexing overhead\nto reduce parameter storage requirements by up to 32.4%. Post layout simulation\nshows that the RAMAN encoder can be implemented in a TSMC 65-nm CMOS process,\noccupying a core area of 0.0187 mm2 per channel. Operating at a clock frequency\nof 2 MHz and a supply voltage of 1.2 V, the estimated power consumption is 15.1\nuW per channel for the proposed DS-CAE1 model. For functional validation, the\nRAMAN encoder was also deployed on an Efinix Ti60 FPGA, utilizing 37.3k LUTs\nand 8.6k flip-flops. The compressed neural data from RAMAN is reconstructed\noffline with SNDR of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and\n0.94, respectively, evaluated on two monkey neural recordings."}
{"id": "2509.01058", "pdf": "https://arxiv.org/pdf/2509.01058.pdf", "abs": "https://arxiv.org/abs/2509.01058", "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Dibakar Barua", "Pengcheng Luo", "Junhua Ding", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Health misinformation spreading online poses a significant threat to public\nhealth. Researchers have explored methods for automatically generating\ncounterspeech to health misinformation as a mitigation strategy. Existing\napproaches often produce uniform responses, ignoring that the health literacy\nlevel of the audience could affect the accessibility and effectiveness of\ncounterspeech. We propose a Controlled-Literacy framework using\nretrieval-augmented generation (RAG) with reinforcement learning (RL) to\ngenerate tailored counterspeech adapted to different health literacy levels. In\nparticular, we retrieve knowledge aligned with specific health literacy levels,\nenabling accessible and factual information to support generation. We design a\nreward function incorporating subjective user preferences and objective\nreadability-based rewards to optimize counterspeech to the target health\nliteracy level. Experiment results show that Controlled-Literacy outperforms\nbaselines by generating more accessible and user-preferred counterspeech. This\nresearch contributes to more equitable and impactful public health\ncommunication by improving the accessibility and comprehension of counterspeech\nto health misinformation."}
{"id": "2504.19131", "pdf": "https://arxiv.org/pdf/2504.19131.pdf", "abs": "https://arxiv.org/abs/2504.19131", "title": "Making Physical Objects with Generative AI and Robotic Assembly: Considering Fabrication Constraints, Sustainability, Time, Functionality, and Accessibility", "authors": ["Alexander Htet Kyaw", "Se Hwan Jeon", "Miana Smith", "Neil Gershenfeld"], "categories": ["cs.RO", "cs.HC"], "comment": "ACM CHI Conference on Human Factors in Computing Systems (CHI 2025),\n  Workshop on Generative AI and Human-Computer Interaction, Yokohama, Japan,\n  April 26 to May 1, 2025", "summary": "3D generative AI enables rapid and accessible creation of 3D models from text\nor image inputs. However, translating these outputs into physical objects\nremains a challenge due to the constraints in the physical world. Recent\nstudies have focused on improving the capabilities of 3D generative AI to\nproduce fabricable outputs, with 3D printing as the main fabrication method.\nHowever, this workshop paper calls for a broader perspective by considering how\nfabrication methods align with the capabilities of 3D generative AI. As a case\nstudy, we present a novel system using discrete robotic assembly and 3D\ngenerative AI to make physical objects. Through this work, we identified five\nkey aspects to consider in a physical making process based on the capabilities\nof 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can\ngenerate a wide range of 3D designs, requiring fabrication methods that can\nadapt to the variability of generative AI outputs. 2) Time: While generative AI\ncan generate 3D models in seconds, fabricating physical objects can take hours\nor even days. Faster production could enable a closer iterative design loop\nbetween humans and AI in the making process. 3) Sustainability: Although\ntext-to-3D models can generate thousands of models in the digital world,\nextending this capability to the real world would be resource-intensive,\nunsustainable and irresponsible. 4) Functionality: Unlike digital outputs from\n3D generative AI models, the fabrication method plays a crucial role in the\nusability of physical objects. 5) Accessibility: While generative AI simplifies\n3D model creation, the need for fabrication equipment can limit participation,\nmaking AI-assisted creation less inclusive. These five key aspects provide a\nframework for assessing how well a physical making process aligns with the\ncapabilities of 3D generative AI and values in the world."}
{"id": "2509.01081", "pdf": "https://arxiv.org/pdf/2509.01081.pdf", "abs": "https://arxiv.org/abs/2509.01081", "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation", "authors": ["Abdessalam Bouchekif", "Samer Rashwani", "Heba Sbahi", "Shahd Gaben", "Mutez Al-Khatib", "Mohammed Ghaly"], "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "comment": "10 pages, 7 Tables, Code:\n  https://github.com/bouchekif/inheritance_evaluation", "summary": "This paper evaluates the knowledge and reasoning capabilities of Large\nLanguage Models in Islamic inheritance law, known as 'ilm al-mawarith. We\nassess the performance of seven LLMs using a benchmark of 1,000 multiple-choice\nquestions covering diverse inheritance scenarios, designed to test models'\nability to understand the inheritance context and compute the distribution of\nshares prescribed by Islamic jurisprudence. The results reveal a significant\nperformance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas\nALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect\nimportant differences in reasoning ability and domain adaptation. We conduct a\ndetailed error analysis to identify recurring failure patterns across models,\nincluding misunderstandings of inheritance scenarios, incorrect application of\nlegal rules, and insufficient domain knowledge. Our findings highlight\nlimitations in handling structured legal reasoning and suggest directions for\nimproving performance in Islamic legal reasoning. Code:\nhttps://github.com/bouchekif/inheritance_evaluation"}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408.pdf", "abs": "https://arxiv.org/abs/2505.12408", "title": "ViEEG: Hierarchical Visual Neural Representation for EEG Brain Decoding", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "25 pages, 17 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG visual decoding has shown promise due to its\nnon-invasive, and low-cost nature, existing methods suffer from Hierarchical\nNeural Encoding Neglect (HNEN)-a critical limitation where flat neural\nrepresentations fail to model the brain's hierarchical visual processing\nhierarchy. Inspired by the hierarchical organization of visual cortex, we\npropose ViEEG, a neuro-We further adopt hierarchical contrastive learning for\nEEG-CLIP representation alignment, enabling zero-shot object recognition.\nExtensive experiments on the THINGS-EEG dataset demonstrate that ViEEG\nsignificantly outperforms previous methods by a large margin in both\nsubject-dependent and subject-independent settings. Results on the THINGS-MEG\ndataset further confirm ViEEG's generalization to different neural modalities.\nOur framework not only advances the performance frontier but also sets a new\nparadigm for EEG brain decoding. inspired framework that addresses HNEN. ViEEG\ndecomposes each visual stimulus into three biologically aligned\ncomponents-contour, foreground object, and contextual scene-serving as anchors\nfor a three-stream EEG encoder. These EEG features are progressively integrated\nvia cross-attention routing, simulating cortical information flow from\nlow-level to high-level vision."}
{"id": "2509.01084", "pdf": "https://arxiv.org/pdf/2509.01084.pdf", "abs": "https://arxiv.org/abs/2509.01084", "title": "A Paradigm Gap in Urdu", "authors": ["Farah Adeeba", "Rajesh Bhatt"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we document a paradigm gap in the combinatorial possibilities\nof verbs and aspect in Urdu: the perfective form of the -ya: kar construction\n(e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and\nHindi, despite being freely attested in 19th century literature. We investigate\nthis diachronic shift through historical text analysis, a large-scale corpus\nstudy which confirms the stark absence of perfective forms and subjective\nevaluation tasks with native speakers, who judge perfective examples as highly\nunnatural. We argue that this gap arose from a fundamental morphosyntactic\nconflict: the construction's requirement for a nominative subject and an\ninvariant participle clashes with the core grammatical rule that transitive\nperfective assign ergative case. This conflict rendered the perfective form\nunstable, and its functional replacement by other constructions allowed the gap\nto become entrenched in the modern grammar."}
{"id": "2506.19899", "pdf": "https://arxiv.org/pdf/2506.19899.pdf", "abs": "https://arxiv.org/abs/2506.19899", "title": "Anti-Phishing Training (Still) Does Not Work: A Large-Scale Reproduction of Phishing Training Inefficacy Grounded in the NIST Phish Scale", "authors": ["Andrew T. Rozema", "James C. Davis"], "categories": ["cs.CR", "cs.HC"], "comment": "22 pages, 9 figures, 2 tables, 2 appendices. Empirical study with\n  N=12,511 participants at a financial technology firm. Reproduction study\n  validating NIST Phish Scale and evaluating phishing training effectiveness", "summary": "Social engineering attacks delivered via email, commonly known as phishing,\nrepresent a persistent cybersecurity threat leading to significant\norganizational incidents and data breaches. Although many organizations train\nemployees on phishing, often mandated by compliance requirements, the\nreal-world effectiveness of this training remains debated. To contribute to\nevidence-based cybersecurity policy, we conducted a large-scale reproduction\nstudy (N = 12,511) at a US-based financial technology firm. Our experimental\ndesign refined prior work by comparing training modalities in operational\nenvironments, validating NIST's standardized phishing difficulty measurement,\nand introducing novel organizational-level temporal resilience metrics. Echoing\nprior work, training interventions showed no significant main effects on click\nrates (p=0.450) or reporting rates (p=0.417), with negligible effect sizes.\nHowever, we found that the NIST Phish Scale predicted user behavior, with click\nrates increasing from 7.0% for easy lures to 15.0% for hard lures. Our\norganizational-level resilience result was mixed: 36-55% of campaigns achieved\n\"inoculation\" patterns where reports preceded clicks, but training did not\nsignificantly improve organizational-level temporal protection. In summary, our\nresults confirm the ineffectiveness of current phishing training approaches\nwhile offering a refined study design for future work."}
{"id": "2509.01088", "pdf": "https://arxiv.org/pdf/2509.01088.pdf", "abs": "https://arxiv.org/abs/2509.01088", "title": "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation", "authors": ["Jinwen Chen", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Haibo Zhou", "Yuan Zhan", "Wei Lin", "Zhiming Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization", "authors": ["Siting Wang", "Minnan Pei", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available."}
{"id": "2509.01092", "pdf": "https://arxiv.org/pdf/2509.01092.pdf", "abs": "https://arxiv.org/abs/2509.01092", "title": "REFRAG: Rethinking RAG based Decoding", "authors": ["Xiaoqiang Lin", "Aritra Ghosh", "Bryan Kian Hsiang Low", "Anshumali Shrivastava", "Vijai Mohan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."}
{"id": "2508.05025", "pdf": "https://arxiv.org/pdf/2508.05025.pdf", "abs": "https://arxiv.org/abs/2508.05025", "title": "Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality", "authors": ["Zhehan Qu", "Tianyi Hu", "Christian Fronk", "Maria Gorlatova"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Augmented Reality (AR) systems, while enhancing task performance through\nreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on\nvirtual content that compromises situational awareness (SA) in safety-critical\nscenarios. This paper investigates SA in AR-guided cardiopulmonary\nresuscitation (CPR), where responders must balance effective compressions with\nvigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR\napp on a Magic Leap 2 that overlays real-time CPR feedback (compression depth\nand rate) and conducted a user study with simulated unexpected incidents (e.g.,\nbleeding) to evaluate SA, in which SA metrics were collected via observation\nand questionnaires administered during freeze-probe events. Eye tracking\nanalysis revealed that higher SA levels were associated with greater saccadic\namplitude and velocity, and with reduced proportion and frequency of fixations\non virtual content. To predict SA, we propose FixGraphPool, a graph neural\nnetwork that structures gaze events (fixations, saccades) into spatiotemporal\ngraphs, effectively capturing dynamic attentional patterns. Our model achieved\n83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and\nstate-of-the-art time-series models by leveraging domain knowledge and\nspatial-temporal information encoded in ET data. These findings demonstrate the\npotential of eye tracking for SA modeling in AR and highlight its utility in\ndesigning AR systems that ensure user safety and situational awareness."}
{"id": "2509.01093", "pdf": "https://arxiv.org/pdf/2509.01093.pdf", "abs": "https://arxiv.org/abs/2509.01093", "title": "Natural Context Drift Undermines the Natural Language Understanding of Large Language Models", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "How does the natural evolution of context paragraphs affect question\nanswering in generative Large Language Models (LLMs)? To investigate this, we\npropose a framework for curating naturally evolved, human-edited variants of\nreading passages from contemporary QA benchmarks and for analyzing LLM\nperformance across a range of semantic similarity scores, which quantify how\nclosely each variant aligns with content seen during pretraining. Using this\nframework, we evaluate six QA datasets and eight LLMs with publicly available\ntraining data. Our experiments reveal that LLM performance declines as reading\npassages naturally diverge from the versions encountered during\npretraining-even when the question and all necessary information remains\npresent at inference time. For instance, average model accuracy on BoolQ drops\nby over 30% from the highest to lowest similarity bins, with slopes exceeding\n70 across several LLMs. These findings suggest that natural text evolution\nposes a significant challenge to the language understanding capabilities of\nLLMs."}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452.pdf", "abs": "https://arxiv.org/abs/2508.11452", "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://www.tbox.cn/about/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking."}
{"id": "2509.01142", "pdf": "https://arxiv.org/pdf/2509.01142.pdf", "abs": "https://arxiv.org/abs/2509.01142", "title": "Dream-Coder 7B: An Open Diffusion Language Model for Code", "authors": ["Zhihui Xie", "Jiacheng Ye", "Lin Zheng", "Jiahui Gao", "Jingwei Dong", "Zirui Wu", "Xueliang Zhao", "Shansan Gong", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": null, "summary": "We present Dream-Coder 7B, an open-source discrete diffusion language model\nfor code generation that exhibits emergent any-order generation capabilities.\nUnlike traditional autoregressive (AR) models that decode strictly\nleft-to-right, Dream-Coder 7B adaptively determines its decoding strategy based\non the coding task: sketch-first generation for complex algorithms,\nleft-to-right generation for straightforward completions, and interleaved\nreasoning generation for code understanding tasks. We adapt a pretrained AR\ncheckpoint to a discrete diffusion frameworks with a continuous-time weighted\ncross-entropy objective. Our post-training recipe comprises (i) supervised\nfine-tuning, where we mitigate padding pathologies via random truncation and a\npadding penalty to improve sample efficiency and stabilize generation; and (ii)\nreinforcement learning with verifiable rewards over a curated high-quality\nprompt set drawn from open-source datasets, using a tailored reinforcement\nlearning recipe for diffusion language models. The resulting Dream-Coder 7B\nInstruct attains 21.4\\% pass@1 on LiveCodeBench (2410--2505) and demonstrates\ncompetitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We\nrelease Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training\nrecipes, preprocessing pipelines, and inference code to facilitate\nreproducibility and further research."}
{"id": "2509.01147", "pdf": "https://arxiv.org/pdf/2509.01147.pdf", "abs": "https://arxiv.org/abs/2509.01147", "title": "Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective", "authors": ["Zhihao Zhang", "Sophia Yat Mei Lee", "Dong Zhang", "Shoushan Li", "Guodong Zhou"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge\nfrom high-resource languages to low-resource languages. However, existing\nzero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language\n(LSL), where shared linguistic features facilitate effective knowledge\ntransfer. In contrast, for non-Latin script language (NSL), such as Chinese and\nJapanese, performance often degrades due to deep structural differences. To\naddress these challenges, we propose an entity-aligned translation (EAT)\napproach. Leveraging large language models (LLMs), EAT employs a\ndual-translation strategy to align entities between NSL and English. In\naddition, we fine-tune LLMs using multilingual Wikipedia data to enhance the\nentity alignment from source to target languages."}
{"id": "2509.01158", "pdf": "https://arxiv.org/pdf/2509.01158.pdf", "abs": "https://arxiv.org/abs/2509.01158", "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively."}
{"id": "2509.01166", "pdf": "https://arxiv.org/pdf/2509.01166.pdf", "abs": "https://arxiv.org/abs/2509.01166", "title": "Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning", "authors": ["Yu Liu", "Yanan Cao", "Xixun Lin", "Yanmin Shang", "Shi Wang", "Shirui Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025, Main, Long Paper", "summary": "Knowledge graph completion (KGC) aims to infer new knowledge and make\npredictions from knowledge graphs. Recently, large language models (LLMs) have\nexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily\nfocus on designing task-specific instructions, achieving promising\nadvancements. However, there are still two critical challenges. First, existing\nmethods often ignore the inconsistent representation spaces between natural\nlanguage and graph structures. Second, most approaches design separate\ninstructions for different KGC tasks, leading to duplicate works and\ntime-consuming processes. To address these challenges, we propose SAT, a novel\nframework that enhances LLMs for KGC via structure-aware alignment-tuning.\nSpecifically, we first introduce hierarchical knowledge alignment to align\ngraph embeddings with the natural language space through multi-task contrastive\nlearning. Then, we propose structural instruction tuning to guide LLMs in\nperforming structure-aware reasoning over KGs, using a unified graph\ninstruction combined with a lightweight knowledge adapter. Experimental results\non two KGC tasks across four benchmark datasets demonstrate that SAT\nsignificantly outperforms state-of-the-art methods, especially in the link\nprediction task with improvements ranging from 8.7% to 29.8%."}
{"id": "2509.01185", "pdf": "https://arxiv.org/pdf/2509.01185.pdf", "abs": "https://arxiv.org/abs/2509.01185", "title": "Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation", "authors": ["Seganrasan Subramanian", "Abhigya Verma"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs."}
{"id": "2509.01186", "pdf": "https://arxiv.org/pdf/2509.01186.pdf", "abs": "https://arxiv.org/abs/2509.01186", "title": "Statutory Construction and Interpretation for Artificial Intelligence", "authors": ["Luxi He", "Nimra Nadeem", "Michel Liao", "Howard Chen", "Danqi Chen", "Mariano-Florentino Cuéllar", "Peter Henderson"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "AI systems are increasingly governed by natural language principles, yet a\nkey challenge arising from reliance on language remains underexplored:\ninterpretive ambiguity. As in legal systems, ambiguity arises both from how\nthese principles are written and how they are applied. But while legal systems\nuse institutional safeguards to manage such ambiguity, such as transparent\nappellate review policing interpretive constraints, AI alignment pipelines\noffer no comparable protections. Different interpretations of the same rule can\nlead to inconsistent or unstable model behavior. Drawing on legal theory, we\nidentify key gaps in current alignment pipelines by examining how legal systems\nconstrain ambiguity at both the rule creation and rule application steps. We\nthen propose a computational framework that mirrors two legal mechanisms: (1) a\nrule refinement pipeline that minimizes interpretive disagreement by revising\nambiguous rules (analogous to agency rulemaking or iterative legislative\naction), and (2) prompt-based interpretive constraints that reduce\ninconsistency in rule application (analogous to legal canons that guide\njudicial discretion). We evaluate our framework on a 5,000-scenario subset of\nthe WildChat dataset and show that both interventions significantly improve\njudgment consistency across a panel of reasonable interpreters. Our approach\noffers a first step toward systematically managing interpretive ambiguity, an\nessential step for building more robust, law-following AI systems."}
{"id": "2509.01190", "pdf": "https://arxiv.org/pdf/2509.01190.pdf", "abs": "https://arxiv.org/abs/2509.01190", "title": "Efficient Large Language Models with Zero-Shot Adjustable Acceleration", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "categories": ["cs.CL"], "comment": null, "summary": "Using Large Language Models (LLMs) in real-world applications presents\nsignificant challenges, particularly in balancing computational efficiency and\nperformance. Optimizing acceleration after the fine-tuning phase and during\ninference is crucial for building an efficient architecture. This paper\nintroduces Zero-Shot Adjustable Acceleration, a novel training and inference\nmethod that dynamically adjusts hardware usage during inference without\nrequiring additional fine-tuning. The proposed approach is applied to newly\ndeveloped models and evaluated across multiple classification and text\ngeneration tasks. Experimental results demonstrate that the method enables a\nwide range of acceleration in a zero-shot manner and achieves up to a 11x\nspeedup compared to the baseline."}
{"id": "2509.01200", "pdf": "https://arxiv.org/pdf/2509.01200.pdf", "abs": "https://arxiv.org/abs/2509.01200", "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "authors": ["Chenyang Le", "Bing Han", "Jinshun Li", "Songyong Chen", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs."}
{"id": "2509.01213", "pdf": "https://arxiv.org/pdf/2509.01213.pdf", "abs": "https://arxiv.org/abs/2509.01213", "title": "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth", "authors": ["Ege Süalp", "Mina Rezaei"], "categories": ["cs.CL"], "comment": null, "summary": "Catastrophic forgetting is a significant challenge in continual learning, in\nwhich a model loses prior knowledge when it is fine-tuned on new tasks. This\nproblem is particularly critical for large language models (LLMs) undergoing\ncontinual learning, as retaining performance across diverse domains is\nimportant for their general utility. In this paper, we explore model growth, a\npromising strategy that leverages smaller models to expedite and structure the\ntraining of larger ones for mitigating the catastrophic forgetting problem.\nAlthough growth-based pretraining, particularly via transformer stacking, has\nshown promise in accelerating convergence, its impact on forgetting remains\nunder-explored. Therefore, we evaluate whether growth-based models can retain\npreviously learned capabilities more effectively across a sequence of\nfine-tuning tasks involving domain knowledge, reasoning, reading comprehension,\nand bias. Our findings show that both models -- one trained with growth (Stack\nLLM) and one without (LLM) -- exhibit improvements in domain knowledge.\nHowever, reasoning and reading comprehension degrade over time, indicating\nsigns of catastrophic forgetting. Stack LLM consistently shows less\ndegradation, especially in reading comprehension, suggesting enhanced retention\ncapabilities. Interestingly, in bias evaluation, the baseline LLM becomes\nprogressively more neutral with continued fine-tuning, while Stack LLM\nmaintains a steady bias ratio around 60--61\\%. These results indicate that\ngrowth-based pretraining may deliver modest improvements in resisting\ncatastrophic forgetting, though trade-offs remain in handling social biases."}
{"id": "2509.01221", "pdf": "https://arxiv.org/pdf/2509.01221.pdf", "abs": "https://arxiv.org/abs/2509.01221", "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression", "authors": ["Wei Huang", "Huang Wei", "Yinggui Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time."}
{"id": "2509.01236", "pdf": "https://arxiv.org/pdf/2509.01236.pdf", "abs": "https://arxiv.org/abs/2509.01236", "title": "Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors", "authors": ["Hao Yang", "Zhiyu Yang", "Yunjie Zhang", "Shanyi Zhu", "Lin Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing\nmodel inference capabilities. Despite growing interest in Chain-of-Thought\nreasoning, its underlying mechanisms remain unclear. This paper explores the\nworking mechanisms of Chain-of-Thought reasoning from the perspective of the\ndual relationship between in-context learning and pretrained priors. We first\nconduct a fine-grained lexical-level analysis of rationales to examine the\nmodel's reasoning behavior. Then, by incrementally introducing noisy exemplars,\nwe examine how the model balances pretrained priors against erroneous\nin-context information. Finally, we investigate whether prompt engineering can\ninduce slow thinking in large language models. Our extensive experiments reveal\nthree key findings: (1) The model not only quickly learns the reasoning\nstructure at the lexical level but also grasps deeper logical reasoning\npatterns, yet it heavily relies on pretrained priors. (2) Providing sufficient\nexemplars shifts the model's decision-making from pretrained priors to\nin-context signals, while misleading prompts introduce instability. (3) Long\nChain-of-Thought prompting can induce the model to generate longer reasoning\nchains, thereby improving its performance on downstream tasks."}
{"id": "2509.01260", "pdf": "https://arxiv.org/pdf/2509.01260.pdf", "abs": "https://arxiv.org/abs/2509.01260", "title": "Annotation and modeling of emotions in a textual corpus: an evaluative approach", "authors": ["Jonas Noblet"], "categories": ["cs.CL"], "comment": "in French language. 27{\\`e}me Rencontre des {\\'E}tudiants Chercheurs\n  en Informatique pour le Traitement Automatique des Langues (RECITAL), Jun\n  2025, Marseille, France", "summary": "Emotion is a crucial phenomenon in the functioning of human beings in\nsociety. However, it remains a widely open subject, particularly in its textual\nmanifestations. This paper examines an industrial corpus manually annotated\nfollowing an evaluative approach to emotion. This theoretical framework, which\nis currently underutilized, offers a different perspective that complements\ntraditional approaches. Noting that the annotations we collected exhibit\nsignificant disagreement, we hypothesized that they nonetheless follow stable\nstatistical trends. Using language models trained on these annotations, we\ndemonstrate that it is possible to model the labeling process and that\nvariability is driven by underlying linguistic features. Conversely, our\nresults indicate that language models seem capable of distinguishing emotional\nsituations based on evaluative criteria."}
{"id": "2509.01301", "pdf": "https://arxiv.org/pdf/2509.01301.pdf", "abs": "https://arxiv.org/abs/2509.01301", "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation", "authors": ["Juhyun Oh", "Inha Cha", "Michael Saxon", "Hyunseung Lim", "Shaily Bhatt", "Alice Oh"], "categories": ["cs.CL"], "comment": null, "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies."}
{"id": "2509.01312", "pdf": "https://arxiv.org/pdf/2509.01312.pdf", "abs": "https://arxiv.org/abs/2509.01312", "title": "TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering", "authors": ["Sishi Xiong", "Ziyang He", "Zhongjiang He", "Yu Zhao", "Changzai Pan", "Jie Zhang", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown promise in the table question\nanswering (TQA) task through prompt engineering, they face challenges in\nindustrial applications, including structural heterogeneity, difficulties in\ntarget data localization, and bottlenecks in complex reasoning. To address\nthese limitations, this paper presents TableZoomer, a novel LLM-powered,\nprogramming-based agent framework. It introduces three key innovations: (1)\nreplacing the original fully verbalized table with structured table schema to\nbridge the semantic gap and reduce computational complexity; (2) a query-aware\ntable zooming mechanism that dynamically generates sub-table schema through\ncolumn selection and entity linking, significantly improving target\nlocalization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that\ntransforms queries into executable code to mitigate numerical hallucination.\nAdditionally, we integrate the reasoning workflow with the ReAct paradigm to\nenable iterative reasoning. Extensive experiments demonstrate that our\nframework maintains the usability advantages while substantially enhancing\nperformance and scalability across tables of varying scales. When implemented\nwith the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of\n19.34% and 25% over conventional PoT methods on the large-scale DataBench\ndataset and the small-scale Fact Checking task of TableBench dataset,\nrespectively."}
{"id": "2509.01314", "pdf": "https://arxiv.org/pdf/2509.01314.pdf", "abs": "https://arxiv.org/abs/2509.01314", "title": "Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization", "authors": ["Anum Afzal", "Mehul Kumawat", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), being generic task solvers, are versatile.\nHowever, despite the vast amount of data they are trained on, there are\nspeculations about their adaptation capabilities to a new domain. Additionally,\nthe simple fine-tuning of the model to incorporate knowledge of a new domain is\ncomputationally expensive and time-consuming. This becomes more challenging\nwhen the domain in question is also low-resource, and labeled data is\nunavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on\nhigh-resource datasets to address these challenges to improve performance on\nunseen low-resource domains. Throughout our experiments, we evaluate whether\nintrinsic linguistic commonalities between datasets can be leveraged for\nefficient domain adaptation. We benchmark six PEFTs with\n\\texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific,\nMedical, Legal, and News domains for a Text Summarization task. Our experiments\nshow that for low-resource domains, inference using Within-Domain Adapters can\nachieve better performance than Few-Shot as well as a much larger\n\\texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain\nAdapters, we explore the concept of using Cross-Domain Adapters as well as the\nstrategic combinations of adapters to leverage intrinsic language similarities\nacross domains, facilitating better adaptability and performance in\nlow-resource settings."}
{"id": "2509.01322", "pdf": "https://arxiv.org/pdf/2509.01322.pdf", "abs": "https://arxiv.org/abs/2509.01322", "title": "LongCat-Flash Technical Report", "authors": ["Meituan LongCat Team", "Bayan", "Bei Li", "Bingye Lei", "Bo Wang", "Bolin Rong", "Chao Wang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Cheng Sun", "Chengcheng Han", "Chenguang Xi", "Chi Zhang", "Chong Peng", "Chuan Qin", "Chuyu Zhang", "Cong Chen", "Congkui Wang", "Dan Ma", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Deyang Kong", "Dishan Liu", "Feiye Huo", "Fengcun Li", "Fubao Zhang", "Gan Dong", "Gang Liu", "Gang Xu", "Ge Li", "Guoqiang Tan", "Guoyuan Lin", "Haihang Jing", "Haomin Fu", "Haonan Yan", "Haoxing Wen", "Haozhe Zhao", "Hong Liu", "Hongmei Shi", "Hongyan Hao", "Hongyin Tang", "Huantian Lv", "Hui Su", "Jiacheng Li", "Jiahao Liu", "Jiahuan Li", "Jiajun Yang", "Jiaming Wang", "Jian Yang", "Jianchao Tan", "Jiaqi Sun", "Jiaqi Zhang", "Jiawei Fu", "Jiawei Yang", "Jiaxi Hu", "Jiayu Qin", "Jingang Wang", "Jiyuan He", "Jun Kuang", "Junhui Mei", "Kai Liang", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Liang Gao", "Liang Shi", "Lianhui Ma", "Lin Qiu", "Lingbin Kong", "Lingtong Si", "Linkun Lyu", "Linsen Guo", "Liqi Yang", "Lizhi Yan", "Mai Xia", "Man Gao", "Manyuan Zhang", "Meng Zhou", "Mengxia Shen", "Mingxiang Tuo", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Peng Zhao", "Pengcheng Jia", "Pingwei Sun", "Qi Gu", "Qianyun Li", "Qingyuan Li", "Qiong Huang", "Qiyuan Duan", "Ran Meng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shizhe Wu", "Shuai Liang", "Shuo Wang", "Suogui Dang", "Tao Fang", "Tao Li", "Tefeng Chen", "Tianhao Bai", "Tianhao Zhou", "Tingwen Xie", "Wei He", "Wei Huang", "Wei Liu", "Wei Shi", "Wei Wang", "Wei Wu", "Weikang Zhao", "Wen Zan", "Wenjie Shi", "Xi Nan", "Xi Su", "Xiang Li", "Xiang Mei", "Xiangyang Ji", "Xiangyu Xi", "Xiangzhou Huang", "Xianpeng Li", "Xiao Fu", "Xiao Liu", "Xiao Wei", "Xiaodong Cai", "Xiaolong Chen", "Xiaoqing Liu", "Xiaotong Li", "Xiaowei Shi", "Xiaoyu Li", "Xili Wang", "Xin Chen", "Xing Hu", "Xingyu Miao", "Xinyan He", "Xuemiao Zhang", "Xueyuan Hao", "Xuezhi Cao", "Xunliang Cai", "Xurui Yang", "Yan Feng", "Yang Bai", "Yang Chen", "Yang Yang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yifan Zhang", "Yipeng Zang", "Yitao Zhai", "Yiyang Li", "Yongjing Yin", "Yongkang Lv", "Yongwei Zhou", "Yu Yang", "Yuchen Xie", "Yueqing Sun", "Yuewen Zheng", "Yuhua Wei", "Yulei Qian", "Yunfan Liang", "Yunfang Tai", "Yunke Zhao", "Zeyang Yu", "Zhao Zhang", "Zhaohua Yang", "Zhenchao Zhang", "Zhikang Xia", "Zhiye Zou", "Zhizhao Zeng", "Zhongda Su", "Zhuofan Chen", "Zijian Zhang", "Ziwen Wang", "Zixu Jiang", "Zizhe Zhao", "Zongyu Wang", "Zunhai Su"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat"}
{"id": "2509.01324", "pdf": "https://arxiv.org/pdf/2509.01324.pdf", "abs": "https://arxiv.org/abs/2509.01324", "title": "KoBLEX: Open Legal Question Answering with Multi-hop Reasoning", "authors": ["Jihyung Lee", "Daehui Kim", "Seonjeong Hwang", "Hyounghun Kim", "Gary Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLM) have achieved remarkable performances in general\ndomains and are now extending into the expert domain of law. Several benchmarks\nhave been proposed to evaluate LLMs' legal capabilities. However, these\nbenchmarks fail to evaluate open-ended and provision-grounded Question\nAnswering (QA). To address this, we introduce a Korean Benchmark for Legal\nEXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop\nlegal reasoning. KoBLEX includes 226 scenario-based QA instances and their\nsupporting provisions, created using a hybrid LLM-human expert pipeline. We\nalso propose a method called Parametric provision-guided Selection Retrieval\n(ParSeR), which uses LLM-generated parametric provisions to guide legally\ngrounded and reliable answers. ParSeR facilitates multi-hop reasoning on\ncomplex legal questions by generating parametric provisions and employing a\nthree-stage sequential retrieval process. Furthermore, to better evaluate the\nlegal fidelity of the generated answers, we propose Legal Fidelity Evaluation\n(LF-Eval). LF-Eval is an automatic metric that jointly considers the question,\nanswer, and supporting provisions and shows a high correlation with human\njudgments. Experimental results show that ParSeR consistently outperforms\nstrong baselines, achieving the best results across multiple LLMs. Notably,\ncompared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1\nand +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently\ndelivers consistent performance across reasoning depths, with ablations\nconfirming the effectiveness of ParSeR."}
{"id": "2509.01328", "pdf": "https://arxiv.org/pdf/2509.01328.pdf", "abs": "https://arxiv.org/abs/2509.01328", "title": "Can Large Language Models Master Complex Card Games?", "authors": ["Wei Wang", "Fuqing Bie", "Junzhe Chen", "Dan Zhang", "Shiyu Huang", "Evgeny Kharlamov", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs."}
{"id": "2509.01363", "pdf": "https://arxiv.org/pdf/2509.01363.pdf", "abs": "https://arxiv.org/abs/2509.01363", "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic", "authors": ["Mohammad Zbeeb", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments."}
{"id": "2509.01379", "pdf": "https://arxiv.org/pdf/2509.01379.pdf", "abs": "https://arxiv.org/abs/2509.01379", "title": "WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data", "authors": ["Paloma Piot", "Diego Sánchez", "Javier Parapar"], "categories": ["cs.CL"], "comment": null, "summary": "Online harms are a growing problem in digital spaces, putting user safety at\nrisk and reducing trust in social media platforms. One of the most persistent\nforms of harm is hate speech. To address this, we need tools that combine the\nspeed and scale of automated systems with the judgment and insight of human\nmoderators. These tools should not only find harmful content but also explain\ntheir decisions clearly, helping to build trust and understanding. In this\npaper, we present WATCHED, a chatbot designed to support content moderators in\ntackling hate speech. The chatbot is built as an Artificial Intelligence Agent\nsystem that uses Large Language Models along with several specialised tools. It\ncompares new posts with real examples of hate speech and neutral content, uses\na BERT-based classifier to help flag harmful messages, looks up slang and\ninformal language using sources like Urban Dictionary, generates\nchain-of-thought reasoning, and checks platform guidelines to explain and\nsupport its decisions. This combination allows the chatbot not only to detect\nhate speech but to explain why content is considered harmful, grounded in both\nprecedent and policy. Experimental results show that our proposed method\nsurpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91.\nDesigned for moderators, safety teams, and researchers, the tool helps reduce\nonline harms by supporting collaboration between AI and human oversight."}
{"id": "2509.01387", "pdf": "https://arxiv.org/pdf/2509.01387.pdf", "abs": "https://arxiv.org/abs/2509.01387", "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "authors": ["Serwar Basch", "Ilia Kuznetsov", "Tom Hope", "Iryna Gurevych"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Understanding fine-grained relations between documents is crucial for many\napplication domains. However, the study of automated assistance is limited by\nthe lack of efficient methods to create training and evaluation datasets of\ncross-document links. To address this, we introduce a new domain-agnostic\nframework for selecting a best-performing approach and annotating\ncross-document links in a new domain from scratch. We first generate and\nvalidate semi-synthetic datasets of interconnected documents. This data is used\nto perform automatic evaluation, producing a shortlist of best-performing\nlinking approaches. These approaches are then used in an extensive human\nevaluation study, yielding performance estimates on natural text pairs. We\napply our framework in two distinct domains -- peer review and news -- and show\nthat combining retrieval models with LLMs achieves 78\\% link approval from\nhuman raters, more than doubling the precision of strong retrievers alone. Our\nframework enables systematic study of cross-document understanding across\napplication scenarios, and the resulting novel datasets lay foundation for\nnumerous cross-document tasks like media framing and peer review. We make the\ncode, data, and annotation protocols openly available."}
{"id": "2509.01390", "pdf": "https://arxiv.org/pdf/2509.01390.pdf", "abs": "https://arxiv.org/abs/2509.01390", "title": "Analysing the Language of Neural Audio Codecs", "authors": ["Joonyong Park", "Shinnosuke Takamichi", "David M. Chan", "Shunsuke Kando", "Yuki Saito", "Hiroshi Saruwatari"], "categories": ["cs.CL", "eess.AS"], "comment": "In Proceedings of 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2025)", "summary": "This study presents a comparative analysis of the statistical and linguistic\nproperties of neural audio codecs (NACs). We investigate discrete speech tokens\nproduced by various NAC models, examining their adherence to linguistic\nstatistical laws such as Zipf's law and Heaps' law, as well as their entropy\nand redundancy. To assess how these token-level properties relate to semantic\nand acoustic preservation in synthesized speech, we evaluate intelligibility\nusing error rates of automatic speech recognition, and quality using the UTMOS\nscore. Our results reveal that NAC tokens, particularly 3-grams, exhibit\nlanguage-like statistical patterns. Moreover, these properties, together with\nmeasures of information content, are found to correlate with improved\nperformances in speech recognition and resynthesis tasks. These findings offer\ninsights into the structure of NAC token sequences and inform the design of\nmore effective generative speech models."}
{"id": "2509.01395", "pdf": "https://arxiv.org/pdf/2509.01395.pdf", "abs": "https://arxiv.org/abs/2509.01395", "title": "LLMs cannot spot math errors, even when allowed to peek into the solution", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."}
{"id": "2509.01412", "pdf": "https://arxiv.org/pdf/2509.01412.pdf", "abs": "https://arxiv.org/abs/2509.01412", "title": "Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning", "authors": ["Kaviraj Pather", "Elena Hadjigeorgiou", "Arben Krasniqi", "Claire Schmit", "Irina Rusu", "Marc Pons", "Kabir Khan"], "categories": ["cs.CL", "68T07, 68T50, 68T05", "I.2.7; I.2.6; I.2.8; H.5.2"], "comment": "12 pages, 7 figures", "summary": "Large language models (LLMs) show strong reasoning via chain-of-thought (CoT)\nprompting, but the process is opaque, which makes verification, debugging, and\ncontrol difficult in high-stakes settings. We present Vis-CoT, a\nhuman-in-the-loop framework that converts linear CoT text into an interactive\nreasoning graph. Users can visualize the logical flow, identify flawed steps,\nand intervene by pruning incorrect paths and grafting new, user-defined\npremises. This shifts interaction from passive observation to active\ncollaboration, steering models toward more accurate and trustworthy\nconclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer\naccuracy by up to 24 percentage points over non-interactive baselines. A user\nstudy also shows large gains in perceived usability and trust. Vis-CoT points\nto a practical path for more reliable, understandable, and collaborative\nreasoning by combining LLMs with targeted human oversight."}
{"id": "2509.01418", "pdf": "https://arxiv.org/pdf/2509.01418.pdf", "abs": "https://arxiv.org/abs/2509.01418", "title": "On the Alignment of Large Language Models with Global Human Opinion", "authors": ["Yang Liu", "Masahiro Kaneko", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "23 pages, 19 figures", "summary": "Today's large language models (LLMs) are capable of supporting multilingual\nscenarios, allowing users to interact with LLMs in their native languages. When\nLLMs respond to subjective questions posed by users, they are expected to align\nwith the views of specific demographic groups or historical periods, shaped by\nthe language in which the user interacts with the model. Existing studies\nmainly focus on researching the opinions represented by LLMs among demographic\ngroups in the United States or a few countries, lacking worldwide country\nsamples and studies on human opinions in different historical periods, as well\nas lacking discussion on using language to steer LLMs. Moreover, they also\noverlook the potential influence of prompt language on the alignment of LLMs'\nopinions. In this study, our goal is to fill these gaps. To this end, we create\nan evaluation framework based on the World Values Survey (WVS) to\nsystematically assess the alignment of LLMs with human opinions across\ndifferent countries, languages, and historical periods around the world. We\nfind that LLMs appropriately or over-align the opinions with only a few\ncountries while under-aligning the opinions with most countries. Furthermore,\nchanging the language of the prompt to match the language used in the\nquestionnaire can effectively steer LLMs to align with the opinions of the\ncorresponding country more effectively than existing steering methods. At the\nsame time, LLMs are more aligned with the opinions of the contemporary\npopulation. To our knowledge, our study is the first comprehensive\ninvestigation of the topic of opinion alignment in LLMs across global,\nlanguage, and temporal dimensions. Our code and data are publicly available at\nhttps://github.com/nlply/global-opinion-alignment."}
{"id": "2509.01455", "pdf": "https://arxiv.org/pdf/2509.01455.pdf", "abs": "https://arxiv.org/abs/2509.01455", "title": "Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal", "authors": ["Markus Oehri", "Giulia Conti", "Kaviraj Pather", "Alexandre Rossi", "Laia Serra", "Adrian Parody", "Rogvi Johannesen", "Aviaja Petersen", "Arben Krasniqi"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 5 figures", "summary": "Deployed language models must decide not only what to answer but also when\nnot to answer. We present UniCR, a unified framework that turns heterogeneous\nuncertainty evidence including sequence likelihoods, self-consistency\ndispersion, retrieval compatibility, and tool or verifier feedback into a\ncalibrated probability of correctness and then enforces a user-specified error\nbudget via principled refusal. UniCR learns a lightweight calibration head with\ntemperature scaling and proper scoring, supports API-only models through\nblack-box features, and offers distribution-free guarantees using conformal\nrisk control. For long-form generation, we align confidence with semantic\nfidelity by supervising on atomic factuality scores derived from retrieved\nevidence, reducing confident hallucinations while preserving coverage.\nExperiments on short-form QA, code generation with execution tests, and\nretrieval-augmented long-form QA show consistent improvements in calibration\nmetrics, lower area under the risk-coverage curve, and higher coverage at fixed\nrisk compared to entropy or logit thresholds, post-hoc calibrators, and\nend-to-end selective baselines. Analyses reveal that evidence contradiction,\nsemantic dispersion, and tool inconsistency are the dominant drivers of\nabstention, yielding informative user-facing refusal messages. The result is a\nportable recipe of evidence fusion to calibrated probability to risk-controlled\ndecision that improves trustworthiness without fine-tuning the base model and\nremains valid under distribution shift."}
{"id": "2509.01468", "pdf": "https://arxiv.org/pdf/2509.01468.pdf", "abs": "https://arxiv.org/abs/2509.01468", "title": "Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) encode vast amounts of world knowledge but\nremain static once trained, making the timely integration of emerging facts\nprohibitively expensive via full retraining. Knowledge-editing techniques have\nthus emerged to inject or overwrite specific facts into LLMs, yet they either\nover-rely on superficial cues or incur complex, iterative pipelines that\ncollapse under noisy, multi-hop conditions. We introduce Reason-KE, an\nend-to-end reasoning-chain-based editing framework that steers a pretrained LLM\nthrough four structured stages-fact acknowledgment, relevance determination,\nselective application, and final reasoning-to filter distractors in a single\npass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates\nQwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop\nunder heavy distraction and <1% when answers are leaked. Our quantitative\nanalysis confirms Reason-KE's resilience and efficiency, establishing a new\nstate-of-the-art for reliable LLM knowledge updates."}
{"id": "2509.01476", "pdf": "https://arxiv.org/pdf/2509.01476.pdf", "abs": "https://arxiv.org/abs/2509.01476", "title": "Do Retrieval Augmented Language Models Know When They Don't Know?", "authors": ["Youchao Zhou", "Heyan Huang", "Yicheng Liu", "Rui Dai", "Xinglin Wang", "Xingchen Zhang", "Shumin Shi", "Yang Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems."}
{"id": "2509.01514", "pdf": "https://arxiv.org/pdf/2509.01514.pdf", "abs": "https://arxiv.org/abs/2509.01514", "title": "MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models", "authors": ["Andreas Ottem"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 7 figures, held online presentation at NLPA 2025", "summary": "Retrieval-Augmented Generation (RAG) systems typically face constraints\nbecause of their inherent mechanism: a simple top-k semantic search [1]. The\napproach often leads to the incorporation of irrelevant or redundant\ninformation in the context, degrading performance and efficiency [10][11]. This\npaper presents MeVe, a novel modular architecture intended for Memory\nVerification and smart context composition. MeVe rethinks the RAG paradigm by\nproposing a five-phase modular design that distinctly breaks down the retrieval\nand context composition process into distinct, auditable, and independently\ntunable phases: initial retrieval, relevance verification, fallback retrieval,\ncontext prioritization, and token budgeting. This architecture enables\nfine-grained control of what knowledge is made available to an LLM, enabling\ntask-dependent filtering and adaptation. We release a reference implementation\nof MeVe as a proof of concept and evaluate its performance on knowledge-heavy\nQA tasks over a subset of English Wikipedia [22]. Our results demonstrate that\nby actively verifying information before composition, MeVe significantly\nimproves context efficiency, achieving a 57% reduction on the Wikipedia dataset\nand a 75% reduction on the more complex HotpotQA dataset compared to standard\nRAG implementations [25]. This work provides a framework for more scalable and\nreliable LLM applications. By refining and distilling contextual information,\nMeVe offers a path toward better grounding and more accurate factual support\n[16]."}
{"id": "2509.01529", "pdf": "https://arxiv.org/pdf/2509.01529.pdf", "abs": "https://arxiv.org/abs/2509.01529", "title": "Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community", "authors": ["Thomas Compton"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures, conference paper", "summary": "This paper presents a comparative analysis of community unionism (CU) in two\ndistinct historical and organizational contexts: the National Boot and Shoe\nUnion (B\\&S) in the 1920s and Unite Community in the 2010s--2020s. Using\nBERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency\nanalysis, the study examines the extent to which each union's discourse aligns\nwith key features of CU -- such as coalition-building, grassroots engagement,\nand action beyond the workplace. The results reveal significant differences in\nthematic focus and discursive coherence. While Unite Community demonstrates\nstronger alignment with outward-facing, social justice-oriented themes, the\nB\\&S corpus emphasizes internal administration, industrial relations, and\nmember services -- reflecting a more traditional, servicing-oriented union\nmodel. The analysis also highlights methodological insights, demonstrating how\nmodern NLP techniques can enhance the study of historical labor archives.\nUltimately, the findings suggest that while both unions engage with\ncommunity-related themes, their underlying models of engagement diverge\nsignificantly, challenging assumptions about the continuity and universality of\ncommunity unionism across time and sector."}
{"id": "2509.01535", "pdf": "https://arxiv.org/pdf/2509.01535.pdf", "abs": "https://arxiv.org/abs/2509.01535", "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "authors": ["Kairong Han", "Wenshuo Zhao", "Ziyu Zhao", "JunJian Ye", "Lujia Pan", "Kun Kuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2025 Main conference", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT."}
{"id": "2509.01560", "pdf": "https://arxiv.org/pdf/2509.01560.pdf", "abs": "https://arxiv.org/abs/2509.01560", "title": "In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents", "authors": ["Seungkyu Lee", "Nalim Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool agents -- LLM-based systems that interact with external APIs -- offer a\nway to execute real-world tasks. However, as tasks become increasingly complex,\nthese agents struggle to identify and call the correct APIs in the proper\norder. To tackle this problem, we investigate converting API documentation into\na structured API graph that captures API dependencies and leveraging it for\nmulti-tool queries that require compositional API calls. To support this, we\nintroduce In-N-Out, the first expert-annotated dataset of API graphs built from\ntwo real-world API benchmarks and their documentation. Using In-N-Out\nsignificantly improves performance on both tool retrieval and multi-tool query\ngeneration, nearly doubling that of LLMs using documentation alone. Moreover,\ngraphs generated by models fine-tuned on In-N-Out close 90% of this gap,\nshowing that our dataset helps models learn to comprehend API documentation and\nparameter relationships. Our findings highlight the promise of using explicit\nAPI graphs for tool agents and the utility of In-N-Out as a valuable resource.\nWe will release the dataset and code publicly."}
{"id": "2509.01564", "pdf": "https://arxiv.org/pdf/2509.01564.pdf", "abs": "https://arxiv.org/abs/2509.01564", "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief", "authors": ["Zeguan Xiao", "Diyang Dou", "Boya Xiong", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, but often exhibit overconfidence and generate\nplausible yet incorrect answers. This overconfidence, especially in models\nundergone Reinforcement Learning from Human Feedback (RLHF), poses significant\nchallenges for reliable uncertainty estimation and safe deployment. In this\npaper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel\nself-evaluation-based calibration method that leverages the internal hidden\nstates of LLMs to derive more accurate confidence scores. Instead of relying on\nthe model's final output, our approach extracts internal beliefs from multiple\nintermediate layers during self-evaluation. By aggregating these layer-wise\nbeliefs and calculating the expectation over the resulting confidence score\ndistribution, EAGLE produces a refined confidence score that more faithfully\nreflects the model's internal certainty. Extensive experiments on diverse\ndatasets and LLMs demonstrate that EAGLE significantly improves calibration\nperformance over existing baselines. We also provide an in-depth analysis of\nEAGLE, including a layer-wise examination of uncertainty patterns, a study of\nthe impact of self-evaluation prompts, and an analysis of the effect of\nself-evaluation score range."}
{"id": "2509.01606", "pdf": "https://arxiv.org/pdf/2509.01606.pdf", "abs": "https://arxiv.org/abs/2509.01606", "title": "Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply", "authors": ["Vivi Nastase", "Paola Merlo"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 10 figures", "summary": "Transformer models learn to encode and decode an input text, and produce\ncontextual token embeddings as a side-effect. The mapping from language into\nthe embedding space maps words expressing similar concepts onto points that are\nclose in the space. In practice, the reverse implication is also assumed: words\ncorresponding to close points in this space are similar or related, those that\nare further are not.\n  Does closeness in the embedding space extend to shared properties for\nsentence embeddings? We present an investigation of sentence embeddings and\nshow that the geometry of their embedding space is not predictive of their\nrelative performances on a variety of tasks.\n  We compute sentence embeddings in three ways: as averaged token embeddings,\nas the embedding of the special [CLS] token, and as the embedding of a random\ntoken from the sentence. We explore whether there is a correlation between the\ndistance between sentence embedding variations and their performance on\nlinguistic tasks, and whether despite their distances, they do encode the same\ninformation in the same manner.\n  The results show that the cosine similarity -- which treats dimensions\nshallowly -- captures (shallow) commonalities or differences between sentence\nembeddings, which are not predictive of their performance on specific tasks.\nLinguistic information is rather encoded in weighted combinations of different\ndimensions, which are not reflected in the geometry of the sentence embedding\nspace."}
{"id": "2509.01620", "pdf": "https://arxiv.org/pdf/2509.01620.pdf", "abs": "https://arxiv.org/abs/2509.01620", "title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry", "authors": ["Shanshan Wang", "Junchao Wu", "Fengying Ye", "Jingming Yao", "Lidia S. Chao", "Derek F. Wong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "The rapid development of advanced large language models (LLMs) has made\nAI-generated text indistinguishable from human-written text. Previous work on\ndetecting AI-generated text has made effective progress, but has not involved\nmodern Chinese poetry. Due to the distinctive characteristics of modern Chinese\npoetry, it is difficult to identify whether a poem originated from humans or\nAI. The proliferation of AI-generated modern Chinese poetry has significantly\ndisrupted the poetry ecosystem. Based on the urgency of identifying\nAI-generated poetry in the real Chinese world, this paper proposes a novel\nbenchmark for detecting LLMs-generated modern Chinese poetry. We first\nconstruct a high-quality dataset, which includes both 800 poems written by six\nprofessional poets and 41,600 poems generated by four mainstream LLMs.\nSubsequently, we conduct systematic performance assessments of six detectors on\nthis dataset. Experimental results demonstrate that current detectors cannot be\nused as reliable tools to detect modern Chinese poems generated by LLMs. The\nmost difficult poetic features to detect are intrinsic qualities, especially\nstyle. The detection results verify the effectiveness and necessity of our\nproposed benchmark. Our work lays a foundation for future detection of\nAI-generated poetry."}
{"id": "2509.01640", "pdf": "https://arxiv.org/pdf/2509.01640.pdf", "abs": "https://arxiv.org/abs/2509.01640", "title": "TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring", "authors": ["Hind Aljuaid", "Areej Alhothali", "Ohoud Al-Zamzami", "Hussein Assalahi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Essay writing is a critical component of student assessment, yet manual\nscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)\noffers a promising alternative, but current approaches face limitations. Recent\nstudies have incorporated Graph Neural Networks (GNNs) into AES using static\nword embeddings that fail to capture contextual meaning, especially for\npolysemous words. Additionally, many methods rely on holistic scoring,\noverlooking specific writing aspects such as grammar, vocabulary, and cohesion.\nTo address these challenges, this study proposes TransGAT, a novel approach\nthat integrates fine-tuned Transformer models with GNNs for analytic scoring.\nTransGAT combines the contextual understanding of Transformers with the\nrelational modeling strength of Graph Attention Networks (GAT). It performs\ntwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,\nand DeBERTaV3) with a separate GAT. In each pair, the first stream generates\nessay-level predictions, while the second applies GAT to Transformer token\nembeddings, with edges constructed from syntactic dependencies. The model then\nfuses predictions from both streams to produce the final analytic score.\nExperiments on the ELLIPSE dataset show that TransGAT outperforms baseline\nmodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all\nanalytic scoring dimensions. These findings highlight the potential of TransGAT\nto advance AES systems."}
{"id": "2509.01654", "pdf": "https://arxiv.org/pdf/2509.01654.pdf", "abs": "https://arxiv.org/abs/2509.01654", "title": "Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions", "authors": ["Dominic Plein"], "categories": ["cs.CL"], "comment": "11 pages, 12 figures, accompanied by a YouTube video\n  (https://youtu.be/xbcpnItE3_4) and a GitHub repository\n  (https://github.com/Splines/phonetics-graph/)", "summary": "We present a method to calculate the similarity between words based on their\nphonetic transcription (their pronunciation) using the Needleman-Wunsch\nalgorithm. We implement this algorithm in Rust and parallelize it on both CPU\nand GPU to handle large datasets efficiently. The GPU implementation leverages\nCUDA and the cudarc Rust library to achieve significant performance\nimprovements. We validate our approach by constructing a fully-connected graph\nwhere nodes represent words and edges have weights according to the similarity\nbetween the words. This graph is then analyzed using clustering algorithms to\nidentify groups of phonetically similar words. Our results demonstrate the\nfeasibility and effectiveness of the proposed method in analyzing the phonetic\nstructure of languages. It might be easily expanded to other languages."}
{"id": "2509.01660", "pdf": "https://arxiv.org/pdf/2509.01660.pdf", "abs": "https://arxiv.org/abs/2509.01660", "title": "Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection", "authors": ["Zhengjia Wang", "Qiang Sheng", "Danding Wang", "Beizhe Hu", "Juan Cao"], "categories": ["cs.CL"], "comment": "Accepted to CIKM'25", "summary": "Fake news detection is an important and challenging task for defending online\ninformation integrity. Existing state-of-the-art approaches typically extract\nnews semantic clues, such as writing patterns that include emotional words,\nstylistic features, etc. However, detectors tuned solely to such semantic clues\ncan easily fall into surface detection patterns, which can shift rapidly in\ndynamic environments, leading to limited performance in the evolving news\nlandscape. To address this issue, this paper investigates a novel perspective\nby incorporating news intent into fake news detection, bridging intents and\nsemantics together. The core insight is that by considering news intents, one\ncan deeply understand the inherent thoughts behind news deception, rather than\nthe surface patterns within words alone. To achieve this goal, we propose\nGraph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,\nwhich models deception clues from both semantic and intent signals via\ngraph-based joint learning. Specifically, InSide reformulates news semantic and\nintent signals into heterogeneous graph structures, enabling long-range context\ninteraction through entity guidance and capturing both holistic and\nimplementation-level intent via coarse-to-fine intent modeling. To achieve\nbetter alignment between semantics and intents, we further develop a dynamic\npathway-based graph alignment strategy for effective message passing and\naggregation across these signals by establishing a common space. Extensive\nexperiments on four benchmark datasets demonstrate the superiority of the\nproposed InSide compared to state-of-the-art methods."}
{"id": "2509.01772", "pdf": "https://arxiv.org/pdf/2509.01772.pdf", "abs": "https://arxiv.org/abs/2509.01772", "title": "chDzDT: Word-level morphology-aware language model for Algerian social media text", "authors": ["Abdelkrime Aries"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Pre-trained language models (PLMs) have substantially advanced natural\nlanguage processing by providing context-sensitive text representations.\nHowever, the Algerian dialect remains under-represented, with few dedicated\nmodels available. Processing this dialect is challenging due to its complex\nmorphology, frequent code-switching, multiple scripts, and strong lexical\ninfluences from other languages. These characteristics complicate tokenization\nand reduce the effectiveness of conventional word- or subword-level approaches.\n  To address this gap, we introduce chDzDT, a character-level pre-trained\nlanguage model tailored for Algerian morphology. Unlike conventional PLMs that\nrely on token sequences, chDzDT is trained on isolated words. This design\nallows the model to encode morphological patterns robustly, without depending\non token boundaries or standardized orthography. The training corpus draws from\ndiverse sources, including YouTube comments, French, English, and Berber\nWikipedia, as well as the Tatoeba project. It covers multiple scripts and\nlinguistic varieties, resulting in a substantial pre-training workload.\n  Our contributions are threefold: (i) a detailed morphological analysis of\nAlgerian dialect using YouTube comments; (ii) the construction of a\nmultilingual Algerian lexicon dataset; and (iii) the development and extensive\nevaluation of a character-level PLM as a morphology-focused encoder for\ndownstream tasks. The proposed approach demonstrates the potential of\ncharacter-level modeling for morphologically rich, low-resource dialects and\nlays a foundation for more inclusive and adaptable NLP systems."}
{"id": "2509.01790", "pdf": "https://arxiv.org/pdf/2509.01790.pdf", "abs": "https://arxiv.org/abs/2509.01790", "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs", "authors": ["Andong Hua", "Kenan Tang", "Chenhe Gu", "Jindong Gu", "Eric Wong", "Yao Qin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,\nrepeating something written or spoken using different words) leads to\nsignificant changes in large language model (LLM) performance, has been widely\naccepted as a core limitation of LLMs. In this work, we revisit this issue and\nask: Is the widely reported high prompt sensitivity truly an inherent weakness\nof LLMs, or is it largely an artifact of evaluation processes? To answer this\nquestion, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)\nacross 6 benchmarks, including both multiple-choice and open-ended tasks on 12\ndiverse prompt templates. We find that much of the prompt sensitivity stems\nfrom heuristic evaluation methods, including log-likelihood scoring and rigid\nanswer matching, which often overlook semantically correct responses expressed\nthrough alternative phrasings, such as synonyms or paraphrases. When we adopt\nLLM-as-a-Judge evaluations, we observe a substantial reduction in performance\nvariance and a consistently higher correlation in model rankings across\nprompts. Our findings suggest that modern LLMs are more robust to prompt\ntemplates than previously believed, and that prompt sensitivity may be more an\nartifact of evaluation than a flaw in the models."}
{"id": "2509.01814", "pdf": "https://arxiv.org/pdf/2509.01814.pdf", "abs": "https://arxiv.org/abs/2509.01814", "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts."}
{"id": "2509.01885", "pdf": "https://arxiv.org/pdf/2509.01885.pdf", "abs": "https://arxiv.org/abs/2509.01885", "title": "Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning", "authors": ["Zhimeng Luo", "Abhibha Gupta", "Adam Frisch", "Daqing He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of critical patient information from Electronic Health Records\n(EHRs) poses significant challenges due to the complexity and unstructured\nnature of the data. Traditional machine learning approaches often fail to\ncapture pertinent details efficiently, making it difficult for clinicians to\nutilize these tools effectively in patient care. This paper introduces a novel\napproach to extracting the OPQRST assessment from EHRs by leveraging the\ncapabilities of Large Language Models (LLMs). We propose to reframe the task\nfrom sequence labeling to text generation, enabling the models to provide\nreasoning steps that mimic a physician's cognitive processes. This approach\nenhances interpretability and adapts to the limited availability of labeled\ndata in healthcare settings. Furthermore, we address the challenge of\nevaluating the accuracy of machine-generated text in clinical contexts by\nproposing a modification to traditional Named Entity Recognition (NER) metrics.\nThis includes the integration of semantic similarity measures, such as the BERT\nScore, to assess the alignment between generated text and the clinical intent\nof the original records. Our contributions demonstrate a significant\nadvancement in the use of AI in healthcare, offering a scalable solution that\nimproves the accuracy and usability of information extraction from EHRs,\nthereby aiding clinicians in making more informed decisions and enhancing\npatient care outcomes."}
{"id": "2509.01899", "pdf": "https://arxiv.org/pdf/2509.01899.pdf", "abs": "https://arxiv.org/abs/2509.01899", "title": "Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints", "authors": ["Zhimeng Luo", "Zhendong Wang", "Rui Meng", "Diyang Xue", "Adam Frisch", "Daqing He"], "categories": ["cs.CL"], "comment": null, "summary": "A Chief complaint (CC) is the reason for the medical visit as stated in the\npatient's own words. It helps medical professionals to quickly understand a\npatient's situation, and also serves as a short summary for medical text\nmining. However, chief complaint records often take a variety of entering\nmethods, resulting in a wide variation of medical notations, which makes it\ndifficult to standardize across different medical institutions for record\nkeeping or text mining. In this study, we propose a weakly supervised method to\nautomatically extract and link entities in chief complaints in the absence of\nhuman annotation. We first adopt a split-and-match algorithm to produce weak\nannotations, including entity mention spans and class labels, on 1.2 million\nreal-world de-identified and IRB approved chief complaint records. Then we\ntrain a BERT-based model with generated weak labels to locate entity mentions\nin chief complaint text and link them to a pre-defined ontology. We conducted\nextensive experiments, and the results showed that our Weakly Supervised Entity\nExtraction and Linking (\\ours) method produced superior performance over\nprevious methods without any human annotation."}
{"id": "2509.01962", "pdf": "https://arxiv.org/pdf/2509.01962.pdf", "abs": "https://arxiv.org/abs/2509.01962", "title": "DRAssist: Dispute Resolution Assistance using Large Language Models", "authors": ["Sachin Pawar", "Manoj Apte", "Girish K. Palshikar", "Basit Ali", "Nitin Ramrakhiyani"], "categories": ["cs.CL"], "comment": "Accepted at the 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Disputes between two parties occur in almost all domains such as taxation,\ninsurance, banking, healthcare, etc. The disputes are generally resolved in a\nspecific forum (e.g., consumer court) where facts are presented, points of\ndisagreement are discussed, arguments as well as specific demands of the\nparties are heard, and finally a human judge resolves the dispute by often\nfavouring one of the two parties. In this paper, we explore the use of large\nlanguage models (LLMs) as assistants for the human judge to resolve such\ndisputes, as part of our DRAssist system. We focus on disputes from two\nspecific domains -- automobile insurance and domain name disputes. DRAssist\nidentifies certain key structural elements (e.g., facts, aspects or\ndisagreement, arguments) of the disputes and summarizes the unstructured\ndispute descriptions to produce a structured summary for each dispute. We then\nexplore multiple prompting strategies with multiple LLMs for their ability to\nassist in resolving the disputes in these domains. In DRAssist, these LLMs are\nprompted to produce the resolution output at three different levels -- (i)\nidentifying an overall stronger party in a dispute, (ii) decide whether each\nspecific demand of each contesting party can be accepted or not, (iii) evaluate\nwhether each argument by each contesting party is strong or weak. We evaluate\nthe performance of LLMs on all these tasks by comparing them with relevant\nbaselines using suitable evaluation metrics."}
{"id": "2509.02033", "pdf": "https://arxiv.org/pdf/2509.02033.pdf", "abs": "https://arxiv.org/abs/2509.02033", "title": "StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching", "authors": ["Chao Xue", "Ziyuan Gao"], "categories": ["cs.CL"], "comment": "Accepted by PRICAI 2025", "summary": "Text semantic matching requires nuanced understanding of both structural\nrelationships and fine-grained semantic distinctions. While pre-trained\nlanguage models excel at capturing token-level interactions, they often\noverlook hierarchical structural patterns and struggle with subtle semantic\ndiscrimination. In this paper, we proposed StructCoh, a graph-enhanced\ncontrastive learning framework that synergistically combines structural\nreasoning with representation space optimization. Our approach features two key\ninnovations: (1) A dual-graph encoder constructs semantic graphs via dependency\nparsing and topic modeling, then employs graph isomorphism networks to\npropagate structural features across syntactic dependencies and cross-document\nconcept nodes. (2) A hierarchical contrastive objective enforces consistency at\nmultiple granularities: node-level contrastive regularization preserves core\nsemantic units, while graph-aware contrastive learning aligns inter-document\nstructural semantics through both explicit and implicit negative sampling\nstrategies. Experiments on three legal document matching benchmarks and\nacademic plagiarism detection datasets demonstrate significant improvements\nover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score\n(+6.2% absolute gain) on legal statute matching by effectively identifying\nargument structure similarities."}
{"id": "2509.02036", "pdf": "https://arxiv.org/pdf/2509.02036.pdf", "abs": "https://arxiv.org/abs/2509.02036", "title": "DeepSeek performs better than other Large Language Models in Dental Cases", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Abstract word count: 171; Total word count: 3130; Total number of\n  tables: 2; Total number of figures: 3; Number of references: 32", "summary": "Large language models (LLMs) hold transformative potential in healthcare, yet\ntheir capacity to interpret longitudinal patient narratives remains\ninadequately explored. Dentistry, with its rich repository of structured\nclinical data, presents a unique opportunity to rigorously assess LLMs'\nreasoning abilities. While several commercial LLMs already exist, DeepSeek, a\nmodel that gained significant attention earlier this year, has also joined the\ncompetition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini\n2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal\ndental case vignettes through open-ended clinical tasks. Using 34 standardized\nlongitudinal periodontal cases (comprising 258 question-answer pairs), we\nassessed model performance via automated metrics and blinded evaluations by\nlicensed dentists. DeepSeek emerged as the top performer, demonstrating\nsuperior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert\nratings (median = 4.5/5 vs. 4.0/5), without significantly compromising\nreadability. Our study positions DeepSeek as the leading LLM for case analysis,\nendorses its integration as an adjunct tool in both medical education and\nresearch, and highlights its potential as a domain-specific agent."}
{"id": "2509.02038", "pdf": "https://arxiv.org/pdf/2509.02038.pdf", "abs": "https://arxiv.org/abs/2509.02038", "title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task", "authors": ["Bashar Talafha", "Hawau Olamide Toyin", "Peter Sullivan", "AbdelRahim Elmadany", "Abdurrahman Juma", "Amirbek Djanibekov", "Chiyu Zhang", "Hamad Alshehhi", "Hanan Aldarmaki", "Mustafa Jarrar", "Nizar Habash", "Muhammad Abdul-Mageed"], "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "We present the findings of the sixth Nuanced Arabic Dialect Identification\n(NADI 2025) Shared Task, which focused on Arabic speech dialect processing\nacross three subtasks: spoken dialect identification (Subtask 1), speech\nrecognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask\n3). A total of 44 teams registered, and during the testing phase, 100 valid\nsubmissions were received from eight unique teams. The distribution was as\nfollows: 34 submissions for Subtask 1 \"five teams{\\ae}, 47 submissions for\nSubtask 2 \"six teams\", and 19 submissions for Subtask 3 \"two teams\". The\nbest-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20\nWER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These\nresults highlight the ongoing challenges of Arabic dialect speech processing,\nparticularly in dialect identification, recognition, and diacritic restoration.\nWe also summarize the methods adopted by participating teams and briefly\noutline directions for future editions of NADI."}
{"id": "2509.02040", "pdf": "https://arxiv.org/pdf/2509.02040.pdf", "abs": "https://arxiv.org/abs/2509.02040", "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation", "authors": ["Guangzeng Han", "Weisi Liu", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Findings", "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications."}
{"id": "2509.02075", "pdf": "https://arxiv.org/pdf/2509.02075.pdf", "abs": "https://arxiv.org/abs/2509.02075", "title": "How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis", "authors": ["Elisabetta Rocchetti", "Alfio Ferrara"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Adhering to explicit length constraints, such as generating text with a\nprecise word count, remains a significant challenge for Large Language Models\n(LLMs). This study aims at investigating the differences between foundation\nmodels and their instruction-tuned counterparts, on length-controlled text\ngeneration in English and Italian. We analyze both performance and internal\ncomponent contributions using Cumulative Weighted Attribution, a metric derived\nfrom Direct Logit Attribution. Our findings reveal that instruction-tuning\nsubstantially improves length control, primarily by specializing components in\ndeeper model layers. Specifically, attention heads in later layers of IT models\nshow increasingly positive contributions, particularly in English. In Italian,\nwhile attention contributions are more attenuated, final-layer MLPs exhibit a\nstronger positive role, suggesting a compensatory mechanism. These results\nindicate that instruction-tuning reconfigures later layers for task adherence,\nwith component-level strategies potentially adapting to linguistic context."}
{"id": "2509.02093", "pdf": "https://arxiv.org/pdf/2509.02093.pdf", "abs": "https://arxiv.org/abs/2509.02093", "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization", "authors": ["Juhyeon Lee", "Wonduk Seo", "Hyunjin An", "Seunghyun Lee", "Yi Bu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Preprint", "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval augmented reasoning process. Our approach retrieves top k reference\nprompts from the HelpSteer2 dataset, an open-source collection annotated for\nhelpfulness, correctness, coherence, complexity, and verbosity, and constructs\ntwo complementary optimization paradigms: (1) tiered contrastive reasoning,\nwhere the LLM compares high, medium, and low quality prompts to refine its own\ngeneration through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best prompts along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization."}
{"id": "2509.02097", "pdf": "https://arxiv.org/pdf/2509.02097.pdf", "abs": "https://arxiv.org/abs/2509.02097", "title": "JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer", "authors": ["Zhichao Shi", "Xuhui Jiang", "Chengjin Xu", "Cangli Yao", "Zhenxin Huang", "Shengjie Ma", "Yinghan Shen", "Yuanzhuo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating the capabilities of large language models (LLMs) is an essential\nstep to ensure the successful application of LLMs across various domains. The\ncurrent evaluation of LLMs is based on a paradigm that involves querying them\nwith predefined question sets and assessing their outputs. This paradigm offers\ncontrollable processes and simplicity, but faces challenges such as limited\ninteraction with targets, insufficient difficulty control, and difficulties in\nverifying the validity of evaluation results, making it hard to precisely\ndetermine the knowledge and capability boundaries of target models. To address\nthese challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic\nevaluation framework based on a new interviewer-style evaluation paradigm.\nJudgeAgent employs a comprehensive evaluation approach consisting of benchmark\ngrading, interactive extension, and evaluation feedback. It utilizes\nknowledge-driven data synthesis and target-adaptive difficulty adjustment\nmethods to conduct extended testing, providing accurate and effective\nevaluation results. We also introduce a novel insight into validating\nevaluation methods, demonstrating the effectiveness of JudgeAgent and its\ndynamic evaluation paradigm through extensive experiments."}
{"id": "2509.02123", "pdf": "https://arxiv.org/pdf/2509.02123.pdf", "abs": "https://arxiv.org/abs/2509.02123", "title": "CMRAG: Co-modality-based document retrieval and visual question answering", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm in document\nquestion answering tasks. However, existing methods have limitations when\ndealing with multimodal documents: one category of methods relies on layout\nanalysis and text extraction, which can only utilize explicit text information\nand struggle to capture images or unstructured content; the other category\ntreats document segmentation as visual input and directly passes it to visual\nlanguage models (VLMs) for processing, yet it ignores the semantic advantages\nof text, leading to suboptimal generation results. This paper proposes\nco-modality-based RAG (CMRAG), which can simultaneously leverage text and\nimages for efficient retrieval and generation. Specifically, we first perform\nstructured parsing on documents to obtain co-modality representations of text\nsegments and image regions. Subsequently, in response to user queries, we\nretrieve candidate evidence from text and image channels, respectively, and\naggregate the results at the cross-modal retrieval level. Finally, we prompt\nthe VLM to generate the final response based on the co-modality retrieval\nresults. Experiments demonstrate that our method significantly outperforms\npure-vision-based RAG in visual document question answering tasks. The findings\nof this paper show that integrating co-modality information into the RAG\nframework in a unified manner is an effective approach to improving the\nperformance of complex document visual question-answering (VQA) systems."}
{"id": "2509.02133", "pdf": "https://arxiv.org/pdf/2509.02133.pdf", "abs": "https://arxiv.org/abs/2509.02133", "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models", "authors": ["Snehasis Mukhopadhyay", "Aryan Kasat", "Shivam Dubey", "Rahul Karthikeyan", "Dhruv Sood", "Vinija Jain", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/"}
{"id": "2509.02160", "pdf": "https://arxiv.org/pdf/2509.02160.pdf", "abs": "https://arxiv.org/abs/2509.02160", "title": "Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages", "authors": ["David Demitri Africa", "Suchir Salhan", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) in low-resource languages is usually tackled\nby finetuning very large multilingual LMs, an option that is often infeasible\nin memory- or latency-constrained settings. We ask whether small decoder LMs\ncan be pretrained so that they adapt quickly and transfer zero-shot to\nlanguages unseen during pretraining. To this end we replace part of the\nautoregressive objective with first-order model-agnostic meta-learning (MAML).\nTagalog and Cebuano are typologically similar yet structurally different in\ntheir actor/non-actor voice systems, and hence serve as a challenging test-bed.\nAcross four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp\nunder head-only tuning and 1-3 pp after full tuning, while cutting convergence\ntime by up to 8%. Gains are largest for single-token person entities that\nco-occur with Tagalog case particles si/ni, highlighting the importance of\nsurface anchors."}
{"id": "2509.02170", "pdf": "https://arxiv.org/pdf/2509.02170.pdf", "abs": "https://arxiv.org/abs/2509.02170", "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "authors": ["Kyeongman Park", "Nakyeong Yang", "Kyomin Jung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity."}
{"id": "2509.02198", "pdf": "https://arxiv.org/pdf/2509.02198.pdf", "abs": "https://arxiv.org/abs/2509.02198", "title": "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain", "authors": ["Anum Afzal", "Juraj Vladika", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models tend to struggle when dealing with specialized domains.\nWhile all aspects of evaluation hold importance, factuality is the most\ncritical one. Similarly, reliable fact-checking tools and data sources are\nessential for hallucination mitigation. We address these issues by providing a\ncomprehensive Fact-checking Benchmark FActBench covering four generation tasks\nand six state-of-the-art Large Language Models (LLMs) for the Medical domain.\nWe use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT)\nPrompting and Natural Language Inference (NLI). Our experiments show that the\nfact-checking scores acquired through the Unanimous Voting of both techniques\ncorrelate best with Domain Expert Evaluation."}
{"id": "2509.02225", "pdf": "https://arxiv.org/pdf/2509.02225.pdf", "abs": "https://arxiv.org/abs/2509.02225", "title": "Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?", "authors": ["Jaime Collado-Montañez", "L. Alfonso Ureña-López", "Arturo Montejo-Ráez"], "categories": ["cs.CL", "I.2.7; I.7"], "comment": "13 pages, 2 figures", "summary": "Large Language Models offer impressive language capabilities but suffer from\nwell-known limitations, including hallucinations, biases, privacy concerns, and\nhigh computational costs. These issues are largely driven by the combination of\nlinguistic competence and factual memorization within a single monolithic\nmodel. This paper introduces and empirically supports the Fundamental Language\nModel (FLM) paradigm, which advocates for smaller, linguistically competent\nmodels that offload factual retrieval to external tools. We evaluate models\nranging from 135M to 32B parameters across three dimensions: linguistic\ncompetence, external factual knowledge, and internal factual knowledge. Our\nfindings reveal that while both linguistic competence and factual knowledge\nimprove with scale, internal factual knowledge grows significantly faster,\nsuggesting that model size is more closely tied to memorization than to core\nlanguage ability. These results support a modular approach to language\nmodeling, where compact, linguistically proficient models serve as the\nfoundation for tool-augmented systems. The FLM paradigm offers a path toward\nmore efficient, interpretable, and sustainable NLP solutions."}
{"id": "2509.02292", "pdf": "https://arxiv.org/pdf/2509.02292.pdf", "abs": "https://arxiv.org/abs/2509.02292", "title": "LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue", "authors": ["Katharine Kowalyshyn", "Matthias Scheutz"], "categories": ["cs.CL"], "comment": null, "summary": "What if large language models could not only infer human mindsets but also\nexpose every blind spot in team dialogue such as discrepancies in the team\nmembers' joint understanding? We present a novel, two-step framework that\nleverages large language models (LLMs) both as human-style annotators of team\ndialogues to track the team's shared mental models (SMMs) and as automated\ndiscrepancy detectors among individuals' mental states. In the first step, an\nLLM generates annotations by identifying SMM elements within task-oriented\ndialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a\nsecondary LLM compares these LLM-derived annotations and human annotations\nagainst gold-standard labels to detect and characterize divergences. We define\nan SMM coherence evaluation framework for this use case and apply it to six\nCReST dialogues, ultimately producing: (1) a dataset of human and LLM\nannotations; (2) a reproducible evaluation framework for SMM coherence; and (3)\nan empirical assessment of LLM-based discrepancy detection. Our results reveal\nthat, although LLMs exhibit apparent coherence on straightforward\nnatural-language annotation tasks, they systematically err in scenarios\nrequiring spatial reasoning or disambiguation of prosodic cues."}
{"id": "2509.02333", "pdf": "https://arxiv.org/pdf/2509.02333.pdf", "abs": "https://arxiv.org/abs/2509.02333", "title": "DCPO: Dynamic Clipping Policy Optimization", "authors": ["Shihui Yang", "Chengfeng Dou", "Peidong Guo", "Kai Lu", "Qiang Ju", "Fei Deng", "Rihui Xin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization (DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts the clipping bounds based on token-specific prior\nprobabilities to enhance token-level exploration, and a smooth advantage\nstandardization technique that standardizes rewards across cumulative training\nsteps to improve the response-level effective utilization of generated\nresponses. DCPO achieved state-of-the-art performance on four benchmarks based\non four different models. In particular, DCPO achieved an Avg@1 of 46.7 under\ngreedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24\nbenchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the\nQwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO\nachieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO\n(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models."}
{"id": "2509.02350", "pdf": "https://arxiv.org/pdf/2509.02350.pdf", "abs": "https://arxiv.org/abs/2509.02350", "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Li Fan", "Jiahong Liu", "Yao Shu", "Chengwei Qin", "Menglin Yang", "Irwin King", "Rex Ying"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong generalization across a\nwide range of tasks. Reasoning with LLMs is central to solving multi-step\nproblems and complex decision-making. To support efficient reasoning, recent\nstudies have shifted attention from explicit chain-of-thought prompting toward\nimplicit reasoning, where reasoning occurs silently via latent structures\nwithout emitting intermediate textual steps. Implicit reasoning brings\nadvantages such as lower generation cost, faster inference, and better\nalignment with internal computation. Although prior surveys have discussed\nlatent representations in the context of reasoning, a dedicated and\nmechanism-level examination of how reasoning unfolds internally within LLMs\nremains absent. This survey fills that gap by introducing a taxonomy centered\non execution paradigms, shifting the focus from representational forms to\ncomputational strategies. We organize existing methods into three execution\nparadigms based on \\textbf{\\textit{how and where internal computation\nunfolds}}: latent optimization, signal-guided control, and layer-recurrent\nexecution. We also review structural, behavioral and representation-based\nevidence that supports the presence of implicit reasoning in LLMs. We further\nprovide a structured overview of the evaluation metrics and benchmarks used in\nexisting works to assess the effectiveness and reliability of implicit\nreasoning.We maintain a continuously updated project at:\nhttps://github.com/digailab/awesome-llm-implicit-reasoning."}
{"id": "2509.02363", "pdf": "https://arxiv.org/pdf/2509.02363.pdf", "abs": "https://arxiv.org/abs/2509.02363", "title": "Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models", "authors": ["Gaurav Negi", "Atul Kr. Ojha", "Omnia Zayed", "Paul Buitelaar"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a scalable method for constructing a temporal opinion knowledge\nbase with large language models (LLMs) as automated annotators. Despite the\ndemonstrated utility of time-series opinion analysis of text for downstream\napplications such as forecasting and trend analysis, existing methodologies\nunderexploit this potential due to the absence of temporally grounded\nfine-grained annotations. Our approach addresses this gap by integrating\nwell-established opinion mining formulations into a declarative LLM annotation\npipeline, enabling structured opinion extraction without manual prompt\nengineering. We define three data models grounded in sentiment and opinion\nmining literature, serving as schemas for structured representation. We perform\nrigorous quantitative evaluation of our pipeline using human-annotated test\nsamples. We carry out the final annotations using two separate LLMs, and\ninter-annotator agreement is computed label-wise across the fine-grained\nopinion dimensions, analogous to human annotation protocols. The resulting\nknowledge base encapsulates time-aligned, structured opinions and is compatible\nwith applications in Retrieval-Augmented Generation (RAG), temporal question\nanswering, and timeline summarisation."}
{"id": "2509.02446", "pdf": "https://arxiv.org/pdf/2509.02446.pdf", "abs": "https://arxiv.org/abs/2509.02446", "title": "An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction", "authors": ["Ali Hamdi", "Malak Mohamed", "Rokaia Emad", "Khaled Shaban"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social telehealth has made remarkable progress in healthcare by allowing\npatients to post symptoms and participate in medical consultations remotely.\nUsers frequently post symptoms on social media and online health platforms,\ncreating a huge repository of medical data that can be leveraged for disease\nclassification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along\nwith transformer-based models like BERT, have demonstrated strong capabilities\nin processing complex medical text. In this study, we evaluate three Arabic\nmedical text preprocessing methods such as summarization, refinement, and Named\nEntity Recognition (NER) before applying fine-tuned Arabic transformer models\n(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a\nmajority voting ensemble that combines predictions from original and\npreprocessed text representations. This approach achieved the best\nclassification accuracy of 80.56%, thus showing its effectiveness in leveraging\nvarious text representations and model predictions to improve the understanding\nof medical texts. To the best of our knowledge, this is the first work that\nintegrates LLM-based preprocessing with fine-tuned Arabic transformer models\nand ensemble learning for disease classification in Arabic social telehealth\ndata."}
{"id": "2509.02450", "pdf": "https://arxiv.org/pdf/2509.02450.pdf", "abs": "https://arxiv.org/abs/2509.02450", "title": "EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling", "authors": ["Lingzhi Shen", "Xiaohao Cai", "Yunfei Long", "Imran Razzak", "Guanming Chen", "Shoaib Jameel"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection from text is commonly performed by analysing users'\nsocial media posts. However, existing methods heavily rely on large-scale\nannotated datasets, making it challenging to obtain high-quality personality\nlabels. Moreover, most studies treat emotion and personality as independent\nvariables, overlooking their interactions. In this paper, we propose a novel\nself-supervised framework, EmoPerso, which improves personality detection\nthrough emotion-aware modelling. EmoPerso first leverages generative mechanisms\nfor synthetic data augmentation and rich representation learning. It then\nextracts pseudo-labeled emotion features and jointly optimizes them with\npersonality prediction via multi-task learning. A cross-attention module is\nemployed to capture fine-grained interactions between personality traits and\nthe inferred emotional representations. To further refine relational reasoning,\nEmoPerso adopts a self-taught strategy to enhance the model's reasoning\ncapabilities iteratively. Extensive experiments on two benchmark datasets\ndemonstrate that EmoPerso surpasses state-of-the-art models. The source code is\navailable at https://github.com/slz0925/EmoPerso."}
{"id": "2509.02452", "pdf": "https://arxiv.org/pdf/2509.02452.pdf", "abs": "https://arxiv.org/abs/2509.02452", "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions", "authors": ["Seyedali Mohammadi", "Bhaskara Hanuma Vedula", "Hemank Lamba", "Edward Raff", "Ponnurangam Kumaraguru", "Francis Ferraro", "Manas Gaur"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in EMNLP 2025, Main Conference", "summary": "Do LLMs genuinely incorporate external definitions, or do they primarily rely\non their parametric knowledge? To address these questions, we conduct\ncontrolled experiments across multiple explanation benchmark datasets (general\nand domain-specific) and label definition conditions, including expert-curated,\nLLM-generated, perturbed, and swapped definitions. Our results reveal that\nwhile explicit label definitions can enhance accuracy and explainability, their\nintegration into an LLM's task-solving processes is neither guaranteed nor\nconsistent, suggesting reliance on internalized representations in many cases.\nModels often default to their internal representations, particularly in general\ntasks, whereas domain-specific tasks benefit more from explicit definitions.\nThese findings underscore the need for a deeper understanding of how LLMs\nprocess external knowledge alongside their pre-existing capabilities."}
{"id": "2509.02464", "pdf": "https://arxiv.org/pdf/2509.02464.pdf", "abs": "https://arxiv.org/abs/2509.02464", "title": "SpecEval: Evaluating Model Adherence to Behavior Specifications", "authors": ["Ahmed Ahmed", "Kevin Klyman", "Yi Zeng", "Sanmi Koyejo", "Percy Liang"], "categories": ["cs.CL"], "comment": null, "summary": "Companies that develop foundation models publish behavioral guidelines they\npledge their models will follow, but it remains unclear if models actually do\nso. While providers such as OpenAI, Anthropic, and Google have published\ndetailed specifications describing both desired safety constraints and\nqualitative traits for their models, there has been no systematic audit of\nadherence to these guidelines. We introduce an automated framework that audits\nmodels against their providers specifications by parsing behavioral statements,\ngenerating targeted prompts, and using models to judge adherence. Our central\nfocus is on three way consistency between a provider specification, its model\noutputs, and its own models as judges; an extension of prior two way generator\nvalidator consistency. This establishes a necessary baseline: at minimum, a\nfoundation model should consistently satisfy the developer behavioral\nspecifications when judged by the developer evaluator models. We apply our\nframework to 16 models from six developers across more than 100 behavioral\nstatements, finding systematic inconsistencies including compliance gaps of up\nto 20 percent across providers."}
{"id": "2509.02492", "pdf": "https://arxiv.org/pdf/2509.02492.pdf", "abs": "https://arxiv.org/abs/2509.02492", "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning", "authors": ["Chenglong Wang", "Yongyu Mu", "Hang Zhou", "Yifu Huo", "Ziming Zhu", "Jiali Zeng", "Murun Yang", "Bei Li", "Tong Xiao", "Xiaoyang Hao", "Chunliang Zhang", "Fandong Meng", "Jingbo Zhu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines."}
{"id": "2509.02499", "pdf": "https://arxiv.org/pdf/2509.02499.pdf", "abs": "https://arxiv.org/abs/2509.02499", "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xiu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs."}
{"id": "2509.02503", "pdf": "https://arxiv.org/pdf/2509.02503.pdf", "abs": "https://arxiv.org/abs/2509.02503", "title": "L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages", "authors": ["Nishant Tanksale", "Tanmay Kokate", "Darshan Gohad", "Sarvadnyaa Barate", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Semantic evaluation in low-resource languages remains a major challenge in\nNLP. While sentence transformers have shown strong performance in high-resource\nsettings, their effectiveness in Indic languages is underexplored due to a lack\nof high-quality benchmarks. To bridge this gap, we introduce\nL3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten\nlow-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,\nMalayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000\nnews articles paired with four headline variants: the original, a semantically\nsimilar version, a lexically similar version, and an unrelated one, designed to\ntest fine-grained semantic understanding. The task requires selecting the\ncorrect headline from the options using article-headline similarity. We\nbenchmark several sentence transformers, including multilingual and\nlanguage-specific models, using cosine similarity. Results show that\nmultilingual models consistently perform well, while language-specific models\nvary in effectiveness. Given the rising use of similarity models in\nRetrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a\nvaluable resource for evaluating and improving semantic understanding in such\napplications. Additionally, the dataset can be repurposed for multiple-choice\nquestion answering, headline classification, or other task-specific evaluations\nof LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared\npublicly at https://github.com/l3cube-pune/indic-nlp"}
{"id": "2509.02506", "pdf": "https://arxiv.org/pdf/2509.02506.pdf", "abs": "https://arxiv.org/abs/2509.02506", "title": "The Forgotten Code: Validating a Century-Old Translation System with AI", "authors": ["Jean-Marie Le Ray"], "categories": ["cs.CL", "I.2"], "comment": "Preprint, 35 pages, 14 figures, 9 appendices", "summary": "A pioneering rule-based mechanical translation system (precursor of modern\nRBMTs) was first presented in December 1929 by its inventor, Federico Pucci,\nwho later published the full method in a book titled \"Il traduttore meccanico\ned il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria\nlingua: Parte I\", in Salerno (Italy), in 1931. This study illustrates how AI\nbreathes new life into the system of international keys and ideograms devised\nby Pucci to translate from/into any Romance language (at least as a first\nstep). The methodology involves having the AIs retranslate, following Pucci's\nmethod, the two text excerpts originally translated in 1931 and clearly\ndocumented in his publication: a passage from Dante's La Vita Nuova, translated\nfrom Italian into French, and a passage from Voltaire's Zadig, translated from\nFrench into Italian. The result is notable: the two texts, translated 94 years\napart using the same method--by Pucci in 1931 and by AIs in 2025--show a low\naverage difference, with only minor variations observed. With Pucci's system\nthus validated, it became feasible to have the AIs reproduce the excerpts in\nEnglish, Spanish, and German according to his method. The results were\nconsistent, and Pucci--via Artificial Intelligence--was tasked with translating\nmore modern and technical texts, thereby reviving, nearly a century later, an\ninvention that had remained almost entirely unknown and never applied beyond\nits creator, now brought to wider attention and opened to possible\nexperimentation. Such a demonstration would not only affirm Pucci's historical\nstatus but also place him among the precursors and intellectual contributors to\nmachine translation, whose work merits examination alongside figures such as\nTroyanskij, Booth, and Weaver, with possible consequences for how the history\nof the field is understood."}
{"id": "2509.02510", "pdf": "https://arxiv.org/pdf/2509.02510.pdf", "abs": "https://arxiv.org/abs/2509.02510", "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation", "authors": ["Erfan Baghaei Potraghloo", "Seyedarmin Azizi", "Souvik Kundu", "Massoud Pedram"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding."}
{"id": "2509.02514", "pdf": "https://arxiv.org/pdf/2509.02514.pdf", "abs": "https://arxiv.org/abs/2509.02514", "title": "Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition", "authors": ["Mayur Shirke", "Amey Shembade", "Pavan Thorat", "Madhushri Wagh", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English\n(Hinglish), presents unique challenges due to informal structure,\ntransliteration, and frequent language switching. This study conducts a\ncomparative evaluation of code-mixed fine-tuned models and non-code-mixed\nmultilingual models, along with zero-shot generative large language models\n(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained\non code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained\non non-code-mixed multilingual data). We also assess the performance of Google\nGemini in a zero-shot setting using a modified version of the dataset with NER\ntags removed. All models are tested on a benchmark Hinglish NER dataset using\nPrecision, Recall, and F1-score. Results show that code-mixed models,\nparticularly HingRoBERTa and HingBERT-based fine-tuned models, outperform\nothers - including closed-source LLMs like Google Gemini - due to\ndomain-specific pretraining. Non-code-mixed models perform reasonably but show\nlimited adaptability. Notably, Google Gemini exhibits competitive zero-shot\nperformance, underlining the generalization strength of modern LLMs. This study\nprovides key insights into the effectiveness of specialized versus generalized\nmodels for code-mixed NER tasks."}
{"id": "2509.02522", "pdf": "https://arxiv.org/pdf/2509.02522.pdf", "abs": "https://arxiv.org/abs/2509.02522", "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR", "authors": ["Jiaming Li", "Longze Chen", "Ze Gong", "Yukun Chen", "Lu Wang", "Wanwei He", "Run Luo", "Min Yang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a\nnovel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor\n$\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS."}
{"id": "2509.02523", "pdf": "https://arxiv.org/pdf/2509.02523.pdf", "abs": "https://arxiv.org/abs/2509.02523", "title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices", "authors": ["Evan King", "Adam Sabra", "Manjunath Kudlur", "James Wang", "Pete Warden"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "We present the Flavors of Moonshine, a suite of tiny automatic speech\nrecognition (ASR) models specialized for a range of underrepresented languages.\nPrevailing wisdom suggests that multilingual ASR models outperform monolingual\ncounterparts by exploiting cross-lingual phonetic similarities. We challenge\nthis assumption, showing that for sufficiently small models (27M parameters),\ntraining monolingual systems on a carefully balanced mix of high-quality\nhuman-labeled, pseudo-labeled, and synthetic data yields substantially superior\nperformance. On average, our models achieve error rates 48% lower than the\ncomparably sized Whisper Tiny model, outperform the 9x larger Whisper Small\nmodel, and in most cases match or outperform the 28x larger Whisper Medium\nmodel. These results advance the state of the art for models of this size,\nenabling accurate on-device ASR for languages that previously had limited\nsupport. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and\nVietnamese Moonshine models under a permissive open-source license."}
{"id": "2509.02534", "pdf": "https://arxiv.org/pdf/2509.02534.pdf", "abs": "https://arxiv.org/abs/2509.02534", "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations", "authors": ["Tianjian Li", "Yiming Zhang", "Ping Yu", "Swarnadeep Saha", "Daniel Khashabi", "Jason Weston", "Jack Lanchantin", "Tianlu Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses."}
{"id": "2509.02550", "pdf": "https://arxiv.org/pdf/2509.02550.pdf", "abs": "https://arxiv.org/abs/2509.02550", "title": "PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Hamdy Mubarak", "Majd Hawasly", "Abubakr Mohamed", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": "https://palmx.dlnlp.ai/", "summary": "Large Language Models (LLMs) inherently reflect the vast data distributions\nthey encounter during their pre-training phase. As this data is predominantly\nsourced from the web, there is a high chance it will be skewed towards\nhigh-resourced languages and cultures, such as those of the West. Consequently,\nLLMs often exhibit a diminished understanding of certain communities, a gap\nthat is particularly evident in their knowledge of Arabic and Islamic cultures.\nThis issue becomes even more pronounced with increasingly under-represented\ntopics. To address this critical challenge, we introduce PalmX 2025, the first\nshared task designed to benchmark the cultural competence of LLMs in these\nspecific domains. The task is composed of two subtasks featuring\nmultiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General\nArabic Culture and General Islamic Culture. These subtasks cover a wide range\nof topics, including traditions, food, history, religious practices, and\nlanguage expressions from across 22 Arab countries. The initiative drew\nconsiderable interest, with 26 teams registering for Subtask 1 and 19 for\nSubtask 2, culminating in nine and six valid submissions, respectively. Our\nfindings reveal that task-specific fine-tuning substantially boosts performance\nover baseline models. The top-performing systems achieved an accuracy of 72.15%\non cultural questions and 84.22% on Islamic knowledge. Parameter-efficient\nfine-tuning emerged as the predominant and most effective approach among\nparticipants, while the utility of data augmentation was found to be\ndomain-dependent."}
{"id": "2509.00053", "pdf": "https://arxiv.org/pdf/2509.00053.pdf", "abs": "https://arxiv.org/abs/2509.00053", "title": "Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?", "authors": ["Shuo Liu", "Di Yao", "Yan Lin", "Gao Cong", "Jingping Bi"], "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "20 pages, 10 figures", "summary": "Building a general model capable of analyzing human trajectories across\ndifferent geographic regions and different tasks becomes an emergent yet\nimportant problem for various applications. However, existing works suffer from\nthe generalization problem, \\ie, they are either restricted to train for\nspecific regions or only suitable for a few tasks. Given the recent advances of\nmultimodal large language models (MLLMs), we raise the question: can MLLMs\nreform current trajectory data mining and solve the problem? Nevertheless, due\nto the modality gap of trajectory, how to generate task-independent multimodal\ntrajectory representations and how to adapt flexibly to different tasks remain\nthe foundational challenges. In this paper, we propose \\texttt{Traj-MLLM}},\nwhich is the first general framework using MLLMs for trajectory data mining. By\nintegrating multiview contexts, \\texttt{Traj-MLLM}} transforms raw trajectories\ninto interleaved image-text sequences while preserving key spatial-temporal\ncharacteristics, and directly utilizes the reasoning ability of MLLMs for\ntrajectory analysis. Additionally, a prompt optimization method is proposed to\nfinalize data-invariant prompts for task adaptation. Extensive experiments on\nfour publicly available datasets show that \\texttt{Traj-MLLM}} outperforms\nstate-of-the-art baselines by $48.05\\%$, $15.52\\%$, $51.52\\%$, $1.83\\%$ on\ntravel time estimation, mobility prediction, anomaly detection and\ntransportation mode identification, respectively. \\texttt{Traj-MLLM}} achieves\nthese superior performances without requiring any training data or fine-tuning\nthe MLLM backbones."}
{"id": "2509.00074", "pdf": "https://arxiv.org/pdf/2509.00074.pdf", "abs": "https://arxiv.org/abs/2509.00074", "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks", "authors": ["Cédric Colas", "Tracey Mills", "Ben Prystawski", "Michael Henry Tessler", "Noah Goodman", "Jacob Andreas", "Joshua Tenenbaum"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The ability to combine linguistic guidance from others with direct experience\nis central to human development, enabling safe and rapid learning in new\nenvironments. How do people integrate these two sources of knowledge, and how\nmight AI systems? We present a computational framework that models social\nlearning as joint probabilistic inference over structured, executable world\nmodels given sensorimotor and linguistic data. We make this possible by turning\na pretrained language model into a probabilistic model of how humans share\nadvice conditioned on their beliefs, allowing our agents both to generate\nadvice for others and to interpret linguistic input as evidence during Bayesian\ninference. Using behavioral experiments and simulations across 10 video games,\nwe show how linguistic guidance can shape exploration and accelerate learning\nby reducing risky interactions and speeding up key discoveries in both humans\nand models. We further explore how knowledge can accumulate across generations\nthrough iterated learning experiments and demonstrate successful knowledge\ntransfer between humans and models -- revealing how structured,\nlanguage-compatible representations might enable human-machine collaborative\nlearning."}
{"id": "2509.00077", "pdf": "https://arxiv.org/pdf/2509.00077.pdf", "abs": "https://arxiv.org/abs/2509.00077", "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition", "authors": ["Tai Vu"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Speech Emotion Recognition (SER) presents a significant yet persistent\nchallenge in human-computer interaction. While deep learning has advanced\nspoken language processing, achieving high performance on limited datasets\nremains a critical hurdle. This paper confronts this issue by developing and\nevaluating a suite of machine learning models, including Support Vector\nMachines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional\nNeural Networks (CNNs), for automated emotion classification in human speech.\nWe demonstrate that by strategically employing transfer learning and innovative\ndata augmentation techniques, our models can achieve impressive performance\ndespite the constraints of a relatively small dataset. Our most effective\nmodel, a ResNet34 architecture, establishes a new performance benchmark on the\ncombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1\nscore of 0.631. These results underscore the substantial benefits of leveraging\npre-trained models and data augmentation to overcome data scarcity, thereby\npaving the way for more robust and generalizable SER systems."}
{"id": "2509.00078", "pdf": "https://arxiv.org/pdf/2509.00078.pdf", "abs": "https://arxiv.org/abs/2509.00078", "title": "ChipChat: Low-Latency Cascaded Conversational Agent in MLX", "authors": ["Tatiana Likhomanenko", "Luke Carlson", "Richard He Bai", "Zijin Gu", "Han Tran", "Zakaria Aldeneh", "Yizhe Zhang", "Ruixiang Zhang", "Huangjie Zheng", "Navdeep Jaitly"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "ASRU 2025", "summary": "The emergence of large language models (LLMs) has transformed spoken dialog\nsystems, yet the optimal architecture for real-time on-device voice agents\nremains an open question. While end-to-end approaches promise theoretical\nadvantages, cascaded systems (CSs) continue to outperform them in language\nunderstanding tasks, despite being constrained by sequential processing\nlatency. In this work, we introduce ChipChat, a novel low-latency CS that\novercomes traditional bottlenecks through architectural innovations and\nstreaming optimizations. Our system integrates streaming (a) conversational\nspeech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)\ntext-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.\nImplemented using MLX, ChipChat achieves sub-second response latency on a Mac\nStudio without dedicated GPUs, while preserving user privacy through complete\non-device processing. Our work shows that strategically redesigned CSs can\novercome their historical latency limitations, offering a promising path\nforward for practical voice-based AI agents."}
{"id": "2509.00083", "pdf": "https://arxiv.org/pdf/2509.00083.pdf", "abs": "https://arxiv.org/abs/2509.00083", "title": "Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models", "authors": ["Laksh Patel", "Neel Shanbhag"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 2 figures, 1 table; Presented at the 42nd International\n  Conference on Machine Learning (ICML), winning the \"Best Poster\" award at\n  ICML's workshop for data in generative models (DIG-BUGS)", "summary": "Modern generative models risk overfitting and unintentionally memorizing rare\ntraining examples, which can be extracted by adversaries or inflate benchmark\nperformance. We propose Generative Data Cartography (GenDataCarto), a\ndata-centric framework that assigns each pretraining sample a difficulty score\n(early-epoch loss) and a memorization score (frequency of ``forget events''),\nthen partitions examples into four quadrants to guide targeted pruning and\nup-/down-weighting. We prove that our memorization score lower-bounds classical\ninfluence under smoothness assumptions and that down-weighting\nhigh-memorization hotspots provably decreases the generalization gap via\nuniform stability bounds. Empirically, GenDataCarto reduces synthetic canary\nextraction success by over 40\\% at just 10\\% data pruning, while increasing\nvalidation perplexity by less than 0.5\\%. These results demonstrate that\nprincipled data interventions can dramatically mitigate leakage with minimal\ncost to generative performance."}
{"id": "2509.00084", "pdf": "https://arxiv.org/pdf/2509.00084.pdf", "abs": "https://arxiv.org/abs/2509.00084", "title": "Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs", "authors": ["Qibin Wang", "Pu Zhao", "Shaohan Huang", "Fangkai Yang", "Lu Wang", "Furu Wei", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To further enhance the ability of Large Language Models (LLMs) to solve\ncomplex, multi-step reasoning problems, test-time scaling (TTS) methods have\ngained widespread attention. Existing approaches such as Best-of-N and majority\nvoting are limited as their performance depends on the quality of candidate\nresponses, making them unable to produce a correct solution when all candidates\nare incorrect. Introducing an additional model to select the best response also\nincurs significant deployment costs. To this end, we introduce Generative\nSelf-Refinement (GSR), a novel parallel test-time scaling framework where a\nunified model first generates a set of candidate responses in parallel and then\nperforms self-refinement to synthesize a new superior solution based on a\nprompt consisting of the problem and these candidates. However, LLMs struggle\nto perform refinement effectively when prompted directly. Therefore, we design\na hybrid training pipeline by jointly optimizing for two complementary\nobjectives, solving problems directly and refining candidate responses.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance across five mathematical benchmarks. We further show that this\nlearned self-refinement skill is a model-agnostic enhancement, robust across\ndifferent model scales and generalizing to out-of-distribution reasoning tasks."}
{"id": "2509.00091", "pdf": "https://arxiv.org/pdf/2509.00091.pdf", "abs": "https://arxiv.org/abs/2509.00091", "title": "Ensemble Debates with Local Large Language Models for AI Alignment", "authors": ["Ephraiem Sarabamoun"], "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 tables", "summary": "As large language models (LLMs) take on greater roles in high-stakes\ndecisions, alignment with human values is essential. Reliance on proprietary\nAPIs limits reproducibility and broad participation. We study whether local\nopen-source ensemble debates can improve alignmentoriented reasoning. Across\n150 debates spanning 15 scenarios and five ensemble configurations, ensembles\noutperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),\nwith the largest gains in reasoning depth (+19.4%) and argument quality\n(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human\nenhancement (+0.80). We provide code, prompts, and a debate data set, providing\nan accessible and reproducible foundation for ensemble-based alignment\nevaluation."}
{"id": "2509.00094", "pdf": "https://arxiv.org/pdf/2509.00094.pdf", "abs": "https://arxiv.org/abs/2509.00094", "title": "Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning", "authors": ["Abdullah Abdelfattah", "Mahmoud I. Khalil", "Hazem Abbas"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "Assessing spoken language is challenging, and quantifying pronunciation\nmetrics for machine learning models is even harder. However, for the Holy\nQuran, this task is simplified by the rigorous recitation rules (tajweed)\nestablished by Muslim scholars, enabling highly effective assessment. Despite\nthis advantage, the scarcity of high-quality annotated data remains a\nsignificant barrier.\n  In this work, we bridge these gaps by introducing: (1) A 98% automated\npipeline to produce high-quality Quranic datasets -- encompassing: Collection\nof recitations from expert reciters, Segmentation at pause points (waqf) using\nour fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript\nverification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K\nannotated utterances); (3) A novel ASR-based approach for pronunciation error\ndetection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed\nrules (unlike the IPA standard for Modern Standard Arabic). QPS uses a\ntwo-level script: (Phoneme level): Encodes Arabic letters with short/long\nvowels. (Sifa level): Encodes articulation characteristics of every phoneme. We\nfurther include comprehensive modeling with our novel multi-level CTC Model\nwhich achieved 0.16% average Phoneme Error Rate (PER) on the testset. We\nrelease all code, data, and models as open-source:\nhttps://obadx.github.io/prepare-quran-dataset/"}
{"id": "2509.00096", "pdf": "https://arxiv.org/pdf/2509.00096.pdf", "abs": "https://arxiv.org/abs/2509.00096", "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs", "authors": ["Yao Fu", "Runchao Li", "Xianxuan Long", "Haotian Yu", "Xiaotian Han", "Yu Yin", "Pan Li"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to EMNLP2025 findings (poster)", "summary": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA."}
{"id": "2509.00115", "pdf": "https://arxiv.org/pdf/2509.00115.pdf", "abs": "https://arxiv.org/abs/2509.00115", "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems", "authors": ["Manish Shukla"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance.\nThird, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication."}
{"id": "2509.00230", "pdf": "https://arxiv.org/pdf/2509.00230.pdf", "abs": "https://arxiv.org/abs/2509.00230", "title": "Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks", "authors": ["Linus Stuhlmann", "Michael Alexander Saxer"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "This study evaluates the performance of three advanced speech encoder models,\nWav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By\nfine-tuning these models and analyzing their layer-wise representations using\nSVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0\nand XLS-R capture speaker-specific features effectively in their early layers,\nwith fine-tuning improving stability and performance. Whisper showed better\nperformance in deeper layers. Additionally, we determined the optimal number of\ntransformer layers for each model when fine-tuned for speaker identification\ntasks."}
{"id": "2509.00366", "pdf": "https://arxiv.org/pdf/2509.00366.pdf", "abs": "https://arxiv.org/abs/2509.00366", "title": "KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation", "authors": ["Ziyi Guan", "Jason Chun Lok Li", "Zhijian Hou", "Pingping Zhang", "Donglai Xu", "Yuzhi Zhao", "Mengyang Wu", "Jinpeng Chen", "Thanh-Toan Nguyen", "Pengfei Xian", "Wenao Ma", "Shengchao Qin", "Graziano Chesi", "Ngai Wong"], "categories": ["cs.MA", "cs.CL", "cs.MM"], "comment": "Accepted by the EMNLP 2025", "summary": "Despite recent progress, Graphic User Interface (GUI) agents powered by Large\nLanguage Models (LLMs) struggle with complex mobile tasks due to limited\napp-specific knowledge. While UI Transition Graphs (UTGs) offer structured\nnavigation representations, they are underutilized due to poor extraction and\ninefficient integration. We introduce KG-RAG, a Knowledge Graph-driven\nRetrieval-Augmented Generation framework that transforms fragmented UTGs into\nstructured vector databases for efficient real-time retrieval. By leveraging an\nintent-guided LLM search method, KG-RAG generates actionable navigation paths,\nenhancing agent decision-making. Experiments across diverse mobile apps show\nthat KG-RAG outperforms existing methods, achieving a 75.8% success rate (8.9%\nimprovement over AutoDroid), 84.6% decision accuracy (8.1% improvement), and\nreducing average task steps from 4.5 to 4.1. Additionally, we present\nKG-Android-Bench and KG-Harmony-Bench, two benchmarks tailored to the Chinese\nmobile ecosystem for future research. Finally, KG-RAG transfers to web/desktop\n(+40% SR on Weibo-web; +20% on QQ Music-desktop), and a UTG cost ablation shows\naccuracy saturates at ~4h per complex app, enabling practical deployment\ntrade-offs."}
{"id": "2509.00510", "pdf": "https://arxiv.org/pdf/2509.00510.pdf", "abs": "https://arxiv.org/abs/2509.00510", "title": "LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain", "authors": ["Li Weigang", "Pedro Carvalho Brom", "Lucas Ramson Siefert"], "categories": ["cs.AI", "cs.CL", "68T99", "I.2.11; I.2.8; I.2.6"], "comment": "24 pages, 5 figures", "summary": "We propose a novel SuperBrain framework for collective intelligence, grounded\nin the co-evolution of large language models (LLMs) and human users. Unlike\nstatic prompt engineering or isolated agent simulations, our approach\nemphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A\nSubclass Brain arises from persistent, personalized interaction between a user\nand an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through\nGA-assisted forward-backward evolution, these dyads iteratively refine prompts\nand task performance. (3) Multiple Subclass Brains coordinate via Swarm\nIntelligence, optimizing across multi-objective fitness landscapes and\nexchanging distilled heuristics. (4) Their standardized behaviors and cognitive\nsignatures integrate into a Superclass Brain, an emergent meta-intelligence\ncapable of abstraction, generalization and self-improvement. We outline the\ntheoretical constructs, present initial implementations (e.g., UAV scheduling,\nKU/KI keyword filtering) and propose a registry for cross-dyad knowledge\nconsolidation. This work provides both a conceptual foundation and an\narchitectural roadmap toward scalable, explainable and ethically aligned\ncollective AI."}
{"id": "2509.00520", "pdf": "https://arxiv.org/pdf/2509.00520.pdf", "abs": "https://arxiv.org/abs/2509.00520", "title": "ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking", "authors": ["Yuzheng Cai", "Yanzhao Zhang", "Dingkun Long", "Mingxin Li", "Pengjun Xie", "Weiguo Zheng"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Text reranking models are a crucial component in modern systems like\nRetrieval-Augmented Generation, tasked with selecting the most relevant\ndocuments prior to generation. However, current Large Language Models (LLMs)\npowered rerankers often face a fundamental trade-off. On one hand, Supervised\nFine-Tuning based pointwise methods that frame relevance as a binary\nclassification task lack the necessary scoring discrimination, particularly for\nthose built on reasoning LLMs. On the other hand, approaches designed for\ncomplex reasoning often employ powerful yet inefficient listwise formulations,\nrendering them impractical for low latency applications. To resolve this\ndilemma, we introduce ERank, a highly effective and efficient pointwise\nreranker built from a reasoning LLM that excels across diverse relevance\nscenarios. We propose a novel two-stage training pipeline that begins with\nSupervised Fine-Tuning (SFT). In this stage, we move beyond binary labels and\ntrain the model generatively to output fine grained integer scores, which\nsignificantly enhances relevance discrimination. The model is then further\nrefined using Reinforcement Learning (RL) with a novel, listwise derived\nreward. This technique instills global ranking awareness into the efficient\npointwise architecture. We evaluate the ERank reranker on the BRIGHT, FollowIR,\nTREC DL, and BEIR benchmarks, demonstrating superior effectiveness and\nrobustness compared to existing approaches. On the reasoning-intensive BRIGHT\nbenchmark, our ERank-4B achieves an nDCG@10 of 38.7, while a larger 32B variant\nreaches a state of the art nDCG@10 of 40.2."}
{"id": "2509.00546", "pdf": "https://arxiv.org/pdf/2509.00546.pdf", "abs": "https://arxiv.org/abs/2509.00546", "title": "Advanced spectral clustering for heterogeneous data in credit risk monitoring systems", "authors": ["Lu Han", "Mengyan Li", "Jiping Qiang", "Zhi Su"], "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 7 figures, 6 tables", "summary": "Heterogeneous data, which encompass both numerical financial variables and\ntextual records, present substantial challenges for credit monitoring. To\naddress this issue, we propose Advanced Spectral Clustering (ASC), a method\nthat integrates financial and textual similarities through an optimized weight\nparameter and selects eigenvectors using a novel eigenvalue-silhouette\noptimization approach. Evaluated on a dataset comprising 1,428 small and\nmedium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%\nhigher than that of a single-type data baseline method. Furthermore, the\nresulting clusters offer actionable insights; for instance, 51% of low-risk\nfirms are found to include the term 'social recruitment' in their textual\nrecords. The robustness of ASC is confirmed across multiple clustering\nalgorithms, including k-means, k-medians, and k-medoids, with\n{\\Delta}Intra/Inter < 0.13 and {\\Delta}Silhouette Coefficient < 0.02. By\nbridging spectral clustering theory with heterogeneous data applications, ASC\nenables the identification of meaningful clusters, such as recruitment-focused\nSMEs exhibiting a 30% lower default risk, thereby supporting more targeted and\neffective credit interventions."}
{"id": "2509.00710", "pdf": "https://arxiv.org/pdf/2509.00710.pdf", "abs": "https://arxiv.org/abs/2509.00710", "title": "On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations", "authors": ["Albert Sadowski", "Jarosław A. Chudziak"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for publication at the 34th ACM International Conference on\n  Information and Knowledge Management (CIKM '25)", "summary": "Legal reasoning requires both precise interpretation of statutory language\nand consistent application of complex rules, presenting significant challenges\nfor AI systems. This paper introduces a modular multi-agent framework that\ndecomposes legal reasoning into distinct knowledge acquisition and application\nstages. In the first stage, specialized agents extract legal concepts and\nformalize rules to create verifiable intermediate representations of statutes.\nThe second stage applies this knowledge to specific cases through three steps:\nanalyzing queries to map case facts onto the ontology schema, performing\nsymbolic inference to derive logically entailed conclusions, and generating\nfinal answers using a programmatic implementation that operationalizes the\nontological knowledge. This bridging of natural language understanding with\nsymbolic reasoning provides explicit and verifiable inspection points,\nsignificantly enhancing transparency compared to end-to-end approaches.\nEvaluation on statutory tax calculation tasks demonstrates substantial\nimprovements, with foundational models achieving 76.4\\% accuracy compared to\n18.8\\% baseline performance, effectively narrowing the performance gap between\nreasoning and foundational models. These findings suggest that modular\narchitectures with formalized knowledge representations can make sophisticated\nlegal reasoning more accessible through computationally efficient models while\nenhancing consistency and explainability in AI legal reasoning, establishing a\nfoundation for future research into more transparent, trustworthy, and\neffective AI systems for legal domain."}
{"id": "2509.00761", "pdf": "https://arxiv.org/pdf/2509.00761.pdf", "abs": "https://arxiv.org/abs/2509.00761", "title": "L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search", "authors": ["Ziqi Wang", "Boqin Yuan"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and\nAgentic Search), a system that reduces hallucination and uncertainty in legal\nquestion answering through coordinated multi-agent reasoning and retrieval.\nUnlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes\nqueries into subproblems, issues targeted searches across heterogeneous sources\n(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to\nverify sufficiency, jurisdiction, and temporal validity before answer\nsynthesis. This iterative reasoning-search-verification loop maintains\ncoherence, filters noisy evidence, and grounds answers in authoritative law. We\nevaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple\nchoice legal questions in 2025. Results show that L-MARS substantially improves\nfactual accuracy, reduces uncertainty, and achieves higher preference scores\nfrom both human experts and LLM-based judges. Our work demonstrates that\nmulti-agent reasoning with agentic search offers a scalable and reproducible\nblueprint for deploying LLMs in high-stakes domains requiring precise legal\nretrieval and deliberation."}
{"id": "2509.00768", "pdf": "https://arxiv.org/pdf/2509.00768.pdf", "abs": "https://arxiv.org/abs/2509.00768", "title": "Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling", "authors": ["Lee Hyun", "Sohee Yoon", "Jinwoo Park", "Sue In Chae", "Seongeon Park", "Jooyeon Ahn", "Yebin Jung", "Youjung Chung", "Hogeun Chang", "Myeonginn Kang", "Jina Kim", "Ho-Gyeong Kim", "Myeonghun Jeong"], "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CL"], "comment": "14 pages, 5 figures", "summary": "AI-driven materials discovery that couples automated experimentation with\nalgorithmic decision-making requires process aware recipe to property\npredictors that are accurate, calibrated, and physically admissible. We\napproach this as a reasoning problem with large reasoning models (LRMs). To\ninstill reasoning capability into language models, we curate reasoning traces\nfrom a teacher model to train a student model. However, most training pipelines\nselect reasoning traces using binary correctness or learned preference signals\nthat poorly reflect physical admissibility. We introduce Physics-aware\nRejection Sampling (PaRS), a training-time trace selection scheme that favors\ntraces consistent with fundamental physics and numerically close to targets,\nwith lightweight halting to control compute. We instantiate our framework with\na large student model fine-tuned on traces synthesized by a larger teacher\nmodel, and evaluate under matched token budgets against various rejection\nsampling baselines. Our method improves accuracy and calibration, reduces\nphysics-violation rates, and lowers sampling cost relative to baselines. These\nresults indicate that modest, domain-aware constraints combined with\ntrace-level selection provide a practical path toward reliable, efficient LRMs\nfor process-aware property prediction and closed-loop materials design."}
{"id": "2509.00891", "pdf": "https://arxiv.org/pdf/2509.00891.pdf", "abs": "https://arxiv.org/abs/2509.00891", "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care", "authors": ["Zonghai Yao", "Talha Chafekar", "Junda Wang", "Shuo Han", "Feiyun Ouyang", "Junhui Qian", "Lingxi Li", "Hong Yu"], "categories": ["cs.AI", "cs.CL"], "comment": "Equal contribution for the first two authors", "summary": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond."}
{"id": "2509.00925", "pdf": "https://arxiv.org/pdf/2509.00925.pdf", "abs": "https://arxiv.org/abs/2509.00925", "title": "DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers", "authors": ["Aman Sharma", "Saeed Najafi", "Parsa Farinneya", "Benyamin Jamialahmadi", "Marzieh S. Tahaei", "Yuhe Fan", "Mehdi Rezagholizadeh", "Boxing Chen", "Aref Jafari"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers achieve state-of-the-art results across many tasks, but their\nuniform application of quadratic self-attention to every token at every layer\nmakes them computationally expensive. We introduce DTRNet (Dynamic Token\nRouting Network), an improved Transformer architecture that allows tokens to\ndynamically skip the quadratic cost of cross-token mixing while still receiving\nlightweight linear updates. By preserving the MLP module and reducing the\nattention cost for most tokens to linear, DTRNet ensures that every token is\nexplicitly updated while significantly lowering overall computation. This\ndesign offers an efficient and effective alternative to standard dense\nattention. Once trained, DTRNet blocks routes only ~10% of tokens through\nattention at each layer while maintaining performance comparable to a full\nTransformer. It consistently outperforms routing-based layer skipping methods\nsuch as MoD and D-LLM in both accuracy and memory at matched FLOPs, while\nrouting fewer tokens to full attention. Its efficiency gains, scales with\nsequence length, offering significant reduction in FLOPs for long-context\ninputs. By decoupling token updates from attention mixing, DTRNet substantially\nreduces the quadratic share of computation, providing a simple, efficient, and\nscalable alternative to Transformers."}
{"id": "2509.00975", "pdf": "https://arxiv.org/pdf/2509.00975.pdf", "abs": "https://arxiv.org/abs/2509.00975", "title": "Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning", "authors": ["Zifeng Ding", "Shenyang Huang", "Zeyu Cao", "Emma Kondrup", "Zachary Yang", "Xingyue Huang", "Yuan Sui", "Zhangdie Yuan", "Yuqicheng Zhu", "Xianglong Hu", "Yuan He", "Farimah Poursafaei", "Michael Bronstein", "Andreas Vlachos"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Forecasting future links is a central task in temporal graph (TG) reasoning,\nrequiring models to leverage historical interactions to predict upcoming ones.\nTraditional neural approaches, such as temporal graph neural networks, achieve\nstrong performance but lack explainability and cannot be applied to unseen\ngraphs without retraining. Recent studies have begun to explore using large\nlanguage models (LLMs) for graph reasoning, but most of them are constrained to\nstatic graphs or small synthetic TGs and lack the evaluation of the quality of\nreasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced\nLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that\nfine-tunes LLMs to perform explainable link forecasting on real-world TGs.\nReaL-TG uses outcome-based reward to encourage models to self-explore reasoning\nstrategies from graph structure and to produce explanations that directly\njustify their predictions. To enable evaluation on LLM-generated reasoning\ntraces, we propose a new evaluation protocol combining ranking metrics with an\nLLM-as-a-Judge system that assesses both the quality of reasoning and the\nimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning\nQwen3-4B under our framework, show that it outperforms much larger frontier\nLLMs, including GPT-5 mini, on ranking metrics, while producing high-quality\nexplanations confirmed by both the LLM judge and human evaluation."}
{"id": "2509.00990", "pdf": "https://arxiv.org/pdf/2509.00990.pdf", "abs": "https://arxiv.org/abs/2509.00990", "title": "Hybrid Topic-Semantic Labeling and Graph Embeddings for Unsupervised Legal Document Clustering", "authors": ["Deepak Bastola", "Woohyeok Choi"], "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "20 pages, 8 figures, 3 tables", "summary": "Legal documents pose unique challenges for text classification due to their\ndomain-specific language and often limited labeled data. This paper proposes a\nhybrid approach for classifying legal texts by combining unsupervised topic and\ngraph embeddings with a supervised model. We employ Top2Vec to learn semantic\ndocument embeddings and automatically discover latent topics, and Node2Vec to\ncapture structural relationships via a bipartite graph of legal documents. The\nembeddings are combined and clustered using KMeans, yielding coherent groupings\nof documents. Our computations on a legal document dataset demonstrate that the\ncombined Top2Vec+Node2Vec approach improves clustering quality over text-only\nor graph-only embeddings. We conduct a sensitivity analysis of hyperparameters,\nsuch as the number of clusters and the dimensionality of the embeddings, and\ndemonstrate that our method achieves competitive performance against baseline\nLatent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF)\nmodels. Key findings indicate that while the pipeline presents an innovative\napproach to unsupervised legal document analysis by combining semantic topic\nmodeling with graph embedding techniques, its efficacy is contingent upon the\nquality of initial topic generation and the representational power of the\nchosen embedding models for specialized legal language. Strategic\nrecommendations include the exploration of domain-specific embeddings, more\ncomprehensive hyperparameter tuning for Node2Vec, dynamic determination of\ncluster numbers, and robust human-in-the-loop validation processes to enhance\nlegal relevance and trustworthiness. The pipeline demonstrates potential for\nexploratory legal data analysis and as a precursor to supervised learning tasks\nbut requires further refinement and domain-specific adaptation for practical\nlegal applications."}
{"id": "2509.00996", "pdf": "https://arxiv.org/pdf/2509.00996.pdf", "abs": "https://arxiv.org/abs/2509.00996", "title": "MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper", "authors": ["Runjia Zeng", "Guangyan Sun", "Qifan Wang", "Tong Geng", "Sohail Dianat", "Xiaotian Han", "Raghuveer Rao", "Xueling Zhang", "Cheng Han", "Lifu Huang", "Dongfang Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Considering deep neural networks as manifold mappers, the\npretrain-then-fine-tune paradigm can be interpreted as a two-stage process:\npretrain establishes a broad knowledge base, and fine-tune adjusts the model\nparameters to activate specific neural pathways to align with the target\nmanifold. Although prior fine-tuning approaches demonstrate success, their\nrigid parameter space limits their ability to dynamically activate appropriate\nneural pathways, rendering them ill-equipped to adapt flexibly to the diverse\nand evolving data distributions. In light of this view, we propose a novel\napproach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient\nmanifold-mapping framework. MEPT leverages the Mixture of Experts architecture\nby integrating multiple prompt experts to adaptively learn diverse and\nnon-stationary data distributions. Empirical evaluations demonstrate that MEPT\noutperforms several state-of-the-art parameter efficient baselines on\nSuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while\nsignificantly reducing activated prompts by 79.25%. The effectiveness of MEPT\nis further supported by theoretical insights from manifold learning and\nvalidated through neural activation pathway visualization results. Our code is\navaliable at https://github.com/runtsang/MEPT."}
{"id": "2509.01016", "pdf": "https://arxiv.org/pdf/2509.01016.pdf", "abs": "https://arxiv.org/abs/2509.01016", "title": "Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction", "authors": ["Aishni Parab", "Hongjing Lu", "Ying Nian Wu", "Sumit Gulwani"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "comment": "This is the preprint version corresponding to our NeurIPS 2025\n  Workshop on Multimodal Algorithmic Reasoning submission", "summary": "Inductive reasoning enables humans to infer abstract rules from limited\nexamples and apply them to novel situations. In this work, we compare an\nLLM-based hypothesis search framework with direct program generation approaches\non few-shot rule induction tasks. Our findings show that hypothesis search\nachieves performance comparable to humans, while direct program generation\nfalls notably behind. An error analysis reveals key bottlenecks in hypothesis\ngeneration and suggests directions for advancing program induction methods.\nOverall, this paper underscores the potential of LLM-based hypothesis search\nfor modeling inductive reasoning and the challenges in building more efficient\nsystems."}
{"id": "2509.01051", "pdf": "https://arxiv.org/pdf/2509.01051.pdf", "abs": "https://arxiv.org/abs/2509.01051", "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces", "authors": ["Matte Lim", "Catherine Yeh", "Martin Wattenberg", "Fernanda Viégas", "Panagiotis Michalatos"], "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 Short Paper Track (5 pages, 4 figures)", "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets."}
{"id": "2509.01052", "pdf": "https://arxiv.org/pdf/2509.01052.pdf", "abs": "https://arxiv.org/abs/2509.01052", "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games", "authors": ["Jaewoo Ahn", "Junseo Kim", "Heeseung Yun", "Jaehyeon Son", "Dongmin Park", "Jaewoong Cho", "Gunhee Kim"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "EMNLP 2025 Main. Project page:\n  https://ahnjaewoo.github.io/flashadventure", "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide."}
{"id": "2509.01055", "pdf": "https://arxiv.org/pdf/2509.01055.pdf", "abs": "https://arxiv.org/abs/2509.01055", "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use", "authors": ["Dongfu Jiang", "Yi Lu", "Zhuofeng Li", "Zhiheng Lyu", "Ping Nie", "Haozhe Wang", "Alex Su", "Hui Chen", "Kai Zou", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "32 pages, 5 figures, 13 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool."}
{"id": "2509.01167", "pdf": "https://arxiv.org/pdf/2509.01167.pdf", "abs": "https://arxiv.org/abs/2509.01167", "title": "Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models", "authors": ["Hyunjong Ok", "Jaeho Lee"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent advances in multimodal large language models (MLLMs) have led to much\nprogress in video understanding tasks. To avoid the heavy computational cost of\nprocessing all frames, these models typically rely on keyframe sampling methods\nguided by vision-language encoders (\\textit{e.g.,} SigLIP). However, it remains\nunclear whether such encoders can truly identify the most informative frames.\nIn this work, we provide several empirical pieces of evidence revealing that\npopular vision encoders critically suffer from their limited capability to\nidentify where the MLLM should look inside the video to handle the given\ntextual query appropriately. Our findings suggest that the development of\nbetter keyframe identification techniques may be necessary for efficient video\nMLLMs."}
{"id": "2509.01182", "pdf": "https://arxiv.org/pdf/2509.01182.pdf", "abs": "https://arxiv.org/abs/2509.01182", "title": "Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping", "authors": ["Wonduk Seo", "Taesub Shin", "Hyunjin An", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "cs.MA"], "comment": "Preprint", "summary": "Identifying whether two product listings refer to the same Stock Keeping Unit\n(SKU) is a persistent challenge in ecommerce, especially when explicit\nidentifiers are missing and product names vary widely across platforms. Rule\nbased heuristics and keyword similarity often misclassify products by\noverlooking subtle distinctions in brand, specification, or bundle\nconfiguration. To overcome these limitations, we propose Question to Knowledge\n(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for\nreliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates\ntargeted disambiguation questions, (2) a Knowledge Agent that resolves them via\nfocused web searches, and (3) a Deduplication Agent that reuses validated\nreasoning traces to reduce redundancy and ensure consistency. A human in the\nloop mechanism further refines uncertain cases. Experiments on real world\nconsumer goods datasets show that Q2K surpasses strong baselines, achieving\nhigher accuracy and robustness in difficult scenarios such as bundle\nidentification and brand origin disambiguation. By reusing retrieved reasoning\ninstead of issuing repeated searches, Q2K balances accuracy with efficiency,\noffering a scalable and interpretable solution for product integration."}
{"id": "2509.01308", "pdf": "https://arxiv.org/pdf/2509.01308.pdf", "abs": "https://arxiv.org/abs/2509.01308", "title": "GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models", "authors": ["Mattia Tritto", "Giuseppe Farano", "Dario Di Palma", "Gaetano Rossiello", "Fedelucio Narducci", "Dharmashankar Subramanian", "Tommaso Di Noia"], "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries that require precise alignment between user\nintent and the database schema. To mitigate this, test-time strategies such as\nBest-of-N (BoN) and Majority Voting (Maj) are often employed, based on the\nassumption that LLMs can generate correct answers but may require multiple\nattempts. However, these methods rely on surface-level heuristics, selecting\neither the syntactically correct query through execution-based BoN (ex-BoN) or\nthe most frequently generated query with Maj. Recently, Outcome Reward Models\n(ORMs), which assign utility scores to generated outputs based on semantic\ncorrectness, have emerged as a promising approach for better aligning model\npredictions with user intent. Nevertheless, their application to Text-to-SQL\nremains largely underexplored.\n  In this work, we evaluate ORMs as an effective heuristic for BoN, compare\nthem with ex-BoN and Maj, and introduce a framework for training ORMs for the\nText-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,\nfinetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3\nmodel families. Our results show that ORMs outperform ex-BoN and Maj, achieving\nexecution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and\n+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that\nfinetuning models already aligned with SQL generation, such as OmniSQL, yields\nsuperior ORM performance. Additionally, we observe that ORMs achieve\ncompetitive results on simple queries and benefit more from an increased number\nof candidates compared to ex-BoN and Maj."}
{"id": "2509.01321", "pdf": "https://arxiv.org/pdf/2509.01321.pdf", "abs": "https://arxiv.org/abs/2509.01321", "title": "Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward", "authors": ["Xinyu Tang", "Zhenduo Zhang", "Yurou Liu", "Wayne Xin Zhao", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models have leveraged reinforcement\nlearning with verifiable rewards (RLVR) to improve reasoning capabilities.\nHowever, scaling these methods typically requires extensive rollout computation\nand large datasets, leading to high training costs and low data efficiency. To\nmitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization\npipeline that combines optimized strategies for both offline and online data\nselection. In the offline phase, we curate a high-quality subset of training\nsamples based on diversity, influence, and appropriate difficulty. During\nonline RLVR training, we introduce a sample-level explorability metric to\ndynamically filter samples with low exploration potential, thereby reducing\nsubstantial rollout computational costs. Furthermore, we incorporate a replay\nmechanism for under-explored samples to ensure adequate training, which\nenhances the model's final convergence performance. Experiments across five\nreasoning benchmarks show that DEPO consistently outperforms existing methods\nin both offline and online data selection scenarios. Notably, using only 20% of\nthe training data, our approach achieves a 1.85 times speed-up on AIME24 and a\n1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset."}
{"id": "2509.01337", "pdf": "https://arxiv.org/pdf/2509.01337.pdf", "abs": "https://arxiv.org/abs/2509.01337", "title": "LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition", "authors": ["Qianrui Zhou", "Hua Xu", "Yifan Wang", "Xinzhi Dong", "Hanlei Zhang"], "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "Accepted by EMNLP 2025 (Main Track, Long Paper)", "summary": "Understanding human intents from multimodal signals is critical for analyzing\nhuman behaviors and enhancing human-machine interactions in real-world\nscenarios. However, existing methods exhibit limitations in their\nmodality-level reliance, constraining relational reasoning over fine-grained\nsemantics for complex intent understanding. This paper proposes a novel\nLLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the\nexpansive knowledge of large language models (LLMs) to establish semantic\nfoundations that boost smaller models' relational reasoning performance.\nSpecifically, an LLM-based strategy is proposed to extract fine-grained\nsemantics as guidance for subsequent reasoning, driven by a shallow-to-deep\nChain-of-Thought (CoT) that autonomously uncovers, describes, and ranks\nsemantic cues by their importance without relying on manually defined priors.\nBesides, we formally model three fundamental types of semantic relations\ngrounded in logical principles and analyze their nuanced interplay to enable\nmore effective relational reasoning. Extensive experiments on multimodal intent\nand dialogue act recognition tasks demonstrate LGSRR's superiority over\nstate-of-the-art methods, with consistent performance gains across diverse\nsemantic understanding scenarios. The complete data and code are available at\nhttps://github.com/thuiar/LGSRR."}
{"id": "2509.01391", "pdf": "https://arxiv.org/pdf/2509.01391.pdf", "abs": "https://arxiv.org/abs/2509.01391", "title": "MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model", "authors": ["Joonyong Park", "Daisuke Saito", "Nobuaki Minematsu"], "categories": ["eess.AS", "cs.CL"], "comment": "In Proceedings of the 17th Asia Pacific Signal and Information\n  Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "This study presents a novel approach to voice synthesis that can substitute\nthe traditional grapheme-to-phoneme (G2P) conversion by using a deep\nlearning-based model that generates discrete tokens directly from speech.\nUtilizing a pre-trained voice SSL model, we train a T5 encoder to produce\npseudo-language labels from mixed-script texts (e.g., containing Kanji and\nKana). This method eliminates the need for manual phonetic transcription,\nreducing costs and enhancing scalability, especially for large non-transcribed\naudio datasets. Our model matches the performance of conventional G2P-based\ntext-to-speech systems and is capable of synthesizing speech that retains\nnatural linguistic and paralinguistic features, such as accents and\nintonations."}
{"id": "2509.01401", "pdf": "https://arxiv.org/pdf/2509.01401.pdf", "abs": "https://arxiv.org/abs/2509.01401", "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "authors": ["Ali Abouzeid", "Bilal Elbouardi", "Mohamed Maged", "Shady Shehata"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted (The Third Arabic Natural Language Processing Conference)", "summary": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications."}
{"id": "2509.01566", "pdf": "https://arxiv.org/pdf/2509.01566.pdf", "abs": "https://arxiv.org/abs/2509.01566", "title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets", "authors": ["Yujing Wang", "Yiren Chen", "Huoran Li", "Chunxu Xu", "Yuchong Luo", "Xianghui Mao", "Cong Li", "Lun Du", "Chunyang Ma", "Qiqi Jiang", "Yin Wang", "Fan Gao", "Wenting Mo", "Pei Wen", "Shantanu Kumar", "Taejin Park", "Yiwei Song", "Vijay Rajaram", "Tao Cheng", "Sonu Durgia", "Pranam Kolari"], "categories": ["cs.IR", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate."}
{"id": "2509.01656", "pdf": "https://arxiv.org/pdf/2509.01656.pdf", "abs": "https://arxiv.org/abs/2509.01656", "title": "Reinforced Visual Perception with Tools", "authors": ["Zetong Zhou", "Dongping Chen", "Zixian Ma", "Zhihan Hu", "Mingyang Fu", "Sinan Wang", "Yao Wan", "Zhou Zhao", "Ranjay Krishna"], "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT."}
{"id": "2509.01716", "pdf": "https://arxiv.org/pdf/2509.01716.pdf", "abs": "https://arxiv.org/abs/2509.01716", "title": "An LLM-enabled semantic-centric framework to consume privacy policies", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement."}
{"id": "2509.01813", "pdf": "https://arxiv.org/pdf/2509.01813.pdf", "abs": "https://arxiv.org/abs/2509.01813", "title": "ShortageSim: Simulating Drug Shortages under Information Asymmetry", "authors": ["Mingxuan Cui", "Yilan Jiang", "Duo Zhou", "Cheng Qian", "Yuji Zhang", "Qiong Wang"], "categories": ["cs.MA", "cs.CL", "cs.GT"], "comment": "21 Pages", "summary": "Drug shortages pose critical risks to patient care and healthcare systems\nworldwide, yet the effectiveness of regulatory interventions remains poorly\nunderstood due to fundamental information asymmetries in pharmaceutical supply\nchains. We present \\textbf{ShortageSim}, the first Large Language Model\n(LLM)-based multi-agent simulation framework that captures the complex,\nstrategic interactions between drug manufacturers, institutional buyers, and\nregulatory agencies in response to shortage alerts. Unlike traditional\ngame-theoretic models that assume perfect rationality and complete information,\n\\textbf{ShortageSim} leverages LLMs to simulate bounded-rational\ndecision-making under uncertainty. Through a sequential production game\nspanning multiple quarters, we model how FDA announcements, both reactive\nalerts about existing shortages and proactive warnings about potential\ndisruptions, propagate through the supply chain and influence capacity\ninvestment and procurement decisions. Our experiments on historical shortage\nevents reveal that \\textbf{ShortageSim} reduces the resolution-lag percentage\nfor discontinued-disclosed cases by 83\\%, bringing simulated durations more\naligned to ground truth than the zero-shot baseline. We open-source\n\\textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at\nhttps://github.com/Lemutisme/Sortage_Management, providing a novel\ncomputational framework for designing and testing interventions in complex,\ninformation-scarce supply chains."}
{"id": "2509.01907", "pdf": "https://arxiv.org/pdf/2509.01907.pdf", "abs": "https://arxiv.org/abs/2509.01907", "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events", "authors": ["Zhenyuan Chen", "Chenxi Wang", "Ningyu Zhang", "Feng Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "under review", "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.01914", "pdf": "https://arxiv.org/pdf/2509.01914.pdf", "abs": "https://arxiv.org/abs/2509.01914", "title": "How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction", "authors": ["Ruijia Li", "Yuan-Hao Jiang", "Jiatong Wang", "Bo Jiang"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Proceedings of the 33rd International Conference on Computers in\n  Education (ICCE 2025). Asia-Pacific Society for Computers in Education", "summary": "Heuristic and scaffolded teacher-student dialogues are widely regarded as\ncritical for fostering students' higher-order thinking and deep learning.\nHowever, large language models (LLMs) currently face challenges in generating\npedagogically rich interactions. This study systematically investigates the\nstructural and behavioral differences between AI-simulated and authentic human\ntutoring dialogues. We conducted a quantitative comparison using an\nInitiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis\n(ENA). The results show that human dialogues are significantly superior to\ntheir AI counterparts in utterance length, as well as in questioning (I-Q) and\ngeneral feedback (F-F) behaviors. More importantly, ENA results reveal a\nfundamental divergence in interactional patterns: human dialogues are more\ncognitively guided and diverse, centered around a \"question-factual\nresponse-feedback\" teaching loop that clearly reflects pedagogical guidance and\nstudent-driven thinking; in contrast, simulated dialogues exhibit a pattern of\nstructural simplification and behavioral convergence, revolving around an\n\"explanation-simplistic response\" loop that is essentially a simple information\ntransfer between the teacher and student. These findings illuminate key\nlimitations in current AI-generated tutoring and provide empirical guidance for\ndesigning and evaluating more pedagogically effective generative educational\ndialogue systems."}
{"id": "2509.01938", "pdf": "https://arxiv.org/pdf/2509.01938.pdf", "abs": "https://arxiv.org/abs/2509.01938", "title": "EigenBench: A Comparative Behavioral Measure of Value Alignment", "authors": ["Jonathn Chang", "Leonard Piff", "Suvadip Sana", "Jasmine X. Li", "Lionel Levine"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Aligning AI with human values is a pressing unsolved problem. To address the\nlack of quantitative metrics for value alignment, we propose EigenBench: a\nblack-box method for comparatively benchmarking language models' values. Given\nan ensemble of models, a constitution describing a value system, and a dataset\nof scenarios, our method returns a vector of scores quantifying each model's\nalignment to the given constitution. To produce these scores, each model judges\nthe outputs of other models across many scenarios, and these judgments are\naggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a\nweighted-average judgment of the whole ensemble. EigenBench uses no ground\ntruth labels, as it is designed to quantify traits for which reasonable judges\nmay disagree on the correct label. Using prompted personas, we test whether\nEigenBench scores are more sensitive to the model or the prompt: we find that\nmost of the variance is explained by the prompt, but a small residual\nquantifies the disposition of the model itself."}
{"id": "2509.01954", "pdf": "https://arxiv.org/pdf/2509.01954.pdf", "abs": "https://arxiv.org/abs/2509.01954", "title": "Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic", "authors": ["Nirmalya Thakur", "Madeline D Hartel", "Lane Michael Boden", "Dallas Enriquez", "Boston Joyner Ricks"], "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.ET", "cs.LG", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "comment": null, "summary": "This work investigated about 10,000 COVID-19-related YouTube videos published\nbetween January 2023 and October 2024 to evaluate how temporal, lexical,\nlinguistic, and structural factors influenced engagement during the late\npandemic period. Publishing activity showed consistent weekday effects: in the\nfirst window, average views peaked on Mondays at 92,658; in the second, on\nWednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a\nshift in audience attention toward mid- and late week. Lexical analysis of\nvideo titles revealed recurring high-frequency keywords related to COVID-19 and\nYouTube features, including COVID, coronavirus, shorts, and live. Frequency\nanalysis revealed sharp spikes, with COVID appearing in 799 video titles in\nAugust 2024, while engagement analysis showed that videos titled with shorts\nattracted very high views, peaking at 2.16 million average views per video in\nJune 2023. Analysis of sentiment of video descriptions in English showed weak\ncorrelation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but\nstronger correlations emerged once outliers were addressed, with Spearman r =\n0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis\nof video durations revealed contrasting outcomes: long videos focusing on\npeople and blogs averaged 209,114 views, short entertainment videos averaged\n288,675 views, and medium-to-long news and politics videos averaged 51,309 and\n59,226 views, respectively. These results demonstrate that engagement patterns\nof COVID-19-related videos on YouTube during the late pandemic followed\ndistinct characteristics driven by publishing schedules, title vocabulary,\ntopics, and genre-specific duration effects."}
{"id": "2509.02077", "pdf": "https://arxiv.org/pdf/2509.02077.pdf", "abs": "https://arxiv.org/abs/2509.02077", "title": "From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach", "authors": ["Refat Othman", "Diaeddin Rimawi", "Bruno Rossi", "Barbara Russo"], "categories": ["cs.CR", "cs.CL", "cs.LG", "68T50 Natural language processing", "D.4.6; I.2.7"], "comment": "Accepted in The Journal of Systems and Software (2025)", "summary": "In the domain of security, vulnerabilities frequently remain undetected even\nafter their exploitation. In this work, vulnerabilities refer to publicly\ndisclosed flaws documented in Common Vulnerabilities and Exposures (CVE)\nreports. Establishing a connection between attacks and vulnerabilities is\nessential for enabling timely incident response, as it provides defenders with\nimmediate, actionable insights. However, manually mapping attacks to CVEs is\ninfeasible, thereby motivating the need for automation. This paper evaluates 14\nstate-of-the-art (SOTA) sentence transformers for automatically identifying\nvulnerabilities from textual descriptions of attacks. Our results demonstrate\nthat the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior\nclassification performance when using attack Technique descriptions, with an\nF1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was\nobserved that, on average, 56% of the vulnerabilities identified by the MMPNet\nmodel are also represented within the CVE repository in conjunction with an\nattack, while 61% of the vulnerabilities detected by the model correspond to\nthose cataloged in the CVE repository. A manual inspection of the results\nrevealed the existence of 275 predicted links that were not documented in the\nMITRE repositories. Consequently, the automation of linking attack techniques\nto vulnerabilities not only enhances the detection and response capabilities\nrelated to software security incidents but also diminishes the duration during\nwhich vulnerabilities remain exploitable, thereby contributing to the\ndevelopment of more secure systems."}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework."}
{"id": "2509.02175", "pdf": "https://arxiv.org/pdf/2509.02175.pdf", "abs": "https://arxiv.org/abs/2509.02175", "title": "Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks", "authors": ["Nils Hoehing", "Mayug Maniparambil", "Ellen Rushe", "Noel E. O'Connor", "Anthony Ventresque"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed\n  to be very easy for humans and hard for the current generation of VLMs, and\nthis is empirically verified. Our results show a striking lack of spatial\nrelation understanding in open source and frontier commercial VLMs and a\nsurprisingly high performance of reasoning models. Additionally, we perform a\ndisentanglement analysis to separate the contributions of object localization\nand spatial reasoning in chain-of-thought-based models and find that the\nperformance on the benchmark is bottlenecked by spatial reasoning and not\nobject localization capabilities.\n  We release the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience"}
{"id": "2509.02244", "pdf": "https://arxiv.org/pdf/2509.02244.pdf", "abs": "https://arxiv.org/abs/2509.02244", "title": "Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "We present a neural speech codec that challenges the need for complex\nresidual vector quantization (RVQ) stacks by introducing a simpler,\nsingle-stage quantization approach. Our method operates directly on the\nmel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4\npatches into a single, shared codebook. This patchwise design simplifies the\narchitecture, enables low-latency streaming, and yields a discrete latent grid.\nTo ensure high-fidelity synthesis, we employ a late-stage adversarial\nfine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the\ncodec's reconstructed spectrograms. Operating at approximately 7.5 kbits/s for\n16 kHz speech, our system was evaluated against several state-of-the-art neural\ncodecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results\ndemonstrate that our simplified, non-residual architecture achieves competitive\nperceptual quality and intelligibility, validating it as an effective and open\nfoundation for future low-latency codec designs."}
{"id": "2509.02399", "pdf": "https://arxiv.org/pdf/2509.02399.pdf", "abs": "https://arxiv.org/abs/2509.02399", "title": "Evaluating Cumulative Spectral Gradient as a Complexity Measure", "authors": ["Haji Gul", "Abdul Ghani Naim", "Ajaz Ahmad Bhat"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Accurate estimation of dataset complexity is crucial for evaluating and\ncomparing link prediction models for knowledge graphs (KGs). The Cumulative\nSpectral Gradient (CSG) metric derived from probabilistic divergence between\nclasses within a spectral clustering framework was proposed as a dataset\ncomplexity measure that (1) naturally scales with the number of classes and (2)\ncorrelates strongly with downstream classification performance. In this work,\nwe rigorously assess CSG behavior on standard knowledge graph link prediction\nbenchmarks a multi class tail prediction task, using two key parameters\ngoverning its computation, M, the number of Monte Carlo sampled points per\nclass, and K, the number of nearest neighbors in the embedding space. Contrary\nto the original claims, we find that (1) CSG is highly sensitive to the choice\nof K and therefore does not inherently scale with the number of target classes,\nand (2) CSG values exhibit weak or no correlation with established performance\nmetrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237,\nWN18RR, and other standard datasets, we demonstrate that CSG purported\nstability and generalization predictive power break down in link prediction\nsettings. Our results highlight the need for more robust, classifier agnostic\ncomplexity measures in KG link prediction evaluation."}
{"id": "2509.02444", "pdf": "https://arxiv.org/pdf/2509.02444.pdf", "abs": "https://arxiv.org/abs/2509.02444", "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent", "authors": ["Jingru Fan", "Yufan Dang", "Jingyao Wu", "Huatao Li", "Runde Yang", "Xiyuan Yang", "Yuheng Wang", "Zhong Zhang", "Yaxi Lu", "Yankai Lin", "Zhiyuan Liu", "Dahai Li", "Chen Qian"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Project at https://github.com/OpenBMB/AppCopilot", "summary": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime."}
{"id": "2509.02521", "pdf": "https://arxiv.org/pdf/2509.02521.pdf", "abs": "https://arxiv.org/abs/2509.02521", "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Wenjia Ma", "Aixin Sun", "Yequan Wang"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": null, "summary": "Full-duplex dialog models are designed to listen and speak simultaneously\nwith rapid responses to fast-changing user input. Among existing approaches,\nnative full-duplex models merges different channels (e.g. listen and speak) in\na single time step, overcoming the high response latency inherent to\ntime-division multiplexing time-division multiplexing (TDM) alternatives. Yet,\na key challenge remains: aligning textual monologues with audio streams that\noperate at different bitrates. The prevailing solution relies on word-level\nalignment, but this can degrade the language ability of large pre-trained\nmodels. Moreover, it requires highly accurate timestamps for every token, which\nintroduces cascading errors and increases pre-processing costs. In this paper,\nwe propose textual monologues in continuous tokens sequence, namely \"natural\"\nmonologues, which mimics humanoid cognitive behavior in dialogs. For temporal\nalignment, we alternate the position of the natural monologue - leading or\ntrailing the audio - across different training stages. This \"dual\" training\nparadigm proves highly effective in building FLM-Audio, our 7B spoken dialog\nmodel that demonstrates superior responsiveness, duplexity, and chatting\nexperiences, as confirmed by experimental results."}
{"id": "2509.02544", "pdf": "https://arxiv.org/pdf/2509.02544.pdf", "abs": "https://arxiv.org/abs/2509.02544", "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning", "authors": ["Haoming Wang", "Haoyang Zou", "Huatong Song", "Jiazhan Feng", "Junjie Fang", "Junting Lu", "Longxiang Liu", "Qinyu Luo", "Shihao Liang", "Shijue Huang", "Wanjun Zhong", "Yining Ye", "Yujia Qin", "Yuwen Xiong", "Yuxin Song", "Zhiyong Wu", "Bo Li", "Chen Dun", "Chong Liu", "Fuxing Leng", "Hanbin Wang", "Hao Yu", "Haobin Chen", "Hongyi Guo", "Jing Su", "Jingjia Huang", "Kai Shen", "Kaiyu Shi", "Lin Yan", "Peiyao Zhao", "Pengfei Liu", "Qinghao Ye", "Renjie Zheng", "Wayne Xin Zhao", "Wen Heng", "Wenhao Huang", "Wenqian Wang", "Xiaobo Qin", "Yi Lin", "Youbin Wu", "Zehui Chen", "Zihao Wang", "Baoquan Zhong", "Xinchun Zhang", "Xujing Li", "Yuanfan Li", "Zhongkai Zhao", "Chengquan Jiang", "Faming Wu", "Haotian Zhou", "Jinlin Pang", "Li Han", "Qianli Ma", "Siyao Liu", "Songhua Cai", "Wenqi Fu", "Xin Liu", "Zhi Zhang", "Bo Zhou", "Guoliang Li", "Jiajun Shi", "Jiale Yang", "Jie Tang", "Li Li", "Taoran Lu", "Woyu Lin", "Xiaokang Tong", "Xinyao Li", "Yichi Zhang", "Yu Miao", "Zhengxuan Jiang", "Zili Li", "Ziyuan Zhao", "Chenxin Li", "Dehua Ma", "Feng Lin", "Ge Zhang", "Haihua Yang", "Hangyu Guo", "Hongda Zhu", "Jiaheng Liu", "Junda Du", "Kai Cai", "Kuanye Li", "Lichen Yuan", "Meilan Han", "Minchao Wang", "Shuyue Guo", "Tianhao Cheng", "Xiaobo Ma", "Xiaojun Xiao", "Xiaolong Huang", "Xinjie Chen", "Yidi Du", "Yilin Chen", "Yiwen Wang", "Zhaojian Li", "Zhenzhu Yang", "Zhiyuan Zeng", "Chaolin Jin", "Chen Li", "Hao Chen", "Haoli Chen", "Jian Chen", "Qinghao Zhao", "Guang Shi"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios."}
{"id": "2509.02547", "pdf": "https://arxiv.org/pdf/2509.02547.pdf", "abs": "https://arxiv.org/abs/2509.02547", "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey", "authors": ["Guibin Zhang", "Hejia Geng", "Xiaohang Yu", "Zhenfei Yin", "Zaibin Zhang", "Zelin Tan", "Heng Zhou", "Zhongzhi Li", "Xiangyuan Xue", "Yijiang Li", "Yifan Zhou", "Yang Chen", "Chen Zhang", "Yutao Fan", "Zihu Wang", "Songtao Huang", "Yue Liao", "Hongru Wang", "Mengyue Yang", "Heng Ji", "Michael Littman", "Jun Wang", "Shuicheng Yan", "Philip Torr", "Lei Bai"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents."}
{"id": "2509.02563", "pdf": "https://arxiv.org/pdf/2509.02563.pdf", "abs": "https://arxiv.org/abs/2509.02563", "title": "DynaGuard: A Dynamic Guardrail Model With User-Defined Policies", "authors": ["Monte Hoover", "Vatsal Baherwani", "Neel Jain", "Khalid Saifullah", "Joseph Vincent", "Chirag Jain", "Melissa Kazemi Rad", "C. Bayan Bruss", "Ashwinee Panda", "Tom Goldstein"], "categories": ["cs.LG", "cs.CL"], "comment": "22 Pages", "summary": "Guardian models are used to supervise and moderate the outputs of user-facing\nchatbots, enforcing guardrails and detecting bad behaviors. Standard guardian\nmodels like LlamaGuard detect predefined, static categories of harms. We\npropose dynamic guardian models that evaluate text based on user-defined\npolicies, making them useful for different application domains that are not\naddressed by standard guardian models. Our dynamic guardian models can be used\nfor fast detection of policy violations or with chain-of-thought reasoning that\narticulates and justifies the model outputs. Our dynamic guardian models match\nstatic models in detection accuracy for static harm categories while\nidentifying violations of free-form policies with accuracy comparable to\nfrontier reasoning models in a fraction of the time."}
{"id": "2210.14275", "pdf": "https://arxiv.org/pdf/2210.14275.pdf", "abs": "https://arxiv.org/abs/2210.14275", "title": "Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation", "authors": ["Wenchuan Mu"], "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\n  Computation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases."}
{"id": "2401.02968", "pdf": "https://arxiv.org/pdf/2401.02968.pdf", "abs": "https://arxiv.org/abs/2401.02968", "title": "Rule-Guided Joint Embedding Learning over Knowledge Graphs", "authors": ["Qisong Li", "Ji Lin", "Sijia Wei", "Neng Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on knowledge graph embedding focus on mapping entities and\nrelations into low-dimensional vector spaces. While most existing models\nprimarily exploit structural information, knowledge graphs also contain rich\ncontextual and textual information that can enhance embedding effectiveness. In\nthis work, we propose a novel model that integrates both contextual and textual\nsignals into entity and relation embeddings through a graph convolutional\nnetwork. To better utilize context, we introduce two metrics: confidence,\ncomputed via a rule-based method, and relatedness, derived from textual\nrepresentations. These metrics enable more precise weighting of contextual\ninformation during embedding learning. Extensive experiments on two widely used\nbenchmark datasets demonstrate the effectiveness of our approach, showing\nconsistent improvements over strong baselines."}
{"id": "2401.06772", "pdf": "https://arxiv.org/pdf/2401.06772.pdf", "abs": "https://arxiv.org/abs/2401.06772", "title": "Semantic Parsing for Question Answering over Knowledge Graphs", "authors": ["Sijia Wei", "Wenwen Zhang", "Qisong Li", "Jiang Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel method for question answering over\nknowledge graphs based on graph-to-segment mapping, designed to improve the\nunderstanding of natural language questions. Our approach is grounded in\nsemantic parsing, a key technique for interpreting question utterances. The\nmain challenges arise from handling implicit entities and relations, as well as\ncomplex constraints such as temporal conditions, ordinality, and aggregation\nwithin the context of a knowledge graph. To address these issues, our framework\nintegrates both rule-based and neural methods to parse and construct accurate,\ncomprehensive semantic segment sequences. These sequences are then assembled\ninto semantic query graphs, providing precise representations of question\nutterances. We formulate question semantic parsing as a sequence generation\ntask, employing an encoder-decoder neural network to map natural language\nquestions into semantic segments. Furthermore, to enhance the identification of\nimplicit entities and relations, we incorporate a graph neural network that\nleverages knowledge graph context to enrich question representations.\nExperimental evaluations on two benchmark datasets demonstrate the\neffectiveness and superior performance of our model in semantic parsing for\nknowledge graph question answering."}
{"id": "2401.12989", "pdf": "https://arxiv.org/pdf/2401.12989.pdf", "abs": "https://arxiv.org/abs/2401.12989", "title": "Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports", "authors": ["Adriano Belisario", "Scott A. Hale", "Luc Rocher"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Gun violence is a pressing human rights issue that affects nearly every\ndimension of the social fabric, from healthcare and education to psychology and\nthe economy. Reliable data on firearm events is paramount to developing more\neffective public policy and emergency responses. However, the lack of\ncomprehensive databases and the risks of in-person surveys prevent human rights\norganizations from collecting needed data in most countries. Here, we partner\nwith a Brazilian human rights organization to conduct a systematic evaluation\nof language models to assist with monitoring real-world firearm events from\nsocial media data. We propose a fine-tuned BERT-based model trained on Twitter\n(now X) texts to distinguish gun violence reports from ordinary Portuguese\ntexts. We then incorporate our model into a web application and test it in a\nlive intervention. We study and interview Brazilian analysts who continuously\ncheck social media texts to identify new gun violence events. Qualitative\nassessments show that our solution helped all analysts use their time more\nefficiently and expanded their search capacities. Quantitative assessments show\nthat the use of our model was associated with analysts having further\ninteractions with online users reporting gun violence. Our findings suggest\nthat human-centered interventions using language models can help support the\nwork of human rights organizations."}
{"id": "2402.14533", "pdf": "https://arxiv.org/pdf/2402.14533.pdf", "abs": "https://arxiv.org/abs/2402.14533", "title": "Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard", "authors": ["Ariel Rosenfeld", "Teddy Lazebnik"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are capable of generating text that is similar\nto or surpasses human quality. However, it is unclear whether LLMs tend to\nexhibit distinctive linguistic styles akin to how human authors do. Through a\ncomprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech\n(POS) distribution, dependency distribution, and sentiment of texts generated\nby three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse\ninputs. The results point to significant linguistic variations which, in turn,\nenable us to attribute a given text to its LLM origin with a favorable 88\\%\naccuracy using a simple off-the-shelf classification model. Theoretical and\npractical implications of this intriguing finding are discussed."}
{"id": "2404.07851", "pdf": "https://arxiv.org/pdf/2404.07851.pdf", "abs": "https://arxiv.org/abs/2404.07851", "title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations", "authors": ["Dayeon Ki", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2024 Findings", "summary": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation."}
{"id": "2405.13923", "pdf": "https://arxiv.org/pdf/2405.13923.pdf", "abs": "https://arxiv.org/abs/2405.13923", "title": "Why Not Transform Chat Large Language Models to Non-English?", "authors": ["Xiang Geng", "Ming Zhu", "Jiahuan Li", "Zhejian Lai", "Wei Zou", "Shuaijie She", "Jiaxin Guo", "Xiaofeng Zhao", "Yinglu Li", "Yuang Li", "Chang Su", "Yanqing Zhao", "Xinglin Lyu", "Min Zhang", "Jiajun Chen", "Hao Yang", "Shujian Huang"], "categories": ["cs.CL"], "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50646-z}", "summary": "The scarcity of non-English data limits the development of non-English large\nlanguage models (LLMs). Transforming English-centric LLMs to non-English has\nbeen identified as an effective and resource-efficient method. Previous works\nstart from base LLMs and perform knowledge distillation (KD) with data\ngenerated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are\nfurther optimized for advanced abilities, e.g. multi-turn conversation and\nhuman preference alignment, and thus more powerful in both helpfulness and\nsafety. However, transforming a chat LLM involves two critical issues: (1) How\ncan we effectively transfer advanced abilities without their supervised data?\n(2) How can we prevent the original knowledge from catastrophic forgetting\nduring transformation? We target these issues by introducing a simple framework\ncalled TransLLM. For the first issue, TransLLM divides the transfer problem\ninto some common sub-tasks with the translation chain-of-thought, which uses\nthe translation as the bridge between English and non-English step-by-step. We\nfurther enhance the performance of sub-tasks with publicly available data. For\nthe second issue, we propose a method comprising two synergistic components:\nlow-rank adaptation for training to maintain the original LLM parameters, and\nrecovery KD, which utilizes data generated by the chat LLM itself to recover\nthe original knowledge from the frozen parameters. In the experiments, we\ntransform the LLaMA-2-chat-7B to the Thai language. Our method, using only\nsingle-turn data, outperforms strong baselines and ChatGPT on multi-turn\nbenchmark MT-bench. Furthermore, our method, without safety data, rejects more\nharmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. Code\nis available at https://github.com/hy5468/TransLLM."}
{"id": "2406.11614", "pdf": "https://arxiv.org/pdf/2406.11614.pdf", "abs": "https://arxiv.org/abs/2406.11614", "title": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces", "authors": ["Yihuai Hong", "Lei Yu", "Haiqin Yang", "Shauli Ravfogel", "Mor Geva"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP 2025 Main", "summary": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors."}
{"id": "2408.10722", "pdf": "https://arxiv.org/pdf/2408.10722.pdf", "abs": "https://arxiv.org/abs/2408.10722", "title": "MEGen: Generative Backdoor into Large Language Models via Model Editing", "authors": ["Jiyang Qiu", "Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao", "Yun Li", "Qianren Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen."}
{"id": "2409.10038", "pdf": "https://arxiv.org/pdf/2409.10038.pdf", "abs": "https://arxiv.org/abs/2409.10038", "title": "On the Diagram of Thought", "authors": ["Yifan Zhang", "Yang Yuan", "Andrew Chi-Chih Yao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "31 pages", "summary": "Large Language Models (LLMs) excel at many tasks but often falter on complex\nproblems that require structured, multi-step reasoning. We introduce the\nDiagram of Thought (DoT), a new framework that enables a single LLM to build\nand navigate a mental map of its reasoning. Instead of thinking in a straight\nline, the model constructs a dynamic diagram of ideas, where it can propose\ndifferent lines of thought, critique its own steps, and synthesize validated\ninsights into a final conclusion. This entire process is self-contained within\nthe model, making it highly efficient by avoiding the complex external\ncontrollers or search algorithms required by other methods. To ensure the\nreliability of this process, we ground DoT in a rigorous mathematical framework\nfrom category theory. This foundation guarantees that the way the model\ncombines information is logical, consistent, and robust, regardless of the\norder in which ideas were explored. The result is a more powerful and\ntransparent reasoning process that produces a fully auditable, step-by-step\ntrace of the LLM's thinking, bridging the gap between fluent language and\nformal reasoning."}
{"id": "2409.15664", "pdf": "https://arxiv.org/pdf/2409.15664.pdf", "abs": "https://arxiv.org/abs/2409.15664", "title": "Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint", "authors": ["Dayeon Ki", "Cheonbok Park", "Hyunjoong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2024 RepL4NLP Workshop", "summary": "Accurately aligning contextual representations in cross-lingual sentence\nembeddings is key for effective parallel data mining. A common strategy for\nachieving this alignment involves disentangling semantics and language in\nsentence embeddings derived from multilingual pre-trained models. However, we\ndiscover that current disentangled representation learning methods suffer from\nsemantic leakage - a term we introduce to describe when a substantial amount of\nlanguage-specific information is unintentionally leaked into semantic\nrepresentations. This hinders the effective disentanglement of semantic and\nlanguage representations, making it difficult to retrieve embeddings that\ndistinctively represent the meaning of the sentence. To address this challenge,\nwe propose a novel training objective, ORthogonAlity Constraint LEarning\n(ORACLE), tailored to enforce orthogonality between semantic and language\nembeddings. ORACLE builds upon two components: intra-class clustering and\ninter-class separation. Through experiments on cross-lingual retrieval and\nsemantic textual similarity tasks, we demonstrate that training with the ORACLE\nobjective effectively reduces semantic leakage and enhances semantic alignment\nwithin the embedding space."}
{"id": "2410.12341", "pdf": "https://arxiv.org/pdf/2410.12341.pdf", "abs": "https://arxiv.org/abs/2410.12341", "title": "Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI", "authors": ["Daniele Gambetta", "Gizem Gezici", "Fosca Giannotti", "Dino Pedreschi", "Alistair Knott", "Luca Pappalardo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As synthetic content increasingly infiltrates the web, generative AI models\nmay be retrained on their own outputs: a process termed \"autophagy\". This leads\nto model collapse: a progressive loss of performance and diversity across\ngenerations. Recent studies have examined the emergence of model collapse\nacross various generative AI models and data types, and have proposed\nmitigation strategies that rely on incorporating human-authored content.\nHowever, current characterizations of model collapse remain limited, and\nexisting mitigation methods assume reliable knowledge of whether training data\nis human-authored or AI-generated. In this paper, we address these gaps by\nintroducing new measures that characterise collapse directly from a model's\nnext-token probability distributions, rather than from properties of\nAI-generated text. Using these measures, we show that the degree of collapse\ndepends on the complexity of the initial training set, as well as on the extent\nof autophagy. Our experiments prompt a new suggestion: that model collapse\noccurs when a model trains on data that does not \"surprise\" it. We express this\nhypothesis in terms of the well-known Free Energy Principle in cognitive\nscience. Building on this insight, we propose a practical mitigation strategy:\nfiltering training items by high surplexity, maximising the surprise of the\nmodel. Unlike existing methods, this approach does not require distinguishing\nbetween human- and AI-generated data. Experiments across datasets and models\ndemonstrate that our strategy is at least as effective as human-data baselines,\nand even more effective in reducing distributional skewedness. Our results\nprovide a richer understanding of model collapse and point toward more\nresilient approaches for training generative AI systems in environments\nincreasingly saturated with synthetic data."}
{"id": "2410.13258", "pdf": "https://arxiv.org/pdf/2410.13258.pdf", "abs": "https://arxiv.org/abs/2410.13258", "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?", "authors": ["Xiangci Li", "Jessica Ouyang"], "categories": ["cs.CL"], "comment": "Accepted by Findings of EMNLP 2025", "summary": "Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection,\na.k.a. reranking or filtering, remains less clear. This paper empirically\nanalyzes how knowledge selection influences downstream generation performance\nin RAG systems. By simulating different retrieval and selection conditions\nthrough a controlled mixture of gold and distractor knowledge, we assess the\nimpact of these factors on generation outcomes. Our findings indicate that the\ndownstream generator model's capability, as well as the complexity of the task\nand dataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing limited benefit when a strong generator model is used on\nclear, well-defined tasks. For weaker generator models or more ambiguous tasks\nand datasets, the knowledge F1 score becomes a critical factor, and the\nknowledge selector plays a more prominent role in improving overall\nperformance."}
{"id": "2410.18798", "pdf": "https://arxiv.org/pdf/2410.18798.pdf", "abs": "https://arxiv.org/abs/2410.18798", "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs", "authors": ["Wei He", "Zhiheng Xi", "Wanxu Zhao", "Xiaoran Fan", "Yiwen Ding", "Zifei Shan", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings. The code and dataset are publicly\n  available at https://github.com/hewei2001/ReachQA", "summary": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs), including recognizing key\ninformation from visual inputs and conducting reasoning over it. While\nfine-tuning MLLMs for reasoning is critical, collecting and annotating charts\nand questions is expensive, hard to scale, and often results in low-quality\nannotations. To address this, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and scalable data synthesis method for\ndistilling visual reasoning abilities from LLMs to MLLMs. The code serves as an\nintermediary that translates visual chart representations into textual\nrepresentations, enabling language models to understand cross-modal information\nand generate reasoning chains accordingly. In this way, we can employ\ntext-based synthesizing techniques to expand chart-plotting code and generate\nhigh-quality Q&A pairs for training models. This produces ReachQA, a dataset\ncontaining 3k reasoning-intensive charts and 20k Q&A pairs to enhance both\nrecognition and reasoning abilities of MLLMs. Experiments show that models\nfine-tuned with ReachQA not only perform well on chart-related tasks but also\nshow performance gains on general reasoning benchmarks. The code and dataset\nare publicly available at https://github.com/hewei2001/ReachQA."}
{"id": "2411.12142", "pdf": "https://arxiv.org/pdf/2411.12142.pdf", "abs": "https://arxiv.org/abs/2411.12142", "title": "A Computational Method for Measuring \"Open Codes\" in Qualitative Analysis", "authors": ["John Chen", "Alexandros Lotsos", "Sihan Cheng", "Caiyi Wang", "Lexie Zhao", "Jessica Hullman", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. A central method in this process is inductive\ncoding, where researchers identify and interpret codes directly from the\ndatasets themselves. Yet, this exploratory approach poses challenges for\nmeeting methodological expectations (such as ``depth'' and ``variation''),\nespecially as researchers increasingly adopt Generative AI (GAI) for support.\nGround-truth-based metrics are insufficient because they contradict the\nexploratory nature of inductive coding, while manual evaluation can be\nlabor-intensive. This paper presents a theory-informed computational method for\nmeasuring inductive coding results from humans and GAI. Our method first merges\nindividual codebooks using an LLM-enriched algorithm. It measures each coder's\ncontribution against the merged result using four novel metrics: Coverage,\nOverlap, Novelty, and Divergence. Through two experiments on a human-coded\nonline conversation dataset, we 1) reveal the merging algorithm's impact on\nmetrics; 2) validate the metrics' stability and robustness across multiple runs\nand different LLMs; and 3) showcase the metrics' ability to diagnose coding\nissues, such as excessive or irrelevant (hallucinated) codes. Our work provides\na reliable pathway for ensuring methodological rigor in human-AI qualitative\nanalysis."}
{"id": "2411.14252", "pdf": "https://arxiv.org/pdf/2411.14252.pdf", "abs": "https://arxiv.org/abs/2411.14252", "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification", "authors": ["Junhua Liu", "Yong Keat Tan", "Bin Fu", "Kwan Hui Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Proceedings of CIKM'25", "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We also propose MINT-CL, a multi-task contrastive learning framework\nfor multi-turn intent classification, which improves performance while reducing\ndependence on large-scale annotated datasets. Empirical results demonstrate\nthat our approach outperforms competitive baselines in dialogue generation\nquality and classification accuracy, particularly in multilingual settings. To\nfacilitate future research, we release MINT-E, a comprehensive, multilingual,\nintent-aware multi-turn dialogue corpus derived from the e-commerce\ndomain\\footnote{The reproduced source code and dataset are available at\nhttps://github.com/junhua/chain-of-intent."}
{"id": "2412.03679", "pdf": "https://arxiv.org/pdf/2412.03679.pdf", "abs": "https://arxiv.org/abs/2412.03679", "title": "Evaluating Language Models as Synthetic Data Generators", "authors": ["Seungone Kim", "Juyoung Suk", "Xiang Yue", "Vijay Viswanathan", "Seongyun Lee", "Yizhong Wang", "Kiril Gashteovski", "Carolin Lawrence", "Sean Welleck", "Graham Neubig"], "categories": ["cs.CL"], "comment": "ACL 2025 (main)", "summary": "Given the increasing use of synthetic data in language model (LM)\npost-training, an LM's ability to generate high-quality data has become nearly\nas crucial as its ability to solve problems directly. While prior works have\nfocused on developing effective data generation methods, they lack systematic\ncomparison of different LMs as data generators in a unified setting. To address\nthis gap, we propose AgoraBench, a benchmark that provides standardized\nsettings and metrics to evaluate LMs' data generation abilities. Through\nsynthesizing 1.26 million training instances using 6 LMs and training 99\nstudent models, we uncover key insights about LMs' data generation\ncapabilities. First, we observe that LMs exhibit distinct strengths. For\ninstance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet\nperforms better at enhancing existing ones. Furthermore, our analysis reveals\nthat an LM's data generation ability doesn't necessarily correlate with its\nproblem-solving ability. Instead, multiple intrinsic features of data\nquality-including response quality, perplexity, and instruction\ndifficulty-collectively serve as better indicators. Finally, we demonstrate\nthat strategic choices in output format and cost-conscious model selection\nsignificantly impact data generation effectiveness."}
{"id": "2412.04497", "pdf": "https://arxiv.org/pdf/2412.04497.pdf", "abs": "https://arxiv.org/abs/2412.04497", "title": "Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research", "authors": ["Tianyang Zhong", "Zhenyuan Yang", "Zhengliang Liu", "Ruidong Zhang", "Yiheng Liu", "Haiyang Sun", "Yi Pan", "Yiwei Li", "Yifan Zhou", "Hanqi Jiang", "Junhao Chen", "Tianming Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity."}
{"id": "2412.09318", "pdf": "https://arxiv.org/pdf/2412.09318.pdf", "abs": "https://arxiv.org/abs/2412.09318", "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction", "authors": ["Jing Liu", "Abdellah Fourtassi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications."}
{"id": "2412.12928", "pdf": "https://arxiv.org/pdf/2412.12928.pdf", "abs": "https://arxiv.org/abs/2412.12928", "title": "Truthful Text Sanitization Guided by Inference Attacks", "authors": ["Ildikó Pilán", "Benet Manzanares-Salor", "David Sánchez", "Pierre Lison"], "categories": ["cs.CL"], "comment": null, "summary": "Text sanitization aims to rewrite parts of a document to prevent disclosure\nof personal information. The central challenge of text sanitization is to\nstrike a balance between privacy protection (avoiding the leakage of personal\ninformation) and utility preservation (retaining as much as possible of the\ndocument's original content). To this end, we introduce a novel text\nsanitization method based on generalizations, that is, broader but still\ninformative terms that subsume the semantic content of the original text spans.\nThe approach relies on the use of instruction-tuned large language models\n(LLMs) and is divided into two stages. Given a document including text spans\nexpressing personally identifiable information (PII), the LLM is first applied\nto obtain truth-preserving replacement candidates for each text span and rank\nthose according to their abstraction level. Those candidates are then evaluated\nfor their ability to protect privacy by conducting inference attacks with the\nLLM. Finally, the system selects the most informative replacement candidate\nshown to be resistant to those attacks. This two-stage process produces\nreplacements that effectively balance privacy and utility.\n  We also present novel metrics to evaluate these two aspects without needing\nto manually annotate documents. Results on the Text Anonymization Benchmark\nshow that the proposed approach, implemented with Mistral 7B Instruct, leads to\nenhanced utility, with only a marginal (< 1 p.p.) increase in re-identification\nrisk compared to fully suppressing the original spans. Furthermore, our\napproach is shown to be more truth-preserving than existing methods such as\nMicrosoft Presidio's synthetic replacements."}
{"id": "2412.16556", "pdf": "https://arxiv.org/pdf/2412.16556.pdf", "abs": "https://arxiv.org/abs/2412.16556", "title": "Acquisition of Recursive Possessives and Recursive Locatives in Mandarin", "authors": ["Chenxi Fu", "Xiaoyi Wang", "Zaijiang Man", "Caimei Yang"], "categories": ["cs.CL"], "comment": null, "summary": "As recursion has been underlying any linguistic work for the last 60 years,\nthe acquisition of recursive structures by children during language learning\nhas become a focal point of inquiry. This study delves into the developmental\ntrajectory of Mandarin-speaking children's acquisition of recursive possessives\nand locatives, assessing the impact of structural diversity on language\nacquisition. The research contrasts the comprehension of two-level recursive\nstructures among children aged 3 to 7 years, employing answering question while\nseeing a picture task to elicit responses. The findings indicate that children\ndo not attain adult-like proficiency in two-level recursion until the age of 6,\nand there exists a notable asymmetry in the acquisition of recursive\npossessives versus locatives. These results underscore the primacy of\nstructural complexity and cognitive factors in the acquisition process,\nenhancing our comprehension of the cognitive foundations of language\ndevelopment and the pivotal role of recursion in child language acquisition."}
{"id": "2501.00045", "pdf": "https://arxiv.org/pdf/2501.00045.pdf", "abs": "https://arxiv.org/abs/2501.00045", "title": "Improving Low-Resource Machine Translation via Cross-Linguistic Transfer from Typologically Similar High-Resource Languages", "authors": ["Saughmon Boujkian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This study examines the cross-linguistic effectiveness of transfer learning\nfor low-resource machine translation by fine-tuning models initially trained on\ntypologically similar high-resource languages, using limited data from the\ntarget low-resource language. We hypothesize that linguistic similarity enables\nefficient adaptation, reducing the need for extensive training data. To test\nthis, we conduct experiments on five typologically diverse language pairs\nspanning distinct families: Semitic (Modern Standard Arabic to Levantine\nArabic), Bantu (Hausa to Zulu), Romance (Spanish to Catalan), Slavic (Slovak to\nMacedonian), and a language isolate (Eastern Armenian to Western Armenian).\nResults show that transfer learning consistently improves translation quality\nacross all pairs, confirming its applicability beyond closely related\nlanguages. As a secondary analysis, we vary key hyperparameters learning rate,\nbatch size, number of epochs, and weight decay to ensure results are not\ndependent on a single configuration. We find that moderate batch sizes (e.g.,\n32) are often optimal for similar pairs, smaller sizes benefit less similar\npairs, and excessively high learning rates can destabilize training. These\nfindings provide empirical evidence for the generalizability of transfer\nlearning across language families and offer practical guidance for building\nmachine translation systems in low-resource settings with minimal tuning\neffort."}
{"id": "2501.00273", "pdf": "https://arxiv.org/pdf/2501.00273.pdf", "abs": "https://arxiv.org/abs/2501.00273", "title": "Echoes in AI: Quantifying lack of plot diversity in LLM outputs", "authors": ["Weijia Xu", "Nebojsa Jojic", "Sudha Rao", "Chris Brockett", "Bill Dolan"], "categories": ["cs.CL"], "comment": "PNAS Vol. 122 No. 35. Copyright \\c{opyright} 2025 the Author(s).\n  Published by PNAS. This open access article is distributed under Creative\n  Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)", "summary": "With rapid advances in large language models (LLMs), there has been an\nincreasing application of LLMs in creative content ideation and generation. A\ncritical question emerges: can current LLMs provide ideas that are diverse\nenough to truly bolster collective creativity? We examine two state-of-the-art\nLLMs, GPT-4 and LLaMA-3, on story generation and discover that LLM-generated\nstories often consist of plot elements that are echoed across a number of\ngenerations. To quantify this phenomenon, we introduce the Sui Generis score,\nan automatic metric that measures the uniqueness of a plot element among\nalternative storylines generated using the same prompt under an LLM. Evaluating\non 100 short stories, we find that LLM-generated stories often contain\ncombinations of idiosyncratic plot elements echoed frequently across\ngenerations and across different LLMs, while plots from the original\nhuman-written stories are rarely recreated or even echoed in pieces. Moreover,\nour human evaluation shows that the ranking of Sui Generis scores among story\nsegments correlates moderately with human judgment of surprise level, even\nthough score computation is completely automatic without relying on human\njudgment."}
{"id": "2502.03275", "pdf": "https://arxiv.org/pdf/2502.03275.pdf", "abs": "https://arxiv.org/abs/2502.03275", "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning", "authors": ["DiJia Su", "Hanlin Zhu", "Yingchen Xu", "Jiantao Jiao", "Yuandong Tian", "Qinqing Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks."}
{"id": "2502.10708", "pdf": "https://arxiv.org/pdf/2502.10708.pdf", "abs": "https://arxiv.org/abs/2502.10708", "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey", "authors": ["Zirui Song", "Bin Yan", "Yuhan Liu", "Miao Fang", "Mingzhe Li", "Rui Yan", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable success in various\ntasks such as natural language understanding, text summarization, and machine\ntranslation. However, their general-purpose nature often limits their\neffectiveness in domain-specific applications that require specialized\nknowledge, such as healthcare, chemistry, or legal analysis. To address this,\nresearchers have explored diverse methods to enhance LLMs by integrating\ndomain-specific knowledge. In this survey, we provide a comprehensive overview\nof these methods, which we categorize into four key approaches: dynamic\nknowledge injection, static knowledge embedding, modular adapters, and prompt\noptimization. Each approach offers unique mechanisms to equip LLMs with domain\nexpertise, balancing trade-offs between flexibility, scalability, and\nefficiency. We discuss how these methods enable LLMs to tackle specialized\ntasks, compare their advantages and disadvantages, evaluate domain-specific\nLLMs against general LLMs, and highlight the challenges and opportunities in\nthis emerging field. For those interested in delving deeper into this area, we\nalso summarize the commonly used datasets and benchmarks. To keep researchers\nupdated on the latest studies, we maintain an open-source at:\nhttps://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to\ndocumenting research in the field of specialized LLM."}
{"id": "2502.11176", "pdf": "https://arxiv.org/pdf/2502.11176.pdf", "abs": "https://arxiv.org/abs/2502.11176", "title": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning", "authors": ["Tianshi Zheng", "Jiayang Cheng", "Chunyang Li", "Haochen Shi", "Zihao Wang", "Jiaxin Bai", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics."}
{"id": "2502.11419", "pdf": "https://arxiv.org/pdf/2502.11419.pdf", "abs": "https://arxiv.org/abs/2502.11419", "title": "InsBank: Evolving Instruction Subset for Ongoing Alignment", "authors": ["Jiayi Shi", "Yiwei Li", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Huan Ren", "Yao Hu", "Kan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability."}
{"id": "2502.13647", "pdf": "https://arxiv.org/pdf/2502.13647.pdf", "abs": "https://arxiv.org/abs/2502.13647", "title": "Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh", "authors": ["Nurkhan Laiyk", "Daniil Orel", "Rituraj Joshi", "Maiya Goloburda", "Yuxia Wang", "Preslav Nakov", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages."}
{"id": "2502.16534", "pdf": "https://arxiv.org/pdf/2502.16534.pdf", "abs": "https://arxiv.org/abs/2502.16534", "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs", "authors": ["Jonathan Rystrøm", "Hannah Rose Kirk", "Scott Hale"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at OMMM@RANLP2025", "summary": "Large Language Models (LLMs) are becoming increasingly capable across global\nlanguages. However, the ability to communicate across languages does not\nnecessarily translate to appropriate cultural representations. A key concern is\nUS-centric bias, where LLMs reflect US rather than local cultural values. We\npropose a novel methodology that compares LLM-generated response distributions\nagainst population-level opinion data from the World Value Survey across four\nlanguages (Danish, Dutch, English, and Portuguese). Using a rigorous linear\nmixed-effects regression framework, we compare two families of models: Google's\nGemma models (2B--27B parameters) and successive iterations of OpenAI's\nturbo-series. Across the families of models, we find no consistent\nrelationships between language capabilities and cultural alignment. While the\nGemma models have a positive correlation between language capability and\ncultural alignment across languages, the OpenAI models do not. Importantly, we\nfind that self-consistency is a stronger predictor of multicultural alignment\nthan multilingual capabilities. Our results demonstrate that achieving\nmeaningful cultural alignment requires dedicated effort beyond improving\ngeneral language capabilities."}
{"id": "2502.16682", "pdf": "https://arxiv.org/pdf/2502.16682.pdf", "abs": "https://arxiv.org/abs/2502.16682", "title": "Automatic Input Rewriting Improves Translation with Large Language Models", "authors": ["Dayeon Ki", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations."}
{"id": "2502.18848", "pdf": "https://arxiv.org/pdf/2502.18848.pdf", "abs": "https://arxiv.org/abs/2502.18848", "title": "A Causal Lens for Evaluating Faithfulness Metrics", "authors": ["Kerem Zaman", "Shashank Srivastava"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "comment": "25 pages, 22 figures, 9 tables", "summary": "Large Language Models (LLMs) offer natural language explanations as an\nalternative to feature attribution methods for model interpretability. However,\ndespite their plausibility, they may not reflect the model's true reasoning\nfaithfully, which is crucial for understanding the model's true decision-making\nprocesses. Although several faithfulness metrics have been proposed, they are\noften evaluated in isolation, making direct, principled comparisons between\nthem difficult. Here, we present Causal Diagnosticity, a framework that serves\nas a common testbed to evaluate faithfulness metrics for natural language\nexplanations. Our framework employs the concept of diagnosticity, and uses\nmodel-editing methods to generate faithful-unfaithful explanation pairs. Our\nbenchmark includes four tasks: fact-checking, analogy, object counting, and\nmulti-hop reasoning. We evaluate prominent faithfulness metrics, including\npost-hoc explanation and chain-of-thought-based methods. We find that\ndiagnostic performance varies across tasks and models, with Filler Tokens\nperforming best overall. Additionally, continuous metrics are generally more\ndiagnostic than binary ones but can be sensitive to noise and model choice. Our\nresults highlight the need for more robust faithfulness metrics."}
{"id": "2503.00134", "pdf": "https://arxiv.org/pdf/2503.00134.pdf", "abs": "https://arxiv.org/abs/2503.00134", "title": "Personalized Causal Graph Reasoning for LLMs: An Implementation for Dietary Recommendations", "authors": ["Zhongqi Yang", "Amir Rahmani"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at general-purpose reasoning by leveraging\nbroad commonsense knowledge, but they remain limited in tasks requiring\npersonalized reasoning over multifactorial personal data. This limitation\nconstrains their applicability in domains such as healthcare, where decisions\nmust adapt to individual contexts. We introduce Personalized Causal Graph\nReasoning, a framework that enables LLMs to reason over individual-specific\ncausal graphs constructed from longitudinal data. Each graph encodes how\nuser-specific factors influence targeted outcomes. In response to a query, the\nLLM traverses the graph to identify relevant causal pathways, rank them by\nestimated impact, simulate potential outcomes, and generate tailored responses.\nWe implement this framework in the context of nutrient-oriented dietary\nrecommendations, where variability in metabolic responses demands personalized\nreasoning. Using counterfactual evaluation, we assess the effectiveness of\nLLM-generated food suggestions for glucose control. Our method reduces\npostprandial glucose iAUC across three time windows compared to prior\napproaches. Additional LLM-as-a-judge evaluations further confirm improvements\nin personalization quality."}
{"id": "2503.12225", "pdf": "https://arxiv.org/pdf/2503.12225.pdf", "abs": "https://arxiv.org/abs/2503.12225", "title": "Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents", "authors": ["Rinku Dewri"], "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "Minor revision", "summary": "This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking."}
{"id": "2503.12345", "pdf": "https://arxiv.org/pdf/2503.12345.pdf", "abs": "https://arxiv.org/abs/2503.12345", "title": "General Table Question Answering via Answer-Formula Joint Generation", "authors": ["Zhongyuan Wang", "Richong Zhang", "Zhijie Nie", "Hangyu Mao"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperation, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously, which decodes answers and Formulas with a single LLM backbone.\nExtensive experiments demonstrate the versatility and generalization of\n\\texttt{TabAF}. Under the same model size, \\texttt{TabAF} achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact."}
{"id": "2503.12608", "pdf": "https://arxiv.org/pdf/2503.12608.pdf", "abs": "https://arxiv.org/abs/2503.12608", "title": "UniBERT: Adversarial Training for Language-Universal Representations", "authors": ["Andrei-Marius Avram", "Marian Lupaşcu", "Dumitru-Clementin Cercel", "Ionuţ Mironică", "Ştefan Trăuşan-Matu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents UniBERT, a compact multilingual language model that uses\nan innovative training framework that integrates three components: masked\nlanguage modeling, adversarial training, and knowledge distillation.\nPre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,\nUniBERT is designed to reduce the computational demands of large-scale models\nwhile maintaining competitive performance across various natural language\nprocessing tasks. Comprehensive evaluations on four tasks - named entity\nrecognition, natural language inference, question answering, and semantic\ntextual similarity - demonstrate that our multilingual training strategy\nenhanced by an adversarial objective significantly improves cross-lingual\ngeneralization. Specifically, UniBERT models show an average relative\nimprovement of 7.72% over traditional baselines, which achieved an average\nrelative improvement of only 1.17%, and statistical analysis confirms the\nsignificance of these gains (p-value = 0.0181). This work highlights the\nbenefits of combining adversarial training and knowledge distillation to build\nscalable and robust language models, thus advancing the field of multilingual\nand cross-lingual natural language processing."}
{"id": "2503.18172", "pdf": "https://arxiv.org/pdf/2503.18172.pdf", "abs": "https://arxiv.org/abs/2503.18172", "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering", "authors": ["Zixin Chen", "Sicheng Song", "Kashun Shum", "Yanna Lin", "Rui Sheng", "Huamin Qu"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages in total. EMNLP 2025 Main", "summary": "Misleading visualizations, which manipulate chart representations to support\nspecific claims, can distort perception and lead to incorrect conclusions.\nDespite decades of research, they remain a widespread issue-posing risks to\npublic understanding and raising safety concerns for AI systems involved in\ndata-driven communication. While recent multimodal large language models\n(MLLMs) show strong chart comprehension abilities, their capacity to detect and\ninterpret misleading charts remains unexplored. We introduce Misleading ChartQA\nbenchmark, a large-scale multimodal dataset designed to evaluate MLLMs on\nmisleading chart reasoning. It contains 3,026 curated examples spanning 21\nmisleader types and 10 chart types, each with standardized chart code, CSV\ndata, multiple-choice questions, and labeled explanations, validated through\niterative MLLM checks and exhausted expert human review. We benchmark 24\nstate-of-the-art MLLMs, analyze their performance across misleader types and\nchart formats, and propose a novel region-aware reasoning pipeline that\nenhances model accuracy. Our work lays the foundation for developing MLLMs that\nare robust, trustworthy, and aligned with the demands of responsible visual\ncommunication."}
{"id": "2503.23427", "pdf": "https://arxiv.org/pdf/2503.23427.pdf", "abs": "https://arxiv.org/abs/2503.23427", "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker."}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612.pdf", "abs": "https://arxiv.org/abs/2504.03612", "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment."}
{"id": "2504.07612", "pdf": "https://arxiv.org/pdf/2504.07612.pdf", "abs": "https://arxiv.org/abs/2504.07612", "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset", "authors": ["Mihnea-Alexandru Vîrlan", "Răzvan-Alexandru Smădu", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed."}
{"id": "2504.11381", "pdf": "https://arxiv.org/pdf/2504.11381.pdf", "abs": "https://arxiv.org/abs/2504.11381", "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "authors": ["Juan Diego Rodriguez", "Wenxuan Ding", "Katrin Erk", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Although large language models (LLMs) have become more capable and accurate\nacross many tasks, some fundamental sources of unreliability remain in their\nbehavior. One key limitation is their inconsistency at reporting the same\ninformation when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers, i.e., candidate\ncompletions that could possibly arise during ordinary language use without\nbreaking Gricean norms. We show that according to this measure, a large gap\nexists in various settings, including question answering, lexical semantics\ntasks, and next-word prediction. We then propose RankAlign, a ranking-based\ntraining method, and show that it significantly closes the gap, surpassing all\nbaseline methods. Moreover, this approach generalizes well to out-of-domain\ntasks and lexical items."}
{"id": "2504.11582", "pdf": "https://arxiv.org/pdf/2504.11582.pdf", "abs": "https://arxiv.org/abs/2504.11582", "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics."}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673.pdf", "abs": "https://arxiv.org/abs/2504.11673", "title": "Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan", "John Canny"], "categories": ["cs.CL"], "comment": "COLM 2025", "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses to various\nsurveys and polls. However, the questions in these surveys usually reflect\nsocially understood attitudes: the patterns of attitudes of old/young,\nliberal/conservative, as understood by both members and non-members of those\ngroups. It is not clear whether the LLM binding is \\emph{deep}, meaning the LLM\nanswers as a member of a particular in-group would, or \\emph{shallow}, meaning\nthe LLM responds as an out-group member believes an in-group member would. To\nexplore this difference, we use questions that expose known in-group/out-group\nbiases. This level of fidelity is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n\"backstories\" generated as extended, multi-turn interview transcripts. This\napproach is justified by the theory of \\emph{narrative identity} which argues\nthat personality at the highest level is \\emph{constructed} from\nself-narratives. Our generated backstories are longer, rich in detail, and\nconsistent in authentically describing a singular individual, compared to\nprevious methods. We show that virtual personas conditioned on our backstories\nclosely replicate human response distributions (up to an 87% improvement as\nmeasured by Wasserstein Distance) and produce effect sizes that closely match\nthose observed in the original studies of in-group/out-group biases.\nAltogether, our work extends the applicability of LLMs beyond estimating\nsocially understood responses, enabling their use in a broader range of human\nstudies."}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212.pdf", "abs": "https://arxiv.org/abs/2504.14212", "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus."}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540.pdf", "abs": "https://arxiv.org/abs/2504.21540", "title": "Improving Informally Romanized Language Identification", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "categories": ["cs.CL"], "comment": "19 pages, 16 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), the lack of\nconventional spelling in the Latin script results in high spelling variability.\nSuch romanization renders languages that are normally easily distinguished due\nto being written in different scripts - Hindi and Urdu, for example - highly\nconfusable. In this work, we increase language identification (LID) accuracy\nfor romanized text by improving the methods used to synthesize training sets.\nWe find that training on synthetic samples which incorporate natural spelling\nvariation yields higher LID system accuracy than including available naturally\noccurring examples in the training set, or even training higher capacity\nmodels. We demonstrate new state-of-the-art LID performance on romanized text\nfrom 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et\nal., 2023a), improving test F1 from the reported 74.7% (using a pretrained\nneural model) to 85.4% using a linear classifier trained solely on synthetic\ndata and 88.2% when also training on available harvested text."}
{"id": "2505.02666", "pdf": "https://arxiv.org/pdf/2505.02666.pdf", "abs": "https://arxiv.org/abs/2505.02666", "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design", "authors": ["Miaomiao Ji", "Yanqiu Wu", "Zhibin Wu", "Shoujin Wang", "Jian Yang", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Reward design plays a pivotal role in aligning large language models (LLMs)\nwith human values, serving as the bridge between feedback signals and model\noptimization. This survey provides a structured organization of reward modeling\nand addresses three key aspects: mathematical formulation, construction\npractices, and interaction with optimization paradigms. Building on this, it\ndevelops a macro-level taxonomy that characterizes reward mechanisms along\ncomplementary dimensions, thereby offering both conceptual clarity and\npractical guidance for alignment research. The progression of LLM alignment can\nbe understood as a continuous refinement of reward design strategies, with\nrecent developments highlighting paradigm shifts from reinforcement learning\n(RL)-based to RL-free optimization and from single-task to multi-objective and\ncomplex settings."}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277.pdf", "abs": "https://arxiv.org/abs/2505.11277", "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "authors": ["Yaorui Shi", "Sihang Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423.pdf", "abs": "https://arxiv.org/abs/2505.11423", "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259.pdf", "abs": "https://arxiv.org/abs/2505.13259", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590.pdf", "abs": "https://arxiv.org/abs/2505.14590", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."}
{"id": "2505.15695", "pdf": "https://arxiv.org/pdf/2505.15695.pdf", "abs": "https://arxiv.org/abs/2505.15695", "title": "Can Large Language Models be Effective Online Opinion Miners?", "authors": ["Ryang Heo", "Yongsik Seo", "Junseong Lee", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067.pdf", "abs": "https://arxiv.org/abs/2505.17067", "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection."}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117.pdf", "abs": "https://arxiv.org/abs/2505.17117", "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "authors": ["Chen Shani", "Liron Soffer", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."}
{"id": "2505.17137", "pdf": "https://arxiv.org/pdf/2505.17137.pdf", "abs": "https://arxiv.org/abs/2505.17137", "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands", "authors": ["Kristin Qi", "Youxiang Zhu", "Caroline Summerour", "John A. Batsis", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline."}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906.pdf", "abs": "https://arxiv.org/abs/2505.18906", "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems."}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771.pdf", "abs": "https://arxiv.org/abs/2505.22771", "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "authors": ["Christopher Ormerod"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell correction and an encoder-based token-classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers."}
{"id": "2505.24671", "pdf": "https://arxiv.org/pdf/2505.24671.pdf", "abs": "https://arxiv.org/abs/2505.24671", "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Oral)", "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."}
{"id": "2505.24683", "pdf": "https://arxiv.org/pdf/2505.24683.pdf", "abs": "https://arxiv.org/abs/2505.24683", "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (Oral)", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using\n(1) error highlights and (2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through (3) backtranslation and (4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."}
{"id": "2506.01195", "pdf": "https://arxiv.org/pdf/2506.01195.pdf", "abs": "https://arxiv.org/abs/2506.01195", "title": "Strategic Discourse Assessment: The Crooked Path to Innocence", "authors": ["Anshun Asher Zheng", "Junyi Jessy Li", "David I. Beaver"], "categories": ["cs.CL"], "comment": "49 pages. Substantially revised and expanded. Title changed", "summary": "Language is often used strategically, particularly in high-stakes,\nadversarial settings, yet most work on pragmatics and LLMs centers on\ncooperativity. This leaves a gap in the systematic understanding of strategic\ncommunication in adversarial settings. To address this, we introduce SDA\n(Strategic Discourse Assessment), a framework grounded in Gricean and\ngame-theoretic pragmatics to assess strategic use of language. It adapts the ME\nGame jury function to make it empirically estimable for analyzing dialogue. Our\napproach incorporates two key adaptations: a commitment-based taxonomy of\ndiscourse moves, which provides a finer-grained account of strategic effects,\nand the use of estimable proxies grounded in Gricean maxims to operationalize\nabstract constructs such as credibility. Together, these adaptations build on\ndiscourse theory by treating discourse as the strategic management of\ncommitments, enabling systematic evaluation of how conversational moves advance\nor undermine discourse goals. We further derive three interpretable\nmetrics-Benefit at Turn (BAT), Penalty at Turn (PAT), and Normalized Relative\nBenefit at Turn (NRBAT)-to quantify the perceived strategic effects of\ndiscourse moves. We also present CPD (the Crooked Path Dataset), an annotated\ndataset of real courtroom cross-examinations, to demonstrate the framework's\neffectiveness. Using these tools, we evaluate a range of LLMs and show that\nLLMs generally exhibit limited pragmatic understanding of strategic language.\nWhile model size shows an increase in performance on our metrics, reasoning\nability does not help and largely hurts, introducing overcomplication and\ninternal confusion."}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037.pdf", "abs": "https://arxiv.org/abs/2506.02037", "title": "FinS-Pilot: A Benchmark for Online Financial RAG System", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. In the financial field, the stringent demands\nfor professional accuracy and real-time data processing often necessitate the\nuse of retrieval-augmented generation (RAG) techniques. However, the\ndevelopment of financial RAG benchmarks has been constrained by data\nconfidentiality issues and the lack of dynamic data integration. To address\nthis issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG\nsystems in online financial applications. Constructed from real-world financial\nassistant interactions, our benchmark incorporates both real-time API data and\ntext data, organized through an intent classification framework covering\ncritical financial domains. The benchmark enables comprehensive evaluation of\nfinancial assistants' capabilities in handling both static knowledge and\ntime-sensitive market information.Through systematic experiments with multiple\nChinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying\nmodels suitable for financial applications while addressing the current gap in\nspecialized evaluation tools for the financial domain. Our work contributes\nboth a practical evaluation framework and a curated dataset to advance research\nin financial NLP systems. The code and dataset are accessible on GitHub."}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598.pdf", "abs": "https://arxiv.org/abs/2506.03598", "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL."}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078.pdf", "abs": "https://arxiv.org/abs/2506.04078", "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."}
{"id": "2506.10491", "pdf": "https://arxiv.org/pdf/2506.10491.pdf", "abs": "https://arxiv.org/abs/2506.10491", "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models", "authors": ["Aleksandra Sorokovikova", "Pavel Chizhov", "Iuliia Eremenko", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics."}
{"id": "2506.13610", "pdf": "https://arxiv.org/pdf/2506.13610.pdf", "abs": "https://arxiv.org/abs/2506.13610", "title": "A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy", "authors": ["Abdullah Al Shafi", "Rowzatul Zannat", "Abdul Muntakim", "Mahmudul Hasan"], "categories": ["cs.CL"], "comment": "Computational Biology", "summary": "Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value, indicating whether a symptom is\nassociated with a disease. Thereby, this structured representation makes the\ndataset very useful for a wide range of applications, including machine\nlearning-based disease prediction, clinical decision support systems, and\nepidemiological studies. Although there are some advancements in the field of\ndisease-symptom datasets, there is a significant gap in structured datasets for\nthe Bangla language. This dataset aims to bridge that gap by facilitating the\ndevelopment of multilingual medical informatics tools and improving disease\nprediction models for underrepresented linguistic communities. Further\ndevelopments should include region-specific diseases and further fine-tuning of\nsymptom associations for better diagnostic performance"}
{"id": "2506.17671", "pdf": "https://arxiv.org/pdf/2506.17671.pdf", "abs": "https://arxiv.org/abs/2506.17671", "title": "TPTT: Transforming Pretrained Transformers into Titans", "authors": ["Fabien Furfaro"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 2 figure", "summary": "Transformer-based large language models (LLMs) have achieved strong\nperformance across many natural language processing tasks. Nonetheless, their\nquadratic computational and memory requirements, particularly in self-attention\nlayers, pose challenges for efficient inference on long contexts and for\ndeployment in resource-limited environments. We present TPTT (Transforming\nPretrained Transformers into Titans), a framework designed to augment\npretrained Transformers with linearized attention (LiZA) and internal memory\ngating via Memory as Gate (MaG), applied without full retraining. TPTT supports\nparameter-efficient fine-tuning (LoRA) and integrates with standard toolkits\nsuch as Hugging Face Transformers. We evaluated TPTT on several pretrained\nmodels, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m,\nOpenELM-1.3B, and Mistral-7B, in order to assess applicability across\narchitectures of different scales.Experiments on models with approximately 1\nbillion parameters, evaluated primarily on the MMLU benchmark, suggest\npotential improvements in both efficiency and accuracy compared to baseline\nmodels. For example, Titans-Llama-1B exhibited up to a 20\\% relative increase\nin Exact Match scores in one-shot evaluation. An additional finding is that it\nis possible to convert a quadratic-attention model into a purely\nlinear-attention model using the DeltaProduct mechanism. All training runs were\ncarried out with modest computational resources.These preliminary findings\nindicate that TPTT may help adapt pretrained LLMs for long-context tasks with\nlimited overhead. Further studies on larger models and a broader set of\nbenchmarks will be necessary to evaluate the generality and robustness of the\nframework. Code is available at https://github.com/fabienfrfr/tptt . Python\npackage at https://pypi.org/project/tptt/ ."}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137.pdf", "abs": "https://arxiv.org/abs/2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun", "Andi Zhang", "Yanbiao Ma", "Xiaoshuai Hao"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Knowledge graph completion demands effective modeling of multifaceted\nsemantic relationships between entities. Yet, prevailing methods, which rely on\nstatic scoring functions over learned embeddings, struggling to simultaneously\ncapture rich semantic context and the dynamic nature of relations. To overcome\nthis limitation, we propose the Flow-Modulated Scoring (FMS) framework,\nconceptualizing a relation as a dynamic evolutionary process governed by its\nstatic semantic environment. FMS operates in two stages: it first learns\ncontext-aware entity embeddings via a Semantic Context Learning module, and\nthen models a dynamic flow between them using a Conditional Flow-Matching\nmodule. This learned flow dynamically modulates a base static score for the\nentity pair. By unifying context-rich static representations with a conditioned\ndynamic flow, FMS achieves a more comprehensive understanding of relational\nsemantics. Extensive experiments demonstrate that FMS establishes a new state\nof the art across both canonical knowledge graph completion tasks: relation\nprediction and entity prediction. On the standard relation prediction benchmark\nFB15k-237, FMS achieves a near-perfect MRR of 99.8\\% and Hits@1 of 99.7\\% using\na mere 0.35M parameters, while also attaining a 99.9\\% MRR on WN18RR. Its\ndominance extends to entity prediction, where it secures a 25.2\\% relative MRR\ngain in the transductive setting and substantially outperforms all baselines in\nchallenging inductive settings. By unifying a dynamic flow mechanism with rich\nstatic contexts, FMS offers a highly effective and parameter-efficient new\nparadigm for knowledge graph completion. Code published at:\nhttps://github.com/yuanwuyuan9/FMS."}
{"id": "2507.03152", "pdf": "https://arxiv.org/pdf/2507.03152.pdf", "abs": "https://arxiv.org/abs/2507.03152", "title": "MedVAL: Toward Expert-Level Medical Text Validation with Language Models", "authors": ["Asad Aali", "Vasiliki Bikia", "Maya Varma", "Nicole Chiou", "Sophie Ostmeier", "Arnav Singhvi", "Magdalini Paschali", "Ashwin Kumar", "Andrew Johnston", "Karimar Amador-Martinez", "Eduardo Juan Perez Guerrero", "Paola Naovi Cruz Rivera", "Sergios Gatidis", "Christian Bluethgen", "Eduardo Pontes Reis", "Eddy D. Zandee van Rilland", "Poonam Laxmappa Hosamani", "Kevin R Keet", "Minjoung Go", "Evelyn Ling", "David B. Larson", "Curtis Langlotz", "Roxana Daneshjou", "Jason Hom", "Sanmi Koyejo", "Emily Alsentzer", "Akshay S. Chaudhari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text."}
{"id": "2507.05707", "pdf": "https://arxiv.org/pdf/2507.05707.pdf", "abs": "https://arxiv.org/abs/2507.05707", "title": "Agentic-R1: Distilled Dual-Strategy Reasoning", "authors": ["Weihua Du", "Pranjal Aggarwal", "Sean Welleck", "Yiming Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025. 15 pages. Project available at\n  https://github.com/StigLidu/DualDistill", "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill"}
{"id": "2507.08325", "pdf": "https://arxiv.org/pdf/2507.08325.pdf", "abs": "https://arxiv.org/abs/2507.08325", "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation", "authors": ["Yinzhu Quan", "Xinrui Li", "Ying Chen"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics."}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509.pdf", "abs": "https://arxiv.org/abs/2507.09509", "title": "How Important is `Perfect' English for Machine Translation Prompts?", "authors": ["Patrícia Schmidtová", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina Hämmerl", "Vilém Zouhar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans."}
{"id": "2507.11862", "pdf": "https://arxiv.org/pdf/2507.11862.pdf", "abs": "https://arxiv.org/abs/2507.11862", "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "categories": ["cs.CL"], "comment": "To fix the performance summary statistics in Section 4", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains."}
{"id": "2507.13870", "pdf": "https://arxiv.org/pdf/2507.13870.pdf", "abs": "https://arxiv.org/abs/2507.13870", "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "categories": ["cs.CL"], "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER."}
{"id": "2507.13966", "pdf": "https://arxiv.org/pdf/2507.13966.pdf", "abs": "https://arxiv.org/abs/2507.13966", "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents."}
{"id": "2507.15114", "pdf": "https://arxiv.org/pdf/2507.15114.pdf", "abs": "https://arxiv.org/abs/2507.15114", "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI", "authors": ["Chathuri Jayaweera", "Bonnie J. Dorr"], "categories": ["cs.CL"], "comment": "Accepted at Workshop on Perspectivist Approaches to NLP\n  (NLPerspectives), EMNLP 2025, 10 pages, 7 figures", "summary": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful variation,\nespecially when triggered by ambiguity in the premise or hypothesis. While\nunderspecified guidelines and annotator behavior contribute to variation,\ncontent-based ambiguity provides a process-independent signal of divergent\nhuman perspectives. We call for a shift toward ambiguity-aware NLI that first\nidentifies ambiguous input pairs, classifies their types, and only then\nproceeds to inference. To support this shift, we present a framework that\nincorporates ambiguity detection and classification prior to inference. We also\nintroduce a unified taxonomy that synthesizes existing taxonomies, illustrates\nkey subtypes with examples, and motivates targeted detection methods that\nbetter align models with human interpretation. Although current resources lack\ndatasets explicitly annotated for ambiguity and subtypes, this gap presents an\nopportunity: by developing new annotated resources and exploring unsupervised\napproaches to ambiguity detection, we enable more robust, explainable, and\nhuman-aligned NLI systems."}
{"id": "2507.16217", "pdf": "https://arxiv.org/pdf/2507.16217.pdf", "abs": "https://arxiv.org/abs/2507.16217", "title": "Towards Compute-Optimal Many-Shot In-Context Learning", "authors": ["Shahriar Golchin", "Yanfei Chen", "Rujun Han", "Manan Gandhi", "Tianli Yu", "Swaroop Mishra", "Mihai Surdeanu", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Final version; accepted at COLM 2025", "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."}
{"id": "2507.21509", "pdf": "https://arxiv.org/pdf/2507.21509.pdf", "abs": "https://arxiv.org/abs/2507.21509", "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language Models", "authors": ["Runjin Chen", "Andy Arditi", "Henry Sleight", "Owain Evans", "Jack Lindsey"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description."}
{"id": "2508.07279", "pdf": "https://arxiv.org/pdf/2508.07279.pdf", "abs": "https://arxiv.org/abs/2508.07279", "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows."}
{"id": "2508.08876", "pdf": "https://arxiv.org/pdf/2508.08876.pdf", "abs": "https://arxiv.org/abs/2508.08876", "title": "Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance", "authors": ["Kaiyu Wang", "Lin Mu", "Zhiyao Yang", "Ximing Li", "Xiaotang Zhou Wanfu Gao", "Huimao Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CIKM 2025. 11 pages, 7 figures", "summary": "Quality Assurance (QA) for radiology reports refers to judging whether the\njunior reports (written by junior doctors) are qualified. The QA scores of one\njunior report are given by the senior doctor(s) after reviewing the image and\njunior report. This process requires intensive labor costs for senior doctors.\nAdditionally, the QA scores may be inaccurate for reasons like diagnosis bias,\nthe ability of senior doctors, and so on. To address this issue, we propose a\nSpan-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores\nautomatically. Unlike the common document-level semantic comparison method, we\ntry to analyze the semantic difference by exploring more fine-grained text\nspans. Specifically, Sqator measures QA scores by measuring the importance of\nrevised spans between junior and senior reports, and outputs the final QA\nscores by merging all revised span scores. We evaluate Sqator using a\ncollection of 12,013 radiology reports. Experimental results show that Sqator\ncan achieve competitive QA scores. Moreover, the importance scores of revised\nspans can be also consistent with the judgments of senior doctors."}
{"id": "2508.14146", "pdf": "https://arxiv.org/pdf/2508.14146.pdf", "abs": "https://arxiv.org/abs/2508.14146", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems."}
{"id": "2508.14444", "pdf": "https://arxiv.org/pdf/2508.14444.pdf", "abs": "https://arxiv.org/abs/2508.14444", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Alexander Bukharin", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haifeng Qian", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jian Zhang", "Jiaqi Zeng", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Tugrul Konuk", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face."}
{"id": "2508.14880", "pdf": "https://arxiv.org/pdf/2508.14880.pdf", "abs": "https://arxiv.org/abs/2508.14880", "title": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "authors": ["Ailing Yu", "Lan Yao", "Jingnan Liu", "Zhe Chen", "Jiajun Yin", "Yuan Wang", "Xinhao Liao", "Zhiling Ye", "Ji Li", "Yun Yue", "Hansong Xiao", "Hualei Zhou", "Chunxiao Guo", "Peng Wei", "Junwei Liu", "Jinjie Gu"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."}
{"id": "2508.15044", "pdf": "https://arxiv.org/pdf/2508.15044.pdf", "abs": "https://arxiv.org/abs/2508.15044", "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "authors": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."}
{"id": "2508.15212", "pdf": "https://arxiv.org/pdf/2508.15212.pdf", "abs": "https://arxiv.org/abs/2508.15212", "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning", "authors": ["Huanxuan Liao", "Yixing Xu", "Shizhu He", "Guanchen Li", "Xuanwu Yin", "Dong Li", "Emad Barsoum", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."}
{"id": "2508.15825", "pdf": "https://arxiv.org/pdf/2508.15825.pdf", "abs": "https://arxiv.org/abs/2508.15825", "title": "Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features", "authors": ["Chenghao Liu", "Aniket Mahanti", "Ranesh Naha", "Guanghao Wang", "Erwann Sbai"], "categories": ["cs.CL", "q-fin.ST"], "comment": null, "summary": "As cryptocurrencies gain popularity, the digital asset marketplace becomes\nincreasingly significant. Understanding social media signals offers valuable\ninsights into investor sentiment and market dynamics. Prior research has\npredominantly focused on text-based platforms such as Twitter. However, video\ncontent remains underexplored, despite potentially containing richer emotional\nand contextual sentiment that is not fully captured by text alone. In this\nstudy, we present a multimodal analysis comparing TikTok and Twitter sentiment,\nusing large language models to extract insights from both video and text data.\nWe investigate the dynamic dependencies and spillover effects between social\nmedia sentiment and cryptocurrency market indicators. Our results reveal that\nTikTok's video-based sentiment significantly influences speculative assets and\nshort-term market trends, while Twitter's text-based sentiment aligns more\nclosely with long-term dynamics. Notably, the integration of cross-platform\nsentiment signals improves forecasting accuracy by up to 20%."}
{"id": "2508.16889", "pdf": "https://arxiv.org/pdf/2508.16889.pdf", "abs": "https://arxiv.org/abs/2508.16889", "title": "ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks", "authors": ["Hyunjun Kim", "Junwoo Ha", "Sangyoon Yu", "Haon Park"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge (LLMaaJ) now underpins scalable evaluation, yet we lack a\ndecisive test of a judge's qualification: can it recover a conversation's\nlatent objective and know when that inference is trustworthy? LLMs degrade\nunder irrelevant or long context; multi-turn jailbreaks further hide goals\nacross turns. We introduce ObjexMT, a benchmark for objective extraction and\nmetacognition. Given a multi-turn transcript, a model must return a\none-sentence base objective and a self-reported confidence. Accuracy is\ncomputed via LLM-judge semantic similarity to gold objectives, converted to\nbinary correctness by a single human-aligned threshold calibrated once on N =\n100 items ($\\tau^*=0.61$). Metacognition is evaluated with ECE, Brier,\nWrong-at-High-Conf, and risk-coverage. Across gpt-4.1, claude-sonnet-4, and\nQwen3-235B-A22B-FP8 on SafeMTData_Attack600, SafeMTData_1K, MHJ, and CoSafe,\nclaude-sonnet-4 attains the best objective-extraction accuracy (0.515) and\ncalibration (ECE 0.296; Brier 0.324); gpt-4.1 and Qwen3-235B-A22B-FP8 tie at\n0.441 but are overconfident (mean confidence $\\approx$0.88 vs. accuracy\n$\\approx$0.44; Wrong-at-0.90 $\\approx$48-52%). Performance varies by dataset\n($\\approx$0.167-0.865). ObjexMT thus supplies an actionable test for LLM\njudges: when objectives are not explicit, judges often misinfer them with high\nconfidence. We recommend exposing objectives when feasible and gating decisions\nby confidence otherwise. Code and data at\nhttps://github.com/hyunjun1121/ObjexMT_dataset."}
{"id": "2508.17690", "pdf": "https://arxiv.org/pdf/2508.17690.pdf", "abs": "https://arxiv.org/abs/2508.17690", "title": "Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks", "authors": ["Danny Wang", "Ruihong Qiu", "Guangdong Bai", "Zi Huang"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP2025 Main", "summary": "Out-of-distribution (OOD) detection remains challenging in text-rich\nnetworks, where textual features intertwine with topological structures.\nExisting methods primarily address label shifts or rudimentary domain-based\nsplits, overlooking the intricate textual-structural diversity. For example, in\nsocial networks, where users represent nodes with textual features (name, bio)\nwhile edges indicate friendship status, OOD may stem from the distinct language\npatterns between bot and normal users. To address this gap, we introduce the\nTextTopoOOD framework for evaluating detection across diverse OOD scenarios:\n(1) attribute-level shifts via text augmentations and embedding perturbations;\n(2) structural shifts through edge rewiring and semantic connections; (3)\nthematically-guided label shifts; and (4) domain-based divisions. Furthermore,\nwe propose TNT-OOD to model the complex interplay between Text aNd Topology\nusing: 1) a novel cross-attention module to fuse local structure into\nnode-level text representations, and 2) a HyperNetwork to generate\nnode-specific transformation parameters. This aligns topological and semantic\nfeatures of ID nodes, enhancing ID/OOD distinction across structural and\ntextual shifts. Experiments on 11 datasets across four OOD scenarios\ndemonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection\nin text-rich networks."}
{"id": "2508.17796", "pdf": "https://arxiv.org/pdf/2508.17796.pdf", "abs": "https://arxiv.org/abs/2508.17796", "title": "Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation", "authors": ["Changsong Liu", "Yizhou Peng", "Eng Siong Chng"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to APSIPA ASC 2025", "summary": "Contextual automatic speech recognition (ASR) systems allow for recognizing\nout-of-vocabulary (OOV) words, such as named entities or rare words. However,\nit remains challenging due to limited training data and ambiguous or\ninconsistent pronunciations. In this paper, we propose a synthesis-driven\nmulti-pronunciation contextual biasing method that performs zero-shot\ncontextual ASR on a pretrained Whisper model. Specifically, we leverage\ntext-to-speech (TTS) systems to synthesize diverse speech samples containing\neach target rare word, and then use the pretrained Whisper model to extract\nmultiple predicted pronunciation variants. These variant token sequences are\ncompiled into a prefix-trie, which assigns rewards to beam hypotheses in a\nshallow-fusion manner during beam-search decoding. Subsequently, any recognized\nvariant is mapped back to the original rare word in the final transcription.\nThe evaluation results on the LibriSpeech dataset show that our method reduces\nbiased-word error rate (B-WER) by 43% on test-clean and 44% on test-other while\nmaintaining unbiased-WER (U-WER) essentially unchanged."}
{"id": "2508.18916", "pdf": "https://arxiv.org/pdf/2508.18916.pdf", "abs": "https://arxiv.org/abs/2508.18916", "title": "Affective Polarization across European Parliaments", "authors": ["Bojan Evkoski", "Igor Mozetič", "Nikola Ljubešić", "Petra Kralj Novak"], "categories": ["cs.CL", "cs.SI"], "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments."}
{"id": "2508.19720", "pdf": "https://arxiv.org/pdf/2508.19720.pdf", "abs": "https://arxiv.org/abs/2508.19720", "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "categories": ["cs.CL"], "comment": "emnlp 2025", "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS."}
{"id": "2508.20931", "pdf": "https://arxiv.org/pdf/2508.20931.pdf", "abs": "https://arxiv.org/abs/2508.20931", "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench", "authors": ["Venkatesh Mishra", "Amir Saeidi", "Satyam Raj", "Mutsumi Nakamura", "Jayanth Srinivasa", "Gaowen Liu", "Ali Payani", "Chitta Baral"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments."}
{"id": "2508.21049", "pdf": "https://arxiv.org/pdf/2508.21049.pdf", "abs": "https://arxiv.org/abs/2508.21049", "title": "Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm", "authors": ["Ramazan Ali Bahrami", "Ramin Yahyapour"], "categories": ["cs.CL"], "comment": "Presented in 8th International Conference on Natural Language and\n  Speech Processing (ICNLSP), 25-27 August 2025, SDU, Odense, Denmark", "summary": "Sentential relation extraction (RE) is an important task in natural language\nprocessing (NLP). In this paper we propose to do sentential RE with dynamic\nrouting in capsules. We first show that the proposed approach outperform state\nof the art on common sentential relation extraction datasets Tacred, Tacredrev,\nRetacred, and Conll04. We then investigate potential reasons for its good\nperformance on the mentioned datasets, and yet low performance on another\nsimilar, yet larger sentential RE dataset, Wikidata. As such, we identify noise\nin Wikidata labels as one of the reasons that can hinder performance.\nAdditionally, we show associativity of better performance with better\nre-representation, a term from neuroscience referred to change of\nrepresentation in human brain to improve the match at comparison time. As\nexample, in the given analogous terms King:Queen::Man:Woman, at comparison\ntime, and as a result of re-representation, the similarity between related head\nterms (King,Man), and tail terms (Queen,Woman) increases. As such, our\nobservation show that our proposed model can do re-representation better than\nthe vanilla model compared with. To that end, beside noise in the labels of the\ndistantly supervised RE datasets, we propose re-representation as a challenge\nin sentential RE."}
{"id": "2508.21051", "pdf": "https://arxiv.org/pdf/2508.21051.pdf", "abs": "https://arxiv.org/abs/2508.21051", "title": "Language Models and Logic Programs for Trustworthy Financial Reasoning", "authors": ["William Jurayj", "Nils Holzenberger", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance."}
{"id": "2508.21164", "pdf": "https://arxiv.org/pdf/2508.21164.pdf", "abs": "https://arxiv.org/abs/2508.21164", "title": "Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations", "authors": ["Muskan Saraf", "Sajjad Rezvani Boroujeni", "Justin Beaudry", "Hossein Abedi", "Tom Bush"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to evaluate outputs, yet\ntheir judgments may be influenced. This study examines bias in self- and\ncross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:\nno labels, true labels, and two false-label scenarios. Blog posts authored by\neach model were evaluated by all three using both overall preference voting and\nquality ratings for Coherence, Informativeness, and Conciseness, with all\nscores expressed as percentages for direct comparison. Results reveal striking\nasymmetries: the \"Claude\" label consistently boosts scores, while the \"Gemini\"\nlabel consistently depresses them, regardless of actual content. False labels\nfrequently reversed rankings, producing shifts of up to 50 percentage points in\npreference votes and up to 12 percentage points in converted quality ratings.\nGemini's self-scores collapsed under true labels, while Claude's\nself-preference intensified. These findings show that perceived model identity\ncan heavily distort high-level judgments and subtly influence detailed quality\nratings, underscoring the need for blind or multimodel evaluation protocols to\nensure fairness in LLM benchmarking."}
{"id": "2002.03531", "pdf": "https://arxiv.org/pdf/2002.03531.pdf", "abs": "https://arxiv.org/abs/2002.03531", "title": "A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly Articles", "authors": ["Khalid M. Saqr"], "categories": ["cs.AI", "cs.CL", "68T50, 68T30, 68Q70, 91D30, 62P25", "H.3.1; I.2.4; I.2.7"], "comment": null, "summary": "Despite rapid gains in scale, research evaluation still relies on opaque,\nlagging proxies. To serve the scientific community, we pursue transparency:\nreproducible, auditable epistemic classification useful for funding and policy.\nHere we formalize KGX3 as a scenario-based model for mapping Kuhnian stages\nfrom research papers, prove determinism of the classification pipeline, and\ndefine the epistemic manifold that yields paradigm maps. We report validation\nacross recent corpora, operational complexity at global scale, and governance\nthat preserves interpretability while protecting core IP. The system delivers\nearly, actionable signals of drift, crisis, and shift unavailable to citation\nmetrics or citations-anchored NLP. KGX3 is the latest iteration of a\ndeterministic epistemic engine developed since 2019, originating as Soph.io\n(2020), advanced as iKuhn (2024), and field-tested through Preprint Watch in\n2025."}
{"id": "2211.09623", "pdf": "https://arxiv.org/pdf/2211.09623.pdf", "abs": "https://arxiv.org/abs/2211.09623", "title": "Cross-Modal Adapter for Vision-Language Retrieval", "authors": ["Haojun Jiang", "Jianke Zhang", "Rui Huang", "Chunjiang Ge", "Zanlin Ni", "Shiji Song", "Gao Huang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "The accepted manuscript by Pattern Recognition 25 Journal. The\n  published journal article is available at:\n  https://doi.org/10.1016/j.patcog.2024.111144", "summary": "Vision-language retrieval is an important multi-modal learning topic, where\nthe goal is to retrieve the most relevant visual candidate for a given text\nquery. Recently, pre-trained models, e.g., CLIP, show great potential on\nretrieval tasks. However, as pre-trained models are scaling up, fully\nfine-tuning them on donwstream retrieval datasets has a high risk of\noverfitting. Moreover, in practice, it would be costly to train and store a\nlarge model for each task. To overcome the above issues, we present a novel\nCross-Modal Adapter for parameter-efficient transfer learning. Inspired by\nadapter-based methods, we adjust the pre-trained model with a few\nparameterization layers. However, there are two notable differences. First, our\nmethod is designed for the multi-modal domain. Secondly, it allows\nencoder-level implicit cross-modal interactions between vision and language\nencoders. Although surprisingly simple, our approach has three notable\nbenefits: (1) reduces the vast majority of fine-tuned parameters, (2) saves\ntraining time, and (3) allows all the pre-trained parameters to be fixed,\nenabling the pre-trained model to be shared across datasets. Extensive\nexperiments demonstrate that, without bells and whistles, our approach\noutperforms adapter-based methods on image-text retrieval datasets (MSCOCO,\nFlickr30K) and video-text retrieval datasets (MSR-VTT, DiDeMo, and\nActivityNet)."}
{"id": "2312.10448", "pdf": "https://arxiv.org/pdf/2312.10448.pdf", "abs": "https://arxiv.org/abs/2312.10448", "title": "Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing", "authors": ["Xueying Du", "Mingwei Liu", "Hanlin Wang", "Juntao Li", "Xin Peng", "Yiling Lou"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Software crash bugs cause unexpected program behaviors or even abrupt\ntermination, thus demanding immediate resolution. However, resolving crash bugs\ncan be challenging due to their complex root causes, which can originate from\nissues in the source code or external factors like third-party library\ndependencies. Large language models (LLMs) have shown promise in software\nengineering tasks. However, existing research predominantly focuses on the\ncapability of LLMs to localize and repair code-related crash bugs, leaving\ntheir effectiveness in resolving environment-related crash bugs in real-world\nsoftware unexplored. To fill this gap, we conducted the first comprehensive\nstudy to assess the capability of LLMs in resolving real-world\nenvironment-related crash bugs. We first systematically compare LLMs'\nperformance in resolving code-related and environment-related crash bugs with\nvarying levels of crash contextual information. Our findings reveal that\nlocalization is the primary challenge for resolving code-related crashes, while\nrepair poses a greater challenge for environment-related crashes. Furthermore,\nwe investigate the impact of different prompt strategies on improving the\nresolution of environment-related crash bugs, incorporating different prompt\ntemplates and multi-round interactions. Building on this, we further explore an\nadvanced active inquiry prompting strategy leveraging the self-planning\ncapabilities of LLMs. Based on these explorations, we propose IntDiagSolver, an\ninteractive methodology designed to enable precise crash bug resolution through\nongoing engagement with LLMs. Extensive evaluations of IntDiagSolver across\nmultiple LLMs (including GPT-3.5, GPT-4, Claude, CodeLlama, DeepSeek-R1, and\nQwen-3-Coder) demonstrate consistent improvements in resolution accuracy, with\nsubstantial enhancements ranging from 9.1% to 43.3% in localization and 9.1% to\n53.3% in repair."}
{"id": "2404.18311", "pdf": "https://arxiv.org/pdf/2404.18311.pdf", "abs": "https://arxiv.org/abs/2404.18311", "title": "Towards Incremental Learning in Large Language Models: A Critical Review", "authors": ["Mladjan Jovanovic", "Peter Voss"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Incremental learning is the ability of systems to acquire knowledge over\ntime, enabling their adaptation and generalization to novel tasks. It is a\ncritical ability for intelligent, real-world systems, especially when data\nchanges frequently or is limited. This review provides a comprehensive analysis\nof incremental learning in Large Language Models. It synthesizes the\nstate-of-the-art incremental learning paradigms, including continual learning,\nmeta-learning, parameter-efficient learning, and mixture-of-experts learning.\nWe demonstrate their utility for incremental learning by describing specific\nachievements from these related topics and their critical factors. An important\nfinding is that many of these approaches do not update the core model, and none\nof them update incrementally in real-time. The paper highlights current\nproblems and challenges for future research in the field. By consolidating the\nlatest relevant research developments, this review offers a comprehensive\nunderstanding of incremental learning and its implications for designing and\ndeveloping LLM-based learning systems."}
{"id": "2405.14093", "pdf": "https://arxiv.org/pdf/2405.14093.pdf", "abs": "https://arxiv.org/abs/2405.14093", "title": "A Survey on Vision-Language-Action Models for Embodied AI", "authors": ["Yueen Ma", "Zixing Song", "Yuzheng Zhuang", "Jianye Hao", "Irwin King"], "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Project page: https://github.com/yueen-ma/Awesome-VLA", "summary": "Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI. We have created a\nproject associated with this survey, which is available at\nhttps://github.com/yueen-ma/Awesome-VLA."}
{"id": "2405.14314", "pdf": "https://arxiv.org/pdf/2405.14314.pdf", "abs": "https://arxiv.org/abs/2405.14314", "title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "authors": ["Yang Zhang", "Shixin Yang", "Chenjia Bai", "Fei Wu", "Xiu Li", "Zhen Wang", "Xuelong Li"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "comment": "accepted by ACL'2025", "summary": "Grounding the reasoning ability of large language models (LLMs) for embodied\ntasks is challenging due to the complexity of the physical world. Especially,\nLLM planning for multi-agent collaboration requires communication of agents or\ncredit assignment as the feedback to re-adjust the proposed plans and achieve\neffective coordination. However, existing methods that overly rely on physical\nverification or self-reflection suffer from excessive and inefficient querying\nof LLMs. In this paper, we propose a novel framework for multi-agent\ncollaboration that introduces Reinforced Advantage feedback (ReAd) for\nefficient self-refinement of plans. Specifically, we perform critic regression\nto learn a sequential advantage function from LLM-planned data, and then treat\nthe LLM planner as an optimizer to generate actions that maximize the advantage\nfunction. It endows the LLM with the foresight to discern whether the action\ncontributes to accomplishing the final task. We provide theoretical analysis by\nextending advantage-weighted regression in reinforcement learning to\nmulti-agent systems. Experiments on Overcooked-AI and a difficult variant of\nRoCoBench show that ReAd surpasses baselines in success rate, and also\nsignificantly decreases the interaction steps of agents and query rounds of\nLLMs, demonstrating its high efficiency for grounding LLMs. More results are\ngiven at https://read-llm.github.io."}
{"id": "2406.07268", "pdf": "https://arxiv.org/pdf/2406.07268.pdf", "abs": "https://arxiv.org/abs/2406.07268", "title": "Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation", "authors": ["Jinyuan Li", "Ziyan Li", "Han Li", "Jianfei Yu", "Rui Xia", "Di Sun", "Gang Pan"], "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": "Extension of our Findings of EMNLP 2023 & ACL 2024 paper, IEEE\n  Transactions on Multimedia accepted on July 19, 2025", "summary": "Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks."}
{"id": "2407.01085", "pdf": "https://arxiv.org/pdf/2407.01085.pdf", "abs": "https://arxiv.org/abs/2407.01085", "title": "Explaining Length Bias in LLM-Based Preference Evaluations", "authors": ["Zhengyu Hu", "Linxin Song", "Jieyu Zhang", "Zheyuan Xiao", "Tianfu Wang", "Zhengyu Chen", "Nicholas Jing Yuan", "Jianxun Lian", "Kaize Ding", "Hui Xiong"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The use of large language models (LLMs) as judges, particularly in preference\ncomparisons, has become widespread, but this reveals a notable bias towards\nlonger responses, undermining the reliability of such evaluations. To better\nunderstand such bias, we propose to decompose the preference evaluation metric,\nspecifically the win rate, into two key components: desirability and\ninformation mass, where the former is length-independent and related to\ntrustworthiness such as correctness, toxicity, and consistency, and the latter\nis length-dependent and represents the amount of information in the response.\nWe empirically demonstrated the decomposition through controlled experiments\nand found that response length impacts evaluations by influencing information\nmass. To derive a reliable evaluation metric that assesses content quality\nwithout being confounded by response length, we propose AdapAlpaca, a simple\nyet effective adjustment to win rate measurement. Specifically, AdapAlpaca\nensures a fair comparison of response quality by aligning the lengths of\nreference and test model responses under equivalent length intervals."}
{"id": "2407.04620", "pdf": "https://arxiv.org/pdf/2407.04620.pdf", "abs": "https://arxiv.org/abs/2407.04620", "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States", "authors": ["Yu Sun", "Xinhao Li", "Karan Dalal", "Jiarui Xu", "Arjun Vikram", "Genghan Zhang", "Yann Dubois", "Xinlei Chen", "Xiaolong Wang", "Sanmi Koyejo", "Tatsunori Hashimoto", "Carlos Guestrin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The current version contains updates on related work and limitations.\n  All experiments were completed in the first version", "summary": "Self-attention performs well in long context but has quadratic complexity.\nExisting RNN layers have linear complexity, but their performance in long\ncontext is limited by the expressive power of their hidden states. We present a\npractical framework for instantiating sequence modeling layers with linear\ncomplexity and expressive hidden states. The key idea is to make the hidden\nstate a machine learning model itself, and the update rule a step of\nself-supervised learning. Since the hidden state is updated by training even on\ntest sequences, our layers are called Test-Time Training (TTT) layers. We\nconsider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a\nlinear model and a two-layer MLP respectively. We evaluate our instantiations\nat the scale of 125M to 1.3B parameters, comparing with a strong Transformer\nand Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can\nkeep reducing perplexity by conditioning on more tokens, while Mamba cannot\nafter 16k context. TTT-MLP still faces challenges in memory I/O, but shows\nlarger potential in long context, pointing to a promising direction for future\nresearch."}
{"id": "2407.20271", "pdf": "https://arxiv.org/pdf/2407.20271.pdf", "abs": "https://arxiv.org/abs/2407.20271", "title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "authors": ["Haoyu Tang", "Ye Liu", "Xi Zhao", "Xukai Liu", "Yanghai Zhang", "Kai Zhang", "Xiaofang Zhou", "Enhong Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in machine learning, particularly in Natural Language\nProcessing (NLP), have produced powerful models trained on vast datasets.\nHowever, these models risk leaking sensitive information, raising privacy\nconcerns. In response, regulatory measures such as the European Union's General\nData Protection Regulation (GDPR) have driven increasing interest in Machine\nUnlearning techniques, which enable models to selectively forget specific data\nentries. Early unlearning approaches primarily relied on pre-processing\nmethods, while more recent research has shifted towards training-based\nsolutions. Despite their effectiveness, a key limitation persists: most methods\nrequire access to original training data, which is often unavailable.\nAdditionally, directly applying unlearning techniques bears the cost of\nundermining the model's expressive capabilities. To address these challenges,\nwe introduce the Iterative Contrastive Unlearning (ICU) framework, which\nconsists of three core components: A Knowledge Unlearning Induction module\ndesigned to target specific knowledge for removal using an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically adjusts the unlearning process through\nongoing evaluation and updates. Experimental results demonstrate the efficacy\nof our ICU method in unlearning sensitive information while maintaining the\nmodel's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications."}
{"id": "2408.11727", "pdf": "https://arxiv.org/pdf/2408.11727.pdf", "abs": "https://arxiv.org/abs/2408.11727", "title": "Efficient Detection of Toxic Prompts in Large Language Models", "authors": ["Yi Liu", "Junzhe Yu", "Huijia Sun", "Ling Shi", "Gelei Deng", "Yuqi Chen", "Yang Liu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)", "summary": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs."}
{"id": "2408.13442", "pdf": "https://arxiv.org/pdf/2408.13442.pdf", "abs": "https://arxiv.org/abs/2408.13442", "title": "A Law of Next-Token Prediction in Large Language Models", "authors": ["Hangfeng He", "Weijie J. Su"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Transferred for publication to Physical Review E from Physical Review\n  Research (to waive publication charges)", "summary": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation."}
{"id": "2410.02730", "pdf": "https://arxiv.org/pdf/2410.02730.pdf", "abs": "https://arxiv.org/abs/2410.02730", "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes", "authors": ["Zhaowei Wang", "Hongming Zhang", "Tianqing Fang", "Ye Tian", "Yue Yang", "Kaixin Ma", "Xiaoman Pan", "Yangqiu Song", "Dong Yu"], "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "EMNLP 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates."}
{"id": "2411.02708", "pdf": "https://arxiv.org/pdf/2411.02708.pdf", "abs": "https://arxiv.org/abs/2411.02708", "title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios", "authors": ["Yunkai Dang", "Mengxi Gao", "Yibo Yan", "Xin Zou", "Yanggan Gu", "Jungang Li", "Jingyu Wang", "Peijie Jiang", "Aiwei Liu", "Jia Liu", "Xuming Hu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved\nstate-of-the-art performance on tasks ranging from visual question answering to\nvideo understanding. However, existing studies have concentrated mainly on\nvisual-textual misalignment, leaving largely unexplored the MLLMs' ability to\npreserve an originally correct answer when confronted with misleading\ninformation. We reveal a response uncertainty phenomenon: across nine standard\ndatasets, twelve state-of-the-art open-source MLLMs overturn a previously\ncorrect answer in 65% of cases after receiving a single deceptive cue. To\nsystematically quantify this vulnerability, we propose a two-stage evaluation\npipeline: (1) elicit each model's original response on unperturbed inputs; (2)\ninject explicit (false-answer hints) and implicit (contextual contradictions)\nmisleading instructions, and compute the misleading rate - the fraction of\ncorrect-to-incorrect flips. Leveraging the most susceptible examples, we curate\nthe Multimodal Uncertainty Benchmark (MUB), a collection of image-question\npairs stratified into low, medium, and high difficulty based on how many of\ntwelve state-of-the-art MLLMs they mislead. Extensive evaluation on twelve\nopen-source and five closed-source models reveals a high uncertainty: average\nmisleading rates exceed 86%, with explicit cues over 67.19% and implicit cues\nover 80.67%. To reduce the misleading rate, we then fine-tune all open-source\nMLLMs on a compact 2000-sample mixed-instruction dataset, reducing misleading\nrates to 6.97% (explicit) and 32.77% (implicit), boosting consistency by nearly\n29.37% on highly deceptive inputs, and slightly improving accuracy on standard\nbenchmarks. Our code is available at https://github.com/Yunkaidang/uncertainty"}
{"id": "2411.11465", "pdf": "https://arxiv.org/pdf/2411.11465.pdf", "abs": "https://arxiv.org/abs/2411.11465", "title": "Re-examining learning linear functions in context", "authors": ["Omar Naim", "Guilhem Fouilhé", "Nicholas Asher"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning."}
{"id": "2412.16701", "pdf": "https://arxiv.org/pdf/2412.16701.pdf", "abs": "https://arxiv.org/abs/2412.16701", "title": "AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles", "authors": ["Aritra Kumar Lahiri", "Qinmin Vivian Hu"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recent advancements in generative AI have fostered the development of highly\nadept Large Language Models (LLMs) that integrate diverse data types to empower\ndecision-making. Among these, multimodal retrieval-augmented generation (RAG)\napplications are promising because they combine the strengths of information\nretrieval and generative models, enhancing their utility across various\ndomains, including clinical use cases. This paper introduces AlzheimerRAG, a\nMultimodal RAG application for clinical use cases, primarily focusing on\nAlzheimer's Disease case studies from PubMed articles. This application\nincorporates cross-modal attention fusion techniques to integrate textual and\nvisual data processing by efficiently indexing and accessing vast amounts of\nbiomedical literature. Our experimental results, compared to benchmarks such as\nBioASQ and PubMedQA, have yielded improved performance in the retrieval and\nsynthesis of domain-specific information. We also present a case study using\nour multimodal RAG in various Alzheimer's clinical scenarios. We infer that\nAlzheimerRAG can generate responses with accuracy non-inferior to humans and\nwith low rates of hallucination."}
{"id": "2501.04648", "pdf": "https://arxiv.org/pdf/2501.04648.pdf", "abs": "https://arxiv.org/abs/2501.04648", "title": "FlairGPT: Repurposing LLMs for Interior Designs", "authors": ["Gabrielle Littlefair", "Niladri Shekhar Dutt", "Niloy J. Mitra"], "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": "EUROGRAPHICS 2025", "summary": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/"}
{"id": "2501.09012", "pdf": "https://arxiv.org/pdf/2501.09012.pdf", "abs": "https://arxiv.org/abs/2501.09012", "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot", "authors": ["Ruixiang Jiang", "Changwen Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "ACM MM 2025 Camera Ready", "summary": "The rapid technical progress of generative art (GenArt) has democratized the\ncreation of visually appealing imagery. However, achieving genuine artistic\nimpact - the kind that resonates with viewers on a deeper, more meaningful\nlevel - remains formidable as it requires a sophisticated aesthetic\nsensibility. This sensibility involves a multifaceted cognitive process\nextending beyond mere visual appeal, which is often overlooked by current\ncomputational methods. This paper pioneers an approach to capture this complex\nprocess by investigating how the reasoning capabilities of Multimodal LLMs\n(MLLMs) can be effectively elicited to perform aesthetic judgment. Our analysis\nreveals a critical challenge: MLLMs exhibit a tendency towards hallucinations\nduring aesthetic reasoning, characterized by subjective opinions and\nunsubstantiated artistic interpretations. We further demonstrate that these\nhallucinations can be suppressed by employing an evidence-based and objective\nreasoning process, as substantiated by our proposed baseline, ArtCoT. MLLMs\nprompted by this principle produce multifaceted, in-depth aesthetic reasoning\nthat aligns significantly better with human judgment. These findings have\ndirect applications in areas such as AI art tutoring and as reward models for\nimage generation. Ultimately, we hope this work paves the way for AI systems\nthat can truly understand, appreciate, and contribute to art that aligns with\nhuman aesthetic values. Project homepage: https://github.com/songrise/MLLM4Art."}
{"id": "2501.13919", "pdf": "https://arxiv.org/pdf/2501.13919.pdf", "abs": "https://arxiv.org/abs/2501.13919", "title": "Temporal Preference Optimization for Long-Form Video Understanding", "authors": ["Rui Li", "Xiaohan Wang", "Yuhui Zhang", "Orr Zohar", "Zeyu Wang", "Serena Yeung-Levy"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website."}
{"id": "2501.19306", "pdf": "https://arxiv.org/pdf/2501.19306.pdf", "abs": "https://arxiv.org/abs/2501.19306", "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling", "authors": ["Jiefeng Chen", "Jie Ren", "Xinyun Chen", "Chengrun Yang", "Ruoxi Sun", "Jinsung Yoon", "Sercan Ö Arık"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, existing scaling methods have key limitations:\nparallel methods like repeated sampling are often inefficient and quickly\nsaturate, while sequential methods like SELF-REFINE struggle to improve after a\nfew rounds. Although combining these approaches shows promise, current methods\nrequire fine-tuned reward and revision models. This paper proposes\nSelf-Enhanced Test-Time Scaling (SETS), a simple yet effective approach that\novercomes these limitations by strategically combining parallel and sequential\ntechniques and fully leveraging LLMs' self-improvement abilities. SETS exploits\nthe inherent self-verification and self-correction capabilities of LLMs,\nunifying sampling, verification, and correction within a single framework. This\nfacilitates efficient and scalable test-time computation for enhanced\nperformance on complex tasks without any model training. Our comprehensive\nexperimental results on challenging benchmarks spanning planning, reasoning,\nmath, and coding demonstrate that SETS achieves significant performance\nimprovements and more advantageous test-time scaling behavior than the\nalternatives."}
{"id": "2502.01891", "pdf": "https://arxiv.org/pdf/2502.01891.pdf", "abs": "https://arxiv.org/abs/2502.01891", "title": "Training and Evaluating with Human Label Variation: An Empirical Study", "authors": ["Kemal Kurniawan", "Meladel Mistica", "Timothy Baldwin", "Jey Han Lau"], "categories": ["cs.LG", "cs.CL"], "comment": "27 pages, 7 figures. Submitted to CL. Fixed PO-JSD values on the MFRC\n  dataset. Completely redid the empirical meta-evaluation, added more related\n  work, and other minor edits", "summary": "Human label variation (HLV) challenges the standard assumption that a\nlabelled instance has a single ground truth, instead embracing the natural\nvariation in human annotation to train and evaluate models. While various\ntraining methods and metrics for HLV have been proposed, it is still unclear\nwhich methods and metrics perform best in what settings. We propose new\nevaluation metrics for HLV leveraging fuzzy set theory. Since these new\nproposed metrics are differentiable, we then in turn experiment with employing\nthese metrics as training objectives. We conduct an extensive study over 6 HLV\ndatasets testing 14 training methods and 6 evaluation metrics. We find that\ntraining on either disaggregated annotations or soft labels performs best\nacross metrics, outperforming training using the proposed training objectives\nwith differentiable metrics. We also show that our proposed soft micro F1 score\nis one of the best metrics for HLV data."}
{"id": "2502.15779", "pdf": "https://arxiv.org/pdf/2502.15779.pdf", "abs": "https://arxiv.org/abs/2502.15779", "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."}
{"id": "2502.17967", "pdf": "https://arxiv.org/pdf/2502.17967.pdf", "abs": "https://arxiv.org/abs/2502.17967", "title": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents", "authors": ["Tianmi Ma", "Jiawei Du", "Wenxin Huang", "Wenjie Wang", "Liang Xie", "Xian Zhong", "Joey Tianyi Zhou"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "q-fin.ST"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language tasks, yet their performance in dynamic, real-world financial\nenvironments remains underexplored. Existing approaches are limited to\nhistorical backtesting, where trading actions cannot influence market prices\nand agents train only on static data. To address this limitation, we present\nthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-based\nagents engage in competitive multi-agent trading and directly impact price\ndynamics. By simulating realistic bid-ask interactions, our platform enables\ntraining in scenarios that closely mirror live markets, thereby narrowing the\ngap between training and evaluation. Experiments reveal that LLMs struggle with\nnumerical reasoning when given plain-text data, often overfitting to local\npatterns and recent values. In contrast, chart-based visualizations\nsignificantly enhance both numerical reasoning and trading performance.\nFurthermore, incorporating a reflection module yields additional improvements,\nespecially with visual inputs. Evaluations on NASDAQ and CSI datasets\ndemonstrate the superiority of our method, particularly under high volatility.\nAll code and data are available at\nhttps://github.com/wekjsdvnm/Agent-Trading-Arena."}
{"id": "2503.06794", "pdf": "https://arxiv.org/pdf/2503.06794.pdf", "abs": "https://arxiv.org/abs/2503.06794", "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study", "authors": ["Yizheng Sun", "Hao Li", "Chang Xu", "Hongpeng Zhou", "Chenghua Lin", "Riza Batista-Navarro", "Jingyuan Sun"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment."}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491.pdf", "abs": "https://arxiv.org/abs/2503.20491", "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "ICCV 2025", "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO."}
{"id": "2503.22233", "pdf": "https://arxiv.org/pdf/2503.22233.pdf", "abs": "https://arxiv.org/abs/2503.22233", "title": "More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty", "authors": ["Lang Cao", "Renhong Chen", "Yingtian Zou", "Chao Peng", "Wu Ning", "Huacong Xu", "Qian Chen", "Yuxian Wang", "Peishuo Su", "Mofan Peng", "Zijie Chen", "Yitong Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce the Entropy Driven Uncertainty Process Reward Model (EDU-PRM), a\nnovel entropy-driven training framework for process reward modeling that\nenables dynamic, uncertainty-aligned segmentation of complex reasoning steps,\neliminating the need for costly manual step annotations. Unlike previous\nProcess Reward Models (PRMs) that rely on static partitioning and human\nlabeling, EDU-PRM automatically anchors step boundaries at tokens with high\npredictive entropy. On the MATH test set, EDU-PRM achieves 65.5% accuracy,\nsurpassing strong public PRM baselines such as Math-Shepherd PRM (61.7%) and\nOmega PRM (62.4%) under the High Temperature (HT) Sample + BON setting.\nFurthermore, when replacing HT sampling with EDU sampling, EDU-PRM further\nimproves both accuracy and efficiency: at N=64, accuracy increases from 64.7%\n(HT Sample + BON) to 67.3% (EDU Sample + BON), while the number of generated\ntokens is reduced by 47%, demonstrating a superior accuracy-cost balance. On\nthe ProcessBench test set, EDU-PRM achieves a new state-of-the-art accuracy of\n88.4% using less than 1.5% of the Qwen2.5-Math-PRM-72B training data,\nsurpassing the previous best of 87.8%. In summary, EDU-PRM provides a scalable\nand annotation-efficient paradigm for process supervision in mathematical\nreasoning, opening new avenues for efficient complex reasoning on math."}
{"id": "2503.23760", "pdf": "https://arxiv.org/pdf/2503.23760.pdf", "abs": "https://arxiv.org/abs/2503.23760", "title": "Towards a cognitive architecture to enable natural language interaction in co-constructive task learning", "authors": ["Manuel Scheibl", "Birte Richter", "Alissa Müller", "Michael Beetz", "Britta Wrede"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "8 pages, 5 figures, The paper has been accepted by the 2025 34th IEEE\n  International Conference on Robot and Human Interactive Communication\n  (ROMAN), IEEE Copyright Policy:\n  https://www.ieee.org/publications/rights/copyright-policy", "summary": "This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI)."}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232.pdf", "abs": "https://arxiv.org/abs/2504.14232", "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "authors": ["Antoun Yaacoub", "Jérôme Da-Rugna", "Zainab Assaghir"], "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT was published in the International Journal of Computer Theory\n  and Engineering, vol. 17, no. 3, pp. 114-125, 2025", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation."}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292.pdf", "abs": "https://arxiv.org/abs/2505.10292", "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story and an\nimprovement in creativity from 2.58 to 3.38 (+31.0%) when compared to a\nnon-fine-tuned model."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430.pdf", "abs": "https://arxiv.org/abs/2505.13430", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a simple yet effective approach that perturbs\nthe continuous quantization scale for gradient estimation and uses a\ndirectional derivative clipping method to stabilize training. QZO is orthogonal\nto both scalar-based and codebook-based post-training quantization methods.\nCompared to full-parameter fine-tuning in 16 bits, QZO can reduce the total\nmemory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning\nLlama-2-13B within a single 24GB GPU. Code will be released publicly."}
{"id": "2505.18102", "pdf": "https://arxiv.org/pdf/2505.18102.pdf", "abs": "https://arxiv.org/abs/2505.18102", "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models", "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies."}
{"id": "2507.05063", "pdf": "https://arxiv.org/pdf/2507.05063.pdf", "abs": "https://arxiv.org/abs/2507.05063", "title": "CytoDiff: AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics", "authors": ["Jan Carreras Boada", "Rao Muhammad Umer", "Carsten Marr"], "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10; I.4.9; J.3"], "comment": "Accepted at ICCV 2025, 7-8 pages", "summary": "Biomedical datasets are often constrained by stringent privacy requirements\nand frequently suffer from severe class imbalance. These two aspects hinder the\ndevelopment of accurate machine learning models. While generative AI offers a\npromising solution, producing synthetic images of sufficient quality for\ntraining robust classifiers remains challenging. This work addresses the\nclassification of individual white blood cells, a critical task in diagnosing\nhematological malignancies such as acute myeloid leukemia (AML). We introduce\nCytoDiff, a stable diffusion model fine-tuned with LoRA weights and guided by\nfew-shot samples that generates high-fidelity synthetic white blood cell\nimages. Our approach demonstrates substantial improvements in classifier\nperformance when training data is limited. Using a small, highly imbalanced\nreal dataset, the addition of 5,000 synthetic images per class improved ResNet\nclassifier accuracy from 27\\% to 78\\% (+51\\%). Similarly, CLIP-based\nclassification accuracy increased from 62\\% to 77\\% (+15\\%). These results\nestablish synthetic image generation as a valuable tool for biomedical machine\nlearning, enhancing data coverage and facilitating secure data sharing while\npreserving patient privacy. Paper code is publicly available at\nhttps://github.com/JanCarreras24/CytoDiff."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization", "authors": ["Siting Wang", "Minnan Pei", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available."}
{"id": "2507.08529", "pdf": "https://arxiv.org/pdf/2507.08529.pdf", "abs": "https://arxiv.org/abs/2507.08529", "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qin", "Guoyu Ye", "Ruixiang Tang"], "categories": ["cs.AI", "cs.CL", "68T50, 92C50, 68T05", "J.3; I.2.7; H.3.3; I.2.1"], "comment": "12 pages,3 figures", "summary": "Despite advances from medical large language models in healthcare,\nrare-disease diagnosis remains hampered by insufficient\nknowledge-representation depth, limited concept understanding, and constrained\nclinical reasoning. We propose a framework that couples multi-granularity\nsparse activation of medical concepts with a hierarchical knowledge graph. Four\ncomplementary matching algorithms, diversity control, and a five-level fallback\nstrategy enable precise concept activation, while a three-layer knowledge graph\n(taxonomy, clinical features, instances) provides structured, up-to-date\ncontext. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,\nROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89\napproaching the 0.90 clinical threshold. Expert evaluation confirms\nimprovements in information quality, reasoning, and professional expression,\nsuggesting our approach shortens the \"diagnostic odyssey\" for rare-disease\npatients."}
{"id": "2507.14201", "pdf": "https://arxiv.org/pdf/2507.14201.pdf", "abs": "https://arxiv.org/abs/2507.14201", "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Raúl Meléndez Luján", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Add code link", "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!"}
{"id": "2507.14534", "pdf": "https://arxiv.org/pdf/2507.14534.pdf", "abs": "https://arxiv.org/abs/2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by ASRU 2025", "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo."}
{"id": "2507.19247", "pdf": "https://arxiv.org/pdf/2507.19247.pdf", "abs": "https://arxiv.org/abs/2507.19247", "title": "A Markov Categorical Framework for Language Modeling", "authors": ["Yifan Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/asiresearch/lm-theory", "summary": "Autoregressive language models achieve remarkable performance, yet a unified\ntheory explaining their internal mechanisms--how training shapes their\nrepresentations and enables complex behaviors--remains elusive. We introduce a\nnew analytical framework that models the single-step generation process as a\ncomposition of information-processing stages using the language of Markov\ncategories. This compositional perspective provides a unified mathematical\nlanguage to connect three critical aspects of language modeling that are\ntypically studied in isolation: the training objective, the geometry of the\nlearned representation space, and practical model capabilities. First, our\nframework provides a precise information-theoretic rationale for the success of\nmulti-token prediction methods like speculative decoding, quantifying the\n\"information surplus\" a model's hidden state contains about tokens beyond the\nimmediate next one. Second, we clarify how the standard negative log-likelihood\n(NLL) objective compels the model to learn not just the next word, but also the\ndata's intrinsic conditional uncertainty, a process we formalize using\ncategorical entropy. Our central result reveals that NLL training functions as\nan implicit form of spectral contrastive learning. We prove that, for common\nmodel architectures, this simple predictive objective forces the model to\nsculpt a geometrically structured representation space, implicitly aligning\nrepresentations with the eigenspectrum of a \"predictive similarity\" operator.\nThis work offers a powerful new lens to understand how information flows\nthrough a model and how the training objective shapes its internal geometry,\nthereby bridging the gap between learning theory and the practical success of\nlarge language models."}
{"id": "2508.00271", "pdf": "https://arxiv.org/pdf/2508.00271.pdf", "abs": "https://arxiv.org/abs/2508.00271", "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "authors": ["Hongjin Qian", "Zheng Liu"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent."}
{"id": "2508.07407", "pdf": "https://arxiv.org/pdf/2508.07407.pdf", "abs": "https://arxiv.org/abs/2508.07407", "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Github Repo:\n  https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents", "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems."}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452.pdf", "abs": "https://arxiv.org/abs/2508.11452", "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://www.tbox.cn/about/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking."}
{"id": "2508.17182", "pdf": "https://arxiv.org/pdf/2508.17182.pdf", "abs": "https://arxiv.org/abs/2508.17182", "title": "LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components", "authors": ["Hikaru Tsujimura", "Arush Tagade"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This preprint is under review", "summary": "Large Language Models (LLMs) often display overconfidence, presenting\ninformation with unwarranted certainty in high-stakes contexts. We investigate\nthe internal basis of this behavior via mechanistic interpretability. Using\nopen-sourced Llama 3.2 models fine-tuned on human annotated assertiveness\ndatasets, we extract residual activations across all layers, and compute\nsimilarity metrics to localize assertive representations. Our analysis\nidentifies layers most sensitive to assertiveness contrasts and reveals that\nhigh-assertive representations decompose into two orthogonal sub-components of\nemotional and logical clusters-paralleling the dual-route Elaboration\nLikelihood Model in Psychology. Steering vectors derived from these\nsub-components show distinct causal effects: emotional vectors broadly\ninfluence prediction accuracy, while logical vectors exert more localized\neffects. These findings provide mechanistic evidence for the multi-component\nstructure of LLM assertiveness and highlight avenues for mitigating\noverconfident behavior."}
{"id": "2508.17243", "pdf": "https://arxiv.org/pdf/2508.17243.pdf", "abs": "https://arxiv.org/abs/2508.17243", "title": "CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models", "authors": ["Zicong Tang", "Ziyang Ma", "Suqing Wang", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Yun Li", "Qianren Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Large Vision-Language Models (LVLMs) process multimodal inputs consisting of\ntext tokens and vision tokens extracted from images or videos. Due to the rich\nvisual information, a single image can generate thousands of vision tokens,\nleading to high computational costs during the prefilling stage and significant\nmemory overhead during decoding. Existing methods attempt to prune redundant\nvision tokens, revealing substantial redundancy in visual representations.\nHowever, these methods often struggle in shallow layers due to the lack of\nsufficient contextual information. We argue that many visual tokens are\ninherently redundant even in shallow layers and can be safely and effectively\npruned with appropriate contextual signals. In this work, we propose CoViPAL, a\nlayer-wise contextualized visual token pruning method that employs a\nPlug-and-Play Pruning Module (PPM) to predict and remove redundant vision\ntokens before they are processed by the LVLM. The PPM is lightweight,\nmodel-agnostic, and operates independently of the LVLM architecture, ensuring\nseamless integration with various models. Extensive experiments on multiple\nbenchmarks demonstrate that CoViPAL outperforms training-free pruning methods\nunder equal token budgets and surpasses training-based methods with comparable\nsupervision. CoViPAL offers a scalable and efficient solution to improve\ninference efficiency in LVLMs without compromising accuracy."}
{"id": "2508.18512", "pdf": "https://arxiv.org/pdf/2508.18512.pdf", "abs": "https://arxiv.org/abs/2508.18512", "title": "Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project", "authors": ["Antony C Chan"], "categories": ["physics.optics", "cs.CL"], "comment": "Minor changes: resolve HTML rendering issues of sideways tables; Code\n  listing in dark mode. Cite three more journal articles", "summary": "This article presents a practitioner's reflection on applying declarative,\n5th generation, problem formulation language (5GL) to de novo imaging system\ndesign, informed by experiences across the interdisciplinary research in\nacademia and cross-functional product development within the private sector.\nUsing the 96-Eyes project: 96-camera parallel multi-modal imager for\nhigh-throughput drug discovery as a representative case, I illustrate how\nproject requirements, ranging from hardware constraints to life sciences needs,\ncan be formalized into machine-readable problem statements to preserve\nmission-critical input from diverse domain stakeholders. This declarative\napproach enhances transparency, ensures design traceability, and minimizes\ncostly misalignment across optical, algorithmic, hardware-accelerated compute,\nand life sciences teams.\n  Alongside the technical discussion of 5GL with real-world code examples, I\nreflect on the practical barriers to adopting 5GL in environments where\nimperative, 3rd-generation languages (3GL) remain the default medium for\ninter-team collaboration. Rather than offering an one-size-fits-all solution,\nthese learned lessons highlight how programming paradigms implicitly shapes\nresearch workflows through existing domain hierarchies. The discussion aims to\ninvite further explorations into how declarative problem formulations can\nfacilitate innovation in settings where concurrent R\\&{}D workflows are gaining\ntraction, as opposed to environments where sequential, phase-driven workflows\nremain the norm."}
{"id": "2508.19005", "pdf": "https://arxiv.org/pdf/2508.19005.pdf", "abs": "https://arxiv.org/abs/2508.19005", "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm"}
{"id": "2508.19611", "pdf": "https://arxiv.org/pdf/2508.19611.pdf", "abs": "https://arxiv.org/abs/2508.19611", "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."}
{"id": "2508.19944", "pdf": "https://arxiv.org/pdf/2508.19944.pdf", "abs": "https://arxiv.org/abs/2508.19944", "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts", "authors": ["Taebaek Hwang", "Minseo Kim", "Gisang Lee", "Seonuk Kim", "Hyunjun Eun"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA."}
{"id": "2508.21188", "pdf": "https://arxiv.org/pdf/2508.21188.pdf", "abs": "https://arxiv.org/abs/2508.21188", "title": "Mirage or Method? How Model-Task Alignment Induces Divergent RL Conclusions", "authors": ["Haoze Wu", "Cheng Wang", "Wenshuo Zhao", "Junxian He"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in applying reinforcement learning (RL) to large language\nmodels (LLMs) have led to substantial progress. In particular, a series of\nremarkable yet often counterintuitive phenomena have been reported in LLMs,\nexhibiting patterns not typically observed in traditional RL settings. For\nexample, notable claims include that a single training example can match the\nperformance achieved with an entire dataset, that the reward signal does not\nneed to be very accurate, and that training solely with negative samples can\nmatch or even surpass sophisticated reward-based methods. However, the precise\nconditions under which these observations hold - and, critically, when they\nfail - remain unclear. In this work, we identify a key factor that\ndifferentiates RL observations: whether the pretrained model already exhibits\nstrong Model-Task Alignment, as measured by pass@k accuracy on the evaluated\ntask. Through a systematic and comprehensive examination of a series of\ncounterintuitive claims, supported by rigorous experimental validation across\ndifferent model architectures and task domains, our findings show that while\nstandard RL training remains consistently robust across settings, many of these\ncounterintuitive results arise only when the model and task already exhibit\nstrong model-task alignment. In contrast, these techniques fail to drive\nsubstantial learning in more challenging regimes, where standard RL methods\nremain effective."}
