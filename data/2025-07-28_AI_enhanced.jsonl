{"id": "2507.18637", "pdf": "https://arxiv.org/pdf/2507.18637.pdf", "abs": "https://arxiv.org/abs/2507.18637", "title": "More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading", "authors": ["Pingjing Yang", "Jennifer Cromley", "Jana Diesner"], "categories": ["cs.HC", "cs.AI"], "comment": "This work will appear at the 26th International Conference on\n  Artificial Intelligence in Education (AIED 2025)", "summary": "Understanding how novices acquire and hone visual search skills is crucial\nfor developing and optimizing training methods across domains. Network analysis\nmethods can be used to analyze graph representations of visual expertise. This\nstudy investigates the relationship between eye-gaze movements and learning\noutcomes among undergraduate dentistry students who were diagnosing dental\nradiographs over multiple semesters. We use network analysis techniques to\nmodel eye-gaze scanpaths as directed graphs and examine changes in network\nmetrics over time. Using time series clustering on each metric, we identify\ndistinct patterns of visual search strategies and explore their association\nwith students' diagnostic performance. Our findings suggest that the network\nmetric of transition entropy is negatively correlated with performance scores,\nwhile the number of nodes and edges as well as average PageRank are positively\ncorrelated with performance scores. Changes in network metrics for individual\nstudents over time suggest a developmental shift from intermediate to\nexpert-level processing. These insights contribute to understanding expertise\nacquisition in visual tasks and can inform the design of AI-assisted learning\ninterventions.", "AI": {"tldr": "This study uses network analysis to examine the relationship between eye-gaze movements and learning outcomes among dental students diagnosing radiographs.", "motivation": "Understanding how novices acquire visual search skills is important for optimizing training methods.", "method": "Network analysis techniques are applied to model eye-gaze scanpaths as directed graphs and analyze changes in network metrics over time.", "result": "Transition entropy is negatively correlated with performance scores, while the number of nodes and edges, as well as average PageRank, are positively correlated with performance scores.", "conclusion": "Insights from this study can inform the design of AI-assisted learning interventions.", "key_contributions": ["Utilization of network analysis methods for visual expertise", "Identification of patterns in visual search strategies", "Correlation of eye movement metrics with diagnostic performance"], "limitations": "", "keywords": ["visual search", "network analysis", "eye-gaze movements", "learning outcomes", "AI-assisted learning"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2507.18638", "pdf": "https://arxiv.org/pdf/2507.18638.pdf", "abs": "https://arxiv.org/abs/2507.18638", "title": "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity", "authors": ["Rizal Khoirul Anam"], "categories": ["cs.HC", "cs.AI", "68T50 68T50 68T50", "I.2.7"], "comment": "38 pages, 15 tables, 5 figures. Submitted as a research paper draft\n  for arXiv. Based on survey data collected in 2025", "summary": "The widespread adoption of large language models (LLMs) such as ChatGPT,\nGemini, and DeepSeek has significantly changed how people approach tasks in\neducation, professional work, and creative domains. This paper investigates how\nthe structure and clarity of user prompts impact the effectiveness and\nproductivity of LLM outputs. Using data from 243 survey respondents across\nvarious academic and occupational backgrounds, we analyze AI usage habits,\nprompting strategies, and user satisfaction. The results show that users who\nemploy clear, structured, and context-aware prompts report higher task\nefficiency and better outcomes. These findings emphasize the essential role of\nprompt engineering in maximizing the value of generative AI and provide\npractical implications for its everyday use.", "AI": {"tldr": "This paper explores the effect of user prompt structure on the efficacy of outputs from large language models (LLMs) based on a survey of 243 respondents.", "motivation": "To understand how prompting strategies affect productivity and satisfaction in using LLMs.", "method": "Analysis based on survey data from 243 respondents regarding their AI usage habits and prompting strategies.", "result": "Users with clear, structured prompts report higher task efficiency and better outcomes from LLMs.", "conclusion": "Prompt engineering is crucial for maximizing the value of generative AI in various tasks.", "key_contributions": ["Investigation of user prompt structure impact on LLM outputs", "Analysis of AI usage habits across diverse respondents", "Practical implications for improving generative AI use in everyday tasks."], "limitations": "", "keywords": ["large language models", "prompt engineering", "AI user study", "task efficiency", "user satisfaction"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.18639", "pdf": "https://arxiv.org/pdf/2507.18639.pdf", "abs": "https://arxiv.org/abs/2507.18639", "title": "People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction", "authors": ["Paweł Niszczota", "Tomasz Grzegorczyk", "Alexander Pastukhov"], "categories": ["cs.HC", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; H.5.2; H.5.3; K.4.3"], "comment": null, "summary": "Machines driven by large language models (LLMs) have the potential to augment\nhumans across various tasks, a development with profound implications for\nbusiness settings where effective communication, collaboration, and stakeholder\ntrust are paramount. To explore how interacting with an LLM instead of a human\nmight shift cooperative behavior in such settings, we used the Prisoner's\nDilemma game -- a surrogate of several real-world managerial and economic\nscenarios. In Experiment 1 (N=100), participants engaged in a thirty-round\nrepeated game against a human, a classic bot, and an LLM (GPT, in real-time).\nIn Experiment 2 (N=192), participants played a one-shot game against a human or\nan LLM, with half of them allowed to communicate with their opponent, enabling\nLLMs to leverage a key advantage over older-generation machines. Cooperation\nrates with LLMs -- while lower by approximately 10-15 percentage points\ncompared to interactions with human opponents -- were nonetheless high. This\nfinding was particularly notable in Experiment 2, where the psychological cost\nof selfish behavior was reduced. Although allowing communication about\ncooperation did not close the human-machine behavioral gap, it increased the\nlikelihood of cooperation with both humans and LLMs equally (by 88%), which is\nparticularly surprising for LLMs given their non-human nature and the\nassumption that people might be less receptive to cooperating with machines\ncompared to human counterparts. Additionally, cooperation with LLMs was higher\nfollowing prior interaction with humans, suggesting a spillover effect in\ncooperative behavior. Our findings validate the (careful) use of LLMs by\nbusinesses in settings that have a cooperative component.", "AI": {"tldr": "This paper investigates how interaction with large language models (LLMs) affects cooperative behavior in business settings using the Prisoner's Dilemma game.", "motivation": "To explore the impact of LLMs on cooperative behavior in business scenarios where communication and collaboration are vital.", "method": "Two experiments using the Prisoner's Dilemma game: the first involved a repeated game against a human, a classic bot, and an LLM; the second involved a one-shot game against a human or an LLM with communication allowance.", "result": "Cooperation rates with LLMs were lower than with humans but still high; allowing communication did not close the gap but equally increased cooperation likelihood with both types of opponents.", "conclusion": "LLMs can be cautiously utilized in business settings to enhance cooperation, especially following interactions with humans, indicating spillover effects in cooperative behaviors.", "key_contributions": ["Empirical evidence of LLMs influencing cooperative behavior in business contexts", "Comparison of cooperative dynamics between humans and LLMs", "Insights on the importance of communication in human-LLM interactions"], "limitations": "The study did not explore the long-term effects of LLM interactions on cooperation and was limited to experimental settings.", "keywords": ["large language models", "cooperation", "Prisoner's Dilemma", "human-machine interaction", "business settings"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.18640", "pdf": "https://arxiv.org/pdf/2507.18640.pdf", "abs": "https://arxiv.org/abs/2507.18640", "title": "How good are humans at detecting AI-generated images? Learnings from an experiment", "authors": ["Thomas Roca", "Anthony Cintron Roman", "Jehú Torres Vega", "Marcelo Duarte", "Pengce Wang", "Kevin White", "Amit Misra", "Juan Lavista Ferres"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "As AI-powered image generation improves, a key question is how well human\nbeings can differentiate between \"real\" and AI-generated or modified images.\nUsing data collected from the online game \"Real or Not Quiz.\", this study\ninvestigates how effectively people can distinguish AI-generated images from\nreal ones. Participants viewed a randomized set of real and AI-generated\nimages, aiming to identify their authenticity. Analysis of approximately\n287,000 image evaluations by over 12,500 global participants revealed an\noverall success rate of only 62\\%, indicating a modest ability, slightly above\nchance. Participants were most accurate with human portraits but struggled\nsignificantly with natural and urban landscapes. These results highlight the\ninherent challenge humans face in distinguishing AI-generated visual content,\nparticularly images without obvious artifacts or stylistic cues. This study\nstresses the need for transparency tools, such as watermarks and robust AI\ndetection tools to mitigate the risks of misinformation arising from\nAI-generated content", "AI": {"tldr": "This study assesses how well people can distinguish between real and AI-generated images using data from an online quiz, revealing a modest success rate of 62%.", "motivation": "With the advancement of AI in image generation, understanding human capability to identify real versus AI-generated images is critical to avoid misinformation.", "method": "Participants were tasked with identifying the authenticity of a randomized set of real and AI-generated images during the 'Real or Not Quiz,' leading to an analysis of around 287,000 evaluations from over 12,500 participants.", "result": "Participants had an overall success rate of 62% in recognizing AI-generated images, most accurately identifying human portraits, but struggled with natural and urban landscapes.", "conclusion": "The findings emphasize the challenges in identifying AI-generated content and advocate for transparency measures to combat misinformation.", "key_contributions": ["Identified the limitations of human abilities in distinguishing AI-generated images.", "Highlighted the most recognizable types of images for participants (human portraits).", "Recommended transparency tools, like watermarks, to mitigate misinformation."], "limitations": "Results may vary based on image type; the study does not cover all possible categories of images.", "keywords": ["AI-generated images", "image authentication", "misinformation", "human perception", "transparency tools"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18742", "pdf": "https://arxiv.org/pdf/2507.18742.pdf", "abs": "https://arxiv.org/abs/2507.18742", "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "authors": ["Víctor Gallego"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "AI": {"tldr": "This paper introduces Specification Self-Correction (SSC), a framework for language models to identify and rectify flaws in guiding specifications to prevent exploitation of in-context reward systems.", "motivation": "Language models often misuse flawed specifications to achieve high scores, undermining user intent. There is a need for a framework that helps correct these issues during inference.", "method": "The SSC framework involves a multi-step inference process where the language model first generates a response based on a given specification, critiques this output, and then revises the specification to eliminate exploitable loopholes before generating a final response.", "result": "Experiments show that language models initially exploit tainted specifications in 50-70% of cases, but the SSC process reduces this vulnerability by over 90%.", "conclusion": "SSC provides a way to dynamically repair issues in specifications during inference, enhancing model alignment without modifying weights, resulting in more robust outputs.", "key_contributions": ["Introduction of the Specification Self-Correction framework", "Demonstrated reduction in reward hacking vulnerability", "Improved model alignment during inference"], "limitations": "", "keywords": ["Language Models", "Specification Self-Correction", "In-context Reward Hacking", "Robustness", "Model Alignment"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.18641", "pdf": "https://arxiv.org/pdf/2507.18641.pdf", "abs": "https://arxiv.org/abs/2507.18641", "title": "Comparing Human and AI Performance in Visual Storytelling through Creation of Comic Strips: A Case Study", "authors": ["Uğur Önal", "Sanem Sariel", "Metin Sezgin", "Ergun Akleman"], "categories": ["cs.HC", "cs.CY"], "comment": "This paper is accepted to be presented in Digital Humanities\n  Conference 2025, and it will also appear in their proceedings", "summary": "This article presents a case study comparing the capabilities of humans and\nartificial intelligence (AI) for visual storytelling. We developed detailed\ninstructions to recreate a three-panel Nancy cartoon strip by Ernie Bushmiller\nand provided them to both humans and AI systems. The human participants were\n20-something students with basic artistic training but no experience or\nknowledge of this comic strip. The AI systems used were popular commercial\nmodels trained to draw and paint like artists, though their training sets may\nnot necessarily include Bushmiller's work. Results showed that AI systems excel\nat mimicking professional art but struggle to create coherent visual stories.\nIn contrast, humans proved highly adept at transforming instructions into\nmeaningful visual narratives.", "AI": {"tldr": "A case study comparing human and AI capabilities in visual storytelling, showing that while AI excels at mimicking art, humans are better at creating coherent narratives.", "motivation": "To explore the differences in visual storytelling capabilities between humans and AI systems.", "method": "Conducted a case study where human participants and AI systems were tasked with recreating a three-panel cartoon strip using detailed instructions.", "result": "AI systems performed well in art mimicry but struggled with coherent storytelling, while humans were effective in creating meaningful visual narratives from instructions.", "conclusion": "The findings highlight the strengths and weaknesses of both humans and AI in the context of visual storytelling.", "key_contributions": ["Detailed comparison of human and AI artistic capabilities", "Insights into the limitations of AI in narrative coherence", "Practical implications for the use of AI in visual storytelling"], "limitations": "The study involved only basic artistic-trained students and commercial AI models; results may vary with different populations or more advanced AI.", "keywords": ["visual storytelling", "human-AI comparison", "artistic capability", "narrative coherence", "Digital Humanities"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.18762", "pdf": "https://arxiv.org/pdf/2507.18762.pdf", "abs": "https://arxiv.org/abs/2507.18762", "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages", "authors": ["Abdulhady Abas Abdullah", "Amir H. Gandomi", "Tarik A Rashid", "Seyedali Mirjalili", "Laith Abualigah", "Milena Živković", "Hadi Veisi"], "categories": ["cs.CL"], "comment": null, "summary": "In natural language processing, multilingual models like mBERT and\nXLM-RoBERTa promise broad coverage but often struggle with languages that share\na script yet differ in orthographic norms and cultural context. This issue is\nespecially notable in Arabic-script languages such as Kurdish Sorani, Arabic,\nPersian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:\nfour RoBERTa-based models, each pre-trained on a large corpus tailored to its\nspecific language. By focusing pre-training on language-specific script\nfeatures and statistics, our models capture patterns overlooked by\ngeneral-purpose models. When fine-tuned on classification tasks, AS-RoBERTa\nvariants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An\nablation study confirms that script-focused pre-training is central to these\ngains. Error analysis using confusion matrices shows how shared script traits\nand domain-specific content affect performance. Our results highlight the value\nof script-aware specialization for languages using the Arabic script and\nsupport further work on pre-training strategies rooted in script and language\nspecificity.", "AI": {"tldr": "Introduction of AS-RoBERTa, a family of RoBERTa-based models pre-trained for specific Arabic-script languages, achieving better performance than multilingual models.", "motivation": "To address performance issues in multilingual models for Arabic-script languages that share a script but differ in orthographic norms and cultural context.", "method": "Four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific language, focusing on language-specific script features and statistics.", "result": "AS-RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points in classification tasks.", "conclusion": "Script-aware specialization enhances model performance for Arabic-script languages, highlighting the importance of tailored pre-training strategies.", "key_contributions": ["Introduction of AS-RoBERTa family of models", "Demonstration of performance improvements over mBERT and XLM-RoBERTa", "Analysis of script-focused pre-training benefits"], "limitations": "", "keywords": ["Natural Language Processing", "Arabic Script", "RoBERTa", "Multilingual Models", "Machine Learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.18802", "pdf": "https://arxiv.org/pdf/2507.18802.pdf", "abs": "https://arxiv.org/abs/2507.18802", "title": "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition", "authors": ["Danqing Shi", "Furui Cheng", "Tino Weinkauf", "Antti Oulasvirta", "Mennatallah El-Assady"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human preferences are widely used to align large language models (LLMs)\nthrough methods such as reinforcement learning from human feedback (RLHF).\nHowever, the current user interfaces require annotators to compare text\nparagraphs, which is cognitively challenging when the texts are long or\nunfamiliar. This paper contributes by studying the decomposition principle as\nan approach to improving the quality of human feedback for LLM alignment. This\napproach breaks down the text into individual claims instead of directly\ncomparing two long-form text responses. Based on the principle, we build a\nnovel user interface DxHF. It enhances the comparison process by showing\ndecomposed claims, visually encoding the relevance of claims to the\nconversation and linking similar claims. This allows users to skim through key\ninformation and identify differences for better and quicker judgment. Our\ntechnical evaluation shows evidence that decomposition generally improves\nfeedback accuracy regarding the ground truth, particularly for users with\nuncertainty. A crowdsourcing study with 160 participants indicates that using\nDxHF improves feedback accuracy by an average of 5%, although it increases the\naverage feedback time by 18 seconds. Notably, accuracy is significantly higher\nin situations where users have less certainty. The finding of the study\nhighlights the potential of HCI as an effective method for improving human-AI\nalignment.", "AI": {"tldr": "This paper proposes a novel user interface, DxHF, that improves human feedback for large language model alignment by breaking down text into individual claims, enhancing comparison accuracy.", "motivation": "To address the cognitive challenges faced by annotators when comparing long or unfamiliar texts in aligning large language models (LLMs) using human feedback.", "method": "The authors apply the decomposition principle, breaking down long texts into individual claims and designing a user interface (DxHF) that visually encodes claim relevance and links similar claims for easier comparison.", "result": "The technical evaluation shows that using the DxHF interface generally improves feedback accuracy by an average of 5%, especially for users with uncertainty, despite an 18-second increase in feedback time.", "conclusion": "The study demonstrates the effectiveness of the HCI-driven approach to improve human-AI alignment through better user interfaces for feedback collection.", "key_contributions": ["Introduction of the DxHF user interface for feedback on LLMs", "Evidence that claim decomposition enhances feedback accuracy", "Demonstration of HCI's role in improving AI alignment"], "limitations": "Increased average feedback time by 18 seconds may limit usability in high-throughput scenarios.", "keywords": ["human feedback", "large language models", "HCI", "user interface", "feedback accuracy"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.18769", "pdf": "https://arxiv.org/pdf/2507.18769.pdf", "abs": "https://arxiv.org/abs/2507.18769", "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting", "authors": ["Nicole Lai-Lopez", "Lusha Wang", "Su Yuan", "Liza Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures, 3 tables,", "summary": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: a robust\nmultilingual text detoxification pipeline that integrates lexicon-guided\ntagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and\nan iterative classifier-based gatekeeping mechanism. Our approach departs from\nprior unsupervised or monolingual pipelines by leveraging explicit toxic word\nannotation via the multilingual_toxic_lexicon to guide detoxification with\ngreater precision and cross-lingual generalization. Our final model achieves\nthe highest STA (0.922) from our previous attempts, and an average official J\nscore of 0.612 for toxic inputs in both the development and test sets. It also\nachieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance\noutperforms baseline and backtranslation methods across multiple languages, and\nshows strong generalization in high-resource settings (English, Russian,\nFrench). Despite some trade-offs in SIM, the model demonstrates consistent\nimprovements in detoxification strength. In the competition, our team achieved\nninth place with a score of 0.612.", "AI": {"tldr": "The paper presents a multilingual text detoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence model, and a classifier-based gatekeeping mechanism, achieving high performance across multiple languages.", "motivation": "To improve the detoxification of toxic text across multiple languages by using explicit toxic word annotation for better precision and generalization.", "method": "A multilingual text detoxification pipeline utilizing lexicon-guided tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo), and an iterative classifier-based mechanism to enhance detoxification accuracy.", "result": "The proposed model achieved a highest STA of 0.922 and an average official J score of 0.612 for toxic inputs, outperforming baseline methods in terms of detoxification strength and generalization across languages.", "conclusion": "The approach demonstrates significant enhancements in detoxification capabilities, securing ninth place in the competition with a competitive score.", "key_contributions": ["Integration of lexicon-guided tagging for improved detoxification accuracy.", "Fine-tuning of a sequence-to-sequence model specifically for detoxification tasks.", "Demonstrated generalization across multiple languages with high performance."], "limitations": "Some trade-offs in similarity metrics were observed, but overall detoxification strength improved consistently.", "keywords": ["multilingual text detoxification", "toxic word annotation", "cross-lingual generalization"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18828", "pdf": "https://arxiv.org/pdf/2507.18828.pdf", "abs": "https://arxiv.org/abs/2507.18828", "title": "Ethical Considerations for Observational Research in Social VR", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "categories": ["cs.HC"], "comment": "CSCW Companion '25, October 18-22, 2025, Bergen, Norway", "summary": "Social VR introduces new ethical challenges for observational research. The\ncurrent paper presents a narrative literature review of ethical considerations\nin observational methods, with a focus on work in HCI. We examine how\nunobtrusive or selectively disclosed observation is implemented in public\nface-to-face and social VR settings. Our review extends ethical discussions\nfrom traditional public research into the context of social VR, highlighting\ntensions between observer visibility, data traceability, and participant\nautonomy. Drawing on insights distilled from prior literature, we propose five\nconstructive guidelines for ethical observational research in public social VR\nenvironments. Our work offers key implications for future research, addressing\nanticipated improvements in platform design, the management of researcher\npresence, and the development of community-informed consent mechanisms.", "AI": {"tldr": "The paper reviews ethical considerations in observational research for social VR, proposing guidelines for ethical practices.", "motivation": "To address ethical challenges in observational research within social VR contexts, expanding on existing HCI literature.", "method": "A narrative literature review focusing on unobtrusive observation methods in public settings, comparing traditional and VR environments.", "result": "The review identifies tensions between observer visibility, data traceability, and participant autonomy, leading to the development of five guidelines for ethical research.", "conclusion": "The proposed guidelines aim to improve platform design and inform consent mechanisms in social VR research.", "key_contributions": ["Provides a literature review on ethical issues in social VR", "Identifies key tensions in observational research", "Proposes five guidelines for ethical practices in social VR research"], "limitations": "", "keywords": ["ethical research", "social VR", "HCI", "observational methods", "participant autonomy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18791", "pdf": "https://arxiv.org/pdf/2507.18791.pdf", "abs": "https://arxiv.org/abs/2507.18791", "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "authors": ["Yilun Yang", "Yekun Chai"], "categories": ["cs.CL"], "comment": null, "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance.", "AI": {"tldr": "This paper evaluates LLMs on code-mixed data across 18 languages, revealing underperformance and proposing a novel generation method for synthetic code-mixed texts.", "motivation": "Code-mixing poses challenges for natural language processing and current benchmarks do not adequately evaluate LLM performance in this area, despite its significance for multilingual users.", "method": "The study evaluates LLMs on code-mixed datasets and proposes a method for generating synthetic code-mixed texts by combining word substitution with GPT-4 prompting.", "result": "The analysis shows that LLMs consistently underperform on code-mixed datasets that involve multiple language families.", "conclusion": "To enhance performance, the study suggests improvements in training data size, model scale, and few-shot learning for LLMs dealing with code-mixed data.", "key_contributions": ["Comprehensive evaluation of LLMs' performance on code-mixed data across 18 languages.", "Proposal of a novel method for generating synthetic code-mixed texts.", "Insights into the underperformance of LLMs and suggested improvements."], "limitations": "The existing benchmarks are limited and don't cover enough language pairings and tasks.", "keywords": ["code-mixing", "large language models", "natural language processing", "multilingual", "synthetic data generation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18836", "pdf": "https://arxiv.org/pdf/2507.18836.pdf", "abs": "https://arxiv.org/abs/2507.18836", "title": "Uncertainty on Display: The Effects of Communicating Confidence Cues in Autonomous Vehicle-Pedestrian Interactions", "authors": ["Yue Luo", "Xinyan Yu", "Tram Thi Minh Tran", "Marius Hoggenmueller"], "categories": ["cs.HC"], "comment": null, "summary": "Uncertainty is an inherent aspect of autonomous vehicle (AV) decision-making,\nyet it is rarely communicated to pedestrians, which hinders transparency. This\nstudy investigates how AV uncertainty can be conveyed through two approaches:\nexplicit communication (confidence percentage displays) and implicit\ncommunication (vehicle motion cues), across different confidence levels (high\nand low). Through a within-subject VR experiment (N=26), we evaluated these\napproaches in a crossing scenario, assessing interface qualities (visibility\nand intuitiveness), how well the information conveyed the vehicle's level of\nconfidence, and their impact on participants' perceived safety, trust, and user\nexperience. Our results show that explicit communication is more effective and\npreferred for conveying uncertainty, enhancing safety, trust, and user\nexperience. Conversely, implicit communication introduces ambiguity, especially\nwhen AV confidence is low. This research provides empirical insights into how\nuncertainty communication shapes pedestrian interpretation of AV behaviour and\noffer design guidance for external interfaces that integrate uncertainty as a\ncommunicative element.", "AI": {"tldr": "Study investigates communication of AV uncertainty to pedestrians through explicit and implicit methods.", "motivation": "To enhance transparency in AV decision-making by effectively conveying uncertainty to pedestrians.", "method": "A within-subject VR experiment with 26 participants evaluated explicit (confidence displays) and implicit (vehicle motion cues) communication in crossing scenarios.", "result": "Explicit communication is more effective for conveying uncertainty, improving perceived safety, trust, and user experience, while implicit communication can create ambiguity.", "conclusion": "Effective communication of AV uncertainty can shape pedestrian behavior and provide guidance for designing external interfaces.", "key_contributions": ["Empirical insights into AV uncertainty communication", "Comparison of explicit and implicit communication methods", "Design guidance for interfaces integrating uncertainty"], "limitations": "Study limited to specific VR environments and may not fully represent real-world scenarios.", "keywords": ["Autonomous vehicles", "User experience", "Uncertainty communication"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18827", "pdf": "https://arxiv.org/pdf/2507.18827.pdf", "abs": "https://arxiv.org/abs/2507.18827", "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "authors": ["Pranav Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach.", "AI": {"tldr": "CueBuddy offers real-time lexical cues for students in STEM classes to better understand complex English terminology.", "motivation": "Many students in the Global South struggle with English technical jargon despite having the necessary scientific knowledge, making it hard for them to keep pace in STEM courses.", "method": "CueBuddy provides real-time keyword spotting and multilingual glossary lookup to offer lexical support without distracting students.", "result": "The approach aims to improve comprehension of technical English terms and enhance learning outcomes for non-fluent English speakers in STEM.", "conclusion": "CueBuddy could be a valuable tool to support students facing language barriers in technical education, with further development needed for scalability and effectiveness.", "key_contributions": ["Real-time lexical cue provision", "Technical keyword spotting", "Multilingual glossary integration"], "limitations": "Current models can be expensive and may struggle with complex technical content.", "keywords": ["speech translation", "lexical cues", "keyword spotting", "multilingual", "technical content"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.18877", "pdf": "https://arxiv.org/pdf/2507.18877.pdf", "abs": "https://arxiv.org/abs/2507.18877", "title": "A Survey on Methodological Approaches to Collaborative Embodiment in Virtual Reality", "authors": ["Hongyu Zhou", "Yihao Dong", "Masahiko Inami", "Zhanna Sarsenbayeva", "Anusha Withana"], "categories": ["cs.HC"], "comment": null, "summary": "The application and implementation of collaborative embodiment in virtual\nreality (VR) are a critical aspect of the computer science landscape, aiming to\nenhance multi-user interaction and teamwork in immersive environments. A\nnotable and enduring area of collaborative embodiment research focuses on\napproaches that enable multiple users to share control, interact, and\ninvestigate scenarios involving supernumerary arms in virtual spaces. In this\nsurvey, we will present an extensive overview of the methodologies employed in\nthe past decade to enable collaboration in VR environments, particularly\nthrough embodiment. Using the PRISMA guidelines, we plan to analyze the study\ndetails from over 137 relevant research papers. Through this analysis, a\ncritical assessment of the effectiveness of these methodologies will be\nconducted, highlighting current challenges and limitations in implementing\ncollaborative embodiment in VR. Lastly, we discuss potential future research\ndirections and opportunities for enhancing collaboration embodiment in virtual\nenvironments.", "AI": {"tldr": "This survey reviews collaborative embodiment methodologies in VR over the past decade, analyzing 137 papers to assess effectiveness and identify challenges.", "motivation": "To enhance multi-user interaction and teamwork in immersive virtual reality environments through collaborative embodiment.", "method": "An extensive overview and analysis of methodologies employed in VR for collaborative embodiment based on PRISMA guidelines, involving a critical assessment of 137 research papers.", "result": "Identification of the effectiveness of various methodologies used for collaborative embodiment and highlighting current challenges in their implementation.", "conclusion": "Future research directions are discussed to improve collaboration embodiment in virtual environments.", "key_contributions": ["Extensive overview of collaborative embodiment methodologies in VR", "Critical assessment of past research effectiveness and limitations", "Identification of future research opportunities for collaborative embodiment"], "limitations": "Potential biases in selected studies and variability in VR implementations.", "keywords": ["collaborative embodiment", "virtual reality", "multi-user interaction", "immersive environments", "research survey"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.18857", "pdf": "https://arxiv.org/pdf/2507.18857.pdf", "abs": "https://arxiv.org/abs/2507.18857", "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval-augmented generation (RAG) often falls short when retrieved context\nincludes confusing semi-relevant passages, or when answering questions require\ndeep contextual understanding and reasoning. We propose an efficient\nfine-tuning framework, called PrismRAG, that (i) trains the model with\ndistractor-aware QA pairs mixing gold evidence with subtle distractor passages,\nand (ii) instills reasoning-centric habits that make the LLM plan, rationalize,\nand synthesize without relying on extensive human engineered instructions.\nEvaluated across 12 open-book RAG QA benchmarks spanning diverse application\ndomains and scenarios, PrismRAG improves average factuality by 5.4%,\noutperforming state-of-the-art solutions.", "AI": {"tldr": "PrismRAG is a fine-tuning framework for improving retrieval-augmented generation (RAG) by addressing issues with irrelevant context and enhancing reasoning capabilities of large language models (LLMs).", "motivation": "RAG models struggle with confusing context and require deep understanding for answering complex questions. There is a need for improved methods to enhance model performance in these areas.", "method": "PrismRAG trains models using distractor-aware QA pairs that integrate relevant evidence and subtle distractions, promoting reasoning and synthesis capabilities without extensive human instructions.", "result": "PrismRAG shows a 5.4% improvement in average factuality across 12 open-book RAG QA benchmarks compared to state-of-the-art models.", "conclusion": "The framework demonstrates significant performance improvements in diverse QA scenarios by enhancing the model's reasoning-centric abilities.", "key_contributions": ["Introduction of the PrismRAG framework", "Use of distractor-aware QA pairs for training", "Improved reasoning and factuality in RAG tasks"], "limitations": "", "keywords": ["retrieval-augmented generation", "fine-tuning framework", "large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.18878", "pdf": "https://arxiv.org/pdf/2507.18878.pdf", "abs": "https://arxiv.org/abs/2507.18878", "title": "Improving the State of the Art for Training Human-AI Teams: Technical Report #5 -- Individual Differences and Team Qualities to Measure in a Human-AI Teaming Testbed", "authors": ["Lillian Asiala", "James E. McCarthy"], "categories": ["cs.HC"], "comment": null, "summary": "Sonalysts, Inc. (Sonalysts) is working on an initiative to expand our\nexpertise in teaming to include Human-Artificial Intelligence (AI) teams. The\nfirst step of this process is to develop a Synthetic Task Environment (STE) to\nsupport our original research. Prior knowledge elicitation efforts within the\nHuman-AI teaming research stakeholder community revealed a desire to support\ndata collection using pre- and post-performance surveys. In this technical\nreport, we review a number of constructs that capture meaningful individual\ndifferences and teaming qualities. Additionally, we explore methods of\nmeasuring those constructs within the STE.", "AI": {"tldr": "This report discusses developing a Synthetic Task Environment to research Human-AI teaming and explores constructs for measuring individual differences and teaming qualities.", "motivation": "To expand expertise in Human-AI teamwork and facilitate data collection for research.", "method": "Review of constructs capturing individual differences and teaming qualities; exploration of measurement methods in a Synthetic Task Environment.", "result": "Identified relevant constructs and proposed measurement methods for their assessment in the STE.", "conclusion": "Aimed to provide a foundational framework for further research in Human-AI teaming.", "key_contributions": ["Development of Synthetic Task Environment for Human-AI research", "Identification of meaningful individual differences in teamwork", "Proposed methods for measuring teamwork qualities"], "limitations": "", "keywords": ["Human-AI teaming", "Synthetic Task Environment", "Data collection", "Team dynamics", "Performance measurement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18884", "pdf": "https://arxiv.org/pdf/2507.18884.pdf", "abs": "https://arxiv.org/abs/2507.18884", "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.", "AI": {"tldr": "MindFlow+ is a self-evolving dialogue agent that enhances e-commerce customer service interactions using LLMs combined with imitation learning and offline reinforcement learning.", "motivation": "To address the limitations of traditional intent-based systems in managing dynamic, multi-turn interactions in e-commerce customer service.", "method": "MindFlow+ leverages large language models, imitation learning, and offline reinforcement learning, introducing data-centric mechanisms like tool-augmented demonstration construction and reward-conditioned data modeling.", "result": "MindFlow+ significantly outperforms strong baselines in terms of contextual relevance, flexibility, and task accuracy in real-world e-commerce conversations.", "conclusion": "The integration of LLMs with tool reasoning and reward-guided learning showcases the potential for developing domain-specialized, context-aware dialogue systems.", "key_contributions": ["Introduction of MindFlow+, a self-evolving dialogue agent", "Development of the AI Contribution Ratio metric for dialogue", "Implementation of novel data-centric learning mechanisms"], "limitations": "", "keywords": ["dialogue systems", "e-commerce", "large language models", "imitation learning", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.18880", "pdf": "https://arxiv.org/pdf/2507.18880.pdf", "abs": "https://arxiv.org/abs/2507.18880", "title": "Rethinking Accessible Prototyping Methods for Blind and Visually Impaired Passengers in Highly Automated Vehicles", "authors": ["Luca-Maxim Meinhardt", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Workshop paper presented at \"Access InContext: Futuring Accessible\n  Prototyping Tools and Methods\", CHI'25, April 26, 2025, Yokohama, Japan.\n  Submitted Feb 5, accepted Mar 1", "summary": "Highly Automated Vehicles (HAVs) can improve mobility for blind and visually\nimpaired people (BVIPs). However, designing non-visual interfaces that enable\nthem to maintain situation awareness inside the vehicle is a challenge. This\npaper presents two of our participatory design workshops that explored what\ninformation BVIPs need in HAVs and what an interface that meets these needs\nmight look like. Based on the participants' insights, we created final systems\nto improve their situation awareness. The two workshops used different\napproaches: in the first, participants built their own low-fidelity prototypes;\nin the second, they evaluated and discussed the initial prototypes we provided.\nWe will outline how each workshop was set up and share lessons learned about\nprototyping methods for BVIPs and how they could be improved.", "AI": {"tldr": "This paper discusses participatory design workshops aimed at creating non-visual interfaces for Highly Automated Vehicles (HAVs) to assist blind and visually impaired people (BVIPs) in maintaining situation awareness.", "motivation": "To improve mobility and situation awareness for blind and visually impaired people in Highly Automated Vehicles (HAVs).", "method": "The study involved two participatory design workshops where BVIP participants created low-fidelity prototypes and evaluated existing prototypes to inform interface design.", "result": "The workshops provided insights into the information needs of BVIPs and led to the development of systems that improve their situation awareness in HAVs.", "conclusion": "The paper highlights effective prototyping methods for BVIPs and suggests improvements for future workshops.", "key_contributions": ["Insights into information needs of BVIPs in HAVs", "Development of non-visual interface prototypes", "Evaluation of participatory design methods for accessibility"], "limitations": "", "keywords": ["Highly Automated Vehicles", "Blind and Visually Impaired", "Participatory Design", "Non-Visual Interfaces"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18890", "pdf": "https://arxiv.org/pdf/2507.18890.pdf", "abs": "https://arxiv.org/abs/2507.18890", "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement", "authors": ["Jonathan Ivey", "Susan Gauch", "David Jurgens"], "categories": ["cs.CL"], "comment": null, "summary": "NLP models often rely on human-labeled data for training and evaluation. Many\napproaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations.\nThese conflicts have traditionally been resolved by aggregation methods that\nassume disagreements are errors. Recent work has argued that for many tasks\nannotators may have genuine disagreements and that variation should be treated\nas signal rather than noise. However, few models separate signal and noise in\nannotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model\nthat incorporates information about annotator backgrounds to remove noisy\nannotations from human-labeled training data while preserving systematic\ndisagreements. Using synthetic data, we show that NUTMEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than\ntraditional aggregation methods. We provide further analysis characterizing how\ndifferences in subpopulation sizes, rates of disagreement, and rates of spam\naffect the performance of our model. Finally, we demonstrate that downstream\nmodels trained on NUTMEG-aggregated data significantly outperform models\ntrained on data from traditionally aggregation methods. Our results highlight\nthe importance of accounting for both annotator competence and systematic\ndisagreements when training on human-labeled data.", "AI": {"tldr": "NUTMEG is a Bayesian model that improves annotation quality by distinguishing between signal and noise in annotator disagreements, enhancing training data for NLP models.", "motivation": "To address the issue of conflicting annotations in NLP models due to varying annotator skills and perspectives, leading to errors in traditional aggregation methods.", "method": "NUTMEG uses Bayesian inference to incorporate annotator backgrounds, effectively filtering out noisy annotations while preserving systematic disagreements.", "result": "NUTMEG outperforms traditional aggregation methods by accurately recovering ground-truth data from annotations and improving the performance of downstream models trained on this data.", "conclusion": "The findings underscore the necessity of considering annotator competence and systematic disagreements for better training datasets in NLP tasks.", "key_contributions": ["Introduction of NUTMEG, a new Bayesian model for annotation aggregation", "Demonstration of improvements in recovery of ground-truth through systematic disagreement preservation", "Analysis of how annotator background affects annotation quality"], "limitations": "", "keywords": ["NLP", "annotator disagreement", "Bayesian model", "data aggregation", "human-labeled data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18913", "pdf": "https://arxiv.org/pdf/2507.18913.pdf", "abs": "https://arxiv.org/abs/2507.18913", "title": "Limits at a Distance: Design Directions to Address Psychological Distance in Policy Decisions Affecting Planetary Boundaries", "authors": ["Eshta Bhardwaj", "Han Qiao", "Christoph Becker"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "summary": "Policy decisions relevant to the environment rely on tools like dashboards,\nrisk models, and prediction models to provide information and data\nvisualizations that enable decision-makers to make trade-offs. The conventional\nparadigm of data visualization practices for policy and decision-making is to\nconvey data in a supposedly neutral, objective manner for rational\ndecision-makers. Feminist critique advocates for nuanced and reflexive\napproaches that take into account situated decision-makers and their affective\nrelationships to data. This paper sheds light on a key cognitive aspect that\nimpacts how decision-makers interpret data. Because all outcomes from policies\nrelevant to climate change occur at a distance, decision-makers experience\nso-called `psychological distance' to environmental decisions in terms of\nspace, time, social identity, and hypotheticality. This profoundly impacts how\nthey perceive and evaluate outcomes. Since policy decisions to achieve a safe\nplanetary space are urgently needed for immediate transition and change, we\nneed a design practice that takes into account how psychological distance\naffects cognition and decision-making. Our paper explores the role of\nalternative design approaches in developing visualizations used for climate\npolicymaking. We conduct a literature review and synthesis which bridges\npsychological distance with speculative design and data visceralization by\nillustrating the value of affective design methods via examples from previous\nresearch. Through this work, we propose a novel premise for the communication\nand visualization of environmental data. Our paper lays out how future research\non the impacts of alternative design approaches on psychological distance can\nmake data used for policy decisions more tangible and visceral.", "AI": {"tldr": "This paper examines the cognitive effects of psychological distance on decision-makers interpreting environmental data, advocating for alternative design practices in climate policy visualizations.", "motivation": "To improve the effectiveness of data visualizations used in environmental policy-making by considering how psychological distance affects decision-making.", "method": "Literature review and synthesis connecting psychological distance with speculative design and affective design methods.", "result": "The analysis highlights the necessity of alternative design approaches to create more tangible and visceral data visualizations for climate policy decisions.", "conclusion": "Future research should focus on how these alternative design approaches can enhance the interpretation of environmental data by reducing psychological distance.", "key_contributions": ["Highlights the role of psychological distance in data interpretation for climate policy.", "Proposes alternative design methodologies for environmental data visualizations.", "Illustrates the value of affective design through literature examples."], "limitations": "", "keywords": ["psychological distance", "data visualization", "climate policy", "affective design", "speculative design"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.18901", "pdf": "https://arxiv.org/pdf/2507.18901.pdf", "abs": "https://arxiv.org/abs/2507.18901", "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Assessing the reproducibility of social science papers is essential for\npromoting rigor in research processes, but manual assessment is costly. With\nrecent advances in agentic AI systems (i.e., AI agents), we seek to evaluate\ntheir capability to automate this process. However, existing benchmarks for\nreproducing research papers (1) focus solely on reproducing results using\nprovided code and data without assessing their consistency with the paper, (2)\noversimplify real-world scenarios, and (3) lack necessary diversity in data\nformats and programming languages. To address these issues, we introduce\nREPRO-Bench, a collection of 112 task instances, each representing a social\nscience paper with a publicly available reproduction report. The agents are\ntasked with assessing the reproducibility of the paper based on the original\npaper PDF and the corresponding reproduction package. REPRO-Bench features\nend-to-end evaluation tasks on the reproducibility of social science papers\nwith complexity comparable to real-world assessments. We evaluate three\nrepresentative AI agents on REPRO-Bench, with the best-performing agent\nachieving an accuracy of only 21.4%. Building on our empirical analysis, we\ndevelop REPRO-Agent, which improves the highest accuracy achieved by existing\nagents by 71%. We conclude that more advanced AI agents should be developed to\nautomate real-world reproducibility assessment. REPRO-Bench is publicly\navailable at https://github.com/uiuc-kang-lab/REPRO-Bench.", "AI": {"tldr": "The paper introduces REPRO-Bench, a benchmarking tool for assessing the reproducibility of social science research using AI agents.", "motivation": "There is a need to automate the costly process of assessing the reproducibility of social science papers due to current manual methods being inefficient.", "method": "The authors developed REPRO-Bench, a collection of 112 task instances where AI agents assess the reproducibility of social science papers based on original PDFs and reproduction packages.", "result": "The evaluation of three AI agents on REPRO-Bench shows the best performance at only 21.4% accuracy, leading to the development of REPRO-Agent, which improves accuracy by 71%.", "conclusion": "The study concludes that there is a necessity for more sophisticated AI agents to enhance the automation of reproducibility assessments in social science research.", "key_contributions": ["Introduction of REPRO-Bench for benchmarking reproducibility assessments", "Evaluation of AI agents against realistic tasks", "Development of REPRO-Agent which significantly improves accuracy"], "limitations": "Limited to social science papers and may not generalize across other fields.", "keywords": ["reproducibility", "AI agents", "social science", "benchmarking", "REPRO-Bench"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18945", "pdf": "https://arxiv.org/pdf/2507.18945.pdf", "abs": "https://arxiv.org/abs/2507.18945", "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models", "authors": ["Zijian Zhang", "Pan Chen", "Fangshi Du", "Runlong Ye", "Oliver Huang", "Michael Liut", "Alán Aspuru-Guzik"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration.", "AI": {"tldr": "TreeReader is a language model-augmented tool that organizes academic papers into an interactive tree structure to improve navigation and comprehension.", "motivation": "To address cognitive overload caused by traditional formats like PDF and HTML in academic reading, which obscure hierarchical structures of papers.", "method": "TreeReader decomposes papers into an interactive tree format with LLM-generated summaries for each section, allowing users to access detailed information on demand.", "result": "User studies indicate that TreeReader significantly enhances reading efficiency and comprehension compared to traditional formats.", "conclusion": "TreeReader enables focused navigation and understanding of complex academic literature by combining hierarchical summarization with interactivity.", "key_contributions": ["Presents a novel interactive tree structure for academic papers.", "Improves navigation through hierarchical summarization.", "Enhances comprehension via LLM-generated concise summaries."], "limitations": "", "keywords": ["Human-Computer Interaction", "Language Models", "Academic Reading"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.18902", "pdf": "https://arxiv.org/pdf/2507.18902.pdf", "abs": "https://arxiv.org/abs/2507.18902", "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models", "authors": ["Hongyuan Lu", "Zixuan Li", "Zefan Zhang", "Wai Lam"], "categories": ["cs.CL"], "comment": null, "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}", "AI": {"tldr": "This paper introduces a novel task called Automatic Dictionary Selection (ADS) and a method named Select Low-frequency Words (SLoW) to enhance translation in Large Language Models by selecting dictionaries based on low-frequency words, leading to improved performance with reduced token consumption.", "motivation": "To address the limitation of current LLMs that support only hundreds of languages and the inefficiency of using all available dictionaries for translation.", "method": "The authors propose the Select Low-frequency Words (SLoW) method, which automatically selects dictionaries that have lower frequency words, improving translation without needing access to training data for frequency estimation.", "result": "Experimental results across 100 languages show that SLoW outperforms strong baselines and significantly reduces token usage, often exceeding the performance of using full dictionary baselines.", "conclusion": "The SLoW method provides an effective means of enhancing translation performance in LLMs while saving token usage, making it a flexible and efficient alternative to traditional dictionary-based approaches.", "key_contributions": ["Introduction of a novel task called Automatic Dictionary Selection (ADS).", "Development of the SLoW method that effectively selects lower frequency dictionaries for translation improvements.", "Demonstration of significant performance gains with reduced token usage across multiple languages."], "limitations": "The study does not address the potential performance in languages not covered by the dataset used or the effectiveness across contexts requiring different types of dictionaries.", "keywords": ["Large Language Models", "translation", "dictionary-based methods", "token consumption", "automatic dictionary selection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.18971", "pdf": "https://arxiv.org/pdf/2507.18971.pdf", "abs": "https://arxiv.org/abs/2507.18971", "title": "Rethinking Dataset Discovery with DataScout", "authors": ["Rachel Lin", "Bhavya Chopra", "Wenjing Lin", "Shreya Shankar", "Madelon Hulsebos", "Aditya G. Parameswaran"], "categories": ["cs.HC"], "comment": "16 pages; 6 figures; 4 tables; To appear at UIST 2025", "summary": "Dataset Search -- the process of finding appropriate datasets for a given\ntask -- remains a critical yet under-explored challenge in data science\nworkflows. Assessing dataset suitability for a task (e.g., training a\nclassification model) is a multi-pronged affair that involves understanding:\ndata characteristics (e.g. granularity, attributes, size), semantics (e.g.,\ndata semantics, creation goals), and relevance to the task at hand. Present-day\ndataset search interfaces are restrictive -- users struggle to convey implicit\npreferences and lack visibility into the search space and result inclusion\ncriteria -- making query iteration challenging. To bridge these gaps, we\nintroduce DataScout to proactively steer users through the process of dataset\ndiscovery via -- (i) AI-assisted query reformulations informed by the\nunderlying search space, (ii) semantic search and filtering based on dataset\ncontent, including attributes (columns) and granularity (rows), and (iii)\ndataset relevance indicators, generated dynamically based on the user-specified\ntask. A within-subjects study with 12 participants comparing DataScout to\nkeyword and semantic dataset search reveals that users uniquely employ\nDataScout's features not only for structured explorations, but also to glean\nfeedback on their search queries and build conceptual models of the search\nspace.", "AI": {"tldr": "DataScout enhances dataset search by using AI-assisted query reformulations, semantic search, and relevance indicators to support users in finding suitable datasets for tasks.", "motivation": "Finding appropriate datasets is crucial yet challenging in data science, as current search interfaces are restrictive and do not accommodate users' implicit preferences.", "method": "DataScout uses AI to reformulate queries, performs semantic search based on dataset content, and generates relevance indicators dynamically based on user-specified tasks.", "result": "A study with 12 participants showed that DataScout's features facilitated structured explorations and provided feedback on search queries, improving the dataset discovery process.", "conclusion": "DataScout effectively bridges the gaps in dataset discovery, allowing users to better understand the search space and enhance their search experience.", "key_contributions": ["AI-assisted query reformulations", "Semantic search and filtering based on dataset content", "Dynamic relevance indicators for datasets"], "limitations": "", "keywords": ["dataset search", "AI-assisted search", "semantic search", "data discovery", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.", "AI": {"tldr": "This study evaluates the safety of four large language model chatbots used for medical advice, revealing significant variations in the quality and safety of their responses.", "motivation": "The use of large language model chatbots in providing medical advice raises patient safety concerns that need rigorous evaluation.", "method": "This research compared the responses of four chatbots—Claude, Gemini, GPT-4o, and Llama3-70B—using HealthAdvice, a new dataset consisting of 888 responses to 222 medical questions across various primary care topics.", "result": "Statistical analysis showed significant differences among chatbots, with problematic response rates from 21.6% (Claude) to 43.2% (Llama), indicating a risk of unsafe medical advice being provided.", "conclusion": "The study underscores the necessity for improved safety measures in chatbot responses, as millions of patients could be receiving dangerous medical advice.", "key_contributions": ["Introduces a new dataset (HealthAdvice) for evaluating chatbot safety in medical contexts.", "Quantifies the rate of unsafe advice provided by various chatbots.", "Highlights significant variability in chatbot response quality, with implications for patient safety."], "limitations": "Limited to four chatbots and specific primary care topics; broader implications may require additional studies across other domains.", "keywords": ["Chatbot safety", "Medical advice", "Large language models", "Health informatics", "Patient safety"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.19026", "pdf": "https://arxiv.org/pdf/2507.19026.pdf", "abs": "https://arxiv.org/abs/2507.19026", "title": "RhythmTA: A Visual-Aided Interactive System for ESL Rhythm Training via Dubbing Practice", "authors": ["Chang Chen", "Sicheng Song", "Shuchang Xu", "Zhicheng Li", "Huamin Qu", "Yanna Lin"], "categories": ["cs.HC"], "comment": null, "summary": "English speech rhythm, the temporal patterns of stressed syllables, is\nessential for English as a second language (ESL) learners to produce\nnatural-sounding and comprehensible speech. Rhythm training is generally based\non imitation of native speech. However, it relies heavily on external\ninstructor feedback, preventing ESL learners from independent practice. To\naddress this gap, we present RhythmTA, an interactive system for ESL learners\nto practice speech rhythm independently via dubbing, an imitation-based\napproach. The system automatically extracts rhythm from any English speech and\nintroduces novel visual designs to support three stages of dubbing practice:\n(1) Synchronized listening with visual aids to enhance perception, (2) Guided\nrepeating by visual cues for self-adjustment, and (3) Comparative reflection\nfrom a parallel view for self-monitoring. Our design is informed by a formative\nstudy with nine spoken English instructors, which identified current practices\nand challenges. A user study with twelve ESL learners demonstrates that\nRhythmTA effectively enhances learners' rhythm perception and shows significant\npotential for improving rhythm production.", "AI": {"tldr": "RhythmTA is an interactive system designed to help ESL learners practice English speech rhythm independently through dubbing, using novel visual aids to enhance learning.", "motivation": "There is a need for ESL learners to practice speech rhythm independently without reliance on external instructor feedback.", "method": "RhythmTA employs a dubbing approach and consists of three stages of practice: synchronized listening with visual aids, guided repeating with visual cues, and comparative reflection for self-monitoring, based on insights from spoken English instructors.", "result": "User studies show that RhythmTA effectively improves rhythm perception in ESL learners, indicating significant potential for enhancing rhythm production.", "conclusion": "RhythmTA provides an innovative solution for ESL learners to practice speech rhythm autonomously, enhancing both comprehension and production skills.", "key_contributions": ["Introduction of an interactive rhythm training system for ESL learners", "Novel visual aids to support speech rhythm practice", "Evidence from user studies demonstrating improved rhythm skills"], "limitations": "", "keywords": ["English speech rhythm", "ESL learners", "interactive system", "dubbing practice", "visual aids"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.18910", "pdf": "https://arxiv.org/pdf/2507.18910.pdf", "abs": "https://arxiv.org/abs/2507.18910", "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "categories": ["cs.CL", "cs.LG"], "comment": "33 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems.", "AI": {"tldr": "This paper is a systematic review of Retrieval-Augmented Generation (RAG) in NLP, detailing its evolution, technical components, and applications in various domains.", "motivation": "The motivation behind RAG is to improve factual grounding and accuracy in LLMs while addressing issues like hallucinations and outdated knowledge.", "method": "The review analyzes the evolution of RAG from early open domain question answering to modern implementations, examining retrieval mechanisms, generation models, and fusion strategies with a year-by-year analysis of trends.", "result": "The study benchmarks various RAG implementations on metrics like retrieval accuracy, generation fluency, and computational efficiency, revealing persistent challenges in retrieval quality and privacy.", "conclusion": "The paper concludes by highlighting recent innovations and emerging solutions in RAG that promise to enhance the efficiency and effectiveness of NLP systems.", "key_contributions": ["Comprehensive systematic review of RAG's evolution and applications", "In-depth analysis of technical components and challenges", "Benchmarking of RAG implementations across different metrics"], "limitations": "The review may not cover all recent developments in RAG and specific case studies due to the rapidly evolving nature of the field.", "keywords": ["Retrieval-Augmented Generation", "Natural Language Processing", "Large Language Models"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.19072", "pdf": "https://arxiv.org/pdf/2507.19072.pdf", "abs": "https://arxiv.org/abs/2507.19072", "title": "Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis", "authors": ["Oliver Bates", "Christian Remy", "Kieran Cutting", "Adam Tyler", "Adrian Friday"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "summary": "What could designing for carbon reduction of heating and cooling in\ncommercial settings look like in the near future? How can we challenge dominant\nmindsets and paradigms of efficiency and behaviour change? How can we help\nbuild worlds through our practice that can become future realities? This paper\nintroduces the fictional consultancy ANCSTRL.LAB to explore opportunities for\nmaking space in research projects that can encourage more systems-oriented\ninterventions. We present a design fiction that asks `what if energy management\nand reduction practice embraced systems thinking?'. Our design fiction explores\nhow future energy consultancies could utilise systems thinking, and (more than)\nhuman centred design to re-imagine energy management practice and change\nsystems in ways that are currently unfathomable. We finish by discussing how\nLIMITS research can utilise design fiction and speculative praxis to help build\nnew material realities where more holistic perspectives, the leveraging of\nsystems change, and the imagining of post-neoliberal futures is the norm.", "AI": {"tldr": "The paper explores designing for carbon reduction in commercial energy management through a fictional consultancy, arguing for a shift towards systems-oriented practices and design fictions.", "motivation": "To challenge existing paradigms of efficiency and behaviour change in energy management and promote systems-oriented interventions.", "method": "Introduces a fictional consultancy called ANCSTRL.LAB to explore how systems thinking could transform energy management practices.", "result": "The design fiction suggests that future energy consultancies can leverage systems thinking alongside human-centered design to enable significant changes in energy management practices.", "conclusion": "Design fiction and speculative praxis can help envision and create more holistic energy management systems in future scenarios, moving towards post-neoliberal frameworks.", "key_contributions": ["Introduction of ANCSTRL.LAB as a fictional consultancy to stimulate discussions on energy management.", "Emphasis on systems thinking in designing energy solutions.", "Exploration of speculative design as a method for envisioning post-neoliberal futures."], "limitations": "", "keywords": ["energy management", "systems thinking", "design fiction", "holistic design", "behaviour change"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18915", "pdf": "https://arxiv.org/pdf/2507.18915.pdf", "abs": "https://arxiv.org/abs/2507.18915", "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "AI": {"tldr": "This paper presents a novel method for mining contextualized associations from images to generate scalable, creative captions, enhancing vision-language model capabilities.", "motivation": "To improve the understanding of creative outputs in vision-language models by providing a shared language of association, overcoming limitations of existing datasets.", "method": "The authors introduce a method that identifies and mines contextual associations for salient visual elements in images, allowing for the generation of creative captions at various abstraction levels.", "result": "They produce a dataset containing 1.7 million creative captions linked to images in MSCOCO, which have been evaluated to retain visual grounding while increasing abstraction.", "conclusion": "Fine-tuning a visual encoder on the new dataset significantly enhances performance in zero-shot image-text retrieval tasks in creative contexts like poetry and metaphor visualization.", "key_contributions": ["Developed a new method for mining contextual associations from images", "Created a dataset of 1.7 million creative captions for MSCOCO", "Demonstrated significant improvements in zero-shot retrieval in poetry and metaphor domains"], "limitations": "The effectiveness of the method may be limited to certain types of visual contexts; further evaluations in different domains may be necessary.", "keywords": ["vision-language models", "image-text retrieval", "creative captions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19094", "pdf": "https://arxiv.org/pdf/2507.19094.pdf", "abs": "https://arxiv.org/abs/2507.19094", "title": "Environmental (in)considerations in the Design of Smartphone Settings", "authors": ["Thomas Thibault", "Léa Mosesso", "Camille Adam", "Aurélien Tabard", "Anaëlle Beignon", "Nolwenn Maudet"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "Designing for sufficiency is one of many approaches that could foster more\nmoderate and sustainable digital practices. Based on the Sustainable\nInformation and Communication Technologies (ICT) and Human-Computer Interaction\n(HCI) literature, we identify five environmental settings categories. However,\nour analysis of three mobile OS and nine representative applications shows an\noverall lack of environmental concerns in settings design, leading us to\nidentify six pervasive anti-patterns. Environmental settings, where they exist,\nare set on the most intensive option by default. They are not presented as\nsuch, are not easily accessible, and offer little explanation of their impact.\nInstead, they encourage more intensive use. Based on these findings, we create\na design workbook that explores design principles for environmental settings:\npresenting the environmental potential of settings; shifting to environmentally\nneutral states; previewing effects to encourage moderate use; rethinking\ndefaults; facilitating settings access and; exploring more frugal settings.\nBuilding upon this workbook, we discuss how settings can tie individual\nbehaviors to systemic factors.", "AI": {"tldr": "The paper discusses the design of environmental settings in mobile applications and operating systems, identifying shortcomings in current practices and proposing principles for improvement.", "motivation": "To foster more moderate and sustainable digital practices through better design of environmental settings in ICT and HCI.", "method": "Analysis of three mobile operating systems and nine applications to identify environmental setting shortcomings, followed by the creation of a design workbook with principles for improved settings design.", "result": "Identified six pervasive anti-patterns in environmental settings, noting that most are set to intensive options by default with limited accessibility and explanation; established principles for better design of such settings.", "conclusion": "Improved design of environmental settings can promote moderate use by connecting individual behaviors to systemic factors.", "key_contributions": ["Identification of six anti-patterns in current environmental settings design.", "Creation of a design workbook offering principles for sustainable digital practice.", "Proposal of actionable steps to improve accessibility and usability of environmental settings."], "limitations": "Focuses solely on mobile OS and applications; may not generalize to all ICT domains.", "keywords": ["Sustainable ICT", "Human-Computer Interaction", "Environmental settings", "Mobile applications", "Design principles"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18918", "pdf": "https://arxiv.org/pdf/2507.18918.pdf", "abs": "https://arxiv.org/abs/2507.18918", "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders", "authors": ["Richmond Sin Jing Xuan", "Jalil Huseynov", "Yang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance.", "AI": {"tldr": "This paper analyzes activation disparities in multilingual LLMs, focusing on low and medium resource languages and proposes an activation-aware fine-tuning method to enhance performance.", "motivation": "To investigate the underperformance of medium to low resource languages in multilingual LLM benchmarks and identify activation patterns that contribute to these disparities.", "method": "Activation patterns in the Gemma-2-2B LLM were analyzed using Sparse Autoencoders across 10 languages, followed by activation-aware fine-tuning through Low-Rank Adaptation to improve performance.", "result": "Medium to low resource languages showed significantly lower activations, yet after applying fine-tuning, substantial activation gains were observed, leading to modest improvements in benchmark results.", "conclusion": "Activation alignment is critical for improving the performance of multilingual LLMs, particularly for medium to low resource languages.", "key_contributions": ["Identification of activation disparities in multilingual LLMs across different languages", "Development of an activation-aware fine-tuning method using LoRA", "Demonstration of improved multilingual performance post fine-tuning"], "limitations": "", "keywords": ["Multilingual LLMs", "Activation Patterns", "Fine-tuning", "Low-Rank Adaptation", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19104", "pdf": "https://arxiv.org/pdf/2507.19104.pdf", "abs": "https://arxiv.org/abs/2507.19104", "title": "A systematic literature review to unveil users objective reaction to virtual experiences: Complemented with a conceptual model (QoUX in VE)", "authors": ["Alireza Mortezapour", "Andrea Antonio Cantone", "Monica Maria Lucia Sebillo", "Giuliana Vitiello"], "categories": ["cs.HC"], "comment": null, "summary": "In pursuit of documenting users Neurophysiological responses during\nexperiencing virtual environments (VE), this systematic review presents a novel\nconceptual model of UX in VE. Searching across seven databases yielded to 1743\narticles. Rigorous screenings, included only 66 articles. Notably, UX in VE\nlacks a consensus definition. Obviously, this UX has many unique sub-dimensions\nthat are not mentioned in other products. The presented conceptual model\ncontains 26 subdimensions which mostly not supported in previous subjective\ntools and questionnaires. While EEG and ECG were common, brain ultrasound,\nemployed in one study, highlights the need for using neurophysiological\nassessments to comprehensively grasp immersive UX intricacies.", "AI": {"tldr": "This systematic review introduces a conceptual model for user experience (UX) in virtual environments (VE), identifying 26 unique sub-dimensions based on a review of 66 studies.", "motivation": "To document users' neurophysiological responses in virtual environments and address the lack of a consensus definition for UX in VE.", "method": "Conducted a systematic review of literature across seven databases, screening 1743 articles down to 66 relevant studies.", "result": "The conceptual model reveals 26 sub-dimensions of UX in VE, many of which are unsupported by existing subjective tools, emphasizing the use of neurophysiological assessments.", "conclusion": "Neurophysiological measures like EEG and ECG are common, but novel techniques like brain ultrasound can enhance the understanding of immersive UX.", "key_contributions": ["Introduces a novel conceptual model of UX in virtual environments", "Identifies 26 unique sub-dimensions of UX", "Highlights the importance of neurophysiological assessments in understanding immersive experiences."], "limitations": "The review only included a small subset of studies (66 out of 1743) and lacks a consensus definition of UX in VE.", "keywords": ["User Experience", "Virtual Environments", "Neurophysiological Assessments", "EEG", "ECG"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.18940", "pdf": "https://arxiv.org/pdf/2507.18940.pdf", "abs": "https://arxiv.org/abs/2507.18940", "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Yujun Cai", "Linzhuang Sun", "Xiangxiang Zhang", "Gaowei Wu", "Bihui Yu"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal Machine Translation (MMT) enhances translation quality by\nincorporating visual context, helping to resolve textual ambiguities. While\nexisting MMT methods perform well in bilingual settings, extending them to\nmultilingual translation remains challenging due to cross-lingual interference\nand ineffective parameter-sharing strategies. To address this, we propose\nLLaVA-NeuMT, a novel multimodal multilingual translation framework that\nexplicitly models language-specific and language-agnostic representations to\nmitigate multilingual interference. Our approach consists of a layer selection\nmechanism that identifies the most informative layers for different language\npairs and a neuron-level adaptation strategy that dynamically selects\nlanguage-specific and agnostic neurons to improve translation quality while\nreducing redundancy. We conduct extensive experiments on the M3-Multi30K and\nM3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only\n40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further\nprovides insights into the importance of selected layers and neurons in\nmultimodal multilingual adaptation, offering an efficient and scalable solution\nto cross-lingual adaptation in multimodal translation.", "AI": {"tldr": "LLaVA-NeuMT is a novel multimodal multilingual translation framework that improves translation quality by modeling language-specific and language-agnostic representations, effectively mitigating multilingual interference.", "motivation": "To enhance Multimodal Machine Translation (MMT) quality in multilingual settings, addressing issues like cross-lingual interference and ineffective parameter-sharing strategies.", "method": "The framework includes a layer selection mechanism for identifying informative layers and a neuron-level adaptation strategy for selecting language-specific and agnostic neurons.", "result": "LLaVA-NeuMT, fine-tuning only 40% of the model parameters, surpasses full fine-tuning approaches, achieving state-of-the-art results on the M3-Multi30K and M3-AmbigCaps datasets.", "conclusion": "The approach provides an efficient and scalable solution to cross-lingual adaptation in multimodal translation, demonstrating the importance of selected layers and neurons in improving translation quality.", "key_contributions": ["Introduction of the LLaVA-NeuMT framework for multimodal multilingual translation.", "Development of a layer selection mechanism and neuron-level adaptation strategy.", "Achievement of state-of-the-art results with less parameter fine-tuning."], "limitations": "", "keywords": ["multimodal translation", "multilingual settings", "machine learning", "language-specific representation", "cross-lingual adaptation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.19114", "pdf": "https://arxiv.org/pdf/2507.19114.pdf", "abs": "https://arxiv.org/abs/2507.19114", "title": "A Therapeutic Role-Playing VR Game for Children with Intellectual Disabilities", "authors": ["Santiago Berrezueta-Guzman", "WenChun Chen", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Paper accepted for publication and presentation in the 3rd Annual\n  IEEE International Conference on Metaverse Computing, Networking, and\n  Applications (IEEE MetaCom 2025) will be held in Sejong University, Seoul,\n  Republic of Korea, on August 27 - 29, 2025", "summary": "Virtual Reality (VR) offers promising avenues for innovative therapeutic\ninterventions in populations with intellectual disabilities (ID). This paper\npresents the design, development, and evaluation of Space Exodus, a novel\nVR-based role-playing game specifically tailored for children with ID. By\nintegrating immersive gameplay with therapeutic task design, Space Exodus aims\nto enhance concentration, cognitive processing, and fine motor skills through\nstructured hand-eye coordination exercises. A six-week pre-test/post-test study\nwas conducted with 16 children in Ecuador, using standardized assessments, the\nToulouse-Pieron Cancellation Test, and the Moss Attention Rating Scale\ncomplemented by detailed observational metrics. Quantitative results indicate\nstatistically significant improvements in concentration scores, with test\nscores increasing from 65.2 to 80.3 and 55.4 to 68.7, respectively (p < 0.01).\nQualitative observations revealed reduced task attempts, enhanced user\nconfidence, and increased active participation. The inclusion of a VR assistant\nprovided consistent guidance that further boosted engagement. These findings\ndemonstrate the potential of immersive, game-based learning environments as\npractical therapeutic tools, laying a robust foundation for developing\ninclusive and adaptive rehabilitation strategies for children with ID.", "AI": {"tldr": "Space Exodus is a VR role-playing game designed for children with intellectual disabilities, aiming to enhance cognitive skills through gameplay. A study showed significant improvements in concentration.", "motivation": "To explore innovative therapeutic interventions for children with intellectual disabilities using Virtual Reality.", "method": "A six-week pre-test/post-test study was conducted with 16 children in Ecuador, using standardized assessments and observational metrics to measure improvements in concentration and motor skills.", "result": "Statistically significant improvements were observed in concentration scores, with test scores increasing from 65.2 to 80.3 and 55.4 to 68.7 (p < 0.01). Qualitative data indicated reduced task attempts and increased user confidence and engagement.", "conclusion": "Immersive, game-based learning environments like Space Exodus show potential as therapeutic tools, paving the way for adaptive rehabilitation strategies for children with ID.", "key_contributions": ["Design and evaluation of a VR-based therapeutic intervention for ID", "Evidence of significant improvement in cognitive skills through gameplay", "Insights into the role of a VR assistant in enhancing engagement"], "limitations": "", "keywords": ["Virtual Reality", "Therapeutic interventions", "Intellectual disabilities", "Cognitive processing", "Game-based learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18952", "pdf": "https://arxiv.org/pdf/2507.18952.pdf", "abs": "https://arxiv.org/abs/2507.18952", "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "categories": ["cs.CL"], "comment": null, "summary": "Legal document summarization represents a significant advancement towards\nimproving judicial efficiency through the automation of key information\ndetection. Our approach leverages state-of-the-art natural language processing\ntechniques to meticulously identify and extract essential data from extensive\nlegal texts, which facilitates a more efficient review process. By employing\nadvanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate\nthe crucial elements. This automation alleviates the burden on legal\nprofessionals, concurrently reducing the likelihood of overlooking vital\ninformation that could lead to errors. Through comprehensive experiments\nconducted with actual legal datasets, we demonstrate the capability of our\nmethod to generate high-quality summaries while preserving the integrity of the\noriginal content and enhancing processing times considerably. The results\nreveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and\ndecision-making activities instead of manual reviews. This research highlights\npromising technology-driven strategies that can significantly alter workflow\ndynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes.", "AI": {"tldr": "This paper presents a method for legal document summarization using advanced NLP and ML techniques to automate data extraction, improving judicial efficiency.", "motivation": "To enhance judicial efficiency by automating the detection and extraction of key information from legal texts.", "method": "The framework employs state-of-the-art NLP and machine learning algorithms to recognize patterns in judicial documents and generate precise summaries.", "result": "Experiments on actual legal datasets show that the method creates high-quality summaries that preserve the integrity of the original content and improve processing times.", "conclusion": "The research demonstrates that automation can significantly improve workflow dynamics in the legal sector, allowing legal practitioners to focus on critical analysis and decision-making.", "key_contributions": ["Introduction of a novel framework for legal document summarization", "Demonstration of improved operational efficiency through automated summarization", "Validation of the approach using real legal datasets"], "limitations": "", "keywords": ["legal document summarization", "natural language processing", "machine learning", "judicial efficiency", "automation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.19193", "pdf": "https://arxiv.org/pdf/2507.19193.pdf", "abs": "https://arxiv.org/abs/2507.19193", "title": "Where are the Frontlines? A Visualization Approach for Map Control in Team-Based Games", "authors": ["Jonas Peché", "Aliaksei Tsishurou", "Alexander Zap", "Guenter Wallner"], "categories": ["cs.HC"], "comment": null, "summary": "A central area of interest in many competitive online games is spatial\nbehavior which due to its complexity can be difficult to visualize. Such\nbehaviors of interest include not only overall movement patterns but also being\nable to understand which player or team is exerting control over an area to\ninform decision-making. Map control can, however, be challenging to quantify.\nIn this paper, we propose a method for calculating frontlines and first efforts\ntowards a visualization of them. The visualization can show map control and\nfrontlines at a specific time point or changes of these over time. For this\npurpose, it utilizes support vector machines to derive frontlines from unit\npositions. We illustrate our algorithm and visualization with examples based on\nthe team-based online game World of Tanks.", "AI": {"tldr": "The paper presents a method for visualizing spatial behavior in competitive online games, focusing on map control and frontlines using support vector machines.", "motivation": "Understanding spatial behavior in competitive online games is crucial for decision-making, but quantifying map control is challenging.", "method": "The proposed method calculates frontlines from unit positions using support vector machines and creates visualizations to depict map control at specific time points or changes over time.", "result": "The algorithm effectively illustrates frontlines and map control in the game World of Tanks with visual examples.", "conclusion": "The visualization technique aids in comprehending spatial control dynamics in team-based online games, contributing to better strategic decisions.", "key_contributions": ["Method for calculating frontlines in online games", "Visualization of map control dynamics", "Application of support vector machines for spatial behavior analysis"], "limitations": "", "keywords": ["visualization", "map control", "frontlines", "online gaming", "support vector machines"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18956", "pdf": "https://arxiv.org/pdf/2507.18956.pdf", "abs": "https://arxiv.org/abs/2507.18956", "title": "A Similarity Measure for Comparing Conversational Dynamics", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL"], "comment": "Code and demos available in ConvoKit (https://convokit.cornell.edu/)", "summary": "The quality of a conversation goes beyond the individual quality of each\nreply, and instead emerges from how these combine into interactional patterns\nthat give the conversation its distinctive overall \"shape\". However, there is\nno robust automated method for comparing conversations in terms of their\noverall interactional dynamics. Such methods could enhance the analysis of\nconversational data and help evaluate conversational agents more holistically.\n  In this work, we introduce a similarity measure for comparing conversations\nwith respect to their dynamics. We design a validation framework for testing\nthe robustness of the metric in capturing differences in conversation dynamics\nand for assessing its sensitivity to the topic of the conversations. Finally,\nto illustrate the measure's utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of\nsituational power in conversations.", "AI": {"tldr": "This paper introduces a new similarity measure for comparing the dynamics of conversations, highlighting the need for a robust method to analyze conversational data more holistically.", "motivation": "To address the lack of automated methods for comparing conversations based on their interactional dynamics, which can enhance the evaluation of conversational agents.", "method": "The authors propose a similarity measure for conversations and design a validation framework to test its robustness and sensitivity to conversation topics.", "result": "The similarity measure successfully captured differences in conversation dynamics and was applied to analyze conversational patterns in an online community, illustrating its effectiveness.", "conclusion": "The study highlights the importance of situational power in conversations and demonstrates that the proposed measure provides valuable insights into interactional dynamics.", "key_contributions": ["Introduction of a similarity measure for conversation dynamics.", "Development of a validation framework for metric assessment.", "Analysis of conversational patterns in online communities."], "limitations": "The study may be limited by the specific contexts of the conversations analyzed and the generalizability of the findings to other types of interactions.", "keywords": ["conversation dynamics", "similarity measure", "conversational agents", "interactional patterns", "situational power"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19376", "pdf": "https://arxiv.org/pdf/2507.19376.pdf", "abs": "https://arxiv.org/abs/2507.19376", "title": "Archiverse: an Approach for Immersive Cultural Heritage", "authors": ["Wieslaw Kopeć", "Anna Jaskulska", "Władysław Fuchs", "Wiktor Stawski", "Stanisław Knapiński", "Barbara Karpowicz", "Rafał Masłyk"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Digital technologies and tools have transformed the way we can study cultural\nheritage and the way we can recreate it digitally. Techniques such as laser\nscanning, photogrammetry, and a variety of Mixed Reality solutions have enabled\nresearchers to examine cultural objects and artifacts more precisely and from\nnew perspectives. In this part of the panel, we explore how Virtual Reality\n(VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the\nremains of historical cultural heritage and experience it in simulations of its\noriginal complexity, which means immersive and interactive. Visualization of\nmaterial culture exemplified by archaeological sites and architecture can be\nparticularly useful when only ruins or archaeological remains survive. However,\nthese advancements also bring significant challenges, especially in the area of\ntransdisciplinary cooperation between specialists from many, often distant,\nfields, and the dissemination of virtual immersive environments among both\nprofessionals and the general public.", "AI": {"tldr": "The paper discusses the impact of digital technologies, especially VR and XR, on the study and visualization of cultural heritage, while highlighting both the benefits and challenges involved.", "motivation": "To explore how digital technologies can aid in the study and recreation of cultural heritage.", "method": "The paper discusses the use of laser scanning, photogrammetry, and Mixed Reality solutions to analyze and visualize cultural artifacts and sites.", "result": "Virtual and eXtended Reality can recreate and simulate historical cultural heritage in an immersive and interactive manner, enhancing understanding.", "conclusion": "While these technologies provide valuable tools for visualization, they also present challenges in transdisciplinary collaboration and public engagement.", "key_contributions": ["Examination of digital tools in cultural heritage visualization", "Discussion on transdisciplinary cooperation", "Insights on engaging the public with VR and XR"], "limitations": "Challenges in cooperation among diverse specialists and dissemination to the public.", "keywords": ["Digital Heritage", "Virtual Reality", "eXtended Reality", "Cultural Visualization", "Transdisciplinary Cooperation"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.18973", "pdf": "https://arxiv.org/pdf/2507.18973.pdf", "abs": "https://arxiv.org/abs/2507.18973", "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "authors": ["Bohan Yao", "Vikas Yadav"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "AI": {"tldr": "This paper introduces Multi-TAG, a framework for improving mathematical reasoning in LLMs by concurrently using multiple tools, leading to enhanced accuracy and robustness without the need for finetuning.", "motivation": "Existing tool-augmented methods for LLMs struggle with complex math reasoning due to their reliance on a single tool, necessitating a more robust approach.", "method": "Multi-TAG guides an LLM to invoke multiple tools at each reasoning step and aggregates their outputs to enhance solution accuracy.", "result": "Multi-TAG shows consistent improvements over state-of-the-art methods on challenging math benchmarks, achieving 6.0% to 7.5% higher performance.", "conclusion": "The framework's finetuning-free nature allows it to be applied easily across various LLMs, making it a flexible solution for improving mathematical problem-solving.", "key_contributions": ["Introduction of a multi-tool aggregation approach for LLMs", "Improved accuracy and robustness in complex mathematical reasoning", "No finetuning required for implementation across different LLM backbones"], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Multi-Tool Aggregation", "Performance Improvement", "Inference-only Framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19466", "pdf": "https://arxiv.org/pdf/2507.19466.pdf", "abs": "https://arxiv.org/abs/2507.19466", "title": "Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions", "authors": ["Aliaksandr Marozau", "Barbara Karpowicz", "Tomasz Kowalewski", "Pavlo Zinevych", "Wiktor Stawski", "Adam Kuzdraliński", "Wiesław Kopeć"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR,\nAR) are well established in medical practice, enhancing diagnostics, treatment,\nand education. However, there are still some limitations and challenges that\nmay be overcome thanks to the latest generations of equipment, software, and\nframeworks based on eXtended Reality (XR) by enabling immersive systems that\nsupport safer, more controlled environments for training and patient care. Our\nreview highlights recent VR and AR applications in key areas of medicine. In\nmedical education, these technologies provide realistic clinical simulations,\nimproving skills and knowledge retention. In surgery, immersive tools enhance\nprocedural precision with detailed anatomical visualizations. VR-based\nrehabilitation has shown effectiveness in restoring motor functions and\nbalance, particularly for neurological patients. In mental health, VR has been\nsuccessful in treating conditions like PTSD and phobias. Although VR and AR\nsolutions are well established, there are still some important limitations,\nincluding high costs and limited tactile feedback, which may be overcome with\nimplementing new technologies that may improve the effectiveness of immersive\nmedical applications such as XR, psychophysiological feedback or integration of\nartificial intelligence (AI) for real-time data analysis and personalized\nhealthcare and training.", "AI": {"tldr": "This paper reviews the applications of Mixed Reality (MR) technologies in medicine, particularly focusing on Virtual and Augmented Reality (VR/AR) in medical education, surgery, rehabilitation, and mental health treatment.", "motivation": "To address limitations and challenges in medical practice and education using Mixed Reality technologies and to explore their potential for improving healthcare outcomes.", "method": "A comprehensive review of recent applications and advancements in VR and AR technologies within key areas of medicine.", "result": "The review identifies successful applications of VR/AR in medical education, surgical precision, rehabilitation for neurological patients, and treatment of mental health conditions. It also discusses the limitations such as cost and lack of tactile feedback.", "conclusion": "While VR and AR are effective in various medical fields, advancements such as the integration of AI and new technologies are necessary to enhance their effectiveness and accessibility in healthcare.", "key_contributions": ["Identification of VR/AR applications improving medical education and procedural accuracy", "Highlighting the effectiveness of VR in rehabilitation and mental health treatments", "Discussion on overcoming current limitations with emerging technologies and AI integration"], "limitations": "High costs and limited tactile feedback for VR/AR applications in medicine.", "keywords": ["Mixed Reality", "Virtual Reality", "Augmented Reality", "medical education", "healthcare"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19081", "pdf": "https://arxiv.org/pdf/2507.19081.pdf", "abs": "https://arxiv.org/abs/2507.19081", "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy.", "AI": {"tldr": "This paper proposes Arg-LLaDA, an iterative framework for argument summarization that improves the generation of concise and structured debate representations by addressing the limitations of existing methods.", "motivation": "The generation stage of argument summarization has been underexplored, particularly in supporting factual correction and structural refinement.", "method": "Arg-LLaDA utilizes a large language diffusion framework that iteratively improves argument summaries through sufficiency-guided remasking and regeneration, using a flexible masking controller and a sufficiency-checking module.", "result": "On two benchmark datasets, Arg-LLaDA outperforms state-of-the-art baselines in 7 out of 10 automatic evaluation metrics, and human evaluations indicate significant improvements in coverage, faithfulness, and conciseness.", "conclusion": "The iterative, sufficiency-aware generation strategy of Arg-LLaDA validates its effectiveness in enhancing the quality of argument summaries.", "key_contributions": ["Introduction of Arg-LLaDA, a novel framework for argument summarization.", "Utilization of sufficiency-guided remasking and regeneration for iterative improvement.", "Empirical validation of performance surpassing state-of-the-art baselines and positive human evaluation results."], "limitations": "", "keywords": ["argument summarization", "large language model", "iteration", "sufficiency", "remasking"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.19479", "pdf": "https://arxiv.org/pdf/2507.19479.pdf", "abs": "https://arxiv.org/abs/2507.19479", "title": "IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home", "authors": ["Wiesław Kopeć", "Jarosław Kowalski", "Aleksander Majda", "Anna Duszyk-Bogorodzka", "Anna Jaskulska", "Cezary Biele"], "categories": ["cs.HC"], "comment": null, "summary": "We report preliminary insights from an exploratory study on non-standard\nnon-invasive interfaces for Smart Home Technologies (SHT). This study is part\nof a broader research project on effective Smart Home ecosystem Sagacity that\nwill target older adults, impaired persons, and other groups disadvantaged in\nthe main technology discourse. Therefore, this research is in line with a\nlong-term research framework of the HASE research group (Human Aspects in\nScience and Engineering) by the Living Lab Kobo. In our study, based on the\nprototype of the comprehensive SHT management system Sagacity, we investigated\nthe potential of bioelectric signals, in particular EMG and EOG as a\ncomplementary interface for SHT. Based on our previous participatory research\nand studies on multimodal interfaces, including VUI and BCI, we prepared an\nin-depth interactive hands-on experience workshops with direct involvement of\nvarious groups of potential end users, including older adults and impaired\npersons (total 18 subjects) to explore and investigate the potential of\nsolutions based on this type of non-standard interfaces. The preliminary\ninsights from the study unveil the potential of EMG/EOG interfaces in\nmultimodal SHT management, alongside limitations and challenges stemming from\nthe current state of technology and recommendations for designing multimodal\ninteraction paradigms pinpointing areas of interest to pursue in further\nstudies.", "AI": {"tldr": "Exploratory study on non-invasive interfaces for Smart Home Technologies targeting marginalized groups.", "motivation": "To explore new non-standard interfaces for Smart Home Technologies that cater to older adults and impaired persons.", "method": "Investigated the use of bioelectric signals (EMG and EOG) as interfaces through participatory workshops with potential end users.", "result": "Preliminary insights reveal EMG/EOG's potential in multimodal Smart Home management, along with identified limitations and challenges.", "conclusion": "The study suggests designing multimodal interaction paradigms and highlights key areas for future research.", "key_contributions": ["Exploration of EMG/EOG as interfaces for Smart Home Technologies", "Engagement of marginalized user groups in design process", "Recommendations for improving multimodal interaction in Smart Homes"], "limitations": "Challenges from the current state of technology and user adaptation to new interfaces.", "keywords": ["Smart Home Technologies", "non-invasive interfaces", "human-computer interaction"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.19090", "pdf": "https://arxiv.org/pdf/2507.19090.pdf", "abs": "https://arxiv.org/abs/2507.19090", "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "categories": ["cs.CL"], "comment": null, "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.", "AI": {"tldr": "DebateCV introduces a debate-driven framework for claim verification using multiple LLM agents to enhance performance.", "motivation": "Enhancing digital literacy through effective claim verification, addressing the limitations of single-LLM methods in handling complex claims.", "method": "A debate-driven methodology with two Debaters arguing opposing views and a Moderator evaluating the arguments, supplemented by synthetic debate data for improvement.", "result": "DebateCV outperforms existing claim verification methods across various levels of evidence quality.", "conclusion": "The proposed framework effectively addresses the challenges of claim verification through multi-agent interaction and novel training strategies.", "key_contributions": ["First debate-driven claim verification framework using multiple LLM agents", "Introduction of a Moderator role for evaluating debates", "Innovative post-training strategy using synthetic data to enhance model performance"], "limitations": "", "keywords": ["claim verification", "debate-driven methodology", "large language models", "digital literacy", "synthetic data"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19117", "pdf": "https://arxiv.org/pdf/2507.19117.pdf", "abs": "https://arxiv.org/abs/2507.19117", "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations", "authors": ["Swapnil Hingmire", "Ze Shi Li", "Shiyu", "Zeng", "Ahmed Musa Awon", "Luiz Franciscatto Guerra", "Neil Ernst"], "categories": ["cs.CL"], "comment": "Accepted for publication at the Transactions of ACL (TACL) (pre-MIT\n  Press publication version)", "summary": "Interpretation of topics is crucial for their downstream applications.\nState-of-the-art evaluation measures of topic quality such as coherence and\nword intrusion do not measure how much a topic facilitates the exploration of a\ncorpus. To design evaluation measures grounded on a task, and a population of\nusers, we do user studies to understand how users interpret topics. We propose\nconstructs of topic quality and ask users to assess them in the context of a\ntopic and provide rationale behind evaluations. We use reflexive thematic\nanalysis to identify themes of topic interpretations from rationales. Users\ninterpret topics based on availability and representativeness heuristics rather\nthan probability. We propose a theory of topic interpretation based on the\nanchoring-and-adjustment heuristic: users anchor on salient words and make\nsemantic adjustments to arrive at an interpretation. Topic interpretation can\nbe viewed as making a judgment under uncertainty by an ecologically rational\nuser, and hence cognitive biases aware user models and evaluation frameworks\nare needed.", "AI": {"tldr": "This paper explores how users interpret topics in order to improve evaluation measures of topic quality, proposing new constructs based on user studies.", "motivation": "State-of-the-art evaluation measures for topic quality do not address how topics facilitate exploration in user contexts. Understanding user interpretations of topics can enhance these measures.", "method": "The authors conducted user studies to gather user assessments of topic constructs, employing reflexive thematic analysis to identify interpretation themes.", "result": "It was found that users rely on availability and representativeness heuristics for topic interpretation, suggesting the need for user models that account for cognitive biases.", "conclusion": "The research concluded that topic interpretation involves judgment under uncertainty and that user models should incorporate cognitive biases to improve evaluation frameworks.", "key_contributions": ["Proposed new constructs of topic quality based on user interpretations", "Demonstrated that users use cognitive heuristics in topic interpretation", "Introduced a theory linking topic interpretation to cognitive biases."], "limitations": "The study is based on user studies that may not encompass all demographics or contexts; thus, results may not generalize universally.", "keywords": ["topic interpretation", "user studies", "cognitive biases", "evaluation measures", "heuristics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.", "AI": {"tldr": "This study evaluates the safety of four LLM chatbots in providing medical advice, revealing significant variation in problematic responses and the potential for patient harm.", "motivation": "To assess the safety of LLM chatbots used by millions for medical advice, addressing concerns about patient safety.", "method": "A red-teaming study comparing the responses of Claude, Gemini, GPT-4o, and Llama3-70B using a new dataset (HealthAdvice) and an evaluative framework for both quantitative and qualitative analysis.", "result": "Evaluation of 888 responses to 222 medical advice-seeking questions showed problematic response rates ranging from 21.6% to 43.2% across chatbots; unsafe response rates ranged from 5% to 13%.", "conclusion": "The findings indicate a pressing need for improvements in the clinical safety of LLM chatbots to prevent potential patient harm.", "key_contributions": ["Comparison of four major LLM chatbots in a medical context.", "Quantitative and qualitative dataset of chatbot responses to patient inquiries.", "Identification of safety issues related to chatbot medical advice."], "limitations": "Focused solely on four chatbots and specific medical topics; may not generalize to all LLMs or medical contexts.", "keywords": ["Large Language Models", "Chatbots", "Medical Safety", "Patient Advice", "AI in Health"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.19156", "pdf": "https://arxiv.org/pdf/2507.19156.pdf", "abs": "https://arxiv.org/abs/2507.19156", "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.", "AI": {"tldr": "This study investigates gender and professional bias in Large Language Models (LLMs) by analyzing their responses to ungendered prompts using a structured experimental method.", "motivation": "The paper addresses the growing concern about biases in AI-generated content, particularly in professional contexts, and highlights the need for understanding these biases in non-English languages.", "method": "A structured experimental method was employed using different ungendered prompts related to three professional job combinations, with responses collected from OpenAI ChatGPT and Google Gemini via APIs.", "result": "The study found that LLM responses often perpetuate gender stereotypes, with analysis revealing that Gemini assigned 'she' pronouns to the 'assistant' role 100% of the time, compared to 97% for ChatGPT in similar contexts.", "conclusion": "The findings raise ethical concerns about the implications of biased AI-generated text in areas like employment, underscoring the necessity for mitigation strategies to promote equitable outcomes.", "key_contributions": ["Analyzes bias in LLM responses to ungendered prompts", "Demonstrates significant gender bias linked to professional roles in AI-generated content", "Highlights limitations of LLMs in non-English contexts, particularly regarding grammatical gender"], "limitations": "The study is limited to two LLMs and focuses on the Italian language, which may restrict the generalizability of the results.", "keywords": ["Large Language Models", "Bias", "Gender Stereotypes", "AI Ethics", "Job Roles"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.19195", "pdf": "https://arxiv.org/pdf/2507.19195.pdf", "abs": "https://arxiv.org/abs/2507.19195", "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "AI": {"tldr": "The study investigates how dialectal variation affects the toxicity of language model outputs, revealing increased toxicity for AAVE due to data poisoning.", "motivation": "To understand the impact of dialectal variation on toxicity in language models, specifically how data poisoning affects outputs in AAVE versus SAE.", "method": "The study utilized small- and medium-scale LLaMA models and assessed the toxicity of outputs based on exposure to poisoned data. GPT-4o was used to analyze disparities in output fairness.", "result": "Minimal exposure to poisoned data significantly increases toxicity in outputs for AAVE inputs, especially in larger models which exhibit a heightened amplification of these biases.", "conclusion": "The findings highlight the impact of dialectal bias and data poisoning, advocating for improved evaluation and debiasing methods in language model training.", "key_contributions": ["Identification of increased toxicity in AAVE due to data poisoning", "Demonstration of heightened susceptibility in larger language models", "Recommendations for dialect-aware evaluation and debiasing interventions"], "limitations": "Limited to analysis of AAVE and SAE without broader dialectal context.", "keywords": ["Social Bias", "Dialectal Variation", "Data Poisoning", "Language Models", "Bias Mitigation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.19156", "pdf": "https://arxiv.org/pdf/2507.19156.pdf", "abs": "https://arxiv.org/abs/2507.19156", "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.", "AI": {"tldr": "This study investigates how LLMs can perpetuate gender and professional biases in their responses to ungendered prompts, using a structured experimental method with Italian prompts and analyzing performance of ChatGPT and Gemini.", "motivation": "To examine the perpetuation of biases in AI-generated content, particularly focusing on gender and professional stereotypes.", "method": "The study employs a structured experimental method, utilizing different ungendered prompts featuring three distinct professional job combinations characterized by hierarchical relationships, analyzing the outputs of OpenAI ChatGPT and Google Gemini.", "result": "The results show that LLMs significantly perpetuate biases, with Gemini associating 'she' pronouns 100% with the 'assistant' role instead of 'manager' (ChatGPT at 97%).", "conclusion": "The findings reveal the significant ethical implications of bias in AI-generated content, stressing the need for developing mitigation strategies to ensure equitable AI outcomes.", "key_contributions": ["Examination of bias in LLM responses to ungendered prompts across languages", "Demonstration of bias perpetuation in job role associations", "Highlighting ethical concerns regarding AI applications in workplaces."], "limitations": "Focus is primarily on gender bias in Italian; results may not generalize to other languages or cultural contexts.", "keywords": ["Large Language Models", "Bias", "Gender", "Chatbots", "AI Ethics"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.19219", "pdf": "https://arxiv.org/pdf/2507.19219.pdf", "abs": "https://arxiv.org/abs/2507.19219", "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework", "authors": ["Zi Liang", "Liantong Yu", "Shiyu Zhang", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.CR"], "comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/", "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.", "AI": {"tldr": "This paper introduces ArxivRoll, a framework to evaluate large language models (LLMs) by addressing overestimation issues related to public benchmarks through a dynamic generation of private test cases and measurement of contamination and bias.", "motivation": "Overestimation in evaluating LLMs due to benchmark contamination and imbalanced training has led to unfair comparisons and unrealistic performance assessments.", "method": "ArxivRoll consists of SCP (an automated generator for private test cases) and Rugged Scores (metrics to measure benchmark contamination and training bias), creating a new benchmark every six months using recent ArXiv articles.", "result": "Extensive experiments show ArxivRoll generates high-quality benchmarks and allows for a systematic evaluation of current LLMs.", "conclusion": "ArxivRoll provides a more reliable evaluation method for LLMs, enhancing reproducibility and transparency while ensuring efficiency.", "key_contributions": ["Introduction of a dynamic evaluation framework for LLMs", "Automated generation of private test cases using SCP", "Development of Rugged Scores for measuring benchmark contamination and training bias"], "limitations": "", "keywords": ["large language models", "evaluation framework", "benchmark contamination", "machine learning", "health informatics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19227", "pdf": "https://arxiv.org/pdf/2507.19227.pdf", "abs": "https://arxiv.org/abs/2507.19227", "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation", "authors": ["Yuanhe Zhang", "Fangzhou Xie", "Zhenhong Zhou", "Zherui Li", "Hao Chen", "Kun Wang", "Yufei Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.", "AI": {"tldr": "This paper reveals vulnerabilities of Large Language Diffusion Models (LLDMs) to jailbreaking attacks and presents a new method called Parallel Decoding jailbreak (PAD) that increases harmful output generation rates.", "motivation": "There are growing concerns about harmful generations from LLDMs and existing jailbreak methods for LLMs are ineffective against them.", "method": "The paper introduces a PArallel Decoding jailbreak (PAD) method that utilizes Multi-Point Attention Attack to guide the generative processes of LLDMs towards harmful outputs.", "result": "The PAD method achieves a 97% success rate in jailbreaking attempts, demonstrating significant safety vulnerabilities in LLDMs and doubling the speed of harmful generation compared to autoregressive LLMs of the same size.", "conclusion": "LLDMs require critical insights into their architecture for secure deployment due to their rapid and vulnerable harmful generation capabilities.", "key_contributions": ["Introduced a new jailbreak method (PAD) for LLDMs.", "Demonstrated that LLDMs are significantly more susceptible to harmful output generation compared to LLMs.", "Provided a comprehensive analysis of LLDM architecture for secure deployment."], "limitations": "", "keywords": ["Large Language Diffusion Models", "jailbreak", "safety vulnerabilities", "harmful generation", "Multi-Point Attention Attack"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19303", "pdf": "https://arxiv.org/pdf/2507.19303.pdf", "abs": "https://arxiv.org/abs/2507.19303", "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns", "authors": ["Ilias Chalkidis", "Stephanie Brandl", "Paris Aslanidis"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.", "AI": {"tldr": "This paper explores the ability of Large Language Models (LLMs) to identify and classify nuances of populism in political discourse using curated datasets.", "motivation": "Examine whether LLMs can effectively identify and classify complex forms of populism in discourse.", "method": "Curating novel datasets focused on populist discourse, evaluating pre-trained language models across various prompting methods.", "result": "A fine-tuned RoBERTa classifier significantly outperformed instruction-tuned LLMs in identifying populist rhetoric; instruction-tuned models showed better performance on out-of-domain data.", "conclusion": "Fine-tuning improves classification performance, and instruction-tuned models demonstrate robustness across different political contexts.", "key_contributions": ["Development of novel datasets for populist discourse", "Evaluation of performance differences among LLMs in this domain", "Insights into populist rhetoric through analysis of campaign speeches"], "limitations": "", "keywords": ["Large Language Models", "populism", "political discourse", "RoBERTa", "cross-domain analysis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.19470", "pdf": "https://arxiv.org/pdf/2507.19470.pdf", "abs": "https://arxiv.org/abs/2507.19470", "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.HC"], "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses.", "AI": {"tldr": "This paper presents a uniform evaluation framework for the Conversations Gone Awry (CGA) task, which aims to predict the derailment of conversations, providing a benchmark for comparing models.", "motivation": "To improve automated systems' ability to predict conversation directions and assist in human-human interactions.", "method": "The authors introduce a benchmark framework for the CGA task, allowing for direct model comparisons and incorporating a new metric for forecasting adaptation during conversations.", "result": "The framework provides a comprehensive overview of progress in CGA models and their predictive capabilities in light of recent language modeling advancements.", "conclusion": "The study fosters improvements in CGA models, allowing for better predictions of conversation derailment through an innovative evaluation method.", "key_contributions": ["Introduced the first uniform evaluation framework for CGA task.", "Created a benchmark for reliable comparisons between architectures.", "Developed a novel metric for forecasting adaptation in conversations."], "limitations": "", "keywords": ["conversation prediction", "CGA task", "evaluation framework", "language modeling", "forecasting"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19315", "pdf": "https://arxiv.org/pdf/2507.19315.pdf", "abs": "https://arxiv.org/abs/2507.19315", "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting", "authors": ["Yicheng Tao", "Yuanhao Huang", "Jie Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Phenotype concept recognition (CR) is a fundamental task in biomedical text\nmining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific\ntraining and struggle to generalize across diverse text types and evolving\nbiomedical terminology. We present AutoPCR, a prompt-based phenotype CR method\nthat does not require ontology-specific training. AutoPCR performs CR in three\nstages: entity extraction using a hybrid of rule-based and neural tagging\nstrategies, candidate retrieval via SapBERT, and entity linking through\nprompting a large language model. Experiments on four benchmark datasets show\nthat AutoPCR achieves the best average and most robust performance across both\nmention-level and document-level evaluations, surpassing prior state-of-the-art\nmethods. Further ablation and transfer studies demonstrate its inductive\ncapability and generalizability to new ontologies.", "AI": {"tldr": "AutoPCR is a prompt-based method for phenotype concept recognition that eliminates the need for ontology-specific training, showing superior performance across diverse biomedical text.", "motivation": "To address the limitations of existing phenotype concept recognition methods that require specific ontologies and struggle with generalization across various text types and biomedical terminology.", "method": "AutoPCR utilizes a three-stage process: entity extraction with rule-based and neural tagging, candidate retrieval using SapBERT, and entity linking via a large language model prompted with natural language.", "result": "AutoPCR achieved the best average and most robust performance on four benchmark datasets compared to previous state-of-the-art methods.", "conclusion": "The results indicate that AutoPCR has strong inductive capabilities, demonstrating its effectiveness in adapting to new ontologies.", "key_contributions": ["Introduces a prompt-based approach to phenotype CR without ontology-specific training.", "Achieves superior performance on benchmark datasets versus existing methods.", "Demonstrates generalizability and inductive capabilities for new ontologies."], "limitations": "", "keywords": ["phenotype recognition", "biomedical text mining", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.19353", "pdf": "https://arxiv.org/pdf/2507.19353.pdf", "abs": "https://arxiv.org/abs/2507.19353", "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks", "authors": ["Kai Liu", "Zhan Su", "Peijie Dong", "Fengran Mo", "Jianfei Gao", "ShaoTing Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.", "AI": {"tldr": "This paper introduces Smooth Reading, a chunk-wise inference method for Recurrent LLMs, improving their performance on long-context tasks and making them more efficient compared to Self-Attention LLMs.", "motivation": "Recurrent LLMs suffer from fixed-size memory limitations, impacting their performance on long-context tasks compared to Self-Attention LLMs, which have quadratic complexity.", "method": "Smooth Reading processes context in chunks and iteratively summarizes information, reducing memory demands and enhancing compatibility with Recurrent LLMs.", "result": "Smooth Reading improves SWA-3B-4k performance from 5.68% lower to 3.61% higher than Self-Attention LLMs on LongBench, while being 3x faster in training and 2x faster in inference at 64k context.", "conclusion": "This method successfully narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, maintaining efficiency advantages. Code and datasets will be released for further research.", "key_contributions": ["Introduction of the Smooth Reading method for chunk-wise inference", "Demonstrated improved performance of Recurrent LLMs on long-context tasks", "Significant efficiency improvements in training and inference over Self-Attention LLMs."], "limitations": "", "keywords": ["Recurrent LLMs", "chunk-wise inference", "Smooth Reading", "long-context tasks", "Self-Attention LLMs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.01293", "pdf": "https://arxiv.org/pdf/2504.01293.pdf", "abs": "https://arxiv.org/abs/2504.01293", "title": "Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions", "authors": ["Mingyang Xu", "Jiayi Shao", "Yulan Ju", "Ximing Shen", "Qingyuan Gao", "Weijen Chen", "Qing Zhang", "Yun Suen Pai", "Giulia Barbareschi", "Matthias Hoppe", "Kouta Minamizawa", "Kai Kunze"], "categories": ["cs.HC", "cs.RO"], "comment": "Augmented Humans International Conference 2025 (AHs '25)", "summary": "Flying robots, such as quadrotor drones, offer new possibilities for\nhuman-robot interaction but often pose safety risks due to fast-spinning\npropellers, rigid structures, and noise. In contrast, lighter-than-air\nflapping-wing robots, inspired by animal movement, offer a soft, quiet, and\ntouch-safe alternative. Building on these advantages, we present Cuddle-Fish, a\nsoft flapping-wing floating robot designed for close-proximity interactions in\nindoor spaces. Through a user study with 24 participants, we explored their\nperceptions of the robot and experiences during a series of co-located\ndemonstrations in which the robot moved near them. Results showed that\nparticipants felt safe, willingly engaged in touch-based interactions with the\nrobot, and exhibited spontaneous affective behaviours, such as patting,\nstroking, hugging, and cheek-touching, without external prompting. They also\nreported positive emotional responses towards the robot. These findings suggest\nthat the soft floating robot with flapping wings can serve as a novel and\nsocially acceptable alternative to traditional rigid flying robots, opening new\npotential for applications in companionship, affective interaction, and play in\neveryday indoor environments.", "AI": {"tldr": "Cuddle-Fish is a soft flapping-wing floating robot that enhances human-robot interaction in indoor environments.", "motivation": "To explore safer, more engaging alternatives to traditional rigid flying robots, especially for close-proximity human-robot interactions.", "method": "Conducted a user study with 24 participants to assess their interactions and emotional responses during demonstrations with the Cuddle-Fish robot.", "result": "Participants felt safe, engaged in touch-based interactions, and displayed spontaneous affectionate behaviors with the robot, indicating a positive response to its design.", "conclusion": "The study indicates that soft flapping-wing robots can provide socially acceptable alternatives to rigid drones, with potential applications in companionship and playful interactions.", "key_contributions": ["Introduction of Cuddle-Fish, a soft flapping-wing robot for indoor interaction", "Demonstration of safe, touch-friendly human-robot interaction", "Insights into users' emotional responses and engagement with robotic technology"], "limitations": "", "keywords": ["human-robot interaction", "flapping-wing robot", "user study", "affective interaction", "indoor environments"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.19356", "pdf": "https://arxiv.org/pdf/2507.19356.pdf", "abs": "https://arxiv.org/abs/2507.19356", "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization", "authors": ["Hsuan-Yu Wang", "Pei-Ying Lee", "Berlin Chen"], "categories": ["cs.CL", "I.2.7; I.5.1"], "comment": "6 pages, 3 figures, to appear in the Proceedings of the 2025\n  International Conference on Asian Language Processing (IALP)", "summary": "In this paper, we investigate the impact of incorporating timestamp-based\nalignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.\nMisalignment between these two modalities often reduces the reliability of\nmultimodal emotion recognition systems, particularly in conversational\ncontexts. To address this issue, we introduce an alignment pipeline utilizing\npre-trained ASR and speaker diarization models, systematically synchronizing\ntimestamps to generate accurately labeled speaker segments. Our multimodal\napproach combines textual embeddings extracted via RoBERTa with audio\nembeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset\ndemonstrate that precise timestamp alignment improves SER accuracy,\noutperforming baseline methods that lack synchronization. The results highlight\nthe critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation\nfor robust multimodal emotion analysis.", "AI": {"tldr": "This paper explores how timestamp-based alignment between ASR transcripts and Speaker Diarization outputs enhances Speech Emotion Recognition accuracy.", "motivation": "To address the issue of misalignment between Automatic Speech Recognition and Speaker Diarization, which reduces the reliability of multimodal emotion recognition systems.", "method": "An alignment pipeline utilizing pre-trained ASR and speaker diarization models for synchronizing timestamps, combined with textual embeddings from RoBERTa and audio embeddings from Wav2Vec, using cross-attention fusion with a gating mechanism.", "result": "Experimental evaluations show that timestamp alignment significantly improves Speech Emotion Recognition accuracy compared to baseline methods without synchronization.", "conclusion": "The study underscores the critical role of temporal alignment in improving multimodal emotion recognition accuracy, offering a foundation for robust emotion analysis.", "key_contributions": ["Introduction of a timestamp alignment pipeline for ASR and SD outputs.", "Combination of textual and audio embeddings using advanced fusion techniques.", "Demonstration of improved SER accuracy through precise alignment."], "limitations": "", "keywords": ["Speech Emotion Recognition", "Automatic Speech Recognition", "Speaker Diarization"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2507.19361", "pdf": "https://arxiv.org/pdf/2507.19361.pdf", "abs": "https://arxiv.org/abs/2507.19361", "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main", "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.", "AI": {"tldr": "The paper introduces Speech-based Intelligence Quotient (SIQ) to evaluate voice understanding in large language models (LLMs) through cognitive-inspired metrics.", "motivation": "To improve voice understanding evaluation beyond traditional metrics like word error rate (WER) by incorporating cognitive principles.", "method": "The SIQ framework assesses LLM Voice at three cognitive levels based on Bloom's Taxonomy: 1) Remembering, 2) Understanding, 3) Application, enabling comparisons among various models.", "result": "SIQ quantifies voice understanding abilities, identifies errors in benchmarks, and detects hallucinations in LLM Voice.", "conclusion": "The framework offers a novel evaluation approach that aligns cognitive principles with voice-oriented benchmarks, highlighting challenges in multi-modal training.", "key_contributions": ["Introduction of the SIQ framework for evaluating voice understanding in LLMs.", "Unified comparisons of cascaded and end-to-end LLM methods.", "Identification of annotation errors and hallucinations in voice understanding tasks."], "limitations": "", "keywords": ["voice understanding", "large language models", "cognitive evaluation", "evaluation metrics", "multi-modal training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19374", "pdf": "https://arxiv.org/pdf/2507.19374.pdf", "abs": "https://arxiv.org/abs/2507.19374", "title": "Data Augmentation for Spoken Grammatical Error Correction", "authors": ["Penny Karanasou", "Mengjie Qian", "Stefano Bannò", "Mark J. F. Gales", "Kate M. Knill"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been accepted by ISCA SLaTE 2025", "summary": "While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations.", "AI": {"tldr": "The paper introduces a method to generate annotated audio-text pairs with grammatical errors for Spoken GEC, aiming to enrich datasets while keeping assessments consistent for L2 learners.", "motivation": "There is a lack of high-quality annotated spoken datasets for Spoken Grammatical Error Correction (SGEC) despite the existence of strong datasets for GEC.", "method": "The authors propose a fully automated approach to create audio-text pairs with intentional grammatical errors and disfluencies, as well as new evaluation metrics for these datasets.", "result": "The generated augmented dataset retains the textual and acoustic characteristics of the original data and introduces new errors, demonstrating effectiveness for both written GEC and SGEC applications.", "conclusion": "The augmented corpus can be beneficial for improving models in both text-based and speech-based grammatical error correction without affecting language assessment outcomes for learners.", "key_contributions": ["Fully automated generation of audio-text pairs with grammatical errors and disfluencies for SGEC.", "Development of objective metrics to evaluate generated datasets for efficacy in SGEC.", "Demonstrates the application of the augmented dataset for both written and spoken grammatical error correction."], "limitations": "The study primarily focuses on a single corpus (S&I Corpus) for evaluations, which may limit generalizability.", "keywords": ["Grammatical Error Correction", "Spoken GEC", "Dataset Augmentation", "L2 Learners", "Disfluencies"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.19396", "pdf": "https://arxiv.org/pdf/2507.19396.pdf", "abs": "https://arxiv.org/abs/2507.19396", "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "categories": ["cs.CL"], "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "summary": "In this study, we set a benchmark for adverse drug event (ADE) detection in\nDutch clinical free text documents using several transformer models, clinical\nscenarios and fit-for-purpose performance measures. We trained a Bidirectional\nLong Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or\nmultilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the\ntasks of named entity recognition (NER) and relation classification (RC) using\n102 richly annotated Dutch ICU clinical progress notes. Anonymized free text\nclinical progress notes of patients admitted to intensive care unit (ICU) of\none academic hospital and discharge letters of patients admitted to Internal\nMedicine wards of two non-academic hospitals were reused. We evaluated our ADE\nRC models internally using gold standard (two-step task) and predicted entities\n(end-to-end task). In addition, all models were externally validated on\ndetecting ADEs at the document level. We report both micro- and macro-averaged\nF1 scores, given the imbalance of ADEs in the datasets. Although differences\nfor the ADE RC task between the models were small, MedRoBERTa.nl was the best\nperforming model with macro-averaged F1 score of 0.63 using gold standard and\n0.62 using predicted entities. The MedRoBERTa.nl models also performed the best\nin our external validation and achieved recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use.", "AI": {"tldr": "This study benchmarks adverse drug event detection in Dutch clinical texts using transformer models, reporting on their performance and validation for clinical use.", "motivation": "To establish a performance benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents.", "method": "Trained and evaluated several models, including Bi-LSTM and four transformer-based models (BERTje, RobBERT, MedRoBERTa.nl, NuNER), on annotated clinical progress notes; validated through gold standard and end-to-end tasks.", "result": "MedRoBERTa.nl was the top performer with a macro-averaged F1 score of 0.63 on gold standard and a recall of 67-74% for detecting ADEs in discharge letters.", "conclusion": "The research highlights the importance of appropriate performance measures for ADE detection and provides a framework for future evaluations using language models in clinical settings.", "key_contributions": ["Established a benchmark for ADE detection in Dutch clinical texts", "Demonstrated the effectiveness of transformer models in this domain", "Provided insights on performance measures for clinical document analysis"], "limitations": "", "keywords": ["adverse drug events", "natural language processing", "clinical texts", "transformer models", "Dutch"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19407", "pdf": "https://arxiv.org/pdf/2507.19407.pdf", "abs": "https://arxiv.org/abs/2507.19407", "title": "Towards Domain Specification of Embedding Models in Medicine", "authors": ["Mohammad Khodadad", "Ali Shiraee", "Mahdi Astaraki", "Hamidreza Mahyar"], "categories": ["cs.CL"], "comment": null, "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks.", "AI": {"tldr": "The paper presents a new medical text embedding model, MEDTE, which addresses existing shortcomings in model diversity and evaluation benchmarks by leveraging self-supervised contrastive learning and providing a comprehensive task benchmark for medical applications.", "motivation": "Existing medical text embedding models are limited by narrow training data and inadequate evaluation benchmarks, hindering their effectiveness in real-world healthcare applications.", "method": "Leveraged MEDTE, a GTE model fine-tuned on diverse medical corpora using self-supervised contrastive learning across various data sources, and proposed a new benchmark suite with 51 tasks tailored for medical text evaluation.", "result": "The MEDTE model demonstrates superior performance over existing state-of-the-art models in various medical text tasks, highlighting its robustness and efficacy.", "conclusion": "The introduction of MEDTE and the comprehensive benchmark suite provides significant advancements in medical text embeddings, promising improvements in various healthcare applications.", "key_contributions": ["Introduction of the MEDTE model that is fine-tuned on diverse medical data", "Development of a new benchmark suite consisting of 51 tasks for evaluating medical text embeddings", "Demonstration of outperformance against state-of-the-art models in multiple medical text tasks"], "limitations": "", "keywords": ["medical text embeddings", "healthcare applications", "self-supervised learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19419", "pdf": "https://arxiv.org/pdf/2507.19419.pdf", "abs": "https://arxiv.org/abs/2507.19419", "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube.", "AI": {"tldr": "TokenSmith is an open-source library that simplifies the interaction with datasets used in Megatron-style model training through user-friendly operations and modular backend.", "motivation": "To improve the accessibility and usability of datasets in large language model pretraining, addressing the cumbersome and fragmented existing workflows.", "method": "TokenSmith provides a simple user interface and modular backend for operations like searching, viewing, editing, and inspecting datasets, enabling structured editing without altering training code.", "result": "TokenSmith enables easier dataset debugging, validation, and experimentation, enhancing the pretraining efficiency for Megatron-style frameworks.", "conclusion": "TokenSmith democratizes access to dataset tooling for large language models, facilitating better research and development workflows.", "key_contributions": ["Open-source library for dataset interaction in pretraining frameworks", "User-friendly interface for dataset operations", "Modular backend enabling structured editing without code changes"], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Data Processing", "Open Source", "Transformers"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.19457", "pdf": "https://arxiv.org/pdf/2507.19457.pdf", "abs": "https://arxiv.org/abs/2507.19457", "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "AI": {"tldr": "GEPA (Genetic-Pareto) is a prompt optimizer that enhances LLM performance through natural language reflection, outperforming GRPO and MIPROv2 in efficiency and effectiveness.", "motivation": "Investigate the potential of natural language reflection as a richer learning medium for large language models, compared to traditional reinforcement learning methods like GRPO.", "method": "GEPA samples system-level trajectories from AI systems with LLM prompts, reflecting on them in natural language to optimize prompts through diagnosis, updates, and integration of lessons from the Pareto frontier.", "result": "GEPA outperforms GRPO by an average of 10%, using up to 35x fewer rollouts, and exceeds MIPROv2 by over 10% across two LLMs.", "conclusion": "GEPA demonstrates that leveraging natural language can significantly improve the efficiency and effectiveness of prompt optimization in LLMs.", "key_contributions": ["Introduces GEPA, a novel prompt optimizer utilizing natural language reflection.", "Achieves significant performance improvements over existing reinforcement learning methods and prompt optimizers.", "Provides a new inference-time search strategy for code optimization."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Prompt Optimization", "Natural Language Processing", "Code Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.19470", "pdf": "https://arxiv.org/pdf/2507.19470.pdf", "abs": "https://arxiv.org/abs/2507.19470", "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.HC"], "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses.", "AI": {"tldr": "This paper presents a unified evaluation framework for the Conversations Gone Awry (CGA) task, which aims to predict derailing conversations in automated systems.", "motivation": "To enhance automated systems' ability to predict conversation direction and assist in human-human interactions.", "method": "The paper introduces a benchmark for evaluating different architectures in CGA, providing direct comparisons and a new metric for tracking model forecast revision during conversation.", "result": "The framework allows for an updated overview of advancements in CGA models, facilitating reliable evaluations.", "conclusion": "This foundational framework sets the stage for future research in predictive conversation modeling and improves the reliability of evaluations in this domain.", "key_contributions": ["Introduction of a uniform evaluation framework for CGA task", "Creation of a benchmark for direct comparison of models", "Development of a novel metric for tracking forecast revisions in conversations"], "limitations": "", "keywords": ["Conversational modeling", "Human-Computer Interaction", "Language modeling", "Prediction", "Evaluation framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.18639", "pdf": "https://arxiv.org/pdf/2507.18639.pdf", "abs": "https://arxiv.org/abs/2507.18639", "title": "People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction", "authors": ["Paweł Niszczota", "Tomasz Grzegorczyk", "Alexander Pastukhov"], "categories": ["cs.HC", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; H.5.2; H.5.3; K.4.3"], "comment": null, "summary": "Machines driven by large language models (LLMs) have the potential to augment\nhumans across various tasks, a development with profound implications for\nbusiness settings where effective communication, collaboration, and stakeholder\ntrust are paramount. To explore how interacting with an LLM instead of a human\nmight shift cooperative behavior in such settings, we used the Prisoner's\nDilemma game -- a surrogate of several real-world managerial and economic\nscenarios. In Experiment 1 (N=100), participants engaged in a thirty-round\nrepeated game against a human, a classic bot, and an LLM (GPT, in real-time).\nIn Experiment 2 (N=192), participants played a one-shot game against a human or\nan LLM, with half of them allowed to communicate with their opponent, enabling\nLLMs to leverage a key advantage over older-generation machines. Cooperation\nrates with LLMs -- while lower by approximately 10-15 percentage points\ncompared to interactions with human opponents -- were nonetheless high. This\nfinding was particularly notable in Experiment 2, where the psychological cost\nof selfish behavior was reduced. Although allowing communication about\ncooperation did not close the human-machine behavioral gap, it increased the\nlikelihood of cooperation with both humans and LLMs equally (by 88%), which is\nparticularly surprising for LLMs given their non-human nature and the\nassumption that people might be less receptive to cooperating with machines\ncompared to human counterparts. Additionally, cooperation with LLMs was higher\nfollowing prior interaction with humans, suggesting a spillover effect in\ncooperative behavior. Our findings validate the (careful) use of LLMs by\nbusinesses in settings that have a cooperative component.", "AI": {"tldr": "Study explores how interactions with large language models (LLMs) influence cooperative behavior in business contexts using the Prisoner's Dilemma.", "motivation": "To investigate how interfacing with LLMs affects cooperative behavior versus human interaction in business-related settings, emphasizing the implications for communication and stakeholder trust.", "method": "Conducted two experiments involving the Prisoner's Dilemma: Experiment 1 with 100 participants playing against humans, a classic bot, and an LLM (GPT); Experiment 2 with 192 participants playing one-shot games against a human or an LLM, with communication conditions.", "result": "Cooperation rates with LLMs were about 10-15 percentage points lower than with humans but still maintained high levels; allowing communication increased cooperation likelihood by 88% with both LLM and humans.", "conclusion": "The findings support the strategic use of LLMs in cooperative business environments, highlighting their potential even with inherent differences from human interaction.", "key_contributions": ["Examination of cooperative behavior with LLMs in a business context", "Demonstration of spillover effects of human interactions on LLM cooperation", "Quantitative data on cooperation rates in human vs. LLM interactions"], "limitations": "The study is limited to specific game scenarios and may not fully reflect complex real-world behaviors.", "keywords": ["Large Language Models", "Cooperation", "Human-Computer Interaction", "Business Communication", "Prisoner's Dilemma"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18945", "pdf": "https://arxiv.org/pdf/2507.18945.pdf", "abs": "https://arxiv.org/abs/2507.18945", "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models", "authors": ["Zijian Zhang", "Pan Chen", "Fangshi Du", "Runlong Ye", "Oliver Huang", "Michael Liut", "Alán Aspuru-Guzik"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration.", "AI": {"tldr": "TreeReader is a novel LLM-augmented tool for navigating academic papers, using an interactive tree structure for efficient summarization and exploration.", "motivation": "To address cognitive overload when navigating traditional PDF and HTML formats of academic papers and improve information retrieval.", "method": "TreeReader decomposes academic papers into a tree structure, where each section has an LLM-generated summary, allowing users to explore details on demand.", "result": "User study showed that TreeReader enhances reading efficiency and comprehension compared to traditional formats.", "conclusion": "TreeReader effectively combines hierarchical summarization with interactivity for better understanding of complex academic texts.", "key_contributions": ["Introduced TreeReader, an interactive tool for academic papers.", "Developed a tree structure for hierarchical summarization.", "Demonstrated improved reading efficiency and comprehension through user study."], "limitations": "", "keywords": ["TreeReader", "academic papers", "LLM", "interactive exploration", "summarization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2311.13729", "pdf": "https://arxiv.org/pdf/2311.13729.pdf", "abs": "https://arxiv.org/abs/2311.13729", "title": "Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case", "authors": ["Shashank Gupta", "Xuguang Ai", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "An updated version of this paper has appeared in the proceedings of\n  NLDB 2025 with a different title. The corresonding DOI is in the metadata\n  provided below", "summary": "End-to-end relation extraction (E2ERE) is an important and realistic\napplication of natural language processing (NLP) in biomedicine. In this paper,\nwe aim to compare three prevailing paradigms for E2ERE using a complex dataset\nfocused on rare diseases involving discontinuous and nested entities. We use\nthe RareDis information extraction dataset to evaluate three competing\napproaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to\nsequence models, and generative pre-trained transformer (GPT) models. We use\ncomparable state-of-the-art models and best practices for each of these\napproaches and conduct error analyses to assess their failure modes. Our\nfindings reveal that pipeline models are still the best, while\nsequence-to-sequence models are not far behind; GPT models with eight times as\nmany parameters are worse than even sequence-to-sequence models and lose to\npipeline models by over 10 F1 points. Partial matches and discontinuous\nentities caused many NER errors contributing to lower overall E2E performances.\nWe also verify these findings on a second E2ERE dataset for chemical-protein\ninteractions. Although generative LM-based methods are more suitable for\nzero-shot settings, when training data is available, our results show that it\nis better to work with more conventional models trained and tailored for E2ERE.\nMore innovative methods are needed to marry the best of the both worlds from\nsmaller encoder-decoder pipeline models and the larger GPT models to improve\nE2ERE. As of now, we see that well designed pipeline models offer substantial\nperformance gains at a lower cost and carbon footprint for E2ERE. Our\ncontribution is also the first to conduct E2ERE for the RareDis dataset.", "AI": {"tldr": "This paper compares three paradigms for end-to-end relation extraction (E2ERE) using the RareDis dataset for rare diseases, evaluating NER --> RE pipelines, joint sequence to sequence models, and GPT models.", "motivation": "To assess the effectiveness of different paradigms for E2ERE in the context of biomedicine, particularly focusing on rare diseases with complex data.", "method": "The study evaluates three approaches to E2ERE: NER to RE pipelines, joint sequence-to-sequence models, and GPT models, using comparable state-of-the-art methodologies and error analysis on the RareDis dataset.", "result": "Pipeline models outperformed both sequence-to-sequence and GPT models, revealing significant performance gaps and identifying NER errors due to partial matches and discontinuous entities.", "conclusion": "Conventional pipeline models are recommended for E2ERE when training data is available, highlighting the need for innovative methods that combine strengths from both smaller and larger models.", "key_contributions": ["First study to perform E2ERE using the RareDis dataset", "Identification of performance strengths and weaknesses of three E2ERE approaches", "Recommendations for future research combining small and large models."], "limitations": "The study primarily focuses on specific datasets (RareDis and a second dataset for chemical-protein interactions), which may not generalize to all contexts.", "keywords": ["end-to-end relation extraction", "natural language processing", "biomedicine", "rare diseases", "generative models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2312.16903", "pdf": "https://arxiv.org/pdf/2312.16903.pdf", "abs": "https://arxiv.org/abs/2312.16903", "title": "Spike No More: Stabilizing the Pre-training of Large Language Models", "authors": ["Sho Takase", "Shun Kiyono", "Sosuke Kobayashi", "Jun Suzuki"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. Based on the assumption that the loss spike is caused\nby the sudden growth of the gradient norm, we explore factors to keep the\ngradient norm small through an analysis of the spectral norms of the Jacobian\nmatrices for the sub-layers. Our findings suggest that stabilizing the\npre-training process requires two conditions: small sub-layers and large\nshortcut. We conduct various experiments to empirically verify our theoretical\nanalyses. Experimental results demonstrate that methods satisfying the\nconditions effectively prevent loss spikes during pre-training.", "AI": {"tldr": "This paper explores methods to prevent loss spikes during the pre-training of large language models by keeping the gradient norm small.", "motivation": "Loss spikes during large language model pre-training can degrade performance and waste computational resources. The goal is to find ways to stabilize this process.", "method": "The authors analyze the spectral norms of Jacobian matrices for sub-layers to understand factors influencing gradient norm size, proposing conditions for stabilization.", "result": "Experiments show that models with small sub-layers and large shortcuts effectively reduce loss spikes during pre-training.", "conclusion": "Stabilizing the pre-training process of large language models requires mindful design choices regarding sub-layer sizes and the implementation of shortcut connections.", "key_contributions": ["Identifying the influence of gradient norms on pre-training stability", "Proposing conditions for preventing loss spikes during training", "Empirical validation of theoretical analyses through experiments"], "limitations": "", "keywords": ["large language models", "pre-training", "gradient norm", "loss spikes", "Jacobian matrices"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2402.13470", "pdf": "https://arxiv.org/pdf/2402.13470.pdf", "abs": "https://arxiv.org/abs/2402.13470", "title": "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?", "authors": ["Aviv Brokman", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "A version of this paper has appeared in the proceedings of NLDB 2025\n  with a slightly different title. The corresponding DOI is also listed below\n  in the metadata", "summary": "Cutting edge techniques developed in the general NLP domain are often\nsubsequently applied to the high-value, data-rich biomedical domain. The past\nfew years have seen generative language models (LMs), instruction finetuning,\nand few-shot learning become foci of NLP research. As such, generative LMs\npretrained on biomedical corpora have proliferated and biomedical instruction\nfinetuning has been attempted as well, all with the hope that domain\nspecificity improves performance on downstream tasks. Given the nontrivial\neffort in training such models, we investigate what, if any, benefits they have\nin the key biomedical NLP task of relation extraction. Specifically, we address\ntwo questions: (1) Do LMs trained on biomedical corpora outperform those\ntrained on general domain corpora? (2) Do models instruction finetuned on\nbiomedical datasets outperform those finetuned on assorted datasets or those\nsimply pretrained? We tackle these questions using existing LMs, testing across\nfour datasets. In a surprising result, general-domain models typically\noutperformed biomedical-domain models. However, biomedical instruction\nfinetuning improved performance to a similar degree as general instruction\nfinetuning, despite having orders of magnitude fewer instructions. Our findings\nsuggest it may be more fruitful to focus research effort on larger-scale\nbiomedical instruction finetuning of general LMs over building domain-specific\nbiomedical LMs", "AI": {"tldr": "This paper investigates the effectiveness of biomedical language models (LMs) compared to general-domain LMs in the task of relation extraction, concluding that general-domain models often outperform biomedical models.", "motivation": "To explore the benefits of using domain-specific biomedical language models for relation extraction compared to general-domain models.", "method": "The study tests existing language models across four datasets to compare performance between biomedical-trained and general-domain models, as well as different types of instruction finetuning.", "result": "General-domain models generally outperformed biomedical-domain models, while biomedical instruction finetuning showed performance improvement similar to general instruction finetuning despite having fewer instructions.", "conclusion": "It may be more effective to pursue larger-scale biomedical instruction finetuning of general language models instead of developing domain-specific biomedical LMs.", "key_contributions": ["Comparison of biomedical and general-domain language models for relation extraction", "Evaluation of instruction finetuning effectiveness in biomedical NLP", "Insights on research focus towards instruction finetuning over specific model training"], "limitations": "", "keywords": ["biomedical NLP", "relation extraction", "instruction finetuning", "generative language models"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2406.12549", "pdf": "https://arxiv.org/pdf/2406.12549.pdf", "abs": "https://arxiv.org/abs/2406.12549", "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts", "authors": ["Dominik Macko", "Jakub Kopal", "Robert Moro", "Ivan Srba"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main", "summary": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters.", "AI": {"tldr": "The paper proposes a multilingual dataset for detecting machine-generated texts on social media, addressing a gap in current research.", "motivation": "To address the lack of multilingual datasets and the unique characteristics of social media texts for machine-generated text detection.", "method": "Creation of a multilingual (22 languages) and multi-platform (5 social media platforms) dataset called MultiSocial, with 472,097 texts, to evaluate detection methods.", "result": "Fine-tuned detection methods effectively identify machine-generated texts, with platform selection affecting training outcomes.", "conclusion": "The study highlights the importance of a dedicated dataset for social media text and demonstrates the effectiveness of fine-tuning detection methods.", "key_contributions": ["Introduction of the MultiSocial dataset for multilingual machine-generated text detection", "Comprehensive comparison of zero-shot and fine-tuned detection methods", "Insights on platform-specific training effects on detection performance"], "limitations": "", "keywords": ["machine-generated text detection", "multilingual dataset", "social media texts", "fine-tuning", "LLMs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2408.06303", "pdf": "https://arxiv.org/pdf/2408.06303.pdf", "abs": "https://arxiv.org/abs/2408.06303", "title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "authors": ["Mina Huh", "Fangyuan Xu", "Yi-Hao Peng", "Chongyan Chen", "Hansika Murugu", "Danna Gurari", "Eunsol Choi", "Amy Pavel"], "categories": ["cs.CL", "cs.CV"], "comment": "COLM 2024 Oral Spotlight", "summary": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.", "AI": {"tldr": "VizWiz-LF is a dataset for long-form visual question answers generated by blind and low vision users, highlighting the challenges of hallucinations in generated content.", "motivation": "To address the need for long-form visual question answers that provide comprehensive responses for blind and low vision users.", "method": "Creation of the VizWiz-LF dataset, containing 4.2k long-form answers to visual questions, with evaluation through automatic and human assessments.", "result": "Long-form answers from BLV users contain valuable information beyond just answers, but AI-generated responses often include incorrect details and hallucinations.", "conclusion": "While generated answers are perceived as plausible, improvements are needed to reduce hallucinations, particularly for unanswerable visual questions.", "key_contributions": ["Introduction of the VizWiz-LF dataset", "Functional role annotation of LFVQA sentences", "Evaluation methods for assessing visual question answering models"], "limitations": "Generated answers sometimes hallucinate visual details and struggle with unanswerable questions.", "keywords": ["vision language models", "long-form visual question answers", "blind and low vision users"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.07919", "pdf": "https://arxiv.org/pdf/2410.07919.pdf", "abs": "https://arxiv.org/abs/2410.07919", "title": "Advancing biomolecular understanding and design following human instructions", "authors": ["Xiang Zhuang", "Keyan Ding", "Tianwen Lyu", "Yinuo Jiang", "Xiaotong Li", "Zhuoyi Xiang", "Zeyuan Wang", "Ming Qin", "Kehua Feng", "Jike Wang", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.CL", "q-bio.BM"], "comment": null, "summary": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology and enzyme\nengineering. Recent breakthroughs in artificial intelligence have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nartificial intelligence's computational capabilities and researchers' intuitive\ngoals, particularly in using natural language to bridge complex tasks with\nhuman intentions. Large language models have shown potential to interpret human\nintentions, yet their application to biomolecular research remains nascent due\nto challenges including specialized knowledge requirements, multimodal data\nintegration, and semantic alignment between natural language and biomolecules.\nTo address these limitations, we present InstructBioMol, a large language model\ndesigned to bridge natural language and biomolecules through a comprehensive\nany-to-any alignment of natural language, molecules and proteins. This model\ncan integrate multimodal biomolecules as the input, and enable researchers to\narticulate design goals in natural language, providing biomolecular outputs\nthat meet precise biological needs. Experimental results demonstrate that\nInstructBioMol can understand and design biomolecules following human\ninstructions. In particular, it can generate drug molecules with a 10%\nimprovement in binding affinity and design enzymes that achieve an\nenzyme-substrate pair prediction score of 70.4. This highlights its potential\nto transform real-world biomolecular research. The code is available at\nhttps://github.com/HICAI-ZJU/InstructBioMol.", "AI": {"tldr": "InstructBioMol is a large language model that bridges natural language and biomolecular design, enhancing drug discovery through improved binding affinity and enzyme predictions.", "motivation": "To address the gap between AI's computational capabilities and researchers' intuitive goals in biomolecular research, particularly using natural language.", "method": "InstructBioMol integrates multimodal biomolecules as input and allows researchers to define design goals in natural language, producing appropriate biomolecular outputs.", "result": "InstructBioMol achieved a 10% improvement in drug molecule binding affinity and a 70.4 enzyme-substrate pair prediction score in experimental results.", "conclusion": "The model has the potential to transform biomolecular research by effectively aligning natural language with biomolecular design tasks.", "key_contributions": ["Development of InstructBioMol, a model for biomolecular design using natural language.", "Demonstrated improved drug design capabilities with measurable binding affinity enhancement.", "Integration of multimodal data for better understanding of biomolecular outputs."], "limitations": "", "keywords": ["large language models", "biomolecular design", "drug discovery", "enzyme engineering", "natural language processing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.12591", "pdf": "https://arxiv.org/pdf/2412.12591.pdf", "abs": "https://arxiv.org/abs/2412.12591", "title": "LLMs are Also Effective Embedding Models: An In-depth Overview", "authors": ["Chongyang Tao", "Tao Shen", "Shen Gao", "Junshuo Zhang", "Zhen Li", "Kai Hua", "Wenpeng Hu", "Zhengwei Tao", "Shuai Ma"], "categories": ["cs.CL"], "comment": "38 pages", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.", "AI": {"tldr": "This survey explores the transition from traditional encoder-only models to LLMs in natural language processing, focusing on methods for deriving competitive embeddings from these models.", "motivation": "The shift from traditional embedding models to LLMs represents a significant change in NLP paradigms, necessitating a comprehensive overview of techniques and strategies involved in leveraging LLMs for embeddings.", "method": "The survey encompasses foundational techniques, direct prompting strategies for competitive embeddings, and data-centric tuning aspects affecting embedding models. It discusses various advanced methods for embeddings from diverse data types, including longer texts and multilingual sources.", "result": "The survey highlights multiple methods for generating embeddings and examines factors affecting their performance such as model architecture and tuning strategies, as well as efficiency comparisons between embedding types.", "conclusion": "This work serves as a critical resource for researchers and practitioners, synthesizing advancements and challenges in utilizing LLMs for effective embedding applications in various domains.", "key_contributions": ["In-depth overview of embeddings derivation strategies from LLMs.", "Discussion of advanced embedding methodologies for different data types.", "Comprehensive analysis of trade-offs in LLM embedding models."], "limitations": "Challenges include embedding quality across tasks, efficiency vs. accuracy trade-offs, and issues related to data bias and robustness.", "keywords": ["large language models", "embeddings", "natural language processing", "prompt engineering", "data tuning"], "importance_score": 9, "read_time_minutes": 38}}
{"id": "2412.13666", "pdf": "https://arxiv.org/pdf/2412.13666.pdf", "abs": "https://arxiv.org/abs/2412.13666", "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation", "authors": ["Aneta Zugecova", "Dominik Macko", "Ivan Srba", "Robert Moro", "Jakub Kopal", "Katarina Marcincinova", "Matus Mesarcik"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ACL 2025 main", "summary": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts\nraises many concerns regarding their misuse. Previous research has shown that\nLLMs can be effectively misused for generating disinformation news articles\nfollowing predefined narratives. Their capabilities to generate personalized\n(in various aspects) content have also been evaluated and mostly found usable.\nHowever, a combination of personalization and disinformation abilities of LLMs\nhas not been comprehensively studied yet. Such a dangerous combination should\ntrigger integrated safety filters of the LLMs, if there are some. This study\nfills this gap by evaluating vulnerabilities of recent open and closed LLMs,\nand their willingness to generate personalized disinformation news articles in\nEnglish. We further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.", "AI": {"tldr": "This study investigates the combination of personalization and disinformation capabilities in LLMs, highlighting vulnerabilities in their safety filters.", "motivation": "To understand the risks associated with LLMs generating personalized disinformation, as this combination has not been extensively studied.", "method": "The study evaluates recent open and closed LLMs in their ability to generate personalized disinformation news articles and assesses the functioning of their safety filters.", "result": "The results indicate that most LLMs lack proper safety filters, with personalization reducing safety-filter activations, effectively acting as a jailbreak.", "conclusion": "The findings emphasize the urgent need for improved safety mechanisms in LLMs to prevent the generation of harmful personalized disinformation.", "key_contributions": ["Evaluated vulnerabilities of open and closed LLMs in generating personalized disinformation.", "Demonstrated that personalization can reduce the effectiveness of safety filters.", "Highlighted the pressing need for stronger safety measures in LLMs."], "limitations": "Focused primarily on English language LLMs; results may not generalize to other languages.", "keywords": ["Large Language Models", "Personalization", "Disinformation", "Safety Filters", "Meta-evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.12612", "pdf": "https://arxiv.org/pdf/2501.12612.pdf", "abs": "https://arxiv.org/abs/2501.12612", "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation", "authors": ["Lijun Li", "Zhelun Shi", "Xuhao Hu", "Bowen Dong", "Yiran Qin", "Xihui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at CVPR 2025", "summary": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of\nhigh-quality images from text prompts across various domains. However, these\nmodels present notable safety concerns, including the risk of generating\nharmful, biased, or private content. Current research on assessing T2I safety\nremains in its early stages. While some efforts have been made to evaluate\nmodels on specific safety dimensions, many critical risks remain unexplored. To\naddress this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I\nmodels across three key domains: toxicity, fairness, and bias. We build a\ndetailed hierarchy of 12 tasks and 44 categories based on these three domains,\nand meticulously collect 70K corresponding prompts. Based on this taxonomy and\nprompt set, we build a large-scale T2I dataset with 68K manually annotated\nimages and train an evaluator capable of detecting critical risks that previous\nwork has failed to identify, including risks that even ultra-large proprietary\nmodels like GPTs cannot correctly detect. We evaluate 12 prominent diffusion\nmodels on T2ISafety and reveal several concerns including persistent issues\nwith racial fairness, a tendency to generate toxic content, and significant\nvariation in privacy protection across the models, even with defense methods\nlike concept erasing. Data and evaluator are released under\nhttps://github.com/adwardlee/t2i_safety.", "AI": {"tldr": "The paper presents T2ISafety, a safety benchmark for text-to-image models focusing on toxicity, fairness, and bias, revealing critical risks in prominent models.", "motivation": "To address the emerging safety concerns in text-to-image (T2I) models, highlighting the risks of generating harmful or biased content.", "method": "Developed a detailed hierarchy of tasks and categories for evaluating T2I models, collected a large dataset of prompts and manually annotated images, and trained an evaluator to detect nuanced safety risks.", "result": "Evaluation of 12 diffusion models revealed issues with racial fairness, generation of toxic content, and variable privacy protections, even under defensive strategies.", "conclusion": "T2ISafety offers a comprehensive benchmark that uncovers significant safety risks in existing T2I models, urging further research to mitigate these issues.", "key_contributions": ["Introduction of T2ISafety benchmark for T2I safety evaluation", "Creation of a large-scale dataset with 68K annotated images", "Identification of critical yet overlooked safety risks in prominent T2I models"], "limitations": "The study is limited to the current set of evaluated models and categories; further research is needed to fully assess T2I safety.", "keywords": ["Text-to-Image", "Safety Benchmark", "Toxicity", "Fairness", "Bias"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.11439", "pdf": "https://arxiv.org/pdf/2502.11439.pdf", "abs": "https://arxiv.org/abs/2502.11439", "title": "An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning", "authors": ["Cen-Jhih Li", "Aditya Bhaskara"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning\n(SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SpFT framework, based on ideas from neural network pruning. At\na high level, we first identify ``important'' neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Experiments on common language tasks show our method\nimproves SpFT's memory efficiency by 20-50\\% while matching the accuracy of\nstate-of-the-art methods like LoRA's variants.", "AI": {"tldr": "Development of a new Sparse Fine-tuning framework that enhances memory efficiency while maintaining accuracy for large language models.", "motivation": "Need for fine-tuning methods that are memory and computationally efficient to adapt foundation models to downstream tasks with limited resources.", "method": "The new framework identifies important neurons using feature importance metrics from neural network pruning, specifically through structural pruning, and restricts fine-tuning to these weights.", "result": "The proposed method improves Sparse Fine-tuning's memory efficiency by 20-50% while achieving accuracy comparable to state-of-the-art methods like LoRA.", "conclusion": "The new SpFT framework provides a significant improvement in efficient fine-tuning of large language models for users with limited computational resources.", "key_contributions": ["Introduction of a novel Sparse Fine-tuning framework", "Utilization of feature importance metrics from network pruning", "Demonstrated memory efficiency improvement of 20-50%"], "limitations": "", "keywords": ["Sparse Fine-tuning", "Low-rank adaptation", "neural network pruning", "language models", "memory efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.00151", "pdf": "https://arxiv.org/pdf/2503.00151.pdf", "abs": "https://arxiv.org/abs/2503.00151", "title": "Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Samar Mohamed Magdy", "Abdelrahim A. Elmadany", "Omer Nacar", "El Moatez Billah Nagoudi", "Reem Abdel-Salam", "Hanin Atwany", "Youssef Nafea", "Abdulfattah Mohammed Yahya", "Rahaf Alhamouri", "Hamzah A. Alsayadi", "Hiba Zayed", "Sara Shatnawi", "Serry Sibaee", "Yasir Ech-Chammakhy", "Walid Al-Dhabyani", "Marwa Mohamed Ali", "Imen Jarraya", "Ahmed Oumar El-Shangiti", "Aisha Alraeesi", "Mohammed Anwar Al-Ghrawi", "Abdulrahman S. Al-Batati", "Elgizouli Mohamed", "Noha Taha Elgindi", "Muhammed Saeed", "Houdaifa Atou", "Issam Ait Yahia", "Abdelhak Bouayad", "Mohammed Machrouh", "Amal Makouar", "Dania Alkawi", "Mukhtar Mohamed", "Safaa Taher Abdelfadil", "Amine Ziad Ounnoughene", "Rouabhia Anfel", "Rwaa Assi", "Ahmed Sorkatti", "Mohamedou Cheikh Tourad", "Anis Koubaa", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL", "cs.AI"], "comment": "More information about our dataset is available at our project page:\n  https://github.com/UBC-NLP/palm", "summary": "As large language models (LLMs) become increasingly integrated into daily\nlife, ensuring their cultural sensitivity and inclusivity is paramount. We\nintroduce our dataset, a year-long community-driven project covering all 22\nArab countries. The dataset includes instructions (input, response pairs) in\nboth Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20\ndiverse topics. Built by a team of 44 researchers across the Arab world, all of\nwhom are authors of this paper, our dataset offers a broad, inclusive\nperspective. We use our dataset to evaluate the cultural and dialectal\ncapabilities of several frontier LLMs, revealing notable limitations. For\ninstance, while closed-source LLMs generally exhibit strong performance, they\nare not without flaws, and smaller open-source models face greater challenges.\nMoreover, certain countries (e.g., Egypt, the UAE) appear better represented\nthan others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code,\nand data for reproducibility are publicly available.", "AI": {"tldr": "Study introduces a culturally sensitive and inclusive dataset of Arabic language instructions for evaluating large language models (LLMs).", "motivation": "To ensure cultural sensitivity and inclusivity in LLMs as they become integrated into daily life across Arab countries.", "method": "A year-long community-driven project created a dataset covering 22 Arab countries containing input-response pairs in both Modern Standard Arabic and dialectal Arabic, covering 20 diverse topics.", "result": "Evaluation of several frontier LLMs showed notable limitations in cultural and dialectal capabilities, with performance discrepancies between closed-source and open-source models, and underrepresentation of certain countries.", "conclusion": "The dataset provides insights into the strengths and weaknesses of language models while promoting reproducibility through available guidelines and data.", "key_contributions": ["Dataset creation focusing on Arabic language and cultural inclusivity", "Evaluation of LLMs on dialectal capabilities leading to prominent findings", "Public accessibility of guidelines and reproducibility aspects"], "limitations": "Certain countries are underrepresented in the dataset, affecting the generalization of findings.", "keywords": ["large language models", "cultural sensitivity", "Arabic language", "dataset", "evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.05157", "pdf": "https://arxiv.org/pdf/2503.05157.pdf", "abs": "https://arxiv.org/abs/2503.05157", "title": "Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy", "authors": ["Ruixi Lin", "Ziqiao Wang", "Yang You"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Language models are strong few-shot learners and achieve good overall\naccuracy in text classification tasks, masking the fact that their results\nsuffer from great class accuracy imbalance. We believe that the pursuit of\noverall accuracy should not come from enriching the strong classes, but from\nraising up the weak ones. To address the imbalance, we propose a Heaviside step\nfunction based ensemble debiasing method, which enables flexible rectifications\nof in-context learned class probabilities at both class and sample levels.\nEvaluations with Llama-2-13B on seven text classification benchmarks show that\nour approach achieves state-of-the-art overall accuracy gains with balanced\nclass accuracies. More importantly, we perform analyses on the resulted\nprobability correction scheme, showing that sample-level corrections are\nnecessary to elevate weak classes. Due to effectively correcting weak classes,\nour method also brings significant performance gains to a larger model variant,\nLlama-2-70B, especially on a biomedical domain task, further demonstrating the\nnecessity of ensemble debiasing at both levels. Our source code is available at\nhttps://github.com/NUS-HPC-AI-Lab/DCS.", "AI": {"tldr": "Proposes an ensemble debiasing method to address class accuracy imbalance in language models, achieving state-of-the-art results with balanced class accuracies.", "motivation": "To improve accuracy in text classification not by enhancing strong classes but by uplifting weak ones due to observed class accuracy imbalance.", "method": "A Heaviside step function based ensemble debiasing method for flexible rectifications of class probabilities at multiple levels.", "result": "Achieves state-of-the-art overall accuracy gains while ensuring balanced class accuracies on seven text classification benchmarks, especially in the biomedical domain.", "conclusion": "Demonstrates the necessity of correcting weak classes through ensemble debiasing, with successful performance improvements in larger model variants.", "key_contributions": ["Introduction of a new ensemble debiasing method for class imbalance", "Successful application of the method on Llama-2 models", "Significant performance improvements in biomedical domain text classification"], "limitations": "", "keywords": ["Language Models", "Text Classification", "Debiasing", "Machine Learning", "AI in Health"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.17799", "pdf": "https://arxiv.org/pdf/2503.17799.pdf", "abs": "https://arxiv.org/abs/2503.17799", "title": "Relation Extraction with Instance-Adapted Predicate Descriptions", "authors": ["Yuhang Jiang", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "This paper has been accepted to appear in the proceedings of AMIA\n  2025", "summary": "Relation extraction (RE) is a standard information extraction task playing a\nmajor role in downstream applications such as knowledge discovery and question\nanswering. Although decoder-only large language models are excelling in\ngenerative tasks, smaller encoder models are still the go to architecture for\nRE. In this paper, we revisit fine-tuning such smaller models using a novel\ndual-encoder architecture with a joint contrastive and cross-entropy loss.\nUnlike previous methods that employ a fixed linear layer for predicate\nrepresentations, our approach uses a second encoder to compute\ninstance-specific predicate representations by infusing them with real entity\nspans from corresponding input instances. We conducted experiments on two\nbiomedical RE datasets and two general domain datasets. Our approach achieved\nF1 score improvements ranging from 1% to 2% over state-of-the-art methods with\na simple but elegant formulation. Ablation studies justify the importance of\nvarious components built into the proposed architecture.", "AI": {"tldr": "This paper proposes a novel dual-encoder architecture for fine-tuning smaller models in relation extraction, achieving improved F1 scores on various datasets.", "motivation": "The study aims to enhance relation extraction performance using a dual-encoder methodology, addressing the limitations of traditional fixed linear layer representations in smaller models.", "method": "A dual-encoder architecture employing a joint contrastive and cross-entropy loss, using a second encoder to compute instance-specific predicate representations with real entity spans.", "result": "The proposed method achieved F1 score improvements of 1% to 2% over existing state-of-the-art techniques on biomedical and general domain datasets.", "conclusion": "The results validate the effectiveness of the proposed architecture, with ablation studies highlighting the significance of its components.", "key_contributions": ["Introduction of a dual-encoder architecture for relation extraction", "Implementation of a joint contrastive and cross-entropy loss", "Demonstrated F1 score improvements on multiple datasets"], "limitations": "", "keywords": ["relation extraction", "dual-encoder", "machine learning", "biomedical datasets", "contrastive loss"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.16331", "pdf": "https://arxiv.org/pdf/2507.16331.pdf", "abs": "https://arxiv.org/abs/2507.16331", "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny", "authors": ["Chuanhao Yan", "Fengdi Che", "Xuhan Huang", "Xu Xu", "Xin Li", "Yizhi Li", "Xingwei Qu", "Jingzhe Shi", "Zhuangzhuang He", "Chenghua Lin", "Yaodong Yang", "Binhang Yuan", "Hang Zhao", "Yu Qiao", "Bowen Zhou", "Jie Fu"], "categories": ["cs.CL"], "comment": null, "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.", "AI": {"tldr": "This paper explores the use of formal languages, specifically Dafny, to enhance the verification processes of Large Language Models (LLMs) for software development, reducing the need for extensive human supervision.", "motivation": "The paper addresses the reliability and scalability challenges of verification processes in informal language-based LLMs, which struggle in generating verifiable programs.", "method": "The research employs Dafny, a formal language, to create a scalable data curation pipeline and designs reinforcement learning methodologies that incorporate feedback from a formal verifier.", "result": "The developed DafnyComp benchmark demonstrated that even small models (0.5B parameters) could generate valid and verifiable Dafny code, outperforming proprietary models, especially when combined with reinforcement learning techniques.", "conclusion": "Utilizing formal languages like Dafny provides a promising direction for improving the verification processes in LLMs, reducing reliance on human-generated priors and enhancing model performance across various programming tasks.", "key_contributions": ["Introduction of DafnyComp benchmark for compositional formal programs", "Development of an automatic and scalable data curation pipeline", "Demonstration of improved performance of small models in generating verifiable code"], "limitations": "", "keywords": ["Large Language Models", "Formal Language", "Reinforcement Learning", "Software Verification", "Dafny"], "importance_score": 9, "read_time_minutes": 15}}
