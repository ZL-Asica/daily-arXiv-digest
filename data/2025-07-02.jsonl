{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152.pdf", "abs": "https://arxiv.org/abs/2507.00152", "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian Möller"], "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables."}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163.pdf", "abs": "https://arxiv.org/abs/2507.00163", "title": "Prompting as Scientific Inquiry", "authors": ["Ari Holtzman", "Chenhao Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs."}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210.pdf", "abs": "https://arxiv.org/abs/2507.00210", "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han Lù", "Massimo Caccia", "Véronique Eglin", "Alexandre Aussem", "Jérémy Espinas", "Alexandre Lacoste"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations."}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214.pdf", "abs": "https://arxiv.org/abs/2507.00214", "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations."}
{"id": "2507.00066", "pdf": "https://arxiv.org/pdf/2507.00066.pdf", "abs": "https://arxiv.org/abs/2507.00066", "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies."}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216.pdf", "abs": "https://arxiv.org/abs/2507.00216", "title": "Towards Style Alignment in Cross-Cultural Translation", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle."}
{"id": "2507.00161", "pdf": "https://arxiv.org/pdf/2507.00161.pdf", "abs": "https://arxiv.org/abs/2507.00161", "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments."}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239.pdf", "abs": "https://arxiv.org/abs/2507.00239", "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior."}
{"id": "2507.00198", "pdf": "https://arxiv.org/pdf/2507.00198.pdf", "abs": "https://arxiv.org/abs/2507.00198", "title": "Exploring AR Label Placements in Visually Cluttered Scenarios", "authors": ["Ji Hwan Park", "Braden Roper", "Amirhossein Arezoumand", "Tien Tran"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate methods for placing labels in AR environments that have\nvisually cluttered scenes. As the number of items increases in a scene within\nthe user' FOV, it is challenging to effectively place labels based on existing\nlabel placement guidelines. To address this issue, we implemented three label\nplacement techniques for in-view objects for AR applications. We specifically\ntarget a scenario, where various items of different types are scattered within\nthe user's field of view, and multiple items of the same type are situated\nclose together. We evaluate three placement techniques for three target tasks.\nOur study shows that using a label to spatially group the same types of items\nis beneficial for identifying, comparing, and summarizing data."}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244.pdf", "abs": "https://arxiv.org/abs/2507.00244", "title": "The Algebraic Structure of Morphosyntax", "authors": ["Isabella Senturia", "Matilde Marcolli"], "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects."}
{"id": "2507.00202", "pdf": "https://arxiv.org/pdf/2507.00202.pdf", "abs": "https://arxiv.org/abs/2507.00202", "title": "Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group", "authors": ["Blade Frisch", "Betts Peters", "Keith Vertanen"], "categories": ["cs.HC"], "comment": null, "summary": "Purpose: Little research has explored the communication needs of autistic\nadults and how their needs differ from those of other disabled populations.\nAugmentative and Alternative Communication (AAC) can support these\ncommunication needs, but more guidance is needed on how to design AAC to\nsupport this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus\ngroup with five autistic adults to explore their social communication and\ncommunity engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that\n1) participants' emotional experiences impacted the communication methods they\nused, 2) speaking autistic adults can benefit from AAC use, and 3) autistic\nshutdown creates dynamic communication needs. We present implications for\nfuture AAC design: supporting communication in times of shutdown, indicating\ncommunication ability to communication partners, and a need to better\nunderstand the fear of using AAC. These implications can inform the design for\nfuture AAC systems. We also provide themes for future autism research:\nexploring the impact of a late diagnosis, gaining a better understanding of the\ncommunication needs during autistic shutdown, and expanding research to include\nthe social and environmental factors that impact communication. Finally, we\nprovide guidance on how future online focus groups can be run in an accessible\nmanner."}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246.pdf", "abs": "https://arxiv.org/abs/2507.00246", "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang."}
{"id": "2507.00271", "pdf": "https://arxiv.org/pdf/2507.00271.pdf", "abs": "https://arxiv.org/abs/2507.00271", "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"", "authors": ["Zhuochao Peng", "Jiaxin Xu", "Jun Hu", "Haian Xue", "Laurens A. G. Kolks", "Pieter M. A. Desmet"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted to International Conference on Social Robotics + AI (ICSR\n  2025)", "summary": "While recent research highlights the potential of social robots to support\nmood regulation, little is known about how prospective users view their\nintegration into everyday life. To explore this, we conducted an exploratory\ncase study that used a speculative robot concept \"Mora\" to provoke reflection\nand facilitate meaningful discussion about using social robots to manage\nsubtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a\ncommon dip in mood that occurs at the end of the weekend, as a relatable\ncontext in which to explore individuals' insights. Using a video prototype and\na co-constructing stories method, we engaged 15 participants in imagining\ninteractions with Mora and discussing their expectations, doubts, and concerns.\nThe study surfaced a range of nuanced reflections around the attributes of\nsocial robots like empathy, intervention effectiveness, and ethical boundaries,\nwhich we translated into design considerations for future research and\ndevelopment in human-robot interaction."}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258.pdf", "abs": "https://arxiv.org/abs/2507.00258", "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option."}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286.pdf", "abs": "https://arxiv.org/abs/2507.00286", "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data."}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297.pdf", "abs": "https://arxiv.org/abs/2507.00297", "title": "Natural language processing for African languages", "authors": ["David Ifeoluwa Adelani"], "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings."}
{"id": "2507.00299", "pdf": "https://arxiv.org/pdf/2507.00299.pdf", "abs": "https://arxiv.org/abs/2507.00299", "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"", "authors": ["Olivia Figueira", "Pranathi Chamarthi", "Tu Le", "Athina Markopoulou"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "TikTok, the social media platform that is popular among children and\nadolescents, offers a more restrictive \"Under 13 Experience\" exclusively for\nyoung users in the US, also known as TikTok's \"Kids Mode\". While prior research\nhas studied various aspects of TikTok's regular mode, including privacy and\npersonalization, TikTok's Kids Mode remains understudied, and there is a lack\nof transparency regarding its content curation and its safety and privacy\nprotections for children. In this paper, (i) we propose an auditing methodology\nto comprehensively investigate TikTok's Kids Mode and (ii) we apply it to\ncharacterize the platform's content curation and determine the prevalence of\nchild-directed content, based on regulations in the Children's Online Privacy\nProtection Act (COPPA). We find that 83% of videos observed on the \"For You\"\npage in Kids Mode are actually not child-directed, and even inappropriate\ncontent was found. The platform also lacks critical features, namely parental\ncontrols and accessibility settings. Our findings have important design and\nregulatory implications, as children may be incentivized to use TikTok's\nregular mode instead of Kids Mode, where they are known to be exposed to\nfurther safety and privacy risks."}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322.pdf", "abs": "https://arxiv.org/abs/2507.00322", "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%."}
{"id": "2507.00305", "pdf": "https://arxiv.org/pdf/2507.00305.pdf", "abs": "https://arxiv.org/abs/2507.00305", "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation", "authors": ["Deland Liu", "Frigyes Samuel Racz", "Zoe Lalji", "Jose del R. Millan"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in\nstate (CLIS) can lose all reliable motor control and are left without any means\nof communication. It remains unknown whether non-invasive electroencephalogram\n(EEG) based brain-computer interfaces (BCIs) can support volitional\ncommunication in CLIS. Here, we show that a CLIS patient was able to operate an\nEEG-based BCI across multiple online sessions to respond to both general\nknowledge and personally relevant assistive questions. The patient delivered\n\"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at\ndifferent channels, guided by real-time auditory feedback from the BCI. The\npatient communicated assistive needs above chance in all sessions, achieving a\nperfect score in the final session. Performance on general knowledge questions\nvaried across sessions, with two sessions showing accurate and above-chance\nresponses, while the first and last sessions remained at chance level. The\npatient also showed consistent modulation patterns over time. These findings\nsuggest that non-invasive BCIs may offer a potential pathway for restoring\nbasic communication in CLIS."}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330.pdf", "abs": "https://arxiv.org/abs/2507.00330", "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios."}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333.pdf", "abs": "https://arxiv.org/abs/2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports."}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355.pdf", "abs": "https://arxiv.org/abs/2507.00355", "title": "Question Decomposition for Retrieval-Augmented Generation", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines."}
{"id": "2507.00513", "pdf": "https://arxiv.org/pdf/2507.00513.pdf", "abs": "https://arxiv.org/abs/2507.00513", "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "ACM CSCW Poster 2025", "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system."}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380.pdf", "abs": "https://arxiv.org/abs/2507.00380", "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "authors": ["Vojtěch Lanz", "Jan Hajič jr"], "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation."}
{"id": "2507.00596", "pdf": "https://arxiv.org/pdf/2507.00596.pdf", "abs": "https://arxiv.org/abs/2507.00596", "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations."}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389.pdf", "abs": "https://arxiv.org/abs/2507.00389", "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL."}
{"id": "2507.00657", "pdf": "https://arxiv.org/pdf/2507.00657.pdf", "abs": "https://arxiv.org/abs/2507.00657", "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling."}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439.pdf", "abs": "https://arxiv.org/abs/2507.00439", "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research."}
{"id": "2507.00775", "pdf": "https://arxiv.org/pdf/2507.00775.pdf", "abs": "https://arxiv.org/abs/2507.00775", "title": "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review", "authors": ["Haonan Yao", "Lingyun Yu", "Lijie Yao"], "categories": ["cs.HC"], "comment": null, "summary": "We present a systematic review on tasks, interactions, and visualization\nwidgets (refer to tangible entities that are used to accomplish data\nexploration tasks through specific interactions) in the context of tangible\ndata exploration. Tangible widgets have been shown to reduce cognitive load,\nenable more natural interactions, and support the completion of complex data\nexploration tasks. Yet, the field lacks a structured understanding of how task\ntypes, interaction methods, and widget designs are coordinated, limiting the\nability to identify recurring design patterns and opportunities for innovation.\nTo address this gap, we conduct a systematic review to analyze existing work\nand characterize the current design of data exploration tasks, interactions,\nand tangible visualization widgets. We next reflect based on our findings and\npropose a research agenda to inform the development of a future widget design\ntoolkit for tangible data exploration. Our systematic review and supplemental\nmaterials are available at physicalviswidget.github.io and osf.io/vjw5e."}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460.pdf", "abs": "https://arxiv.org/abs/2507.00460", "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments."}
{"id": "2507.00821", "pdf": "https://arxiv.org/pdf/2507.00821.pdf", "abs": "https://arxiv.org/abs/2507.00821", "title": "Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures", "authors": ["Mihnea Stefan Calota", "Wessel Nieuwenhuys", "Janet Yi-Ching Huang", "Lin-Lin Chen", "Mathias Funk"], "categories": ["cs.HC"], "comment": null, "summary": "Designers have ample opportunities to impact the healthcare domain. However,\nhospitals are often closed ecosystems that pose challenges in engaging clinical\nstakeholders, developing domain knowledge, and accessing relevant systems and\ndata. In this paper, we introduce a making-oriented approach to help designers\nunderstand the intricacies of their target healthcare context. Using Remote\nPatient Monitoring (RPM) as a case study, we explore how manually crafting\nsynthetic datasets based on real-world observations enables designers to learn\nabout complex data-driven healthcare systems. Our process involves observing\nand modeling the real-world RPM context, crafting synthetic datasets, and\niteratively prototyping a simplified RPM system that balances contextual\nrichness and intentional abstraction. Through this iterative process of\nsensemaking through making, designers can still develop context familiarity\nwhen direct access to the actual healthcare system is limited. Our approach\nemphasizes the value of hands-on interaction with data structures to support\ndesigners in understanding opaque healthcare systems."}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509.pdf", "abs": "https://arxiv.org/abs/2507.00509", "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "authors": ["To Eun Kim", "João Coelho", "Gbemileke Onilude", "Jai Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers."}
{"id": "2507.00881", "pdf": "https://arxiv.org/pdf/2507.00881.pdf", "abs": "https://arxiv.org/abs/2507.00881", "title": "Towards Difficulty-Aware Analysis of Deep Neural Networks", "authors": ["Linhao Meng", "Stef van den Elzen", "Anna Vilanova"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional instance-based model analysis focuses mainly on misclassified\ninstances. However, this approach overlooks the varying difficulty associated\nwith different instances. Ideally, a robust model should recognize and reflect\nthe challenges presented by intrinsically difficult instances. It is also\nvaluable to investigate whether the difficulty perceived by the model aligns\nwith that perceived by humans. To address this, we propose incorporating\ninstance difficulty into the deep neural network evaluation process,\nspecifically for supervised classification tasks on image data. Specifically,\nwe consider difficulty measures from three perspectives -- data, model, and\nhuman -- to facilitate comprehensive evaluation and comparison. Additionally,\nwe develop an interactive visual tool, DifficultyEyes, to support the\nidentification of instances of interest based on various difficulty patterns\nand to aid in analyzing potential data or model issues. Case studies\ndemonstrate the effectiveness of our approach."}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534.pdf", "abs": "https://arxiv.org/abs/2507.00534", "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies."}
{"id": "2507.00963", "pdf": "https://arxiv.org/pdf/2507.00963.pdf", "abs": "https://arxiv.org/abs/2507.00963", "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": null, "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful."}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540.pdf", "abs": "https://arxiv.org/abs/2507.00540", "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions."}
{"id": "2507.01017", "pdf": "https://arxiv.org/pdf/2507.01017.pdf", "abs": "https://arxiv.org/abs/2507.01017", "title": "A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models", "authors": ["Xingyu Xiao", "Hongxu Zhu", "Jingang Liang", "Jiejuan Tong", "Haitao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Human error remains a dominant risk driver in safety-critical sectors such as\nnuclear power, aviation, and healthcare, where seemingly minor mistakes can\ncascade into catastrophic outcomes. Although decades of research have produced\na rich repertoire of mitigation techniques, persistent limitations: scarce\nhigh-quality data, algorithmic opacity, and residual reliance on expert\njudgment, continue to constrain progress. This review synthesizes recent\nadvances at the intersection of risk-informed decision making, human\nreliability assessment (HRA), artificial intelligence (AI), and cognitive\nscience to clarify how their convergence can curb human-error risk. We first\ncategorize the principal forms of human error observed in complex\nsociotechnical environments and outline their quantitative impact on system\nreliability. Next, we examine risk-informed frameworks that embed HRA within\nprobabilistic and data-driven methodologies, highlighting successes and gaps.\nWe then survey cognitive and human-performance models, detailing how\nmechanistic accounts of perception, memory, and decision-making enrich error\nprediction and complement HRA metrics. Building on these foundations, we\ncritically assess AI-enabled techniques for real-time error detection,\noperator-state estimation, and AI-augmented HRA workflows. Across these\nstrands, a recurring insight emerges: integrating cognitive models with\nAI-based analytics inside risk-informed HRA pipelines markedly enhances\npredictive fidelity, yet doing so demands richer datasets, transparent\nalgorithms, and rigorous validation. Finally, we identify promising research\ndirections, coupling resilience engineering concepts with grounded theory,\noperationalizing the iceberg model of incident causation, and establishing\ncross-domain data consortia, to foster a multidisciplinary paradigm that\nelevates human reliability in high-stakes systems."}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547.pdf", "abs": "https://arxiv.org/abs/2507.00547", "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "authors": ["Malmi Amadoru"], "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research."}
{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008.pdf", "abs": "https://arxiv.org/abs/2507.00008", "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning."}
{"id": "2507.00579", "pdf": "https://arxiv.org/pdf/2507.00579.pdf", "abs": "https://arxiv.org/abs/2507.00579", "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "authors": ["Miriam Anschütz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future."}
{"id": "2507.00050", "pdf": "https://arxiv.org/pdf/2507.00050.pdf", "abs": "https://arxiv.org/abs/2507.00050", "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets."}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601.pdf", "abs": "https://arxiv.org/abs/2507.00601", "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks."}
{"id": "2507.00055", "pdf": "https://arxiv.org/pdf/2507.00055.pdf", "abs": "https://arxiv.org/abs/2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks."}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606.pdf", "abs": "https://arxiv.org/abs/2507.00606", "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks."}
{"id": "2507.00224", "pdf": "https://arxiv.org/pdf/2507.00224.pdf", "abs": "https://arxiv.org/abs/2507.00224", "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts."}
{"id": "2507.00665", "pdf": "https://arxiv.org/pdf/2507.00665.pdf", "abs": "https://arxiv.org/abs/2507.00665", "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}"}
{"id": "2507.00253", "pdf": "https://arxiv.org/pdf/2507.00253.pdf", "abs": "https://arxiv.org/abs/2507.00253", "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet."}
{"id": "2507.00700", "pdf": "https://arxiv.org/pdf/2507.00700.pdf", "abs": "https://arxiv.org/abs/2507.00700", "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "authors": ["Ahmed Sabir", "Azinovič Gasper", "Mengsay Loem", "Rajesh Sharma"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs."}
{"id": "2507.00456", "pdf": "https://arxiv.org/pdf/2507.00456.pdf", "abs": "https://arxiv.org/abs/2507.00456", "title": "Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools", "authors": ["Deepak Varuvel Dennison", "Bakhtawar Ahtisham", "Kavyansh Chourasia", "Nirmit Arora", "Rahul Singh", "Rene F. Kizilcec", "Akshay Nambi", "Tanuja Ganu", "Aditya Vashistha"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This study investigates Shiksha copilot, an AI-assisted lesson planning tool\ndeployed in government schools across Karnataka, India. The system combined\nLLMs and human expertise through a structured process in which English and\nKannada lesson plans were co-created by curators and AI; teachers then further\ncustomized these curated plans for their classrooms using their own expertise\nalongside AI support. Drawing on a large-scale mixed-methods study involving\n1,043 teachers and 23 curators, we examine how educators collaborate with AI to\ngenerate context-sensitive lesson plans, assess the quality of AI-generated\ncontent, and analyze shifts in teaching practices within multilingual,\nlow-resource environments. Our findings show that teachers used Shiksha copilot\nboth to meet administrative documentation needs and to support their teaching.\nThe tool eased bureaucratic workload, reduced lesson planning time, and lowered\nteaching-related stress, while promoting a shift toward activity-based\npedagogy. However, systemic challenges such as staffing shortages and\nadministrative demands constrained broader pedagogical change. We frame these\nfindings through the lenses of teacher-AI collaboration and communities of\npractice to examine the effective integration of AI tools in teaching. Finally,\nwe propose design directions for future teacher-centered EdTech, particularly\nin multilingual and Global South contexts."}
{"id": "2507.00718", "pdf": "https://arxiv.org/pdf/2507.00718.pdf", "abs": "https://arxiv.org/abs/2507.00718", "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports."}
{"id": "2507.00481", "pdf": "https://arxiv.org/pdf/2507.00481.pdf", "abs": "https://arxiv.org/abs/2507.00481", "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "authors": ["Philipp M. Zähl", "Sabine Theis", "Martin R. Wolf"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research."}
{"id": "2507.00769", "pdf": "https://arxiv.org/pdf/2507.00769.pdf", "abs": "https://arxiv.org/abs/2507.00769", "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems."}
{"id": "2507.00543", "pdf": "https://arxiv.org/pdf/2507.00543.pdf", "abs": "https://arxiv.org/abs/2507.00543", "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "authors": ["Leila Tavakoli", "Hamed Zamani"], "categories": ["cs.IR", "cs.HC"], "comment": "9 pages,5 figures", "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings."}
{"id": "2507.00782", "pdf": "https://arxiv.org/pdf/2507.00782.pdf", "abs": "https://arxiv.org/abs/2507.00782", "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "authors": ["Matthieu Pierre Boyer"], "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences."}
{"id": "2507.00635", "pdf": "https://arxiv.org/pdf/2507.00635.pdf", "abs": "https://arxiv.org/abs/2507.00635", "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation."}
{"id": "2507.00783", "pdf": "https://arxiv.org/pdf/2507.00783.pdf", "abs": "https://arxiv.org/abs/2507.00783", "title": "Generative AI and the future of scientometrics: current topics and future questions", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production."}
{"id": "2507.00792", "pdf": "https://arxiv.org/pdf/2507.00792.pdf", "abs": "https://arxiv.org/abs/2507.00792", "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "authors": ["Hendric Voss", "Stefan Kopp"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK"}
{"id": "2507.00814", "pdf": "https://arxiv.org/pdf/2507.00814.pdf", "abs": "https://arxiv.org/abs/2507.00814", "title": "Many LLMs Are More Utilitarian Than One", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning."}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875.pdf", "abs": "https://arxiv.org/abs/2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face."}
{"id": "2507.00828", "pdf": "https://arxiv.org/pdf/2507.00828.pdf", "abs": "https://arxiv.org/abs/2507.00828", "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolomé", "Jordan Boyd-Graber", "Philip Resnik"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann"}
{"id": "2504.09296", "pdf": "https://arxiv.org/pdf/2504.09296.pdf", "abs": "https://arxiv.org/abs/2504.09296", "title": "Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation", "authors": ["Zhang Qing", "Rekimoto Jun"], "categories": ["cs.HC"], "comment": null, "summary": "Engaging with AI assistants to gather essential information in a timely\nmanner is becoming increasingly common. Traditional activation methods, like\nwake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by\ntechnical challenges such as false activations, recognition errors, and\ndiscomfort in public settings. Similarly, activating AI systems via physical\nbuttons imposes strict interactive limitations as it demands particular\nphysical actions, which hinders fluid and spontaneous communication with AI.\nOur approach employs eye-tracking technology within AR glasses to discern a\nuser's intention to engage with the AI assistant. By sustaining eye contact on\na virtual AI avatar for a specific time, users can initiate an interaction\nsilently and without using their hands. Preliminary user feedback suggests that\nthis technique is relatively intuitive, natural, and less obtrusive,\nhighlighting its potential for integrating AI assistants fluidly into everyday\ninteractions."}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838.pdf", "abs": "https://arxiv.org/abs/2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type."}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571.pdf", "abs": "https://arxiv.org/abs/2506.16571", "title": "Capturing Visualization Design Rationale", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.HC", "cs.CL"], "comment": "To be presented at IEEE VIS 2025", "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students."}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875.pdf", "abs": "https://arxiv.org/abs/2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face."}
{"id": "2506.21319", "pdf": "https://arxiv.org/pdf/2506.21319.pdf", "abs": "https://arxiv.org/abs/2506.21319", "title": "A Dataset for Enhancing MLLMs in Visualization Understanding and Reconstruction", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Current multimodal large language models (MLLMs), while effective in natural\nimage understanding, struggle with visualization understanding due to their\ninability to decode the data-to-visual mapping and extract structured\ninformation. To address these challenges, we propose SimVec, a compact and\nstructured vector format that encodes chart elements, including mark types,\npositions, and sizes. Then, we present a new visualization dataset, which\nconsists of bitmap images of charts, their corresponding SimVec\nrepresentations, and data-centric question-answering pairs, each accompanied by\nexplanatory chain-of-thought sentences. We fine-tune state-of-the-art MLLMs\nusing our dataset. The experimental results show that fine-tuning leads to\nsubstantial improvements in data-centric reasoning tasks compared to their\nzero-shot versions. SimVec also enables MLLMs to accurately and compactly\nreconstruct chart structures from images. Our dataset and code are available\nat: https://github.com/VIDA-Lab/MLLM4VIS."}
{"id": "2507.00883", "pdf": "https://arxiv.org/pdf/2507.00883.pdf", "abs": "https://arxiv.org/abs/2507.00883", "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks"}
{"id": "2506.22968", "pdf": "https://arxiv.org/pdf/2506.22968.pdf", "abs": "https://arxiv.org/abs/2506.22968", "title": "Against 'softmaxing' culture", "authors": ["Daniel Mwesigwa"], "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,'' and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nconceptual shifts. First, instead of asking \"what is culture?\" at the start of\nsystem evaluations, I propose beginning with the question: \"when is culture?\"\nSecond, while I acknowledge the philosophical claim that cultural universals\nexist, the challenge is not simply to describe them, but to situate them in\nrelation to their particulars. Taken together, these conceptual shifts invite\nevaluation approaches that move beyond technical requirements toward\nperspectives that are more responsive to the complexities of culture."}
{"id": "2507.00885", "pdf": "https://arxiv.org/pdf/2507.00885.pdf", "abs": "https://arxiv.org/abs/2507.00885", "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends."}
{"id": "2506.23116", "pdf": "https://arxiv.org/pdf/2506.23116.pdf", "abs": "https://arxiv.org/abs/2506.23116", "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences", "authors": ["Wei Xu"], "categories": ["cs.HC"], "comment": null, "summary": "User experience (UX) practices have evolved in stages and are entering a\ntransformative phase (UX 3.0), driven by AI technologies and shifting user\nneeds. Human-centered AI (HCAI) experiences are emerging, necessitating new UX\napproaches to support UX practices in the AI era. We propose a UX 3.0 paradigm\nframework to respond and guide UX practices in developing HCAI systems."}
{"id": "2507.00891", "pdf": "https://arxiv.org/pdf/2507.00891.pdf", "abs": "https://arxiv.org/abs/2507.00891", "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI."}
{"id": "2506.23458", "pdf": "https://arxiv.org/pdf/2506.23458.pdf", "abs": "https://arxiv.org/abs/2506.23458", "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs", "authors": ["Xiaoxiao Yang", "Chao Feng", "Jiancheng Chen"], "categories": ["cs.HC", "cs.LG"], "comment": "2 pages short paper", "summary": "Portable and wearable consumer-grade electroencephalography (EEG) devices,\nlike Muse headbands, offer unprecedented mobility for daily brain-computer\ninterface (BCI) applications, including cognitive load detection. However, the\nexacerbated non-stationarity in portable EEG signals constrains data fidelity\nand decoding accuracy, creating a fundamental trade-off between portability and\nperformance. To mitigate such limitation, we propose MuseCogNet (Muse-based\nCognitive Network), a unified joint learning framework integrating\nself-supervised and supervised training paradigms. In particular, we introduce\nan EEG-grounded self-supervised reconstruction loss based on average pooling to\ncapture robust neurophysiological patterns, while cross-entropy loss refines\ntask-specific cognitive discriminants. This joint learning framework resembles\nthe bottom-up and top-down attention in humans, enabling MuseCogNet to\nsignificantly outperform state-of-the-art methods on a publicly available Muse\ndataset and establish an implementable pathway for neurocognitive monitoring in\necological settings."}
{"id": "2507.00911", "pdf": "https://arxiv.org/pdf/2507.00911.pdf", "abs": "https://arxiv.org/abs/2507.00911", "title": "The Cognate Data Bottleneck in Language Phylogenetics", "authors": ["Luise Häuser", "Alexandros Stamatakis"], "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics."}
{"id": "2506.23815", "pdf": "https://arxiv.org/pdf/2506.23815.pdf", "abs": "https://arxiv.org/abs/2506.23815", "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "authors": ["Patrick Stokkink"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."}
{"id": "2507.00985", "pdf": "https://arxiv.org/pdf/2507.00985.pdf", "abs": "https://arxiv.org/abs/2507.00985", "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "categories": ["cs.CL"], "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales."}
{"id": "2506.23952", "pdf": "https://arxiv.org/pdf/2506.23952.pdf", "abs": "https://arxiv.org/abs/2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Bermúdez"], "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action."}
{"id": "2507.00994", "pdf": "https://arxiv.org/pdf/2507.00994.pdf", "abs": "https://arxiv.org/abs/2507.00994", "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "André F. T. Martins", "Céline Hudelot", "Pierre Colombo"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch."}
{"id": "2411.04037", "pdf": "https://arxiv.org/pdf/2411.04037.pdf", "abs": "https://arxiv.org/abs/2411.04037", "title": "Investigating the heterogenous effects of a massive content moderation intervention via Difference-in-Differences", "authors": ["Lorenzo Cima", "Benedetta Tessa", "Stefano Cresci", "Amaury Trujillo", "Marco Avvenuti"], "categories": ["cs.CY", "cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2401.11254 This work is an\n  extension of this conference paper: Cima, L., Trujillo, A., Avvenuti, M., &\n  Cresci, S. (2024, May). The Great Ban: Efficacy and Unintended Consequences\n  of a Massive Deplatforming Operation on Reddit. In Companion Publication of\n  the 16th ACM Web Science Conference (pp. 85-93)", "summary": "In today's online environments, users encounter harm and abuse on a daily\nbasis. Therefore, content moderation is crucial to ensure their safety and\nwell-being. However, the effectiveness of many moderation interventions is\nstill uncertain. Here, we apply a causal inference approach to shed light on\nthe effectiveness of The Great Ban, a massive social media deplatforming\nintervention on Reddit. We analyze 53M comments shared by nearly 34K users,\nproviding in-depth results on both the intended and unintended consequences of\nthe ban. Our causal analyses reveal that 15.6% of the moderated users abandoned\nthe platform while the remaining ones decreased their overall toxicity by 4.1%.\nNonetheless, a small subset of users exhibited marked increases in both the\nintensity and volume of toxic behavior, particularly among those whose activity\nlevels changed after the intervention. However, these reactions were not\naccompanied by greater activity or engagement, suggesting that even the most\ntoxic users maintained a limited overall impact. Our findings bring to light\nnew insights on the effectiveness of deplatforming moderation interventions.\nFurthermore, they also contribute to informing future content moderation\nstrategies and regulations."}
{"id": "2507.00999", "pdf": "https://arxiv.org/pdf/2507.00999.pdf", "abs": "https://arxiv.org/abs/2507.00999", "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "authors": ["María Grandury", "Javier Aula-Blasco", "Júlia Falcão", "Clémentine Fourrier", "Miguel González", "Gonzalo Martínez", "Gonzalo Santamaría", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena Gómez", "Marta Guerrero", "Guido Ivetta", "Natalia López", "Flor Miriam Plaza-del-Arco", "María Teresa Martín-Valdivia", "Helena Montoro", "Carmen Muñoz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "María Estrella Vallecillo-Rodríguez", "Jorge Vallego", "Irune Zubiaga"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community."}
{"id": "2507.01001", "pdf": "https://arxiv.org/pdf/2507.01001.pdf", "abs": "https://arxiv.org/abs/2507.01001", "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods."}
{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002.pdf", "abs": "https://arxiv.org/abs/2507.00002", "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "authors": ["Christopher James Augeri"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures."}
{"id": "2507.00018", "pdf": "https://arxiv.org/pdf/2507.00018.pdf", "abs": "https://arxiv.org/abs/2507.00018", "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation."}
{"id": "2507.00022", "pdf": "https://arxiv.org/pdf/2507.00022.pdf", "abs": "https://arxiv.org/abs/2507.00022", "title": "GLU Attention Improve Transformer", "authors": ["Zehao Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github."}
{"id": "2507.00026", "pdf": "https://arxiv.org/pdf/2507.00026.pdf", "abs": "https://arxiv.org/abs/2507.00026", "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text."}
{"id": "2507.00033", "pdf": "https://arxiv.org/pdf/2507.00033.pdf", "abs": "https://arxiv.org/abs/2507.00033", "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach."}
{"id": "2507.00045", "pdf": "https://arxiv.org/pdf/2507.00045.pdf", "abs": "https://arxiv.org/abs/2507.00045", "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities."}
{"id": "2507.00054", "pdf": "https://arxiv.org/pdf/2507.00054.pdf", "abs": "https://arxiv.org/abs/2507.00054", "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "authors": ["Shreyansh Padarha"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes."}
{"id": "2507.00068", "pdf": "https://arxiv.org/pdf/2507.00068.pdf", "abs": "https://arxiv.org/abs/2507.00068", "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "authors": ["Ziqi Zhong", "Daniel Tang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text."}
{"id": "2507.00078", "pdf": "https://arxiv.org/pdf/2507.00078.pdf", "abs": "https://arxiv.org/abs/2507.00078", "title": "The language of time: a language model perspective on time-series foundation models", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels."}
{"id": "2507.00081", "pdf": "https://arxiv.org/pdf/2507.00081.pdf", "abs": "https://arxiv.org/abs/2507.00081", "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments."}
{"id": "2507.00082", "pdf": "https://arxiv.org/pdf/2507.00082.pdf", "abs": "https://arxiv.org/abs/2507.00082", "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 16 figures, IEEE Internet of Things", "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications."}
{"id": "2507.00092", "pdf": "https://arxiv.org/pdf/2507.00092.pdf", "abs": "https://arxiv.org/abs/2507.00092", "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery."}
{"id": "2507.00234", "pdf": "https://arxiv.org/pdf/2507.00234.pdf", "abs": "https://arxiv.org/abs/2507.00234", "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making."}
{"id": "2507.00248", "pdf": "https://arxiv.org/pdf/2507.00248.pdf", "abs": "https://arxiv.org/abs/2507.00248", "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "authors": ["Nikita Nikitin", "Eugene Fomin"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference."}
{"id": "2507.00310", "pdf": "https://arxiv.org/pdf/2507.00310.pdf", "abs": "https://arxiv.org/abs/2507.00310", "title": "Open-ended Scientific Discovery via Bayesian Surprise", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems."}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316.pdf", "abs": "https://arxiv.org/abs/2507.00316", "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks."}
{"id": "2507.00417", "pdf": "https://arxiv.org/pdf/2507.00417.pdf", "abs": "https://arxiv.org/abs/2507.00417", "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs."}
{"id": "2507.00425", "pdf": "https://arxiv.org/pdf/2507.00425.pdf", "abs": "https://arxiv.org/abs/2507.00425", "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework."}
{"id": "2507.00432", "pdf": "https://arxiv.org/pdf/2507.00432.pdf", "abs": "https://arxiv.org/abs/2507.00432", "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models."}
{"id": "2507.00449", "pdf": "https://arxiv.org/pdf/2507.00449.pdf", "abs": "https://arxiv.org/abs/2507.00449", "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA)."}
{"id": "2507.00466", "pdf": "https://arxiv.org/pdf/2507.00466.pdf", "abs": "https://arxiv.org/abs/2507.00466", "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "authors": ["Sebastian Murgul", "Michael Heizmann"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation."}
{"id": "2507.00487", "pdf": "https://arxiv.org/pdf/2507.00487.pdf", "abs": "https://arxiv.org/abs/2507.00487", "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool."}
{"id": "2507.00693", "pdf": "https://arxiv.org/pdf/2507.00693.pdf", "abs": "https://arxiv.org/abs/2507.00693", "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment."}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740.pdf", "abs": "https://arxiv.org/abs/2507.00740", "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "authors": ["Craig S Wright"], "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients."}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808.pdf", "abs": "https://arxiv.org/abs/2507.00808", "title": "Multi-interaction TTS toward professional recording reproduction", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/"}
{"id": "2507.00877", "pdf": "https://arxiv.org/pdf/2507.00877.pdf", "abs": "https://arxiv.org/abs/2507.00877", "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench."}
{"id": "2507.00898", "pdf": "https://arxiv.org/pdf/2507.00898.pdf", "abs": "https://arxiv.org/abs/2507.00898", "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY."}
{"id": "2507.00979", "pdf": "https://arxiv.org/pdf/2507.00979.pdf", "abs": "https://arxiv.org/abs/2507.00979", "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks."}
{"id": "2210.06230", "pdf": "https://arxiv.org/pdf/2210.06230.pdf", "abs": "https://arxiv.org/abs/2210.06230", "title": "Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": "CoNLL2025 (Best Paper nomination)", "summary": "Formal/symbolic semantics can provide canonical, rigid controllability and\ninterpretability to sentence representations due to their \\textit{localisation}\nor \\textit{composition} property. How can we deliver such property to the\ncurrent distributional sentence representations to control and interpret the\ngeneration of language models (LMs)? In this work, we theoretically frame the\nsentence semantics as the composition of \\textit{semantic role - word content}\nfeatures and propose the formal semantic geometry. To inject such geometry into\nTransformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational\nAutoEncoder with a supervision approach, where the sentence generation can be\nmanipulated and explained over low-dimensional latent Gaussian space. In\naddition, we propose a new probing algorithm to guide the movement of sentence\nvectors over such geometry. Experimental results reveal that the formal\nsemantic geometry can potentially deliver better control and interpretation to\nsentence generation."}
{"id": "2401.14640", "pdf": "https://arxiv.org/pdf/2401.14640.pdf", "abs": "https://arxiv.org/abs/2401.14640", "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs", "authors": ["Nan Hu", "Jiaoyan Chen", "Yike Wu", "Guilin Qi", "Hongru Wang", "Sheng Bi", "Yongrui Chen", "Tongtong Wu", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark."}
{"id": "2406.04370", "pdf": "https://arxiv.org/pdf/2406.04370.pdf", "abs": "https://arxiv.org/abs/2406.04370", "title": "Large Language Model Confidence Estimation via Black-Box Access", "authors": ["Tejaswini Pedapati", "Amit Dhurandhar", "Soumya Ghosh", "Soham Dan", "Prasanna Sattigeri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to TMLR 2025", "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset."}
{"id": "2410.01141", "pdf": "https://arxiv.org/pdf/2410.01141.pdf", "abs": "https://arxiv.org/abs/2410.01141", "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs", "authors": ["Doohee You", "S Fraiberger"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics."}
{"id": "2410.14405", "pdf": "https://arxiv.org/pdf/2410.14405.pdf", "abs": "https://arxiv.org/abs/2410.14405", "title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion", "authors": ["Denitsa Saynova", "Lovisa Hagström", "Moa Johansson", "Richard Johansson", "Marco Kuhlmann"], "categories": ["cs.CL"], "comment": "accepted to ACL Findings 2025", "summary": "Language models (LMs) can make a correct prediction based on many possible\nsignals in a prompt, not all corresponding to recall of factual associations.\nHowever, current interpretations of LMs fail to take this into account. For\nexample, given the query \"Astrid Lindgren was born in\" with the corresponding\ncompletion \"Sweden\", no difference is made between whether the prediction was\nbased on knowing where the author was born or assuming that a person with a\nSwedish-sounding name was born in Sweden. In this paper, we present a\nmodel-specific recipe - PrISM - for constructing datasets with examples of four\ndifferent prediction scenarios: generic language modeling, guesswork,\nheuristics recall and exact fact recall. We apply two popular interpretability\nmethods to the scenarios: causal tracing (CT) and information flow analysis. We\nfind that both yield distinct results for each scenario. Results for exact fact\nrecall and generic language modeling scenarios confirm previous conclusions\nabout the importance of mid-range MLP sublayers for fact recall, while results\nfor guesswork and heuristics indicate a critical role of late last token\nposition MLP sublayers. In summary, we contribute resources for a more\nextensive and granular study of fact completion in LMs, together with analyses\nthat provide a more nuanced understanding of how LMs process fact-related\nqueries."}
{"id": "2412.14373", "pdf": "https://arxiv.org/pdf/2412.14373.pdf", "abs": "https://arxiv.org/abs/2412.14373", "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling", "authors": ["William Han", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "comment": "38 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods."}
{"id": "2501.01144", "pdf": "https://arxiv.org/pdf/2501.01144.pdf", "abs": "https://arxiv.org/abs/2501.01144", "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference", "authors": ["Wonsuk Jang", "Thierry Tambe"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."}
{"id": "2501.09310", "pdf": "https://arxiv.org/pdf/2501.09310.pdf", "abs": "https://arxiv.org/abs/2501.09310", "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors", "authors": ["Jiawei Shen", "Chengcheng Wan", "Ruoyi Qiao", "Jiazhen Zou", "Hang Xu", "Yuchen Shao", "Yueling Zhang", "Weikai Miao", "Geguang Pu"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead."}
{"id": "2502.14051", "pdf": "https://arxiv.org/pdf/2502.14051.pdf", "abs": "https://arxiv.org/abs/2502.14051", "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", "authors": ["Payman Behnam", "Yaosheng Fu", "Ritchie Zhao", "Po-An Tsai", "Zhiding Yu", "Alexey Tumanov"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."}
{"id": "2503.03040", "pdf": "https://arxiv.org/pdf/2503.03040.pdf", "abs": "https://arxiv.org/abs/2503.03040", "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation", "authors": ["Yizhe Zhang", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages main text", "summary": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen"}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044.pdf", "abs": "https://arxiv.org/abs/2503.15044", "title": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "categories": ["cs.CL"], "comment": "ACL LLMSEC", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248.pdf", "abs": "https://arxiv.org/abs/2503.21248", "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention."}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393.pdf", "abs": "https://arxiv.org/abs/2503.21393", "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "authors": ["Rohitash Chandra", "Aryan Chaudhari", "Yeshwanth Rayavarapu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models."}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856.pdf", "abs": "https://arxiv.org/abs/2504.19856", "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Bela Gipp"], "categories": ["cs.CL"], "comment": "accepted to TSD 2025", "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g., masked\nlanguage modeling (MLM), when common domain adaptation via LM fine-tuning is\nnot possible due to a lack of labeled task data. Although popular, MLM requires\na significant corpus of domain-related data, which is difficult to obtain for\nspecific domains in languages other than English, such as the process industry\nin the German language. This paper introduces an efficient approach called\nICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL)\nand k-nearest neighbors (kNN) to augment target data with domain-related and\nin-domain texts, significantly reducing GPU time while maintaining strong model\nperformance. Our results show that the best configuration of ICL-APT performed\nbetter than the state-of-the-art DAPT by 28.7% (7.87 points) and requires\nalmost 4 times less GPU-computing time, providing a cost-effective solution for\nindustries with limited computational capacity. The findings highlight the\nbroader applicability of this framework to other low-resource industries,\nmaking NLP-based solutions more accessible and feasible in production\nenvironments."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Prasoon Varshney", "Makesh Narsimhan", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Shaona Ghosh", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Chris Alexiuk", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722.pdf", "abs": "https://arxiv.org/abs/2505.16722", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080.pdf", "abs": "https://arxiv.org/abs/2505.17080", "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "authors": ["Davide Picca"], "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge."}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117.pdf", "abs": "https://arxiv.org/abs/2505.17117", "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778.pdf", "abs": "https://arxiv.org/abs/2505.24778", "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710.pdf", "abs": "https://arxiv.org/abs/2506.18710", "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "authors": ["Maxime Lelièvre", "Amy Waldock", "Meng Liu", "Natalia Valdés Aspillaga", "Alasdair Mackintosh", "María José Ogando Portela", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."}
{"id": "2506.19089", "pdf": "https://arxiv.org/pdf/2506.19089.pdf", "abs": "https://arxiv.org/abs/2506.19089", "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."}
{"id": "2506.21096", "pdf": "https://arxiv.org/pdf/2506.21096.pdf", "abs": "https://arxiv.org/abs/2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding", "Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines."}
{"id": "2506.21098", "pdf": "https://arxiv.org/pdf/2506.21098.pdf", "abs": "https://arxiv.org/abs/2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations."}
{"id": "2506.22403", "pdf": "https://arxiv.org/pdf/2506.22403.pdf", "abs": "https://arxiv.org/abs/2506.22403", "title": "HyperCLOVA X THINK Technical Report", "authors": ["NAVER Cloud HyperCLOVA X Team"], "categories": ["cs.CL", "cs.AI"], "comment": "50 pages, 13 figures; fixed figures in the appendix", "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community."}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698.pdf", "abs": "https://arxiv.org/abs/2506.22698", "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "authors": ["Emily Dux Speltz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration."}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137.pdf", "abs": "https://arxiv.org/abs/2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results."}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146.pdf", "abs": "https://arxiv.org/abs/2506.23146", "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success."}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431.pdf", "abs": "https://arxiv.org/abs/2506.23431", "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption."}
{"id": "2506.23743", "pdf": "https://arxiv.org/pdf/2506.23743.pdf", "abs": "https://arxiv.org/abs/2506.23743", "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "categories": ["cs.CL"], "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct."}
{"id": "2506.23940", "pdf": "https://arxiv.org/pdf/2506.23940.pdf", "abs": "https://arxiv.org/abs/2506.23940", "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs."}
{"id": "2506.24117", "pdf": "https://arxiv.org/pdf/2506.24117.pdf", "abs": "https://arxiv.org/abs/2506.24117", "title": "Intertextual Parallel Detection in Biblical Hebrew: A Transformer-Based Benchmark", "authors": ["David M. Smiley"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying parallel passages in biblical Hebrew (BH) is central to biblical\nscholarship for understanding intertextual relationships. Traditional methods\nrely on manual comparison, a labor-intensive process prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between Samuel/Kings and\nChronicles, I assessed each model's capability to generate word embeddings\ndistinguishing parallel from non-parallel passages. Using cosine similarity and\nWasserstein Distance measures, I found that E5 and AlephBERT show promise; E5\nexcels in parallel detection, while AlephBERT demonstrates stronger\nnon-parallel differentiation. These findings indicate that pre-trained models\ncan enhance the efficiency and accuracy of detecting intertextual parallels in\nancient texts, suggesting broader applications for ancient language studies."}
{"id": "2407.19342", "pdf": "https://arxiv.org/pdf/2407.19342.pdf", "abs": "https://arxiv.org/abs/2407.19342", "title": "Parameter-Efficient Fine-Tuning via Circular Convolution", "authors": ["Aochuan Chen", "Jiashun Cheng", "Zijing Liu", "Ziqi Gao", "Fugee Tsung", "Yu Li", "Jia Li"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025", "summary": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose Circular Convolution Adaptation (C$^3$A), which not only\nachieves high-rank adaptation with enhanced performance but also excels in both\ncomputational power and memory utilization. Extensive experiments demonstrate\nthat C$^3$A consistently outperforms LoRA and its variants across various\nfine-tuning tasks."}
{"id": "2408.10774", "pdf": "https://arxiv.org/pdf/2408.10774.pdf", "abs": "https://arxiv.org/abs/2408.10774", "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "authors": ["Chenxing Wei", "Yao Shu", "Ying Tiffany He", "Fei Richard Yu"], "categories": ["cs.AI", "cs.CL"], "comment": "40 pages, 15 figures", "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."}
{"id": "2409.20302", "pdf": "https://arxiv.org/pdf/2409.20302.pdf", "abs": "https://arxiv.org/abs/2409.20302", "title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "15 pages, 8 figures, 1 table", "summary": "Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose a fresh approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurements for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in an OV testbed originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some apparent false mappings\ndetected by OV systems are not actually untrue."}
{"id": "2411.19906", "pdf": "https://arxiv.org/pdf/2411.19906.pdf", "abs": "https://arxiv.org/abs/2411.19906", "title": "A Graph-Based Classical and Quantum Approach to Deterministic L-System Inference", "authors": ["Ali Lotfi", "Ian McQuillan", "Steven Rayan"], "categories": ["quant-ph", "cs.CL", "cs.DS", "cs.FL", "cs.LG"], "comment": "17 pages, 1 figure", "summary": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a massively time-consuming process.\nIt would be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-systems) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem."}
{"id": "2412.03704", "pdf": "https://arxiv.org/pdf/2412.03704.pdf", "abs": "https://arxiv.org/abs/2412.03704", "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Linjie Li", "Hongjin Lu", "Yuancheng Xu", "Chung-Ching Lin", "Kevin Lin", "Furong Huang", "Lijuan Wang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite significant advancements in vision-language models (VLMs), there\nlacks effective approaches to enhance response quality by scaling\ninference-time computation. This capability is known to be a core step towards\nthe self-improving models in recent large language model studies. In this\npaper, we present Vision Value Model (VisVM) that can guide VLM inference-time\nsearch to generate responses with better visual comprehension. Specifically,\nVisVM not only evaluates the generated sentence quality in the current search\nstep, but also anticipates the quality of subsequent sentences that may result\nfrom the current step, thus providing a long-term value. In this way, VisVM\nsteers VLMs away from generating sentences prone to hallucinations or\ninsufficient detail, thereby producing higher quality responses. Experimental\nresults demonstrate that VisVM-guided search significantly enhances VLMs'\nability to generate descriptive captions with richer visual details and fewer\nhallucinations, compared with greedy decoding and search methods with other\nvisual reward signals. Furthermore, we find that self-training the model with\nthe VisVM-guided captions improve VLM's performance across a wide range of\nmultimodal benchmarks, indicating the potential for developing self-improving\nVLMs. Our value model and code are available at\nhttps://github.com/si0wang/VisVM."}
{"id": "2412.06432", "pdf": "https://arxiv.org/pdf/2412.06432.pdf", "abs": "https://arxiv.org/abs/2412.06432", "title": "Integrating Expert Labels into LLM-based Emission Goal Detection: Example Selection vs Automatic Prompt Design", "authors": ["Marco Wrzalik", "Adrian Ulges", "Anne Uersfeld", "Florian Faust", "Viola Campos"], "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": null, "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task."}
{"id": "2412.19351", "pdf": "https://arxiv.org/pdf/2412.19351.pdf", "abs": "https://arxiv.org/abs/2412.19351", "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models", "authors": ["Sang-gil Lee", "Zhifeng Kong", "Arushi Goel", "Sungwon Kim", "Rafael Valle", "Bryan Catanzaro"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "ICML 2025. Demo: https://research.nvidia.com/labs/adlr/ETTA/ Code:\n  https://github.com/NVIDIA/elucidated-text-to-audio", "summary": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks."}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288.pdf", "abs": "https://arxiv.org/abs/2504.05288", "title": "Seeking and Updating with Live Visual Knowledge", "authors": ["Mingyang Fu", "Yuyang Peng", "Dongping Chen", "Zetong Zhou", "Benlin Liu", "Yao Wan", "Zhou Zhao", "Philip S. Yu", "Ranjay Krishna"], "categories": ["cs.CV", "cs.CL"], "comment": "Preprint. Under Review", "summary": "The visual world around us constantly evolves, from real-time news and social\nmedia trends to global infrastructure changes visible through satellite imagery\nand augmented reality enhancements. However, Multimodal Large Language Models\n(MLLMs), which automate many tasks, struggle to stay current, limited by the\ncutoff dates in their fixed training datasets. To quantify this stagnation, we\nintroduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and\n12 categories data specifically designed to support research in both seeking\nand updating with live visual knowledge. Drawing from recent news articles,\nvideo platforms, and academic publications in April 2024-May 2025, LiveVQA\nenables evaluation of how models handle latest visual information beyond their\nknowledge boundaries and how current methods help to update them. Our\ncomprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant\nperformance gaps on content beyond knowledge cutoff, and tool-use or agentic\nvisual seeking framework drastically gain an average of 327% improvement.\nFurthermore, we explore parameter-efficient fine-tuning (PEFT) methods to\nupdate MLLMs with new visual knowledge. We dive deeply to the critical balance\nbetween adapter capacity and model capability when updating MLLMs with new\nvisual knowledge. All the experimental dataset and source code are publicly\navailable at: https://livevqa.github.io."}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416.pdf", "abs": "https://arxiv.org/abs/2504.07416", "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language (VL) alignment in radiology. However, existing approaches\nstruggle to effectively utilize complex radiology reports for learning and\noffer limited interpretability through attention probability visualizations. To\naddress these challenges, we introduce RadZero, a novel framework for VL\nalignment in radiology with zero-shot multi-task capability. A key component of\nour approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity),\nwhich aligns text embeddings with local image features for interpretable,\nfine-grained VL reasoning. RadZero leverages large language models to extract\nconcise semantic sentences from radiology reports and employs multi-positive\ncontrastive training to effectively capture relationships between images and\nmultiple relevant textual descriptions. It uses a pre-trained vision encoder\nwith additional trainable Transformer layers, allowing efficient\nhigh-resolution image processing. By computing similarity between text\nembeddings and local image patch features, VL-CABS enables zero-shot inference\nwith similarity probability for classification, and pixel-level VL similarity\nmaps for grounding and segmentation. Experimental results on public chest\nradiograph benchmarks show that RadZero outperforms state-of-the-art methods in\nzero-shot classification, grounding, and segmentation. Furthermore, VL\nsimilarity map analysis highlights the potential of VL-CABS for improving\nexplainability in VL alignment. Additionally, qualitative evaluation\ndemonstrates RadZero's capability for open-vocabulary semantic segmentation,\nfurther validating its effectiveness in medical imaging."}
{"id": "2505.00703", "pdf": "https://arxiv.org/pdf/2505.00703.pdf", "abs": "https://arxiv.org/abs/2505.00703", "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "authors": ["Dongzhi Jiang", "Ziyu Guo", "Renrui Zhang", "Zhuofan Zong", "Hao Li", "Le Zhuo", "Shilin Yan", "Pheng-Ann Heng", "Hongsheng Li"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://github.com/CaraJ7/T2I-R1", "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1"}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952.pdf", "abs": "https://arxiv.org/abs/2505.02952", "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "authors": ["Fabrizio Marozzo"], "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output."}
{"id": "2505.14518", "pdf": "https://arxiv.org/pdf/2505.14518.pdf", "abs": "https://arxiv.org/abs/2505.14518", "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025. Project Website:\n  https://kuan2jiu99.github.io/Balsa", "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211.pdf", "abs": "https://arxiv.org/abs/2505.16211", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232.pdf", "abs": "https://arxiv.org/abs/2505.18232", "title": "Two-Stage Regularization-Based Structured Pruning for LLMs", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955.pdf", "abs": "https://arxiv.org/abs/2505.19955", "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "42 pages, 9 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2506.11999", "pdf": "https://arxiv.org/pdf/2506.11999.pdf", "abs": "https://arxiv.org/abs/2506.11999", "title": "Generative Representational Learning of Foundation Models for Recommendation", "authors": ["Zheli Zhou", "Chenxu Zhu", "Jianghao Lin", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "categories": ["cs.IR", "cs.CL"], "comment": "Project page is available at https://junkfood436.github.io/RecFound/", "summary": "Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines."}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571.pdf", "abs": "https://arxiv.org/abs/2506.16571", "title": "Capturing Visualization Design Rationale", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.HC", "cs.CL"], "comment": "To be presented at IEEE VIS 2025", "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students."}
{"id": "2506.22419", "pdf": "https://arxiv.org/pdf/2506.22419.pdf", "abs": "https://arxiv.org/abs/2506.22419", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119.pdf", "abs": "https://arxiv.org/abs/2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."}
