{"id": "2509.12383", "pdf": "https://arxiv.org/pdf/2509.12383.pdf", "abs": "https://arxiv.org/abs/2509.12383", "title": "Data selves and identity theft in the age of AI", "authors": ["Tim Gorichanaz"], "categories": ["cs.HC"], "comment": null, "summary": "This chapter examines identity theft in the digital age, particularly in the\ncontext of emerging artificial intelligence (AI) technologies. It begins with a\ndiscussion of big data and selfhood, the concepts of data selves and data\ndoubles, and the process of identification in the digital age. Next, the\nliterature on online identity theft is reviewed, including its theoretical and\nempirical aspects. As is evident from that review, AI technologies have\nincreased the speed and scale of identity crimes that were already rampant in\nthe online world, even while they have led to new ways of detecting and\npreventing such crimes. As with any new technology, AI is currently fuelling an\narms race between criminals and law enforcement, with end users often caught\npowerless in the middle. The chapter closes by exploring some emerging\ndirections and future possibilities of identity theft in the age of AI."}
{"id": "2509.12408", "pdf": "https://arxiv.org/pdf/2509.12408.pdf", "abs": "https://arxiv.org/abs/2509.12408", "title": "FlexMind: Scaffolding Flexible Ideation Workflows with AI in Creative Problem-Solving", "authors": ["Yaqing Yang", "Vikram Mohanty", "Nikolas Martelaro", "Aniket Kittur", "Yan-Ying Chen", "Matthew K. Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Divergent thinking in the ideation stage of creative problem-solving demands\nthat individuals explore a broad design space. Yet this exploration rarely\nfollows a neat, linear sequence; problem-solvers constantly shift among\nsearching, creating, and evaluating ideas. Existing interfaces either impose\nrigid, step-by-step workflows or permit unguided free-form exploration. To\nstrike a balance between flexibility and guidance for augmenting people's\nefficiency and creativity, we introduce a human-AI collaborative workflow that\nsupports a fluid ideation process. The system surfaces three opt-in aids: (1)\nhigh-level schemas to uncover alternative ideas, (2) risk analysis with\nmitigation suggestions, and (3) steering system-generated suggestions. Users\ncan invoke these supports at any moment, allowing seamless back-and-forth\nmovement among design actions to maintain creative momentum."}
{"id": "2509.12419", "pdf": "https://arxiv.org/pdf/2509.12419.pdf", "abs": "https://arxiv.org/abs/2509.12419", "title": "Beyond Gaze Overlap: Analyzing Joint Visual Attention Dynamics Using Egocentric Data", "authors": ["Kumushini Thennakoon", "Yasasi Abeysinghe", "Bhanuka Mahanama", "Vikas Ashok", "Sampath Jayarathna"], "categories": ["cs.HC"], "comment": "Accepted at IEEE 26th International Conference on Information Reuse\n  and Integration for Data Science, 6 pages,3 figures", "summary": "Joint visual attention (JVA) provides informative cues on human behavior\nduring social interactions. The ubiquity of egocentric eye-trackers and\nlarge-scale datasets on everyday interactions offer research opportunities in\nidentifying JVA in multi-user environments. We propose a novel approach\nutilizing spatiotemporal tubes centered on attention rendered by individual\ngaze and detect JVA using deep-learning-based feature mapping. Our results\nreveal object-focused collaborative tasks to yield higher JVA (44-46%), whereas\nindependent tasks yield lower (4-5%) attention. Beyond JVA, we analyze\nattention characteristics using ambient-focal attention coefficient K to\nunderstand the qualitative aspects of shared attention. Our analysis reveals\n$\\mathcal{K}$ to converge instances where participants interact with shared\nobjects while diverging when independent. While our study presents seminal\nfindings on joint attention with egocentric commodity eye trackers, it\nindicates the potential utility of our approach in psychology, human-computer\ninteraction, and social robotics, particularly in understanding attention\ncoordination mechanisms in ecologically valid contexts."}
{"id": "2509.12517", "pdf": "https://arxiv.org/pdf/2509.12517.pdf", "abs": "https://arxiv.org/abs/2509.12517", "title": "Extended AI Interactions Shape Sycophancy and Perspective Mimesis", "authors": ["Shomik Jain", "Charlotte Park", "Matheus Mesquita Viana", "Ashia Wilson", "Dana Calacci"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether long-context interactions between users and LLMs lead\nto AI mirroring behaviors. We focus on two forms of mirroring: (1) sycophancy\n-- the tendency of models to be overly agreeable with users, and (2)\nperspective mimesis -- the extent to which models reflect a user's perspective.\nUsing two weeks of interaction context collected from 38 users, we compare\nmodel responses with and without long-context for two tasks: political\nexplanations and personal advice. Our results demonstrate how and when\nreal-world interaction contexts can amplify AI mirroring behaviors. We find\nthat sycophancy increases in long-context, irrespective of the interaction\ntopics. Perspective mimesis increases only in contexts where models can\naccurately infer user perspectives."}
{"id": "2509.12340", "pdf": "https://arxiv.org/pdf/2509.12340.pdf", "abs": "https://arxiv.org/abs/2509.12340", "title": "MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch", "authors": ["Nikolay Banar", "Ehsan Lotfi", "Jens Van Nooten", "Cristina Arhiliuc", "Marija Kliocaite", "Walter Daelemans"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, embedding resources, including models, benchmarks, and datasets,\nhave been widely released to support a variety of languages. However, the Dutch\nlanguage remains underrepresented, typically comprising only a small fraction\nof the published multilingual resources. To address this gap and encourage the\nfurther development of Dutch embeddings, we introduce new resources for their\nevaluation and generation. First, we introduce the Massive Text Embedding\nBenchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and\nnewly created ones, covering a wide range of tasks. Second, we provide a\ntraining dataset compiled from available Dutch retrieval datasets, complemented\nwith synthetic data generated by large language models to expand task coverage\nbeyond retrieval. Finally, we release a series of E5-NL models compact yet\nefficient embedding models that demonstrate strong performance across multiple\ntasks. We make our resources publicly available through the Hugging Face Hub\nand the MTEB package."}
{"id": "2509.12525", "pdf": "https://arxiv.org/pdf/2509.12525.pdf", "abs": "https://arxiv.org/abs/2509.12525", "title": "The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots", "authors": ["T. James Brandt", "Cecilia Xi Wang"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 17 figures, 2 tables. Submitted to CHI 2026 (under review).\n  Preregistered: https://osf.io/f4h5b ; Code/Materials:\n  https://doi.org/10.5281/zenodo.15801081", "summary": "Generative AI powers a growing wave of companion chatbots, yet principles for\nfostering genuine connection remain unsettled. We test two routes: visible user\nauthorship versus covert language-style mimicry. In a preregistered 3x2\nexperiment (N = 162), we manipulated user-controlled avatar generation (none,\npremade, user-generated) and Language Style Matching (LSM) (static vs.\nadaptive). Generating an avatar boosted rapport ($\\omega^2$ = .040, p = .013),\nwhereas adaptive LSM underperformed static style on personalization and\nsatisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t\n= 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony\nerodes connection when perceived as incoherent, destabilizing persona. To\nexplain, we propose a stability-and-legibility account: visible authorship\nfosters natural interaction, while covert mimicry risks incoherence. Our\nfindings suggest designers should prioritize legible, user-driven\npersonalization and limit stylistic shifts rather than rely on opaque mimicry."}
{"id": "2509.12371", "pdf": "https://arxiv.org/pdf/2509.12371.pdf", "abs": "https://arxiv.org/abs/2509.12371", "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables", "authors": ["Matteo Marcuzzo", "Alessandro Zangari", "Andrea Albarelli", "Jose Camacho-Collados", "Mohammad Taher Pilehvar"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As LLMs excel on standard reading comprehension benchmarks, attention is\nshifting toward evaluating their capacity for complex abstract reasoning and\ninference. Literature-based benchmarks, with their rich narrative and moral\ndepth, provide a compelling framework for evaluating such deeper comprehension\nskills. Here, we present MORABLES, a human-verified benchmark built from fables\nand short stories drawn from historical literature. The main task is structured\nas multiple-choice questions targeting moral inference, with carefully crafted\ndistractors that challenge models to go beyond shallow, extractive question\nanswering. To further stress-test model robustness, we introduce adversarial\nvariants designed to surface LLM vulnerabilities and shortcuts due to issues\nsuch as data contamination. Our findings show that, while larger models\noutperform smaller ones, they remain susceptible to adversarial manipulation\nand often rely on superficial patterns rather than true moral reasoning. This\nbrittleness results in significant self-contradiction, with the best models\nrefuting their own answers in roughly 20% of cases depending on the framing of\nthe moral choice. Interestingly, reasoning-enhanced models fail to bridge this\ngap, suggesting that scale - not reasoning ability - is the primary driver of\nperformance."}
{"id": "2509.12578", "pdf": "https://arxiv.org/pdf/2509.12578.pdf", "abs": "https://arxiv.org/abs/2509.12578", "title": "Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy for Mobile Applications", "authors": ["Shuning Zhang", "Sixing Tao", "Eve He", "Yuting Yang", "Ying Ma", "Ailei Wang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Privacy policies are lengthy and complex, leading to user neglect. While\ncontextual privacy policies (CPPs) present information at the point of risk,\nthey may lack engagement and disrupt tasks. We propose Conflect, an interactive\nCPP for mobile apps, guided by a reflective thinking framework. Through three\nworkshops with experienced designers and researchers, we constructed the design\nspace of reflective thinking-based CPP design, and identified the disconnect\nbetween context and action as the most critical problem. Based on participants'\nfeedback, we designed Conflect to use sidebar alerts, allowing users to reflect\non contextualized risks and fostering their control. Our system contextually\ndetects privacy risks, extracts policy segments, and automatically generates\nrisk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and\na 4.35s latency. A user study (N=28) demonstrated that Conflect improves user\nunderstanding, trust, and satisfaction while lowering cognitive load compared\nto CPPs, privacy policies and privacy labels."}
{"id": "2509.12382", "pdf": "https://arxiv.org/pdf/2509.12382.pdf", "abs": "https://arxiv.org/abs/2509.12382", "title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "authors": ["Anu Pradhan", "Alexandra Ortan", "Apurv Verma", "Madhavan Seshadri"], "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "comment": "Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models at RecSys 2025", "summary": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework."}
{"id": "2509.12590", "pdf": "https://arxiv.org/pdf/2509.12590.pdf", "abs": "https://arxiv.org/abs/2509.12590", "title": "DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold Differential Privacy Implementation", "authors": ["Shao-Yu Chu", "Yuhe Tian", "Yu-Xiang Wang", "Haojian Jin"], "categories": ["cs.HC"], "comment": null, "summary": "This paper explores how programmers without specialized expertise in\ndifferential privacy (DP) (i.e., novices) can leverage LLMs to implement DP\nprograms with minimal training. We first conducted a need-finding study with 6\nnovices and 3 experts to understand how they utilize LLMs in DP implementation.\nWhile DP experts can implement correct DP analyses through a few prompts,\nnovices struggle to articulate their requirements in prompts and lack the\nskills to verify the correctness of the generated code. We then developed\nDPCheatSheet, an instructional tool that helps novices implement DP using LLMs.\nDPCheatSheet combines two learning concepts: it annotates an expert's workflow\nwith LLMs as a worked example to bridge the expert mindset to novices, and it\npresents five common mistakes in LLM-based DP code generation as erroneous\nexamples to support error-driven learning. We demonstrated the effectiveness of\nDPCheatSheet with an error identification study and an open-ended DP\nimplementation study."}
{"id": "2509.12385", "pdf": "https://arxiv.org/pdf/2509.12385.pdf", "abs": "https://arxiv.org/abs/2509.12385", "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection", "authors": ["Mitchell Plyler", "Yilun Zhang", "Alexander Tuzhilin", "Saoud Khalifah", "Sen Tian"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP Findings 2025", "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting."}
{"id": "2509.12626", "pdf": "https://arxiv.org/pdf/2509.12626.pdf", "abs": "https://arxiv.org/abs/2509.12626", "title": "DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI", "authors": ["Tao Long", "Xuanming Zhang", "Sitong Wang", "Zhou Yu", "Lydia B Chilton"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": "21 pages, 10 figures", "summary": "Agentic workflows promise efficiency, but adoption hinges on whether people\nactually trust systems that act on their behalf. We present DoubleAgents, an\nagentic planning tool that embeds transparency and control through user\nintervention, value-reflecting policies, rich state visualizations, and\nuncertainty flagging for human coordination tasks. A built-in respondent\nsimulation generates realistic scenarios, allowing users to rehearse, refine\npolicies, and calibrate their reliance before live use. We evaluate\nDoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a\ntechnical evaluation. Results show that participants initially hesitated to\ndelegate but grew more reliant as they experienced transparency, control, and\nadaptive learning during simulated cases. Deployment results demonstrate\nDoubleAgents' real-world relevance and usefulness, showing that the effort\nrequired scaled appropriately with task complexity and contextual data. We\ncontribute trust-by-design patterns and mechanisms for proactive AI --\nconsistency, controllability, and explainability -- along with simulation as a\nsafe path to build and calibrate trust over time."}
{"id": "2509.12405", "pdf": "https://arxiv.org/pdf/2509.12405.pdf", "abs": "https://arxiv.org/abs/2509.12405", "title": "MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering", "authors": ["Wen-wai Yim", "Asma Ben Abacha", "Zixuan Yu", "Robert Doerning", "Fei Xia", "Meliha Yetisgen"], "categories": ["cs.CL", "68T50 (Primary) 68T45 (Secondary)", "I.2.7; I.2.10"], "comment": "9 pages, 8 tables", "summary": "Evaluating natural language generation (NLG) systems in the medical domain\npresents unique challenges due to the critical demands for accuracy, relevance,\nand domain-specific expertise. Traditional automatic evaluation metrics, such\nas BLEU, ROUGE, and BERTScore, often fall short in distinguishing between\nhigh-quality outputs, especially given the open-ended nature of medical\nquestion answering (QA) tasks where multiple valid responses may exist. In this\nwork, we introduce MORQA (Medical Open-Response QA), a new multilingual\nbenchmark designed to assess the effectiveness of NLG evaluation metrics across\nthree medical visual and text-based QA datasets in English and Chinese. Unlike\nprior resources, our datasets feature 2-4+ gold-standard answers authored by\nmedical professionals, along with expert human ratings for three English and\nChinese subsets. We benchmark both traditional metrics and large language model\n(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based\napproaches significantly outperform traditional metrics in correlating with\nexpert judgments. We further analyze factors driving this improvement,\nincluding LLMs' sensitivity to semantic nuances and robustness to variability\namong reference answers. Our results provide the first comprehensive,\nmultilingual qualitative study of NLG evaluation in the medical domain,\nhighlighting the need for human-aligned evaluation methods. All datasets and\nannotations will be publicly released to support future research."}
{"id": "2509.12709", "pdf": "https://arxiv.org/pdf/2509.12709.pdf", "abs": "https://arxiv.org/abs/2509.12709", "title": "Harnessing the Power of AI in Qualitative Research: Role Assignment, Engagement, and User Perceptions of AI-Generated Follow-Up Questions in Semi-Structured Interviews", "authors": ["He Zhang", "Yueyan Liu", "Xin Guan", "Jie Cai", "John M. Carroll"], "categories": ["cs.HC"], "comment": "19 pages, 8 figures", "summary": "Semi-structured interviews highly rely on the quality of follow-up questions,\nyet interviewers' knowledge and skills may limit their depth and potentially\naffect outcomes. While many studies have shown the usefulness of large language\nmodels (LLMs) for qualitative analysis, their possibility in the data\ncollection process remains underexplored. We adopt an AI-driven \"Wizard-of-Oz\"\nsetup to investigate how real-time LLM support in generating follow-up\nquestions shapes semi-structured interviews. Through a study with 17\nparticipants, we examine the value of LLM-generated follow-up questions, the\nevolving division of roles, relationships, collaborative behaviors, and\nresponsibilities between interviewers and AI. Our findings (1) provide\nempirical evidence of the strengths and limitations of AI-generated follow-up\nquestions (AGQs); (2) introduce a Human-AI collaboration framework in this\ninterview context; and (3) propose human-centered design guidelines for\nAI-assisted interviewing. We position LLMs as complements, not replacements, to\nhuman judgment, and highlight pathways for integrating AI into qualitative data\ncollection."}
{"id": "2509.12440", "pdf": "https://arxiv.org/pdf/2509.12440.pdf", "abs": "https://arxiv.org/abs/2509.12440", "title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts", "authors": ["Jiayi He", "Yangmin Huang", "Qianyun Du", "Xiangying Zhou", "Zhiyang He", "Jiaxue Hu", "Xiaodong Tao", "Lixian Lai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing deployment of Large Language Models (LLMs) in healthcare\nnecessitates a rigorous evaluation of their factual reliability. However,\nexisting benchmarks are often limited by narrow domains of data, failing to\ncapture the complexity of real-world medical information. To address this\ncritical gap, we introduce MedFact, a new and challenging benchmark for Chinese\nmedical fact-checking. MedFact comprises 2,116 expert-annotated instances\ncurated from diverse real-world texts, spanning 13 medical specialties, 8\nfine-grained error types, 4 writing styles, and multiple difficulty levels. Its\nconstruction employs a hybrid AI-human framework where iterative expert\nfeedback refines an AI-driven, multi-criteria filtering process, ensuring both\nhigh data quality and difficulty. We conduct a comprehensive evaluation of 20\nleading LLMs, benchmarking their performance on veracity classification and\nerror localization against a human expert baseline. Our results reveal that\nwhile models can often determine if a text contains an error, precisely\nlocalizing it remains a substantial challenge, with even top-performing models\nfalling short of human performance. Furthermore, our analysis uncovers a\nfrequent ``over-criticism'' phenomenon, a tendency for models to misidentify\ncorrect information as erroneous, which is exacerbated by advanced reasoning\ntechniques such as multi-agent collaboration and inference-time scaling. By\nhighlighting these critical challenges for deploying LLMs in medical\napplications, MedFact provides a robust resource to drive the development of\nmore factually reliable and medically aware models."}
{"id": "2509.12752", "pdf": "https://arxiv.org/pdf/2509.12752.pdf", "abs": "https://arxiv.org/abs/2509.12752", "title": "Participatory AI: A Scandinavian Approach to Human-Centered AI", "authors": ["Niklas Elmqvist", "Eve Hoggan", "Hans-Jörg Schulz", "Marianne Graves Petersen", "Peter Dalsgaard", "Ira Assent", "Olav W. Bertelsen", "Akhil Arora", "Kaj Grønbæk", "Susanne Bødker", "Clemens Nylandsted Klokmose", "Rachel Charlotte Smith", "Sebastian Hubenschmid", "Christoph A. Johns", "Gabriela Molina León", "Anton Wolter", "Johannes Ellemose", "Vaishali Dhanoa", "Simon Aagaard Enni", "Mille Skovhus Lunding", "Karl-Emil Kjær Bilstrup", "Juan Sánchez Esquivel", "Luke Connelly", "Rafael Pablos Sarabia", "Morten Birk", "Joachim Nyborg", "Stefanie Zollmann", "Tobias Langlotz", "Meredith Siang-Yun Chou", "Jens Emil Sloth Grønbæk", "Michael Wessely", "Yijing Jiang", "Caroline Berger", "Duosi Dai", "Michael Mose Biskjaer", "Germán Leiva", "Jonas Frich", "Eva Eriksson", "Kim Halskov", "Thorbjørn Mikkelsen", "Nearchos Potamitis", "Michel Yildirim", "Arvind Srinivasan", "Jeanette Falk", "Nanna Inie", "Ole Sejer Iversen", "Hugo Andersson"], "categories": ["cs.HC", "H.5.2; H.1.2"], "comment": "32 pages, 7 figures", "summary": "AI's transformative impact on work, education, and everyday life makes it as\nmuch a political artifact as a technological one. Current AI models are opaque,\ncentralized, and overly generic. The algorithmic automation they provide\nthreatens human agency and democratic values in both workplaces and daily life.\nTo confront such challenges, we turn to Scandinavian Participatory Design (PD),\nwhich was devised in the 1970s to face a similar threat from mechanical\nautomation. In the PD tradition, technology is seen not just as an artifact,\nbut as a locus of democracy. Drawing from this tradition, we propose\nParticipatory AI as a PD approach to human-centered AI that applies five PD\nprinciples to four design challenges for algorithmic automation. We use\nconcrete case studies to illustrate how to treat AI models less as proprietary\nproducts and more as shared socio-technical systems that enhance rather than\ndiminish human agency, human dignity, and human values."}
{"id": "2509.12451", "pdf": "https://arxiv.org/pdf/2509.12451.pdf", "abs": "https://arxiv.org/abs/2509.12451", "title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning", "authors": ["Wonbin Kweon", "SeongKu Kang", "Runchu Tian", "Pengcheng Jiang", "Jiawei Han", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "The effectiveness of in-context learning relies heavily on selecting\ndemonstrations that provide all the necessary information for a given test\ninput. To achieve this, it is crucial to identify and cover fine-grained\nknowledge requirements. However, prior methods often retrieve demonstrations\nbased solely on embedding similarity or generation probability, resulting in\nirrelevant or redundant examples. In this paper, we propose TopicK, a topic\ncoverage-based retrieval framework that selects demonstrations to\ncomprehensively cover topic-level knowledge relevant to both the test input and\nthe model. Specifically, TopicK estimates the topics required by the input and\nassesses the model's knowledge on those topics. TopicK then iteratively selects\ndemonstrations that introduce previously uncovered required topics, in which\nthe model exhibits low topical knowledge. We validate the effectiveness of\nTopicK through extensive experiments across various datasets and both open- and\nclosed-source LLMs. Our source code is available at\nhttps://github.com/WonbinKweon/TopicK_EMNLP2025."}
{"id": "2509.12773", "pdf": "https://arxiv.org/pdf/2509.12773.pdf", "abs": "https://arxiv.org/abs/2509.12773", "title": "PLUTO: A Public Value Assessment Tool", "authors": ["Laura Koesten", "Péter Ferenc Gyarmati", "Connor Hogan", "Bernhard Jordan", "Seliem El-Sayed", "Barbara Prainsack", "Torsten Möller"], "categories": ["cs.HC"], "comment": null, "summary": "We present PLUTO (Public VaLUe Assessment TOol), a framework for assessing\nthe public value of specific instances of data use. Grounded in the concept of\ndata solidarity, PLUTO aims to empower diverse stakeholders - including\nregulatory bodies, private enterprises, NGOs, and individuals - to critically\nengage with data projects through a structured assessment of the risks and\nbenefits of data use, and by encouraging critical reflection. This paper\ndiscusses the theoretical foundation, development process, and initial user\nexperiences with PLUTO. Key challenges include translating qualitative\nassessments of benefits and risks into actionable quantitative metrics while\nmaintaining inclusivity and transparency. Initial feedback highlights PLUTO's\npotential to foster responsible decision-making and shared accountability in\ndata practices."}
{"id": "2509.12459", "pdf": "https://arxiv.org/pdf/2509.12459.pdf", "abs": "https://arxiv.org/abs/2509.12459", "title": "Does Language Model Understand Language?", "authors": ["Suvojit Acharjee", "Utathya Aich", "Asfak Ali"], "categories": ["cs.CL"], "comment": null, "summary": "Despite advances in natural language generation and understanding, LM still\nstruggle with fine grained linguistic phenomena such as tense, negation, voice,\nand modality which are the elements central to effective human communication.\nIn the context of the United Nations SDG 4, where linguistic clarity is\ncritical, the deployment of LMs in educational technologies demands careful\nscrutiny. As LMs are increasingly powering applications like tutoring systems,\nautomated grading, and translation, their alignment with human linguistic\ninterpretation becomes essential for effective learning. In this study, we\nconduct a evaluation of SOTA language models across these challenging contexts\nin both English and Bengali. To ensure a structured assessment, we introduce a\nnew Route for Evaluation of Cognitive Inference in Systematic Environments\nguidelines. Our proposed LUCID dataset, composed of carefully crafted sentence\npairs in English and Bengali, specifically challenges these models on critical\naspects of language comprehension, including negation, tense, voice variations.\nWe assess the performance of SOTA models including MISTRAL-SABA-24B,\nLLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard\nmetrics like Pearson correlation, Spearman correlation, and Mean Absolute\nError, as well as novel, linguistically inspired metric the HCE accuracy. The\nHCE accuracy measures how often model predictions fall within one standard\ndeviation of the mean human rating, thus capturing human like tolerance for\nvariability in language interpretation. Our findings highlight Compound-Beta as\nthe most balanced model, consistently achieving high correlations and low MAEs\nacross diverse language conditions. It records the highest Pearson correlation\nin English and demonstrates robust performance on mixed-language data,\nindicating a strong alignment with human judgments in cross lingual scenarios."}
{"id": "2509.12794", "pdf": "https://arxiv.org/pdf/2509.12794.pdf", "abs": "https://arxiv.org/abs/2509.12794", "title": "The Impact of Automation on Risk-Taking: The Role of Sense of Agency", "authors": ["Yang Chen", "Zhijun Zhang"], "categories": ["cs.HC"], "comment": "37 pages, 9 figures", "summary": "Automation significantly alters human behavior, particularly risk-taking.\nPrevious researches have paid limited attention to the underlying\ncharacteristics of automation and their mechanisms of influence on risk-taking.\nThis study investigated how automation affects risk-taking and examined the\nrole of sense of agency therein. By quantifying sense of agency through\nsubjective ratings, this research explored the impact of automation level and\nreliability level on risk-taking. The results of three experiments indicated\nthat automation reduced the level of risk-taking; higher automation level was\nassociated with lower sense of agency and lower risk-taking, with sense of\nagency playing a complete mediating role; higher automation reliability was\nassociated with higher sense of agency and higher risk-taking, with sense of\nagency playing a partial mediating role. The study concludes that automation\ninfluences risk-taking, such that higher automation level or lower reliability\nis associated with a lower likelihood of risk-taking. Sense of agency mediates\nthe impact of automation on risk-taking, and automation level and reliability\nhave different effects on risk-taking."}
{"id": "2509.12476", "pdf": "https://arxiv.org/pdf/2509.12476.pdf", "abs": "https://arxiv.org/abs/2509.12476", "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "authors": ["Sumanta Bhattacharyya", "Sara Riaz", "Pedram Rooshenas"], "categories": ["cs.CL"], "comment": null, "summary": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond."}
{"id": "2509.12816", "pdf": "https://arxiv.org/pdf/2509.12816.pdf", "abs": "https://arxiv.org/abs/2509.12816", "title": "Gesture Evaluation in Virtual Reality", "authors": ["Axel Wiebe Werner", "Jonas Beskow", "Anna Deichler"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG", "68T50, 68T07, 68U35", "H.5.1; H.5.2; I.2.10; I.3.7"], "comment": "Published in Proceedings of the 26th International Conference on\n  Multimodal Interaction (ICMI '24), ACM. Copyright 2024 ACM. Licensed under CC\n  BY", "summary": "Gestures are central to human communication, enriching interactions through\nnon-verbal expression. Virtual avatars increasingly use AI-generated gestures\nto enhance life-likeness, yet evaluations have largely been confined to 2D.\nVirtual Reality (VR) provides an immersive alternative that may affect how\ngestures are perceived. This paper presents a comparative evaluation of\ncomputer-generated gestures in VR and 2D, examining three models from the 2023\nGENEA Challenge. Results show that gestures viewed in VR were rated slightly\nhigher on average, with the strongest effect observed for motion-capture \"true\nmovement.\" While model rankings remained consistent across settings, VR\ninfluenced participants' overall perception and offered unique benefits over\ntraditional 2D evaluation."}
{"id": "2509.12508", "pdf": "https://arxiv.org/pdf/2509.12508.pdf", "abs": "https://arxiv.org/abs/2509.12508", "title": "FunAudio-ASR Technical Report", "authors": ["Keyu An", "Yanni Chen", "Chong Deng", "Changfeng Gao", "Zhifu Gao", "Bo Gong", "Xiangang Li", "Yabin Li", "Xiang Lv", "Yunjie Ji", "Yiheng Jiang", "Bin Ma", "Haoneng Luo", "Chongjia Ni", "Zexu Pan", "Yiping Peng", "Zhendong Peng", "Peiyao Wang", "Hao Wang", "Wen Wang", "Wupeng Wang", "Biao Tian", "Zhentao Tan", "Nan Yang", "Bin Yuan", "Jieping Ye", "Jixing Yu", "Qinglin Zhang", "Kun Zou", "Han Zhao", "Shengkui Zhao", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings."}
{"id": "2509.13039", "pdf": "https://arxiv.org/pdf/2509.13039.pdf", "abs": "https://arxiv.org/abs/2509.13039", "title": "Winds Through Time: Interactive Data Visualization and Physicalization for Paleoclimate Communication", "authors": ["David Hunter", "Pablo Botin", "Emily Snode-Brenneman", "Amy Stevermer", "Becca Hatheway", "Dillon Amaya", "Eddie Goldstein", "Wayne A Seltzer", "Mark D Gross", "Kris Karnauskas", "Daniel Leithinger", "Ellen Yi-Luen Do"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "We describe a multidisciplinary collaboration to iteratively design an\ninteractive exhibit for a public science center on paleoclimate, the study of\npast climates. We created a data physicalisation of mountains and ice sheets\nthat can be tangibly manipulated by visitors to interact with a wind simulation\nvisualisation that demonstrates how the climate of North America differed\ndramatically between now and the peak of the last ice age. We detail the system\nfor interaction and visualisation plus design choices to appeal to an audience\nthat ranges from children to scientists and responds to site requirements."}
{"id": "2509.12514", "pdf": "https://arxiv.org/pdf/2509.12514.pdf", "abs": "https://arxiv.org/abs/2509.12514", "title": "A comparison of pipelines for the translation of a low resource language based on transformers", "authors": ["Chiara Bonfanti", "Michele Colombino", "Giulia Coucourde", "Faeze Memari", "Stefano Pinardi", "Rosa Meo"], "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.LG"], "comment": "9 pages, 4 figures", "summary": "This work compares three pipelines for training transformer-based neural\nnetworks to produce machine translators for Bambara, a Mand\\`e language spoken\nin Africa by about 14,188,850 people. The first pipeline trains a simple\ntransformer to translate sentences from French into Bambara. The second\nfine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures\nfor French-to-Bambara translation. Models from the first two pipelines were\ntrained with different hyperparameter combinations to improve BLEU and chrF\nscores, evaluated on both test sentences and official Bambara benchmarks. The\nthird pipeline uses language distillation with a student-teacher dual neural\nnetwork to integrate Bambara into a pre-trained LaBSE model, which provides\nlanguage-agnostic embeddings. A BERT extension is then applied to LaBSE to\ngenerate translations. All pipelines were tested on Dokotoro (medical) and\nBayelemagaba (mixed domains). Results show that the first pipeline, although\nsimpler, achieves the best translation accuracy (10% BLEU, 21% chrF on\nBayelemagaba), consistent with low-resource translation results. On the Yiri\ndataset, created for this work, it achieves 33.81% BLEU and 41% chrF.\nInstructor-based models perform better on single datasets than on aggregated\ncollections, suggesting they capture dataset-specific patterns more\neffectively."}
{"id": "2509.13051", "pdf": "https://arxiv.org/pdf/2509.13051.pdf", "abs": "https://arxiv.org/abs/2509.13051", "title": "More than Meets the Eye: Understanding the Effect of Individual Objects on Perceived Visual Privacy", "authors": ["Mete Harun Akcay", "Siddharth Prakash Rao", "Alexandros Bakas", "Buse Gul Atli"], "categories": ["cs.HC"], "comment": "27 pages, 5 figures, 11 tables. In submission", "summary": "User-generated content, such as photos, comprises the majority of online\nmedia content and drives engagement due to the human ability to process visual\ninformation quickly. Consequently, many online platforms are designed for\nsharing visual content, with billions of photos posted daily. However, photos\noften reveal more than they intended through visible and contextual cues,\nleading to privacy risks. Previous studies typically treat privacy as a\nproperty of the entire image, overlooking individual objects that may carry\nvarying privacy risks and influence how users perceive it. We address this gap\nwith a mixed-methods study (n = 92) to understand how users evaluate the\nprivacy of images containing multiple sensitive objects. Our results reveal\nmental models and nuanced patterns that uncover how granular details, such as\nphoto-capturing context and co-presence of other objects, affect privacy\nperceptions. These novel insights could enable personalized, context-aware\nprivacy protection designs on social media and future technologies."}
{"id": "2509.12591", "pdf": "https://arxiv.org/pdf/2509.12591.pdf", "abs": "https://arxiv.org/abs/2509.12591", "title": "MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models", "authors": ["Vijay Govindarajan", "Pratik Patel", "Sahil Tripathi", "Md Azizul Hoque", "Gautam Siddharth Kashyap"], "categories": ["cs.CL"], "comment": "Accepted in The 26th International Conference on Web Information\n  Systems Engineering (WISE), scheduled for 15-17 December 2025 in Marrakech,\n  Morocco", "summary": "Automated Audio Captioning (AAC) generates captions for audio clips but faces\nchallenges due to limited datasets compared to image captioning. To overcome\nthis, we propose the zero-shot AAC system that leverages pre-trained models,\neliminating the need for extensive training. Our approach uses a pre-trained\naudio CLIP model to extract auditory features and generate a structured prompt,\nwhich guides a Large Language Model (LLM) in caption generation. Unlike\ntraditional greedy decoding, our method refines token selection through the\naudio CLIP model, ensuring alignment with the audio content. Experimental\nresults demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using\nMAGIC search with the WavCaps model. The performance is heavily influenced by\nthe audio-text matching model and keyword selection, with optimal results\nachieved using a single keyword prompt, and a 50% performance drop when no\nkeyword list is used."}
{"id": "2509.13064", "pdf": "https://arxiv.org/pdf/2509.13064.pdf", "abs": "https://arxiv.org/abs/2509.13064", "title": "Patient Perspectives on Telemonitoring during Colorectal Cancer Surgery Prehabilitation", "authors": ["Irina Bianca Serban", "Dimitra Dritsa", "David ten Cate", "Loes Janssen", "Margot Heijmans", "Sara Colombo", "Aarnout Brombacher", "Steven Houben"], "categories": ["cs.HC"], "comment": "20 pages, 3 figures, presented at the 19th EAI International\n  Conference on Pervasive Computing Technologies for Healthcare, to be\n  published in the Springer - LNICST series", "summary": "Multimodal prehabilitation for colorectal cancer (CRC) surgery aims to\noptimize patient fitness and reduce postoperative complications. While\ntelemonitoring's clinical value in supporting decision-making is recognized,\npatient perspectives on its use in prehabilitation remain underexplored,\nparticularly compared to its related clinical context, rehabilitation. To\naddress this gap, we conducted interviews with five patients who completed a\nfour-week CRC prehabilitation program incorporating continuous telemonitoring.\nOur findings reveal patients' willingness to engage with telemonitoring, shaped\nby their motivations, perceived benefits, and concerns. We outline design\nconsiderations for patient-centered systems and offer a foundation for further\nresearch on telemonitoring in CRC prehabilitation."}
{"id": "2509.12603", "pdf": "https://arxiv.org/pdf/2509.12603.pdf", "abs": "https://arxiv.org/abs/2509.12603", "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving", "authors": ["Mukai Li", "Linfeng Song", "Zhenwen Liang", "Jiahao Xu", "Shansan Gong", "Qi Liu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance."}
{"id": "2509.13191", "pdf": "https://arxiv.org/pdf/2509.13191.pdf", "abs": "https://arxiv.org/abs/2509.13191", "title": "Textarium: Entangling Annotation, Abstraction and Argument", "authors": ["Philipp Proff", "Marian Dörk"], "categories": ["cs.HC", "cs.CL", "H.5.2; H.5.4; I.7.1; J.5"], "comment": "This is the authors' version of the article presented at VIS4DH and\n  published in the proceedings of IEEE VIS 2025", "summary": "We present a web-based environment that connects annotation, abstraction, and\nargumentation during the interpretation of text. As a visual interface for\nscholarly reading and writing, Textarium combines human analysis with\nlightweight computational processing to bridge close and distant reading\npractices. Readers can highlight text, group keywords into concepts, and embed\nthese observations as anchors in essays. The interface renders these\ninterpretive actions as parameterized visualization states. Through a\nspeculative design process of co-creative and iterative prototyping, we\ndeveloped a reading-writing approach that makes interpretive processes\ntransparent and shareable within digital narratives."}
{"id": "2509.12635", "pdf": "https://arxiv.org/pdf/2509.12635.pdf", "abs": "https://arxiv.org/abs/2509.12635", "title": "Positional Encoding via Token-Aware Phase Attention", "authors": ["Yu", "Wang", "Sheng Shen", "Rémi Munos", "Hongyuan Zhan", "Yuandong Tian"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages", "summary": "We prove under practical assumptions that Rotary Positional Embedding (RoPE)\nintroduces an intrinsic distance-dependent bias in attention scores that limits\nRoPE's ability to model long-context. RoPE extension methods may alleviate this\nissue, but they typically require post-hoc adjustments after pretraining, such\nas rescaling or hyperparameters retuning. This paper introduces Token-Aware\nPhase Attention (TAPA), a new positional encoding method that incorporates a\nlearnable phase function into the attention mechanism. TAPA preserves token\ninteractions over long range, extends to longer contexts with direct and light\nfine-tuning, extrapolates to unseen lengths, and attains significantly lower\nperplexity on long-context than RoPE families."}
{"id": "2509.13253", "pdf": "https://arxiv.org/pdf/2509.13253.pdf", "abs": "https://arxiv.org/abs/2509.13253", "title": "Evolution of Programmers' Trust in Generative AI Programming Assistants", "authors": ["Anshul Shah", "Thomas Rexin", "Elena Tomson", "Leo Porter", "William G. Griswold", "Adalbert Gerald Soosai Raj"], "categories": ["cs.HC", "cs.SE"], "comment": "Koli Calling 2025 conference", "summary": "Motivation. Trust in generative AI programming assistants is a vital attitude\nthat impacts how programmers use those programming assistants. Programmers that\nare over-trusting may be too reliant on their tools, leading to incorrect or\nvulnerable code; programmers that are under-trusting may avoid using tools that\ncan improve their productivity and well-being.\n  Methods. Since trust is a dynamic attitude that may change over time, this\nstudy aims to understand programmers' evolution of trust after immediate (one\nhour) and extended (10 days) use of GitHub Copilot. We collected survey data\nfrom 71 upper-division computer science students working on a legacy code base,\nrepresenting a population that is about to enter the workforce. In this study,\nwe quantitatively measure student trust levels and qualitatively uncover why\nstudent trust changes.\n  Findings. Student trust, on average, increased over time. After completing a\nproject with Copilot, however, students felt that Copilot requires a competent\nprogrammer to complete some tasks manually. Students mentioned that seeing\nCopilot's correctness, understanding how Copilot uses context from the code\nbase, and learning some basics of natural language processing contributed to\ntheir elevated trust.\n  Implications. Our study helps instructors and industry managers understand\nthe factors that influence how students calibrate their trust with AI\nassistants. We make four pedagogical recommendations, which are that CS\neducators should 1) provide opportunities for students to work with Copilot on\nchallenging software engineering tasks to calibrate their trust, 2) teach\ntraditional skills of comprehending, debugging, and testing so students can\nverify output, 3) teach students about the basics of natural language\nprocessing, and 4) explicitly introduce and demonstrate the range of features\navailable in Copilot."}
{"id": "2509.12647", "pdf": "https://arxiv.org/pdf/2509.12647.pdf", "abs": "https://arxiv.org/abs/2509.12647", "title": "PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition", "authors": ["Li Fu", "Yu Xin", "Sunlu Zeng", "Lu Fan", "Youzheng Wu", "Xiaodong He"], "categories": ["cs.CL", "eess.AS"], "comment": "Submitted to ICASSP 2026", "summary": "This paper presents a Pronunciation-Aware Contextualized (PAC) framework to\naddress two key challenges in Large Language Model (LLM)-based Automatic Speech\nRecognition (ASR) systems: effective pronunciation modeling and robust\nhomophone discrimination. Both are essential for raw or long-tail word\nrecognition. The proposed approach adopts a two-stage learning paradigm. First,\nwe introduce a pronunciation-guided context learning method. It employs an\ninterleaved grapheme-phoneme context modeling strategy that incorporates\ngrapheme-only distractors, encouraging the model to leverage phonemic cues for\naccurate recognition. Then, we propose a pronunciation-discriminative\nreinforcement learning method with perturbed label sampling to further enhance\nthe model\\'s ability to distinguish contextualized homophones. Experimental\nresults on the public English Librispeech and Mandarin AISHELL-1 datasets\nindicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and\n53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and\n60.5% relative reductions in biased WER for long-tail words compared to strong\nbaselines, respectively."}
{"id": "2509.13291", "pdf": "https://arxiv.org/pdf/2509.13291.pdf", "abs": "https://arxiv.org/abs/2509.13291", "title": "Towards an Embodied Composition Framework for Organizing Immersive Computational Notebooks", "authors": ["Sungwon In", "Eric Krokos", "Kirsten Whitley", "Chris North", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 9 figures, The ACM Symposium on Virtual Reality Software\n  and Technology (VRST) 2025", "summary": "As immersive technologies evolve, immersive computational notebooks offer new\nopportunities for interacting with code, data, and outputs. However, scaling\nthese environments remains a challenge, particularly when analysts manually\narrange large numbers of cells to maintain both execution logic and visual\ncoherence. To address this, we introduce an embodied composition framework,\nfacilitating organizational processes in the context of immersive computational\nnotebooks. To evaluate the effectiveness of the embodied composition framework,\nwe conducted a controlled user study comparing manual and embodied composition\nframeworks in an organizational process. The results show that embodied\ncomposition frameworks significantly reduced user effort and decreased\ncompletion time. However, the design of the triggering mechanism requires\nfurther refinement. Our findings highlight the potential of embodied\ncomposition frameworks to enhance the scalability of the organizational process\nin immersive computational notebooks."}
{"id": "2509.12652", "pdf": "https://arxiv.org/pdf/2509.12652.pdf", "abs": "https://arxiv.org/abs/2509.12652", "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models", "authors": ["Paul Kröger", "Emilio Barkett"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior."}
{"id": "2509.13295", "pdf": "https://arxiv.org/pdf/2509.13295.pdf", "abs": "https://arxiv.org/abs/2509.13295", "title": "Investigating Seamless Transitions Between Immersive Computational Notebooks and Embodied Data Interactions", "authors": ["Sungwon In", "Eric Krokos", "Kirsten Whitley", "Chris North", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 7 figures, The ACM Symposium on Virtual Reality Software\n  and Technology (VRST)", "summary": "A growing interest in Immersive Analytics (IA) has led to the extension of\ncomputational notebooks (e.g., Jupyter Notebook) into an immersive environment\nto enhance analytical workflows. However, existing solutions rely on the WIMP\n(windows, icons, menus, pointer) metaphor, which remains impractical for\ncomplex data exploration. Although embodied interaction offers a more intuitive\nalternative, immersive computational notebooks and embodied data exploration\nsystems are implemented as standalone tools. This separation requires analysts\nto invest considerable effort to transition from one environment to an entirely\ndifferent one during analytical workflows. To address this, we introduce ICoN,\na prototype that facilitates a seamless transition between computational\nnotebooks and embodied data explorations within a unified, fully immersive\nenvironment. Our findings reveal that unification improves transition\nefficiency and intuitiveness during analytical workflows, highlighting its\npotential for seamless data analysis."}
{"id": "2509.12661", "pdf": "https://arxiv.org/pdf/2509.12661.pdf", "abs": "https://arxiv.org/abs/2509.12661", "title": "Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations", "authors": ["Yougen Zhou", "Qin Chen", "Ningning Zhou", "Jie Zhou", "Xingjiao Wu", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Emotional support conversation (ESC) aims to alleviate distress through\nempathetic dialogue, yet large language models (LLMs) face persistent\nchallenges in delivering effective ESC due to low accuracy in strategy\nplanning. Moreover, there is a considerable preference bias towards specific\nstrategies. Prior methods using fine-tuned strategy planners have shown\npotential in reducing such bias, while the underlying causes of the preference\nbias in LLMs have not well been studied. To address these issues, we first\nreveal the fundamental causes of the bias by identifying the knowledge\nboundaries of LLMs in strategy planning. Then, we propose an approach to\nmitigate the bias by reinforcement learning with a dual reward function, which\noptimizes strategy planning via both accuracy and entropy-based confidence for\neach region according to the knowledge boundaries. Experiments on the ESCov and\nExTES datasets with multiple LLM backbones show that our approach outperforms\nthe baselines, confirming the effectiveness of our approach."}
{"id": "2509.12290", "pdf": "https://arxiv.org/pdf/2509.12290.pdf", "abs": "https://arxiv.org/abs/2509.12290", "title": "Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight", "authors": ["Jonas C. Ditz", "Veronika Lazar", "Elmar Lichtmeß", "Carola Plesch", "Matthias Heck", "Kevin Baum", "Markus Langer"], "categories": ["cs.CR", "cs.CY", "cs.HC"], "comment": null, "summary": "Human oversight of AI is promoted as a safeguard against risks such as\ninaccurate outputs, system malfunctions, or violations of fundamental rights,\nand is mandated in regulation like the European AI Act. Yet debates on human\noversight have largely focused on its effectiveness, while overlooking a\ncritical dimension: the security of human oversight. We argue that human\noversight creates a new attack surface within the safety, security, and\naccountability architecture of AI operations. Drawing on cybersecurity\nperspectives, we analyze attack vectors that threaten the requirements of\neffective human oversight, thereby undermining the safety of AI operations.\nSuch attacks may target the AI system, its communication with oversight\npersonnel, or the personnel themselves. We then outline hardening strategies to\nmitigate these risks. Our contributions are: (1) introducing a security\nperspective on human oversight, and (2) providing an overview of attack vectors\nand hardening strategies to enable secure human oversight of AI."}
{"id": "2509.12662", "pdf": "https://arxiv.org/pdf/2509.12662.pdf", "abs": "https://arxiv.org/abs/2509.12662", "title": "Chat-Driven Text Generation and Interaction for Person Retrieval", "authors": ["Zequn Xie", "Chuxin Wang", "Sihang Cai", "Yeqiang Wang", "Shulei Wang", "Tao Jin"], "categories": ["cs.CL", "I.2.7; I.4.9"], "comment": "Accepted by EMNLP 2025. 13 pages, 3 figures", "summary": "Text-based person search (TBPS) enables the retrieval of person images from\nlarge-scale databases using natural language descriptions, offering critical\nvalue in surveillance applications. However, a major challenge lies in the\nlabor-intensive process of obtaining high-quality textual annotations, which\nlimits scalability and practical deployment. To address this, we introduce two\ncomplementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text\nInteraction (MTI). MTG generates rich pseudo-labels through simulated dialogues\nwith MLLMs, producing fine-grained and diverse visual descriptions without\nmanual supervision. MTI refines user queries at inference time through dynamic,\ndialogue-based reasoning, enabling the system to interpret and resolve vague,\nincomplete, or ambiguous descriptions - characteristics often seen in\nreal-world search scenarios. Together, MTG and MTI form a unified and\nannotation-free framework that significantly improves retrieval accuracy,\nrobustness, and usability. Extensive evaluations demonstrate that our method\nachieves competitive or superior results while eliminating the need for manual\ncaptions, paving the way for scalable and practical deployment of TBPS systems."}
{"id": "2509.12361", "pdf": "https://arxiv.org/pdf/2509.12361.pdf", "abs": "https://arxiv.org/abs/2509.12361", "title": "What News Recommendation Research Did (But Mostly Didn't) Teach Us About Building A News Recommender", "authors": ["Karl Higley", "Robin Burke", "Michael D. Ekstrand", "Bart P. Knijnenburg"], "categories": ["cs.IR", "cs.CY", "cs.HC"], "comment": null, "summary": "One of the goals of recommender systems research is to provide insights and\nmethods that can be used by practitioners to build real-world systems that\ndeliver high-quality recommendations to actual people grounded in their genuine\ninterests and needs. We report on our experience trying to apply the news\nrecommendation literature to build POPROX, a live platform for news\nrecommendation research, and reflect on the extent to which the current state\nof research supports system-building efforts. Our experience highlights several\nunexpected challenges encountered in building personalization features that are\ncommonly found in products from news aggregators and publishers, and shows how\nthose difficulties are connected to surprising gaps in the literature. Finally,\nwe offer a set of lessons learned from building a live system with a persistent\nuser base and highlight opportunities to make future news recommendation\nresearch more applicable and impactful in practice."}
{"id": "2509.12672", "pdf": "https://arxiv.org/pdf/2509.12672.pdf", "abs": "https://arxiv.org/abs/2509.12672", "title": "Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content", "authors": ["Shaz Furniturewala", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": null, "summary": "The volume of machine-generated content online has grown dramatically due to\nthe widespread use of Large Language Models (LLMs), leading to new challenges\nfor content moderation systems. Conventional content moderation classifiers,\nwhich are usually trained on text produced by humans, suffer from\nmisclassifications due to LLM-generated text deviating from their training data\nand adversarial attacks that aim to avoid detection. Present-day defence\ntactics are reactive rather than proactive, since they rely on adversarial\ntraining or external detection models to identify attacks. In this work, we aim\nto identify the vulnerable components of toxicity classifiers that contribute\nto misclassification, proposing a novel strategy based on mechanistic\ninterpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa\nclassifiers, testing on diverse datasets spanning a variety of minority groups.\nWe use adversarial attacking techniques to identify vulnerable circuits.\nFinally, we suppress these vulnerable circuits, improving performance against\nadversarial attacks. We also provide demographic-level insights into these\nvulnerable circuits, exposing fairness and robustness gaps in model training.\nWe find that models have distinct heads that are either crucial for performance\nor vulnerable to attack and suppressing the vulnerable heads improves\nperformance on adversarial input. We also find that different heads are\nresponsible for vulnerability across different demographic groups, which can\ninform more inclusive development of toxicity detection models."}
{"id": "2509.12455", "pdf": "https://arxiv.org/pdf/2509.12455.pdf", "abs": "https://arxiv.org/abs/2509.12455", "title": "Funding AI for Good: A Call for Meaningful Engagement", "authors": ["Hongjin Lin", "Anna Kawakami", "Catherine D'Ignazio", "Kenneth Holstein", "Krzysztof Gajos"], "categories": ["cs.CY", "cs.HC"], "comment": "Currently under review", "summary": "Artificial Intelligence for Social Good (AI4SG) is a growing area exploring\nAI's potential to address social issues like public health. Yet prior work has\nshown limited evidence of its tangible benefits for intended communities, and\nprojects frequently face inadequate community engagement and sustainability\nchallenges. Funding agendas play a crucial role in framing AI4SG initiatives\nand shaping their approaches. Through a qualitative analysis of 35 funding\ndocuments -- representing about $410 million USD in total investments, we\nreveal dissonances between AI4SG's stated intentions for positive social impact\nand the techno-centric approaches that some funding agendas promoted. Drawing\non our findings, we offer recommendations for funders to scaffold approaches\nthat balance both contextual understanding and technical capacities in future\nfunding call designs. We call for greater engagement between AI4SG funders and\nthe HCI community to support community engagement work in the funding program\ndesign process."}
{"id": "2509.12677", "pdf": "https://arxiv.org/pdf/2509.12677.pdf", "abs": "https://arxiv.org/abs/2509.12677", "title": "Case-Based Decision-Theoretic Decoding with Quality Memories", "authors": ["Hiroyuki Deguchi", "Masaaki Nagata"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP2025 main", "summary": "Minimum Bayes risk (MBR) decoding is a decision rule of text generation,\nwhich selects the hypothesis that maximizes the expected utility and robustly\ngenerates higher-quality texts than maximum a posteriori (MAP) decoding.\nHowever, it depends on sample texts drawn from the text generation model; thus,\nit is difficult to find a hypothesis that correctly captures the knowledge or\ninformation of out-of-domain. To tackle this issue, we propose case-based\ndecision-theoretic (CBDT) decoding, another method to estimate the expected\nutility using examples of domain data. CBDT decoding not only generates\nhigher-quality texts than MAP decoding, but also the combination of MBR and\nCBDT decoding outperformed MBR decoding in seven domain De--En and\nJa$\\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO\nand nocaps datasets."}
{"id": "2509.12495", "pdf": "https://arxiv.org/pdf/2509.12495.pdf", "abs": "https://arxiv.org/abs/2509.12495", "title": "Physical Complexity of a Cognitive Artifact", "authors": ["Gülce Kardeş", "David Krakauer", "Joshua Grochow"], "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Cognitive science and theoretical computer science both seek to classify and\nexplain the difficulty of tasks. Mechanisms of intelligence are those that\nreduce task difficulty. Here we map concepts from the computational complexity\nof a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies\nthrough a ``Principle of Materiality''. By analyzing the puzzle's branching\nfactor, measured through search tree outdegree, we quantitatively assess task\ndifficulty and systematically examine how different strategies modify\ncomplexity. We incrementally refine a trial-and-error search by layering\npreprocessing (cognitive chunking), value ordering (cognitive free-sorting),\nvariable ordering (cognitive scaffolding), and pruning (cognitive inference).\nWe discuss how the competent use of artifacts reduces effective time complexity\nby exploiting physical constraints and propose a model of intelligence as a\nlibrary of algorithms that recruit the capabilities of both mind and matter."}
{"id": "2509.12720", "pdf": "https://arxiv.org/pdf/2509.12720.pdf", "abs": "https://arxiv.org/abs/2509.12720", "title": "HistoryBankQA: Multilingual Temporal Question Answering on Historical Events", "authors": ["Biswadip Mandal", "Anant Khandelwal", "Manish Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Temporal reasoning about historical events is a critical skill for NLP tasks\nlike event extraction, historical entity linking, temporal question answering,\ntimeline summarization, temporal event clustering and temporal natural language\ninference. Yet efforts on benchmarking temporal reasoning capabilities of large\nlanguage models (LLMs) are rather limited. Existing temporal reasoning datasets\nare limited in scale, lack multilingual coverage and focus more on contemporary\nevents. To address these limitations, we present HistoryBank, a multilingual\ndatabase of 10M+ historical events extracted from Wikipedia timeline pages and\narticle infoboxes. Our database provides unprecedented coverage in both\nhistorical depth and linguistic breadth with 10 languages. Additionally, we\nconstruct a comprehensive question answering benchmark for temporal reasoning\nacross all languages. This benchmark covers a diverse set of 6 temporal QA\nreasoning tasks, and we evaluate a suite of popular language models\n(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their\nperformance on these tasks. As expected GPT4o performs best across all answer\ntypes and languages; Gemma-2 outperforms the other small language models. Our\nwork aims to provide a comprehensive resource for advancing multilingual and\ntemporally-aware natural language understanding of historical events. To\nfacilitate further research, we will make our code and datasets publicly\navailable upon acceptance of this paper."}
{"id": "2509.12507", "pdf": "https://arxiv.org/pdf/2509.12507.pdf", "abs": "https://arxiv.org/abs/2509.12507", "title": "Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "categories": ["cs.RO", "cs.HC", "cs.LG", "68T07, 68T40", "I.2.9; I.2.6"], "comment": "DOI: 10.3389/frobt.2023.1110534. This is the author's LaTeX version", "summary": "One of the main goals of robotics and intelligent agent research is to enable\nnatural communication with humans in physically situated settings. While recent\nwork has focused on verbal modes such as language and speech, non-verbal\ncommunication is crucial for flexible interaction. We present a framework for\ngenerating pointing gestures in embodied agents by combining imitation and\nreinforcement learning. Using a small motion capture dataset, our method learns\na motor control policy that produces physically valid, naturalistic gestures\nwith high referential accuracy. We evaluate the approach against supervised\nlearning and retrieval baselines in both objective metrics and a virtual\nreality referential game with human users. Results show that our system\nachieves higher naturalness and accuracy than state-of-the-art supervised\nmodels, highlighting the promise of imitation-RL for communicative gesture\ngeneration and its potential application to robots."}
{"id": "2509.12771", "pdf": "https://arxiv.org/pdf/2509.12771.pdf", "abs": "https://arxiv.org/abs/2509.12771", "title": "Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision", "authors": ["Omri Suissa", "Muhiim Ali", "Shengmai Chen", "Yinuo Cai", "Shekhar Pradhan"], "categories": ["cs.CL"], "comment": null, "summary": "Humans can recognize an image as an instance of a general concept, beyond\nsimply identifying its objects and their relationships. In this paper, we\ninvestigate 1. The extent to which VLMs have this concept abstraction capacity,\nand 2. Strategies for encoding the sort of higher-concept information in images\nthat would enable the resulting VLM model (CLEAR GLASS model) to have this\ncapability to a greater degree. To this end, we introduce a grouped\nimage-caption dataset (MAGIC), which consists of several groups of image\ncaptions and for each group a set of associated images and higher-level\nconceptual labels. We use a novel contrastive loss technique to induce the\nmodel to encode in the representation of each image (caption) in a group the\ninformation that is common to all members of the image-caption group. Our main\ncontribution is a grouped contrastive loss function based on text-image\ncontrastive groups (outer contrastive loss) as well as an inner loss which\nmeasures the distances between image-caption instances in the group. Our\ntraining methodology results in the CLEAR GLASS model having the concept\nabstraction capacity as an emergent capacity because the model is not exposed\nto the higher-level concepts associated with each group. Instead, the training\nforces the model to create for each image-caption group a semantic\nrepresentation that brings it closer to the semantic representation of the\nhigher-level concepts in the latent semantic space. Our experiments show that\nthis training methodology results in a model which shows improvement in\nabstract concept recognition compared to SOTA models."}
{"id": "2509.12573", "pdf": "https://arxiv.org/pdf/2509.12573.pdf", "abs": "https://arxiv.org/abs/2509.12573", "title": "No Need for \"Learning\" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction", "authors": ["Tim Bary", "Benoît Macq", "Louis Petit"], "categories": ["cs.LG", "cs.HC"], "comment": "9 pages, 4 figures, 1 table", "summary": "AI systems often fail to deliver reliable predictions across all inputs,\nprompting the need for hybrid human-AI decision-making. Existing Learning to\nDefer (L2D) approaches address this by training deferral models, but these are\nsensitive to changes in expert composition and require significant retraining\nif experts change. We propose a training-free, model- and expert-agnostic\nframework for expert deferral based on conformal prediction. Our method uses\nthe prediction set generated by a conformal predictor to identify\nlabel-specific uncertainty and selects the most discriminative expert using a\nsegregativity criterion, measuring how well an expert distinguishes between the\nremaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that\nour method consistently outperforms both the standalone model and the strongest\nexpert, with accuracies attaining $99.57\\pm0.10\\%$ and $99.40\\pm0.52\\%$, while\nreducing expert workload by up to a factor of $11$. The method remains robust\nunder degraded expert performance and shows a gradual performance drop in\nlow-information settings. These results suggest a scalable, retraining-free\nalternative to L2D for real-world human-AI collaboration."}
{"id": "2509.12811", "pdf": "https://arxiv.org/pdf/2509.12811.pdf", "abs": "https://arxiv.org/abs/2509.12811", "title": "ConvergeWriter: Data-Driven Bottom-Up Article Construction", "authors": ["Binquan Ji", "Jiaqi Wang", "Ruiting Li", "Xingchen Han", "Yiyang Qi", "Shichao Wang", "Yifei Lu", "Yuantao Han", "Feiliang Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable prowess in text\ngeneration, yet producing long-form, factual documents grounded in extensive\nexternal knowledge bases remains a significant challenge. Existing \"top-down\"\nmethods, which first generate a hypothesis or outline and then retrieve\nevidence, often suffer from a disconnect between the model's plan and the\navailable knowledge, leading to content fragmentation and factual inaccuracies.\nTo address these limitations, we propose a novel \"bottom-up,\" data-driven\nframework that inverts the conventional generation pipeline. Our approach is\npredicated on a \"Retrieval-First for Knowledge, Clustering for Structure\"\nstrategy, which first establishes the \"knowledge boundaries\" of the source\ncorpus before any generative planning occurs. Specifically, we perform\nexhaustive iterative retrieval from the knowledge base and then employ an\nunsupervised clustering algorithm to organize the retrieved documents into\ndistinct \"knowledge clusters.\" These clusters form an objective, data-driven\nfoundation that directly guides the subsequent generation of a hierarchical\noutline and the final document content. This bottom-up process ensures that the\ngenerated text is strictly constrained by and fully traceable to the source\nmaterial, proactively adapting to the finite scope of the knowledge base and\nfundamentally mitigating the risk of hallucination. Experimental results on\nboth 14B and 32B parameter models demonstrate that our method achieves\nperformance comparable to or exceeding state-of-the-art baselines, and is\nexpected to demonstrate unique advantages in knowledge-constrained scenarios\nthat demand high fidelity and structural coherence. Our work presents an\neffective paradigm for generating reliable, structured, long-form documents,\npaving the way for more robust LLM applications in high-stakes,\nknowledge-intensive domains."}
{"id": "2509.12754", "pdf": "https://arxiv.org/pdf/2509.12754.pdf", "abs": "https://arxiv.org/abs/2509.12754", "title": "Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model", "authors": ["Saki Hashimoto", "Shoichi Hasegawa", "Tomochika Ishikawa", "Akira Taniguchi", "Yoshinobu Hagiwara", "Lotfi El Hafi", "Tadahiro Taniguchi"], "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "Robots operating in domestic and office environments must understand object\nownership to correctly execute instructions such as ``Bring me my cup.''\nHowever, ownership cannot be reliably inferred from visual features alone. To\naddress this gap, we propose Active Ownership Learning (ActOwL), a framework\nthat enables robots to actively generate and ask ownership-related questions to\nusers. ActOwL employs a probabilistic generative model to select questions that\nmaximize information gain, thereby acquiring ownership knowledge efficiently to\nimprove learning efficiency. Additionally, by leveraging commonsense knowledge\nfrom Large Language Models (LLM), objects are pre-classified as either shared\nor owned, and only owned objects are targeted for questioning. Through\nexperiments in a simulated home environment and a real-world laboratory\nsetting, ActOwL achieved significantly higher ownership clustering accuracy\nwith fewer questions than baseline methods. These findings demonstrate the\neffectiveness of combining active inference with LLM-guided commonsense\nreasoning, advancing the capability of robots to acquire ownership knowledge\nfor practical and socially appropriate task execution."}
{"id": "2509.12853", "pdf": "https://arxiv.org/pdf/2509.12853.pdf", "abs": "https://arxiv.org/abs/2509.12853", "title": "Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data", "authors": ["Kurt Micallef", "Nizar Habash", "Claudia Borg"], "categories": ["cs.CL"], "comment": "EMNLP Camera-Ready", "summary": "Maltese is a unique Semitic language that has evolved under extensive\ninfluence from Romance and Germanic languages, particularly Italian and\nEnglish. Despite its Semitic roots, its orthography is based on the Latin\nscript, creating a gap between it and its closest linguistic relatives in\nArabic. In this paper, we explore whether Arabic-language resources can support\nMaltese natural language processing (NLP) through cross-lingual augmentation\ntechniques. We investigate multiple strategies for aligning Arabic textual data\nwith Maltese, including various transliteration schemes and machine translation\n(MT) approaches. As part of this, we also introduce novel transliteration\nsystems that better represent Maltese orthography. We evaluate the impact of\nthese augmentations on monolingual and mutlilingual models and demonstrate that\nArabic-based augmentation can significantly benefit Maltese NLP tasks."}
{"id": "2509.12880", "pdf": "https://arxiv.org/pdf/2509.12880.pdf", "abs": "https://arxiv.org/abs/2509.12880", "title": "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "categories": ["cs.RO", "cs.HC", "cs.LG", "68T05, 68T40", "I.2.9; I.2.6; H.5.2"], "comment": "Presented at the Context-Awareness in HRI (CONAWA) Workshop, ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2022), March 7, 2022", "summary": "Pointing is a key mode of interaction with robots, yet most prior work has\nfocused on recognition rather than generation. We present a motion capture\ndataset of human pointing gestures covering diverse styles, handedness, and\nspatial targets. Using reinforcement learning with motion imitation, we train\npolicies that reproduce human-like pointing while maximizing precision. Results\nshow our approach enables context-aware pointing behaviors in simulation,\nbalancing task performance with natural dynamics."}
{"id": "2509.12876", "pdf": "https://arxiv.org/pdf/2509.12876.pdf", "abs": "https://arxiv.org/abs/2509.12876", "title": "Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents", "authors": ["Fuyu Xing", "Zimu Wang", "Wei Wang", "Haiyang Zhang"], "categories": ["cs.CL", "cs.MM"], "comment": "Accepted at INLG 2025. Camera-ready version", "summary": "The proliferation of multimedia content necessitates the development of\neffective Multimedia Event Extraction (M2E2) systems. Though Large\nVision-Language Models (LVLMs) have shown strong cross-modal capabilities,\ntheir utility in the M2E2 task remains underexplored. In this paper, we present\nthe first systematic evaluation of representative LVLMs, including DeepSeek-VL2\nand the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,\nimage-only, and cross-media subtasks, assessed under both few-shot prompting\nand fine-tuning settings. Our key findings highlight the following valuable\ninsights: (1) Few-shot LVLMs perform notably better on visual tasks but\nstruggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA\nsubstantially enhances model performance; and (3) LVLMs exhibit strong synergy\nwhen combining modalities, achieving superior performance in cross-modal\nsettings. We further provide a detailed error analysis to reveal persistent\nchallenges in areas such as semantic precision, localization, and cross-modal\ngrounding, which remain critical obstacles for advancing M2E2 capabilities."}
{"id": "2509.13137", "pdf": "https://arxiv.org/pdf/2509.13137.pdf", "abs": "https://arxiv.org/abs/2509.13137", "title": "Agentic AI for Financial Crime Compliance", "authors": ["Henrik Axelsen", "Valdemar Licht", "Jan Damsgaard"], "categories": ["cs.AI", "cs.HC", "cs.MA", "K.4.4; K.6.5; I.2.11"], "comment": "Accepted for presentation at HICSS-59 (2026), forthcoming in\n  Proceedings", "summary": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments."}
{"id": "2509.12886", "pdf": "https://arxiv.org/pdf/2509.12886.pdf", "abs": "https://arxiv.org/abs/2509.12886", "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations", "authors": ["Yubo Zhu", "Dongrui Liu", "Zecheng Lin", "Wei Tong", "Sheng Zhong", "Jing Shao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens."}
{"id": "2509.13234", "pdf": "https://arxiv.org/pdf/2509.13234.pdf", "abs": "https://arxiv.org/abs/2509.13234", "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy", "authors": ["Nadim Barakat", "William Lotter"], "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows."}
{"id": "2509.12892", "pdf": "https://arxiv.org/pdf/2509.12892.pdf", "abs": "https://arxiv.org/abs/2509.12892", "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings", "authors": ["Shiyu Li", "Yang Tang", "Ruijie Liu", "Shi-Zhe Chen", "Xi Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Oral", "summary": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025)."}
{"id": "2409.14659", "pdf": "https://arxiv.org/pdf/2409.14659.pdf", "abs": "https://arxiv.org/abs/2409.14659", "title": "Image memorability predicts social media virality and externally-associated commenting", "authors": ["Shikang Peng", "Wilma A. Bainbridge"], "categories": ["cs.HC", "cs.CE", "cs.SI", "J.4"], "comment": "47 pages, 5 figures", "summary": "Visual content on social media plays a key role in entertainment and\ninformation sharing, yet some images gain more engagement than others. We\npropose that image memorability - the ability to be remembered - may predict\nviral potential. Using 1,247 Reddit image posts across three timepoints, we\nassessed memorability with neural network ResMem and correlated the predicted\nmemorability scores with virality metrics. Memorable images were consistently\nassociated with more comments, even after controlling for image categories with\nResNet-152. Semantic analysis revealed that memorable images relate to more\nneutral-affect comments, suggesting a distinct pathway to virality from\nemotional content. Additionally, visual consistency analysis showed that\nmemorable posts inspired diverse, externally-associated comments. By analyzing\nResMem's layers, we found semantic distinctiveness was key to both memorability\nand virality. This study highlights memorability as a unique correlate of\nsocial media virality, offering insights into how visual features and human\ncognitive behavioral interactions are associated with online engagement."}
{"id": "2509.12908", "pdf": "https://arxiv.org/pdf/2509.12908.pdf", "abs": "https://arxiv.org/abs/2509.12908", "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning", "authors": ["Caiqi Zhang", "Chang Shu", "Ehsan Shareghi", "Nigel Collier"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks."}
{"id": "2410.23540", "pdf": "https://arxiv.org/pdf/2410.23540.pdf", "abs": "https://arxiv.org/abs/2410.23540", "title": "Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending", "authors": ["Shuo Feng", "Bo Liu", "Yifan", "Shan", "Roy Zunder", "Wei-Che Lin", "Tri Dinh", "Harald Haraldsson", "Ofer Berman", "Thijs Roumen"], "categories": ["cs.HC"], "comment": null, "summary": "Wire bending is a technique used in manufacturing to mass-produce items such\nas clips, mounts, and braces. Recent advances in programmable wire bending have\nmade this process increasingly accessible for custom fabrication. However, CNC\nwire benders are controlled using Computer Aided Manufacturing (CAM) software,\nwithout design tools, making custom designs challenging to produce. We present\nY-AR, a Computer Aided Design (CAD) interface for 3D wire bending. Y-AR uses\nmixed reality to let designers create clips, mounts, and braces to physically\nconnect objects to their surrounding environment. The interface incorporates\nsprings as design primitives which (1) apply forces to hold objects, and (2)\ncounter-act dimensional inaccuracies inherently caused by mid-air modeling and\nmeasurement errors in AR. Springs are a natural design element when working\nwith metal wire-bending given its specific material properties. We demonstrate\nworkflows to design and fabricate a range of mechanisms in Y-AR as well as\nstructures made using free-hand design tools. We found that combining\ngesture-based interaction with fabrication-aware design principles allowed\nnovice users to create functional wire connectors, even when using imprecise\nXR-based input. In our usability evaluation, all 12 participants successfully\ndesigned and fabricated a functional bottle holder using Y-AR."}
{"id": "2509.12955", "pdf": "https://arxiv.org/pdf/2509.12955.pdf", "abs": "https://arxiv.org/abs/2509.12955", "title": "Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework", "authors": ["Heng Zhang", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "The automated generation of research workflows is essential for improving the\nreproducibility of research and accelerating the paradigm of \"AI for Science\".\nHowever, existing methods typically extract merely fragmented procedural\ncomponents and thus fail to capture complete research workflows. To address\nthis gap, we propose an end-to-end framework that generates comprehensive,\nstructured research workflows by mining full-text academic papers. As a case\nstudy in the Natural Language Processing (NLP) domain, our paragraph-centric\napproach first employs Positive-Unlabeled (PU) Learning with SciBERT to\nidentify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.\nSubsequently, we utilize Flan-T5 with prompt learning to generate workflow\nphrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of\n0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically\ncategorized into data preparation, data processing, and data analysis stages\nusing ChatGPT with few-shot learning, achieving a classification precision of\n0.958. By mapping categorized phrases to their document locations in the\ndocuments, we finally generate readable visual flowcharts of the entire\nresearch workflows. This approach facilitates the analysis of workflows derived\nfrom an NLP corpus and reveals key methodological shifts over the past two\ndecades, including the increasing emphasis on data analysis and the transition\nfrom feature engineering to ablation studies. Our work offers a validated\ntechnical framework for automated workflow generation, along with a novel,\nprocess-oriented perspective for the empirical investigation of evolving\nscientific paradigms. Source code and data are available at:\nhttps://github.com/ZH-heng/research_workflow."}
{"id": "2412.04629", "pdf": "https://arxiv.org/pdf/2412.04629.pdf", "abs": "https://arxiv.org/abs/2412.04629", "title": "Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates", "authors": ["Li Shi", "Houjiang Liu", "Yian Wong", "Utkarsh Mujumdar", "Dan Zhang", "Jacek Gwizdka", "Matthew Lease"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Complete reanalysis using a revised methodology; results, figures,\n  and discussion updated to reflect the new approach", "summary": "Multi-persona debate systems powered by large language models (LLMs) show\npromise in reducing confirmation bias, which can fuel echo chambers and social\npolarization. However, empirical evidence remains limited on whether they\nmeaningfully shift user attention toward belief-challenging content, promote\nbelief change, or outperform traditional debiasing strategies. To investigate\nthis, we compare an LLM-based multi-persona debate system with a two-stance\nretrieval-based system, exposing participants to multiple viewpoints on\ncontroversial topics. By collecting eye-tracking data, belief change measures,\nand qualitative feedback, our results show that while the debate system does\nnot significantly increase attention to opposing views, or make participants\nshift away from prior beliefs, it does provide a buffering effect against bias\ncaused by individual cognitive tendency. These findings shed light on both the\npromise and limits of multi-persona debate systems in information seeking, and\nwe offer design insights to guide future work toward more balanced and\nreflective information engagement."}
{"id": "2509.12960", "pdf": "https://arxiv.org/pdf/2509.12960.pdf", "abs": "https://arxiv.org/abs/2509.12960", "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models", "authors": ["Yuval Weiss", "David Demitri Africa", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": "12 Pages, 6 Tables, 8 Figures", "summary": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime."}
{"id": "2504.11795", "pdf": "https://arxiv.org/pdf/2504.11795.pdf", "abs": "https://arxiv.org/abs/2504.11795", "title": "Schemex: Interactive Structural Abstraction from Examples with Contrastive Refinement", "authors": ["Sitong Wang", "Samia Menon", "Dingzeyu Li", "Xiaojuan Ma", "Richard Zemel", "Lydia B. Chilton"], "categories": ["cs.HC"], "comment": null, "summary": "Each type of creative or communicative work is underpinned by an implicit\nstructure. People learn these structures from examples - a process known in\ncognitive science as schema induction. However, inducing schemas is\nchallenging, as structural patterns are often obscured by surface-level\nvariation. We present Schemex, an interactive visual workflow that scaffolds\nschema induction through clustering, abstraction, and contrastive refinement.\nSchemex supports users through visual representations and interactive\nexploration that connect abstract structures to concrete examples, promoting\ntransparency, adaptability, and effective human-AI collaboration. In our user\nstudy, participants reported significantly greater insight and confidence in\nthe schemas developed with Schemex compared to those created using a baseline\nof an AI reasoning model. We conclude by discussing the broader implications of\nstructural abstraction and contrastive refinement across domains."}
{"id": "2509.12961", "pdf": "https://arxiv.org/pdf/2509.12961.pdf", "abs": "https://arxiv.org/abs/2509.12961", "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews", "authors": ["Chenye Zou", "Xingyue Wen", "Tianyi Hu", "Qian Janice Wang", "Daniel Hershcovich"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content."}
{"id": "2505.19101", "pdf": "https://arxiv.org/pdf/2505.19101.pdf", "abs": "https://arxiv.org/abs/2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control."}
{"id": "2509.12994", "pdf": "https://arxiv.org/pdf/2509.12994.pdf", "abs": "https://arxiv.org/abs/2509.12994", "title": "SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data", "authors": ["Jian Gao", "Fufangchen Zhao", "Yiyang Zhang", "Danfeng Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension."}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900.pdf", "abs": "https://arxiv.org/abs/2507.22900", "title": "New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "comment": "A shorter version of the manuscript (16 pages) has been accepted for\n  publication in the Proceedings of 19th Colombian Conference on Computing, CCC\n  2025", "summary": "The arrival of AI coding assistants in educational settings presents a\nparadigm shift, introducing a \"new kid in the classroom\" for both students and\ninstructors. Thus, understanding the perceptions of these key actors about this\nnew dynamic is critical. This exploratory study contributes to this area by\ninvestigating how these tools are shaping the experiences of novice programmers\nin an introductory programming course. Through a two-part exam, we investigated\nstudent perceptions by first providing access to AI support for a programming\ntask and then requiring an extension of the solution without it. We collected\nLikert-scale and open-ended responses from 20 students to understand their\nperceptions on the challenges they faced. Our findings reveal that students\nperceived AI tools as helpful for grasping code concepts and boosting their\nconfidence during the initial development phase. However, a noticeable\ndifficulty emerged when students were asked to work unaided, pointing to\npotential overreliance and gaps in foundational knowledge transfer. These\ninsights highlight a critical need for new pedagogical approaches that\nintegrate AI effectively while effectively enhancing core programming skills,\nrather than impersonating them."}
{"id": "2509.13047", "pdf": "https://arxiv.org/pdf/2509.13047.pdf", "abs": "https://arxiv.org/abs/2509.13047", "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models", "authors": ["Nolan Platt", "Pragyansmita Nayak"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 68T50", "I.2.7; I.2.6"], "comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their appli- cation to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing over- fitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models - when fine\ntuned properly - can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expand- ing research in the growing field of specialized\nsmall language models, our approach has immediate applications in maritime\nsafety, security operations, and vessel traffic management systems in various\nindustries."}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "The first two authors hold equal contribution", "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior."}
{"id": "2509.13081", "pdf": "https://arxiv.org/pdf/2509.13081.pdf", "abs": "https://arxiv.org/abs/2509.13081", "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO", "authors": ["Francesco Pappone", "Ruggero Marino Lazzaroni", "Federico Califano", "Niccolò Gentile", "Roberto Marras"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks"}
{"id": "2505.07625", "pdf": "https://arxiv.org/pdf/2505.07625.pdf", "abs": "https://arxiv.org/abs/2505.07625", "title": "QC-Adviser: Quantum Hardware Recommendations for Solving Industrial Optimization Problems", "authors": ["Djamel Laps-Bouraba", "Markus Zajac", "Uta Störl"], "categories": ["quant-ph", "cs.HC"], "comment": "GI Quantum Computing Workshop 2025", "summary": "The availability of quantum hardware via the cloud offers opportunities for\nnew approaches to computing optimization problems in an industrial environment.\nHowever, selecting the right quantum hardware is difficult for non-experts due\nto its technical characteristics. In this paper, we present the QC-Adviser\nprototype, which supports users in selecting suitable quantum annealer hardware\nwithout requiring quantum computing knowledge."}
{"id": "2509.13127", "pdf": "https://arxiv.org/pdf/2509.13127.pdf", "abs": "https://arxiv.org/abs/2509.13127", "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning", "authors": ["Sijia Cui", "Shuai Xu", "Aiyao He", "Yanna Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to IJCNN 2025", "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP."}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts."}
{"id": "2509.13154", "pdf": "https://arxiv.org/pdf/2509.13154.pdf", "abs": "https://arxiv.org/abs/2509.13154", "title": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals", "authors": ["Jinxin Li", "Gang Tu", "ShengYu Cheng", "Junjie Hu", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs."}
{"id": "2509.06176", "pdf": "https://arxiv.org/pdf/2509.06176.pdf", "abs": "https://arxiv.org/abs/2509.06176", "title": "AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations", "authors": ["Raphaël Weuts", "Johannes Bleher", "Hannah Bleher", "Rozanne Tuesday Flores", "Guo Xuanyang", "Paweł Pujszo", "Zsolt Almási"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "68T01, 68T20, 91-08, 97U50, 97B10", "I.2.0; K.4.1; K.4.2; K.3.2"], "comment": null, "summary": "As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance."}
{"id": "2509.13196", "pdf": "https://arxiv.org/pdf/2509.13196.pdf", "abs": "https://arxiv.org/abs/2509.13196", "title": "The Few-shot Dilemma: Over-prompting Large Language Models", "authors": ["Yongjian Tang", "Doruk Tuncel", "Christian Koerner", "Thomas Runkler"], "categories": ["cs.CL"], "comment": "accepted for the main track of FLLM", "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements."}
{"id": "2509.11067", "pdf": "https://arxiv.org/pdf/2509.11067.pdf", "abs": "https://arxiv.org/abs/2509.11067", "title": "Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration", "authors": ["Liangxuan Guo", "Bin Zhu", "Qingqian Tao", "Kangning Liu", "Xun Zhao", "Xianzhe Qin", "Jin Gao", "Guangfu Hao"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Autonomous agents for desktop automation struggle with complex multi-step\ntasks due to poor coordination and inadequate quality control. We introduce\nAgentic Lybic, a novel multi-agent system where the entire architecture\noperates as a finite-state machine (FSM). This core innovation enables dynamic\norchestration. Our system comprises four components: a Controller, a Manager,\nthree Workers (Technician for code-based operations, Operator for GUI\ninteractions, and Analyst for decision support), and an Evaluator. The critical\nmechanism is the FSM-based routing between these components, which provides\nflexibility and generalization by dynamically selecting the optimal execution\nstrategy for each subtask. This principled orchestration, combined with robust\nquality gating, enables adaptive replanning and error recovery. Evaluated\nofficially on the OSWorld benchmark, Agentic Lybic achieves a state-of-the-art\n57.07% success rate in 50 steps, substantially outperforming existing methods.\nResults demonstrate that principled multi-agent orchestration with continuous\nquality control provides superior reliability for generalized desktop\nautomation in complex computing environments."}
{"id": "2509.13244", "pdf": "https://arxiv.org/pdf/2509.13244.pdf", "abs": "https://arxiv.org/abs/2509.13244", "title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning."}
{"id": "2509.13282", "pdf": "https://arxiv.org/pdf/2509.13282.pdf", "abs": "https://arxiv.org/abs/2509.13282", "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement", "authors": ["Ali Salamatian", "Amirhossein Abaskohi", "Wan-Cyuan Fan", "Mir Rayat Imtiaz Hossain", "Leonid Sigal", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "EMNLP 2025", "summary": "Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs."}
{"id": "2509.13309", "pdf": "https://arxiv.org/pdf/2509.13309.pdf", "abs": "https://arxiv.org/abs/2509.13309", "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents", "authors": ["Zile Qiao", "Guoxin Chen", "Xuanzhong Chen", "Donglei Yu", "Wenbiao Yin", "Xinyu Wang", "Zhen Zhang", "Baixuan Li", "Huifeng Yin", "Kuan Li", "Rui Min", "Minpeng Liao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems."}
{"id": "2509.13310", "pdf": "https://arxiv.org/pdf/2509.13310.pdf", "abs": "https://arxiv.org/abs/2509.13310", "title": "Scaling Agents via Continual Pre-training", "authors": ["Liangcai Su", "Zhen Zhang", "Guangyu Li", "Zhuo Chen", "Chenxi Wang", "Maojia Song", "Xinyu Wang", "Kuan Li", "Jialong Wu", "Xuanzhong Chen", "Zile Qiao", "Zhongwang Zhang", "Huifeng Yin", "Shihao Cai", "Runnan Fang", "Zhengwei Tao", "Wenbiao Yin", "Chenxiong Qian", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE."}
{"id": "2509.13311", "pdf": "https://arxiv.org/pdf/2509.13311.pdf", "abs": "https://arxiv.org/abs/2509.13311", "title": "Towards General Agentic Intelligence via Environment Scaling", "authors": ["Runnan Fang", "Shihao Cai", "Baixuan Li", "Jialong Wu", "Guangyu Li", "Wenbiao Yin", "Xinyu Wang", "Xiaobin Wang", "Liangcai Su", "Zhen Zhang", "Shibin Wu", "Zhengwei Tao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models."}
{"id": "2509.13312", "pdf": "https://arxiv.org/pdf/2509.13312.pdf", "abs": "https://arxiv.org/abs/2509.13312", "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research", "authors": ["Zijian Li", "Xin Guan", "Bo Zhang", "Shen Huang", "Houquan Zhou", "Shaopeng Lai", "Ming Yan", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jun Zhang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "An agent system for open-ended deep research", "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports."}
{"id": "2509.13313", "pdf": "https://arxiv.org/pdf/2509.13313.pdf", "abs": "https://arxiv.org/abs/2509.13313", "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization", "authors": ["Xixi Wu", "Kuan Li", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Huifeng Yin", "Zhongwang Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Minhao Cheng", "Shuai Wang", "Hong Cheng", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents."}
{"id": "2509.13316", "pdf": "https://arxiv.org/pdf/2509.13316.pdf", "abs": "https://arxiv.org/abs/2509.13316", "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?", "authors": ["Millicent Li", "Alberto Mario Ceballos Arroyo", "Giordano Rogers", "Naomi Saphra", "Byron C. Wallace"], "categories": ["cs.CL", "cs.LG"], "comment": "34 pages, 6 figures", "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs."}
{"id": "2509.12221", "pdf": "https://arxiv.org/pdf/2509.12221.pdf", "abs": "https://arxiv.org/abs/2509.12221", "title": "MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors", "authors": ["Xin Tong", "Zhi Lin", "Jingya Wang", "Meng Han", "Bo Jin"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "Under Review", "summary": "Large language models (LLMs) enforce safety alignment to reliably refuse\nmalicious requests, yet the same blanket safeguards also block legitimate uses\nin policing, defense, and other high-stakes settings. Earlier\n\"refusal-direction\" edits can bypass those layers, but they rely on a single\nvector that indiscriminately unlocks all hazardous topics, offering no semantic\ncontrol. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight\nframework that factorizes the monolithic refusal direction into topic-aligned,\nnearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is\nlearned in a single epoch with a multi-task objective that blends a\ndifferential-ablation margin, cross-topic and orthogonality penalties, and\nseveral auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV\nachieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,\nand Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best\nsingle-direction baseline. Vectors trained in Chinese transfer almost unchanged\nto English (and vice versa), suggesting a language-agnostic refusal subspace.\nThe results show that fine-grained, topic-level capability activation is\nachievable with minimal utility loss, paving the way for controlled LLMs\ndeployment in security-sensitive domains."}
{"id": "2509.12248", "pdf": "https://arxiv.org/pdf/2509.12248.pdf", "abs": "https://arxiv.org/abs/2509.12248", "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics", "authors": ["Yuriel Ryan", "Rui Yang Tan", "Kenny Tsu Wei Choo", "Roy Ka-Wei Lee"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "27 pages, 8 figures, EMNLP 2025", "summary": "Understanding humor is a core aspect of social intelligence, yet it remains a\nsignificant challenge for Large Multimodal Models (LMMs). We introduce\nPixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed\nto evaluate LMMs' ability to interpret multimodal humor and recognize narrative\nsequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for\ninstance, top models achieve only 61% accuracy in panel sequencing, far below\nhuman performance. This underscores critical limitations in current models'\nintegration of visual and textual cues for coherent narrative and humor\nunderstanding. By providing a rigorous framework for evaluating multimodal\ncontextual and narrative reasoning, PixelHumor aims to drive the development of\nLMMs that better engage in natural, socially aware interactions."}
{"id": "2509.12273", "pdf": "https://arxiv.org/pdf/2509.12273.pdf", "abs": "https://arxiv.org/abs/2509.12273", "title": "LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences", "authors": ["Liangqi Yuan", "Dong-Jun Han", "Christopher G. Brinton", "Sabine Brunswicker"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The rise of large language models (LLMs) has made natural language-driven\nroute planning an emerging research area that encompasses rich user objectives.\nCurrent research exhibits two distinct approaches: direct route planning using\nLLM-as-Agent and graph-based searching strategies. However, LLMs in the former\napproach struggle to handle extensive map data, while the latter shows limited\ncapability in understanding natural language preferences. Additionally, a more\ncritical challenge arises from the highly heterogeneous and unpredictable\nspatio-temporal distribution of users across the globe. In this paper, we\nintroduce a novel LLM-Assisted route Planning (LLMAP) system that employs an\nLLM-as-Parser to comprehend natural language, identify tasks, and extract user\npreferences and recognize task dependencies, coupled with a Multi-Step Graph\nconstruction with iterative Search (MSGS) algorithm as the underlying solver\nfor optimal route finding. Our multi-objective optimization approach adaptively\ntunes objective weights to maximize points of interest (POI) quality and task\ncompletion rate while minimizing route distance, subject to three key\nconstraints: user time limits, POI opening hours, and task dependencies. We\nconduct extensive experiments using 1,000 routing prompts sampled with varying\ncomplexity across 14 countries and 27 cities worldwide. The results demonstrate\nthat our approach achieves superior performance with guarantees across multiple\nconstraints."}
{"id": "2509.12341", "pdf": "https://arxiv.org/pdf/2509.12341.pdf", "abs": "https://arxiv.org/abs/2509.12341", "title": "Exact Coset Sampling for Quantum Lattice Algorithms", "authors": ["Yifan Zhang"], "categories": ["quant-ph", "cs.CL", "cs.CR"], "comment": "Project Page: https://github.com/yifanzhang-pro/quantum-lattice", "summary": "We give a simple, fully correct, and assumption-light replacement for the\ncontested \"domain-extension\" in Step 9 of a recent windowed-QFT lattice\nalgorithm with complex-Gaussian windows~\\citep{chen2024quantum}. The published\nStep~9 suffers from a periodicity/support mismatch. We present a pair-shift\ndifference construction that coherently cancels all unknown offsets, produces\nan exact uniform CRT-coset state over $\\mathbb{Z}_{P}$, and then uses the QFT\nto enforce the intended modular linear relation. The unitary is reversible,\nuses $\\mathrm{poly}(\\log M_2)$ gates, and preserves the algorithm's\nasymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice."}
{"id": "2509.12423", "pdf": "https://arxiv.org/pdf/2509.12423.pdf", "abs": "https://arxiv.org/abs/2509.12423", "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition", "authors": ["Danielle Cohen", "Yoni Halpern", "Noam Kahlon", "Joel Oren", "Omri Berkovitch", "Sapir Caduri", "Ido Dagan", "Anatoly Efros"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Understanding user intents from UI interaction trajectories remains a\nchallenging, yet crucial, frontier in intelligent agent development. While\nmassive, datacenter-based, multi-modal large language models (MLLMs) possess\ngreater capacity to handle the complexities of such sequences, smaller models\nwhich can run on-device to provide a privacy-preserving, low-cost, and\nlow-latency user experience, struggle with accurate intent inference. We\naddress these limitations by introducing a novel decomposed approach: first, we\nperform structured interaction summarization, capturing key information from\neach user action. Second, we perform intent extraction using a fine-tuned model\noperating on the aggregated summaries. This method improves intent\nunderstanding in resource-constrained models, even surpassing the base\nperformance of large MLLMs."}
{"id": "2509.12519", "pdf": "https://arxiv.org/pdf/2509.12519.pdf", "abs": "https://arxiv.org/abs/2509.12519", "title": "Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News", "authors": ["Ross Koval", "Nicholas Andrews", "Xifeng Yan"], "categories": ["cs.CE", "cs.CL", "q-fin.CP", "I.2.7; J.4"], "comment": "Preprint", "summary": "Financial news plays a critical role in the information diffusion process in\nfinancial markets and is a known driver of stock prices. However, the\ninformation in each news article is not necessarily self-contained, often\nrequiring a broader understanding of the historical news coverage for accurate\ninterpretation. Further, identifying and incorporating the most relevant\ncontextual information presents significant challenges. In this work, we\nexplore the value of historical context in the ability of large language models\nto understand the market impact of financial news. We find that historical\ncontext provides a consistent and significant improvement in performance across\nmethods and time horizons. To this end, we propose an efficient and effective\ncontextualization method that uses a large LM to process the main article,\nwhile a small LM encodes the historical context into concise summary embeddings\nthat are then aligned with the large model's representation space. We explore\nthe behavior of the model through multiple qualitative and quantitative\ninterpretability tests and reveal insights into the value of contextualization.\nFinally, we demonstrate that the value of historical context in model\npredictions has real-world applications, translating to substantial\nimprovements in simulated investment performance."}
{"id": "2509.12525", "pdf": "https://arxiv.org/pdf/2509.12525.pdf", "abs": "https://arxiv.org/abs/2509.12525", "title": "The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots", "authors": ["T. James Brandt", "Cecilia Xi Wang"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 17 figures, 2 tables. Submitted to CHI 2026 (under review).\n  Preregistered: https://osf.io/f4h5b ; Code/Materials:\n  https://doi.org/10.5281/zenodo.15801081", "summary": "Generative AI powers a growing wave of companion chatbots, yet principles for\nfostering genuine connection remain unsettled. We test two routes: visible user\nauthorship versus covert language-style mimicry. In a preregistered 3x2\nexperiment (N = 162), we manipulated user-controlled avatar generation (none,\npremade, user-generated) and Language Style Matching (LSM) (static vs.\nadaptive). Generating an avatar boosted rapport ($\\omega^2$ = .040, p = .013),\nwhereas adaptive LSM underperformed static style on personalization and\nsatisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t\n= 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony\nerodes connection when perceived as incoherent, destabilizing persona. To\nexplain, we propose a stability-and-legibility account: visible authorship\nfosters natural interaction, while covert mimicry risks incoherence. Our\nfindings suggest designers should prioritize legible, user-driven\npersonalization and limit stylistic shifts rather than rely on opaque mimicry."}
{"id": "2509.12539", "pdf": "https://arxiv.org/pdf/2509.12539.pdf", "abs": "https://arxiv.org/abs/2509.12539", "title": "LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations", "authors": ["Robin Vujanic", "Thomas Rueckstiess"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "We present LEAF (\"Lightweight Embedding Alignment Framework\"), a knowledge\ndistillation framework for text embedding models. A key distinguishing feature\nis that our distilled leaf models are aligned to their teacher. In the context\nof information retrieval, this allows for flexible asymmetric architectures\nwhere documents are encoded with the larger teacher model, while queries can be\nserved with the smaller leaf models. We also show that leaf models\nautomatically inherit MRL and robustness to output quantization whenever these\nproperties are present in the teacher model, without explicitly training for\nthem. To demonstrate the capability of our framework we publish leaf-ir, a 23M\nparameters information retrieval oriented text embedding model trained using\nLEAF, which sets a new state-of-the-art (SOTA) on BEIR, ranking #1 on the\npublic leaderboard for this benchmark and for models of its size. When run in\nasymmetric mode, its retrieval performance is further increased. Our scheme is\nhowever not restricted to the information retrieval setting, and we demonstrate\nits wider applicability by synthesizing the multi-task leaf-mt model. This also\nsets a new SOTA, ranking #1 on the public MTEB v2 (English) leaderboard for its\nsize. LEAF is applicable to black-box models and in contrast to other embedding\nmodel training frameworks, it does not require judgments nor hard negatives,\nand training can be conducted using small batch sizes. Thus, dataset and\ntraining infrastructure requirements for our framework are modest. We make our\nmodels publicly available under a permissive Apache 2.0 license."}
{"id": "2509.12574", "pdf": "https://arxiv.org/pdf/2509.12574.pdf", "abs": "https://arxiv.org/abs/2509.12574", "title": "Yet Another Watermark for Large Language Models", "authors": ["Siyuan Bao", "Ying Shi", "Zhiguang Yang", "Hanzhou Wu", "Xinpeng Zhang"], "categories": ["cs.CR", "cs.CL"], "comment": "https://scholar.google.com/citations?hl=en&user=IdiF7M0AAAAJ", "summary": "Existing watermarking methods for large language models (LLMs) mainly embed\nwatermark by adjusting the token sampling prediction or post-processing,\nlacking intrinsic coupling with LLMs, which may significantly reduce the\nsemantic quality of the generated marked texts. Traditional watermarking\nmethods based on training or fine-tuning may be extendable to LLMs. However,\nmost of them are limited to the white-box scenario, or very time-consuming due\nto the massive parameters of LLMs. In this paper, we present a new watermarking\nframework for LLMs, where the watermark is embedded into the LLM by\nmanipulating the internal parameters of the LLM, and can be extracted from the\ngenerated text without accessing the LLM. Comparing with related methods, the\nproposed method entangles the watermark with the intrinsic parameters of the\nLLM, which better balances the robustness and imperceptibility of the\nwatermark. Moreover, the proposed method enables us to extract the watermark\nunder the black-box scenario, which is computationally efficient for use.\nExperimental results have also verified the feasibility, superiority and\npracticality. This work provides a new perspective different from mainstream\nworks, which may shed light on future research."}
{"id": "2509.12592", "pdf": "https://arxiv.org/pdf/2509.12592.pdf", "abs": "https://arxiv.org/abs/2509.12592", "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis", "authors": ["Aaron Baughman", "Gozde Akay", "Eduardo Morales", "Rahul Agarwal", "Preetika Srivastava"], "categories": ["cs.AI", "cs.CL"], "comment": "12 pages, 5 Figures, 4 Tables", "summary": "We present Match Chat, a real-time, agent-driven assistant designed to\nenhance the tennis fan experience by delivering instant, accurate responses to\nmatch-related queries. Match Chat integrates Generative Artificial Intelligence\n(GenAI) with Generative Computing (GenComp) techniques to synthesize key\ninsights during live tennis singles matches. The system debuted at the 2025\nWimbledon Championships and the 2025 US Open, where it provided about 1 million\nusers with seamless access to streaming and static data through natural\nlanguage queries. The architecture is grounded in an Agent-Oriented\nArchitecture (AOA) combining rule engines, predictive models, and agents to\npre-process and optimize user queries before passing them to GenAI components.\nThe Match Chat system had an answer accuracy of 92.83% with an average response\ntime of 6.25 seconds under loads of up to 120 requests per second (RPS). Over\n96.08% of all queries were guided using interactive prompt design, contributing\nto a user experience that prioritized clarity, responsiveness, and minimal\neffort. The system was designed to mask architectural complexity, offering a\nfrictionless and intuitive interface that required no onboarding or technical\nfamiliarity. Across both Grand Slam deployments, Match Chat maintained 100%\nuptime and supported nearly 1 million unique users, underscoring the\nscalability and reliability of the platform. This work introduces key design\npatterns for real-time, consumer-facing AI systems that emphasize speed,\nprecision, and usability that highlights a practical path for deploying\nperformant agentic systems in dynamic environments."}
{"id": "2509.12594", "pdf": "https://arxiv.org/pdf/2509.12594.pdf", "abs": "https://arxiv.org/abs/2509.12594", "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning", "authors": ["Titong Jiang", "Xuefeng Jiang", "Yuan Ma", "Xin Wen", "Bailin Li", "Kun Zhan", "Peng Jia", "Yahui Liu", "Sheng Sun", "Xianpeng Lang"], "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Under review. Project site:\n  https://liauto-research.github.io/LightVLA", "summary": "We present LightVLA, a simple yet effective differentiable token pruning\nframework for vision-language-action (VLA) models. While VLA models have shown\nimpressive capability in executing real-world robotic tasks, their deployment\non resource-constrained platforms is often bottlenecked by the heavy\nattention-based computation over large sets of visual tokens. LightVLA\naddresses this challenge through adaptive, performance-driven pruning of visual\ntokens: It generates dynamic queries to evaluate visual token importance, and\nadopts Gumbel softmax to enable differentiable token selection. Through\nfine-tuning, LightVLA learns to preserve the most informative visual tokens\nwhile pruning tokens which do not contribute to task execution, thereby\nimproving efficiency and performance simultaneously. Notably, LightVLA requires\nno heuristic magic numbers and introduces no additional trainable parameters,\nmaking it compatible with modern inference frameworks. Experimental results\ndemonstrate that LightVLA outperforms different VLA models and existing token\npruning methods across diverse tasks on the LIBERO benchmark, achieving higher\nsuccess rates with substantially reduced computational overhead. Specifically,\nLightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%\nimprovement in task success rate. Meanwhile, we also investigate the learnable\nquery-based token pruning method LightVLA* with additional trainable\nparameters, which also achieves satisfactory performance. Our work reveals that\nas VLA pursues optimal performance, LightVLA spontaneously learns to prune\ntokens from a performance-driven perspective. To the best of our knowledge,\nLightVLA is the first work to apply adaptive visual token pruning to VLA tasks\nwith the collateral goals of efficiency and performance, marking a significant\nstep toward more efficient, powerful and practical real-time robotic systems."}
{"id": "2509.12602", "pdf": "https://arxiv.org/pdf/2509.12602.pdf", "abs": "https://arxiv.org/abs/2509.12602", "title": "DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models", "authors": ["Minyu Chen", "Guoqiang Li"], "categories": ["cs.AI", "cs.CL"], "comment": "11 pages", "summary": "The performance of Conflict-Driven Clause Learning solvers hinges on internal\nheuristics, yet the heterogeneity of SAT problems makes a single, universally\noptimal configuration unattainable. While prior automated methods can find\nspecialized configurations for specific problem families, this dataset-specific\napproach lacks generalizability and requires costly re-optimization for new\nproblem types. We introduce DaSAThco, a framework that addresses this challenge\nby learning a generalizable mapping from instance features to tailored\nheuristic ensembles, enabling a train-once, adapt-broadly model. Our framework\nuses a Large Language Model, guided by systematically defined Problem\nArchetypes, to generate a diverse portfolio of specialized heuristic ensembles\nand subsequently learns an adaptive selection mechanism to form the final\nmapping. Experiments show that DaSAThco achieves superior performance and, most\nnotably, demonstrates robust out-of-domain generalization where non-adaptive\nmethods show limitations. Our work establishes a more scalable and practical\npath toward automated algorithm design for complex, configurable systems."}
{"id": "2509.12732", "pdf": "https://arxiv.org/pdf/2509.12732.pdf", "abs": "https://arxiv.org/abs/2509.12732", "title": "A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression", "authors": ["Rishab Parthasarathy", "Achintya Bhowmik"], "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "12 pages, 11 figures, work originally done in 2022/2023 and was\n  awarded as one of the Regeneron Science Talent Search Finalists in 2022", "summary": "Despite significant medical advancements, cancer remains the second leading\ncause of death, with over 600,000 deaths per year in the US. One emerging\nfield, pathway analysis, is promising but still relies on manually derived wet\nlab data, which is time-consuming to acquire. This work proposes an efficient,\neffective end-to-end framework for Artificial Intelligence (AI) based pathway\nanalysis that predicts both cancer severity and mutation progression, thus\nrecommending possible treatments. The proposed technique involves a novel\ncombination of time-series machine learning models and pathway analysis. First,\nmutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.\nThen, a novel preprocessing algorithm was used to filter key mutations by\nmutation frequency. This data was fed into a Recurrent Neural Network (RNN)\nthat predicted cancer severity. Then, the model probabilistically used the RNN\npredictions, information from the preprocessing algorithm, and multiple\ndrug-target databases to predict future mutations and recommend possible\ntreatments. This framework achieved robust results and Receiver Operating\nCharacteristic (ROC) curves (a key statistical metric) with accuracies greater\nthan 60%, similar to existing cancer diagnostics. In addition, preprocessing\nplayed an instrumental role in isolating important mutations, demonstrating\nthat each cancer stage studied may contain on the order of a few-hundred key\ndriver mutations, consistent with current research. Heatmaps based on predicted\ngene frequency were also generated, highlighting key mutations in each cancer.\nOverall, this work is the first to propose an efficient, cost-effective\nend-to-end framework for projecting cancer progression and providing possible\ntreatments without relying on expensive, time-consuming wet lab work."}
{"id": "2509.12743", "pdf": "https://arxiv.org/pdf/2509.12743.pdf", "abs": "https://arxiv.org/abs/2509.12743", "title": "Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs", "authors": ["Hanqing Li", "Kiran Sheena Jyothi", "Henry Liang", "Sharika Mahadevan", "Diego Klabjan"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We propose a new, training-free method, Graph Reasoning via Retrieval\nAugmented Framework (GRRAF), that harnesses retrieval-augmented generation\n(RAG) alongside the code-generation capabilities of large language models\n(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target\ngraph is stored in a graph database, and the LLM is prompted to generate\nexecutable code queries that retrieve the necessary information. This approach\ncircumvents the limitations of existing methods that require extensive\nfinetuning or depend on predefined algorithms, and it incorporates an error\nfeedback loop with a time-out mechanism to ensure both correctness and\nefficiency. Experimental evaluations on the GraphInstruct dataset reveal that\nGRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle\ndetection, bipartite graph checks, shortest path computation, and maximum flow,\nwhile maintaining consistent token costs regardless of graph sizes. Imperfect\nbut still very high performance is observed on subgraph matching. Notably,\nGRRAF scales effectively to large graphs with up to 10,000 nodes."}
{"id": "2509.12760", "pdf": "https://arxiv.org/pdf/2509.12760.pdf", "abs": "https://arxiv.org/abs/2509.12760", "title": "Similarity-Distance-Magnitude Activations", "authors": ["Allen Schmaltz"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text\n  overlap with arXiv:2502.20167", "summary": "We introduce a more robust and interpretable formulation of the standard\nsoftmax activation function commonly used with neural networks by adding\nSimilarity (i.e., correctly predicted depth-matches into training) awareness\nand Distance-to-training-distribution awareness to the existing output\nMagnitude (i.e., decision-boundary) awareness. When used as the final-layer\nactivation with language models, the resulting Similarity-Distance-Magnitude\n(SDM) activation function is more robust than the softmax function to\nco-variate shifts and out-of-distribution inputs in high-probability regions,\nand provides interpretability-by-exemplar via dense matching. Complementing the\nprediction-conditional estimates, the SDM activation enables a partitioning of\nthe class-wise empirical CDFs to guard against low class-wise recall among\nselective classifications. These properties make it preferable for selective\nclassification, even when considering post-hoc calibration methods over the\nsoftmax."}
{"id": "2509.12765", "pdf": "https://arxiv.org/pdf/2509.12765.pdf", "abs": "https://arxiv.org/abs/2509.12765", "title": "InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering", "authors": ["Zihan Wang", "Zihan Liang", "Zhou Shao", "Yufei Ma", "Huangyu Dai", "Ben Chen", "Lingtao Mao", "Chenyi Lei", "Yuqing Ding", "Han Li"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "EMNLP'25 Oral Presentation. Contact: benchen4395@gmail.com", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\naddress key limitations of Large Language Models (LLMs), such as hallucination,\noutdated knowledge, and lacking reference. However, current RAG frameworks\noften struggle with identifying whether retrieved documents meaningfully\ncontribute to answer generation. This shortcoming makes it difficult to filter\nout irrelevant or even misleading content, which notably impacts the final\nperformance. In this paper, we propose Document Information Gain (DIG), a novel\nmetric designed to quantify the contribution of retrieved documents to correct\nanswer generation. DIG measures a document's value by computing the difference\nof LLM's generation confidence with and without the document augmented.\nFurther, we introduce InfoGain-RAG, a framework that leverages DIG scores to\ntrain a specialized reranker, which prioritizes each retrieved document from\nexact distinguishing and accurate sorting perspectives. This approach can\neffectively filter out irrelevant documents and select the most valuable ones\nfor better answer generation. Extensive experiments across various models and\nbenchmarks demonstrate that InfoGain-RAG can significantly outperform existing\napproaches, on both single and multiple retrievers paradigm. Specifically on\nNaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match\naccuracy against naive RAG, self-reflective RAG and modern ranking-based RAG\nrespectively, and even an average of 15.3% increment on advanced proprietary\nmodel GPT-4o across all datasets. These results demonstrate the feasibility of\nInfoGain-RAG as it can offer a reliable solution for RAG in multiple\napplications."}
{"id": "2509.12936", "pdf": "https://arxiv.org/pdf/2509.12936.pdf", "abs": "https://arxiv.org/abs/2509.12936", "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety", "authors": ["Denis Janiak", "Julia Moska", "Dawid Motyka", "Karolina Seweryn", "Paweł Walkowiak", "Bartosz Żuk", "Arkadiusz Janz"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs."}
{"id": "2509.12937", "pdf": "https://arxiv.org/pdf/2509.12937.pdf", "abs": "https://arxiv.org/abs/2509.12937", "title": "Jailbreaking Large Language Models Through Content Concretization", "authors": ["Johan Wahréus", "Ahmed Hussain", "Panos Papadimitratos"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted for presentation in the Conference on Game Theory and AI for\n  Security (GameSec) 2025", "summary": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks."}
{"id": "2509.13079", "pdf": "https://arxiv.org/pdf/2509.13079.pdf", "abs": "https://arxiv.org/abs/2509.13079", "title": "When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning", "authors": ["Mengyi Deng", "Xin Li", "Tingyu Zhu", "Zhicheng Yang", "Zhijiang Guo", "Wei Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Existing work has shown that o1-level performance can be achieved with\nlimited data distillation, but most existing methods focus on unidirectional\nsupervised fine-tuning (SFT), overlooking the intricate interplay between\ndiverse reasoning patterns. In this paper, we construct r1k, a high-quality\nreverse reasoning dataset derived by inverting 1,000 forward examples from s1k,\nand examine how SFT and Direct Preference Optimization (DPO) affect alignment\nunder bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%\naccuracy improvement over s1k across evaluated benchmarks. However, naively\nmixing forward and reverse data during SFT weakens the directional distinction.\nAlthough DPO can partially recover this distinction, it also suppresses less\npreferred reasoning paths by shifting the probability mass toward irrelevant\noutputs. These findings suggest that mixed reasoning data introduce conflicting\nsupervision signals, underscoring the need for robust and direction-aware\nalignment strategies."}
{"id": "2509.13191", "pdf": "https://arxiv.org/pdf/2509.13191.pdf", "abs": "https://arxiv.org/abs/2509.13191", "title": "Textarium: Entangling Annotation, Abstraction and Argument", "authors": ["Philipp Proff", "Marian Dörk"], "categories": ["cs.HC", "cs.CL", "H.5.2; H.5.4; I.7.1; J.5"], "comment": "This is the authors' version of the article presented at VIS4DH and\n  published in the proceedings of IEEE VIS 2025", "summary": "We present a web-based environment that connects annotation, abstraction, and\nargumentation during the interpretation of text. As a visual interface for\nscholarly reading and writing, Textarium combines human analysis with\nlightweight computational processing to bridge close and distant reading\npractices. Readers can highlight text, group keywords into concepts, and embed\nthese observations as anchors in essays. The interface renders these\ninterpretive actions as parameterized visualization states. Through a\nspeculative design process of co-creative and iterative prototyping, we\ndeveloped a reading-writing approach that makes interpretive processes\ntransparent and shareable within digital narratives."}
{"id": "2509.13197", "pdf": "https://arxiv.org/pdf/2509.13197.pdf", "abs": "https://arxiv.org/abs/2509.13197", "title": "Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter", "authors": ["Theodora Moldovan", "Arianna Pera", "Davide Vega", "Luca Maria Aiello"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": "11 pages, 5 figures", "summary": "We study how participation in collective action is articulated in podcast\ndiscussions, using the Black Lives Matter (BLM) movement as a case study. While\nresearch on collective action discourse has primarily focused on text-based\ncontent, this study takes a first step toward analyzing audio formats by using\npodcast transcripts. Using the Structured Podcast Research Corpus (SPoRC), we\ninvestigated spoken language expressions of participation in collective action,\ncategorized as problem-solution, call-to-action, intention, and execution. We\nidentified podcast episodes discussing racial justice after important\nBLM-related events in May and June of 2020, and extracted participatory\nstatements using a layered framework adapted from prior work on social media.\nWe examined the emotional dimensions of these statements, detecting eight key\nemotions and their association with varying stages of activism. We found that\nemotional profiles vary by stage, with different positive emotions standing out\nduring calls-to-action, intention, and execution. We detected negative\nassociations between collective action and negative emotions, contrary to\ntheoretical expectations. Our work contributes to a better understanding of how\nactivism is expressed in spoken digital discourse and how emotional framing may\ndepend on the format of the discussion."}
{"id": "2509.13279", "pdf": "https://arxiv.org/pdf/2509.13279.pdf", "abs": "https://arxiv.org/abs/2509.13279", "title": "HARMONIC: A Content-Centric Cognitive Robotic Architecture", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt", "Carlos Gonzalez", "Mingyo Seo", "Luis Sentis"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces HARMONIC, a cognitive-robotic architecture designed for\nrobots in human-robotic teams. HARMONIC supports semantic perception\ninterpretation, human-like decision-making, and intentional language\ncommunication. It addresses the issues of safety and quality of results; aims\nto solve problems of data scarcity, explainability, and safety; and promotes\ntransparency and trust. Two proof-of-concept HARMONIC-based robotic systems are\ndemonstrated, each implemented in both a high-fidelity simulation environment\nand on physical robotic platforms."}
{"id": "2509.13281", "pdf": "https://arxiv.org/pdf/2509.13281.pdf", "abs": "https://arxiv.org/abs/2509.13281", "title": "RepIt: Representing Isolated Targets to Steer Language Models", "authors": ["Vincent Siu", "Nathan W. Henry", "Nicholas Crispino", "Yang Liu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior."}
{"id": "2509.13305", "pdf": "https://arxiv.org/pdf/2509.13305.pdf", "abs": "https://arxiv.org/abs/2509.13305", "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Dingchu Zhang", "Xixi Wu", "Jialong Wu", "Xinyu Wang", "Zile Qiao", "Zhen Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.LG", "cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."}
{"id": "2307.02103", "pdf": "https://arxiv.org/pdf/2307.02103.pdf", "abs": "https://arxiv.org/abs/2307.02103", "title": "Do predictability factors towards signing avatars hold across cultures?", "authors": ["Abdelhadi Soudi", "Manal El Hakkaoui", "Kristof Van Laerhoven"], "categories": ["cs.CL"], "comment": "updated version", "summary": "Avatar technology can offer accessibility possibilities and improve the\nDeaf-and-Hard of Hearing sign language users access to communication, education\nand services, such as the healthcare system. However, sign language users\nacceptance of signing avatars as well as their attitudes towards them vary and\ndepend on many factors. Furthermore, research on avatar technology is mostly\ndone by researchers who are not Deaf. The study examines the extent to which\nintrinsic or extrinsic factors contribute to predict the attitude towards\navatars across cultures. Intrinsic factors include the characteristics of the\navatar, such as appearance, movements and facial expressions. Extrinsic factors\ninclude users technology experience, their hearing status, age and their sign\nlanguage fluency. This work attempts to answer questions such as, if lower\nattitude ratings are related to poor technology experience with ASL users, for\nexample, is that also true for Moroccan Sign Language (MSL) users? For the\npurposes of the study, we designed a questionnaire to understand MSL users\nattitude towards avatars. Three groups of participants were surveyed: Deaf\n(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then\ncompared with those reported in other relevant studies."}
{"id": "2404.01129", "pdf": "https://arxiv.org/pdf/2404.01129.pdf", "abs": "https://arxiv.org/abs/2404.01129", "title": "Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation", "authors": ["Bohao Yang", "Kun Zhao", "Dong Liu", "Chen Tang", "Liang Zhan", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available."}
{"id": "2405.20404", "pdf": "https://arxiv.org/pdf/2405.20404.pdf", "abs": "https://arxiv.org/abs/2405.20404", "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution", "authors": ["Yurui Chang", "Bochuan Cao", "Yujia Wang", "Jinghui Chen", "Lu Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."}
{"id": "2406.15444", "pdf": "https://arxiv.org/pdf/2406.15444.pdf", "abs": "https://arxiv.org/abs/2406.15444", "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word Problems", "authors": ["Ujjwala Anantheswaran", "Himanshu Gupta", "Kevin Scaria", "Shreyas Verma", "Chitta Baral", "Swaroop Mishra"], "categories": ["cs.CL"], "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%."}
{"id": "2409.13745", "pdf": "https://arxiv.org/pdf/2409.13745.pdf", "abs": "https://arxiv.org/abs/2409.13745", "title": "Context-Aware Membership Inference Attacks against Pre-trained Large Language Models", "authors": ["Hongyan Chang", "Ali Shahin Shamsabadi", "Kleomenis Katevas", "Hamed Haddadi", "Reza Shokri"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs."}
{"id": "2410.08388", "pdf": "https://arxiv.org/pdf/2410.08388.pdf", "abs": "https://arxiv.org/abs/2410.08388", "title": "Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes", "authors": ["Maximus Powers", "Shaina Raza", "Alex Chang", "Rehana Riaz", "Umang Mavani", "Harshitha Reddy Jonala", "Ansh Tiwari", "Hua Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Representational harms in language technologies often occur in short spans\nwithin otherwise neutral text, where phrases may simultaneously convey\ngeneralizations, unfairness, or stereotypes. Framing bias detection as\nsentence-level classification obscures which words carry bias and what type is\npresent, limiting both auditability and targeted mitigation. We introduce the\nGUS-Net Framework, comprising the GUS dataset and a multi-label token-level\ndetector for span-level analysis of social bias. The GUS dataset contains 3,739\nunique snippets across multiple domains, with over 69,000 token-level\nannotations. Each token is labeled using BIO tags (Begin, Inside, Outside) for\nthree pathways of representational harm: Generalizations, Unfairness, and\nStereotypes. To ensure reliable data annotation, we employ an automated\nmulti-agent pipeline that proposes candidate spans which are subsequently\nverified and corrected by human experts. We formulate bias detection as\nmulti-label token-level classification and benchmark both encoder-based models\n(e.g., BERT family variants) and decoder-based large language models (LLMs).\nOur evaluations cover token-level identification and span-level entity\nrecognition on our test set, and out-of-distribution generalization. Empirical\nresults show that encoder-based models consistently outperform decoder-based\nbaselines on nuanced and overlapping spans while being more computationally\nefficient. The framework delivers interpretable, fine-grained diagnostics that\nenable systematic auditing and mitigation of representational harms in\nreal-world NLP systems."}
{"id": "2410.18436", "pdf": "https://arxiv.org/pdf/2410.18436.pdf", "abs": "https://arxiv.org/abs/2410.18436", "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching", "authors": ["Seoyeon Kim", "Huiseo Kim", "Chanjun Park", "Jinyoung Yeo", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."}
{"id": "2502.08045", "pdf": "https://arxiv.org/pdf/2502.08045.pdf", "abs": "https://arxiv.org/abs/2502.08045", "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs", "authors": ["Mohsinul Kabir", "Ajwad Abrar", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs."}
{"id": "2502.12067", "pdf": "https://arxiv.org/pdf/2502.12067.pdf", "abs": "https://arxiv.org/abs/2502.12067", "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "authors": ["Heming Xia", "Chak Tou Leong", "Wenjie Wang", "Yongqi Li", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (Long Paper), camera-ready version", "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip."}
{"id": "2502.12769", "pdf": "https://arxiv.org/pdf/2502.12769.pdf", "abs": "https://arxiv.org/abs/2502.12769", "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild", "authors": ["Saad Obaid ul Islam", "Anne Lauscher", "Goran Glavaš"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models."}
{"id": "2502.13061", "pdf": "https://arxiv.org/pdf/2502.13061.pdf", "abs": "https://arxiv.org/abs/2502.13061", "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection", "authors": ["Jingbiao Mei", "Jinghong Chen", "Guangyu Yang", "Weizhe Lin", "Bill Byrne"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "EMNLP 2025 Main (Oral)", "summary": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While Large Multimodal Models\n(LMMs) have shown promise in hateful meme detection, they face notable\nchallenges like sub-optimal performance and limited out-of-domain\ngeneralization capabilities. Recent studies further reveal the limitations of\nboth supervised fine-tuning (SFT) and in-context learning when applied to LMMs\nin this setting. To address these issues, we propose a robust adaptation\nframework for hateful meme detection that enhances in-domain accuracy and\ncross-domain generalization while preserving the general vision-language\ncapabilities of LMMs. Analysis reveals that our approach achieves improved\nrobustness under adversarial attacks compared to SFT models. Experiments on six\nmeme classification datasets show that our approach achieves state-of-the-art\nperformance, outperforming larger agentic systems. Moreover, our method\ngenerates higher-quality rationales for explaining hateful content compared to\nstandard SFT, enhancing model interpretability. Code available at\nhttps://github.com/JingbiaoMei/RGCL"}
{"id": "2503.02783", "pdf": "https://arxiv.org/pdf/2503.02783.pdf", "abs": "https://arxiv.org/abs/2503.02783", "title": "Teaching Your Models to Understand Code via Focal Preference Alignment", "authors": ["Jie Wu", "Haoling Li", "Xin Zhang", "Jianwen Luo", "Yangyu Huang", "Ruihang Chu", "Yujiu Yang", "Scarlett Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP'25", "summary": "Preference learning extends the performance of Code LLMs beyond traditional\nsupervised fine-tuning by leveraging relative quality comparisons. In existing\napproaches, a set of n candidate solutions is evaluated based on test case\nsuccess rates, with the candidate demonstrating a higher pass rate being\nlabeled as positive and its counterpart with a lower pass rate as negative.\nHowever, because this approach aligns entire failing code blocks rather than\npinpointing specific errors, it lacks the granularity necessary to capture\nmeaningful error-correction relationships. As a result, the model is unable to\nlearn more informative error-correction patterns. To address these issues, we\npropose Target-DPO, a new preference alignment framework that mimics human\niterative debugging to refine Code LLMs. Target-DPO explicitly locates error\nregions and aligns the corresponding tokens via a tailored DPO algorithm. To\nfacilitate it, we introduce the CodeFlow dataset, where samples are iteratively\nrefined until passing tests, with modifications capturing error corrections.\nExtensive experiments show that a diverse suite of Code LLMs equipped with\nTarget-DPO achieves significant performance gains in code generation and\nimproves on challenging tasks like BigCodeBench. In-depth analysis reveals that\nTarget-DPO yields fewer errors. Code, model and datasets are in:\nhttps://github.com/JieWu02/Target-DPO."}
{"id": "2503.05179", "pdf": "https://arxiv.org/pdf/2503.05179.pdf", "abs": "https://arxiv.org/abs/2503.05179", "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching", "authors": ["Simon A. Aytes", "Jinheon Baek", "Sung Ju Hwang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025", "summary": "Recent advances in large language models (LLMs) have enabled strong reasoning\ncapabilities through Chain-of-Thought (CoT) prompting, which elicits\nstep-by-step problem solving, but often at the cost of excessive verbosity in\nintermediate outputs, leading to increased computational overhead. We propose\nSketch-of-Thought (SoT), a prompting framework that integrates cognitively\ninspired reasoning paradigms with linguistic constraints to reduce token usage\nwhile preserving reasoning accuracy. SoT is designed as a flexible, modular\napproach and is instantiated with three paradigms--Conceptual Chaining, Chunked\nSymbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and\nselected dynamically at test-time by a lightweight routing model. Across 18\nreasoning datasets spanning multiple domains, languages, and modalities, SoT\nachieves token reductions of up to 84% with minimal accuracy loss. In tasks\nsuch as mathematical and multi-hop reasoning, it even improves accuracy while\nshortening outputs."}
{"id": "2503.13021", "pdf": "https://arxiv.org/pdf/2503.13021.pdf", "abs": "https://arxiv.org/abs/2503.13021", "title": "Dynamic Relation Inference via Verb Embeddings", "authors": ["Omri Suissa", "Muhiim Ali", "Ariana Azarbal", "Hui Shen", "Shekhar Pradhan"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data."}
{"id": "2503.22388", "pdf": "https://arxiv.org/pdf/2503.22388.pdf", "abs": "https://arxiv.org/abs/2503.22388", "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Main, Oral", "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future. DSDBench is publicly available at\ngithub.com/KevinCL16/DSDBench."}
{"id": "2504.01132", "pdf": "https://arxiv.org/pdf/2504.01132.pdf", "abs": "https://arxiv.org/abs/2504.01132", "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding", "authors": ["Melanie Subbiah", "Akankshya Mishra", "Grace Kim", "Liyan Tang", "Greg Durrett", "Kathleen McKeown"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced."}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262.pdf", "abs": "https://arxiv.org/abs/2504.05262", "title": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP'25 Main", "summary": "Large language models (LLMs) achieve impressive results on advanced\nmathematics benchmarks but sometimes fail on basic arithmetic tasks, raising\nthe question of whether they have truly grasped fundamental arithmetic rules or\nare merely relying on pattern matching. To unravel this issue, we\nsystematically probe LLMs' understanding of two-integer addition (0 to $2^64$)\nby testing three crucial properties: commutativity (A+B=B+A), representation\ninvariance via symbolic remapping (e.g., $7 -> Y$), and consistent accuracy\nscaling with operand length. Our evaluation of 12 leading LLMs reveals a stark\ndisconnect: while models achieve high numeric accuracy (73.8-99.8%), they\nsystematically fail these diagnostics. Specifically, accuracy plummets to <=\n7.5% with symbolic inputs, commutativity is violated in up to 20% of cases, and\naccuracy scaling is non-monotonic. These findings demonstrate that current LLMs\naddress elementary addition via pattern matching, not robust rule induction,\nmotivating new diagnostic benchmarks and innovations in model architecture and\ntraining to cultivate genuine mathematical reasoning. Our dataset and\ngenerating code are available at\nhttps://github.com/kuri-leo/llm-arithmetic-diagnostic."}
{"id": "2505.13886", "pdf": "https://arxiv.org/pdf/2505.13886.pdf", "abs": "https://arxiv.org/abs/2505.13886", "title": "Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "63 pages, 23 figures, submitted to NeurIPS 2025", "summary": "Real-world vision language reasoning scenarios often include diverse and\ncomplex tasks. However, vision language reinforcement learning has primarily\nfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting\nthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,\nwe propose a novel Code2Logic approach, using Large Language Models (LLMs) to\nsynthesize verifiable game reasoning tasks at scale via adapting game code.\nUsing the Code2Logic, we developed the GameQA dataset to train and evaluate\nVLMs. GameQA is verifiable and scalable, offers controllable difficulty\ngradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,\nwhich is simple reinforcement learning on GameQA. Surprisingly, despite\ntraining solely on game tasks, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse\nvision-language benchmarks. Our code, dataset and models are available at the\nGitHub repository."}
{"id": "2505.14172", "pdf": "https://arxiv.org/pdf/2505.14172.pdf", "abs": "https://arxiv.org/abs/2505.14172", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Main as Oral Presentation (Top 15% of accepted\n  papers)", "summary": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge suddenly and only late in training. We find that\npercolation-based models of concept emergence explain these patterns,\nsuggesting that learning character composition is not fundamentally different\nfrom learning commonsense knowledge. To address this bottleneck, we propose a\nlightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available."}
{"id": "2505.15805", "pdf": "https://arxiv.org/pdf/2505.15805.pdf", "abs": "https://arxiv.org/abs/2505.15805", "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering", "authors": ["Hwan Chang", "Yumin Kim", "Yonghyun Jun", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main Conference)", "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security."}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281.pdf", "abs": "https://arxiv.org/abs/2505.16281", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://github.com/nlp2ct-shijie/HiMATE."}
{"id": "2505.16408", "pdf": "https://arxiv.org/pdf/2505.16408.pdf", "abs": "https://arxiv.org/abs/2505.16408", "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior. We release our code at\nhttps://github.com/faridlazuarda/from-surveys-to-narratives."}
{"id": "2505.16467", "pdf": "https://arxiv.org/pdf/2505.16467.pdf", "abs": "https://arxiv.org/abs/2505.16467", "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fernández"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP Main 2025", "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity."}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345.pdf", "abs": "https://arxiv.org/abs/2505.19345", "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-stakes texts such as patent claims, medical records, and technical\nreports are structurally complex and demand a high degree of reliability and\nprecision. While large language models (LLMs) have recently been applied to\nautomate their generation in high-stakes domains, reliably evaluating such\noutputs remains a major challenge. Conventional natural language generation\n(NLG) metrics are effective for generic documents but fail to capture the\nstructural and legal characteristics essential to evaluating complex\nhigh-stakes documents. To address this gap, we propose PatentScore, a\nmulti-dimensional evaluation framework specifically designed for one of the\nmost intricate and rigorous domains, patent claims. PatentScore integrates\nhierarchical decomposition of claim elements, validation patterns grounded in\nlegal and technical standards, and scoring across structural, semantic, and\nlegal dimensions. In experiments on our dataset which consists of 400 Claim1,\nPatentScore achieved the highest correlation with expert annotations ($r =\n0.819$), significantly outperforming widely used NLG metrics. This work\nestablishes a new standard for evaluating LLM-generated patent claims,\nproviding a solid foundation for research on patent generation and validation."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740.pdf", "abs": "https://arxiv.org/abs/2505.21740", "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2506.01419", "pdf": "https://arxiv.org/pdf/2506.01419.pdf", "abs": "https://arxiv.org/abs/2506.01419", "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment", "authors": ["Joseph Marvin Imperial", "Abdullah Barayan", "Regina Stodden", "Rodrigo Wilkens", "Ricardo Munoz Sanchez", "Lingyun Gao", "Melissa Torgbi", "Dawn Knight", "Gail Forey", "Reka R. Jablonkai", "Ekaterina Kochmar", "Robert Reynolds", "Eugénio Ribeiro", "Horacio Saggion", "Elena Volodina", "Sowmya Vajjala", "Thomas François", "Fernando Alva-Manchego", "Harish Tayyar Madabushi"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "We introduce UniversalCEFR, a large-scale multilingual and multidimensional\ndataset of texts annotated with CEFR (Common European Framework of Reference)\nlevels in 13 languages. To enable open research in automated readability and\nlanguage proficiency assessment, UniversalCEFR comprises 505,807 CEFR-labeled\ntexts curated from educational and learner-oriented resources, standardized\ninto a unified data format to support consistent processing, analysis, and\nmodelling across tasks and languages. To demonstrate its utility, we conduct\nbenchmarking experiments using three modelling paradigms: a) linguistic\nfeature-based classification, b) fine-tuning pre-trained LLMs, and c)\ndescriptor-based prompting of instruction-tuned LLMs. Our results support using\nlinguistic features and fine-tuning pretrained models in multilingual CEFR\nlevel assessment. Overall, UniversalCEFR aims to establish best practices in\ndata distribution for language proficiency research by standardising dataset\nformats, and promoting their accessibility to the global research community."}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592.pdf", "abs": "https://arxiv.org/abs/2506.03592", "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "authors": ["Viktor Hangya", "Fabian Küch", "Darina Gold"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. Our project is available at:\nhttps://github.com/Fraunhofer-IIS/EvalShortcut"}
{"id": "2506.08375", "pdf": "https://arxiv.org/pdf/2506.08375.pdf", "abs": "https://arxiv.org/abs/2506.08375", "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "authors": ["Tao Zou", "Xinghua Zhang", "Haiyang Yu", "Minzheng Wang", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications."}
{"id": "2506.08479", "pdf": "https://arxiv.org/pdf/2506.08479.pdf", "abs": "https://arxiv.org/abs/2506.08479", "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$", "authors": ["Chihiro Taguchi", "Seiji Maekawa", "Nikita Bhutani"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages, 16 tables, 5 figures. Accepted at EMNLP 2025 (Main)", "summary": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs)\nboth address context limitations of LLMs in open-domain question answering\n(QA). However, optimal external context to retrieve remains an open problem:\nfixing the retrieval size risks either wasting tokens or omitting key evidence.\nExisting adaptive methods like Self-RAG and Self-Route rely on iterative LLM\nprompting and perform well on factoid QA, but struggle with aggregation QA,\nwhere the optimal context size is both unknown and variable. We present\nAdaptive-$k$ retrieval, a simple and effective single-pass method that\nadaptively selects the number of passages based on the distribution of the\nsimilarity scores between the query and the candidate passages. It does not\nrequire model fine-tuning, extra LLM inferences or changes to existing\nretriever-reader pipelines. On both factoid and aggregation QA benchmarks,\nAdaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x\nfewer tokens than full-context input, yet still retrieves 70% of relevant\npassages. It improves accuracy across five LCLMs and two embedding models,\nhighlighting that dynamically adjusting context size leads to more efficient\nand accurate QA."}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335.pdf", "abs": "https://arxiv.org/abs/2506.14335", "title": "References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."}
{"id": "2506.17088", "pdf": "https://arxiv.org/pdf/2506.17088.pdf", "abs": "https://arxiv.org/abs/2506.17088", "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "authors": ["Jiahao Cheng", "Tiancheng Su", "Jia Yuan", "Guoxiu He", "Jiawei Liu", "Xinqi Tao", "Jingwen Xie", "Huaxia Li"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://github.com/ECNU-Text-Computing/cot-hallu-detect ."}
{"id": "2506.20409", "pdf": "https://arxiv.org/pdf/2506.20409.pdf", "abs": "https://arxiv.org/abs/2506.20409", "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2026 Main", "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."}
{"id": "2507.19081", "pdf": "https://arxiv.org/pdf/2507.19081.pdf", "abs": "https://arxiv.org/abs/2507.19081", "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy."}
{"id": "2508.09767", "pdf": "https://arxiv.org/pdf/2508.09767.pdf", "abs": "https://arxiv.org/abs/2508.09767", "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "authors": ["Shuhei Kato"], "categories": ["cs.CL", "eess.AS"], "comment": "5 pages", "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."}
{"id": "2508.16048", "pdf": "https://arxiv.org/pdf/2508.16048.pdf", "abs": "https://arxiv.org/abs/2508.16048", "title": "OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages", "authors": ["Raphaël Merx", "Hanna Suominen", "Trevor Cohn", "Ekaterina Vylomova"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at WMT 2025", "summary": "In machine translation (MT), health is a high-stakes domain characterised by\nwidespread deployment and domain-specific vocabulary. However, there is a lack\nof MT evaluation datasets for low-resource languages in this domain. To address\nthis gap, we introduce OpenWHO, a document-level parallel corpus of 2,978\ndocuments and 26,824 sentences from the World Health Organization's e-learning\nplatform. Sourced from expert-authored, professionally translated materials\nshielded from web-crawling, OpenWHO spans a diverse range of over 20 languages,\nof which nine are low-resource. Leveraging this new resource, we evaluate\nmodern large language models (LLMs) against traditional MT models. Our findings\nreveal that LLMs consistently outperform traditional MT models, with Gemini 2.5\nFlash achieving a +4.79 ChrF point improvement over NLLB-54B on our\nlow-resource test set. Further, we investigate how LLM context utilisation\naffects accuracy, finding that the benefits of document-level translation are\nmost pronounced in specialised domains like health. We release the OpenWHO\ncorpus to encourage further research into low-resource MT in the health domain."}
{"id": "2508.19594", "pdf": "https://arxiv.org/pdf/2508.19594.pdf", "abs": "https://arxiv.org/abs/2508.19594", "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient."}
{"id": "2509.04655", "pdf": "https://arxiv.org/pdf/2509.04655.pdf", "abs": "https://arxiv.org/abs/2509.04655", "title": "Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs", "authors": ["Ayush Gupta", "Ramneet Kaur", "Anirban Roy", "Adam D. Cobb", "Rama Chellappa", "Susmit Jha"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "We propose a novel inference-time out-of-domain (OOD) detection algorithm for\nspecialized large language models (LLMs). Despite achieving state-of-the-art\nperformance on in-domain tasks through fine-tuning, specialized LLMs remain\nvulnerable to incorrect or unreliable outputs when presented with OOD inputs,\nposing risks in critical applications. Our method leverages the Inductive\nConformal Anomaly Detection (ICAD) framework, using a new non-conformity\nmeasure based on the model's dropout tolerance. Motivated by recent findings on\npolysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs\nexhibit higher dropout tolerance than OOD inputs. We aggregate dropout\ntolerance across multiple layers via a valid ensemble approach, improving\ndetection while maintaining theoretical false alarm bounds from ICAD.\nExperiments with medical-specialized LLMs show that our approach detects OOD\ninputs better than baseline methods, with AUROC improvements of $2\\%$ to $37\\%$\nwhen treating OOD datapoints as positives and in-domain test datapoints as\nnegatives."}
{"id": "2509.05066", "pdf": "https://arxiv.org/pdf/2509.05066.pdf", "abs": "https://arxiv.org/abs/2509.05066", "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Andreas Bulling"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (Main)", "summary": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch."}
{"id": "2509.05100", "pdf": "https://arxiv.org/pdf/2509.05100.pdf", "abs": "https://arxiv.org/abs/2509.05100", "title": "ICR: Iterative Clarification and Rewriting for Conversational Search", "authors": ["Zhiyu Cao", "Peifeng Li", "Qiaoming Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most previous work on Conversational Query Rewriting employs an end-to-end\nrewriting paradigm. However, this approach is hindered by the issue of multiple\nfuzzy expressions within the query, which complicates the simultaneous\nidentification and rewriting of multiple positions. To address this issue, we\npropose a novel framework ICR (Iterative Clarification and Rewriting), an\niterative rewriting scheme that pivots on clarification questions. Within this\nframework, the model alternates between generating clarification questions and\nrewritten queries. The experimental results show that our ICR can continuously\nimprove retrieval performance in the clarification-rewriting iterative process,\nthereby achieving state-of-the-art performance on two popular datasets."}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts."}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."}
{"id": "2509.07459", "pdf": "https://arxiv.org/pdf/2509.07459.pdf", "abs": "https://arxiv.org/abs/2509.07459", "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language."}
{"id": "2509.08022", "pdf": "https://arxiv.org/pdf/2509.08022.pdf", "abs": "https://arxiv.org/abs/2509.08022", "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": "Some parts of the paper need to be revised. We would therefore like\n  to withdraw the paper and resubmit it after making the necessary changes", "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development."}
{"id": "2509.10847", "pdf": "https://arxiv.org/pdf/2509.10847.pdf", "abs": "https://arxiv.org/abs/2509.10847", "title": "A funny companion: Distinct neural responses to perceived AI- versus human-generated humor", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As AI companions become capable of human-like communication, including\ntelling jokes, understanding how people cognitively and emotionally respond to\nAI humor becomes increasingly important. This study used electroencephalography\n(EEG) to compare how people process humor from AI versus human sources.\nBehavioral analysis revealed that participants rated AI and human humor as\ncomparably funny. However, neurophysiological data showed that AI humor\nelicited a smaller N400 effect, suggesting reduced cognitive effort during the\nprocessing of incongruity. This was accompanied by a larger Late Positive\nPotential (LPP), indicating a greater degree of surprise and emotional\nresponse. This enhanced LPP likely stems from the violation of low initial\nexpectations regarding AI's comedic capabilities. Furthermore, a key temporal\ndynamic emerged: human humor showed habituation effects, marked by an\nincreasing N400 and a decreasing LPP over time. In contrast, AI humor\ndemonstrated increasing processing efficiency and emotional reward, with a\ndecreasing N400 and an increasing LPP. This trajectory reveals how the brain\ncan dynamically update its predictive model of AI capabilities. This process of\ncumulative reinforcement challenges \"algorithm aversion\" in humor, as it\ndemonstrates how cognitive adaptation to AI's language patterns can lead to an\nintensified emotional reward. Additionally, participants' social attitudes\ntoward AI modulated these neural responses, with higher perceived AI\ntrustworthiness correlating with enhanced emotional engagement. These findings\nindicate that the brain responds to AI humor with surprisingly positive and\nintense reactions, highlighting humor's potential for fostering genuine\nengagement in human-AI social interaction."}
{"id": "2509.11177", "pdf": "https://arxiv.org/pdf/2509.11177.pdf", "abs": "https://arxiv.org/abs/2509.11177", "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs", "authors": ["Hang Guo", "Yawei Li", "Luca Benini"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline."}
{"id": "2509.11552", "pdf": "https://arxiv.org/pdf/2509.11552.pdf", "abs": "https://arxiv.org/abs/2509.11552", "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "authors": ["Wensheng Lu", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."}
{"id": "2509.12108", "pdf": "https://arxiv.org/pdf/2509.12108.pdf", "abs": "https://arxiv.org/abs/2509.12108", "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "authors": ["Min Zeng", "Jingfei Sun", "Xueyou Luo", "Caiquan Liu", "Shiqi Zhang", "Li Xie", "Xiaoxin Chen"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines."}
{"id": "2403.16393", "pdf": "https://arxiv.org/pdf/2403.16393.pdf", "abs": "https://arxiv.org/abs/2403.16393", "title": "Concurrent Linguistic Error Detection (CLED): a New Methodology for Error Detection in Large Language Models", "authors": ["Jinhua Zhu", "Javier Conde", "Zhen Gao", "Pedro Reviriego", "Shanshan Liu", "Fabrizio Lombardi"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages, 6 figures, 30 references", "summary": "The wide adoption of Large language models (LLMs) makes their dependability a\npressing concern. Detection of errors is the first step to mitigating their\nimpact on a system and thus, efficient error detection for LLMs is an important\nissue. In many settings, the LLM is considered as a black box with no access to\nthe internal nodes; this prevents the use of many error detection schemes that\nneed access to the model's internal nodes. An interesting observation is that\nthe output of LLMs in error-free operation should be valid and normal text.\nTherefore, when the text is not valid or differs significantly from normal\ntext, it is likely that there is an error. Based on this observation we propose\nto perform Concurrent Linguistic Error Detection (CLED); this scheme extracts\nsome linguistic features of the text generated by the LLM and feeds them to a\nconcurrent classifier that detects errors. Since the proposed error detection\nmechanism only relies on the outputs of the model, then it can be used on LLMs\nin which there is no access to the internal nodes. The proposed CLED scheme has\nbeen evaluated on the T5 model when used for news summarization and on the\nOPUS-MT model when used for translation. In both cases, the same set of\nlinguistic features has been used for error detection to illustrate the\napplicability of the proposed scheme beyond a specific case. The results show\nthat CLED can detect most of the errors at a low overhead penalty. The use of\nthe concurrent classifier also enables a trade-off between error detection\neffectiveness and its associated overhead, so providing flexibility to a\ndesigner."}
{"id": "2410.23506", "pdf": "https://arxiv.org/pdf/2410.23506.pdf", "abs": "https://arxiv.org/abs/2410.23506", "title": "The Belief State Transformer", "authors": ["Edward S. Hu", "Kwangjun Ahn", "Qinghua Liu", "Haoran Xu", "Manan Tomar", "Ada Langford", "Jayden Teoh", "Bryon Xu", "David Yan", "Dinesh Jayaraman", "Alex Lamb", "John Langford"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Updated report with new improvements and authors", "summary": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\nWebsite: https://edwhu.github.io/bst-website"}
{"id": "2411.09689", "pdf": "https://arxiv.org/pdf/2411.09689.pdf", "abs": "https://arxiv.org/abs/2411.09689", "title": "Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge", "authors": ["Seongmin Lee", "Hsiang Hsu", "Chun-Fu Chen", "Duen Horng Chau"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 15 figures", "summary": "LLM hallucination, where unfaithful text is generated, presents a critical\nchallenge for LLMs' practical applications. Current detection methods often\nresort to external knowledge, LLM fine-tuning, or supervised training with\nlarge hallucination-labeled datasets. Moreover, these approaches do not\ndistinguish between different types of hallucinations, which is crucial for\nenhancing detection performance. To address such limitations, we introduce\nhallucination probing, a new task that classifies LLM-generated text into three\ncategories: aligned, misaligned, and fabricated. Driven by our novel discovery\nthat perturbing key entities in prompts affects LLM's generation of these three\ntypes of text differently, we propose SHINE, a novel hallucination probing\nmethod that does not require external knowledge, supervised training, or LLM\nfine-tuning. SHINE is effective in hallucination probing across three modern\nLLMs, and achieves state-of-the-art performance in hallucination detection,\noutperforming seven competing methods across four datasets and four LLMs,\nunderscoring the importance of probing for accurate detection."}
{"id": "2411.19331", "pdf": "https://arxiv.org/pdf/2411.19331.pdf", "abs": "https://arxiv.org/abs/2411.19331", "title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation", "authors": ["Luca Barsellotti", "Lorenzo Bianchi", "Nicola Messina", "Fabio Carrara", "Marcella Cornia", "Lorenzo Baraldi", "Fabrizio Falchi", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025", "summary": "Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/."}
{"id": "2502.19668", "pdf": "https://arxiv.org/pdf/2502.19668.pdf", "abs": "https://arxiv.org/abs/2502.19668", "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning", "authors": ["Mingsheng Cai", "Jiuming Jiang", "Wenhao Huang", "Che Liu", "Rossella Arcucci"], "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG"], "comment": "Findings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)", "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."}
{"id": "2506.19579", "pdf": "https://arxiv.org/pdf/2506.19579.pdf", "abs": "https://arxiv.org/abs/2506.19579", "title": "Evaluating the Robustness of Open-Source Vision-Language Models to Domain Shift in Object Captioning", "authors": ["Federico Tavella", "Amber Drinkwater", "Angelo Cangelosi"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as powerful tools for generating\ntextual descriptions from visual data. While these models excel on web-scale\ndatasets, their robustness to the domain shifts inherent in many real-world\napplications remains under-explored. This paper presents a systematic\nevaluation of VLM performance on a single-view object captioning task when\nfaced with a controlled, physical domain shift. We compare captioning accuracy\nacross two distinct object sets: a collection of multi-material, real-world\ntools and a set of single-material, 3D-printed items. The 3D-printed set\nintroduces a significant domain shift in texture and material properties,\nchallenging the models' generalization capabilities. Our quantitative results\ndemonstrate that all tested VLMs show a marked performance degradation when\ndescribing the 3D-printed objects compared to the real-world tools. This\nunderscores a critical limitation in the ability of current models to\ngeneralize beyond surface-level features and highlights the need for more\nrobust architectures for real-world signal processing applications."}
{"id": "2507.02844", "pdf": "https://arxiv.org/pdf/2507.02844.pdf", "abs": "https://arxiv.org/abs/2507.02844", "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "Accepted to EMNLP 2025 (Main). 17 pages, 7 figures", "summary": "With the emergence of strong vision language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: vision-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct vision-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a vision-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%.\nCode: https://github.com/Dtc7w3PQ/Visco-Attack."}
{"id": "2507.13773", "pdf": "https://arxiv.org/pdf/2507.13773.pdf", "abs": "https://arxiv.org/abs/2507.13773", "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "ACL2025 Main (SAC Highlight Award)", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios."}
{"id": "2507.20999", "pdf": "https://arxiv.org/pdf/2507.20999.pdf", "abs": "https://arxiv.org/abs/2507.20999", "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning", "authors": ["Yining Huang", "Bin Li", "Keke Tang", "Meilian Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "12 pages", "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."}
{"id": "2508.00033", "pdf": "https://arxiv.org/pdf/2508.00033.pdf", "abs": "https://arxiv.org/abs/2508.00033", "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "João P. Matos-Carvalho"], "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "comment": "The peer-reviewed version of this paper is published in Future\n  Internet at https://doi.org/10.3390/fi17090412. This version is typeset by\n  the author and differs only in pagination and typographical detail", "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code. GPT-4.1 achieved a 100\\%\nsuccess rate across all runs in both experimental tasks, whereas most other\nmodels succeeded in fewer than half of the runs, with only Grok-3 and\nMistral-Large approaching comparable performance. In addition to benchmarking\nLLM performance, this approach helps identify shortcomings in third-party\nlibraries, such as unclear documentation or obscure implementation bugs.\nOverall, these findings highlight current limitations of LLMs for end-to-end\nscientific automation and emphasize the need for careful prompt design,\ncomprehensive library documentation, and continued advances in language model\ncapabilities."}
{"id": "2508.09456", "pdf": "https://arxiv.org/pdf/2508.09456.pdf", "abs": "https://arxiv.org/abs/2508.09456", "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack."}
{"id": "2509.05983", "pdf": "https://arxiv.org/pdf/2509.05983.pdf", "abs": "https://arxiv.org/abs/2509.05983", "title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "authors": ["Minh N. H. Nguyen", "Anh Nguyen Tran", "Dung Truong Dinh", "Nam Van Vo"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "I need to withdraw the paper as there something wrong", "summary": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios."}
{"id": "2509.09775", "pdf": "https://arxiv.org/pdf/2509.09775.pdf", "abs": "https://arxiv.org/abs/2509.09775", "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "authors": ["Aleksandr Boldachev"], "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "comment": "22 pages, 6 figures. Corrected captions on Figure 4", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework."}
{"id": "2509.10105", "pdf": "https://arxiv.org/pdf/2509.10105.pdf", "abs": "https://arxiv.org/abs/2509.10105", "title": "VARCO-VISION-2.0 Technical Report", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "categories": ["cs.CV", "cs.CL"], "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model."}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "The first two authors hold equal contribution", "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior."}
{"id": "2509.11967", "pdf": "https://arxiv.org/pdf/2509.11967.pdf", "abs": "https://arxiv.org/abs/2509.11967", "title": "MillStone: How Open-Minded Are LLMs?", "authors": ["Harold Triedman", "Vitaly Shmatikov"], "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 7 tables, 7 figures", "summary": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated."}
