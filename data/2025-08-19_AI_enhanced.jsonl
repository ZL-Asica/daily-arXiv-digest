{"id": "2508.11770", "pdf": "https://arxiv.org/pdf/2508.11770.pdf", "abs": "https://arxiv.org/abs/2508.11770", "title": "FairVizARD: A Visualization System for Assessing Multi-Party Fairness of Ride-Sharing Matching Algorithms", "authors": ["Ashwin Kumar", "Sanket Shah", "Meghna Lowalekar", "Pradeep Varakantham", "Alvitta Ottley", "William Yeoh"], "categories": ["cs.HC"], "comment": null, "summary": "There is growing interest in algorithms that match passengers with drivers in\nride-sharing problems and their fairness for the different parties involved\n(passengers, drivers, and ride-sharing companies). Researchers have proposed\nvarious fairness metrics for matching algorithms, but it is often unclear how\none should balance the various parties' fairness, given that they are often in\nconflict. We present FairVizARD, a visualization-based system that aids users\nin evaluating the fairness of ride-sharing matching algorithms. FairVizARD\npresents the algorithms' results by visualizing relevant spatio-temporal\ninformation using animation and aggregated information in charts. FairVizARD\nalso employs efficient techniques for visualizing a large amount of information\nin a user friendly manner, which makes it suitable for real-world settings. We\nconduct our experiments on a real-world large-scale taxi dataset and, through\nuser studies and an expert interview, we show how users can use FairVizARD not\nonly to evaluate the fairness of matching algorithms but also to expand on\ntheir notions of fairness.", "AI": {"tldr": "FairVizARD is a visualization-based system designed to help users evaluate the fairness of ride-sharing matching algorithms, presenting results through animation and charting methods.", "motivation": "To address the growing interest in algorithms matching passengers with drivers in ride-sharing, and to explore the fairness of these algorithms among different parties.", "method": "A visualization-based system employing spatio-temporal information and efficient visual techniques to present algorithmic results.", "result": "User studies and an expert interview demonstrated that FairVizARD helps users evaluate fairness and broaden their understanding of the concept.", "conclusion": "FairVizARD provides a practical tool for real-world applications, enhancing the evaluation of fairness in ride-sharing matching algorithms.", "key_contributions": ["Introduction of FairVizARD for visualizing fairness in ride-sharing algorithms", "Utilization of spatio-temporal information in visualizations", "Empirical validation through user studies and expert interviews"], "limitations": "", "keywords": ["fairness", "ride-sharing", "visualization", "algorithms", "user studies"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11778", "pdf": "https://arxiv.org/pdf/2508.11778.pdf", "abs": "https://arxiv.org/abs/2508.11778", "title": "XR-First Design for Productivity: A Conceptual Framework for Enabling Efficient Task Switching in XR", "authors": ["Matt Gottsacker", "Yahya Hmaiti", "Mykola Maslych", "Gerd Bruder", "Joseph J. LaViola Jr.", "Gregory F. Welch"], "categories": ["cs.HC"], "comment": "IEEE ISMAR xrWORKS Workshop", "summary": "A core component of completing tasks efficiently in computer-supported\nknowledge work is the ability for users to rapidly switch their focus (and\ninteraction) across different applications using various shortcuts and\ngestures. This feature set has been explored in research, and several modern\nconsumer extended reality (XR) headsets now support loading multiple\napplications windows at once. However, many XR applications that are useful for\nknowledge work involve rich spatial information, which window-based metaphors\ndo not sufficiently represent nor afford appropriate interaction. In modern XR\nheadsets, such immersive applications run as siloed experiences, requiring the\nuser to fully exit one before starting another. We present a vision for\nachieving an XR-first, user-centric paradigm for efficient context switching in\nXR to encourage and guide future research and development of XR context- and\ntask-switching interfaces.", "AI": {"tldr": "The paper presents a vision for improving context switching in extended reality (XR) environments, emphasizing the need for user-centric interfaces to facilitate multitasking and better interaction with spatial information applications.", "motivation": "Existing XR applications require users to exit one app to start another, impacting efficiency in knowledge work that benefits from quick context switching.", "method": "The paper discusses a proposed user-centric paradigm for context and task switching in XR, aimed at guiding future development of related interfaces.", "result": "A conceptual framework for XR-first interfaces that support efficient task management and multitasking in immersive environments.", "conclusion": "The study emphasizes the importance of developing new interaction paradigms that support seamless context switching in XR applications for enhanced user efficiency.", "key_contributions": ["Proposed a user-centric paradigm for context switching in XR environments.", "Identified limitations of current window-based metaphors for spatial information in XR.", "Outlined future research directions for developing efficient XR interfaces."], "limitations": "", "keywords": ["extended reality", "context switching", "user-centric design", "knowledge work", "task management"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11781", "pdf": "https://arxiv.org/pdf/2508.11781.pdf", "abs": "https://arxiv.org/abs/2508.11781", "title": "Behavioral and Symbolic Fillers as Delay Mitigation for Embodied Conversational Agents in Virtual Reality", "authors": ["Denmar Mojan Gonzales", "Snehanjali Kalamkar", "Sophie JÃ¶rg", "Jens Grubert"], "categories": ["cs.HC"], "comment": "Accepted to IEEE Transactions on Visualization and Computer Graphics", "summary": "When communicating with embodied conversational agents (ECAs) in virtual\nreality, there might be delays in the responses of the agents lasting several\nseconds, for example, due to more extensive computations of the answers when\nlarge language models are used. Such delays might lead to unnatural or\nfrustrating interactions. In this paper, we investigate filler types to\nmitigate these effects and lead to a more positive experience and perception of\nthe agent. In a within-subject study, we asked 24 participants to communicate\nwith ECAs in virtual reality, comparing four strategies displayed during the\ndelays: a multimodal behavioral filler consisting of conversational and\ngestural fillers, a base condition with only idle motions, and two symbolic\nindicators with progress bars, one embedded as a badge on the agent, the other\none external and visualized as a thinking bubble. Our results indicate that the\nbehavioral filler improved perceived response time, three subscales of\npresence, humanlikeness, and naturalness. Participants looked away from the\nface more often when symbolic indicators were displayed, but the visualizations\ndid not lead to a more positive impression of the agent or to increased\npresence. The majority of participants preferred the behavioral fillers, only\n12.5% and 4.2% favored the symbolic embedded and external conditions,\nrespectively.", "AI": {"tldr": "The paper investigates the impact of different filler types on user experience when interacting with ECAs in virtual reality, aiming to improve perceived response times and interaction quality.", "motivation": "To address the frustrations caused by response delays in embodied conversational agents by exploring effective filler types to enhance user experience.", "method": "A within-subject study with 24 participants compared four strategies used during response delays: multimodal behavioral fillers, idle motion, and two types of symbolic indicators (progress bars, one as an embedded badge and one as an external thinking bubble).", "result": "Behavioral fillers significantly improved perceived response time and user perceptions of presence, humanlikeness, and naturalness, while symbolic indicators led to more gaze aversion without enhancing overall impressions of the agent.", "conclusion": "Participants preferred behavioral fillers over symbolic indicators, indicating their effectiveness in improving the user experience during delays in response.", "key_contributions": ["Identification of effective filler types for improving interactions with ECAs in VR.", "Demonstrated the limitations of symbolic indicators in enhancing user experience.", "Provided insights into user preferences regarding interaction types in virtual environments."], "limitations": "Study limited to specific types of fillers; results may not generalize to all ECAs or VR environments.", "keywords": ["embodied conversational agents", "virtual reality", "filler types", "user experience", "interaction design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11788", "pdf": "https://arxiv.org/pdf/2508.11788.pdf", "abs": "https://arxiv.org/abs/2508.11788", "title": "Two Sides to Every Story: Exploring Hybrid Design Teams' Perceptions of Psychological Safety on Slack", "authors": ["Marjan Naghshbandi", "Sharon Ferguson", "Alison Olechowski"], "categories": ["cs.HC"], "comment": "To be published in the Proceedings of the ACM on Human-Computer\n  Interaction, Vol. 9, No. 7", "summary": "While the unique challenges of hybrid work can compromise collaboration and\nteam dynamics, hybrid teams can thrive with well-informed strategies and tools\nthat nurture interpersonal engagements. To inform future supports, we pursue a\nmixed-methods study of hybrid engineering design capstone teams' Psychological\nSafety (PS) (i.e., their climate of interpersonal risk-taking and mutual\nrespect) to understand how the construct manifests in teams engaged in\ninnovation. Using interviews, we study six teams' perceptions of PS indicators\nand how they present differently on Slack (when compared to in-person\ninteractions). We then leverage the interview insights to design Slack-based PS\nindicators. We present five broad facets of PS in hybrid teams, four perceived\ndifferences of PS on Slack compared to in-person, and 15 Slack-based, PS\nindicators--the groundwork for future automated PS measurement on\ninstant-messaging platforms. These insights produce three design implications\nand illustrative design examples for ways instant-messaging platforms can\nsupport Psychologically Safe hybrid teams, and best practices for hybrid teams\nto support interpersonal risk-taking and build mutual respect.", "AI": {"tldr": "This paper explores the Psychological Safety (PS) of hybrid engineering design teams and develops strategies to enhance team dynamics through Slack-based indicators.", "motivation": "To address the challenges of hybrid work affecting collaboration and team dynamics, and to enhance Psychological Safety in such teams.", "method": "A mixed-methods study involving interviews with six hybrid engineering design teams to assess their perceptions of Psychological Safety indicators and their manifestation on Slack versus in-person.", "result": "Identified five facets of Psychological Safety in hybrid teams, four differences in PS perceptions on Slack and in-person, and developed 15 Slack-based PS indicators.", "conclusion": "The insights lead to design implications for instant-messaging platforms to improve Psychological Safety in hybrid teams and provide best practices for fostering interpersonal risk-taking.", "key_contributions": ["Developed Slack-based Psychological Safety indicators.", "Presented design examples for enhancing hybrid team dynamics.", "Outlined best practices for supporting interpersonal risk-taking in hybrid settings."], "limitations": "", "keywords": ["Psychological Safety", "Hybrid Teams", "Human-Computer Interaction", "Slack Indicators", "Interpersonal Risk-Taking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11676", "pdf": "https://arxiv.org/pdf/2508.11676.pdf", "abs": "https://arxiv.org/abs/2508.11676", "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry.", "AI": {"tldr": "A novel framework utilizing LLM internal weight activations to create a metric space of languages, uncovering linguistic connections across 106 languages.", "motivation": "To provide an automated method for representing linguistic characteristics without relying on hand-crafted features.", "method": "The method computes weight importance scores using an adapted pruning algorithm to generate high-dimensional vector representations of languages.", "result": "The approach aligns well with established linguistic families and uncovers unexpected relationships among languages, suggesting historical contacts or language evolution.", "conclusion": "The framework offers insights into language relationships and is a significant advancement in linguistic analysis using LLMs.", "key_contributions": ["Development of a novel framework for language representation using LLMs", "Discovery of unexpected linguistic connections across languages", "Public availability of source code and tools for further research."], "limitations": "", "keywords": ["large language models", "language representation", "linguistic connections", "metric space", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.11892", "pdf": "https://arxiv.org/pdf/2508.11892.pdf", "abs": "https://arxiv.org/abs/2508.11892", "title": "RPKT: Learning What You Don't -- Know Recursive Prerequisite Knowledge Tracing in Conversational AI Tutors for Personalized Learning", "authors": ["Jinwen Tang", "Qiming Guo", "Zhicheng Tang"], "categories": ["cs.HC"], "comment": null, "summary": "Educational systems often assume learners can identify their knowledge gaps,\nyet research consistently shows that students struggle to recognize what they\ndon't know they need to learn-the \"unknown unknowns\" problem. This paper\npresents a novel Recursive Prerequisite Knowledge Tracing (RPKT) system that\naddresses this challenge through dynamic prerequisite discovery using large\nlanguage models. Unlike existing adaptive learning systems that rely on\npre-defined knowledge graphs, our approach recursively traces prerequisite\nconcepts in real-time until reaching a learner's actual knowledge boundary. The\nsystem employs LLMs for intelligent prerequisite extraction, implements binary\nassessment interfaces for cognitive load reduction, and provides personalized\nlearning paths based on identified knowledge gaps. Demonstration across\ncomputer science domains shows the system can discover multiple nested levels\nof prerequisite dependencies, identify cross-domain mathematical foundations,\nand generate hierarchical learning sequences without requiring pre-built\ncurricula. Our approach shows great potential for advancing personalized\neducation technology by enabling truly adaptive learning across any academic\ndomain.", "AI": {"tldr": "The paper presents a Recursive Prerequisite Knowledge Tracing (RPKT) system that utilizes LLMs for dynamic discovery of knowledge prerequisites to enhance personalized learning in educational systems.", "motivation": "Educational systems struggle with the 'unknown unknowns' problem, where learners cannot identify their knowledge gaps. This paper aims to address this challenge.", "method": "The RPKT system uses large language models to recursively trace prerequisite concepts in real-time, implementing binary assessment interfaces to reduce cognitive load and providing personalized learning paths based on identified gaps.", "result": "Demonstrations show the system can uncover multiple layers of prerequisite dependencies and create customized hierarchical learning sequences without predefined curricula.", "conclusion": "RPKT has significant potential to enhance personalized education technology and enable adaptive learning across various academic domains.", "key_contributions": ["Introduction of a novel system for prerequisite knowledge tracing in education using LLMs", "Real-time dynamic discovery of knowledge gaps", "Personalized learning paths generated without predefined curricula"], "limitations": "", "keywords": ["Educational Technology", "Knowledge Tracing", "Large Language Models", "Adaptive Learning", "Personalized Education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11758", "pdf": "https://arxiv.org/pdf/2508.11758.pdf", "abs": "https://arxiv.org/abs/2508.11758", "title": "Can we Evaluate RAGs with Synthetic Data?", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.", "AI": {"tldr": "The paper explores the effectiveness of synthetic QA data from LLMs as a substitute for human-labeled benchmarks, revealing that while it can rank RAG variants accurately in some scenarios, it struggles with generator comparison due to task and stylistic mismatches.", "motivation": "To determine if synthetic question-answer data from large language models can replace human-labeled benchmarks when such data is scarce.", "method": "Two experiments were conducted: one with variable retriever parameters and fixed generators, and another with fixed retrievers and varying generators across four datasets (two open-domain and two proprietary).", "result": "Synthetic benchmarks were found to reliably rank RAGs based on retriever parameters, aligning well with human-labeled benchmarks, but inconsistencies arose when comparing different generator architectures.", "conclusion": "The observed breakdown in ranking consistency suggests that mismatches in task requirements and biases in generator styles affect the reliability of synthetic benchmarks.", "key_contributions": ["Assessment of synthetic QA data as substitutes for human benchmarks", "Evaluation of retriever and generator configurations on benchmark reliability", "Insights into limitations of synthetic benchmarks in RAG architecture comparisons"], "limitations": "The inconsistency in RAG rankings when comparing generators suggests significant limitations in task alignment and potential stylistic bias.", "keywords": ["synthetic data", "large language models", "question-answering", "human-benchmarks", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.12192", "pdf": "https://arxiv.org/pdf/2508.12192.pdf", "abs": "https://arxiv.org/abs/2508.12192", "title": "Playing telephone with generative models: \"verification disability,\" \"compelled reliance,\" and accessibility in data visualization", "authors": ["Frank Elavsky", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "H.5.2; I.2.6; I.3.3; J.4; K.4.2; I.2.10"], "comment": "11 pages, 7 figures, IEEE VIS Accessibility Workshop, position paper", "summary": "This paper is a collaborative piece between two worlds of expertise in the\nfield of data visualization: accessibility and bias. In particular, the rise of\ngenerative models playing a role in accessibility is a worrying trend for data\nvisualization. These models are increasingly used to help author visualizations\nas well as generate descriptions of existing visualizations for people who are\nblind, low vision, or use assistive technologies such as screen readers.\nSighted human-to-human bias has already been established as an area of concern\nfor theory, research, and design in data visualization. But what happens when\nsomeone is unable to verify the model output or adequately interrogate\nalgorithmic bias, such as a context where a blind person asks a model to\ndescribe a chart for them? In such scenarios, trust from the user is not\nearned, rather reliance is compelled by the model-to-human relationship. In\nthis work, we explored the dangers of AI-generated descriptions for\naccessibility, playing a game of telephone between models, observing bias\nproduction in model interpretation, and re-interpretation of a data\nvisualization. We unpack ways that model failure in visualization is especially\nproblematic for users with visual impairments, and suggest directions forward\nfor three distinct readers of this piece: technologists who build\nmodel-assisted interfaces for end users, users with disabilities leveraging\nmodels for their own purposes, and researchers concerned with bias,\naccessibility, or visualization.", "AI": {"tldr": "The paper examines the challenges of using generative models in data visualization for accessibility, specifically regarding biases that affect users with visual impairments.", "motivation": "To investigate the implications of relying on AI-generated descriptions in data visualization, especially for individuals who are blind or have low vision.", "method": "Collaboration between experts in accessibility and bias; exploration of model failures and biases in AI-generated visualizations through observational studies.", "result": "Identified that users cannot verify AI model outputs, leading to a forced reliance on models for interpreting data visualizations, which could exacerbate bias concerns.", "conclusion": "The study underscores the need to address algorithmic bias in AI-driven visualization tools and suggests paths forward for various stakeholders.", "key_contributions": ["Highlights the dangers of AI bias in accessibility contexts.", "Proposes recommendations for technologists, users with disabilities, and researchers.", "Explores human reliance on AI outputs without verification."], "limitations": "Focus primarily on AI-generated content without extensive empirical testing of user experiences.", "keywords": ["accessibility", "bias", "data visualization", "generative models", "AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11767", "pdf": "https://arxiv.org/pdf/2508.11767.pdf", "abs": "https://arxiv.org/abs/2508.11767", "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks.", "AI": {"tldr": "This paper applies imitation learning to conversational AI, recovering a talking policy and a discriminator that assesses conversation quality. It highlights the limitations of dialog models based on discriminator results.", "motivation": "To enhance conversational AI by using imitation learning in dialog systems and analyze the effectiveness of such models through expert demonstrations.", "method": "The paper implements imitation learning by training a policy on expert conversation demonstrations and developing a discriminator to distinguish between expert and synthetic conversations.", "result": "The developed policy demonstrates effective conversational abilities, while the discriminator's results reveal limitations in current dialog models.", "conclusion": "The findings suggest that imitation learning can be an effective approach for training conversational agents, but also highlight the need for further improvements in dialog models to mitigate adverse behaviors.", "key_contributions": ["Application of imitation learning to conversational AI", "Development of a discriminator for evaluating dialog quality", "Identification of limitations in dialog models through empirical results"], "limitations": "The discriminator indicates significant limitations in existing dialog models, suggesting the need for further refinement.", "keywords": ["Imitation Learning", "Conversational AI", "Dialog Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.12268", "pdf": "https://arxiv.org/pdf/2508.12268.pdf", "abs": "https://arxiv.org/abs/2508.12268", "title": "iTrace: Click-Based Gaze Visualization on the Apple Vision Pro", "authors": ["Esra Mehmedova", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC", "cs.CV"], "comment": "Paper submitted to ACM SIGGRAPH Motion, Interaction and Games 2025\n  (MIG 2025)", "summary": "The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet\nthe privacy restrictions on the device prevent direct access to continuous user\ngaze data. This study introduces iTrace, a novel application that overcomes\nthese limitations through click-based gaze extraction techniques, including\nmanual methods like a pinch gesture, and automatic approaches utilizing dwell\ncontrol or a gaming controller. We developed a system with a client-server\narchitecture that captures the gaze coordinates and transforms them into\ndynamic heatmaps for video and spatial eye tracking. The system can generate\nindividual and averaged heatmaps, enabling analysis of personal and collective\nattention patterns.\n  To demonstrate its effectiveness and evaluate the usability and performance,\na study was conducted with two groups of 10 participants, each testing\ndifferent clicking methods. The 8BitDo controller achieved higher average data\ncollection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell\ncontrol, enabling significantly denser heatmap visualizations. The resulting\nheatmaps reveal distinct attention patterns, including concentrated focus in\nlecture videos and broader scanning during problem-solving tasks. By allowing\ndynamic attention visualization while maintaining a high gaze precision of 91\n%, iTrace demonstrates strong potential for a wide range of applications in\neducational content engagement, environmental design evaluation, marketing\nanalysis, and clinical cognitive assessment. Despite the current gaze data\nrestrictions on the Apple Vision Pro, we encourage developers to use iTrace\nonly in research settings.", "AI": {"tldr": "iTrace is a gaze extraction application for Apple Vision Pro that uses novel click-based methods to generate dynamic heatmaps, enabling detailed analysis of attention patterns in various settings.", "motivation": "To address the privacy limitations of the Apple Vision Pro that restrict direct access to continuous gaze data, facilitating enhanced analysis of user attention across different contexts.", "method": "The study introduces iTrace, a client-server architecture that incorporates manual and automatic click-based gaze extraction methods to produce heatmaps from user gaze coordinates.", "result": "The click methods, particularly using the 8BitDo controller, achieved higher data collection rates compared to dwell control, allowing for more detailed heatmap visualizations that exhibit various attention patterns during different tasks.", "conclusion": "iTrace offers strong potential for applications in education, environmental design, marketing, and clinical assessments, while emphasizing its use should be confined to research settings due to data privacy concerns.", "key_contributions": ["Introduction of click-based gaze extraction techniques for the Apple Vision Pro", "Dynamic visualization of attention patterns through heatmaps", "Demonstration of usability and performance across different click methods"], "limitations": "Current gaze data restrictions imposed by the Apple Vision Pro limit broader application of iTrace.", "keywords": ["gaze extraction", "dynamic heatmaps", "attention patterns", "Apple Vision Pro", "iTrace"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2508.11771", "pdf": "https://arxiv.org/pdf/2508.11771.pdf", "abs": "https://arxiv.org/abs/2508.11771", "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "categories": ["cs.CL"], "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult.", "AI": {"tldr": "This study analyzes transcription inconsistencies in the Faetar ASR benchmark and evaluates the impact of bigram modeling and finite lexicon constraints.", "motivation": "To investigate transcription inconsistencies in the Faetar Automatic Speech Recognition benchmark and their effects on ASR performance in a low-resource context.", "method": "A small, hand-constructed lexicon was utilized to examine transcription inconsistencies and evaluate the effectiveness of bigram word-based language modeling and finite lexicon decoding.", "result": "While transcription inconsistencies exist, they are not the primary challenge; bigram modeling offers no additional benefits, and using a finite lexicon can provide improvement.", "conclusion": "The task remains extremely challenging despite addressing transcription issues and modeling techniques.", "key_contributions": ["Analysis of transcription inconsistencies in ASR tasks", "Evaluation of bigram language modeling's effectiveness", "Demonstration of finite lexicon constraints improving ASR performance"], "limitations": "Focuses only on transcription inconsistencies without exploring other ASR challenges.", "keywords": ["Automatic Speech Recognition", "transcription inconsistencies", "bigram modeling"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.12333", "pdf": "https://arxiv.org/pdf/2508.12333.pdf", "abs": "https://arxiv.org/abs/2508.12333", "title": "Sketchar: Supporting Character Design and Illustration Prototyping Using Generative AI", "authors": ["Long Ling", "Xinyi Chen", "Ruoyu Wen", "Toby Jia-Jun Li", "Ray LC"], "categories": ["cs.HC"], "comment": "Accepted at CHI PLAY 2024", "summary": "Character design in games involves interdisciplinary collaborations,\ntypically between designers who create the narrative content, and illustrators\nwho realize the design vision. However, traditional workflows face challenges\nin communication due to the differing backgrounds of illustrators and\ndesigners, the latter with limited artistic abilities. To overcome these\nchallenges, we created Sketchar, a Generative AI (GenAI) tool that allows\ndesigners to prototype game characters and generate images based on conceptual\ninput, providing visual outcomes that can give immediate feedback and enhance\ncommunication with illustrators' next step in the design cycle. We conducted a\nmixed-method study to evaluate the interaction between game designers and\nSketchar. We showed that the reference images generated in co-creating with\nSketchar fostered refinement of design details and can be incorporated into\nreal-world workflows. Moreover, designers without artistic backgrounds found\nthe Sketchar workflow to be more expressive and worthwhile. This research\ndemonstrates the potential of GenAI in enhancing interdisciplinary\ncollaboration in the game industry, enabling designers to interact beyond their\nown limited expertise.", "AI": {"tldr": "Sketchar is a Generative AI tool designed to enhance communication between game designers and illustrators by allowing designers to prototype characters and generate images from concepts.", "motivation": "Traditional workflows in game character design struggle with communication barriers between designers and illustrators due to differing backgrounds and skills.", "method": "A mixed-method study was conducted to evaluate designer interactions with the Sketchar tool.", "result": "Reference images generated by Sketchar improved design refinement and were found to be useful in real-world workflows, especially for designers lacking artistic skills.", "conclusion": "GenAI, as demonstrated in the Sketchar tool, can significantly improve interdisciplinary collaboration in gaming by enabling designers to work beyond their artistic limitations.", "key_contributions": ["Development of Sketchar, a GenAI tool for character design", "Empirical evidence of improved communication in design workflows", "Demonstration of GenAI's utility for non-artistic designers"], "limitations": "", "keywords": ["Generative AI", "Game Design", "Interdisciplinary Collaboration"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11779", "pdf": "https://arxiv.org/pdf/2508.11779.pdf", "abs": "https://arxiv.org/abs/2508.11779", "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.", "AI": {"tldr": "This paper evaluates the potential of large language models (LLMs) in aiding academic peer review with various tasks, discovering limitations in their text-processing capabilities.", "motivation": "The potential of large language models (LLMs) to assist scientific discovery, particularly in the realm of academic peer review, is under scrutiny.", "method": "The study organizes computer science tasks related to academic text processing into a workflow to assess LLMs, specifically focusing on content reproduction, comparison, scoring, and reflection roles of LLMs.", "result": "The evaluation of Google's Gemini revealed compromised performance in text processing tasks, with reliability in summaries and paraphrases but limitations in text ranking and grading.", "conclusion": "The study does not recommend the unchecked use of LLMs in peer reviews due to their inconsistent and often inadequate performance.", "key_contributions": ["Proposed a workflow for assessing LLMs in academic context", "Conducted rigorous performance evaluation using top Information Systems journals", "Documented specific limitations of LLMs in text-related tasks"], "limitations": "The paper primarily focuses on one LLM and does not explore comparative analysis with other models.", "keywords": ["large language models", "academic peer review", "text processing", "scientific discovery", "performance evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12385", "pdf": "https://arxiv.org/pdf/2508.12385.pdf", "abs": "https://arxiv.org/abs/2508.12385", "title": "System-driven Interactive Design Support for Cloud Architecture: A Qualitative User Experience Study with Novice Engineers", "authors": ["Ryosuke Kohita", "Akira Kasuga"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Cloud architecture design presents significant challenges due to the\nnecessity of clarifying ambiguous requirements and systematically addressing\ncomplex trade-offs, especially for novice engineers with limited cloud\nexperience. While recent advances in the use of AI tools have broadened\navailable options, system-driven approaches that offer explicit guidance and\nstep-by-step information management may be especially effective in supporting\nnovices during the design process. This study qualitatively examines the\nexperiences of 60 novice engineers using such a system-driven cloud design\nsupport tool. The findings indicate that structured and proactive system\nguidance helps novices engage more effectively in architectural design,\nespecially when addressing tasks where knowledge and experience gaps are most\ncritical. For example, participants found it easier to create initial\narchitectures and did not need to craft prompts themselves. In addition,\nparticipants reported that the ability to simulate and compare multiple\narchitecture options enabled them to deepen their understanding of cloud design\nprinciples and trade-offs, demonstrating the educational value of system-driven\nsupport. The study also identifies areas for improvement, including more\nadaptive information delivery tailored to user expertise, mechanisms for\nvalidating system outputs, and better integration with implementation workflows\nsuch as infrastructure-as-code generation and deployment guidance. Addressing\nthese aspects can further enhance the educational and practical value of\nsystem-driven support tools for cloud architecture design.", "AI": {"tldr": "This study investigates a system-driven support tool for novice engineers in cloud architecture design, highlighting its effective guidance and educational value.", "motivation": "To address challenges faced by novice engineers in cloud architecture design due to ambiguous requirements and complex trade-offs.", "method": "Qualitative examination of the experiences of 60 novice engineers using a system-driven cloud design support tool.", "result": "Participants found that structured guidance improved engagement and understanding of cloud design principles, aiding in the creation of initial architectures without needing to craft prompts.", "conclusion": "Improvements in adaptive information delivery, validation of system outputs, and integration with implementation workflows can enhance the educational and practical value of such support tools.", "key_contributions": ["Identification of the value of structured system guidance for novices", "Demonstration of the educational benefits of simulating and comparing architecture options", "Recommendations for enhancing system-driven support tools based on user experiences"], "limitations": "The study focuses only on novice engineers and may not generalize to experienced practitioners.", "keywords": ["cloud architecture", "system-driven support", "novice engineers", "educational tools", "cloud design principles"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11816", "pdf": "https://arxiv.org/pdf/2508.11816.pdf", "abs": "https://arxiv.org/abs/2508.11816", "title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text.", "AI": {"tldr": "This paper presents a two-stage approach for scientific text simplification using large language models.", "motivation": "The paper addresses the need for effective simplification of scientific texts to improve accessibility and understanding.", "method": "The methodology includes generating structured plans for sentence-level simplification and producing concise summaries for document-level simplification, both guided by LLMs.", "result": "The proposed framework improves coherence and contextual fidelity in simplifications of scientific text.", "conclusion": "Leveraging LLMs in both sentence and document-level processes enhances the quality of scientific text simplification.", "key_contributions": ["Two-stage, LLM-based simplification framework", "Use of structured planning for sentence-level simplification", "Guided document simplification through concise summaries"], "limitations": "", "keywords": ["Text Simplification", "hallucination detection", "LLMs", "CLEF 2025", "SimpleText"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.12388", "pdf": "https://arxiv.org/pdf/2508.12388.pdf", "abs": "https://arxiv.org/abs/2508.12388", "title": "When motivation can be more than a message: designing agents to boost physical activity", "authors": ["Alessandro Silacci", "Maurizio Caon", "Mauro Cherubini"], "categories": ["cs.HC"], "comment": "This is a pre-peer-review version of a paper with the same title\n  accepted at 20th IFIP TC13 International Conference on Human-Computer\n  Interaction (INTERACT 2025)", "summary": "Virtual agents are commonly used in physical activity interventions to\nsupport behavior change, often taking the role of coaches that deliver\nencouragement and feedback. While effective for compliance, this role typically\nlacks relational depth. This pilot study explores how such agents might be\nperceived not just as instructors, but as co-participants: entities that appear\nto exert effort alongside users. Drawing on thematic analysis of\nsemi-structured interviews with 12 participants from a prior physical activity\nintervention, we examine how users interpret and evaluate agent effort in\nsocial comparison contexts. Our findings reveal a recurring tension between\nperceived performance and authenticity. Participants valued social features\nwhen they believed others were genuinely trying. In contrast, ambiguous or\nimplausible activity levels undermined trust and motivation. Many participants\nexpressed skepticism toward virtual agents unless their actions reflected\nvisible effort or were grounded in relatable human benchmarks. Based on these\ninsights, we propose early design directions for fostering co-experienced\nexertion in agents, including behavioral cues, narrative grounding, and\npersonalized performance. These insights contribute to the design of more\nengaging, socially resonant agents capable of supporting co-experienced\nphysical activity.", "AI": {"tldr": "Explores how virtual agents in physical activity interventions can be perceived as co-participants rather than just instructors, emphasizing the importance of visible effort and authenticity in user-agent interactions.", "motivation": "To investigate user perceptions of virtual agents in promoting physical activity and to enhance their relational depth in interventions.", "method": "Thematic analysis of semi-structured interviews with 12 participants from a physical activity intervention.", "result": "Participants valued agents that appeared to exert visible effort and expressed skepticism towards those that did not seem authentic or relatable, leading to insights on user trust and motivation.", "conclusion": "Designing agents that effectively simulate co-experienced exertion can enhance engagement and support in physical activity.", "key_contributions": ["Proposes design directions for virtual agents emphasizing co-experienced exertion", "Identifies the importance of visible effort in fostering trust", "Highlights skepticism towards virtual agent authenticity in performance"], "limitations": "", "keywords": ["virtual agents", "physical activity", "human-computer interaction", "social features", "user trust"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11823", "pdf": "https://arxiv.org/pdf/2508.11823.pdf", "abs": "https://arxiv.org/abs/2508.11823", "title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts.", "AI": {"tldr": "This paper discusses a methodology for detecting and evaluating creative generation and information distortion in scientific text simplification for the CLEF 2025 SimpleText Task 2.", "motivation": "To improve the accuracy of text simplification by detecting spurious generation and information distortion.", "method": "An ensemble framework is constructed using a BERT-based classifier, semantic similarity measures, a natural language inference model, and LLM reasoning, combined with meta-classifiers.", "result": "The integration of multiple strategies enhances the robustness of detecting distortions and spurious generation in simplified texts.", "conclusion": "A grounded generation approach with an LLM-based post-editing system effectively revises simplifications based on original texts.", "key_contributions": ["Development of an ensemble framework for text simplification", "Incorporation of LLM for post-editing of generated texts", "Improvement in detecting hallucinations and distortions in scientific simplification."], "limitations": "The effectiveness of the approach may vary based on the complexity of original texts and the training of the models used.", "keywords": ["Text Simplification", "hallucination detection", "LLMs", "CLEF 2025", "SimpleText"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.12416", "pdf": "https://arxiv.org/pdf/2508.12416.pdf", "abs": "https://arxiv.org/abs/2508.12416", "title": "fCrit: A Visual Explanation System for Furniture Design Creative Support", "authors": ["Vuong Nguyen", "Gabriel Vigliensoni"], "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "We introduce fCrit, a dialogue-based AI system designed to critique furniture\ndesign with a focus on explainability. Grounded in reflective learning and\nformal analysis, fCrit employs a multi-agent architecture informed by a\nstructured design knowledge base. We argue that explainability in the arts\nshould not only make AI reasoning transparent but also adapt to the ways users\nthink and talk about their designs. We demonstrate how fCrit supports this\nprocess by tailoring explanations to users' design language and cognitive\nframing. This work contributes to Human-Centered Explainable AI (HCXAI) in\ncreative practice, advancing domain-specific methods for situated, dialogic,\nand visually grounded AI support.", "AI": {"tldr": "fCrit is an AI system that critiques furniture design, focusing on explainability and user-centric dialogue.", "motivation": "To improve explainability in AI systems used in creative fields, particularly furniture design, by tailoring AI feedback to users' design language and thought processes.", "method": "fCrit employs a multi-agent architecture supported by a structured design knowledge base to generate critiques that align with users' cognitive framing and dialogue.", "result": "The system allows for tailored critiques that enhance understanding and engagement between the AI and users in the furniture design context.", "conclusion": "fCrit advances Human-Centered Explainable AI (HCXAI) in creative practices by providing domain-specific, dialogic, and visually grounded AI support.", "key_contributions": ["Introduction of fCrit, a dialogue-based critique system for furniture design.", "Focus on explainability through adaptation to users' cognitive and design language.", "Advancement of methods in Human-Centered Explainable AI in creative domains."], "limitations": "", "keywords": ["Explainable AI", "Furniture Design", "Human-Centered AI"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.11828", "pdf": "https://arxiv.org/pdf/2508.11828.pdf", "abs": "https://arxiv.org/abs/2508.11828", "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "authors": ["Michael Flor", "Xinyi Liu", "Anna Feldman"], "categories": ["cs.CL"], "comment": "KONVENS 2025. To appear", "summary": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.", "AI": {"tldr": "This survey reviews idiom datasets in psycholinguistics and computational linguistics, analyzing trends and gaps between the two fields.", "motivation": "Idioms are complex phrases challenging to understand and process, necessitating a focused study on available datasets for effective usage in research.", "method": "The survey reviews and analyzes 53 idiom datasets from psycholinguistics and computational linguistics, categorizing them based on content, form, and intended use.", "result": "The analysis identifies trends in annotation practices, coverage, and task framing, revealing a lack of correlation between psycholinguistic and computational approaches to idioms.", "conclusion": "Despite advancements in language coverage and task diversity in recent datasets, integration of insights from psycholinguistics and computational linguistics is still missing.", "key_contributions": ["Comprehensive review of idiom datasets across two fields", "Identification of annotation trends and gaps", "Insights into idiomaticity detection and classification tasks"], "limitations": "Lack of correlation between psycholinguistic and computational research limits the potential for integrating findings.", "keywords": ["idioms", "psycholinguistics", "computational linguistics", "datasets", "idiomaticity"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.12498", "pdf": "https://arxiv.org/pdf/2508.12498.pdf", "abs": "https://arxiv.org/abs/2508.12498", "title": "Say It, See It: A Systematic Evaluation on Speech-Based 3D Content Generation Methods in Augmented Reality", "authors": ["Yanming Xiu", "Joshua Chilukuri", "Shunav Sen", "Maria Gorlatova"], "categories": ["cs.HC"], "comment": "Accepted to ISMAR 2025 UNAI Workshop", "summary": "As augmented reality (AR) applications increasingly require 3D content,\ngenerative pipelines driven by natural input such as speech offer an\nalternative to manual asset creation. In this work, we design a modular,\nedge-assisted architecture that supports both direct text-to-3D and\ntext-image-to-3D pathways, enabling interchangeable integration of\nstate-of-the-art components and systematic comparison of their performance in\nAR settings. Using this architecture, we implement and evaluate four\nrepresentative pipelines through an IRB-approved user study with 11\nparticipants, assessing six perceptual and usability metrics across three\nobject prompts. Overall, text-image-to-3D pipelines deliver higher generation\nquality: the best-performing pipeline, which used FLUX for image generation and\nTrellis for 3D generation, achieved an average satisfaction score of 4.55 out\nof 5 and an intent alignment score of 4.82 out of 5. In contrast, direct\ntext-to-3D pipelines excel in speed, with the fastest, Shap-E, completing\ngeneration in about 20 seconds. Our results suggest that perceptual quality has\na greater impact on user satisfaction than latency, with users tolerating\nlonger generation times when output quality aligns with expectations. We\ncomplement subjective ratings with system-level metrics and visual analysis,\nproviding practical insights into the trade-offs of current 3D generation\nmethods for real-world AR deployment.", "AI": {"tldr": "This paper presents a modular edge-assisted architecture for generating 3D content from text and images for augmented reality applications, evaluating user satisfaction and performance trade-offs in 3D generation methods.", "motivation": "To improve the creation of 3D content for AR applications by leveraging generative pipelines driven by natural input, such as speech, instead of manual asset creation.", "method": "The authors designed a modular architecture that supports both text-to-3D and text-image-to-3D pathways and implemented four pipelines. They conducted an IRB-approved user study with 11 participants to assess various perceptual and usability metrics across different object prompts.", "result": "Text-image-to-3D pipelines provided higher generation quality, achieving an average satisfaction score of 4.55/5. The best pipeline combined FLUX for image generation and Trellis for 3D generation. Direct text-to-3D pipelines, while faster, showed lower user satisfaction due to perceived quality deficits.", "conclusion": "User satisfaction is more influenced by perceptual quality than generation speed; users tolerate longer generation times if the output quality meets their expectations. This has implications for future AR applications and the trade-offs necessary in current 3D generation methods.", "key_contributions": ["Modular architecture for text and image to 3D generation", "User study evaluating perceptual and usability metrics", "Insights into trade-offs between quality and generation speed in AR"], "limitations": "The study was limited to a small number of participants and specific object prompts, which may affect the generalizability of findings.", "keywords": ["augmented reality", "3D content generation", "text-to-3D", "user study", "generative pipelines"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.11829", "pdf": "https://arxiv.org/pdf/2508.11829.pdf", "abs": "https://arxiv.org/abs/2508.11829", "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "authors": ["Leigh Levinson", "Christopher J. Agostino"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track", "summary": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.", "AI": {"tldr": "The paper proposes a framework that integrates hormonal cycles into Large Language Models (LLMs) to enhance context relevance in AI systems, demonstrating performance variations in line with biological rhythms.", "motivation": "AI systems face challenges in determining contextually relevant information due to the frame problem; this work explores biological rhythms as filters for relevance.", "method": "The framework embeds menstrual and circadian cycles into LLMs using prompts generated from periodic functions representing key hormones.", "result": "Linguistic analysis showed emotional and stylistic variations corresponding to biological phases, with performance benchmarks revealing consistent variations aligning with hormonal cycles.", "conclusion": "This method not only improves contextual AI but also highlights how societal biases related to gender and biology manifest in language models.", "key_contributions": ["A framework integrating biological rhythms into LLMs to enhance relevance filtering.", "Empirical evidence of emotional variations in language correlating with hormonal cycles.", "Insights into gender biases embedded in AI systems through language."], "limitations": "The focus on hormonal cycles may not encompass the full complexity of human emotional and cognitive variability; further research is needed.", "keywords": ["human-computer interaction", "large language models", "biological rhythms", "societal biases", "contextual AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.12504", "pdf": "https://arxiv.org/pdf/2508.12504.pdf", "abs": "https://arxiv.org/abs/2508.12504", "title": "Organization Matters: A Qualitative Study of Organizational Dynamics in Red Teaming Practices For Generative AI", "authors": ["Bixuan Ren", "EunJeong Cheon", "Jianghui Li"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid integration of generative artificial intelligence (GenAI) across\ndiverse fields underscores the critical need for red teaming efforts to\nproactively identify and mitigate associated risks. While previous research\nprimarily addresses technical aspects, this paper highlights organizational\nfactors that hinder the effectiveness of red teaming in real-world settings.\nThrough qualitative analysis of 15 semi-structured interviews with red teamers\nfrom various organizations, we uncover challenges such as the marginalization\nof vulnerable red teamers, the invisibility of nuanced AI risks to vulnerable\nusers until post-deployment, and a lack of user-centered red teaming\napproaches. These issues often arise from underlying organizational dynamics,\nincluding organizational resistance, organizational inertia, and organizational\nmediocracy. To mitigate these dynamics, we discuss the implications of user\nresearch for red teaming and the importance of embedding red teaming throughout\nthe entire development cycle of GenAI systems.", "AI": {"tldr": "This paper examines the organizational factors affecting the effectiveness of red teaming in generative AI, highlighting challenges and proposing user-centered approaches.", "motivation": "To address the critical need for effective red teaming in the face of generative AI risks, particularly focusing on organizational dynamics that hinder these efforts.", "method": "Qualitative analysis based on 15 semi-structured interviews with red teamers from various organizations.", "result": "Identified challenges include marginalization of vulnerable red teamers, visibility issues of AI risks for users until post-deployment, and a deficiency in user-centered approaches to red teaming.", "conclusion": "Embedding red teaming throughout the development cycle of GenAI systems can help mitigate organizational dynamics that impair its effectiveness.", "key_contributions": ["Identification of organizational dynamics affecting red teaming effectiveness", "Emphasis on the need for user-centered red teaming approaches", "Insights from interviews with red teamers that highlight real-world challenges."], "limitations": "", "keywords": ["Generative AI", "Red teaming", "Organizational dynamics", "User-centered design", "AI risk mitigation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11831", "pdf": "https://arxiv.org/pdf/2508.11831.pdf", "abs": "https://arxiv.org/abs/2508.11831", "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "authors": ["Julia Sammartino", "Libby Barak", "Jing Peng", "Anna Feldman"], "categories": ["cs.CL", "cs.AI"], "comment": "RANLP 2025", "summary": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved.", "AI": {"tldr": "The paper explores the impact of cross-lingual transfer through sequential fine-tuning on euphemism detection in five languages, demonstrating improved performance in low-resource languages like Yoruba and Turkish.", "motivation": "Euphemisms vary culturally and are ambiguous, which complicates euphemism detection for language models, particularly in low-resource settings.", "method": "We compare sequential fine-tuning with monolingual and simultaneous fine-tuning techniques on models XLM-R and mBERT across languages including English, Spanish, Chinese, Turkish, and Yoruba.", "result": "Sequential fine-tuning with a high-resource language improves detection performance in low-resource languages. XLM-R shows larger improvements but is sensitive to pretraining gaps, while mBERT offers stable results at lower performance.", "conclusion": "Sequential fine-tuning is an effective method for enhancing euphemism detection in multilingual models, especially beneficial for low-resource languages.", "key_contributions": ["Sequential fine-tuning improves euphemism detection across multiple languages.", "Analysis of language pairings and typological features affecting performance.", "Comparison between XLM-R and mBERT in the context of euphemism detection."], "limitations": "The results are sensitive to pretraining gaps and model selection may affect generalizability.", "keywords": ["euphemism detection", "cross-lingual transfer", "sequential fine-tuning", "low-resource languages", "multilingual models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.12518", "pdf": "https://arxiv.org/pdf/2508.12518.pdf", "abs": "https://arxiv.org/abs/2508.12518", "title": "Towards Adaptive External Communication in Autonomous Vehicles: A Conceptual Design Framework", "authors": ["Tram Thi Minh Tran", "Judy Kay", "Stewart Worrall", "Marius Hoggenmueller", "Callum Parker", "Xinyan Yu", "Julie Stephany Berrio Perez", "Mao Shan", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "External Human-Machine Interfaces (eHMIs) are key to facilitating interaction\nbetween autonomous vehicles and external road actors, yet most remain reactive\nand do not account for scalability and inclusivity. This paper introduces a\nconceptual design framework for adaptive eHMIs-interfaces that dynamically\nadjust communication as road actors vary and context shifts. Using the\ncyber-physical system as a structuring lens, the framework comprises three\nlayers: Input (what the system detects), Processing (how the system decides),\nand Output (how the system communicates). Developed through theory-led\nabstraction and expert discussion, the framework helps researchers and\ndesigners think systematically about adaptive eHMIs and provides a structured\ntool to design, analyse, and assess adaptive communication strategies. We show\nhow such systems may resolve longstanding limitations in eHMI research while\nraising new ethical and technical considerations.", "AI": {"tldr": "This paper presents a conceptual design framework for adaptive external human-machine interfaces (eHMIs) that improve communication between autonomous vehicles and road users by dynamically adjusting to context and user variability.", "motivation": "To enhance interaction between autonomous vehicles and external road actors, adapting to the needs of varying contexts and promoting inclusivity.", "method": "A conceptual design framework with three layers: Input, Processing, and Output, developed through theory-led abstraction and expert discussions.", "result": "The framework provides a systematic approach to designing and analyzing adaptive eHMIs, potentially addressing limitations in current eHMI research.", "conclusion": "The proposed framework can help in creating more effective eHMIs while also introducing new ethical and technical considerations.", "key_contributions": ["Introduction of a structured framework for adaptive eHMIs", "Focus on inclusivity and scalability in vehicle communication", "Addressing long-standing limitations in existing eHMI research"], "limitations": "The framework is conceptual and requires empirical validation in real-world applications.", "keywords": ["external human-machine interfaces", "autonomous vehicles", "adaptive communication"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.11857", "pdf": "https://arxiv.org/pdf/2508.11857.pdf", "abs": "https://arxiv.org/abs/2508.11857", "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "authors": ["Andrei-Valentin TÄnase", "Elena Pelican"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.", "AI": {"tldr": "SupraTok is a novel tokenization architecture that improves efficiency in natural language processing by learning semantic multi-word units and optimizing training data for better performance.", "motivation": "Despite advancements in model architectures, tokenization remains a bottleneck in natural language processing, necessitating innovative strategies for improvement.", "method": "SupraTok introduces cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning to learn 'superword' tokens and enhance tokenization efficiency.", "result": "SupraTok achieves a 31% improvement in English tokenization efficiency and outperforms existing tokenizers across 38 languages, with significant gains in specific NLP benchmarks when integrated with a GPT-2 model.", "conclusion": "Efficient tokenization is crucial and can enhance the performance of language models, suggesting a promising avenue for future research.", "key_contributions": ["Introduction of 'superword' tokens for improved semantic representation", "Improvements in tokenization efficiency surpassing existing tokenizers", "Competitive performance across 38 languages with a single model integration"], "limitations": "Further validation at larger model scales is necessary to fully understand the efficacy of the tokenization approach.", "keywords": ["Tokenization", "Natural Language Processing", "Semantic Units", "Machine Learning", "Language Model Performance"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.12579", "pdf": "https://arxiv.org/pdf/2508.12579.pdf", "abs": "https://arxiv.org/abs/2508.12579", "title": "The Future of Tech Labor: How Workers are Organizing and Transforming the Computing Industry", "authors": ["Cella M. Sum", "Anna Konvicka", "Mona Wang", "Sarah E. Fox"], "categories": ["cs.HC"], "comment": null, "summary": "The tech industry's shifting landscape and the growing precarity of its labor\nforce have spurred unionization efforts among tech workers. These workers turn\nto collective action to improve their working conditions and to protest\nunethical practices within their workplaces. To better understand this\nmovement, we interviewed 44 U.S.-based tech worker-organizers to examine their\nmotivations, strategies, challenges, and future visions for labor organizing.\nThese workers included engineers, product managers, customer support\nspecialists, QA analysts, logistics workers, gig workers, and union staff\norganizers. Our findings reveal that, contrary to popular narratives of\nprestige and privilege within the tech industry, tech workers face fragmented\nand unstable work environments which contribute to their disempowerment and\nhinder their organizing efforts. Despite these difficulties, organizers are\nlaying the groundwork for a more resilient tech worker movement through\ncommunity building and expanding political consciousness. By situating these\ndynamics within broader structural and ideological forces, we identify ways for\nthe CSCW community to build solidarity with tech workers who are materially\ntransforming our field through their organizing efforts.", "AI": {"tldr": "This study explores the motivations and challenges faced by U.S.-based tech worker-organizers in their unionization efforts, highlighting the fragmented and unstable work environments that tech workers navigate.", "motivation": "To understand the unionization efforts among tech workers and improve their working conditions against unethical practices.", "method": "Interviews with 44 U.S.-based tech worker-organizers from various job roles to examine their motivations, strategies, challenges, and visions for labor organizing.", "result": "Findings indicate that tech workers often encounter disempowerment due to precarious work environments, which complicate their organizing efforts. However, they are fostering a resilient community and political awareness to push for labor changes.", "conclusion": "The study identifies essential structural and ideological forces affecting tech workers and suggests ways for the CSCW community to support their labor organizing efforts.", "key_contributions": ["Insights into the motivations and challenges of tech worker organizers.", "Identification of community building as a critical aspect of labor organizing.", "Suggestions for supporting tech workers in the CSCW field."], "limitations": "", "keywords": ["unionization", "tech workers", "collective action", "worker organizing", "CSCW"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2508.11889", "pdf": "https://arxiv.org/pdf/2508.11889.pdf", "abs": "https://arxiv.org/abs/2508.11889", "title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "authors": ["Hui Ma", "Bo Zhang", "Jinpeng Hu", "Zenglin Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines.", "AI": {"tldr": "InitERC is a novel one-stage in-context instruction tuning framework for emotion recognition in conversation, enhancing alignment between speaker characteristics and conversational context.", "motivation": "To improve emotion recognition in conversation by addressing the limitations of existing multi-stage instruction tuning methods in aligning speaker identity, contextual cues, and emotion states.", "method": "InitERC employs in-context instruction tuning with components such as demonstration pool construction, example selection, prompt design, and instruction tuning.", "result": "InitERC shows substantial performance improvements over state-of-the-art baselines in emotion recognition on three major datasets.", "conclusion": "The research validates that a one-stage in-context instruction tuning approach can effectively enhance ERC by better aligning speaker characteristics and emotional contexts.", "key_contributions": ["Proposed InitERC framework for ERC using in-context instruction tuning", "Demonstrated effectiveness through experiments on multiple datasets", "Investigated the impact of retrieval strategy, example ordering, and number of examples"], "limitations": "", "keywords": ["Emotion Recognition", "Conversational AI", "Instruction Tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.13047", "pdf": "https://arxiv.org/pdf/2508.13047.pdf", "abs": "https://arxiv.org/abs/2508.13047", "title": "Using AI for User Representation: An Analysis of 83 Persona Prompts", "authors": ["Joni Salminen", "Danial Amin", "Bernard Jansen"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at AICCSA-2025", "summary": "We analyzed 83 persona prompts from 27 research articles that used large\nlanguage models (LLMs) to generate user personas. Findings show that the\nprompts predominantly generate single personas. Several prompts express a\ndesire for short or concise persona descriptions, which deviates from the\ntradition of creating rich, informative, and rounded persona profiles. Text is\nthe most common format for generated persona attributes, followed by numbers.\nText and numbers are often generated together, and demographic attributes are\nincluded in nearly all generated personas. Researchers use up to 12 prompts in\na single study, though most research uses a small number of prompts. Comparison\nand testing multiple LLMs is rare. More than half of the prompts require the\npersona output in a structured format, such as JSON, and 74% of the prompts\ninsert data or dynamic variables. We discuss the implications of increased use\nof computational personas for user representation.", "AI": {"tldr": "Analysis of prompts used to generate user personas with LLMs, revealing trends and implications for user representation.", "motivation": "To understand how large language models are utilized in generating user personas across multiple studies and the impact of these methods on user representation.", "method": "Analyzed 83 persona prompts from 27 research articles focusing on the generation of user personas using LLMs.", "result": "Findings indicate that the majority of generated personas are single and often concise; while demographic attributes are consistently included, rich descriptions are less common.", "conclusion": "The findings suggest a trend towards basic persona generation which may limit the depth of user representation in research; implications for designing personas in HCI are discussed.", "key_contributions": ["Identification of common trends in persona prompt usage with LLMs", "Highlighting the potential limitations of generated personas", "Discussion on implications for user representation in HCI"], "limitations": "Limited by the scope of analyzed research articles and prompts; potential for unexamined biases in LLM-generated content.", "keywords": ["user personas", "large language models", "HCI", "persona generation", "user representation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11915", "pdf": "https://arxiv.org/pdf/2508.11915.pdf", "abs": "https://arxiv.org/abs/2508.11915", "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "authors": ["Punya Syon Pandey", "Yongjin Yang", "Jiarui Liu", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.", "AI": {"tldr": "Introduction of the Conversational Robustness Evaluation Score (CORE) metric to assess language use effectiveness in multi-agent LLM interactions.", "motivation": "To quantify the linguistic diversity in game-theoretic interactions among agents utilizing Large Language Models.", "method": "CORE combines measures of cluster entropy, lexical repetition, and semantic similarity to evaluate dialog quality in LLM systems during competitive, cooperative, and neutral interactions.", "result": "Cooperative settings show higher cluster entropy and vocabulary growth compared to competitive settings, suggesting that social incentives significantly affect language adaptation in LLM interactions.", "conclusion": "CORE serves as an effective tool for diagnosing linguistic robustness, offering insights into how social dynamics influence language usage among agents in multi-agent systems.", "key_contributions": ["Introduction of the CORE metric for evaluating dialog quality in LLMs.", "Demonstration of linguistic differences in dialog across game-theoretic environments.", "Analysis linking social incentives with language adaptation through statistical measures."], "limitations": "", "keywords": ["Large Language Models", "Conversational Robustness Evaluation", "multi-agent systems", "game-theoretic interactions", "linguistic diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.13074", "pdf": "https://arxiv.org/pdf/2508.13074.pdf", "abs": "https://arxiv.org/abs/2508.13074", "title": "Ashes or Breath: Exploring Moral Dilemmas of Life and Cultural Legacy through Mixed Reality Gaming", "authors": ["Black Sun", "Ge Kacy Fu", "Shichao Guo"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional approaches to teaching moral dilemmas often rely on abstract,\ndisembodied scenarios that limit emotional engagement and reflective depth. To\naddress this gap, we developed \\textit{Ashes or Breath}, a Mixed Reality game\ndelivered via head-mounted displays(MR-HMDs). This places players in an ethical\ncrisis: they must save a living cat or a priceless cultural artifact during a\nmuseum fire. Designed through an iterative, values-centered process, the\nexperience leverages embodied interaction and spatial immersion to heighten\nemotional stakes and provoke ethical reflection. Players face irreversible,\nemotionally charged choices followed by narrative consequences in a reflective\nroom, exploring diverse perspectives and societal implications. Preliminary\nevaluations suggest that embedding moral dilemmas into everyday environments\nvia MR-HMDs intensifies empathy, deepens introspection, and encourages users to\nreconsider their moral assumptions. This work contributes to ethics-based\nexperiential learning in HCI, positioning augmented reality not merely as a\nmedium of interaction but as a stage for ethical encounter.", "AI": {"tldr": "The paper presents a Mixed Reality game called 'Ashes or Breath' that immerses players in moral dilemmas to enhance ethical reflection and empathy.", "motivation": "To address limitations of traditional teaching methods for moral dilemmas, which often lack emotional engagement and depth.", "method": "Developed a Mixed Reality game utilizing head-mounted displays that places players in ethical crises, designed through an iterative values-centered process.", "result": "Preliminary evaluations indicate the game intensifies empathy, deepens introspection, and prompts users to reconsider moral assumptions in immersive settings.", "conclusion": "Embedding moral dilemmas into everyday environments via MR-HMDs provides a powerful framework for ethics-based experiential learning in HCI.", "key_contributions": ["Introduced a Mixed Reality game for ethical training", "Enhanced moral engagement through embodied interaction", "Demonstrated the use of MR for ethical decision-making"], "limitations": "", "keywords": ["Mixed Reality", "Human-Computer Interaction", "ethical dilemmas", "immersive learning", "empathy"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11927", "pdf": "https://arxiv.org/pdf/2508.11927.pdf", "abs": "https://arxiv.org/abs/2508.11927", "title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "authors": ["Jie Lu", "Du Jin", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI.", "AI": {"tldr": "This paper presents a template-based Natural Language Inference (NLI) dataset for Chinese and Japanese, focusing on the perfect aspect and temporal inference challenges faced by LLMs.", "motivation": "To address the lack of distinct grammatical forms for tense in the perfect aspect of Chinese and Japanese, which complicates Natural Language Inference (NLI).", "method": "A linguistically motivated, template-based NLI dataset was constructed, consisting of 1,350 pairs per language, assessing temporal inference capabilities of LLMs.", "result": "Experiments show that even advanced LLMs have difficulty with temporal inference, especially in identifying tense and reference-time shifts.", "conclusion": "The findings emphasize the limitations of current models and the importance of cross-linguistic evaluation within temporal semantics.", "key_contributions": ["Creation of a NLI dataset for Chinese and Japanese focused on the perfect aspect.", "Demonstration of LLMs' challenges in temporal inference.", "Highlighting the need for linguistic diversity in NLI evaluation."], "limitations": "The dataset is limited to the perfect aspect and may not cover all temporal inference scenarios.", "keywords": ["Natural Language Inference", "temporal semantics", "cross-linguistic evaluation"], "importance_score": 4, "read_time_minutes": 9}}
{"id": "2508.13095", "pdf": "https://arxiv.org/pdf/2508.13095.pdf", "abs": "https://arxiv.org/abs/2508.13095", "title": "At the Speed of the Heart: Evaluating Physiologically-Adaptive Visualizations for Supporting Engagement in Biking Exergaming in Virtual Reality", "authors": ["Oliver Hein", "Sandra Wackerl", "Changkun Ou", "Florian Alt", "Francesco Chiossi"], "categories": ["cs.HC"], "comment": null, "summary": "Many exergames face challenges in keeping users within safe and effective\nintensity levels during exercise. Meanwhile, although wearable devices\ncontinuously collect physiological data, this information is seldom leveraged\nfor real-time adaptation or to encourage user reflection. We designed and\nevaluated a VR cycling simulator that dynamically adapts based on users' heart\nrate zones. First, we conducted a user study (N=50) comparing eight\nvisualization designs to enhance engagement and exertion control, finding that\ngamified elements like non-player characters (NPCs) were promising for feedback\ndelivery. Based on these findings, we implemented a physiology-adaptive\nexergame that adjusts visual feedback to keep users within their target heart\nrate zones. A lab study (N=18) showed that our system has potential to help\nusers maintain their target heart rate zones. Subjective ratings of exertion,\nenjoyment, and motivation remained largely unchanged between conditions. Our\nfindings suggest that real-time physiological adaptation through NPC\nvisualizations can improve workout regulation in exergaming.", "AI": {"tldr": "A VR cycling simulator adapts based on users' heart rate zones to enhance engagement and control during exercise.", "motivation": "To address challenges in maintaining safe and effective intensity levels in exergames and leverage wearable physiological data for better user experiences.", "method": "A user study with 50 participants compared eight visualization designs, followed by a lab study with 18 participants evaluating a physiology-adaptive exergame that adjusts visual feedback based on real-time heart rate data.", "result": "Gamified elements, like NPCs, showed promise in enhancing engagement and exertion control. The system helped users maintain target heart rate zones without significant changes in exertion, enjoyment, or motivation ratings.", "conclusion": "Real-time physiological adaptation through NPC visualizations can improve workout regulation in exergaming, potentially making exercise more effective and engaging.", "key_contributions": ["Development of a VR cycling simulator that adapts to heart rate zones", "User studies evaluating visualization designs for engagement and control", "Demonstrated potential for improving workout regulation during exergaming"], "limitations": "Limited to a specific VR environment and small sample sizes in user studies.", "keywords": ["exergames", "wearable devices", "physiological adaptation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.11933", "pdf": "https://arxiv.org/pdf/2508.11933.pdf", "abs": "https://arxiv.org/abs/2508.11933", "title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "authors": ["Yue Wang", "Liesheng Wei", "Yuxiang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques.", "AI": {"tldr": "CAMF is a novel framework for detecting machine-generated text using multiple LLM agents.", "motivation": "Address the urgent need for effective detection of machine-generated text amidst risks like disinformation and threats to academic integrity.", "method": "Introduced the Collaborative Adversarial Multi-agent Framework (CAMF) that utilizes multiple LLM-based agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation.", "result": "CAMF significantly outperforms existing zero-shot machine-generated text detection techniques in empirical evaluations.", "conclusion": "The structured collaborative-adversarial approach of CAMF allows for a nuanced analysis of textual inconsistencies suggesting non-human authorship.", "key_contributions": ["Introduction of CAMF framework for MGT detection", "Three-phase process enhancing detection accuracy", "Empirical evidence of superiority over state-of-the-art methods"], "limitations": "", "keywords": ["Machine-generated text", "Detection", "Large Language Models", "Adversarial frameworks", "Linguistic dimensions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.13116", "pdf": "https://arxiv.org/pdf/2508.13116.pdf", "abs": "https://arxiv.org/abs/2508.13116", "title": "Choosing the Right Engine in the Virtual Reality Landscape", "authors": ["Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Pre-print", "summary": "Virtual reality (VR) development relies on game engines to provide real-time\nrendering, physics simulation, and interaction systems. Among the most widely\nused game engines, Unreal Engine and Unity dominate the industry, offering\ndistinct advantages in graphics rendering, performance optimization, usability,\nresource requirements, and scalability. This study presents a comprehensive\ncomparative analysis of both engines, evaluating their capabilities and\ntrade-offs through empirical assessments and real-world case studies of\nlarge-scale VR projects. The findings highlight key factors such as rendering\nfidelity, computational efficiency, cross-platform compatibility, and\ndevelopment workflows. These provide practical insights for selecting the most\nsuitable engine based on project-specific needs. Furthermore, emerging trends\nin artificial intelligence (AI)-driven enhancements, including Deep Learning\nSuper Sampling (DLSS) and large language models (LLMs), are explored to assess\ntheir impact on VR development workflows. By aligning engine capabilities with\ntechnical and creative requirements, developers can overcome performance\nbottlenecks, enhance immersion, and streamline optimization techniques.\n  This study serves as a valuable resource for VR developers, researchers, and\nindustry professionals, offering data-driven recommendations to navigate the\nevolving landscape of VR technology.", "AI": {"tldr": "A comparative analysis of Unreal Engine and Unity for VR development, assessing their capabilities and impact of AI enhancements.", "motivation": "To evaluate and compare the capabilities and trade-offs of Unreal Engine and Unity in the context of VR development, providing insights for developers.", "method": "Empirical assessments and real-world case studies of large-scale VR projects, analyzing rendering fidelity, computational efficiency, and development workflows.", "result": "Findings reveal key factors for choosing between the engines, including performance optimization and cross-platform compatibility, alongside the influence of AI-driven enhancements.", "conclusion": "Developers can optimize their VR projects by selecting the right engine based on technical requirements and leveraging AI technologies.", "key_contributions": ["Comprehensive comparative analysis of Unreal Engine and Unity.", "Insights into AI-driven enhancements in VR development.", "Practical recommendations for VR developers based on empirical data."], "limitations": "", "keywords": ["Virtual Reality", "Game Engines", "Unreal Engine", "Unity", "AI Enhancements"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.12031", "pdf": "https://arxiv.org/pdf/2508.12031.pdf", "abs": "https://arxiv.org/abs/2508.12031", "title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "authors": ["Shaozhe Yin", "Jinyu Guo", "Kai Shuang", "Xia Liu", "Ruize Ou"], "categories": ["cs.CL"], "comment": null, "summary": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases.", "AI": {"tldr": "This paper presents an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in Continual Relation Extraction (CRE), addressing cognitive biases and improving performance.", "motivation": "To continuously learn new relations in CRE while avoiding catastrophic forgetting and better address cognitive biases by focusing on error cases.", "method": "An instruction-based continual contrastive tuning approach that separates training and memory data based on response correctness and employs dual-task fine-tuning.", "result": "The proposed model achieves new state-of-the-art performance in CRE on TACRED and FewRel datasets, highlighting the impact of focusing on error cases.", "conclusion": "This method effectively mitigates the cognitive bias gap between old and new relations in LLMs, improving their continual learning capabilities.", "key_contributions": ["Introduction of an instruction-based continual contrastive tuning approach for LLMs in CRE.", "Dual-task fine-tuning strategy that separates data based on correctness of responses.", "Achieving state-of-the-art results on TACRED and FewRel datasets."], "limitations": "", "keywords": ["Continual Relation Extraction", "Large Language Models", "Cognitive Bias", "Contrastive Learning", "Instruction Tuning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.13138", "pdf": "https://arxiv.org/pdf/2508.13138.pdf", "abs": "https://arxiv.org/abs/2508.13138", "title": "Human Digital Twin: Data, Models, Applications, and Challenges", "authors": ["Rong Pan", "Hongyue Sun", "Xiaoyu Chen", "Giulia Pedrielli", "Jiapeng Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Human digital twins (HDTs) are dynamic, data-driven virtual representations\nof individuals, continuously updated with multimodal data to simulate, monitor,\nand predict health trajectories. By integrating clinical, physiological,\nbehavioral, and environmental inputs, HDTs enable personalized diagnostics,\ntreatment planning, and anomaly detection. This paper reviews current\napproaches to HDT modeling, with a focus on statistical and machine learning\ntechniques, including recent advances in anomaly detection and failure\nprediction. It also discusses data integration, computational methods, and\nethical, technological, and regulatory challenges in deploying HDTs for\nprecision healthcare.", "AI": {"tldr": "This paper reviews current approaches to Human Digital Twins (HDTs) in healthcare, emphasizing statistical and machine learning techniques, data integration, and ethical challenges.", "motivation": "The motivation behind this paper is to explore how Human Digital Twins (HDTs) can enhance personalized healthcare through the integration of various data types for accurate health predictions.", "method": "The paper reviews existing literature and methodologies related to HDT modeling, focusing on statistical methods, machine learning techniques for anomaly detection, and computational approaches for data integration.", "result": "The review highlights the capabilities of HDTs in diagnosing health conditions, planning treatments, and predicting health trajectories, as well as identifies recent advances in related technologies.", "conclusion": "HDTs present significant potential for precision healthcare, but their deployment is hampered by ethical, technological, and regulatory challenges that must be addressed.", "key_contributions": ["Overview of HDT modeling techniques", "Analysis of machine learning applications in health monitoring", "Discussion of ethical and regulatory issues in HDT deployment"], "limitations": "Limited exploration of specific case studies in real-world applications.", "keywords": ["Human Digital Twins", "machine learning", "healthcare", "anomaly detection", "precision medicine"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12040", "pdf": "https://arxiv.org/pdf/2508.12040.pdf", "abs": "https://arxiv.org/abs/2508.12040", "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "authors": ["Jinyi Han", "Tingyun Li", "Shisong Chen", "Jie Shi", "Xinyi Wang", "Guanglei Yue", "Jiaqing Liang", "Xin Lin", "Liqian Wen", "Zulong Chen", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "The initial versin was made in August 2024", "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.", "AI": {"tldr": "Introducing FineCE, a novel method for accurate and fine-grained confidence estimation for LLMs during text generation.", "motivation": "Improve the trustworthiness of LLM outputs by providing fine-grained confidence scores instead of coarse-grained estimates.", "method": "Develop a comprehensive data pipeline for training and employ a Backward Confidence Integration (BCI) strategy to enhance confidence scores during inference.", "result": "FineCE outperforms existing classical confidence estimation methods across multiple benchmark datasets.", "conclusion": "FineCE enhances the reliability of LLM-generated outputs by providing accurate confidence scores throughout the generation process.", "key_contributions": ["Development of FineCE method for confidence estimation", "Introduction of Backward Confidence Integration (BCI) strategy", "Three strategies for optimal confidence estimation position identification"], "limitations": "", "keywords": ["large language models", "confidence estimation", "Backpropagation Confidence Integration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12086", "pdf": "https://arxiv.org/pdf/2508.12086.pdf", "abs": "https://arxiv.org/abs/2508.12086", "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "authors": ["Yao Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "comment": "9 pages, 3 tables, 1 algorithm", "summary": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning.", "AI": {"tldr": "The paper introduces J6, a Jacobian-based method for optimizing multiple objectives in large language model adaptation, addressing the balance between factuality and confidence while providing insight into parameter interactions.", "motivation": "A need to balance factuality and confidence in large language model adaptation due to the complex interactions of prompt parameters.", "method": "The J6 method decomposes the gradient interaction matrix into six components to facilitate both hard and soft decision-making in prompt parameter updates.", "result": "J6 enables effective conflict-aware prompt optimization, allowing for better handling of geometric structures in multi-objective neural tuning.", "conclusion": "The proposed J6 method not only enhances optimization in LLM adaptation but also provides interpretability and opens new avenues for structured reasoning in neural tuning.", "key_contributions": ["Introduces a structured Jacobian-based approach to LLM adaptation.", "Decomposes gradient interaction into interpretable components.", "Provides both hard and soft strategies for multi-objective optimization."], "limitations": "", "keywords": ["large language models", "multi-objective optimization", "Jacobian decomposition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.12096", "pdf": "https://arxiv.org/pdf/2508.12096.pdf", "abs": "https://arxiv.org/abs/2508.12096", "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "authors": ["Haiquan Hu", "Jiazhi Jiang", "Shiyou Xu", "Ruhan Zeng", "Tian Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submit to AAAI 2026", "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.", "AI": {"tldr": "The paper introduces the Structured Transition Evaluation Method (STEM) for evaluating large language models (LLMs) in a lightweight and interpretable manner, highlighting its effectiveness in estimating model capabilities through significant transition samples.", "motivation": "The motivation is to address the challenges in evaluating LLMs due to rapid advancements, overfitting to benchmarks, and high evaluation costs, making it difficult to differentiate model performance meaningfully.", "method": "The proposed method, STEM, identifies significant transition samples among LLMs of the same architecture but varying parameter scales to estimate the relative capabilities of unknown models efficiently.", "result": "Experimental results demonstrate that STEM reliably captures performance trends and aligns with the ground-truth rankings of model capabilities on diverse benchmarks.", "conclusion": "STEM is presented as a practical and scalable method for architecture-agnostic evaluation of LLMs, enhancing the evaluation process amidst growing model complexity.", "key_contributions": ["Introduction of the Structured Transition Evaluation Method (STEM) for LLM assessment", "Identification of significant transition samples for efficient performance estimation", "Demonstration of STEM's effectiveness across diverse benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Evaluation", "Machine Learning", "Performance Measurement", "Benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12140", "pdf": "https://arxiv.org/pdf/2508.12140.pdf", "abs": "https://arxiv.org/abs/2508.12140", "title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "authors": ["Ziqian Bi", "Lu Chen", "Junhao Song", "Hongying Luo", "Enze Ge", "Junmin Huang", "Tianyang Wang", "Keyu Chen", "Chia Xin Liang", "Zihan Wei", "Huafeng Liu", "Chunjie Tian", "Jibin Guan", "Joe Yeong", "Yongzhi Xu", "Peng Wang", "Junfeng Hao"], "categories": ["cs.CL"], "comment": null, "summary": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment.", "AI": {"tldr": "This study evaluates thinking budget mechanisms in medical reasoning, revealing scaling laws between computational resources and reasoning quality across model families Qwen3 and DeepSeek-R1.", "motivation": "To understand how computational resources influence the quality of reasoning in medical AI systems, and to identify optimal resource allocation strategies for different clinical tasks.", "method": "Comprehensive evaluation of Qwen3 and DeepSeek-R1 models across 15 medical datasets with varying thinking budgets, conducting controlled experiments on model performance at different token limits.", "result": "Logarithmic scaling relationships were established, showing predictability in accuracy improvements based on model size and thinking budget, with distinct efficiency regimes identified for real-time applications and critical diagnostics.", "conclusion": "Thinking budget control is essential for optimizing AI systems in healthcare, allowing for efficient resource allocation while ensuring transparency in deployment.", "key_contributions": ["Identification of efficiency regimes in medical reasoning", "Establishment of scaling laws for reasoning quality vs computational resources", "Insights into domain-specific reasoning requirements in medicine."], "limitations": "", "keywords": ["thinking budget", "medical reasoning", "scaling laws", "AI systems", "resource allocation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12158", "pdf": "https://arxiv.org/pdf/2508.12158.pdf", "abs": "https://arxiv.org/abs/2508.12158", "title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures, 4 tables. Accepted to HAIPS @ CCS 2025", "summary": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.", "AI": {"tldr": "The paper explores using LLMs as privacy evaluators in NLP, demonstrating their potential to model human perspectives on privacy despite challenges in empirical measurement.", "motivation": "The need for accurate evaluation of privacy in NLP, which remains a significant challenge despite advancements in privacy-preserving techniques.", "method": "Utilized a study involving 10 datasets, 13 LLMs, and 677 human participants to assess the capability of LLMs as a judge of privacy sensitivity in textual data.", "result": "LLMs can model a global human perspective on privacy, although empirical measurement of privacy remains difficult, with low inter-human agreement on privacy sensitivity.", "conclusion": "The findings support the potential of LLMs as effective privacy evaluators, while also highlighting the complexities of quantifying privacy.", "key_contributions": ["Proposing LLMs as evaluators for privacy sensitivity in textual data", "Empirical study comparing LLM evaluations to human perceptions of privacy", "Identifying challenges in measuring privacy and inter-human agreement rates"], "limitations": "General low inter-human agreement rates indicate the subjectivity of privacy measurement.", "keywords": ["privacy-preserving NLP", "LLM-as-a-Judge", "privacy evaluation", "human-computer interaction", "textual data"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12227", "pdf": "https://arxiv.org/pdf/2508.12227.pdf", "abs": "https://arxiv.org/abs/2508.12227", "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field.", "AI": {"tldr": "This paper provides a comprehensive survey of Arabic multimodal machine learning (MML), categorizing existing research into datasets, applications, approaches, and challenges, and highlighting research gaps.", "motivation": "To explore the recent developments and state of Arabic MML given its maturity level, and to organize the knowledge in a structured way.", "method": "The authors conducted a comprehensive survey and created a novel taxonomy that categorizes Arabic MML research into four key topics.", "result": "The survey identifies the current state of Arabic MML and highlights unexplored areas and critical research gaps.", "conclusion": "The structured overview offered by the survey aims to empower future research in Arabic MML by addressing identified opportunities and challenges.", "key_contributions": ["Creation of a novel taxonomy for Arabic MML research", "Identification of critical research gaps", "Structured overview of the current state of Arabic MML"], "limitations": "", "keywords": ["Multimodal Machine Learning", "Arabic language", "Survey", "Taxonomy", "Research gaps"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2508.12243", "pdf": "https://arxiv.org/pdf/2508.12243.pdf", "abs": "https://arxiv.org/abs/2508.12243", "title": "SEA-BED: Southeast Asia Embedding Benchmark", "authors": ["Wuttikorn Ponwitayarat", "Raymond Ng", "Jann Railey Montalan", "Thura Aung", "Jian Gang Ngui", "Yosephine Susanto", "William Tjhi", "Panuthep Tasawong", "Erik Cambria", "Ekapol Chuangsuwanich", "Sarana Nutanong", "Peerat Limkonchotiwat"], "categories": ["cs.CL"], "comment": null, "summary": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese.", "AI": {"tldr": "Introducing SEA-BED, the first large-scale sentence embedding benchmark for Southeast Asia with 169 datasets across 9 tasks and 10 languages, highlighting challenges and performance gaps in SEA languages.", "motivation": "The Southeast Asia region lacks a region-specific embedding benchmark for sentence embeddings despite having a large population and unique linguistic properties.", "method": "We constructed the SEA-BED benchmark with 169 datasets, evaluating 17 embedding models across six studies to analyze task and language challenges and translation effects.", "result": "The evaluation shows significant ranking shifts and inconsistent performance among models for SEA languages, emphasizing the need for human-curated datasets.", "conclusion": "The findings reveal the importance of region-specific benchmarks and human-generated datasets, particularly for low-resource languages like Burmese.", "key_contributions": ["The creation of the SEA-BED benchmark for Southeast Asia.", "Evaluation of 17 embedding models revealing unique performance challenges in SEA languages.", "Highlighting the critical role of human-generated datasets for accurate language representation."], "limitations": "The benchmark may not cover all linguistic nuances of SEA languages and relies heavily on human curation which may vary in quality.", "keywords": ["sentence embeddings", "Southeast Asia", "language benchmark", "semantic search", "multilingual NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12255", "pdf": "https://arxiv.org/pdf/2508.12255.pdf", "abs": "https://arxiv.org/abs/2508.12255", "title": "What do Speech Foundation Models Learn? Analysis and Applications", "authors": ["Ankita Pasad"], "categories": ["cs.CL", "eess.AS"], "comment": "Ph.D. Thesis", "summary": "Speech foundation models (SFMs) are designed to serve as general-purpose\nrepresentations for a wide range of speech-processing tasks. The last five\nyears have seen an influx of increasingly successful self-supervised and\nsupervised pre-trained models with impressive performance on various downstream\ntasks.\n  Although the zoo of SFMs continues to grow, our understanding of the\nknowledge they acquire lags behind. This thesis presents a lightweight analysis\nframework using statistical tools and training-free tasks to investigate the\nacoustic and linguistic knowledge encoded in SFM layers. We conduct a\ncomparative study across multiple SFMs and statistical tools. Our study also\nshows that the analytical insights have concrete implications for downstream\ntask performance.\n  The effectiveness of an SFM is ultimately determined by its performance on\nspeech applications. Yet it remains unclear whether the benefits extend to\nspoken language understanding (SLU) tasks that require a deeper understanding\nthan widely studied ones, such as speech recognition. The limited exploration\nof SLU is primarily due to a lack of relevant datasets. To alleviate that, this\nthesis contributes tasks, specifically spoken named entity recognition (NER)\nand named entity localization (NEL), to the Spoken Language Understanding\nEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and find\nthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded\n(speech recognition followed by a text model) approaches. Further, we evaluate\nE2E SLU models across SFMs and adaptation strategies to assess the impact on\ntask performance.\n  Collectively, this thesis tackles previously unanswered questions about SFMs,\nproviding tools and datasets to further our understanding and to enable the\ncommunity to make informed design choices for future model development and\nadoption.", "AI": {"tldr": "This thesis analyzes the knowledge encoded in Speech Foundation Models (SFMs) and their performance on spoken language understanding tasks, contributing new datasets and methods to improve understanding and application in the field.", "motivation": "The growth of Speech Foundation Models (SFMs) necessitates a deeper understanding of the knowledge they encode to better apply them in speech-processing tasks.", "method": "A lightweight analysis framework using statistical tools and training-free tasks was developed to investigate the acoustic and linguistic knowledge in SFM layers. A comparative study across multiple SFMs was conducted, along with the development of tasks for spoken named entity recognition and named entity localization in SLU.", "result": "End-to-end models utilizing SFMs for spoken language understanding outperformed traditional cascaded approaches, demonstrating the potential of leveraging SFMs beyond speech recognition.", "conclusion": "This thesis addressed gaps in understanding SFMs and provided new tools and datasets, paving the way for improved model design and usage in the field of spoken language understanding.", "key_contributions": ["Development of a lightweight analysis framework for SFMs", "Introduction of new SLU tasks: spoken named entity recognition and localization", "Comparative evaluation of E2E SLU models against traditional approaches"], "limitations": "", "keywords": ["Speech Foundation Models", "Spoken Language Understanding", "Named Entity Recognition"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2508.12257", "pdf": "https://arxiv.org/pdf/2508.12257.pdf", "abs": "https://arxiv.org/abs/2508.12257", "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework", "authors": ["Zheye Deng", "Chunkit Chan", "Tianshi Zheng", "Wei Fan", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems.", "AI": {"tldr": "This paper systematically reviews methodologies and challenges in converting unstructured text into structured formats, evaluates existing datasets and metrics, and proposes a universal evaluation framework for structured outputs.", "motivation": "There is a growing need for converting unstructured text into structured formats to support advanced applications in AI, yet existing research lacks a comprehensive overview.", "method": "The paper employs a systematic review methodology to examine text-to-structure conversion techniques, datasets, and evaluation metrics in the context of AI systems.", "result": "The review identifies key challenges in current methodologies and datasets, providing insights into gaps in research and practical applications.", "conclusion": "Text-to-structure conversion is essential for the evolution of next-generation AI systems, and a universal evaluation framework is proposed to improve structured output assessments.", "key_contributions": ["Systematic review of text-to-structure techniques and methodologies", "Evaluation of datasets and assessment criteria", "Proposal of a universal evaluation framework for structured outputs"], "limitations": "The review is limited to existing research and may not cover all emerging techniques and datasets in the rapidly evolving field.", "keywords": ["Text-to-structure", "AI systems", "Evaluation framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.12265", "pdf": "https://arxiv.org/pdf/2508.12265.pdf", "abs": "https://arxiv.org/abs/2508.12265", "title": "Fast, Slow, and Tool-augmented Thinking for LLMs: A Review", "authors": ["Xinda Jia", "Jinpeng Li", "Zezhong Wang", "Jingjing Li", "Xingshan Zeng", "Yasheng Wang", "Weinan Zhang", "Yong Yu", "Weiwen Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nreasoning across diverse domains. However, effective reasoning in real-world\ntasks requires adapting the reasoning strategy to the demands of the problem,\nranging from fast, intuitive responses to deliberate, step-by-step reasoning\nand tool-augmented thinking. Drawing inspiration from cognitive psychology, we\npropose a novel taxonomy of LLM reasoning strategies along two knowledge\nboundaries: a fast/slow boundary separating intuitive from deliberative\nprocesses, and an internal/external boundary distinguishing reasoning grounded\nin the model's parameters from reasoning augmented by external tools. We\nsystematically survey recent work on adaptive reasoning in LLMs and categorize\nmethods based on key decision factors. We conclude by highlighting open\nchallenges and future directions toward more adaptive, efficient, and reliable\nLLMs.", "AI": {"tldr": "This paper proposes a taxonomy for LLM reasoning strategies inspired by cognitive psychology and categorizes methods for adaptive reasoning in real-world tasks.", "motivation": "The need for large language models to effectively adapt their reasoning strategies based on the demands of diverse real-world tasks.", "method": "The authors propose a taxonomy of LLM reasoning strategies divided into a fast/slow boundary (intuitive vs. deliberative) and an internal/external boundary (model parameters vs. external tools). They systematically survey and categorize methods for adaptive reasoning.", "result": "They identify key decision factors influencing reasoning strategies and survey recent work in adaptive reasoning for LLMs.", "conclusion": "The paper concludes with discussions on open challenges and future research directions towards more adaptive and reliable LLMs.", "key_contributions": ["Proposed a novel taxonomy for LLM reasoning strategies.", "Categorized recent work on adaptive reasoning.", "Highlighted future directions for improving LLM reasoning efficiency."], "limitations": "", "keywords": ["Large Language Models", "reasoning strategies", "adaptive reasoning", "cognitive psychology", "taxonomy"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.12277", "pdf": "https://arxiv.org/pdf/2508.12277.pdf", "abs": "https://arxiv.org/abs/2508.12277", "title": "The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution", "authors": ["Elon Ezra", "Ariel Weizman", "Amos Azaria"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Large language models (LLMs) are commonly evaluated on tasks that test their\nknowledge or reasoning abilities. In this paper, we explore a different type of\nevaluation: whether an LLM can predict aspects of its own responses. Since LLMs\nlack the ability to execute themselves, we introduce the Self-Execution\nBenchmark, which measures a model's ability to anticipate properties of its\noutput, such as whether a question will be difficult for it, whether it will\nrefuse to answer, or what kinds of associations it is likely to produce. Our\nexperiments show that models generally perform poorly on this benchmark, and\nthat increased model size or capability does not consistently lead to better\nperformance. These results suggest a fundamental limitation in how LLMs\nrepresent and reason about their own behavior.", "AI": {"tldr": "This paper introduces the Self-Execution Benchmark, which evaluates LLMs on their ability to predict aspects of their own responses, revealing significant limitations in their self-awareness and reasoning.", "motivation": "To explore a novel evaluation method for large language models (LLMs) by assessing their ability to predict characteristics of their own outputs.", "method": "The paper introduces a Self-Execution Benchmark that measures an LLM's ability to anticipate properties of its output, such as difficulty or likely associations.", "result": "The experiments demonstrate that LLMs generally perform poorly on this benchmark, and performance does not consistently improve with larger model size or capability.", "conclusion": "The findings indicate a fundamental limitation in LLMs' representation and reasoning regarding their own behavior.", "key_contributions": ["Introduction of the Self-Execution Benchmark for evaluating LLMs.", "Empirical evidence of poor LLM performance on self-prediction tasks.", "Insights into the limitations of LLM reasoning about their responses."], "limitations": "Limited to the tasks defined in the Self-Execution Benchmark; results may not generalize to all forms of reasoning or other model architectures.", "keywords": ["large language models", "self-execution benchmark", "output prediction", "model limitations", "self-awareness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12281", "pdf": "https://arxiv.org/pdf/2508.12281.pdf", "abs": "https://arxiv.org/abs/2508.12281", "title": "Legal$Î$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain", "authors": ["Xin Dai", "Buqiang Xu", "Zhenghao Liu", "Yukun Yan", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Ge Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Legal Artificial Intelligence (LegalAI) has achieved notable advances in\nautomating judicial decision-making with the support of Large Language Models\n(LLMs). However, existing legal LLMs still struggle to generate reliable and\ninterpretable reasoning processes. They often default to fast-thinking behavior\nby producing direct answers without explicit multi-step reasoning, limiting\ntheir effectiveness in complex legal scenarios that demand rigorous\njustification. To address this challenge, we propose Legal$\\Delta$, a\nreinforcement learning framework designed to enhance legal reasoning through\nchain-of-thought guided information gain. During training, Legal$\\Delta$\nemploys a dual-mode input setup-comprising direct answer and\nreasoning-augmented modes-and maximizes the information gain between them. This\nencourages the model to acquire meaningful reasoning patterns rather than\ngenerating superficial or redundant explanations. Legal$\\Delta$ follows a\ntwo-stage approach: (1) distilling latent reasoning capabilities from a\npowerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning\nquality via differential comparisons, combined with a multidimensional reward\nmechanism that assesses both structural coherence and legal-domain specificity.\nExperimental results on multiple legal reasoning tasks demonstrate that\nLegal$\\Delta$ outperforms strong baselines in both accuracy and\ninterpretability. It consistently produces more robust and trustworthy legal\njudgments without relying on labeled preference data. All code and data will be\nreleased at https://github.com/NEUIR/LegalDelta.", "AI": {"tldr": "LegalAI has made strides in automating judicial decision-making, but current LLMs struggle with reliable reasoning. The proposed framework, LegalÎ, enhances legal reasoning through a dual-mode reinforcement learning approach.", "motivation": "To improve the reliability and interpretability of legal reasoning in LLMs, addressing the challenge of generating explicit multi-step reasoning.", "method": "LegalÎ uses a dual-mode input setup during training to maximize the information gain between direct answers and reasoning-augmented responses, enhancing reasoning patterns through a two-stage approach.", "result": "LegalÎ outperforms strong baselines in legal reasoning tasks, achieving improved accuracy and interpretability without needing labeled preference data.", "conclusion": "LegalÎ enhances the robustness and trustworthiness of legal judgments, demonstrating the effectiveness of reinforcement learning in legal AI applications.", "key_contributions": ["Introduction of LegalÎ framework for legal reasoning", "Use of dual-mode input in reinforcement learning", "Improvement in accuracy and interpretability of legal judgments"], "limitations": "", "keywords": ["Legal Artificial Intelligence", "Large Language Models", "Reinforcement Learning", "Legal Reasoning", "Interpretability"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.12282", "pdf": "https://arxiv.org/pdf/2508.12282.pdf", "abs": "https://arxiv.org/abs/2508.12282", "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation", "authors": ["Ziyang Chen", "Erxue Min", "Xiang Zhao", "Yunxin Li", "Xin Jia", "Jinzhi Liao", "Jichao Li", "Shuaiqiang Wang", "Baotian Hu", "Dawei Yin"], "categories": ["cs.CL", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "comment": "10 pages, 5 figures", "summary": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems.", "AI": {"tldr": "ChronoQA is a benchmark dataset for Chinese question answering focusing on temporal reasoning for Retrieval-Augmented Generation (RAG) systems, featuring over 300,000 news articles and 5,176 questions.", "motivation": "To evaluate temporal reasoning capabilities in RAG systems and provide a comprehensive resource for structured evaluation of temporal tasks in question answering.", "method": "ChronoQA was constructed from 300,000 news articles, containing structured annotations and validated through rule-based, LLM-based, and human evaluations.", "result": "The dataset includes 5,176 questions that address various temporal reasoning types and supports both single and multi-document scenarios, reflecting real-world requirements.", "conclusion": "ChronoQA serves as a scalable resource for evaluating time-sensitive retrieval-augmented question answering systems, enhancing the development of these technologies.", "key_contributions": ["Large-scale dataset for temporal reasoning in question answering.", "Comprehensive structural annotations with multi-stage validation.", "Dynamic resource for advancing time-sensitive RAG systems."], "limitations": "", "keywords": ["Chinese question answering", "temporal reasoning", "RAG systems", "benchmark dataset", "evaluation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.12286", "pdf": "https://arxiv.org/pdf/2508.12286.pdf", "abs": "https://arxiv.org/abs/2508.12286", "title": "Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction", "authors": ["Qinghua Wang", "Xu Zhang", "Lingyan Yang", "Rui Shao", "Bonan Wang", "Fang Wang", "Cunquan Qu"], "categories": ["cs.CL"], "comment": null, "summary": "Probation is a crucial institution in modern criminal law, embodying the\nprinciples of fairness and justice while contributing to the harmonious\ndevelopment of society. Despite its importance, the current Intelligent\nJudicial Assistant System (IJAS) lacks dedicated methods for probation\nprediction, and research on the underlying factors influencing probation\neligibility remains limited. In addition, probation eligibility requires a\ncomprehensive analysis of both criminal circumstances and remorse. Much of the\nexisting research in IJAS relies primarily on data-driven methodologies, which\noften overlooks the legal logic underpinning judicial decision-making. To\naddress this gap, we propose a novel approach that integrates legal logic into\ndeep learning models for probation prediction, implemented in three distinct\nstages. First, we construct a specialized probation dataset that includes fact\ndescriptions and probation legal elements (PLEs). Second, we design a distinct\nprobation prediction model named the Multi-Task Dual-Theory Probation\nPrediction Model (MT-DT), which is grounded in the legal logic of probation and\nthe \\textit{Dual-Track Theory of Punishment}. Finally, our experiments on the\nprobation dataset demonstrate that the MT-DT model outperforms baseline models,\nand an analysis of the underlying legal logic further validates the\neffectiveness of the proposed approach.", "AI": {"tldr": "This paper proposes a novel approach to probation prediction by integrating legal logic into deep learning models, improving upon existing data-driven methodologies.", "motivation": "The current Intelligent Judicial Assistant System lacks methods for effective probation prediction and understanding factors influencing probation eligibility.", "method": "The authors construct a specialized probation dataset and design the Multi-Task Dual-Theory Probation Prediction Model (MT-DT) that incorporates legal logic.", "result": "Experiments show that the MT-DT model outperforms baseline models in predicting probation eligibility.", "conclusion": "The integration of legal logic into the prediction models improves accuracy and addresses gaps in existing research.", "key_contributions": ["Integration of legal logic in deep learning for probation prediction", "Development of a specialized probation dataset", "Novel Multi-Task Dual-Theory Probation Prediction Model (MT-DT)"], "limitations": "", "keywords": ["probation prediction", "legal logic", "deep learning", "judicial decision-making", "machine learning"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.12301", "pdf": "https://arxiv.org/pdf/2508.12301.pdf", "abs": "https://arxiv.org/abs/2508.12301", "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model", "authors": ["Tomer Krichli", "Bhiksha Raj", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "17 pages, 7 Figures, This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR.", "AI": {"tldr": "This paper proposes a method for transforming transformer encoder-decoder models into efficient low-latency streaming models for Automatic Speech Recognition (ASR), achieving better performance in real-time transcription.", "motivation": "Existing ASR models like OpenAI Whisper and NVIDIA Canary excel in offline transcription but struggle with streaming due to architectural limitations.", "method": "The authors modify a non-causal encoder into a causal encoder through fine-tuning with Low-Rank Adaptation (LoRA) and a weakly aligned dataset, and introduce a new inference mechanism for greedy and beam-search decoding.", "result": "The fine-tuned model demonstrates improved performance over non-fine-tuned streaming approaches with low-latency chunk sizes (less than 300 msec), alongside a simpler method for extracting word-level timestamps.", "conclusion": "The proposed approach offers a significant improvement in streaming ASR, making real-time transcription more accurate and feasible while releasing supporting code and models for further research.", "key_contributions": ["Transformation of encoder-decoder models for low-latency streaming ASR", "Introduction of a novel fine-tuning method using LoRA", "Development of an efficient inference mechanism for real-time transcription."], "limitations": "", "keywords": ["Automatic Speech Recognition", "Streaming Model", "Transformer", "Low-Latency"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.12355", "pdf": "https://arxiv.org/pdf/2508.12355.pdf", "abs": "https://arxiv.org/abs/2508.12355", "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering", "authors": ["Eviatar Nachshoni", "Arie Cattan", "Shmuel Amar", "Ori Shapira", "Ido Dagan"], "categories": ["cs.CL"], "comment": "no comments", "summary": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them.", "AI": {"tldr": "This paper addresses the challenges in Multi-Answer Question Answering (MAQA), introducing a new benchmark and methodology to evaluate models on conflict-aware tasks.", "motivation": "To improve Multi-Answer Question Answering (MAQA) by recognizing both valid answers and conflicting answer pairs, which traditional QA methods overlook.", "method": "The study extends the MAQA setting by leveraging fact-checking datasets to create NATCONFQA, a benchmark focusing on realistic and conflict-aware question answering tasks.", "result": "Evaluation of eight LLMs on NATCONFQA shows their fragility in managing various conflicts and reveals ineffective strategies for conflict resolution.", "conclusion": "The introduction of NATCONFQA provides a necessary framework for advancing research on conflict-aware MAQA and highlights the limitations of existing LLMs in handling such scenarios.", "key_contributions": ["Introduction of NATCONFQA benchmark for conflict-aware MAQA", "Development of a novel methodology for dataset construction", "Evaluation of LLMs revealing their weaknesses in conflict resolution"], "limitations": "The benchmark relies on existing fact-checking datasets, which may have their own limitations.", "keywords": ["Multi-Answer Question Answering", "Language Models", "Benchmark", "Conflict-Detection", "Fact-Checking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12387", "pdf": "https://arxiv.org/pdf/2508.12387.pdf", "abs": "https://arxiv.org/abs/2508.12387", "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models", "authors": ["Yuanfeng Xu", "Zehui Dai", "Jian Liang", "Jiapeng Guan", "Guangrun Wang", "Liang Lin", "Xiaohui Lv"], "categories": ["cs.CL"], "comment": "16pages, 3 figures", "summary": "Small Language Models (SLMs) are a cost-effective alternative to Large\nLanguage Models (LLMs), but often struggle with complex reasoning due to their\nlimited capacity and a tendency to produce mistakes or inconsistent answers\nduring multi-step reasoning. Existing efforts have improved SLM performance,\nbut typically at the cost of one or more of three key aspects: (1) reasoning\ncapability, due to biased supervision that filters out negative reasoning paths\nand limits learning from errors; (2) autonomy, due to over-reliance on\nexternally generated reasoning signals; and (3) generalization, which suffers\nwhen models overfit to teacher-specific patterns. In this paper, we introduce\nReaLM, a reinforcement learning framework for robust and self-sufficient\nreasoning in vertical domains. To enhance reasoning capability, we propose\nMulti-Route Process Verification (MRPV), which contrasts both positive and\nnegative reasoning paths to extract decisive patterns. To reduce reliance on\nexternal guidance and improve autonomy, we introduce Enabling Autonomy via\nAsymptotic Induction (EAAI), a training strategy that gradually fades external\nsignals. To improve generalization, we apply guided chain-of-thought\ndistillation to encode domain-specific rules and expert knowledge into SLM\nparameters, making them part of what the model has learned. Extensive\nexperiments on both vertical and general reasoning tasks demonstrate that ReaLM\nsignificantly improves SLM performance across aspects (1)-(3) above.", "AI": {"tldr": "ReaLM is a framework to enhance Small Language Models' reasoning, autonomy, and generalization in vertical domains through novel training strategies.", "motivation": "To address the limitations of Small Language Models in reasoning capability, autonomy, and generalization due to their inherent biases and over-reliance on external signals.", "method": "The framework includes Multi-Route Process Verification (MRPV) to contrast reasoning paths and a training strategy called Enabling Autonomy via Asymptotic Induction (EAAI) to reduce external guidance.", "result": "ReaLM shows significant improvements in Small Language Models' performance on both vertical and general reasoning tasks, demonstrating enhanced reasoning capability, autonomy, and generalization.", "conclusion": "The proposed methods in ReaLM provide a structured approach to improve SLMs' shortfalls and can be beneficial in applications requiring robust reasoning.", "key_contributions": ["Introduction of the ReaLM framework for Small Language Models.", "Development of Multi-Route Process Verification for contrasting reasoning paths.", "Implementation of Enabling Autonomy via Asymptotic Induction to improve model autonomy."], "limitations": "", "keywords": ["Small Language Models", "Reinforcement Learning", "Reasoning"], "importance_score": 7, "read_time_minutes": 16}}
{"id": "2508.12393", "pdf": "https://arxiv.org/pdf/2508.12393.pdf", "abs": "https://arxiv.org/abs/2508.12393", "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90\\%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.", "AI": {"tldr": "MedKGent is a framework for constructing temporally evolving medical knowledge graphs (KGs) using LLMs, addressing limitations in current methods for biomedical literature integration.", "motivation": "The rapid growth of medical literature presents challenges in structuring and integrating knowledge, which Knowledge Graphs can help address, yet current methods are limited by their approaches.", "method": "MedKGent uses a framework with two specialized agents: the Extractor Agent identifies knowledge triples and assigns confidence scores, while the Constructor Agent integrates these into an evolving KG, using a daily time series of PubMed abstracts.", "result": "The constructed KG encompasses 156,275 entities and 2,971,384 relational triples with an accuracy nearing 90%.", "conclusion": "MedKGent demonstrates significant improvements in medical question answering and highlights the KG's value in drug repurposing through causal inference.", "key_contributions": ["Introduction of the MedKGent framework for dynamic KG construction", "Improvement in biomedical knowledge retrieval using LLM-powered agents", "Demonstrated application in literature-based drug repurposing"], "limitations": "", "keywords": ["Knowledge Graphs", "Medical Literature", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12405", "pdf": "https://arxiv.org/pdf/2508.12405.pdf", "abs": "https://arxiv.org/abs/2508.12405", "title": "Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing", "authors": ["Zilong Bai", "Zihan Xu", "Cong Sun", "Chengxi Zang", "H. Timothy Bunnell", "Catherine Sinfield", "Jacqueline Rutter", "Aaron Thomas Martinez", "L. Charles Bailey", "Mark Weiner", "Thomas R. Campion", "Thomas Carton", "Christopher B. Forrest", "Rainu Kaushal", "Fei Wang", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in npj Health Systems", "summary": "Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)\nremains challenging due to its myriad symptoms that evolve over long- and\nvariable-time intervals. To address this issue, we developed a hybrid natural\nlanguage processing pipeline that integrates rule-based named entity\nrecognition with BERT-based assertion detection modules for PASC-symptom\nextraction and assertion detection from clinical notes. We developed a\ncomprehensive PASC lexicon with clinical specialists. From 11 health systems of\nthe RECOVER initiative network across the U.S., we curated 160 intake progress\nnotes for model development and evaluation, and collected 47,654 progress notes\nfor a population-level prevalence study. We achieved an average F1 score of\n0.82 in one-site internal validation and 0.76 in 10-site external validation\nfor assertion detection. Our pipeline processed each note at $2.448\\pm 0.812$\nseconds on average. Spearman correlation tests showed $\\rho >0.83$ for positive\nmentions and $\\rho >0.72$ for negative ones, both with $P <0.0001$. These\ndemonstrate the effectiveness and efficiency of our models and their potential\nfor improving PASC diagnosis.", "AI": {"tldr": "The paper presents a hybrid NLP pipeline for diagnosing Post-Acute Sequelae of COVID-19 (PASC) by integrating named entity recognition and BERT-based assertion detection, achieving high accuracy in symptom extraction from clinical notes.", "motivation": "Diagnosing PASC is challenging due to its evolving symptoms that manifest over varying time intervals, necessitating improved methods for extraction from clinical notes.", "method": "A hybrid natural language processing pipeline integrating rule-based named entity recognition with BERT-based assertion detection modules was developed. A comprehensive lexicon for PASC was created with clinical specialists' input. The approach was evaluated using 160 intake progress notes for model development and 47,654 notes for a population-level prevalence study.", "result": "The model achieved an average F1 score of 0.82 in internal validation and 0.76 in external validation for assertion detection. The average processing time for each note was $2.448\\pm 0.812$ seconds, with high correlation coefficients for detection accuracy.", "conclusion": "The hybrid NLP pipeline demonstrates effectiveness and efficiency in improving the diagnosis of PASC, indicating its potential for practical application in health informatics.", "key_contributions": ["Development of a hybrid NLP pipeline for PASC symptom extraction.", "Creation of a comprehensive PASC lexicon in collaboration with clinical experts.", "High validation scores indicate the reliability of the approach."], "limitations": "", "keywords": ["PASC", "NLP", "health informatics", "BERT", "named entity recognition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2312.12399", "pdf": "https://arxiv.org/pdf/2312.12399.pdf", "abs": "https://arxiv.org/abs/2312.12399", "title": "Development and User Experiences of a Novel Virtual Reality Task for Poststroke Visuospatial Neglect: An Exploratory Pilot Study", "authors": ["Andrew Danso", "Patti Nijhuis", "Alessandro Ansani", "Martin Hartmann", "Gulnara Minkkinen", "Geoff Luck", "Joshua S. Bamford", "Sarah Faber", "Kat R. Agres", "Solange Glasser", "Teppo SÃ¤rkÃ¤mÃ¶", "Rebekah Rousi", "Marc R. Thompson"], "categories": ["cs.HC"], "comment": "19 pages, 5 figures, 4 tables", "summary": "Background: Visuospatial neglect (VSN) affects spatial awareness, leading to\nfunctional and motor challenges. This case study explores virtual reality (VR)\nas a potential complementary tool for VSN rehabilitation.\n  Objective: Specifically, we aim to explore the initial experiences of\npatients and physiotherapists engaging with a novel protocol, using an\naudiovisual cue task to support VSN rehabilitation.\n  Methods: A preliminary VR task integrating audiovisual cues was co-designed\nwith 2 physiotherapists. The task was then tested with 2 patients with VSN over\n12 sessions. The intervention focused on engaging neglected spatial areas, with\nphysiotherapists adapting the task to individual needs and monitoring\nresponses.\n  Results: Initial testing with 2 trainee physiotherapists indicated high\nusability, engagement, and perceived safety. Two patients with VSN completed 12\nVR sessions. For Patient A, completion times increased following the\nintroduction of an audio cue, though modeling indicated a nonsignificant linear\ntrend (beta = 0.08; P = .33) and a marginally significant downward curvature\n(beta = -0.001; P = .08). In contrast, Patient B showed a significant linear\ndecrease in completion times (beta = -0.53; P = .009), with a quadratic trend\nindicating a performance minimum around session 10 (B = 0.007; P = .04).\nIntraweek variability also decreased. Motor scores (Box and Block Test and\n9-Hole Peg Test) remained stable, and subjective feedback indicated improved\nmobility confidence and positive task engagement.\n  Conclusions: Further research with larger cohorts is needed to confirm the VR\ntask's utility and refine the intervention.", "AI": {"tldr": "This study investigates the use of virtual reality (VR) as a rehabilitation tool for patients with visuospatial neglect (VSN), highlighting initial patient and physiotherapist experiences with an audiovisual cue task designed to improve spatial awareness.", "motivation": "To explore innovative rehabilitation methods for individuals suffering from visuospatial neglect (VSN) which impairs their spatial awareness and functionality.", "method": "A VR rehabilitation task was co-designed with physiotherapists, tested on two patients with VSN over 12 sessions, using audiovisual cues to engage neglected spatial areas.", "result": "Initial testing with physiotherapists showed high usability and safety, while patient results indicated varied performance improvements: one patient showed nonsignificant improvements, while the other demonstrated significant reductions in task completion times over sessions.", "conclusion": "The findings suggest potential for VR intervention in VSN rehabilitation, but further research with larger cohorts is needed for validation.", "key_contributions": ["Innovative use of VR for VSN rehabilitation", "Co-designing rehabilitation tasks with physiotherapists", "Initial evidence of performance changes in patients with VSN using VR"], "limitations": "Limited sample size and scope limit generalizability of findings.", "keywords": ["virtual reality", "visuospatial neglect", "rehabilitation", "audiovisual cues", "physiotherapy"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.12407", "pdf": "https://arxiv.org/pdf/2508.12407.pdf", "abs": "https://arxiv.org/abs/2508.12407", "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads", "authors": ["Zhuorui Liu", "Chen Zhang", "Dawei Song"], "categories": ["cs.CL"], "comment": "5 pages, 4 figures", "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.", "AI": {"tldr": "This paper proposes ZigzagAttention, an approach to improve long-context handling in large language models by optimizing the memory footprint of the KV cache and reducing latency related to attention heads.", "motivation": "To address the challenges in deploying large language models with long context due to increased KV cache memory consumption and latency from managing retrieval and streaming heads within attention layers.", "method": "The authors designed a criterion to categorize attention heads into retrieval and streaming heads exclusively within one layer to minimize latency while preserving performance.", "result": "ZigzagAttention reduces overall latency and maintains competitive performance among various baselines, achieving improved efficiency in memory usage during attention processes.", "conclusion": "This approach successfully identifies attention heads to minimize latency effects during deployment of large language models and offers performance comparable to existing methods.", "key_contributions": ["Introduction of ZigzagAttention to optimize KV cache usage in LLMs", "Improved identification process for retrieval vs. streaming heads", "Reduction of latency without significant performance degradation"], "limitations": "", "keywords": ["Large Language Models", "Key-Value Cache", "Attention Heads", "Performance Optimization", "Latent Memory Consumption"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2410.12268", "pdf": "https://arxiv.org/pdf/2410.12268.pdf", "abs": "https://arxiv.org/abs/2410.12268", "title": "VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels", "authors": ["Chen Chen", "Hannah K. Bako", "Peihong Yu", "John Hooker", "Jeffrey Joyal", "Simon C. Wang", "Samuel Kim", "Jessica Wu", "Aoxue Ding", "Lara Sandeep", "Alex Chen", "Chayanika Sinha", "Zhicheng Liu"], "categories": ["cs.HC"], "comment": "Will appear at IEEE VIS 2025 conference and TVCG", "summary": "Chart corpora, which comprise data visualizations and their semantic labels,\nare crucial for advancing visualization research. However, the labels in most\nexisting corpora are high-level (e.g., chart types), hindering their utility\nfor broader applications in the era of AI. In this paper, we contribute\nVISANATOMY, a corpus containing 942 real-world SVG charts produced by over 50\ntools, encompassing 40 chart types and featuring structural and stylistic\ndesign variations. Each chart is augmented with multi-level fine-grained labels\non its semantic components, including each graphical element's type, role, and\nposition, hierarchical groupings of elements, group layouts, and visual\nencodings. In total, VISANATOMY provides labels for more than 383k graphical\nelements. We demonstrate the richness of the semantic labels by comparing\nVISANATOMY with existing corpora. We illustrate its usefulness through four\napplications: semantic role inference for SVG elements, chart semantic\ndecomposition, chart type classification, and content navigation for\naccessibility. Finally, we discuss research opportunities to further improve\nVISANATOMY.", "AI": {"tldr": "This paper presents VISANATOMY, a chart corpus containing 942 real-world SVG charts with fine-grained semantic labels, aimed at enhancing visualization research and applications in AI.", "motivation": "Current chart corpora have high-level labels that limit their utility; VISANATOMY aims to provide more detailed labeling to support broader applications.", "method": "The corpus includes 942 SVG charts with over 383k labeled graphical elements, providing multi-level semantic information on types, roles, positions, groupings, and layouts of chart elements.", "result": "VISANATOMY shows superiority over existing chart corpora by allowing for applications like semantic role inference and accessibility content navigation.", "conclusion": "The paper highlights the potential of VISANATOMY to advance research in visualization and propose future improvements.", "key_contributions": ["Introduction of VISANATOMY corpus with detailed semantic labeling", "Comparison demonstrating the superiority of VISANATOMY to existing corpora", "Showcasing practical applications of the corpus in visualization research"], "limitations": "", "keywords": ["chart corpus", "visualization research", "semantic labeling", "SVG charts", "AI applications"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.12411", "pdf": "https://arxiv.org/pdf/2508.12411.pdf", "abs": "https://arxiv.org/abs/2508.12411", "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases", "authors": ["Emanuel Z. Fenech-Borg", "Tilen P. Meznaric-Kos", "Milica D. Lekovic-Bojovic", "Arni J. Hentze-Djurhuus"], "categories": ["cs.CL", "I.2.7; K.4.1; H.3.3"], "comment": "10 pages, 5 figures, IEEE conference format, submitted to [Conference\n  Name]", "summary": "Large language models (LLMs) are deployed globally, yet their underlying\ncultural and ethical assumptions remain underexplored. We propose the notion of\na \"cultural gene\" -- a systematic value orientation that LLMs inherit from\ntheir training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200\nprompts targeting two classic cross-cultural dimensions:\nIndividualism-Collectivism (IDV) and Power Distance (PDI). Using standardized\nzero-shot prompts, we compare a Western-centric model (GPT-4) and an\nEastern-centric model (ERNIE Bot). Human annotation shows significant and\nconsistent divergence across both dimensions. GPT-4 exhibits individualistic\nand low-power-distance tendencies (IDV score approx 1.21; PDI score approx\n-1.05), while ERNIE Bot shows collectivistic and higher-power-distance\ntendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically\nsignificant (p < 0.001). We further compute a Cultural Alignment Index (CAI)\nagainst Hofstede's national scores and find GPT-4 aligns more closely with the\nUSA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns\nmore closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative\nanalyses of dilemma resolution and authority-related judgments illustrate how\nthese orientations surface in reasoning. Our results support the view that LLMs\nfunction as statistical mirrors of their cultural corpora and motivate\nculturally aware evaluation and deployment to avoid algorithmic cultural\nhegemony.", "AI": {"tldr": "This paper explores the cultural and ethical assumptions of large language models (LLMs) through the introduction of a Cultural Probe Dataset (CPD) and compares Western-centric and Eastern-centric models using cross-cultural dimensions.", "motivation": "To investigate the cultural values that large language models inherit from their training data and to promote culturally aware evaluation and deployment.", "method": "The authors introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting the cross-cultural dimensions of Individualism-Collectivism (IDV) and Power Distance (PDI). They compare responses from the Western-centric model GPT-4 and the Eastern-centric model ERNIE Bot, employing human annotations and computing a Cultural Alignment Index (CAI) against Hofstede's national scores.", "result": "Significant divergence was found in the cultural orientations of GPT-4 and ERNIE Bot, with GPT-4 exhibiting individualistic and low-power-distance tendencies while ERNIE Bot displayed collectivistic and higher-power-distance tendencies. Statistical significance was noted with p < 0.001.", "conclusion": "The findings indicate that LLMs reflect the cultural biases present in their training data, underscoring the importance of culturally aware evaluation to mitigate the risks of algorithmic cultural hegemony.", "key_contributions": ["Introduction of the Cultural Probe Dataset (CPD) for exploring cultural dimensions in LLMs.", "Comparison of cross-cultural orientations in GPT-4 versus ERNIE Bot using statistical analysis.", "Development of the Cultural Alignment Index (CAI) to assess alignment with Hofstede's cultural dimensions."], "limitations": "", "keywords": ["Cultural Gene", "Large Language Models", "Cultural Probe Dataset", "Individualism-Collectivism", "Power Distance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.12488", "pdf": "https://arxiv.org/pdf/2504.12488.pdf", "abs": "https://arxiv.org/abs/2504.12488", "title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process", "authors": ["Mohi Reza", "Jeb Thomas-Mitchell", "Peter Dushniku", "Nathan Laundry", "Joseph Jay Williams", "Anastasia Kuzminykh"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7; I.2.6; I.7.2"], "comment": null, "summary": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms.", "AI": {"tldr": "This paper explores the impact of AI tools on the writing process and the preservation of writers' agency through a systematic review and interviews.", "motivation": "To investigate how generative AI affects writers' sense of agency and ownership during the writing process.", "method": "Conducted a systematic review of 109 HCI papers using the PRISMA approach and complemented it with interviews of 15 writers from diverse backgrounds.", "result": "Identified four design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback, along with insights on how different types of writers value AI intervention at various stages of writing.", "conclusion": "The study highlights the differences in preferences for AI assistance based on writers' goals and their implications for designing human-centered writing tools.", "key_contributions": ["Systematic review identifying key design strategies for AI writing support.", "Insights into how writers' preferences for AI intervention vary across the writing process.", "Recommendations for developing co-writing AI tools that respect human agency."], "limitations": "", "keywords": ["AI Writing Support", "Human-Computer Interaction", "Writers' Agency"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.12448", "pdf": "https://arxiv.org/pdf/2508.12448.pdf", "abs": "https://arxiv.org/abs/2508.12448", "title": "Uncovering Emergent Physics Representations Learned In-Context by Large Language Models", "authors": ["Yeongwoo Song", "Jaeyong Bae", "Dong-Kyum Kim", "Hawoong Jeong"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 10 figures", "summary": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\nabilities, enabling them to solve wide range of tasks via textual prompts\nalone. As these capabilities advance, the range of applicable domains continues\nto expand significantly. However, identifying the precise mechanisms or\ninternal structures within LLMs that allow successful ICL across diverse,\ndistinct classes of tasks remains elusive. Physics-based tasks offer a\npromising testbed for probing this challenge. Unlike synthetic sequences such\nas basic arithmetic or symbolic equations, physical systems provide\nexperimentally controllable, real-world data based on structured dynamics\ngrounded in fundamental principles. This makes them particularly suitable for\nstudying the emergent reasoning behaviors of LLMs in a realistic yet tractable\nsetting. Here, we mechanistically investigate the ICL ability of LLMs,\nespecially focusing on their ability to reason about physics. Using a dynamics\nforecasting task in physical systems as a proxy, we evaluate whether LLMs can\nlearn physics in context. We first show that the performance of dynamics\nforecasting in context improves with longer input contexts. To uncover how such\ncapability emerges in LLMs, we analyze the model's residual stream activations\nusing sparse autoencoders (SAEs). Our experiments reveal that the features\ncaptured by SAEs correlate with key physical variables, such as energy. These\nfindings demonstrate that meaningful physical concepts are encoded within LLMs\nduring in-context learning. In sum, our work provides a novel case study that\nbroadens our understanding of how LLMs learn in context.", "AI": {"tldr": "This paper investigates the in-context learning abilities of large language models (LLMs) through the study of physics-based tasks, revealing how LLMs can learn and reason about physics from contextual data.", "motivation": "To understand the mechanisms that enable large language models to exhibit in-context learning (ICL) across diverse tasks, specifically in the realm of physics.", "method": "The study employs a dynamics forecasting task within physical systems, analyzing LLMs' performance as input context length increases and using sparse autoencoders to evaluate model activations.", "result": "The performance of LLMs in dynamics forecasting improved with longer input contexts, and analysis showed that features captured by sparse autoencoders correlate with key physical variables like energy.", "conclusion": "The findings reveal that meaningful physical concepts are encoded within LLMs during in-context learning, enhancing our understanding of their learning capabilities.", "key_contributions": ["Mechanistic investigation of LLMs' reasoning abilities in physics", "Demonstration of improved performance with longer input contexts", "Correlation of model activations with fundamental physical concepts"], "limitations": "", "keywords": ["in-context learning", "large language models", "physics", "sparse autoencoders", "dynamics forecasting"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2508.12458", "pdf": "https://arxiv.org/pdf/2508.12458.pdf", "abs": "https://arxiv.org/abs/2508.12458", "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following", "authors": ["Ruirui Gao", "Emily Johnson", "Bowen Tan", "Yanfei Qian"], "categories": ["cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).", "AI": {"tldr": "Proposes M3PO, a data-efficient method for optimizing Large Vision-Language Models (LVLMs) in visual instruction following by selecting high-value preference samples.", "motivation": "Address the high cost and inconsistency of human annotation in fine-tuning LVLMs, particularly for multimodal instruction following tasks.", "method": "M3PO uses a selection mechanism combining Multimodal Alignment Score and Self-Consistency/Confidence to identify valuable preference sample pairs for Direct Preference Optimization.", "result": "M3PO outperforms traditional fine-tuning methods and existing preference optimization techniques on multiple multimodal instruction following benchmarks.", "conclusion": "M3PO enhances the efficiency of fine-tuning LVLMs by utilizing high-quality preference pairs for better instruction following performance.", "key_contributions": ["Introduction of M3PO for efficient preference optimization", "Use of M3P-Score for identifying valuable sample pairs", "Demonstrated superior performance over existing methods."], "limitations": "", "keywords": ["Large Vision-Language Models", "preference optimization", "multimodal instruction following"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12459", "pdf": "https://arxiv.org/pdf/2508.12459.pdf", "abs": "https://arxiv.org/abs/2508.12459", "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages", "authors": ["Alham Fikri Aji", "Trevor Cohn"], "categories": ["cs.CL"], "comment": null, "summary": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.", "AI": {"tldr": "Introducing LoraxBench, a benchmark for evaluating NLP on Indonesia's low-resource languages across six tasks, revealing performance discrepancies among languages.", "motivation": "Indonesia, with 700 languages, has lagged in NLP progress, necessitating a benchmark for its low-resource languages.", "method": "Developed LoraxBench as a benchmark covering reading comprehension, QA, language inference, causal reasoning, translation, and cultural QA across 20 languages.", "result": "Evaluation of multilingual and region-focused LLMs highlighted challenges in NLP for low-resource Indonesian languages and performance discrepancies based on language and register.", "conclusion": "The benchmark emphasizes the need for targeted NLP approaches for low-resource languages and shows the impact of language politeness registers on model performance.", "key_contributions": ["Introduction of LoraxBench for low-resource Indonesian languages", "Evaluation of multilingual models on diverse NLP tasks", "Analysis of the impact of language register on model performance"], "limitations": "Focused on low-resource languages; results may not generalize to other contexts outside Indonesia.", "keywords": ["NLP", "Low-resource languages", "Benchmark", "LLM", "Indonesian"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.12461", "pdf": "https://arxiv.org/pdf/2508.12461.pdf", "abs": "https://arxiv.org/abs/2508.12461", "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models", "authors": ["Ziqian Bi", "Keyu Chen", "Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song"], "categories": ["cs.CL"], "comment": null, "summary": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments.", "AI": {"tldr": "A comparison of OpenAI's GPT-OSS models with contemporary open-source LLMs reveals that the 20B variant often outperforms the 120B model while consuming less memory and energy.", "motivation": "To evaluate the performance of OpenAI's latest large language models against other contemporary models and to understand the efficiency and effectiveness of scaling in sparse architectures.", "method": "The models were tested using standardised inference settings across ten benchmarks, employing McNemars test and effect size analysis for statistical validation.", "result": "GPT-OSS 20B consistently outperformed GPT-OSS 120B on several benchmarks like HumanEval and MMLU, with both models showing mid-tier performance overall, strong in code generation but weak in multilingual tasks.", "conclusion": "Scaling in sparse architectures may not produce proportional performance gains, suggesting a need for further optimisation strategies and informing model selection for future deployments.", "key_contributions": ["Evaluation of new GPT-OSS models", "Insights into performance of sparse architecture scalability", "Comparison with contemporary open-source models"], "limitations": "Performance weaknesses noted in multilingual tasks.", "keywords": ["OpenAI", "GPT-OSS", "large language models", "code generation", "multilingual understanding"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.12482", "pdf": "https://arxiv.org/pdf/2508.12482.pdf", "abs": "https://arxiv.org/abs/2508.12482", "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping", "authors": ["Xiaomeng Zhu", "R. Thomas McCoy", "Robert Frank"], "categories": ["cs.CL"], "comment": null, "summary": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels.", "AI": {"tldr": "The paper investigates whether large language models, like RoBERTa and GPT-2, learn verb meanings through syntactic cues similar to children, finding that the degradation of verb representation is greater when syntactic cues are removed rather than co-occurrence information.", "motivation": "To explore if large language models (LLMs) exhibit syntactic bootstrapping in verb meaning acquisition akin to children's language learning.", "method": "Trained RoBERTa and GPT-2 on perturbed datasets with abbated syntactic information and measured the impact on verb and noun representations.", "result": "Models' verb representations were significantly degraded when syntactic information was ablated, particularly for mental verbs compared to physical verbs; noun representation was more affected by co-occurrence distortion.", "conclusion": "The findings support the hypothesis that syntactic bootstrapping plays a crucial role in verb learning and demonstrate a method for testing developmental hypotheses in LLMs.", "key_contributions": ["Demonstrated the influence of syntactic cues on verb representation in LLMs.", "Highlighted differences in learning impacts between mental and physical verbs compared to nouns.", "Provided a framework for exploring developmental language hypotheses using LLMs."], "limitations": "", "keywords": ["Syntactic bootstrapping", "Verbs", "Large language models", "RoBERTa", "GPT-2"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.12495", "pdf": "https://arxiv.org/pdf/2508.12495.pdf", "abs": "https://arxiv.org/abs/2508.12495", "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning", "authors": ["Yuangang Li", "Yiqing Shen", "Yi Nian", "Jiechao Gao", "Ziyi Wang", "Chenxiao Yu", "Shawn Li", "Jie Wang", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.", "AI": {"tldr": "The paper introduces CDCR-SFT, a framework for enhancing causal reasoning in LLMs by constructing directed acyclic graphs (DAGs) to reduce hallucinations.", "motivation": "LLMs often produce logically inconsistent outputs due to hallucinations, yet previous reasoning methods fail to model causal relationships effectively.", "method": "A supervised fine-tuning framework (CDCR-SFT) is proposed, which trains LLMs to construct variable-level DAGs and reason over them.", "result": "CDCR-SFT achieves state-of-the-art accuracy of 95.33% on the CLADDER benchmark and reduces hallucinations by 10% on HaluEval.", "conclusion": "Modeling explicit causal structures in LLMs alleviates logical inconsistencies in their outputs.", "key_contributions": ["Introduction of a novel framework for causal DAG construction in LLMs", "Creation of the CausalDR dataset with 25,368 samples for training and evaluation", "Significant improvements in causal reasoning accuracy and reduction in hallucinations"], "limitations": "", "keywords": ["Large Language Models", "Causal Reasoning", "Directed Acyclic Graph", "Hallucinations", "Supervised Fine-Tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12535", "pdf": "https://arxiv.org/pdf/2508.12535.pdf", "abs": "https://arxiv.org/abs/2508.12535", "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.", "AI": {"tldr": "This paper introduces CorrSteer, a method for selecting features from Sparse Autoencoders (SAEs) using inference-time activations to improve downstream steering tasks in language models.", "motivation": "The limitations of Sparse Autoencoders in effectively steering large language models due to the need for contrastive datasets and significant storage of activation data.", "method": "CorrSteer correlates sample correctness with SAE activations at inference time to select features, automating the selection process based on average activations.", "result": "The method improves task performance, notably achieving a +4.1% enhancement in MMLU performance and a +22.9% boost in HarmBench results while using only 4000 samples, showcasing effective feature selection aligned with task requirements.", "conclusion": "CorrSteer establishes correlation-based feature selection as a scalable and effective strategy for enhancing the performance of large language models in various tasks.", "key_contributions": ["Introduction of CorrSteer for feature selection", "Improvement in performance metrics for QA, bias mitigation, and reasoning", "Automated extraction of relevant features using inference-time activations"], "limitations": "", "keywords": ["Sparse Autoencoders", "feature selection", "language models", "inference-time", "steering tasks"], "importance_score": 8, "read_time_minutes": 42}}
{"id": "2508.12591", "pdf": "https://arxiv.org/pdf/2508.12591.pdf", "abs": "https://arxiv.org/abs/2508.12591", "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning", "authors": ["Yu-Hsuan Fang", "Tien-Hong Lo", "Yao-Ting Sung", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.", "AI": {"tldr": "The paper explores the use of Multimodal Large Language Models for comprehensive Automated Speaking Assessment, addressing inherent limitations of traditional systems.", "motivation": "To improve Automated Speaking Assessment by overcoming limitations of existing text-based and audio-based methods.", "method": "The paper introduces Speech-First Multimodal Training (SFMT) that utilizes curriculum learning to enhance training for MLLM in ASA.", "result": "Experiments show that MLLM improves holistic assessment performance from a PCC value of 0.783 to 0.846, with SFMT yielding a 4% accuracy improvement in delivery assessment.", "conclusion": "MLLM combined with SFMT presents a significant advancement in Automated Speaking Assessment, particularly for evaluating delivery aspects.", "key_contributions": ["First systematic study of MLLM in ASA", "Introduction of Speech-First Multimodal Training (SFMT)", "Demonstrated improvements in assessment performance using MLLM."], "limitations": "Challenges in evaluating the delivery aspect still require specialized training strategies.", "keywords": ["Multimodal Large Language Models", "Automated Speaking Assessment", "Speech-First Multimodal Training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12630", "pdf": "https://arxiv.org/pdf/2508.12630.pdf", "abs": "https://arxiv.org/abs/2508.12630", "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal"], "categories": ["cs.CL"], "comment": "Paper is currently in peer review", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.", "AI": {"tldr": "The paper introduces Semantic Anchoring, a memory architecture that improves long-term dialogue interactions in LLMs by integrating explicit linguistic structures into vector storage.", "motivation": "Current RAG systems store dialogue history ineffectively, limiting LLMs' performance in multi-session interactions due to their lack of memory persistence and neglect of linguistic structures.", "method": "The proposed approach combines dependency parsing, discourse relation tagging, and coreference resolution to enhance vector-based memory storage with structured entries that include explicit linguistic cues.", "result": "Experiments show that Semantic Anchoring improves factual recall and discourse coherence by up to 18% compared to standard RAG systems.", "conclusion": "Semantic Anchoring enhances LLMs' ability to manage long-term dialogue by adding linguistic context to memory storage, demonstrating potential for improved human-computer interactions.", "key_contributions": ["Introduction of Semantic Anchoring as a hybrid memory architecture for LLMs", "Improved factual recall and coherence in dialogue through enhanced memory structures", "Experimental evidence supporting the effectiveness of the proposed method"], "limitations": "", "keywords": ["Large Language Models", "Memory Architecture", "Human-Computer Interaction", "Natural Language Processing", "Dialogue Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12631", "pdf": "https://arxiv.org/pdf/2508.12631.pdf", "abs": "https://arxiv.org/abs/2508.12631", "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing", "authors": ["Yiqun Zhang", "Hao Li", "Jianhao Chen", "Hangfan Zhang", "Peng Ye", "Lei Bai", "Shuyue Hu"], "categories": ["cs.CL"], "comment": "Ongoing work", "summary": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.", "AI": {"tldr": "Avengers-Pro is a test-time routing framework that dynamically assigns queries to LLMs with varying efficiency and capacity, achieving state-of-the-art results in balancing performance and cost.", "motivation": "To address the central challenge of balancing performance and efficiency in large language model advancement.", "method": "Avengers-Pro dynamically routes queries to the most suitable model based on clustered incoming queries and a performance-efficiency score.", "result": "Achieves state-of-the-art results across 6 benchmarks and 8 models, exceeding the accuracy of the strongest single model by +7% while providing significant cost savings.", "conclusion": "Avengers-Pro consistently yields the best accuracy for a given cost and the lowest cost for a given accuracy, establishing a Pareto frontier.", "key_contributions": ["Development of a unified test-time routing framework for LLMs", "Achieving state-of-the-art performance with cost efficiency", "Establishing a Pareto frontier for performance-cost trade-offs"], "limitations": "", "keywords": ["Large Language Models", "Performance Efficiency", "Test-Time Routing", "Dynamic Query Assignment", "Pareto Frontier"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12632", "pdf": "https://arxiv.org/pdf/2508.12632.pdf", "abs": "https://arxiv.org/abs/2508.12632", "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection", "authors": ["Chi Wang", "Min Gao", "Zongwei Wang", "Junwei Yin", "Kai Shu", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.", "AI": {"tldr": "This paper presents Linguistic Fingerprints Extraction (LIFE), a novel method for detecting fake news generated by large language models (LLMs) based on linguistic features.", "motivation": "The rise of large language models has led to easy generation of fake news, highlighting the urgent need for effective detection methods.", "method": "The proposed method, LIFE, analyzes distributional divergence to extract linguistic fingerprints from LLM-generated content, identifying distinct probability shifts when prompted maliciously.", "result": "LIFE demonstrates state-of-the-art performance in detecting LLM-generated fake news while also performing well on human-written fake news.", "conclusion": "The study emphasizes the potential of linguistic fingerprints in enhancing the reliability of fake news detection and provides accessible code and data for implementation.", "key_contributions": ["Introduction of Linguistic Fingerprints Extraction (LIFE) for fake news detection", "Utilization of distributional divergence analysis to identify linguistic patterns", "Demonstrated state-of-the-art detection performance for LLM-generated and human-written fake news"], "limitations": "", "keywords": ["fake news detection", "language models", "linguistic fingerprints"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12662", "pdf": "https://arxiv.org/pdf/2508.12662.pdf", "abs": "https://arxiv.org/abs/2508.12662", "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models", "authors": ["Tanay Nagar", "Grigorii Khvatskii", "Anna Sokol", "Nitesh V. Chawla"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop", "summary": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations.", "AI": {"tldr": "This paper proposes a method to enhance LLM performance in low-resource languages through fine-tuning on synthetic code-switched datasets.", "motivation": "To address the disparity in LLM performance between high-resource and low-resource languages in Common Sense Reasoning tasks, ensuring fairness in multilingual communication.", "method": "The authors fine-tune an LLM on synthetic code-switched text generated through controlled language-mixing techniques.", "result": "Fine-tuning LLMs on synthetic code-switched datasets significantly improves performance in low-resource language models while maintaining or boosting performance in high-resource languages.", "conclusion": "This approach not only enhances LRL performance but also contributes a new dataset for further research in multilingual LLM applications.", "key_contributions": ["Introduction of a method to improve LLMs for low-resource languages", "Creation of a synthetic code-switched dataset derived from CommonSenseQA", "Demonstration of improved CSR task performance across language resources"], "limitations": "", "keywords": ["Low-resource languages", "Common Sense Reasoning", "Code-switching", "Language models", "Multilingual communication"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12669", "pdf": "https://arxiv.org/pdf/2508.12669.pdf", "abs": "https://arxiv.org/abs/2508.12669", "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery", "authors": ["Bishanka Seal", "Rahul Seetharaman", "Aman Bansal", "Abhilash Nandy"], "categories": ["cs.CL", "cs.CY"], "comment": "14 pages, 4 tables", "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub", "AI": {"tldr": "This study explores using LLMs for predicting misery scores from text, employing various prompting strategies and introducing a gamified evaluation framework.", "motivation": "To predict human-perceived misery scores from natural language descriptions using LLMs.", "method": "The study frames the prediction task as regression and evaluates prompting strategies such as zero-shot, few-shot, and a novel gamified framework called the 'Misery Game Show'.", "result": "Few-shot prompting approaches significantly outperform zero-shot baselines, demonstrating the effectiveness of contextual examples in predicting misery.", "conclusion": "The 'Misery Game Show' framework not only evaluates predictive accuracy but also assesses LLMs' adaptability through corrective feedback, indicating their potential in dynamic emotional reasoning tasks.", "key_contributions": ["Introduction of the 'Misery Game Show' for evaluating LLMs", "Demonstration of few-shot prompting advantages over zero-shot", "Expansion of LLM applications in emotional reasoning beyond simple regression."], "limitations": "", "keywords": ["Large Language Models", "Misery scoring", "Affective computing", "Gamified evaluation"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2508.12685", "pdf": "https://arxiv.org/pdf/2508.12685.pdf", "abs": "https://arxiv.org/abs/2508.12685", "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction", "authors": ["Xingshan Zeng", "Weiwen Liu", "Lingzhi Wang", "Liangyou Li", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios.", "AI": {"tldr": "The paper introduces ToolACE-MT, a Non-Autoregressive Iterative Generation framework for high-quality multi-turn agentic dialogues, addressing limitations of existing simulation-based data generation methods.", "motivation": "To improve the performance of agentic task-solving with LLMs, which currently relies on costly autoregressive interactions.", "method": "The ToolACE-MT framework constructs dialogues through three stages: coarse-grained initialization, iterative refinement, and offline verification.", "result": "Experiments show that ToolACE-MT generates efficient, effective, and generalizable agentic data, enhancing data construction in tool-augmented LLM scenarios.", "conclusion": "ToolACE-MT offers a novel approach for generating high-quality multi-turn dialogues, overcoming limitations of previous methods.", "key_contributions": ["Introduction of ToolACE-MT framework", "Three-stage dialogue construction process", "Enhanced efficiency and effectiveness in data generation"], "limitations": "", "keywords": ["Large Language Models", "agentic task-solving", "data generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12726", "pdf": "https://arxiv.org/pdf/2508.12726.pdf", "abs": "https://arxiv.org/abs/2508.12726", "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning", "authors": ["Weize Liu", "Yongchi Zhao", "Yijia Luo", "Mingyu Xu", "Jiaheng Liu", "Yanan Li", "Xiguo Hu", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.", "AI": {"tldr": "The paper presents DESIGNER, a data synthesis pipeline for generating multidisciplinary reasoning questions using large language models (LLMs) and extensive raw documents.", "motivation": "Existing reasoning datasets lack depth and breadth, leading to insufficient complex, multi-step reasoning capabilities in LLMs across diverse disciplines.", "method": "DEISGNER synthesizes reasoning questions by leveraging LLMs to reverse-engineer over 120,000 design logics from existing questions and matching them with source materials from books and web documents.", "result": "Two large-scale datasets, DLR-Book (3.04 million questions) and DLR-Web (1.66 million questions), were synthesized, showing significantly greater difficulty and diversity than existing datasets, validated by strong SFT performance of LLMs.", "conclusion": "The proposed datasets enhance the reasoning capabilities of LLMs in multidisciplinary contexts by providing more challenging and diverse questions for model training.", "key_contributions": ["Introduction of DESIGNER for data synthesis of complex reasoning questions.", "Creation of two extensive reasoning datasets (DLR-Book and DLR-Web) with high difficulty and diversity.", "Demonstration of improved reasoning performance in LLMs trained on the new datasets."], "limitations": "The methodology relies on the quality of the source documents and may not cover all potential domains adequately.", "keywords": ["large language models", "reasoning dataset", "multidisciplinary", "design logic", "synthesis pipeline"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12733", "pdf": "https://arxiv.org/pdf/2508.12733.pdf", "abs": "https://arxiv.org/abs/2508.12733", "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models", "authors": ["Zhiyuan Ning", "Tianle Gu", "Jiaxin Song", "Shixin Hong", "Lingyu Li", "Huacan Liu", "Jie Li", "Yixu Wang", "Meng Lingyu", "Yan Teng", "Yingchun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "7pages, 5 figures", "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.", "AI": {"tldr": "Introduction of LinguaSafe, a comprehensive multilingual safety benchmark for evaluating large language models (LLMs).", "motivation": "The need for a rigorous approach to ensure the safety of LLMs across diverse linguistic and cultural contexts, addressing the gaps in existing multilingual evaluations.", "method": "Development of the LinguaSafe dataset, which includes 45k entries in 12 languages, using translated, transcreated, and natively-sourced data, along with a multidimensional evaluation framework.", "result": "Safety and helpfulness evaluations show significant variation across different domains and languages, indicating the importance of tailored safety assessments for LLMs.", "conclusion": "LinguaSafe fills a critical gap in the multilingual safety evaluation of LLMs and encourages further research by providing a comprehensive suite of metrics and publicly released data.", "key_contributions": ["LinguaSafe dataset with extensive multilingual entries", "Multidimensional evaluation framework for LLM safety", "Public release of dataset and evaluation code"], "limitations": "", "keywords": ["multilingual safety", "large language models", "safety evaluation"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2508.12769", "pdf": "https://arxiv.org/pdf/2508.12769.pdf", "abs": "https://arxiv.org/abs/2508.12769", "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description", "authors": ["Shaoming Duan", "Zirui Wang", "Chuanyi Liu", "Zhibin Zhu", "Yuhao Zhang", "Peiyi Han", "Liang Yan", "Zewu Penge"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git", "AI": {"tldr": "CRED-SQL is a framework that improves Text-to-SQL systems for large-scale databases by integrating cluster retrieval and an Execution Description Language to mitigate semantic mismatches between natural language questions and SQL queries.", "motivation": "The paper addresses the challenge of semantic mismatch between natural language questions and corresponding SQL queries in large-scale databases, which reduces accuracy.", "method": "CRED-SQL employs cluster-based schema retrieval and introduces an Execution Description Language (EDL) to facilitate the transition from natural language to SQL, decomposing the process into Text-to-EDL and EDL-to-SQL stages.", "result": "CRED-SQL achieves state-of-the-art performance on large-scale cross-domain benchmarks, significantly improving the accuracy of Text-to-SQL systems.", "conclusion": "The framework effectively addresses the semantic mismatch problem, demonstrating effectiveness and scalability in performance.", "key_contributions": ["Introduction of the CRED-SQL framework for improved Text-to-SQL accuracy.", "Development of cluster-based schema retrieval to alleviate semantic mismatch.", "Proposal of the Execution Description Language (EDL) to create an intermediate representation."], "limitations": "", "keywords": ["Text-to-SQL", "Natural Language Processing", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.12774", "pdf": "https://arxiv.org/pdf/2508.12774.pdf", "abs": "https://arxiv.org/abs/2508.12774", "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task", "authors": ["Javier Garcia Gilabert", "Xixian Liao", "Severino Da Dalt", "Ella Bohman", "Audrey Mash", "Francesca De Luca Fornaciari", "Irene Baucells", "Joan Llop", "Miguel Claramunt Argote", "Carlos Escolano", "Maite Melero"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1", "AI": {"tldr": "The paper introduces the SALAMANDRATA family of LLMs aimed at improving translation tasks for 38 European languages, showcasing enhancements over previous models.", "motivation": "The motivation behind developing SALAMANDRATA was to improve translation performance across a wide range of European languages, addressing limitations in existing models.", "method": "The models were trained using a two-step approach: initial continual pre-training on parallel data followed by supervised fine-tuning on high-quality instructions. The BSC submission to the WMT25 task utilized the 7B variant with specific adaptations for vocabulary and additional non-European languages.", "result": "The SALAMANDRATA models, specifically the 7B variant, demonstrated strong translation performance in trials, benefiting from advanced decoding strategies like Minimum Bayes Risk Decoding and Tuned Re-ranking.", "conclusion": "The models are publicly released on Hugging Face, providing access to both the 2B and 7B versions, alongside the SALAMANDRATA-V2 model for further enhancements in translation tasks.", "key_contributions": ["Introduction of SALAMANDRATA models with improved translation capabilities", "Application of quality-aware decoding strategies for enhanced performance", "Public release of multiple model variants on Hugging Face"], "limitations": "", "keywords": ["SALAMANDRATA", "LLM", "translation", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.12778", "pdf": "https://arxiv.org/pdf/2508.12778.pdf", "abs": "https://arxiv.org/abs/2508.12778", "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Zhiyuan Zhu", "Haolin Li", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.", "AI": {"tldr": "The paper presents HeteroRAG, a framework that improves medical large vision-language models by integrating diverse knowledge sources for enhanced retrieval and factual accuracy in clinical applications.", "motivation": "To address factual inaccuracies and unreliable outputs in medical large vision-language models (Med-LVLMs), which pose risks in real-world diagnostics.", "method": "The authors constructed MedAtlas, a comprehensive repository of multimodal reports and text corpora, and introduced HeteroRAG, which utilizes Modality-specific CLIPs and a Multi-corpora Query Generator to improve retrieval processes and knowledge alignment.", "result": "HeteroRAG significantly enhances the performance of Med-LVLMs, achieving state-of-the-art results across 12 datasets and 3 modalities, improving factual accuracy and reliability.", "conclusion": "The proposed approach effectively aids medical decision-making by bridging gaps in knowledge retrieval and aligning multimodal information, resulting in improved clinical outcomes.", "key_contributions": ["Development of MedAtlas, a multimodal report repository and diverse text corpus.", "Introduction of HeteroRAG framework for enhanced retrieval and knowledge alignment.", "Implementation of Modality-specific CLIPs and Multi-corpora Query Generator for effective query construction."], "limitations": "", "keywords": ["medical large vision-language models", "heterogeneous knowledge", "retrieval-augmented generation", "clinical applications", "multimodal integration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12800", "pdf": "https://arxiv.org/pdf/2508.12800.pdf", "abs": "https://arxiv.org/abs/2508.12800", "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward", "authors": ["Yong Deng", "Guoqing Wang", "Zhenzhe Ying", "Xiaofeng Wu", "Jinzhen Lin", "Wenwen Xiong", "Yuqin Dai", "Shuo Yang", "Zhanwei Zhang", "Qiwen Wang", "Yang Qin", "Changhua Meng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.", "AI": {"tldr": "This paper presents Atomic Thought and Atom-Searcher, a new RL framework for LLMs that enhances reasoning capabilities.", "motivation": "The paper addresses limitations of LLMs in complex reasoning tasks and the inefficacies of current RL approaches in fine-grained reasoning.", "method": "The authors propose a decomposition of reasoning tasks into Atomic Thoughts, supervised by RRMs. Atom-Searcher integrates this framework with a curriculum-inspired reward schedule.", "result": "Experiments demonstrate that Atom-Searcher achieves consistent improvements across multiple benchmarks in terms of reasoning capability and computational efficiency.", "conclusion": "The proposed methods enhance the interpretability and effectiveness of LLMs in reasoning tasks by providing structured guidance and improving training efficiency.", "key_contributions": ["Introduction of Atomic Thought paradigm for LLMs", "Development of Atom-Searcher, a RL framework for fine-grained reasoning", "Empirical validation showing improved reasoning capabilities over existing models"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Retrieval-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.12803", "pdf": "https://arxiv.org/pdf/2508.12803.pdf", "abs": "https://arxiv.org/abs/2508.12803", "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models", "authors": ["Ahmed Elshabrawy", "Hour Kaing", "Haiyue Song", "Alham Fikri Aji", "Hideki Tanaka", "Masao Utiyama", "Raj Dabre"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.", "AI": {"tldr": "This paper challenges the assumption that aligning low-resource language models with high-resource varieties improves their performance. It introduces an online variational probing framework to decouple dialects from dominant varieties, improving generative modeling in low-resource dialects of Arabic.", "motivation": "The study addresses the misconception that alignment with high-resource languages always benefits low-resource varieties, exemplified through Arabic dialects and Modern Standard Arabic.", "method": "An online variational probing framework is used to estimate and decouple the internal representations of large language models from dominant varieties during fine-tuning.", "result": "Interventions across 25 Arabic dialects led to an average generation quality improvement of +2.0 and up to +4.9 chrF++, while also indicating tradeoffs in performance on standard languages.", "conclusion": "The paper provides causal evidence that high-resource language dominance can hinder generative capacities in low-resource varieties, offering a unified approach to improve generative modeling methodologies.", "key_contributions": ["Introduction of an online variational probing framework to decouple dialectal representations.", "Causal study demonstrating the negative impact of representational entanglement with high-resource languages.", "Tools for improving generative modeling in multilingual contexts."], "limitations": "Tradeoffs exist in performance on standard languages due to the focus on low-resource dialects.", "keywords": ["language models", "generative modeling", "dialectal MT", "variational probing", "Arabic dialects"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.12819", "pdf": "https://arxiv.org/pdf/2508.12819.pdf", "abs": "https://arxiv.org/abs/2508.12819", "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue", "authors": ["Jeongwoo Kang", "Maria Boritchev", "Maximin Coavoux"], "categories": ["cs.CL"], "comment": "Accepted at IWCS 2025", "summary": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.", "AI": {"tldr": "This paper presents a French semantic corpus annotated in Abstract Meaning Representation (AMR) for spontaneous dialogues.", "motivation": "The paper aims to enhance AMR's representation of spontaneous French dialogue, addressing its current limitations.", "method": "The authors annotate the DinG corpus of spontaneous French dialogues and provide extended guidelines for consistent annotation.", "result": "The released corpus supports the training and evaluation of an AMR parser, which aids in initial annotation processes.", "conclusion": "The developed corpus contributes to the creation of semantic resources for French dialogue analysis.", "key_contributions": ["Development of a French semantic corpus using AMR", "Extension of AMR framework for spontaneous speech", "Provision of annotation guidelines for consistent corpus creation"], "limitations": "", "keywords": ["French", "semantic corpus", "Abstract Meaning Representation", "dialogue annotation", "natural language processing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.12828", "pdf": "https://arxiv.org/pdf/2508.12828.pdf", "abs": "https://arxiv.org/abs/2508.12828", "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection", "authors": ["Raneem Alharthi", "Rajwa Alharthi", "Aiqi Jiang", "Arkaitz Zubiaga"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.", "AI": {"tldr": "This study examines the impact of contextual features from parent tweets on the detection of abusive language in replies, revealing significant performance improvements when context is considered.", "motivation": "To address the limitation of existing abusive language detection models, which largely ignore contextual information from prior posts, aiming to enhance detection accuracy in social media conversations.", "method": "The research analyzes conversational exchanges by comparing models that utilize features from both parent tweets and reply tweets versus those that rely solely on reply tweet features. Four different classification models are tested on a dataset of labeled parent-reply tweet pairs.", "result": "The findings indicate that incorporating contextual features leads to substantial improvements in abusive language detection performance, particularly through the use of a diverse range of content-based features.", "conclusion": "The study underscores the importance of leveraging context in abusive language detection, suggesting that combined content-based features yield better performance than selective feature use.", "key_contributions": ["Introduces a context-aware approach to abusive language detection in social media.", "Demonstrates significant performance improvements by incorporating parent tweet context.", "Identifies the superiority of content-based features over account-based features in classification tasks."], "limitations": "The study is limited to specific datasets and social media platforms, and results may not generalize universally across all types of conversations or languages.", "keywords": ["abusive language detection", "contextual features", "social media", "machine learning", "conversational exchanges"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.12830", "pdf": "https://arxiv.org/pdf/2508.12830.pdf", "abs": "https://arxiv.org/abs/2508.12830", "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae", "authors": ["Jan Maliszewski"], "categories": ["cs.CL"], "comment": null, "summary": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.", "AI": {"tldr": "This paper applies stylometric techniques to analyze Stephen Langton's Quaestiones Theologiae, revealing editorial layers and validating hypotheses about its formation through the use of HTR and stylometric analysis.", "motivation": "To uncover layers of editorial work in the literary production based on oral teaching records and validate hypotheses about the formation of Stephen Langton's Quaestiones Theologiae.", "method": "The study applies stylometric techniques, including frequency analysis of words, POS tagging, and pseudo-affix examination, alongside an HTR pipeline to analyze stylometric data from the text.", "result": "The study aims to directly compare performance on manually composed and automatically extracted data, testing the efficacy of transformer-based OCR in the context of scholastic Latin corpora.", "conclusion": "If successful, the study will provide a reusable template for analyzing collaborative literary production from medieval universities, enhancing computational research methodologies.", "key_contributions": ["Application of stylometric techniques to historical texts.", "Validation of hypotheses regarding the formation of medieval literary collections.", "Methodological comparison between manual and automatic data extraction."], "limitations": "", "keywords": ["stylometry", "text analysis", "medieval literature", "authorship attribution", "OCR"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2508.12863", "pdf": "https://arxiv.org/pdf/2508.12863.pdf", "abs": "https://arxiv.org/abs/2508.12863", "title": "Word Meanings in Transformer Language Models", "authors": ["Jumbly Grindrod", "Peter Grindrod"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.", "AI": {"tldr": "This paper studies the representation of word meanings in transformer language models, particularly focusing on RoBERTa-base, and finds that semantic information is well encoded in its token embedding space.", "motivation": "To understand how transformer models represent word meanings and whether they maintain a lexical store of semantic information.", "method": "Extracted the token embedding space of RoBERTa-base and clustered it into 200 groups using k-means clustering; manually inspected clusters for semantic sensitivity and tested against psycholinguistic measures.", "result": "Clusters contained a wide variety of semantic information, indicating transformer models encode meaningful similarities between words.", "conclusion": "The findings challenge certain hypotheses suggesting that transformer models do not effectively represent semantic information.", "key_contributions": ["Demonstrated a successful methodology for analyzing semantic representation in LLMs", "Provided evidence against meaning eliminativist theories in the context of transformer models", "Identified the linkage between embedding clusters and psycholinguistic measures."], "limitations": "", "keywords": ["transformer models", "semantic representation", "RoBERTa", "token embedding", "psycholinguistics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.12868", "pdf": "https://arxiv.org/pdf/2508.12868.pdf", "abs": "https://arxiv.org/abs/2508.12868", "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.", "AI": {"tldr": "This paper presents an LLM-based agent for the Semantic Table Annotation (STA) task, addressing challenges in Column Type Annotation (CTA) and Cell Entity Annotation (CEA) with tailored prompts and external tools, significantly improving annotation accuracy and efficiency.", "motivation": "The STA task is crucial for semantic applications but faces challenges like semantic loss and strict ontological requirements that lower annotation accuracy.", "method": "An LLM-based agent is designed with five external tools that use tailored prompts based on the ReAct framework, allowing dynamic selection of annotation strategies based on table characteristics.", "result": "The proposed method outperforms existing annotation approaches across multiple metrics and achieves a 70% reduction in time costs and a 60% reduction in LLM token usage.", "conclusion": "The study presents an efficient and cost-effective solution for the STA task, enhancing annotation accuracy while reducing resource usage.", "key_contributions": ["Introduction of an LLM-based agent for STA tasks with dynamic strategy selection.", "Implementation of tailored prompt engineering based on the ReAct framework.", "Empirical results demonstrating significant improvements over current methods in efficiency and accuracy."], "limitations": "Does not address the generalization of the approach to all possible table formats or external data sources.", "keywords": ["semantic table annotation", "column type annotation", "cell entity annotation", "large language models", "prompt engineering"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.12903", "pdf": "https://arxiv.org/pdf/2508.12903.pdf", "abs": "https://arxiv.org/abs/2508.12903", "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models", "authors": ["Jinyi Han", "Xinyi Wang", "Haiquan Zhao", "Tingyun li", "Zishang Jiang", "Sihang Jiang", "Jiaqing Liang", "Xin Lin", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.", "AI": {"tldr": "ProActive Self-Refinement (PASR) improves LLM output through iterative refinement during generation based on internal state and context.", "motivation": "To address limitations in existing self-refinement methods that use a reactive process with a fixed number of iterations, making refinement timing and content difficult to optimize.", "method": "ProActive Self-Refinement (PASR) allows LLMs to refine outputs dynamically during the generation process, deciding when and how to refine based on context.", "result": "PASR outperforms standard generation methods, reducing average token consumption by 41.6% and improving accuracy by 8.2% on Qwen3-8B across 10 tasks.", "conclusion": "PASR effectively enhances problem-solving performance in LLMs by allowing dynamic output refinement during generation.", "key_contributions": ["Introduction of ProActive Self-Refinement (PASR) methodology", "Demonstrated significant reductions in token consumption and improvements in accuracy", "Open-source code and baseline availability for further research"], "limitations": "", "keywords": ["self-refinement", "large language models", "contextual generation", "dynamic refinement", "machine learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.12981", "pdf": "https://arxiv.org/pdf/2508.12981.pdf", "abs": "https://arxiv.org/abs/2508.12981", "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning", "authors": ["Tianyue Ou", "Saujas Vaduguru", "Daniel Fried"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.", "AI": {"tldr": "The study presents an LLM-based multi-agent system for travel planning, improving task performance through structured information sharing and orchestration.", "motivation": "To address the challenges of long-horizon, multi-constraint planning tasks that require detailed information and coordination among agents.", "method": "An LLM-based multi-agent system was constructed and evaluated using a notebook for information sharing and an orchestrator for improving coordination among agents during free form conversation.", "result": "The implementation of the notebook reduced errors from hallucinated details by 18%, and the orchestrator helped direct the MAS to further reduce errors by 13.5%, achieving a final pass rate of 25% on the TravelPlanner benchmark, significantly outperforming the single-agent baseline.", "conclusion": "Structured information sharing and reflective orchestration are crucial for enhancing the performance of multi-agent systems in complex planning tasks with LLMs.", "key_contributions": ["Introduction of a notebook for information sharing among LLM agents", "Development of an orchestrator agent for improved coordination", "Demonstration of a significant performance increase in long-horizon planning tasks"], "limitations": "", "keywords": ["multi-agent systems", "large language models", "travel planning", "information sharing", "orchestration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.13024", "pdf": "https://arxiv.org/pdf/2508.13024.pdf", "abs": "https://arxiv.org/abs/2508.13024", "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents", "authors": ["Ralph Peeters", "Aaron Steiner", "Luca Schwarz", "Julian Yuya Caspary", "Christian Bizer"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.", "AI": {"tldr": "WebMall is a benchmark for evaluating LLM-based web agents on multi-shop comparison-shopping tasks, featuring 91 tasks across four simulated online shops.", "motivation": "To automate long-running web tasks in e-commerce, such as comparison shopping, using LLM-based web agents.", "method": "Introduction of WebMall, which includes four simulated online shops and 91 cross-shop tasks for evaluating web agents in comparison-shopping.", "result": "Baseline agents demonstrated completion rates of 75% and 53%, and F1 scores of 87% and 63% for basic and advanced tasks, respectively.", "conclusion": "WebMall enables research on web agents, enhancing navigation, reasoning, and efficiency in e-commerce, and is publicly accessible for further study.", "key_contributions": ["Introduction of a new benchmark for comparison-shopping tasks (WebMall)", "Evaluation of multiple baseline agents and their performance", "Contribution of authentic product offers from real-world shops"], "limitations": "", "keywords": ["LLM", "web agents", "e-commerce", "benchmark", "comparison-shopping"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.13028", "pdf": "https://arxiv.org/pdf/2508.13028.pdf", "abs": "https://arxiv.org/abs/2508.13028", "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Devraj Raghuvanshi", "Nagendra Kumar", "Shekhar Nayak", "Matt Coler"], "categories": ["cs.CL"], "comment": "Speech Synthesis Workshop 2025", "summary": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.", "AI": {"tldr": "This paper presents a novel approach for synthesizing sarcastic speech by integrating a bi-modal sarcasm detection model into the TTS training process, improving the model's sarcasm awareness through a two-stage fine-tuning process.", "motivation": "Enhancing natural interactions in applications like entertainment and human-computer interaction through effective sarcastic speech synthesis.", "method": "The approach incorporates feedback loss from a bi-modal sarcasm detection model into the TTS training process and uses transfer learning with two stages of fine-tuning a pre-trained speech synthesis model.", "result": "The proposed methods improve the quality, naturalness, and sarcasm-awareness of synthesized speech based on objective and subjective evaluations.", "conclusion": "By refining a speech synthesis model in a two-stage process, the study successfully enhances its ability to generate sarcasm-aware speech, addressing challenges related to sarcastic speech synthesis.", "key_contributions": ["Integration of feedback loss from sarcasm detection into TTS training.", "Two-stage fine-tuning process for sarcasm synthesis.", "Improvement of quality and naturalness in synthesized sarcastic speech."], "limitations": "", "keywords": ["sarcastic speech synthesis", "human-computer interaction", "transfer learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.13037", "pdf": "https://arxiv.org/pdf/2508.13037.pdf", "abs": "https://arxiv.org/abs/2508.13037", "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction", "authors": ["Xinhe Li", "Jiajun Liu", "Peng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.", "AI": {"tldr": "The paper introduces LoRID, a novel method enhancing mathematical reasoning in Small Language Models (SLMs) by utilizing a two-mode reasoning approach inspired by human thinking processes.", "motivation": "LLMs excel in mathematical reasoning but are resource-heavy. The paper aims to improve SLMs' reasoning capabilities by mimicking human thought processes.", "method": "LoRID employs multi-LoRA Interaction: it generates datasets with LLMs, trains an Intuitive Reasoner for generating Chain-of-Thoughts, and develops a Knowledge Generator and Deep Reasoner for knowledge use and reasoning, incorporating mutual feedback to enhance consistency.", "result": "LoRID achieves state-of-the-art performance on the GSM8K dataset, outperforming the nearest competitor by notable accuracy improvements across various models.", "conclusion": "The method effectively enhances the reasoning abilities of SLMs through its structured approach to mimicking human thinking.", "key_contributions": ["Introduced LoRID for enhancing SLMs' reasoning abilities.", "Utilized a two-mode reasoning approach analogous to human thinking.", "Achieved state-of-the-art results on the GSM8K dataset."], "limitations": "", "keywords": ["Large Language Models", "Small Language Models", "mathematical reasoning", "multi-LoRA Interaction", "knowledge generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.13044", "pdf": "https://arxiv.org/pdf/2508.13044.pdf", "abs": "https://arxiv.org/abs/2508.13044", "title": "BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans DeÄerlendirmesi, Zorluklar ve Ä°yileÅtirme FÄ±rsatlarÄ±", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih GÃ¼mÃ¼Å", "Banu Diri", "SavaÅ YÄ±ldÄ±rÄ±m", "Ãner AytaÅ"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye", "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.", "AI": {"tldr": "Introduction of the Turkish MMLU benchmark for evaluating LLMs in Turkish.", "motivation": "To address the challenge of evaluating language models in resource-limited languages, specifically Turkish.", "method": "Development of a benchmark called TR-MMLU featuring 6,200 multiple-choice questions across 62 sections of the Turkish education system.", "result": "State-of-the-art LLMs were evaluated on TR-MMLU, revealing areas that need improvement in model design.", "conclusion": "TR-MMLU establishes a new standard in Turkish NLP research, paving the way for future innovations.", "key_contributions": ["Introduction of TR-MMLU, a new evaluation benchmark for Turkish", "Curated dataset of 6,200 questions for assessing LLMs", "Highlights the current capabilities and limitations of LLMs in Turkish"], "limitations": "", "keywords": ["Turkish NLP", "Language models", "TR-MMLU"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.13058", "pdf": "https://arxiv.org/pdf/2508.13058.pdf", "abs": "https://arxiv.org/abs/2508.13058", "title": "DoÄal Dil Ä°Ålemede Tokenizasyon StandartlarÄ± ve ÃlÃ§Ã¼mÃ¼: TÃ¼rkÃ§e Ãzerinden BÃ¼yÃ¼k Dil Modellerinin KarÅÄ±laÅtÄ±rmalÄ± Analizi", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih GÃ¼mÃ¼Å", "Sercan KarakaÅ", "Banu Diri", "SavaÅ YÄ±ldÄ±rÄ±m"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye", "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.", "AI": {"tldr": "The paper presents a novel evaluation framework for assessing tokenization effectiveness in morphologically-rich languages, focused on Turkish, revealing that language-specific tokenization significantly affects NLP model performance.", "motivation": "To address the challenges of tokenization in morphologically-rich and low-resource languages, which are crucial for the performance of language models.", "method": "An evaluation framework was developed using the Turkish MMLU dataset, analyzing tokenizers based on vocabulary size, token count, processing time, language-specific token percentages, and token purity.", "result": "The study found that language-specific token percentages correlate more strongly with downstream performance than token purity, and increasing model parameters does not necessarily improve linguistic performance.", "conclusion": "Tailored, language-specific tokenization methods are essential for enhancing the performance of language models on morphologically complex languages.", "key_contributions": ["Proposed new evaluation metrics for tokenization effectiveness in Turkish and similar languages.", "Demonstrated the significant impact of language-specific tokenization on NLP model performance.", "Established standards for tokenization in morphologically-rich languages."], "limitations": "", "keywords": ["Tokenization", "Natural Language Processing", "Morphologically-rich languages", "Turkish", "Language-specific metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.13060", "pdf": "https://arxiv.org/pdf/2508.13060.pdf", "abs": "https://arxiv.org/abs/2508.13060", "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database", "authors": ["John Alderete", "Macarious Kin Fung Hui", "Aanchan Mohan"], "categories": ["cs.CL"], "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)", "summary": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.", "AI": {"tldr": "The SFUSED database is a resource for testing speech recognition models, comprising annotated speech errors that help evaluate ASR systems.", "motivation": "To provide a public database for linguistic and psycholinguistic research that also serves as a tool for assessing speech recognition models.", "method": "The study involves evaluating WhisperX's transcription accuracy using 5,300 documented speech errors from the SFUSED database.", "result": "The evaluation demonstrates the effectiveness of the SFUSED database as a diagnostic tool for assessing ASR system performance.", "conclusion": "The SFUSED database's annotations are beneficial for evaluating speech recognition models and understanding speech errors.", "key_contributions": ["Introduction of the SFUSED database as a research tool", "Systematic annotation scheme for speech errors", "Demonstration of the database's utility in assessing ASR performance"], "limitations": "", "keywords": ["Speech Recognition", "Speech Errors", "Linguistic Annotation", "ASR Evaluation", "WhisperX"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.13070", "pdf": "https://arxiv.org/pdf/2508.13070.pdf", "abs": "https://arxiv.org/abs/2508.13070", "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning", "authors": ["Long Ma", "Fangwei Zhong", "Yizhou Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.", "AI": {"tldr": "This paper introduces Reinforced Context Order Recovery (ReCOR), a framework using reinforcement learning to enhance token generation order in language models.", "motivation": "Current causal and diffusion models struggle with token generation that requires adaptive ordering, which can hinder performance on complex tasks.", "method": "ReCOR employs reinforcement learning to extract optimal token generation orders from text data, utilizing self-supervised token prediction statistics.", "result": "Experiments show ReCOR outperforms baseline models on reasoning and planning tasks, even exceeding models trained with true token orders.", "conclusion": "ReCOR presents a significant advancement in token generation methodologies for language models, making them more effective in complex problem-solving scenarios.", "key_contributions": ["Introduction of a novel framework (ReCOR) for adaptive token generation order", "Utilization of reinforcement learning without requiring annotated data", "Demonstrated superior performance on challenging datasets compared to baseline models"], "limitations": "", "keywords": ["Causal Language Models", "Reinforcement Learning", "Token Generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.13079", "pdf": "https://arxiv.org/pdf/2508.13079.pdf", "abs": "https://arxiv.org/abs/2508.13079", "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset", "authors": ["DayyÃ¡n O'Brien", "Bhavitvya Malik", "Ona de Gibert", "Pinzhen Chen", "Barry Haddow", "JÃ¶rg Tiedemann"], "categories": ["cs.CL"], "comment": null, "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.", "AI": {"tldr": "DocHPLT is the largest publicly available dataset for document-level machine translation, containing 124 million aligned document pairs across 50 languages, aimed at enhancing multilingual translation and long-context modeling.", "motivation": "To address the scarcity of document-level translation resources for many languages, particularly under-resourced ones, and to improve long-context modeling capabilities.", "method": "Modified a web extraction pipeline to create a dataset that preserves complete document integrity with 124 million document pairs and conducted experiments to identify optimal training strategies for document-level translation.", "result": "LLMs fine-tuned on the DocHPLT dataset significantly outperform baseline models, especially for under-resourced languages.", "conclusion": "This dataset offers valuable resources for advancing multilingual document-level translation and is open-sourced under a permissive license.", "key_contributions": ["Creation of DocHPLT, a large-scale document-level translation dataset", "Demonstrated performance improvements of LLMs for under-resourced languages", "Open-sourcing the dataset for community use"], "limitations": "", "keywords": ["document-level translation", "multilingual dataset", "long-context modeling"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.13107", "pdf": "https://arxiv.org/pdf/2508.13107.pdf", "abs": "https://arxiv.org/abs/2508.13107", "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad"], "categories": ["cs.CL", "cs.IR", "F.2.2, H.3.3, I.2.7"], "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.", "AI": {"tldr": "This paper presents an enhanced Retrieval-Augmented Generation (RAG) pipeline for legal research that features a context-aware query translator, improved retrieval strategies, and a comprehensive evaluation framework for better performance and reliability.", "motivation": "To improve the reliability of large language model outputs in the legal domain by mitigating hallucinations through grounding in cited sources.", "method": "An end-to-end RAG pipeline with a context-aware query translator, open-source retrieval strategies utilizing SBERT and GTE embeddings, and a comprehensive evaluation framework combining RAGAS, BERTScore-F1, and ROUGE-Recall.", "result": "Substantial improvements in retrieval performance metrics (Recall@K by 30-95% and Precision@K by ~2.5x for K>4) were achieved, alongside more faithful and contextually relevant answers compared to baseline methods.", "conclusion": "Carefully designed open-source pipelines can outperform proprietary ones in retrieval quality, highlighting the value of task-aware tuning for legal research assistance.", "key_contributions": ["Context-aware query translator", "Cost-efficient open-source retrieval strategies", "Comprehensive evaluation framework"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "legal domain", "open-source", "query translation", "RAG systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.13118", "pdf": "https://arxiv.org/pdf/2508.13118.pdf", "abs": "https://arxiv.org/abs/2508.13118", "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation", "authors": ["Zefang Liu", "Arman Anwar"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.", "AI": {"tldr": "AutoBnB-RAG enhances incident response simulations using retrieval-augmented generation to improve decision-making in cyber threat scenarios.", "motivation": "To address the limitations of large language models in incident response due to their lack of external knowledge access.", "method": "The AutoBnB-RAG framework incorporates retrieval-augmented generation into multi-agent simulations of incident response, testing across various team structures and configurations.", "result": "Retrieval augmentation significantly improves decision quality and success rates in simulating real-world cyber incidents and multi-stage attacks.", "conclusion": "Integrating retrieval mechanisms into LLM-based systems enhances their effectiveness in cybersecurity decision-making.", "key_contributions": ["Introduction of AutoBnB-RAG framework", "Utilization of retrieval settings with curated documentation and incident reports", "Evaluation across multiple team configurations for critical reasoning"], "limitations": "", "keywords": ["incident response", "large language models", "retrieval-augmented generation", "cybersecurity", "multi-agent systems"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.13124", "pdf": "https://arxiv.org/pdf/2508.13124.pdf", "abs": "https://arxiv.org/abs/2508.13124", "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries", "authors": ["Kawin Mayilvaghanan", "Siddhant Gupta", "Ayush Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.", "AI": {"tldr": "This paper introduces BlindSpot, a framework to identify and quantify operational biases in LLM-generated summaries of contact center call transcripts.", "motivation": "To explore and quantify the previously unexplored operational biases in contact center operations related to LLM-generated summaries.", "method": "BlindSpot uses a taxonomy of 15 operational bias dimensions and employs an LLM as a zero-shot classifier to quantify biases via two metrics: Fidelity Gap and Coverage.", "result": "An empirical study with 2500 call transcripts showed that systemic biases are present in summaries generated by various LLMs, regardless of model size or type.", "conclusion": "Operational biases in generated summaries highlight the need for better understanding and improvement of LLMs in contact centers.", "key_contributions": ["Introduction of the BlindSpot framework for operational bias analysis", "Development of metrics for bias quantification (Fidelity Gap and Coverage)", "Empirical findings demonstrating systemic biases across LLMs."], "limitations": "The scope is limited to operational biases and may not encompass all potential biases in LLM applications.", "keywords": ["Abstractive Summarization", "Operational Bias", "Large Language Models", "Bias Quantification", "Contact Centers"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.13130", "pdf": "https://arxiv.org/pdf/2508.13130.pdf", "abs": "https://arxiv.org/abs/2508.13130", "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation", "authors": ["Kareem Elozeiri", "Mervat Abassy", "Preslav Nakov", "Yuxia Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset.", "AI": {"tldr": "This paper introduces MuDRiC, the first multi-dialect Arabic commonsense reasoning dataset, and a method using Graph Convolutional Networks for improved commonsense validation in Arabic.", "motivation": "The study aims to enhance commonsense validation in Arabic, addressing the underrepresentation of dialects in existing resources, which primarily focus on Modern Standard Arabic.", "method": "The authors introduce a novel method that adapts Graph Convolutional Networks (GCNs) to improve the modeling of semantic relationships for commonsense reasoning in Arabic.", "result": "Experimental results show that the proposed method significantly improves performance in commonsense validation tasks for Arabic compared to existing approaches.", "conclusion": "The work provides a foundational dataset and a novel method for commonsense reasoning in Arabic, contributing to enhanced natural language understanding in the language.", "key_contributions": ["Introduction of MuDRiC, the first multi-dialect Arabic commonsense reasoning dataset.", "Adaptation of Graph Convolutional Networks for commonsense reasoning in Arabic."], "limitations": "", "keywords": ["Arabic", "commonsense reasoning", "Graph Convolutional Networks", "natural language understanding", "dialects"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.13131", "pdf": "https://arxiv.org/pdf/2508.13131.pdf", "abs": "https://arxiv.org/abs/2508.13131", "title": "Improving Detection of Watermarked Language Models", "authors": ["Dara Bahri", "John Wieting"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.", "AI": {"tldr": "The paper explores hybrid watermark detection schemes to improve the identification of large language model generations, overcoming entropy limitations in watermarking alone.", "motivation": "To address the challenges in detecting generations of large language models (LLMs) using watermarking techniques, especially under conditions of limited entropy.", "method": "The authors investigate various hybrid schemes that integrate watermark detectors with non-watermark ones to assess performance improvements.", "result": "The hybrid schemes demonstrated performance gains over individual classes of detectors across a wide range of experimental setups.", "conclusion": "Combining watermarking and non-watermarking detection methods is a promising approach to enhancing the detection of LLM generations.", "key_contributions": ["Investigation of hybrid detection schemes combining watermark and non-watermark techniques.", "Empirical validation showing performance improvements under various conditions. ", "Highlighting the limitations of traditional watermarking related to entropy.", "Proposing a novel approach to improve LLM generation detection efficiency."], "limitations": "The study's effectiveness may vary based on specific conditions and the nature of the input prompts used.", "keywords": ["watermarking", "large language models", "detection", "hybrid schemes", "entropy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.13141", "pdf": "https://arxiv.org/pdf/2508.13141.pdf", "abs": "https://arxiv.org/abs/2508.13141", "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs", "authors": ["Pranjal Aggarwal", "Seungone Kim", "Jack Lanchantin", "Sean Welleck", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 6 tables, 10 figures", "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.", "AI": {"tldr": "The paper presents OptimalThinkingBench, a benchmark to evaluate and improve the balance of overthinking and underthinking in large language models (LLMs).", "motivation": "The performance of LLMs varies significantly between thinking and non-thinking variants, leading to inefficiencies in their application for complex and simple tasks.", "method": "The benchmark includes two components: OverthinkingBench for simple queries in various domains and UnderthinkingBench with challenging reasoning tasks. It uses thinking-adjusted accuracy metrics for evaluation of LLMs.", "result": "Evaluation of 33 models shows that none can optimally balance thinking and efficiency, with thinking models often overthinking and non-thinking models underthinking, leading to performance issues.", "conclusion": "The study emphasizes the necessity for better-designed models that can achieve optimal thinking rather than just segregating models based on their thinking capabilities.", "key_contributions": ["Introduction of OptimalThinkingBench for joint evaluation of LLMs' thinking capacities.", "Discovery that current models fail to balance overthinking and underthinking effectively.", "Identification of the need for unified models that perform well across various tasks."], "limitations": "Limited to 33 models; identified issues in creating a universally optimal thinking model.", "keywords": ["Large Language Models", "Benchmarking", "Cognitive Performance", "Machine Learning", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2508.13144", "pdf": "https://arxiv.org/pdf/2508.13144.pdf", "abs": "https://arxiv.org/abs/2508.13144", "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation", "authors": ["David Heineman", "Valentin Hofmann", "Ian Magnusson", "Yuling Gu", "Noah A. Smith", "Hannaneh Hajishirzi", "Kyle Lo", "Jesse Dodge"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.", "AI": {"tldr": "This paper examines the reliability of evaluation benchmarks for large language models and introduces metrics to improve their design.", "motivation": "The cost of developing large language models necessitates reliable benchmarks to make informed decisions based on small experiments.", "method": "The paper analyzes the signal-to-noise ratio of benchmarks, introduces new metrics for evaluating benchmarks, and tests interventions to improve signal and reduce noise across 30 benchmarks and 375 language models.", "result": "Benchmarks with a better signal-to-noise ratio are shown to be more reliable, resulting in decreased prediction error in scaling laws and more accurate evaluations.", "conclusion": "Creating benchmarks with high signal and low noise is essential for better decision-making in model development.", "key_contributions": ["Introduction of signal and noise metrics for evaluating benchmarks", "Recommendations for improving benchmark design", "Creation of a new publicly available dataset of 900K benchmark results"], "limitations": "", "keywords": ["large language models", "evaluation benchmarks", "signal-to-noise ratio"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.13152", "pdf": "https://arxiv.org/pdf/2508.13152.pdf", "abs": "https://arxiv.org/abs/2508.13152", "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns", "authors": ["Xin Chen", "Junchao Wu", "Shu Yang", "Runzhe Zhan", "Zeyu Wu", "Ziyang Luo", "Di Wang", "Min Yang", "Lidia S. Chao", "Derek F. Wong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version", "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard", "AI": {"tldr": "Proposes RepreGuard, a method for detecting LLM-generated texts using internal representations for improved robustness.", "motivation": "To address the inadequacies of existing detection methods for large language model output, specifically in out-of-distribution scenarios.", "method": "Utilizes a surrogate model to collect and analyze representations of LLM-generated texts and human-written texts, extracting distinct activation features for improved classification.", "result": "RepreGuard achieves an average AUROC of 94.92% across both in-distribution and out-of-distribution scenarios, demonstrating resilience to varying text sizes and attacks.", "conclusion": "RepreGuard presents a robust and effective approach for distinguishing between LLM-generated and human-written texts, outperforming existing methodologies.", "key_contributions": ["Introduction of RepreGuard, a new detection method leveraging LLM internal representations.", "Demonstration of significant differences in neural activation patterns between LGTs and HWTs.", "Proven robust performance across various conditions with high AUROC scores."], "limitations": "", "keywords": ["large language models", "detection methods", "neural representations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2310.10679", "pdf": "https://arxiv.org/pdf/2310.10679.pdf", "abs": "https://arxiv.org/abs/2310.10679", "title": "Large language models can replicate cross-cultural differences in personality", "authors": ["PaweÅ Niszczota", "Mateusz Janczak", "MichaÅ Misiak"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4.2; J.4"], "comment": "27 pages: 12 pages of manuscript + 15 pages of supplementary\n  materials; in V3 added information that this is the Author Accepted\n  Manuscript version; in V4 license changed to CC-BY", "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners.", "AI": {"tldr": "The paper explores whether GPT-4 can accurately replicate cross-cultural personality differences between US and South Korean individuals using the Big Five personality traits measured by a Ten-Item Inventory.", "motivation": "To investigate the ability of large language models (LLMs) like GPT-4 in replicating established psychological findings regarding cross-cultural personality differences.", "method": "A large-scale experiment involving 8000 participants where personality traits were assessed using the Ten-Item Personality Inventory, with variables manipulated including cultural context (US vs. South Korea), language (English vs. Korean), and language model type (GPT-4 vs GPT-3.5).", "result": "GPT-4 successfully replicated the cross-cultural personality differences; however, mean ratings showed an upward bias and lower variation compared to human samples, alongside reduced structural validity.", "conclusion": "LLMs like GPT-4 can serve as useful tools for cross-cultural research, despite certain biases and limitations in replicating human personality assessments.", "key_contributions": ["Demonstrated GPT-4's capability to replicate cross-cultural personality traits.", "Identified the biases and limitations in the model's output compared to human data.", "Provided preliminary evidence for the utility of LLMs in psychological research."], "limitations": "The model exhibited biases in mean ratings and lower structural validity when compared to human assessments.", "keywords": ["GPT-4", "cross-cultural psychology", "Big Five personality", "LLMs", "Ten-Item Personality Inventory"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2405.11430", "pdf": "https://arxiv.org/pdf/2405.11430.pdf", "abs": "https://arxiv.org/abs/2405.11430", "title": "MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation", "authors": ["Jianbo Dai", "Jianqiao Lu", "Yunlong Feng", "Guangtao Zeng", "Rongju Ruan", "Ming Cheng", "Dong Huang", "Haochen Tan", "Zhijiang Guo"], "categories": ["cs.CL"], "comment": "43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP.", "AI": {"tldr": "This paper introduces the Mostly Hard Python Problems (MHPP) dataset to better assess function-level code generation capabilities of large language models (LLMs).", "motivation": "Existing benchmarks for evaluating LLMs in code generation are inadequate due to issues in quality, difficulty, and granularity.", "method": "The study introduces a new dataset, MHPP, consisting of 210 human-curated problems focusing on natural language and code reasoning to evaluate LLM capabilities.", "result": "Initial evaluations showed that many high-performing models on existing benchmarks like HumanEval did not perform as well on MHPP, revealing previously unknown limitations.", "conclusion": "The MHPP dataset can enhance understanding of LLM capabilities and limitations in code generation.", "key_contributions": ["Introduction of the MHPP dataset for evaluating LLMs' code generation capabilities.", "Identification of limitations in existing benchmarks like HumanEval and MBPP.", "Initial evaluation results demonstrating varying performances of LLMs across different datasets."], "limitations": "The dataset may require further validation and does not cover all potential code generation scenarios.", "keywords": ["large language models", "code generation", "MHPP", "HumanEval", "evaluation benchmark"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2406.05328", "pdf": "https://arxiv.org/pdf/2406.05328.pdf", "abs": "https://arxiv.org/abs/2406.05328", "title": "FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models", "authors": ["Yanling Wang", "Haoyang Li", "Hao Zou", "Jing Zhang", "Xinlei He", "Qi Li", "Ke Xu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite advancements in large language models (LLMs), non-factual responses\nstill persist in fact-seeking question answering. Unlike extensive studies on\npost-hoc detection of these responses, this work studies non-factuality\nprediction (NFP), predicting whether an LLM will generate a non-factual\nresponse prior to the response generation. Previous NFP methods have shown\nLLMs' awareness of their knowledge, but they face challenges in terms of\nefficiency and transferability. In this work, we propose a lightweight model\nnamed Factuality Lens (FacLens), which effectively probes hidden\nrepresentations of fact-seeking questions for the NFP task. Moreover, we\ndiscover that hidden question representations sourced from different LLMs\nexhibit similar NFP patterns, enabling the transferability of FacLens across\ndifferent LLMs to reduce development costs. Extensive experiments highlight\nFacLens's superiority in both effectiveness and efficiency.", "AI": {"tldr": "The paper introduces Factuality Lens (FacLens), a lightweight model designed to predict non-factual responses in large language models, enhancing efficiency and transferability across models.", "motivation": "To address the issue of non-factual responses in fact-seeking question answering by predicting non-factuality prior to response generation.", "method": "The study proposes FacLens, which probes hidden representations of fact-seeking questions to predict non-factual responses.", "result": "FacLens demonstrates superior effectiveness and efficiency in predicting non-factual responses compared to previous methods.", "conclusion": "The findings indicate that FacLens is a promising tool for improving the reliability of LLMs in generating factual responses and can easily adapt across different LLMs.", "key_contributions": ["Introduction of Factuality Lens (FacLens) for non-factuality prediction", "Demonstration of hidden representations' transferability across LLMs", "Improved efficiency and effectiveness in predicting non-factual responses."], "limitations": "", "keywords": ["large language models", "non-factuality prediction", "Factuality Lens", "NLP", "transferability"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2409.09866", "pdf": "https://arxiv.org/pdf/2409.09866.pdf", "abs": "https://arxiv.org/abs/2409.09866", "title": "S2Cap: A Benchmark and a Baseline for Singing Style Captioning", "authors": ["Hyunjong Ok", "Jaeho Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "CIKM 2025 Resource Paper", "summary": "Singing voices contain much richer information than common voices, including\nvaried vocal and acoustic properties. However, current open-source audio-text\ndatasets for singing voices capture only a narrow range of attributes and lack\nacoustic features, leading to limited utility towards downstream tasks, such as\nstyle captioning. To fill this gap, we formally define the singing style\ncaptioning task and present S2Cap, a dataset of singing voices with detailed\ndescriptions covering diverse vocal, acoustic, and demographic characteristics.\nUsing this dataset, we develop an efficient and straightforward baseline\nalgorithm for singing style captioning. The dataset is available at\nhttps://zenodo.org/records/15673764.", "AI": {"tldr": "This paper introduces S2Cap, a new dataset for singing voices that enhances vocal style captioning by capturing diverse attributes and features.", "motivation": "Current audio-text datasets for singing voices lack comprehensive acoustic features and varied attributes, limiting their effectiveness in tasks like style captioning.", "method": "We define the singing style captioning task and present the S2Cap dataset, which includes detailed descriptions of singing voices across various dimensions, followed by the development of a baseline algorithm for captioning.", "result": "S2Cap provides a richer dataset for singing voices, enabling improved performance in style captioning tasks compared to existing datasets.", "conclusion": "The introduction of the S2Cap dataset significantly enhances the analysis and understanding of singing voices for machine learning applications in audio tasks, especially style captioning.", "key_contributions": ["Introduction of the S2Cap dataset for singing voices", "Formal definition of the singing style captioning task", "Development of a baseline algorithm for style captioning"], "limitations": "", "keywords": ["Singing Voices", "Audio Datasets", "Style Captioning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2409.11041", "pdf": "https://arxiv.org/pdf/2409.11041.pdf", "abs": "https://arxiv.org/abs/2409.11041", "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "Accepted to ITL4HRI workshop at RO-MAN 2025 conference", "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).", "AI": {"tldr": "The paper investigates the use of Large Language Models (LLMs) for conversational code generation to aid collaborative robots (cobots) in repetitive assembly tasks, aiming to improve interaction through natural language instructions.", "motivation": "The traditional programming of collaborative robots is limited in expressivity and adaptability, which hinders their usability in dynamic environments like household settings.", "method": "The authors propose a repetitive assembly task (RATS) as a framework for simulating industrial assembly scenarios, where programmers use natural language to instruct cobots. They create a dataset with paired target structures, example instructions, and evaluate LLMs in generating code from these instructions.", "result": "LLMs demonstrated the ability to generate accurate first-order code (instruction sequences) but struggled with higher-order code generation (such as functions and loops).", "conclusion": "The research highlights the potential and current limitations of LLMs in assisting cobots with code generation through natural language, providing a basis for future improvements.", "key_contributions": ["Introduces RATS for evaluating cobot programming via natural language", "Demonstrates LLM capabilities in generating first-order and higher-order code", "Establishes a dataset for training and evaluating conversational code generation for assembly tasks"], "limitations": "LLMs have difficulty generating higher-order code and need further improvement in context understanding for complex tasks.", "keywords": ["Collaborative Robots", "Large Language Models", "Conversational Code Generation", "Assembly Tasks", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.05362", "pdf": "https://arxiv.org/pdf/2410.05362.pdf", "abs": "https://arxiv.org/abs/2410.05362", "title": "LLMs Are In-Context Bandit Reinforcement Learners", "authors": ["Giovanni Monea", "Antoine Bosselut", "KiantÃ© Brantley", "Yoav Artzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at COLM 2025", "summary": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.", "AI": {"tldr": "This paper explores in-context reinforcement learning (ICRL) in large language models (LLMs), showing their effectiveness and limitations over various classification tasks.", "motivation": "To investigate how large language models can learn from external rewards in a contextual bandit framework instead of relying solely on supervised data.", "method": "The authors conducted experiments using LLMs ranging from 500M to 70B parameters on challenging classification tasks, focusing on in-context learning with external rewards.", "result": "LLMs demonstrated effective in-context reinforcement learning capabilities while revealing instability and limitations in reasoning about errors during the learning process.", "conclusion": "The study highlights the potential of ICRL in LLMs but also points out critical challenges that need addressing, particularly regarding the model's implicit reasoning capabilities.", "key_contributions": ["Introduction of the contextual bandit approach to in-context learning in LLMs", "Detailed examination of learning stability and scaling trends in LLMs", "Empirical results showing LLM performance with semantic and abstract labels"], "limitations": "The process exhibited instability and limited implicit reasoning about errors.", "keywords": ["Large Language Models", "In-context Learning", "Reinforcement Learning", "Contextual Bandits", "Classification Tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.07745", "pdf": "https://arxiv.org/pdf/2410.07745.pdf", "abs": "https://arxiv.org/abs/2410.07745", "title": "StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning", "authors": ["Yuanqing Yu", "Zhefan Wang", "Weizhi Ma", "Shuai Wang", "Chuhan Wu", "Zhiqiang Guo", "Min Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CIKM'25", "summary": "Despite their powerful text generation capabilities, large language models\n(LLMs) still struggle to effectively utilize external tools to solve complex\ntasks, a challenge known as tool learning. Existing methods primarily rely on\nsupervised fine-tuning, treating tool learning as a text generation problem\nwhile overlooking the decision-making complexities inherent in multi-step\ncontexts. In this work, we propose modeling tool learning as a dynamic\ndecision-making process and introduce StepTool, a novel step-grained\nreinforcement learning framework that enhances LLMs' capabilities in multi-step\ntool use. StepTool comprises two key components: Step-grained Reward Shaping,\nwhich assigns rewards to each tool interaction based on its invocation success\nand contribution to task completion; and Step-grained Optimization, which\napplies policy gradient methods to optimize the model across multiple decision\nsteps. Extensive experiments across diverse benchmarks show that StepTool\nconsistently outperforms both SFT-based and RL-based baselines in terms of task\nPass Rate and Recall of relevant tools. Furthermore, our analysis suggests that\nStepTool helps models discover new tool-use strategies rather than merely\nre-weighting prior knowledge. These results highlight the importance of\nfine-grained decision modeling in tool learning and establish StepTool as a\ngeneral and robust solution for enhancing multi-step tool use in LLMs. Code and\ndata are available at https://github.com/yuyq18/StepTool.", "AI": {"tldr": "Proposes StepTool, a reinforcement learning framework to improve large language models' (LLMs) ability to use external tools through dynamic decision-making and fine-grained reward shaping.", "motivation": "Large language models struggle with multi-step tool learning, which requires effective decision-making rather than just text generation.", "method": "Introduces StepTool, which includes Step-grained Reward Shaping and Step-grained Optimization to enhance decision-making in tool use through reinforcement learning.", "result": "StepTool significantly outperforms traditional fine-tuning and reinforcement learning methods on benchmarks, improving task pass rates and tool recall.", "conclusion": "Fine-grained decision modeling is crucial for tool learning in LLMs, with StepTool providing a robust and general approach to enhance multi-step tool use.", "key_contributions": ["Introduction of StepTool as a novel reinforcement learning framework for LLMs.", "Step-grained Reward Shaping to assess tool interaction effectiveness.", "Step-grained Optimization to improve policy across decision steps."], "limitations": "", "keywords": ["large language models", "tool learning", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.16252", "pdf": "https://arxiv.org/pdf/2411.16252.pdf", "abs": "https://arxiv.org/abs/2411.16252", "title": "NormXLogit: The Head-on-Top Never Lies", "authors": ["Sina Abbasi", "Mohammad Reza Modarres", "Mohammad Taher Pilehvar"], "categories": ["cs.CL"], "comment": "Added comparisons on computational efficiency, included experiments\n  on a new dataset with an additional evaluation metric for classification\n  tasks, expanded explanations and discussions in the experiments, and\n  presented a worked example for alignment metrics computation", "summary": "With new large language models (LLMs) emerging frequently, it is important to\nconsider the potential value of model-agnostic approaches that can provide\ninterpretability across a variety of architectures. While recent advances in\nLLM interpretability show promise, many rely on complex, model-specific methods\nwith high computational costs. To address these limitations, we propose\nNormXLogit, a novel technique for assessing the significance of individual\ninput tokens. This method operates based on the input and output\nrepresentations associated with each token. First, we demonstrate that during\nthe pre-training of LLMs, the norms of word embeddings effectively capture\ntoken importance. Second, we reveal a significant relationship between a\ntoken's importance and the extent to which its representation can resemble the\nmodel's final prediction. Extensive analyses reveal that our approach\noutperforms existing gradient-based methods in terms of faithfulness and offers\ncompetitive performance in layer-wise explanations compared to leading\narchitecture-specific techniques.", "AI": {"tldr": "This paper proposes NormXLogit, a model-agnostic method for assessing token importance in large language models (LLMs) to improve interpretability.", "motivation": "To provide a model-agnostic approach for interpretability in LLMs that overcomes the limitations of existing complex, model-specific methods.", "method": "NormXLogit assesses the significance of individual input tokens by using the norms of word embeddings during LLM pre-training and analyzing the relationship between token importance and final predictions.", "result": "NormXLogit outperforms existing gradient-based methods in terms of faithfulness and provides competitive performance compared to architecture-specific techniques.", "conclusion": "The proposed method effectively enhances the interpretability of LLMs while being computationally efficient and easier to apply universally across different models.", "key_contributions": ["Introduction of NormXLogit for token importance assessment", "Demonstration of relationship between token representation and model predictions", "Evidence of improved interpretability and computational efficiency over existing methods"], "limitations": "", "keywords": ["Large Language Models", "Interpretability", "Token Importance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.14528", "pdf": "https://arxiv.org/pdf/2501.14528.pdf", "abs": "https://arxiv.org/abs/2501.14528", "title": "Idiom Detection in Sorani Kurdish Texts", "authors": ["Skala Kamaran Omer", "Hossein Hassani"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 7 tables", "summary": "Idiom detection using Natural Language Processing (NLP) is the computerized\nprocess of recognizing figurative expressions within a text that convey\nmeanings beyond the literal interpretation of the words. While idiom detection\nhas seen significant progress across various languages, the Kurdish language\nfaces a considerable research gap in this area despite the importance of idioms\nin tasks like machine translation and sentiment analysis. This study addresses\nidiom detection in Sorani Kurdish by approaching it as a text classification\ntask using deep learning techniques. To tackle this, we developed a dataset\ncontaining 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse\ncontexts. Using this dataset, we developed and evaluated three deep learning\nmodels: KuBERT-based transformer sequence classification, a Recurrent\nConvolutional Neural Network (RCNN), and a BiLSTM model with an attention\nmechanism. The evaluations revealed that the transformer model, the fine-tuned\nBERT, consistently outperformed the others, achieving nearly 99% accuracy while\nthe RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the\neffectiveness of Transformer-based architectures in low-resource languages like\nKurdish. This research provides a dataset, three optimized models, and insights\ninto idiom detection, laying a foundation for advancing Kurdish NLP.", "AI": {"tldr": "This study focuses on idiom detection in Sorani Kurdish using deep learning, resulting in a new dataset and superior performance by Transformer models.", "motivation": "There is a significant research gap in idiom detection for the Sorani Kurdish language, which is crucial for applications like machine translation and sentiment analysis.", "method": "The study approached idiom detection as a text classification task utilizing deep learning techniques, specifically developing three models: KuBERT-based transformer, RCNN, and BiLSTM with an attention mechanism.", "result": "The experiments demonstrated that the fine-tuned KuBERT transformer model achieved nearly 99% accuracy, outperforming the RCNN model at 96.5% and the BiLSTM model at 80%.", "conclusion": "This research contributes a dataset of 10,580 sentences with 101 idioms and demonstrates the effectiveness of Transformer architectures in low-resource languages.", "key_contributions": ["Development of a dataset for Sorani Kurdish idioms", "Evaluation of three deep learning models for idiom detection", "Demonstration of high accuracy using Transformer-based architecture"], "limitations": "", "keywords": ["Idiom Detection", "Natural Language Processing", "Sorani Kurdish", "Deep Learning", "Transformer Models"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2501.17771", "pdf": "https://arxiv.org/pdf/2501.17771.pdf", "abs": "https://arxiv.org/abs/2501.17771", "title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs", "authors": ["Fabrizio Sandri", "Elia Cunegatti", "Giovanni Iacca"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR)", "summary": "We propose a novel Two-Stage framework for Structured Pruning (\\textsc{2SSP})\nfor pruning Large Language Models (LLMs), which combines two different\nstrategies of pruning, namely Width and Depth Pruning. The first stage (Width\nPruning) removes entire neurons, hence their corresponding rows and columns,\naiming to preserve the connectivity among the pruned structures in the\nintermediate state of the Feed-Forward Networks in each Transformer block. This\nis done based on an importance score measuring the impact of each neuron on the\noutput magnitude. The second stage (Depth Pruning), instead, removes entire\nAttention submodules. This is done by applying an iterative process that\nremoves the Attention with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n\\textsc{2SSP} on four LLM families and three sparsity rates (25\\%, 37.5\\%, and\n50\\%), measuring the resulting perplexity over three language modeling datasets\nas well as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at https://github.com/FabrizioSandri/2SSP.", "AI": {"tldr": "The paper introduces a Two-Stage framework for Structured Pruning (2SSP) of Large Language Models, combining Width and Depth Pruning strategies to enhance model efficiency while preserving performance metrics.", "motivation": "To improve the efficiency of Large Language Models (LLMs) through structured pruning while maintaining performance on language tasks.", "method": "The proposed method consists of two stages: Width Pruning, which removes entire neurons based on their importance score, and Depth Pruning, which iteratively removes entire Attention submodules with minimal impact on a metric of interest like perplexity.", "result": "2SSP consistently outperforms five state-of-the-art competitors across three language modeling datasets and six downstream tasks, achieving substantial improvements in pruning time.", "conclusion": "The 2SSP framework effectively balances sparsity across its two stages, leading to improved efficiency in LLMs without sacrificing performance.", "key_contributions": ["Introduction of a Two-Stage pruning framework for LLMs", "Balanced sparsity mechanism across Width and Depth Pruning", "Performance improvements over existing pruning methods in multiple tasks"], "limitations": "", "keywords": ["structured pruning", "large language models", "width pruning", "depth pruning", "sparsity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.19258", "pdf": "https://arxiv.org/pdf/2501.19258.pdf", "abs": "https://arxiv.org/abs/2501.19258", "title": "VisualSpeech: Enhancing Prosody Modeling in TTS Using Video", "authors": ["Shumin Que", "Anton Ragni"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Speech (TTS) synthesis faces the inherent challenge of producing\nmultiple speech outputs with varying prosody given a single text input. While\nprevious research has addressed this by predicting prosodic information from\nboth text and speech, additional contextual information, such as video, remains\nunder-utilized despite being available in many applications. This paper\ninvestigates the potential of integrating visual context to enhance prosody\nprediction. We propose a novel model, VisualSpeech, which incorporates visual\nand textual information for improving prosody generation in TTS. Empirical\nresults indicate that incorporating visual features improves prosodic modeling,\nenhancing the expressiveness of the synthesized speech. Audio samples are\navailable at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.", "AI": {"tldr": "This paper presents VisualSpeech, a model that integrates visual context to improve prosody prediction in Text-to-Speech synthesis.", "motivation": "To enhance prosody prediction in TTS by utilizing visual context, which is often available but underutilized.", "method": "A novel model called VisualSpeech is proposed, which combines visual and textual information to refine prosody generation in TTS.", "result": "Empirical results show that integrating visual features leads to improved prosodic modeling and enhanced expressiveness of synthesized speech.", "conclusion": "The findings suggest that visual context is a valuable asset in generating more natural and expressive speech outputs in TTS systems.", "key_contributions": ["Introduction of VisualSpeech model for TTS", "Demonstration of improved prosody generation through visual integration", "Presentation of audio samples showcasing enhanced speech expressiveness"], "limitations": "", "keywords": ["Text-to-Speech", "prosody prediction", "visual information", "synthesis", "speech expressiveness"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.08266", "pdf": "https://arxiv.org/pdf/2502.08266.pdf", "abs": "https://arxiv.org/abs/2502.08266", "title": "Dealing with Annotator Disagreement in Hate Speech Classification", "authors": ["Somaiyeh Dehghan", "Mehmet Umut Sen", "Berrin Yanikoglu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.2.7"], "comment": "20 pages, 3 Tables", "summary": "Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is essential for most natural language processing tasks,\nbut categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate various automatic approaches for aggregating multiple\nannotations, in the context of hate speech classification in Turkish tweets.\nOur work highlights the importance of the problem and provides state-of-the-art\nbenchmark results for the detection and understanding of hate speech in online\ndiscourse.", "AI": {"tldr": "This paper focuses on hate speech detection in social media, specifically Turkish tweets, and addresses the challenge of annotator disagreement in labeling data for machine learning models.", "motivation": "Hate speech detection is essential to mitigate the impact of harmful content spreading on social media.", "method": "The paper evaluates various automatic approaches for aggregating multiple annotations to improve hate speech classification.", "result": "State-of-the-art benchmark results were achieved for detecting and understanding hate speech in online discourse, emphasizing the importance of handling annotator disagreement.", "conclusion": "The findings underline the need for effective strategies in managing annotator disagreement to enhance model performance for hate speech detection.", "key_contributions": ["Examined automatic aggregation methods for multiple annotations.", "Provided benchmark results for hate speech detection in Turkish tweets.", "Highlighted the significance of addressing annotator disagreement."], "limitations": "", "keywords": ["Hate Speech Detection", "Machine Learning", "Natural Language Processing", "Social Media", "Annotation"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2502.13959", "pdf": "https://arxiv.org/pdf/2502.13959.pdf", "abs": "https://arxiv.org/abs/2502.13959", "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent", "authors": ["Reza Averly", "Frazier N. Baker", "Ian A. Watson", "Xia Ning"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Drug discovery is a long, expensive, and complex process, relying heavily on\nhuman medicinal chemists, who can spend years searching the vast space of\npotential therapies. Recent advances in artificial intelligence for chemistry\nhave sought to expedite individual drug discovery tasks; however, there remains\na critical need for an intelligent agent that can navigate the drug discovery\nprocess. Towards this end, we introduce LIDDIA, an autonomous agent capable of\nintelligently navigating the drug discovery process in silico. By leveraging\nthe reasoning capabilities of large language models, LIDDIA serves as a\nlow-cost and highly-adaptable tool for autonomous drug discovery. We\ncomprehensively examine LIDDIA , demonstrating that (1) it can generate\nmolecules meeting key pharmaceutical criteria on over 70% of 30 clinically\nrelevant targets, (2) it intelligently balances exploration and exploitation in\nthe chemical space, and (3) it identifies one promising novel candidate on\nAR/NR3C4, a critical target for both prostate and breast cancers. Code and\ndataset are available at https://github.com/ninglab/LIDDiA", "AI": {"tldr": "LIDDIA is an AI-based autonomous agent that intelligently navigates the drug discovery process, generating pharmaceutical-grade molecules and identifying promising candidates for cancer treatments.", "motivation": "To address the lengthy and complex challenges of drug discovery, which heavily rely on human expertise, and to create an intelligent agent that can streamline the process.", "method": "Development of LIDDIA, an autonomous agent that uses large language models to generate and evaluate potential drug candidates in silico, focusing on balancing exploration and exploitation in chemical space.", "result": "LIDDIA successfully generated molecules for over 70% of targeted clinical trials and identified a novel candidate for a significant cancer target, showcasing its effectiveness in drug discovery.", "conclusion": "LIDDIA demonstrates the potential of AI in transforming the drug discovery landscape, offering a scalable and cost-effective solution.", "key_contributions": ["Introduction of LIDDIA, an AI-driven agent for drug discovery", "Capability to generate viable drug candidates for over 70% of targets", "Identification of a novel drug candidate for cancer treatments, showcasing its application in health informatics"], "limitations": "", "keywords": ["drug discovery", "AI", "large language models", "autonomous agents", "health informatics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.07303", "pdf": "https://arxiv.org/pdf/2503.07303.pdf", "abs": "https://arxiv.org/abs/2503.07303", "title": "An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data", "authors": ["Gideon Yoffe", "Yair Segev", "Barak Sober"], "categories": ["cs.CL"], "comment": null, "summary": "Texts, whether literary or historical, exhibit structural and stylistic\npatterns shaped by their purpose, authorship, and cultural context. Formulaic\ntexts, characterized by repetition and constrained expression, tend to have\nlower variability in self-information compared to more dynamic compositions.\nIdentifying such patterns in historical documents, particularly multi-author\ntexts like the Hebrew Bible provides insights into their origins, purpose, and\ntransmission.\n  This study aims to identify formulaic clusters -- sections exhibiting\nsystematic repetition and structural constraints -- by analyzing recurring\nphrases, syntactic structures, and stylistic markers. However, distinguishing\nformulaic from non-formulaic elements in an unsupervised manner presents a\ncomputational challenge, especially in high-dimensional textual spaces where\npatterns must be inferred without predefined labels.\n  To address this, we develop an information-theoretic algorithm leveraging\nweighted self-information distributions to detect structured patterns in text,\nunlike covariance-based methods, which become unstable in small-sample,\nhigh-dimensional settings, our approach directly models variations in\nself-information to identify formulaicity. By extending classical discrete\nself-information measures with a continuous formulation based on differential\nself-information, our method remains applicable across different types of\ntextual representations, including neural embeddings under Gaussian priors.\n  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach\nsuccessfully isolates stylistic layers, providing a quantitative framework for\ntextual stratification. This method enhances our ability to analyze\ncompositional patterns, offering deeper insights into the literary and cultural\nevolution of texts shaped by complex authorship and editorial processes.", "AI": {"tldr": "This study develops an information-theoretic algorithm to identify formulaic clusters in texts, particularly multi-author documents like the Hebrew Bible, through the analysis of recurring patterns and self-information.", "motivation": "Understanding structural and stylistic patterns in texts provides insights into their origins, purpose, and transmission, especially in complex multi-author documents.", "method": "An information-theoretic algorithm is developed that utilizes weighted self-information distributions to identify structured patterns in texts, extending classical self-information measures with a continuous formulation.", "result": "The algorithm successfully isolates stylistic layers in hypothesized authorial divisions of the Hebrew Bible, providing a quantitative framework for analyzing compositional patterns.", "conclusion": "This method enhances text analysis by offering deeper insights into the literary and cultural evolution of documents shaped by complex authorship and editorial processes.", "key_contributions": ["Development of a novel algorithm for identifying formulaic clusters in literary texts", "Application of the algorithm to the Hebrew Bible to isolate stylistic layers", "Extension of classical self-information measures to improve pattern detection in high-dimensional texts."], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Text Analysis", "Information Theory", "Cultural Context"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2503.15904", "pdf": "https://arxiv.org/pdf/2503.15904.pdf", "abs": "https://arxiv.org/abs/2503.15904", "title": "More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models", "authors": ["Evan Chen", "Run-Jun Zhan", "Yan-Bai Lin", "Hung-Hsuan Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases. This study introduces a novel evaluation framework to uncover gender\nbiases in LLMs: using free-form storytelling to surface biases embedded within\nthe models. A systematic analysis of ten prominent LLMs shows a consistent\npattern of overrepresenting female characters across occupations, likely due to\nsupervised fine-tuning (SFT) and reinforcement learning from human feedback\n(RLHF). Paradoxically, despite this overrepresentation, the occupational gender\ndistributions produced by these LLMs align more closely with human stereotypes\nthan with real-world labor data. This highlights the challenge and importance\nof implementing balanced mitigation measures to promote fairness and prevent\nthe establishment of potentially new biases. We release the prompts and\nLLM-generated stories at GitHub.", "AI": {"tldr": "This study evaluates gender biases in Large Language Models (LLMs) through storytelling, revealing overrepresentation of female characters and alignment with human stereotypes.", "motivation": "To address concerns about gender biases in LLMs and their implications for fairness in AI applications.", "method": "A novel evaluation framework using free-form storytelling to analyze gender representation in ten prominent LLMs.", "result": "The analysis showed a consistent overrepresentation of female characters in various occupations, influenced by supervised fine-tuning and reinforcement learning, aligning more with stereotypes than real-world data.", "conclusion": "The findings stress the need for balanced mitigation measures in LLMs to combat existing biases and avoid creating new ones.", "key_contributions": ["Introduction of a novel framework for evaluating gender biases in LLMs", "Empirical analysis of gender representation in ten LLMs", "Release of prompts and generated content for further research"], "limitations": "Focused solely on gender biases; other social biases may not be addressed.", "keywords": ["Language Models", "Gender Bias", "Human Feedback", "Natural Language Processing", "Fairness"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.24115", "pdf": "https://arxiv.org/pdf/2503.24115.pdf", "abs": "https://arxiv.org/abs/2503.24115", "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection", "authors": ["Zhiming Ma", "Peidong Wang", "Minhua Huang", "Jingpeng Wang", "Kai Wu", "Xiangzhao Lv", "Yachun Pang", "Yin Yang", "Wenjie Tang", "Yuchen Kang"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.", "AI": {"tldr": "The paper introduces TeleAntiFraud-28k, an open-source audio-text dataset for telecom fraud detection, and TeleAntiFraud-Bench, a benchmark for evaluating models on this dataset.", "motivation": "To tackle the significant challenges in telecom fraud detection arising from insufficient multimodal training data that combines audio and text analysis.", "method": "The dataset is generated using privacy-preserved text created from ASR-transcribed recordings, enhanced by LLM-based sampling, and includes adversarially synthesized scenarios.", "result": "The constructed dataset comprises 28,511 speech-text pairs and supports three main tasks: scenario classification, fraud detection, and classification of fraud types; as well as an evaluation benchmark for testing models.", "conclusion": "This work establishes a critical foundation for multimodal anti-fraud research, addressing issues of data privacy and diversity while providing tools for community expansion.", "key_contributions": ["Introduction of TeleAntiFraud-28k audio-text dataset for telecom fraud detection", "Development of TeleAntiFraud-Bench for standardized model evaluation", "Production-optimized supervised fine-tuning model for hybrid data"], "limitations": "", "keywords": ["telecom fraud", "multimodal dataset", "machine learning", "LLM", "audio-text analysis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.03454", "pdf": "https://arxiv.org/pdf/2504.03454.pdf", "abs": "https://arxiv.org/abs/2504.03454", "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing", "authors": ["William Fleshman", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.", "AI": {"tldr": "SPECTR is a method for dynamically composing expert language models at inference time without additional training, improving routing accuracy and task performance.", "motivation": "The paper addresses the challenges of training large, general-purpose language models and presents an alternative in the form of specialized expert models fine-tuned for specific tasks or domains.", "method": "SPECTR dynamically composes expert models at each time step during inference, allowing token- and layer-wise model combinations without requiring additional training.", "result": "Experimental results show that SPECTR improves routing accuracy over other training-free methods and enhances task performance across various expert domains.", "conclusion": "SPECTR offers a flexible and effective approach for leveraging specialized expert models in real-world applications, improving performance without extra training.", "key_contributions": ["Introduction of the SPECTR method for dynamic expert model composition", "Demonstration of improved routing accuracy", "Showcased task performance increase in expert domains"], "limitations": "", "keywords": ["language models", "expert models", "dynamic composition", "inference", "routing accuracy"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823.pdf", "abs": "https://arxiv.org/abs/2504.04823", "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.", "AI": {"tldr": "The paper examines the effects of quantization on reasoning language models, focusing on performance across various benchmarks while managing inference costs.", "motivation": "To understand the impact of quantization on reasoning models, as existing studies have primarily focused on large language models without addressing reasoning efficiency.", "method": "The study evaluates several quantized models, applying weight, KV cache, and activation quantization techniques at different bit-widths, and assesses their performance on mathematical, scientific, and programming reasoning tasks.", "result": "Lossless quantization is achievable with specific configurations, but lower bit-widths pose accuracy risks; model size and task difficulty are influential factors in performance outcomes.", "conclusion": "Quantized reasoning models can maintain performance with certain optimization strategies, and all models tested are available as open-source.", "key_contributions": ["First systematic study on quantized reasoning models", "Explication of performance determinants", "Open-sourced models for community use"], "limitations": "The study is limited to specific model families and tasks; further research could explore broader applications and other model architectures.", "keywords": ["Quantization", "Reasoning Models", "Language Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.05410", "pdf": "https://arxiv.org/pdf/2504.05410.pdf", "abs": "https://arxiv.org/abs/2504.05410", "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling", "authors": ["Benjamin Lipkin", "Benjamin LeBrun", "Jacob Hoover Vigly", "JoÃ£o Loula", "David R. MacIver", "Li Du", "Jason Eisner", "Ryan Cotterell", "Vikash Mansinghka", "Timothy J. O'Donnell", "Alexander K. Lew", "Tim Vieira"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models.", "AI": {"tldr": "This paper introduces a new adaptive rejection sampling algorithm for generating constrained outputs from language models, significantly improving runtime and performance by reducing constraint evaluations and enabling better estimation of importance weights.", "motivation": "The need to efficiently generate outputs from language models under constraints without compromising performance due to the high cost of evaluating large vocabularies at each step.", "method": "An adaptive rejection sampling algorithm that reduces the number of required constraint evaluations while producing unbiased estimates of importance weights, which can correct for local constraint enforcement's myopic behavior.", "result": "The proposed algorithm demonstrated superior performance compared to state-of-the-art baselines across various domains, including text-to-SQL and molecular synthesis, while enhancing runtime efficiency.", "conclusion": "The new method offers a more efficient means of generating constrained outputs from language models, catering to a wider range of constraints and promoting overall runtime and performance improvements.", "key_contributions": ["Introduction of an adaptive rejection sampling algorithm", "Ability to produce low-variance, unbiased estimates of importance weights", "Demonstrated superior performance and broader applicability across multiple domains"], "limitations": "", "keywords": ["constrained decoding", "adaptive rejection sampling", "importance weights"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.15219", "pdf": "https://arxiv.org/pdf/2504.15219.pdf", "abs": "https://arxiv.org/abs/2504.15219", "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web", "authors": ["Manya Wadhwa", "Zayne Sprague", "Chaitanya Malaviya", "Philippe Laban", "Junyi Jessy Li", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.", "AI": {"tldr": "EvalAgent is a framework designed to uncover nuanced and task-specific evaluation criteria for language model outputs by mining expert guidance and proposing criteria that enhance response quality.", "motivation": "To enhance the evaluation process of language model outputs on structured writing tasks by identifying implicit, task-specific criteria that go beyond basic requirements.", "method": "EvalAgent mines expert-authored online guidance to propose diverse evaluation criteria that are grounded in reliable external sources, improving the specificity and quality of evaluation.", "result": "EvalAgent-generated criteria are often implicit yet specific, revealing additional human-valued criteria in model responses. Combining these with LLM-generated criteria yields better evaluations than using LLMs alone.", "conclusion": "The implementation of EvalAgent can significantly improve the quality of language model outputs by focusing on nuanced and actionable evaluation criteria.", "key_contributions": ["Introduction of EvalAgent framework for automatic uncovering of evaluation criteria", "Demonstration that EvalAgent criteria improve upon initial responses", "Evidence that combined criteria yield more valuable evaluations from human perspectives."], "limitations": "", "keywords": ["language models", "evaluation criteria", "human-computer interaction", "LLM", "structured writing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23548", "pdf": "https://arxiv.org/pdf/2505.23548.pdf", "abs": "https://arxiv.org/abs/2505.23548", "title": "Translation in the Wild", "authors": ["Yuri Balashov"], "categories": ["cs.CL"], "comment": "4 figures", "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.", "AI": {"tldr": "This paper explores the translation capabilities of Large Language Models (LLMs), proposing that their abilities stem from incidental bilingualism in training data and instruction tuning.", "motivation": "To understand the underlying reasons behind the translation abilities of LLMs, which were not specifically trained for translation tasks.", "method": "The author reflects on existing studies and user experiences related to LLMs' translation performance, proposing the 'duality' hypothesis regarding pre-training data.", "result": "LLMs exhibit competitive translation performance despite not being trained on translation objectives, attributing this to their utilization of diverse linguistic information from varying contexts.", "conclusion": "The translation abilities of LLMs may redefine our understanding of translation processes, both human and machine, underpinned by deep learning techniques.", "key_contributions": ["Proposition of the 'duality' hypothesis regarding LLMs' pre-training data and its effects on translation ability.", "Reflections on the role of instruction tuning in enhancing LLM translation capabilities.", "Discussion on empirical testing of the duality hypothesis and its implications for translation theory."], "limitations": "", "keywords": ["Large Language Models", "translation", "instruction tuning", "bilingualism", "deep learning"], "importance_score": 8, "read_time_minutes": 10}}
