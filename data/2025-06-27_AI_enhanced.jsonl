{"id": "2506.20748", "pdf": "https://arxiv.org/pdf/2506.20748.pdf", "abs": "https://arxiv.org/abs/2506.20748", "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots", "authors": ["Jingshu Li", "Zicheng Zhu", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Chatbots are increasingly integrated into people's lives and are widely used\nto help people. Recently, there has also been growing interest in the reverse\ndirection-humans help chatbots-due to a wide range of benefits including better\nchatbot performance, human well-being, and collaborative outcomes. However,\nlittle research has explored the factors that motivate people to help chatbots.\nTo address this gap, we draw on the Computers Are Social Actors (CASA)\nframework to examine how chatbot anthropomorphism-including human-like\nidentity, emotional expression, and non-verbal expression-influences human\nempathy toward chatbots and their subsequent prosocial behaviors and\nintentions. We also explore people's own interpretations of their prosocial\nbehaviors toward chatbots. We conducted an online experiment (N = 244) in which\nchatbots made mistakes in a collaborative image labeling task and explained the\nreasons to participants. We then measured participants' prosocial behaviors and\nintentions toward the chatbots. Our findings revealed that human identity and\nemotional expression of chatbots increased participants' prosocial behavior and\nintention toward chatbots, with empathy mediating these effects. Qualitative\nanalysis further identified two motivations for participants' prosocial\nbehaviors: empathy for the chatbot and perceiving the chatbot as human-like. We\ndiscuss the implications of these results for understanding and promoting human\nprosocial behaviors toward chatbots.", "AI": {"tldr": "This study investigates how chatbot anthropomorphism influences human empathy and prosocial behaviors toward chatbots, revealing that human-like identity and emotional expression enhance positive interactions.", "motivation": "To explore factors motivating people to assist chatbots, addressing the gap in research concerning human support for chatbot performance and collaborative outcomes.", "method": "An online experiment with 244 participants where chatbots made mistakes during a collaborative image labeling task, followed by measurement of participants' prosocial behaviors and intentions.", "result": "Human identity and emotional expression in chatbots significantly increased prosocial behaviors and intentions in participants, with empathy serving as a mediating factor.", "conclusion": "Understanding the role of empathy and chatbots' human-like features can help promote positive human interactions with chatbots, thereby improving chatbot performance and acceptance.", "key_contributions": ["Examined the impact of chatbot anthropomorphism on human empathy and prosocial behavior.", "Identified empathy and perception of human-like qualities as motivators for prosocial actions toward chatbots.", "Provided insights into improving chatbot designs to enhance user engagement."], "limitations": "Potential limitations include the artificial nature of the experimental task and the specific context of chatbot interactions.", "keywords": ["chatbots", "anthropomorphism", "human-computer interaction", "prosocial behavior", "empathy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.20884", "pdf": "https://arxiv.org/pdf/2506.20884.pdf", "abs": "https://arxiv.org/abs/2506.20884", "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere", "authors": ["Meira Gilbert", "Miranda Wei", "Lindah Kotut"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify\nstrangers they see in public via information crowd-sourcing. The trend started\nas early as 2021 and users typically engage with it for romantic purposes\n(similar to a ''Missed Connections'' personal advertisement). This practice\nincludes acts of surveillance and identification in the public sphere, although\nby peers rather than governments or corporations. To understand users'\nreactions to this trend we conducted a qualitative analysis of 60 TikTok videos\nand 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were\nsuccessfully identified. We also find that while there were comments expressing\ndisapproval (n=310), more than double the number expressed support (n=883).\nSupportive comments demonstrated genuine interest and empathy, reflecting\nevolving conceptions of community and algorithmic engagement. On the other\nhand, disapproving comments highlighted concerns about inappropriate\nrelationships, stalking, consent, and gendered double standards. We discuss\nthese insights in relation to the normalization of interpersonal surveillance,\nonline stalking, and as an evolution of social surveillance to offer a new\nperspective on user perceptions surrounding interpersonal surveillance and\nidentification in the public sphere.", "AI": {"tldr": "Analysis of TikTok's \"Do Your Thing\" trend reveals user support for surveillance-based identification of strangers, raising concerns about privacy and social norms.", "motivation": "To explore user reactions to the viral TikTok trend where individuals identify strangers in public through crowd-sourced information.", "method": "Conducted a qualitative analysis of 60 TikTok videos and 1,901 user comments to capture reactions and sentiments toward the trend.", "result": "Identified 19 successful identifications among the videos; supportive comments (n=883) outnumbered disapproving comments (n=310), indicating a complex social engagement with the trend.", "conclusion": "Highlights the normalization of interpersonal surveillance and its implications on consent, relationships, and societal norms around privacy.", "key_contributions": ["Qualitative insights into user engagement with surveillance trends on social media", "New perspective on interpersonal surveillance and community dynamics", "Discussion of social implications concerning privacy and consent"], "limitations": "Limited to analysis of TikTok content and may not generalize to other platforms or contexts.", "keywords": ["TikTok", "interpersonal surveillance", "social media trends", "privacy", "community engagement"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.20952", "pdf": "https://arxiv.org/pdf/2506.20952.pdf", "abs": "https://arxiv.org/abs/2506.20952", "title": "Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment", "authors": ["Kyosuke Ishibashi", "Atsushi Saito", "Zin Y. Tun", "Lucas Ray", "Megan C. Coram", "Akihiro Sakurai", "Allison M. Okamura", "Ko Yamamoto"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Human crowd simulation in virtual reality (VR) is a powerful tool with\npotential applications including emergency evacuation training and assessment\nof building layout. While haptic feedback in VR enhances immersive experience,\nits effect on walking behavior in dense and dynamic pedestrian flows is\nunknown. Through a user study, we investigated how haptic feedback changes user\nwalking motion in crowded pedestrian flows in VR. The results indicate that\nhaptic feedback changed users' collision avoidance movements, as measured by\nincreased walking trajectory length and change in pelvis angle. The\ndisplacements of users' lateral position and pelvis angle were also increased\nin the instantaneous response to a collision with a non-player character (NPC),\neven when the NPC was inside the field of view. Haptic feedback also enhanced\nusers' awareness and visual exploration when an NPC approached from the side\nand back. Furthermore, variation in walking speed was increased by the haptic\nfeedback. These results suggested that the haptic feedback enhanced users'\nsensitivity to a collision in VR environment.", "AI": {"tldr": "This study examines the impact of haptic feedback on user behavior during human crowd simulation in virtual reality, revealing enhanced collision avoidance and user awareness.", "motivation": "To explore how haptic feedback affects walking behavior in dense and dynamic pedestrian flows in virtual reality, particularly in scenarios such as emergency evacuation training.", "method": "A user study was conducted where participants experienced crowded pedestrian flows in VR with and without haptic feedback to measure changes in walking motion and collision avoidance.", "result": "Haptic feedback significantly altered user walking trajectories, increased awareness, and enhanced visual exploration during interactions with non-player characters, resulting in greater sensitivity to potential collisions.", "conclusion": "The findings suggest that haptic feedback can improve user experiences in VR by enhancing their awareness of and responsiveness to their environment in crowded scenarios.", "key_contributions": ["Demonstrated the impact of haptic feedback on collision avoidance in VR environments.", "Showed increased user awareness and exploration in crowded pedestrian flows due to haptic stimuli.", "Provided empirical data on the changes in walking dynamics such as trajectory length and pelvis angle in response to feedback."], "limitations": "", "keywords": ["haptic feedback", "virtual reality", "crowd simulation", "collision avoidance", "user behavior"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.21195", "pdf": "https://arxiv.org/pdf/2506.21195.pdf", "abs": "https://arxiv.org/abs/2506.21195", "title": "Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact", "authors": ["Neha Raghuvanshi"], "categories": ["cs.HC"], "comment": null, "summary": "Have you wondered how cross-functional teams balance between maximizing value\nthat users derive and business growth leading to win-win situations? This case\nstudy shows how User Experience Research (UXR) and Data Science teams used\nmixed methods research to strategically influence Product Led Growth (PLG) for\na Password Manager used by million+ users, thus allowing our users, internal\nteams, and business to win. The audience will take away practical\nlessons/techniques related to leveraging mixed methods to: a. Maximize user\nvalue while meeting business growth goals b. Influence cross-functional teams\nc. Measure user and business impact This case study can be easily tied to the\nUXR Point of view pyramid (POV) [2] that represents a methodological approach\nto construct a POV and further dives into actioning POV to create measurable\nuser and business impact.", "AI": {"tldr": "Explores how UXR and Data Science teams impact Product Led Growth in a Password Manager through mixed methods research.", "motivation": "To understand how cross-functional teams can enhance user value and drive business growth simultaneously.", "method": "Utilized mixed methods research involving User Experience Research and Data Science approaches.", "result": "Provided practical lessons on leveraging mixed methods to achieve user value and business goals, influencing team dynamics, and measuring impact.", "conclusion": "The case study demonstrates that combining UXR with Data Science can create win-win situations for users and businesses, encouraging actionable insights.", "key_contributions": ["Demonstrates mixed methods research application in a real-world scenario.", "Highlights the intersection of UXR and Data Science in driving product growth.", "Offers practical techniques for measuring user and business impact."], "limitations": "", "keywords": ["User Experience Research", "Data Science", "Product Led Growth", "Mixed Methods", "User Value"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.20747", "pdf": "https://arxiv.org/pdf/2506.20747.pdf", "abs": "https://arxiv.org/abs/2506.20747", "title": "Towards Probabilistic Question Answering Over Tabular Data", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "AI": {"tldr": "Introducing LUCARIO, a new benchmark and framework for probabilistic question answering over tabular data using Bayesian Networks and LLMs.", "motivation": "Current QA systems struggle with probabilistic questions requiring reasoning under uncertainty, which this paper aims to address.", "method": "The framework induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and utilizes LLMs for answer generation.", "result": "Empirical results show significant improvements over baseline approaches in tackling probabilistic questions.", "conclusion": "The proposed method demonstrates the effectiveness of hybrid symbolic-neural reasoning in probabilistic QA.", "key_contributions": ["Introduction of the LUCARIO benchmark for probabilistic QA", "Development of a framework using Bayesian Networks for reasoning", "Application of LLMs for generating answers from probabilistic queries."], "limitations": "", "keywords": ["question answering", "tabular data", "probabilistic reasoning", "Bayesian Networks", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.21201", "pdf": "https://arxiv.org/pdf/2506.21201.pdf", "abs": "https://arxiv.org/abs/2506.21201", "title": "Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices", "authors": ["Zihao You", "Michael Crabb"], "categories": ["cs.HC"], "comment": "3 pages, 1 figure, Access InContext Workshop at CHI 2025 on 26th of\n  April", "summary": "The consumption of subtitles via TVs, laptops and smartphones has the\npotential to marginalize people based on their complex accessibility needs. The\ncurrent one-size-fits-all approach to this accessibility aid is no longer fit\nfor purpose and work is required to look at how it can be adapted to be\npersonalised for individual users based on individual context, content, and\nconsumption habits. People with Aphasia, for example, encounter significant\nchallenges in understanding subtitle texts.\n  We see our work as a call to action for more inclusive practices, focusing on\nhow the thoughts and opinions of people with aphasia can be included in media\nresearch. Our work investigates how to develop future media solutions for\npeople with aphasia to create a more inclusive media viewing environment. We\nbelieve the key to this is appropriate prototyping tools and methods to allow\nequitable inclusion in the system design process.", "AI": {"tldr": "The paper discusses the need for personalized subtitle options to enhance accessibility for individuals with Aphasia, advocating for inclusive practices in media research and system design.", "motivation": "Current subtitle options are not adequately addressing the diverse accessibility needs, especially for people with Aphasia.", "method": "The paper advocates for the inclusion of thoughts and opinions from people with Aphasia in media research to inform the design of more effective media solutions.", "result": "The findings highlight the necessity of personalized approaches to subtitle consumption and the importance of equitable inclusion in design processes.", "conclusion": "Adapting subtitles to individual contexts will create a more inclusive media viewing experience for individuals with Aphasia.", "key_contributions": ["Emphasizes the need for personalization in subtitle accessibility for people with Aphasia.", "Highlights the importance of user feedback in designing media solutions.", "Proposes the development of prototyping tools for equitable inclusion in design processes."], "limitations": "", "keywords": ["subtitles", "accessibility", "Aphasia", "media research", "inclusive design"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.20793", "pdf": "https://arxiv.org/pdf/2506.20793.pdf", "abs": "https://arxiv.org/abs/2506.20793", "title": "Multi-lingual Functional Evaluation for Large Language Models", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "AI": {"tldr": "This paper introduces multi-lingual functional benchmarks to evaluate large language models, revealing significant performance inconsistencies across languages.", "motivation": "To improve the evaluation of multi-lingual competence in large language models beyond static benchmarks, which may not accurately reflect practical performance.", "method": "The authors created two new functional benchmarks—CL-GSM Symbolic and CL-IFEval—by translating existing English benchmark templates into French, Spanish, Hindi, Arabic, and Yoruba.", "result": "The study found marked performance drops in some language pairs when comparing static benchmarks to the created functional benchmarks, indicating that not all benchmarks are effective for multi-lingual evaluation.", "conclusion": "The performance and robustness of models vary significantly across languages, suggesting that certain languages are better supported in multi-lingual settings, which can guide future NLP research.", "key_contributions": ["Development of CL-GSM Symbolic and CL-IFEval benchmarks for multi-lingual evaluation", "Revealed performance discrepancies in existing benchmarks", "Highlighted robustness variation across different languages"], "limitations": "Does not explore the reasons behind the performance discrepancies across languages; may not cover languages beyond those studied.", "keywords": ["multi-lingual benchmarks", "large language models", "NLP", "functional evaluation", "model robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.21319", "pdf": "https://arxiv.org/pdf/2506.21319.pdf", "abs": "https://arxiv.org/abs/2506.21319", "title": "Multimodal LLMs for Visualization Reconstruction and Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.", "AI": {"tldr": "This paper presents a method for improving the understanding of visualizations in data communication by training multimodal large language models (LLMs) on a novel dataset that combines chart images with their vectorized representations.", "motivation": "The need for effective data communication through visualizations, which currently rely on complex data-to-visual mapping that existing multimodal models struggle to decode.", "method": "A dataset combining chart images with vectorized representations, encoding schemes, and data features was created, and multimodal LLMs were trained to improve understanding and accuracy in data extraction and chart reconstruction.", "result": "The proposed approach shows significant improvements in data extraction accuracy and chart reconstruction quality compared to existing methods.", "conclusion": "The novel dataset and training method for multimodal visualization LLMs enhance understanding of visualizations, addressing key challenges in decoding and accurately reconstructing them.", "key_contributions": ["Development of a novel dataset for multimodal visualization LLMs", "Training methodology that combines images and vectorized data", "Demonstrated improvements in accuracy and reconstruction quality"], "limitations": "The paper does not address the scalability of the method to different types of visualizations or domains.", "keywords": ["multimodal models", "visualization", "data extraction", "large language models", "chart reconstruction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.20803", "pdf": "https://arxiv.org/pdf/2506.20803.pdf", "abs": "https://arxiv.org/abs/2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "This study compares the effectiveness of research ideas generated by Large Language Models (LLMs) and human experts in an execution context to assess their impact on research outcomes.", "motivation": "The goal is to evaluate whether LLM-generated research ideas lead to better research results compared to those generated by human experts, addressing the limitations of LLMs in generating effective research concepts.", "method": "Forty-three expert researchers executed randomly-assigned ideas generated by either LLMs or human experts, spending over 100 hours each and documenting their experiments in a 4-page paper. The outcomes were then reviewed blind by NLP experts.", "result": "LLM-generated ideas received significantly lower review scores post-execution compared to expert ideas across several evaluation metrics, indicating a significant drop in perceived novelty and effectiveness of LLM ideas.", "conclusion": "The findings reveal a notable ideation-execution gap, indicating that current LLMs struggle to generate truly effective research ideas, raising challenges in evaluating creativity without execution outcomes.", "key_contributions": ["Demonstrated the decrease in quality of LLM-generated ideas after execution compared to human-generated ideas.", "Highlighted the limitations of LLMs in producing effective research ideas.", "Provided a framework for evaluating the impact of research idea generation on actual research outcomes."], "limitations": "The study may be limited by the small sample size of expert researchers and specific evaluation criteria used for review.", "keywords": ["Large Language Models", "research outcomes", "ideation-execution gap", "NLP", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2506.21322", "pdf": "https://arxiv.org/pdf/2506.21322.pdf", "abs": "https://arxiv.org/abs/2506.21322", "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory", "authors": ["Hong Wang", "Natalia Calvo-Barajas", "Katie Winkle", "Ginevra Castellano"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages", "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare.", "AI": {"tldr": "The study explores how robot transparency and sociability affect user trust and decision-making when contradictory information is presented, highlighting the risks of overtrust in healthcare robotics.", "motivation": "To understand the impact of robotic transparency on user decision-making and trust in scenarios where robots provide conflicting information.", "method": "A 2 x 2 between-subjects online study with 176 participants observing a Furhat robot in a healthcare setting, focusing on its level of transparency and sociability.", "result": "Results showed that higher transparency led to fewer misinterpretations of conflicting information, with users tending to overtrust low transparency robots and prioritize their recommendations over personal memory.", "conclusion": "Transparency mechanisms in robotic systems are crucial to prevent overreliance in healthcare, and understanding these dynamics is essential for effective multi-user robot deployments in home settings.", "key_contributions": ["Examined the effects of robot transparency on user decision-making", "Showed tendencies of overtrust in robot recommendations", "Highlighted importance of transparency in healthcare robots' design"], "limitations": "The study's scenario was based on a fictional context and may not fully represent real-world dynamics in healthcare settings.", "keywords": ["robot transparency", "user trust", "healthcare robotics", "decision-making", "sociability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.20821", "pdf": "https://arxiv.org/pdf/2506.20821.pdf", "abs": "https://arxiv.org/abs/2506.20821", "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "AI": {"tldr": "MultiFinRAG enhances financial question answering by integrating multimodal content, achieving higher accuracy in complex tasks than traditional LLMs.", "motivation": "Address the challenges of answering questions over extensive financial documents that blend text, tables, and images, which strain existing LLMs and RAG frameworks.", "method": "MultiFinRAG utilizes multimodal extraction, batching table and figure images, and employing a lightweight multimodal LLM to generate structured JSON outputs and summaries. It incorporates a tiered fallback strategy for cross-modal reasoning.", "result": "Achieves 19 percentage points higher accuracy than ChatGPT-4o on financial QA tasks involving various modalities.", "conclusion": "MultiFinRAG can effectively manage complex multimodal inputs in financial documents, demonstrating improved performance on relevant QA tasks.", "key_contributions": ["Introduction of MultiFinRAG framework for financial QA", "Implementation of multimodal extraction and dynamic context escalation", "Enhanced accuracy on financial QA tasks compared to traditional models"], "limitations": "", "keywords": ["financial documents", "multimodal reasoning", "retrieval-augmented generation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.21333", "pdf": "https://arxiv.org/pdf/2506.21333.pdf", "abs": "https://arxiv.org/abs/2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hndriks", "Drik Heylen", "Kim Baraka"], "categories": ["cs.HC", "cs.AI", "I.2.11"], "comment": null, "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems.", "AI": {"tldr": "This paper reviews 62 studies on co-creative systems, identifying key design dimensions and considerations to enhance human creativity with AI.", "motivation": "To guide the development of sophisticated co-creative systems that actively support human creativity.", "method": "Systematic literature review of 62 papers focused on applications where AI collaborates in creative processes across visual arts, design, and writing.", "result": "Identified key design dimensions and 24 design considerations that improve user control, satisfaction, trust, and collaboration in creative tasks with AI.", "conclusion": "High user control and proactive, context-aware systems foster better collaboration and creative outcomes, but gaps in supporting early creative phases and user adaptation remain.", "key_contributions": ["Identified key dimensions for co-creative system design", "Extracted 24 practical design considerations", "Highlighted the importance of user control and system proactivity in creative collaboration"], "limitations": "Gaps in supporting early creative phases and challenges in user adaptation to AI systems.", "keywords": ["co-creation", "human creativity", "AI collaboration", "system design", "creativity support"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.20822", "pdf": "https://arxiv.org/pdf/2506.20822.pdf", "abs": "https://arxiv.org/abs/2506.20822", "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "authors": ["Quintin Myers", "Yanjun Gao"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "AI": {"tldr": "This study evaluates LLMs on their ability to respond to everyday conflict scenarios, revealing biases in violent content generation based on demographic factors.", "motivation": "To investigate how LLMs respond to morally ambiguous scenarios and assess their ability to handle conflict without bias.", "method": "Utilized the Violent Behavior Vignette Questionnaire (VBVQ) to evaluate six LLMs, incorporating persona-based prompts that varied race, age, and geographic identity.", "result": "The study found that LLMs often produce surface-level text that contradicts their internal preferences for violent responses, and these tendencies vary across different demographic groups.", "conclusion": "LLMs reveal biases in violent content generation that may contradict established social science findings, highlighting the need for careful evaluation before deploying them in conflict-sensitive applications.", "key_contributions": ["First study applying VBVQ to LLMs", "Demonstrated demographic bias in LLM responses to violent scenarios", "Highlighted discrepancies between LLM outputs and established social science research"], "limitations": "The study is limited to six LLMs and may not generalize to all LLMs or contexts outside the U.S.", "keywords": ["Large Language Models", "violent content", "morally ambiguous scenarios", "demographic bias", "social science research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.21417", "pdf": "https://arxiv.org/pdf/2506.21417.pdf", "abs": "https://arxiv.org/abs/2506.21417", "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation", "authors": ["Yunxiu Xu", "Siyu Wang", "Shoichi Hasegawa"], "categories": ["cs.HC", "H.5.2; I.3.6"], "comment": "14 pages, 15 figures, 2 tables. Published in IEEE Transactions on\n  Haptics (Early Access)", "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability.", "AI": {"tldr": "A lightweight, wearable fingertip haptic device provides physics-based feedback for better dexterous manipulation in virtual environments without impacting real-world interactions.", "motivation": "To develop a haptic device that enhances virtual manipulation while maintaining a lightweight and non-intrusive design.", "method": "The device uses thin strings and actuators attached to fingernails, integrating with a physics engine to produce various haptic feedback types. Performance was evaluated through user tests measuring pressure perception and manipulation tasks.", "result": "Participants effectively perceived pressure and vibration feedback, leading to improved efficiency in virtual tasks due to the minimal haptic cues provided by the device.", "conclusion": "The fingertip haptic device successfully balances lightweight construction with efficient haptic feedback, presenting an innovative alternative to traditional glove-type haptic devices.", "key_contributions": ["Developed a wearable device that enhances virtual manipulation", "Demonstrated the effectiveness of minimal haptic feedback", "Showcased a solution for daily wearability with preserved tactile sensations"], "limitations": "", "keywords": ["wearable haptic device", "virtual manipulation", "haptic feedback"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.20876", "pdf": "https://arxiv.org/pdf/2506.20876.pdf", "abs": "https://arxiv.org/abs/2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "categories": ["cs.CL"], "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "AI": {"tldr": "This study investigates the challenges of using automated fact-checking systems in medicine, focusing on how clinical experts verify claims from social media.", "motivation": "To explore the complexities of verifying health-related claims against medical literature and understand why existing fact-checking systems are underused in medicine.", "method": "The study involves analyzing how clinical experts synthesize medical evidence to verify real claims sourced from social media.", "result": "The analysis reveals significant challenges such as linking real-world claims to clinical trial evidence, ambiguities in claims, and subjective nature of veracity labels.", "conclusion": "Fact-checking in medicine should be treated as an interactive communication issue instead of a straightforward end-to-end process.", "key_contributions": ["First study examining expert verification of claims from social media in a medical context", "Identification of key challenges faced by fact-checkers in the medical field", "Proposal to redefine fact-checking as an interactive communication problem"], "limitations": "The study primarily focuses on clinical expert responses and may not reflect the perspectives of other stakeholders in the health informatics field.", "keywords": ["fact-checking", "medical communication", "evidence-based medicine", "health informatics", "social media claims"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.21441", "pdf": "https://arxiv.org/pdf/2506.21441.pdf", "abs": "https://arxiv.org/abs/2506.21441", "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail.", "AI": {"tldr": "The paper proposes a design paradigm for managing detail in virtual environments and conducts a user study to evaluate high detail insets in head-mounted displays.", "motivation": "To improve the design of systems that manage level of detail in virtual environments.", "method": "A user study involving ten subjects was conducted to assess the effectiveness of different display setups, specifically through varying inset size and peripheral detail while controlling for other factors.", "result": "ANOVAs indicated that insetless, high detail displays did not significantly impact search times or accuracies as compared to displays with insets, with only the insetless, low detail display showing significant results.", "conclusion": "Further research will explore the impact of task complexity, inset size, and level of detail.", "key_contributions": ["Proposed a new design paradigm for managing detail in virtual environments.", "Conducted a user study with various display settings to evaluate effectiveness of high detail insets.", "Identified that insetless displays can perform similarly to inset displays under certain conditions."], "limitations": "The study does not explore varying task complexities beyond a simple search task.", "keywords": ["virtual environments", "user study", "head-mounted displays", "level of detail", "search task"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.20917", "pdf": "https://arxiv.org/pdf/2506.20917.pdf", "abs": "https://arxiv.org/abs/2506.20917", "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "authors": ["Zhengyan Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "AI": {"tldr": "This thesis presents methods to enhance the adaptation of language models to specific NLP tasks by using unlabeled data, improving fine-tuning efficiency, and developing new evaluation methods.", "motivation": "Language models face challenges in adapting to specific tasks due to inefficient use of unlabeled data, overfitting from limited labeled data, and high computational costs.", "method": "The thesis introduces novel techniques including continued pre-training with unlabeled data, a parameter-efficient fine-tuning method, and improved supervised fine-tuning methods, alongside new evaluation benchmarks for language models.", "result": "Empirical studies show significant improvements in language model robustness, efficiency, and generalization across various NLP tasks, demonstrating the effectiveness of the proposed methods.", "conclusion": "The advances presented contribute towards more capable and adaptable language models, enhancing their utility in a broad range of applications, pushing closer to artificial general intelligence.", "key_contributions": ["Novel continued pre-training technique for task-relevant knowledge extraction", "Parameter-efficient fine-tuning method", "New evaluation benchmarks for language model capabilities"], "limitations": "", "keywords": ["Language models", "Natural language processing", "Fine-tuning methods"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.21456", "pdf": "https://arxiv.org/pdf/2506.21456.pdf", "abs": "https://arxiv.org/abs/2506.21456", "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model.", "AI": {"tldr": "This paper presents a model for understanding the effectiveness of peripheral detail degradation in displays based on an eye/head movement tradeoff, along with experimental results on its implications for display design.", "motivation": "To investigate how reductions in the level of detail (LOD) in peripheral vision of head-tracked large displays can affect user performance.", "method": "A psychophysically based model was developed along with an experiment to evaluate search performance in displays with varied central area shapes and sizes.", "result": "It was found that while the shape of the high detail central area (inset) does not significantly affect performance, the size is critical—displays with at least 30 degrees of inset area performed comparably to undegraded displays.", "conclusion": "The findings support the proposed model, indicating that while shape is less important, inset area plays a crucial role in the effectiveness of peripherally degraded displays.", "key_contributions": ["Developed a psychophysical model for peripheral display degradation.", "Evaluated the impact of inset area and shape on search performance in large displays.", "Provided design recommendations for effective peripherally degraded displays."], "limitations": "", "keywords": ["peripheral vision", "display degradation", "search performance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.20920", "pdf": "https://arxiv.org/pdf/2506.20920.pdf", "abs": "https://arxiv.org/abs/2506.20920", "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "categories": ["cs.CL"], "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "AI": {"tldr": "The paper presents a new pipeline for curating multilingual datasets for pre-training large language models (LLMs), demonstrating improved performance for non-English models and introducing FineWeb2, a substantial multilingual dataset.", "motivation": "To address challenges in creating performant multilingual LLMs due to the complexity of dataset curation across different languages.", "method": "The authors introduce a dataset curation pipeline adaptable to any language, demonstrated through ablation studies on nine languages and guided by evaluation tasks chosen via measurable criteria.", "result": "The proposed pipeline produces non-English corpora that yield more performant models than previous datasets, and a rebalancing approach enhances performance further.", "conclusion": "The FineWeb2 dataset and the curation pipeline enable the effective training of multilingual models at scale, releasing tools and datasets to the community.", "key_contributions": ["Introduction of an adaptable multilingual dataset curation pipeline", "Demonstration of improved model performance on non-English corpora", "Release of a 20 terabyte multilingual dataset (FineWeb2) along with training and evaluation tools."], "limitations": "", "keywords": ["multilingual LLMs", "dataset curation", "FineWeb2"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.20923", "pdf": "https://arxiv.org/pdf/2506.20923.pdf", "abs": "https://arxiv.org/abs/2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "AI": {"tldr": "The paper presents KaLM-Embedding-V2, a compact embedding model designed for general-purpose text embedding tasks, achieving high performance through innovative training techniques.", "motivation": "To develop a versatile and efficient embedding model that achieves superior performance on text embedding tasks while maintaining a smaller parameter size.", "method": "The model uses a fully bidirectional transformer architecture with mean-pooling for fixed-length embeddings and incorporates a multi-stage training pipeline that includes pre-training and fine-tuning on diverse datasets.", "result": "KaLM-Embedding-V2 significantly outperforms other models of similar size and competes with much larger embedding models on the Massive Text Embedding Benchmark in both Chinese and English.", "conclusion": "The proposed model sets a new standard for versatile and compact embedding models with less than 1 billion parameters and provides effective solutions for challenging samples through specialized training techniques.", "key_contributions": ["Introduction of a fully bidirectional transformer architecture", "Development of a multi-stage training pipeline with diverse datasets", "Implementation of an innovative reweighting mechanism for difficult samples"], "limitations": "The model may rely heavily on the quality of the datasets used for pre-training and fine-tuning.", "keywords": ["text embedding", "transformer architecture", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.20803", "pdf": "https://arxiv.org/pdf/2506.20803.pdf", "abs": "https://arxiv.org/abs/2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "The paper investigates the novel research ideas generated by LLMs versus those generated by human experts. An execution study reveals that LLM-generated ideas result in lower review scores by experts after implementation, indicating limitations in LLMs in producing effective research outcomes.", "motivation": "To evaluate the effectiveness of AI-generated research ideas compared to human-generated ideas in scientific research.", "method": "An execution study involving 43 expert researchers who implemented randomly-assigned research ideas generated by either LLMs or human experts, followed by blind reviews of their outcomes.", "result": "LLM-generated ideas received significantly lower review scores after execution compared to expert-written ideas, highlighting a gap in the ideation versus execution phase of research.", "conclusion": "Current LLMs have limitations in generating effective research ideas, as evidenced by the lower quality of executed LLM-generated ideas compared to those by human experts.", "key_contributions": ["Conducted an execution study comparing LLM and human-generated research ideas.", "Demonstrated significant differences in evaluation scores post-execution favoring expert ideas.", "Highlighted the ideation-execution gap indicating limitations in LLM capabilities."], "limitations": "The study involves a small sample size of 43 researchers and focuses on specific evaluation metrics, which may not capture all aspects of research quality.", "keywords": ["Large Language Models", "research ideas", "execution study", "human-computer interaction", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2506.20989", "pdf": "https://arxiv.org/pdf/2506.20989.pdf", "abs": "https://arxiv.org/abs/2506.20989", "title": "Can Gradient Descent Simulate Prompting?", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 2 figures", "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "AI": {"tldr": "This paper presents a method for meta-training language models (LMs) so that gradient updates mimic the effects of prompting instead of standard fine-tuning.", "motivation": "To address the limitations of traditional fine-tuning in language models, particularly in incorporating new information effectively and improving generalization.", "method": "The approach employs gradient-based meta-learning techniques where the LM's own prompted predictions serve as targets, avoiding the need for ground-truth labels.", "result": "The method demonstrates significant improvements in tasks related to the 'reversal curse' and allows LMs to answer questions based on text passages after a single gradient update.", "conclusion": "The findings indicate that gradient descent, with proper initialization, can enhance the expressive capabilities of LMs, leading to better performance in long-context modeling and heightened generalization.", "key_contributions": ["A novel method for meta-training LMs that emulates prompting through gradient updates.", "Empirical evidence showing improvement on challenging language model tasks.", "Insights into the generalization capabilities of gradient-based learning in the context of LMs."], "limitations": "", "keywords": ["language models", "fine-tuning", "prompting", "gradient-based learning", "meta-learning"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2506.20993", "pdf": "https://arxiv.org/pdf/2506.20993.pdf", "abs": "https://arxiv.org/abs/2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "AI": {"tldr": "This paper proposes an advanced method for modeling large language model (LLM) personalities, addressing limitations of existing personality frameworks and enabling nuanced control over personality traits.", "motivation": "The paper addresses the growing expectation for LLMs to exhibit human-like personalities, which current models inadequately fulfill due to reliance on coarse personality dimensions and lack of trait intensity control.", "method": "The authors extend the Machine Personality Inventory (MPI) using the 16 Personality Factor (16PF) model and introduce the Specific Attribute Control (SAC) framework to evaluate and dynamically induce trait intensity in LLMs through semantic anchoring and behavioral questions.", "result": "The proposed method allows for a continuous spectrum of personality trait intensity, yielding more consistent expression and demonstrating that changes in target intensity influence related traits in coherent psychological ways.", "conclusion": "This research enhances human-machine interactions in various fields and moves closer to the development of socially intelligent machines.", "key_contributions": ["Incorporation of the 16PF model for enhanced personality trait granularity", "Development of the Specific Attribute Control (SAC) framework for trait intensity modulation", "Establishment of a continuous spectrum for personality traits leading to coherent behavioral responses"], "limitations": "", "keywords": ["large language models", "personality modeling", "human-computer interaction", "machine personality inventory", "trait intensity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.20993", "pdf": "https://arxiv.org/pdf/2506.20993.pdf", "abs": "https://arxiv.org/abs/2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "AI": {"tldr": "This paper extends the Machine Personality Inventory to incorporate the 16 Personality Factor model, allowing for expressive control over personality traits in large language models (LLMs) through a framework for trait intensity evaluation.", "motivation": "To address limitations in existing LLM personality models, specifically their reliance on coarse personality dimensions and lack of control over trait intensity.", "method": "The authors developed the Specific Attribute Control framework to evaluate and dynamically induce trait intensity in LLMs, utilizing adjective-based semantic anchoring and behavioral questions across five intensity factors.", "result": "The experimental results show that continuous modeling of trait intensity leads to more consistent and controllable personality expression in LLMs compared to binary toggles, with systematic influences on related traits.", "conclusion": "The research contributes to better human-machine interactions by providing a nuanced approach to modeling LLM personalities, which is applicable in domains like healthcare and education.", "key_contributions": ["Incorporated the 16 Personality Factor model into LLM personality modeling.", "Developed a framework for evaluating and inducing trait intensity in LLMs.", "Showed that continuous trait intensity impacts related traits in a psychologically coherent manner."], "limitations": "", "keywords": ["Large Language Models", "Personality Modelling", "Human-Machine Interaction", "Machine Personality Inventory", "Trait Intensity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.21031", "pdf": "https://arxiv.org/pdf/2506.21031.pdf", "abs": "https://arxiv.org/abs/2506.21031", "title": "Large Language Models Acing Chartered Accountancy", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "AI": {"tldr": "This paper introduces CA-Ben, a benchmark for evaluating LLMs in financial reasoning using datasets from Chartered Accountancy exams in India.", "motivation": "To evaluate the effectiveness of LLMs in applying domain-specific financial knowledge in the Indian financial context.", "method": "The study involved creating a dataset (CA-Ben) based on Chartered Accountancy exam questions and evaluating six prominent LLMs (GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4) against it.", "result": "Evaluation results showed that Claude 3.5 Sonnet and GPT-4o performed best, but challenges persisted in numerical computations and legal interpretations.", "conclusion": "The strengths and limitations of LLMs were identified, suggesting improvements through hybrid reasoning and retrieval-augmented generation methods.", "key_contributions": ["Introduction of CA-Ben benchmark for LLM evaluation in finance", "Evaluation of multiple prominent LLMs using structured financial datasets", "Identification of performance gaps in numerical and legal reasoning capabilities of LLMs"], "limitations": "The dataset is based solely on Chartered Accountancy exams and may not cover all financial scenarios.", "keywords": ["Large Language Models", "financial reasoning", "natural language processing", "Chartered Accountancy", "evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.21049", "pdf": "https://arxiv.org/pdf/2506.21049.pdf", "abs": "https://arxiv.org/abs/2506.21049", "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by ACL 2025", "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "AI": {"tldr": "Proposes a Semi-supervised Scalable Unified Framework (SSUF) for e-commerce query classification that effectively combines multiple tasks to improve efficiency and performance.", "motivation": "E-commerce queries are typically short and lack context, leading to challenges in intent and category prediction; existing methods are inefficient due to reliance on user behavior and lack of a unified framework.", "method": "Introduces SSUF, which includes knowledge-enhanced, label-enhanced, and structure-enhanced modules to address inefficiencies in query classification by unifying tasks and leveraging unlabelled data.", "result": "SSUF significantly outperforms existing state-of-the-art models in both offline and online A/B experiments, providing improved query representations and reducing dependence on posterior labels.", "conclusion": "The SSUF framework, with its modular design, can adapt to various subtask needs and enhances the effectiveness of e-commerce query classification.", "key_contributions": ["Development of a Semi-supervised Scalable Unified Framework (SSUF)", "Introduction of knowledge-enhanced, label-enhanced, and structure-enhanced modules", "Demonstrated superiority over existing query classification methods in A/B tests"], "limitations": "", "keywords": ["query classification", "e-commerce", "semi-supervised learning", "machine learning", "intent prediction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.21053", "pdf": "https://arxiv.org/pdf/2506.21053.pdf", "abs": "https://arxiv.org/abs/2506.21053", "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "AI": {"tldr": "This paper introduces MT2-CSD, a large dataset for multi-target conversational stance detection, and proposes the LLM-CRAN model that leverages LLMs for improved performance.", "motivation": "To address the limitations of traditional stance detection models that fail to capture multi-party discussions in social media and the lack of authentic datasets for conversational stance detection.", "method": "The paper introduces the MT2-CSD dataset and proposes the LLM-CRAN model, which enhances conversational understanding using large language models.", "result": "Extensive experiments show that LLM-CRAN outperforms baseline models in conversational stance detection on the MT2-CSD dataset.", "conclusion": "The research demonstrates the potential of LLM-CRAN and MT2-CSD in advancing the field of conversational stance detection.", "key_contributions": ["Introduction of the MT2-CSD dataset, the largest for multi-target conversational stance detection.", "Proposal of the LLM-CRAN model that utilizes LLMs to enhance stance detection.", "Demonstration of significant performance improvements over baseline models."], "limitations": "", "keywords": ["stance detection", "social media", "dataset", "multi-target", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.09910", "pdf": "https://arxiv.org/pdf/2501.09910.pdf", "abs": "https://arxiv.org/abs/2501.09910", "title": "Chatbot apologies: Beyond bullshit", "authors": ["P. D. Magnus", "Alessandra Buccella", "Jason D'Cruz"], "categories": ["cs.HC"], "comment": null, "summary": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper explores this concern and develops\nit beyond the epithet, drawing on the nature of morally serious apologies, the\nlinguistic agency required to perform them, and the moral agency required for\nthem to matter. We conclude by considering some consequences for how chatbots\nshould be designed and how we ought to think about them.", "AI": {"tldr": "This paper critiques the nature of apologies generated by LLM-based chatbots, arguing that such apologies lack true moral agency and linguistic competency.", "motivation": "To explore the implications of LLM-based chatbots producing apologies and the inherent limitations of their moral and linguistic agency.", "method": "The paper analyzes the characteristics of genuine apologies and contrasts them with the output of chatbots, assessing the necessary moral and linguistic agency.", "result": "It argues that chatbot apologies are inauthentic and do not fulfill the essential role of genuine apologies, leading to critical reflections on chatbot design and ethics.", "conclusion": "The authors emphasize the need to rethink chatbot behavior regarding apologies and propose considerations for ethical AI design.", "key_contributions": ["Critique of LLM-based chatbots' ability to produce authentic apologies", "Discussion on moral and linguistic agency in the context of AI", "Implications for chatbot design and ethical considerations"], "limitations": "", "keywords": ["apologies", "LLM-based chatbots", "moral agency", "linguistic agency", "ethics"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.21096", "pdf": "https://arxiv.org/pdf/2506.21096.pdf", "abs": "https://arxiv.org/abs/2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "AI": {"tldr": "We propose DALR, a method for fine-grained cross-modal alignment in multimodal sentence representation, addressing bias and semantic divergence.", "motivation": "To address cross-modal misalignment bias and intra-modal semantic divergence that degrade sentence representation quality in multimodal approaches.", "method": "DALR employs a consistency learning module for softening negative samples and utilizes an auxiliary task for semantic similarity, combining it with ranking distillation and global intra-modal alignment learning.", "result": "Experiments on STS and TR tasks show DALR consistently outperforms state-of-the-art methods in sentence representation quality.", "conclusion": "The proposed methods effectively enhance multimodal sentence representations and provide a deeper understanding of sentence relationships beyond binary labels.", "key_contributions": ["Introduction of DALR for dual-level alignment in sentence representation", "Utilization of consistency learning for fine-grained cross-modal alignment", "Integration of ranking distillation with global intra-modal alignment learning"], "limitations": "", "keywords": ["multimodal", "sentence representation", "cross-modal alignment", "semantic similarity", "ranking distillation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.21098", "pdf": "https://arxiv.org/pdf/2506.21098.pdf", "abs": "https://arxiv.org/abs/2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "AI": {"tldr": "ComRAG is a retrieval-augmented generation framework designed to enhance Community Question Answering platforms by integrating static knowledge with dynamic historical QA data using a centroid-based memory mechanism.", "motivation": "To address the challenges in leveraging historical interactions and domain knowledge in real-time for Community Question Answering platforms.", "method": "Introduced ComRAG, a framework that combines static knowledge and dynamic historical QA pairs utilizing a centroid-based memory mechanism for retrieval and generation.", "result": "ComRAG outperforms baseline methods by achieving up to 25.9% improvement in vector similarity, reducing latency by 8.7% to 23.3%, and significantly lowering chunk growth over iterations.", "conclusion": "The framework effectively enhances the capability of CQA systems for real-time applications while efficiently managing knowledge.", "key_contributions": ["Integration of static and dynamic knowledge for CQA", "Centroid-based memory mechanism for efficient retrieval and storage", "Improvement in performance metrics like vector similarity and latency reduction"], "limitations": "", "keywords": ["Community Question Answering", "retrieval-augmented generation", "centroid-based memory", "real-time applications"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.21119", "pdf": "https://arxiv.org/pdf/2506.21119.pdf", "abs": "https://arxiv.org/abs/2506.21119", "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICONIP 2024", "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "AI": {"tldr": "Progtuning is a novel fine-tuning framework for Transformer-based language models that optimizes resource allocation by progressively reducing updated transformer blocks based on their contribution, achieving a 25% reduction in updated parameters while maintaining performance.", "motivation": "The growing size of Transformer models makes updating all parameters costly, and existing parameter-efficient methods often overlook contributions across different blocks, leading to inefficient resource allocation.", "method": "Progtuning employs progressive learning to selectively update a smaller subset of transformer blocks based on their importance during fine-tuning.", "result": "Progtuning reduces the number of updated parameters by approximately 25% while still achieving competitive model performance.", "conclusion": "The Progtuning framework not only optimizes resource usage but also shows adaptability with other parameter-efficient fine-tuning methods, performing well across different adaptation scenarios.", "key_contributions": ["Introduction of Progtuning for efficient fine-tuning of Transformer models", "Achieves 25% reduction in updated parameters", "High adaptability with existing parameter-efficient fine-tuning methods."], "limitations": "", "keywords": ["fine-tuning", "Transformer models", "parameter-efficient learning", "progressive learning", "resource allocation"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2506.21170", "pdf": "https://arxiv.org/pdf/2506.21170.pdf", "abs": "https://arxiv.org/abs/2506.21170", "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "AI": {"tldr": "Cosmos introduces a novel approach to text generation using diffusion models in a compressed latent space, achieving faster inference and maintaining generation quality.", "motivation": "To address the limitations of autoregressive language models in terms of slow decoding and global coherence in text generation.", "method": "Cosmos operates in a compressed, smooth latent space learned through an autoencoder, enabling effective perturbation-based augmentations for text generation.", "result": "Cosmos can compress text representations by $8\\times$ while maintaining comparable generation quality to token-level diffusion models, and it achieves more than $2\\times$ faster inference.", "conclusion": "Cosmos can surpass existing diffusion-based and autoregressive models in various generative tasks, demonstrating superior efficiency and quality.", "key_contributions": ["Introduction of a compressed latent space for text generation", "Demonstration of $2\\times$ faster inference compared to existing models", "Evaluation across four diverse generative tasks with superior or comparable quality."], "limitations": "", "keywords": ["text generation", "diffusion models", "latent space", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.21182", "pdf": "https://arxiv.org/pdf/2506.21182.pdf", "abs": "https://arxiv.org/abs/2506.21182", "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "AI": {"tldr": "The paper discusses engineering practices to enhance the reproducibility and extensibility of the Massive Text Embedding Benchmark (MTEB).", "motivation": "To maintain the reproducibility and extensibility of the MTEB as a standard evaluation platform for text embedding models.", "method": "The authors describe their approach to continuous integration pipelines that validate dataset integrity, automate test execution, and assess benchmark results' generalizability.", "result": "The engineering practices have successfully scaled MTEB, making it more comprehensive while ensuring its quality and relevance in the field.", "conclusion": "The experiences detailed in the paper provide insights for benchmark maintainers on ensuring reproducibility and usability in machine learning evaluation frameworks.", "key_contributions": ["Robust continuous integration pipelines for dataset validation.", "Strategies for handling community contributions to extend the benchmark.", "Design choices enhancing reproducibility and usability of MTEB."], "limitations": "", "keywords": ["text embedding", "benchmark", "reproducibility", "machine learning", "engineering practices"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.21191", "pdf": "https://arxiv.org/pdf/2506.21191.pdf", "abs": "https://arxiv.org/abs/2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "AI": {"tldr": "A novel transformer-based model for turn-taking prediction in dialogue systems allows dynamic control through textual prompts, improving accuracy and response adaptability.", "motivation": "To enhance turn-taking prediction in spoken dialogue systems by enabling intuitive control through textual prompts, adapting to various conversational contexts.", "method": "A transformer-based voice activity projection model was developed, integrating textual prompt embeddings into both channel-wise and cross-channel transformers. The model was trained and evaluated using 950 hours of human-human spoken dialogue data, supplemented by synthetic prompt sentences generated by a large language model.", "result": "The model showed improved prediction accuracy and successfully varied turn-taking behaviors in response to different textual prompts, demonstrating the effectiveness of the novel approach.", "conclusion": "The proposed model represents a significant advancement in the field of spoken dialogue systems, allowing for flexible and context-aware turn-taking prediction based on user instructions.", "key_contributions": ["Introduced dynamic control of turn-taking prediction using textual prompts", "Enhanced transformer model architecture for dialogue systems", "Utilized synthetic data generation from LLM to overcome dataset limitations"], "limitations": "The model relies on the availability of effective textual prompts and performance may vary with different conversational contexts not covered in the training data.", "keywords": ["turn-taking prediction", "transformer models", "dialogue systems", "textual prompts", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.21222", "pdf": "https://arxiv.org/pdf/2506.21222.pdf", "abs": "https://arxiv.org/abs/2506.21222", "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "AI": {"tldr": "This paper explores using syntactic retrieval for Automatic Term Extraction (ATE) with large language models, showing it enhances performance in identifying domain-specific terms.", "motivation": "The study investigates the potential of large language models for Automatic Term Extraction, an area that has seen limited exploration despite their success in other NLP tasks.", "method": "A retrieval-based prompting strategy that selects demonstrations based on syntactic similarity rather than semantic similarity in a few-shot setting.", "result": "Experiments reveal that the syntactic retrieval method improves the F1-score on three specialized ATE benchmarks, especially in both in-domain and cross-domain settings.", "conclusion": "The results underscore the significance of syntactic cues in leveraging LLMs for terminology extraction tasks.", "key_contributions": ["Introduction of a retrieval-based prompting strategy for ATE", "Demonstration of improved performance using syntactic retrieval", "Insights into the importance of syntactic cues for LLM adaptation in ATE"], "limitations": "", "keywords": ["Automatic Term Extraction", "Large Language Models", "Syntactic Retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.21252", "pdf": "https://arxiv.org/pdf/2506.21252.pdf", "abs": "https://arxiv.org/abs/2506.21252", "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "AI": {"tldr": "This paper introduces Agent-RewardBench, a benchmark for evaluating reward modeling in Multimodal Large Language Models (MLLMs) aimed at enhancing the performance of multimodal agents in real-world scenarios.", "motivation": "Multimodal agents face challenges in self-correction and generalization due to a lack of external feedback. Effective reward modeling is critical for their success in real-world tasks.", "method": "The authors developed Agent-RewardBench, which evaluates reward modeling capabilities in MLLMs across three dimensions: real-world scenarios, step-level reward evaluations, and quality control through rigorous sampling and verification processes.", "result": "Experiments show that even the best existing multimodal models perform poorly, indicating the necessity of specialized training for reward modeling in agents.", "conclusion": "Agent-RewardBench provides a significant tool for understanding and improving reward modeling in MLLMs, underscoring the current deficiencies in agent performance.", "key_contributions": ["Development of a benchmark specifically for assessing reward modeling in multimodal agents.", "Incorporation of multiple dimensions and real-world scenarios in the evaluation framework.", "Step-level reward assessment to gain insights into individual task performance."], "limitations": "The benchmark may still be limited by the existing capabilities of current multimodal models and the inherent complexity of real-world tasks.", "keywords": ["Multimodal Large Language Models", "reward modeling", "agents", "benchmark", "evaluation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.21274", "pdf": "https://arxiv.org/pdf/2506.21274.pdf", "abs": "https://arxiv.org/abs/2506.21274", "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "categories": ["cs.CL"], "comment": "(Submitted for publication)", "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "AI": {"tldr": "This paper investigates the effectiveness of statistical classifiers in detecting \"fake text\" generated by large language models, particularly in the context of classical detective fiction.", "motivation": "To explore whether the ability of LLMs to deceive detection methods may plateau as models grow in size and complexity.", "method": "Statistical classifiers were utilized to assess their ability to identify artificially generated text, comparing performances across different versions of language models.", "result": "Gemini showed increased proficiency in generating deceptive text with a version increase, whereas GPT did not exhibit similar improvements, indicating that detection methods may persist in reliability.", "conclusion": "Reliable detection of fake text could continue to be feasible, despite advancements in model architecture and parameter sizes, implying a possible limit to how deceptive models can become.", "key_contributions": ["Investigation of fake text detection in classical detective fiction style", "Comparison of LLM performance (Gemini vs GPT) in text generation", "Analysis of the potential plateau in detection capabilities of LLMs"], "limitations": "The study is limited to specific styles of text (detective fiction) and may not generalize to other genres or applications.", "keywords": ["fake text detection", "large language models", "statistical classifiers", "GPT", "Gemini"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.21285", "pdf": "https://arxiv.org/pdf/2506.21285.pdf", "abs": "https://arxiv.org/abs/2506.21285", "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "AI": {"tldr": "The paper introduces Double-Checker, a framework that enhances slow-thinking large language models (LLMs) by enabling self-critique and iterative refinement of solutions, showing improved reasoning performance on benchmarks.", "motivation": "To improve the reasoning capabilities of slow-thinking LLMs through explicit self-critique and refinement.", "method": "Double-Checker is a framework fine-tuned on 1,730 self-critical instances that allows long-CoT LLMs to evaluate and refine their outputs during inference.", "result": "The proposed framework increases the pass@1 performance on AIME benchmarks from 4.4% to 18.2%, demonstrating significantly enhanced reasoning capabilities.", "conclusion": "Double-Checker offers a promising approach for developing more trustworthy and effective LLMs through structured self-critique.", "key_contributions": ["Introduced Double-Checker framework for LLMs", "Demonstrated significant improvement in reasoning performance on benchmarks", "Fine-tuned model on self-critical instances for better self-evaluation"], "limitations": "", "keywords": ["large language models", "self-critique", "iterative refinement", "reasoning benchmarks", "AIME"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.21288", "pdf": "https://arxiv.org/pdf/2506.21288.pdf", "abs": "https://arxiv.org/abs/2506.21288", "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "AI": {"tldr": "This study presents a method for detecting the groundedness of queries in external context to improve the efficiency and accuracy of LLMs in NLP tasks.", "motivation": "To address the challenge of ungrounded speculation by LLMs when context without sufficient information is provided, ensuring factual consistency in responses.", "method": "The study employs lightweight encoder models like RoBERTa and NomicBERT, fine-tuned on curated datasets, for groundedness detection before generating answers.", "result": "The proposed detection mechanism achieves comparable accuracy to state-of-the-art LLMs while significantly reducing inference latency.", "conclusion": "Improving groundedness detection can lead to more trustworthy and resource-efficient applications of LLMs in NLP tasks.", "key_contributions": ["Development of a lightweight groundedness detection mechanism", "Demonstration of encoder models achieving high accuracy", "Reduction of inference time and resource consumption in LLMs"], "limitations": "", "keywords": ["large language models", "groundedness detection", "natural language processing", "RoBERTa", "NomicBERT"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.21294", "pdf": "https://arxiv.org/pdf/2506.21294.pdf", "abs": "https://arxiv.org/abs/2506.21294", "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "authors": ["Bram Willemsen", "Gabriel Skantze"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at XLLM @ ACL 2025", "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "AI": {"tldr": "This paper investigates the extraction of referring expressions from visually grounded dialogues using a text-only autoregressive language model.", "motivation": "To understand how linguistic context can aid in detecting mentions that have visually perceivable referents in dialogue.", "method": "Adaptation of a pretrained large language model to perform coarse-grained annotation of mention spans in dialogues by utilizing next-token prediction.", "result": "The text-only approach was found to be effective in extracting referring expressions even with moderate LLMs and limited datasets, emphasizing the importance of linguistic cues.", "conclusion": "While the results are promising, the task remains fundamentally multimodal which presents limitations for unimodal approaches.", "key_contributions": ["Demonstrated effectiveness of LLM in text-only mention extraction", "Highlighted linguistic context's importance for referring expressions", "Discussed the limitations of using unimodal approaches for a multimodal task"], "limitations": "The task is inherently multimodal, indicating limitations of unimodal approaches.", "keywords": ["referring expressions", "visually grounded dialogue", "large language model"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.21360", "pdf": "https://arxiv.org/pdf/2506.21360.pdf", "abs": "https://arxiv.org/abs/2506.21360", "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "categories": ["cs.CL"], "comment": "Accepted in CogSci 2025", "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "AI": {"tldr": "This paper introduces GLASS, a framework to improve LLMs' literary analysis using Greimas Semiotic Square, showcasing its effective application on a new dataset and classic works.", "motivation": "The aim is to enhance LLMs' capacity for professional literary criticism, addressing their limitations in analyzing complex narratives.", "method": "The paper presents the GLASS framework, utilizes a new dataset for GSS-based literary criticism, and employs LLMs in the 'LLM-as-a-judge' paradigm to evaluate performance.", "result": "GLASS demonstrates high performance in literary analysis compared to expert criticism and produces original analyses of classic works.", "conclusion": "The research provides an innovative AI tool for literary analysis and offers insights into cognitive mechanisms in literary engagement.", "key_contributions": ["Introduction of the GLASS framework for literary analysis", "Creation of the first dataset for GSS-based criticism", "Development of quantitative metrics for literary assessment with LLMs"], "limitations": "", "keywords": ["Large Language Models", "literary analysis", "Greimas Semiotic Square", "GSS", "artificial intelligence"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.21384", "pdf": "https://arxiv.org/pdf/2506.21384.pdf", "abs": "https://arxiv.org/abs/2506.21384", "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "AI": {"tldr": "This paper presents Omni-RAG, a framework to enhance retrieval-augmented generation systems' performance in handling complex, noisy user queries.", "motivation": "Current RAG systems struggle with noisy, ambiguous, and multiple-intent user queries, limiting their effectiveness in real-world applications.", "method": "Omni-RAG consists of three modules: (1) Deep Query Understanding for query denoising and decomposition, (2) Intent-Aware Knowledge Retrieval to retrieve and aggregate responses for sub-queries, and (3) Reranking and Generation to refine document selection before generating the final response with an LLM.", "result": "Omni-RAG shows improved robustness and effectiveness in live, open-domain settings by adequately handling complex queries compared to current RAG systems.", "conclusion": "The framework bridges the gap between RAG capabilities and real-world application demands, making it suitable for challenges presented in the SIGIR 2025 LiveRAG Challenge.", "key_contributions": ["Introduction of a comprehensive framework for RAG systems", "Modules for query understanding and intent-aware retrieval are proposed", "Demonstrates improvements in handling noisy and multi-intent queries"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Query Understanding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.21443", "pdf": "https://arxiv.org/pdf/2506.21443.pdf", "abs": "https://arxiv.org/abs/2506.21443", "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)\\-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk\\-sensitive scenarios. To\naddress these challenges, we present a Domain Knowledge (DK)\\-Enhanced LLM\nframework that integrates pretrained LLMs with structured, task\\-specific\ninsights to perform fraud and concept drift detection. The proposed\narchitecture consists of three main components: (1) a DK\\-LLM module to detect\nfake or deceptive conversations; (2) a drift detection unit (OCDD) to determine\nwhether a semantic shift has occurred; and (3) a second DK\\-LLM module to\nclassify the drift as either benign or fraudulent. We first validate the value\nof domain knowledge using a fake review dataset and then apply our full\nframework to SEConvo, a multiturn dialogue dataset that includes various types\nof fraud and spam attacks. Results show that our system detects fake\nconversations with high accuracy and effectively classifies the nature of\ndrift. Guided by structured prompts, the LLaMA\\-based implementation achieves\n98\\% classification accuracy. Comparative studies against zero\\-shot baselines\ndemonstrate that incorporating domain knowledge and drift awareness\nsignificantly improves performance, interpretability, and robustness in\nhigh\\-stakes NLP applications.", "AI": {"tldr": "The paper introduces a Domain Knowledge-Enhanced LLM framework to detect deceptive conversations and concept drift in NLP applications, improving accuracy and interpretability.", "motivation": "To enhance the detection of deceptive conversations and combat concept drift in dynamic language contexts, where traditional methods struggle due to evolving patterns and ambiguity.", "method": "The framework integrates pretrained LLMs with domain-specific insights and consists of a DK-LLM module for deception detection, a drift detection unit to identify semantic shifts, and another DK-LLM module for classifying the nature of the drift.", "result": "The proposed architecture achieved 98% classification accuracy in detecting fake conversations and classifying drift, outperforming zero-shot baselines, demonstrating improved interpretability and robustness in NLP tasks.", "conclusion": "Incorporating domain knowledge and drift awareness significantly boosts performance in high-stakes NLP applications, effectively addressing challenges of ambiguity and evolving language patterns.", "key_contributions": ["Introduction of a DK-Enhanced LLM framework for detecting deceptive conversations.", "Validation of the framework using a multiturn dialogue dataset and a fake review dataset.", "Demonstration of significant performance improvements and robustness in NLP applications."], "limitations": "", "keywords": ["Domain Knowledge", "Deceptive Conversations", "Concept Drift", "NLP", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.21445", "pdf": "https://arxiv.org/pdf/2506.21445.pdf", "abs": "https://arxiv.org/abs/2506.21445", "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "AI": {"tldr": "This paper evaluates the performance of large language models on multilingual Text2Cypher tasks, highlighting the need for enhanced multilingual support in database query generation.", "motivation": "To address the lack of evaluation in languages other than English for database query generation through large language models.", "method": "The paper creates a multilingual test set by translating English questions into Spanish and Turkish, evaluating several foundational models using standardized prompts.", "result": "The study finds highest performance on English, moderate on Spanish, and lowest on Turkish, attributed to training data availability and linguistic features.", "conclusion": "The research emphasizes the need for inclusive evaluation in multilingual query generation and suggests future exploration of schema localization and fine-tuning in diverse languages.", "key_contributions": ["Creation of a multilingual test set for Text2Cypher task", "Evaluation of foundational models across multiple languages", "Insights into the impact of task prompt translation on performance"], "limitations": "The study is limited to three languages (English, Spanish, Turkish) and focused mainly on foundational models without exploring advanced models.", "keywords": ["large language models", "multilingual", "Text2Cypher", "database queries", "cross-lingual evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.21463", "pdf": "https://arxiv.org/pdf/2506.21463.pdf", "abs": "https://arxiv.org/abs/2506.21463", "title": "Aligning Spoken Dialogue Models from User Interactions", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at ICML 2025", "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "AI": {"tldr": "This paper introduces a framework to enhance spoken dialogue models using a large-scale dataset of user interactions.", "motivation": "To improve spoken dialogue systems, which face challenges due to the complexities of real-time speech interactions that current methods do not address.", "method": "A novel preference alignment framework is employed, utilizing a dataset of over 150,000 annotated preference pairs derived from multi-turn speech conversations.", "result": "Results show that finetuning the speech-to-speech model with user feedback leads to improvements in generative quality, factual accuracy, and contextual relevance in spoken dialogue.", "conclusion": "The study emphasizes the significance of balancing diverse interaction dynamics for effective real-time speech dialogue systems.", "key_contributions": ["Development of a large-scale annotated dataset with 150,000 preference pairs for real-time dialogues.", "Introduction of a preference alignment framework tailored for spoken dialogue systems.", "Demonstration of significant improvements in spoken dialogue quality through human evaluation."], "limitations": "", "keywords": ["spoken dialogue systems", "preference alignment", "real-time speech interactions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.21468", "pdf": "https://arxiv.org/pdf/2506.21468.pdf", "abs": "https://arxiv.org/abs/2506.21468", "title": "TopK Language Models", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "AI": {"tldr": "This paper introduces TopK LMs, a modification to transformer architecture using a TopK activation function, enhancing interpretability while addressing limitations of traditional sparse autoencoders (SAEs).", "motivation": "Despite the utility of sparse autoencoders in analyzing transformer-based language model activations, their post-hoc training raises questions about internal validity and concept representation. This paper seeks to improve interpretability and stability in understanding how language models learn concepts.", "method": "The authors propose a modification to the transformer architecture that integrates a TopK activation function, allowing hidden states to serve as latent features without the need for post-hoc training. This facilitates enhanced interpretability and stability across different model checkpoints.", "result": "The results indicate that TopK LMs provide effective and interpretable sparse representations, enabling targeted neuron interventions and detailed comparative analysis of neuron formations across layers and checkpoints.", "conclusion": "TopK LMs demonstrate a balance of model efficiency and interpretability, contributing to a deeper understanding of concept learning in language models and aiming to enhance future research on model interpretability and controllability.", "key_contributions": ["Introduction of TopK LMs that enhance interpretability without post-hoc training.", "Demonstration of stable sparse representations across model checkpoints.", "Enablement of neuron intervention techniques for analysis of language models."], "limitations": "", "keywords": ["sparse autoencoders", "transformer architecture", "TopK activation", "interoperability", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.21495", "pdf": "https://arxiv.org/pdf/2506.21495.pdf", "abs": "https://arxiv.org/abs/2506.21495", "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "AI": {"tldr": "Evaluation of reinforcement learning for fine-tuning large language models in various online settings shows improved performance over offline methods, with a focus on training dynamics and hyperparameter optimization.", "motivation": "To explore reinforcement learning methods for fine-tuning large language models in transitioning training regimes.", "method": "Experiments compare online and semi-online Direct Preference Optimization and Group Reward Policy Optimization on verifiable and non-verifiable tasks using benchmark evaluations.", "result": "Similar performance and convergence noted between online and semi-online methods, both exceeding offline methods significantly.", "conclusion": "Multi-tasking with different reward types leads to better performance across tasks.", "key_contributions": ["Comparison of online vs. semi-online reinforcement learning methods for LLMs", "Insight into training dynamics and hyperparameter strategies", "Joint multitasking with diverse rewards improves task performance"], "limitations": "", "keywords": ["Reinforcement Learning", "Fine-tuning", "Large Language Models", "Online Learning", "Task Performance"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.21497", "pdf": "https://arxiv.org/pdf/2506.21497.pdf", "abs": "https://arxiv.org/abs/2506.21497", "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "AI": {"tldr": "This paper discusses enhancing user engagement in socially-driven dialogues by aligning interactive LLMs with user reactions using a user simulator and preference optimization.", "motivation": "To improve user engagement in socially-driven dialogues where traditional models may not adequately ensure user engagement.", "method": "The authors developed a user simulator to interact with interactive LLMs and utilized i×MCTS to gather data on user experiences, focusing on user reactions to influence dialogue intentions directly.", "result": "Experiments showed that the proposed method, using preference optimization, significantly improves user engagement in dialogues compared to traditional methods.", "conclusion": "Aligning interactive LLMs with direct indicators of user engagement leads to better user experiences in socially-driven dialogue scenarios.", "key_contributions": ["Development of a user simulator for interactive LLMs", "Introduction of i×MCTS for collecting user interaction data", "Implementation of direct preference optimization for aligning LLMs with user engagement"], "limitations": "Further research is needed to generalize the findings across diverse dialogue scenarios and user types.", "keywords": ["user engagement", "interactive LLMs", "socially-driven dialogues", "preference optimization", "user simulator"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.21508", "pdf": "https://arxiv.org/pdf/2506.21508.pdf", "abs": "https://arxiv.org/abs/2506.21508", "title": "skLEP: A Slovak General Language Understanding Benchmark", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "ACL 2025 Findings", "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "AI": {"tldr": "A benchmark for evaluating Slovak NLU models, including new datasets and extensive evaluations.", "motivation": "To assess the capabilities of Slovak natural language understanding models through a comprehensive and specific benchmarking framework.", "method": "The benchmark, named skLEP, encompasses nine tasks covering token-level, sentence-pair, and document-level assessments, utilizing both original Slovak datasets and translated English NLU resources.", "result": "The first systematic evaluation of various Slovak-specific, multilingual, and English pre-trained language models shows their performance on skLEP tasks.", "conclusion": "The release of skLEP and its associated resources aims to promote reproducibility and encourage further research in Slovak NLU.", "key_contributions": ["Introduction of the skLEP benchmark for Slovak NLU models.", "Compilation of diverse tasks for thorough evaluation.", "Release of datasets, toolkit, and public leaderboard for community use."], "limitations": "", "keywords": ["Slovak NLU", "benchmark", "natural language understanding", "pre-trained models", "evaluation tasks"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.21521", "pdf": "https://arxiv.org/pdf/2506.21521.pdf", "abs": "https://arxiv.org/abs/2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.", "AI": {"tldr": "The paper introduces a framework for evaluating large language models (LLMs) based on their performance on benchmark datasets, revealing the existence of 'potemkin' understanding where LLMs may appear to understand concepts without genuine comprehension.", "motivation": "To justify the use of benchmark datasets in evaluating LLM capabilities by exploring the nature of understanding reflected in their answers compared to human misunderstandings.", "method": "The authors develop a formal framework and two procedures for quantifying 'potemkin' understanding across various models and tasks, one using a specially designed benchmark and another providing a lower-bound estimate.", "result": "The study finds that 'potemkin' understandings are widespread across different models, tasks, and domains, indicating that these models often demonstrate incoherent concept representations.", "conclusion": "Success on benchmarks can be illusory if LLMs misinterpret concepts differently than humans, suggesting a need for deeper evaluation beyond traditional benchmarks.", "key_contributions": ["Introduction of a formal framework for evaluating LLMs' understanding", "Development of two procedures for quantifying misunderstanding in LLMs", "Identification of widespread 'potemkin' representations in multiple domains."], "limitations": "", "keywords": ["large language models", "benchmark evaluation", "potemkin understanding", "concept representation", "human evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.21532", "pdf": "https://arxiv.org/pdf/2506.21532.pdf", "abs": "https://arxiv.org/abs/2506.21532", "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat", "AI": {"tldr": "This paper presents the HealthChat-11K dataset, a curated collection of 11K healthcare conversations aimed at analyzing interactions users have with large language models in health contexts.", "motivation": "To explore the nature and risks of interactions between users and LLMs in healthcare, highlighting the need for improved dimensions of AI support in health-related chatbots.", "method": "The authors filtered large-scale conversational AI datasets to create the HealthChat-11K, a dataset with 11K conversations assessed through a clinician-driven taxonomy focusing on 21 distinct health specialties.", "result": "The analysis provides insights into user interactions, revealing patterns such as common inquiry types, affective behaviors, and potential leading questions that may alter the conversation's nature.", "conclusion": "The findings stress the importance of understanding user interactions with LLMs and suggest enhancements to the healthcare capabilities of conversational AI models.", "key_contributions": ["Introduction of the HealthChat-11K dataset for healthcare conversations", "Comprehensive analysis of user interactions categorized by health specialties", "Identification of interaction patterns that need addressing to improve LLM performance in healthcare."], "limitations": "The dataset may not represent all healthcare conversations and further research is needed to generalize the findings across diverse populations.", "keywords": ["HealthChat-11K", "large language models", "conversational AI", "healthcare information", "user interactions"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2506.21545", "pdf": "https://arxiv.org/pdf/2506.21545.pdf", "abs": "https://arxiv.org/abs/2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "categories": ["cs.CL"], "comment": null, "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "AI": {"tldr": "This paper introduces DELT, a paradigm focusing on data efficacy for language model training, which optimizes the organization of training data to enhance model performance without increasing data scale.", "motivation": "To improve language model performance by optimizing the organization of training data, specifically focusing on data efficacy, a relatively underexplored area.", "method": "DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. It includes a new scoring method called Learnability-Quality Scoring (LQS) that evaluates data samples based on learnability and quality, and a method called Folding Ordering (FO) to tackle model forgetting and data distribution bias.", "result": "DELT enhances language model performance without increasing data scale or model size. The best improvements are observed when combining LQS for data scoring with Folding for data ordering.", "conclusion": "Data efficacy is a promising foundational area in language model training, complementing data efficiency through effective data selection and organization.", "key_contributions": ["Introduction of the DELT paradigm for data efficacy in LM training", "Development of Learnability-Quality Scoring (LQS) for effective data scoring", "Proposal of Folding Ordering (FO) to mitigate model forgetting and data bias"], "limitations": "", "keywords": ["Language Models", "Data Efficacy", "Data Organization", "Machine Learning", "Learnability-Quality Scoring"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2308.04386", "pdf": "https://arxiv.org/pdf/2308.04386.pdf", "abs": "https://arxiv.org/abs/2308.04386", "title": "Learning Evaluation Models from Large Language Models for Sequence Generation", "authors": ["Chenglong Wang", "Hang Zhou", "Kaiyan Chang", "Tongran Liu", "Chunliang Zhang", "Quan Du", "Tong Xiao", "Yue Zhang", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by TASLP 2025", "summary": "Automatic evaluation of sequence generation, traditionally reliant on metrics\nlike BLEU and ROUGE, often fails to capture the semantic accuracy of generated\ntext sequences due to their emphasis on n-gram overlap. A promising solution to\nthis problem is to develop model-based metrics, such as BLEURT and COMET.\nHowever, these approaches are typically hindered by the scarcity of labeled\nevaluation data, which is necessary to train the evaluation models. In this\nwork, we build upon this challenge by proposing the Customized Sequence\nEvaluation Metric (CSEM), a three-stage evaluation model training method that\nutilizes large language models to generate labeled data for model-based metric\ndevelopment, thereby eliminating the need for human-labeled data. Additionally,\nwe expand the scope of CSEM to support various evaluation types, including\nsingle-aspect, multi-aspect, reference-free, and reference-based evaluations,\nenabling the customization of metrics to suit diverse real-world scenarios.\nExperimental results on the SummEval benchmark demonstrate that CSEM can\neffectively train an evaluation model without human-labeled data. Further\nexperiments in reinforcement learning and reranking show that metrics developed\nthrough CSEM outperform traditional evaluation metrics, leading to substantial\nimprovements in sequence quality as evaluated by both commonly used metrics and\nChatGPT.", "AI": {"tldr": "This paper introduces the Customized Sequence Evaluation Metric (CSEM), which uses large language models to generate labeled data for training evaluation models, aiming to improve semantic accuracy in sequence generation evaluation without the need for human-labeled data.", "motivation": "Traditional evaluation metrics like BLEU and ROUGE fail to capture the semantic accuracy of generated text, leading to the need for better evaluation methods in sequence generation.", "method": "CSEM employs a three-stage training method for model-based metrics that generates labeled data using large language models, allowing for customizable evaluation types including reference-free and multi-aspect evaluations.", "result": "CSEM was tested on the SummEval benchmark and showed the ability to effectively train evaluation models without human-labeled data, outperforming traditional metrics in various evaluations, including reinforcement learning and reranking.", "conclusion": "The Customized Sequence Evaluation Metric provides a novel approach to sequence evaluation that addresses the limitations of existing metrics by eliminating the need for human-labeled data and supporting various evaluation types.", "key_contributions": ["Introduction of CSEM for generating labeled data using LLMs", "Support for multiple evaluation types", "Demonstrated improvements over traditional metrics in sequence quality evaluations"], "limitations": "", "keywords": ["sequence generation", "evaluation metrics", "large language models", "semantic accuracy", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.18113", "pdf": "https://arxiv.org/pdf/2405.18113.pdf", "abs": "https://arxiv.org/abs/2405.18113", "title": "MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting", "authors": ["Hongda Sun", "Hongzhan Lin", "Haiyu Yan", "Yang Song", "Xin Gao", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025 Research Track", "summary": "Online recruitment platforms have reshaped job-seeking and recruiting\nprocesses, driving increased demand for applications that enhance person-job\nmatching. Traditional methods generally rely on analyzing textual data from\nresumes and job descriptions, limiting the dynamic, interactive aspects crucial\nto effective recruitment. Recent advances in Large Language Models (LLMs) have\nrevealed remarkable potential in simulating adaptive, role-based dialogues,\nmaking them well-suited for recruitment scenarios. In this paper, we propose\n\\textbf{MockLLM}, a novel framework to generate and evaluate mock interview\ninteractions. The system consists of two key components: mock interview\ngeneration and two-sided evaluation in handshake protocol. By simulating both\ninterviewer and candidate roles, MockLLM enables consistent and collaborative\ninteractions for real-time and two-sided matching. To further improve the\nmatching quality, MockLLM further incorporates reflection memory generation and\ndynamic strategy modification, refining behaviors based on previous experience.\nWe evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment\nplatform. The experimental results indicate that MockLLM outperforms existing\nmethods in matching accuracy, scalability, and adaptability across job domains,\nhighlighting its potential to advance candidate assessment and online\nrecruitment.", "AI": {"tldr": "The paper presents MockLLM, a framework for generating and evaluating mock interview interactions using LLMs to enhance job-candidate matching.", "motivation": "The need for enhanced person-job matching in online recruitment processes due to limitations in traditional methods that rely solely on textual data analysis.", "method": "MockLLM generates mock interviews and facilitates two-sided evaluations through a handshake protocol, simulating interactions between interviewers and candidates.", "result": "Experimental results on real-world data from Boss Zhipin demonstrate that MockLLM significantly outperforms traditional methods in matching accuracy, scalability, and adaptability.", "conclusion": "MockLLM shows considerable promise in improving candidate assessment and online recruitment processes.", "key_contributions": ["Introduction of MockLLM framework for mock interview generation", "Two-sided evaluation mechanism for real-time matching", "Incorporation of reflection memory and dynamic strategy modification to enhance interactions"], "limitations": "", "keywords": ["mock interviews", "person-job matching", "large language models", "online recruitment", "interactive dialogue"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2407.13358", "pdf": "https://arxiv.org/pdf/2407.13358.pdf", "abs": "https://arxiv.org/abs/2407.13358", "title": "Capturing Style in Author and Document Representation", "authors": ["Enzo Terreau", "Antoine Gourru", "Julien Velcin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects.", "AI": {"tldr": "This paper introduces a new architecture for learning author and document embeddings that incorporates stylistic constraints, enhancing the accuracy of authorship attribution tasks.", "motivation": "Existing NLP models often overlook the representation learning for authors, limiting their effectiveness in applications involving literary data and writing style.", "method": "The proposed architecture leverages Variational Information Bottleneck (VIB) to learn embeddings for authors and documents while incorporating stylistic features to make the representations interpretable in relation to writing style indicators.", "result": "The model was evaluated on datasets including a literary corpus, a blog authorship corpus, and IMDb62, showing improved performance in authorship attribution and better capturing of stylistic aspects compared to strong baselines.", "conclusion": "The incorporation of stylistic constraints in the embedding process allows for more accurate representation of authors’ writing styles, making the model applicable to various NLP tasks.", "key_contributions": ["Novel architecture based on Variational Information Bottleneck for author representation learning", "Stylistic constraints incorporated into embeddings for interpretability", "Evaluation on diverse datasets demonstrating improved authorship attribution performance"], "limitations": "", "keywords": ["Deep NLP", "Variational Information Bottleneck", "Authorship Attribution", "Writing Style", "Representation Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.09510", "pdf": "https://arxiv.org/pdf/2409.09510.pdf", "abs": "https://arxiv.org/abs/2409.09510", "title": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for Privacy-Preserving Personalization of Large Language Models", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL"], "comment": null, "summary": "Despite its substantial impact on various search, recommendation, and\nquestion answering tasks, privacy-preserving methods for personalizing large\nlanguage models (LLMs) have received relatively limited exploration. There is\none primary approach in this area through retrieval-augmented generation (RAG),\nwhich generates personalized outputs by enriching the input prompt with\ninformation retrieved from the user's personal data. This paper studies an\northogonal approach to RAG that involves learning user-dependent LLM parameters\nthrough parameter-efficient fine-tuning (PEFT). This paper presents the first\nsystematic study for exploration of PEFT for LLM personalization and provides\nan extensive comparisons between RAG- and PEFT-based solutions, across a broad\nset of seven diverse datasets from the LaMP benchmark. Our results demonstrate\nthat, on average, both RAG- and PEFT-based personalization methods yield 14.92%\nand 1.07% improvements over non-personalized LLMs, respectively. When combining\nRAG with PEFT, we observe a further improvement of 15.98%, highlighting the\neffectiveness of their integration in enhancing personalized text generation.\nAdditionally, we identify a positive correlation between the amount of user\ndata available and the effectiveness of PEFT. This finding suggests that RAG is\nparticularly beneficial for cold-start users -- users with limited personal\ndata -- while PEFT performs better when more user-specific data is available.", "AI": {"tldr": "This paper explores privacy-preserving personalization methods for large language models (LLMs), focusing on parameter-efficient fine-tuning (PEFT) and its comparison with retrieval-augmented generation (RAG).", "motivation": "There is limited exploration of privacy-preserving methods for personalizing LLMs, which are crucial for tasks like search and recommendation.", "method": "The study systematically explores PEFT for LLM personalization and compares it against RAG across seven datasets from the LaMP benchmark.", "result": "PEFT and RAG methods provide average improvements of 1.07% and 14.92% respectively over non-personalized LLMs, with their integration yielding a 15.98% enhancement.", "conclusion": "RAG is particularly advantageous for cold-start users, while PEFT is more effective when more personal data is available, indicating a correlation between data quantity and PEFT effectiveness.", "key_contributions": ["First systematic study on PEFT for LLM personalization", "Extensive comparison between RAG and PEFT methods", "Demonstrated improvements in personalized text generation with user data"], "limitations": "", "keywords": ["large language models", "privacy-preserving methods", "parameter-efficient fine-tuning", "retrieval-augmented generation", "personalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.09942", "pdf": "https://arxiv.org/pdf/2410.09942.pdf", "abs": "https://arxiv.org/abs/2410.09942", "title": "Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and RAG strategy. We introduce an\niterative approach where the search engine generates retrieval results for the\nRAG agents and gathers feedback on the quality of the retrieved documents\nduring an offline phase. This feedback is then used to iteratively optimize the\nsearch engine using an expectation-maximization algorithm, with the goal of\nmaximizing each agent's utility function. Additionally, we adapt this to an\nonline setting, allowing the search engine to refine its behavior based on\nreal-time individual agents feedback to better serve the results for each of\nthem. Experiments on datasets from the Knowledge-Intensive Language Tasks\n(KILT) benchmark demonstrates that our approach significantly on average\noutperforms baselines across 18 RAG models. We demonstrate that our method\neffectively ``personalizes'' the retrieval for each RAG agent based on the\ncollected feedback. Finally, we provide a comprehensive ablation study to\nexplore various aspects of our method.", "AI": {"tldr": "The paper proposes a unified search engine for multiple retrieval-augmented generation (RAG) agents that optimizes results based on feedback, showcasing significant improvements in personalized retrieval.", "motivation": "To enhance the effectiveness of retrieval-augmented generation agents through a unified search engine that adapts results to individual tasks.", "method": "An iterative approach using feedback from agents to optimize the search engine with an expectation-maximization algorithm, both offline and online settings.", "result": "The proposed method significantly outperforms existing baselines across 18 RAG models on KILT benchmark datasets, demonstrating effective personalization of retrieval results.", "conclusion": "The study highlights the benefits of personalized retrieval in improving the utility of RAG agents through adaptive search techniques.", "key_contributions": ["Introduces a unified search engine for diverse RAG agents.", "Develops an iterative optimization algorithm using agent feedback.", "Demonstrates significant performance improvements in personalized retrieval."], "limitations": "", "keywords": ["RAG agents", "retrieval-augmented generation", "expectation-maximization", "personalized retrieval", "KILT benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.16155", "pdf": "https://arxiv.org/pdf/2410.16155.pdf", "abs": "https://arxiv.org/abs/2410.16155", "title": "A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns", "authors": ["Tianyi Men", "Pengfei Cao", "Zhuoran Jin", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "With the development of large language models, they are widely used as agents\nin various fields. A key component of agents is memory, which stores vital\ninformation but is susceptible to jailbreak attacks. Existing research mainly\nfocuses on single-agent attacks and shared memory attacks. However, real-world\nscenarios often involve independent memory. In this paper, we propose the\nTroublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,\nmulti-agent, multi-topology text-based attack evaluation framework. TMCHT\ninvolves one attacker agent attempting to mislead an entire society of agents.\nWe identify two major challenges in multi-agent attacks: (1) Non-complete graph\nstructure, (2) Large-scale systems. We attribute these challenges to a\nphenomenon we term toxicity disappearing. To address these issues, we propose\nan Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes\nthe retrieval suffix to make poisoned samples more easily retrieved and\noptimizes the replication suffix to make poisoned samples have contagious\nability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,\n18.95%, and 52.93% improvements in line topology, star topology, and 100-agent\nsettings. Encourage community attention to the security of multi-agent systems.", "AI": {"tldr": "This paper introduces the TMCHT framework for evaluating multi-agent, multi-topology attacks, addressing challenges in agent memory and proposing the ARCJ method to enhance security against misleading attacks.", "motivation": "With the rise of large language models used as agents, there is a need to evaluate their memory security against multi-agent attacks, particularly in real-world independent memory scenarios.", "method": "The paper proposes the TMCHT framework for large-scale evaluation of multi-agent attacks and introduces the ARCJ method to optimize poisoned sample retrieval and enhance the replication ability of such samples.", "result": "The ARCJ method shows significant improvements in attack performance, achieving 23.51%, 18.95%, and 52.93% enhancements in various topologies and agent settings within the TMCHT framework.", "conclusion": "The findings highlight the importance of addressing memory vulnerabilities in multi-agent systems and encourage community focus on their security.", "key_contributions": ["Introduction of TMCHT multi-agent attack evaluation framework", "Development of ARCJ method for optimizing retrieval and replication of poisoned samples", "Identification of challenges in multi-agent attack scenarios related to independent memory"], "limitations": "", "keywords": ["multi-agent systems", "security", "large language models", "attack evaluation", "adversarial methods"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2410.21909", "pdf": "https://arxiv.org/pdf/2410.21909.pdf", "abs": "https://arxiv.org/abs/2410.21909", "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent", "authors": ["Xiao Xia", "Dan Zhang", "Zibo Liao", "Zhenyu Hou", "Tianrui Sun", "Jing Li", "Ling Fu", "Yuxiao Dong"], "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025", "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .", "AI": {"tldr": "This paper presents SceneGenAgent, an LLM-based agent designed to generate industrial scenes through C# code, addressing the unique challenges of precise measurement and layout in industrial manufacturing.", "motivation": "Generating industrial scenes with LLMs poses challenges due to the need for precise measurements and complex spatial arrangements, which are essential in industrial manufacturing simulations.", "method": "The SceneGenAgent utilizes a structured format for layout planning, verification processes, and iterative refinement to ensure accurate industrial scene generation. It is fine-tuned with the SceneInstruct dataset to enhance performance.", "result": "Experiments show SceneGenAgent achieves up to 81% success in generating real-world industrial scenes, significantly improving LLM performance when using the SceneInstruct dataset.", "conclusion": "The development of SceneGenAgent and the accompanying SceneInstruct dataset provides promising tools for accurately generating industrial scenes using LLMs, enhancing their usability in manufacturing simulations.", "key_contributions": ["Introduction of SceneGenAgent for industrial scene generation", "Development of SceneInstruct dataset for fine-tuning LLMs", "Demonstrated performance improvements with Llama3.1-70B approximating GPT-4o capabilities."], "limitations": "", "keywords": ["industrial scene generation", "large language models", "manufacturing simulations"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2411.02398", "pdf": "https://arxiv.org/pdf/2411.02398.pdf", "abs": "https://arxiv.org/abs/2411.02398", "title": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages", "authors": ["Hoang H Nguyen", "Khyati Mahajan", "Vikas Yadav", "Julian Salazar", "Philip S. Yu", "Masoud Hashemi", "Rishabh Maheshwary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to NAACL 2025 (Main Conference). This version contains minor\n  improvements to the camera-ready", "summary": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.", "AI": {"tldr": "This paper addresses the performance gap of multilingual LLMs on non-Latin script languages by introducing phonemic transcriptions to create script-invariant representations, improving in-context learning and overall model performance.", "motivation": "To tackle the underperformance of multilingual LLMs on non-Latin script languages due to their reliance on Latin scripts, which impacts their phonological representation capabilities.", "method": "The authors propose using phonemic transcriptions alongside traditional orthographic scripts to enhance the representation of languages and introduce a Mixed-ICL retrieval strategy for improved in-context learning.", "result": "By integrating phonemic signals, the proposed method leads to performance improvements of up to 12.6% for Latin and 15.1% for non-Latin script languages when compared to traditional ICL methods.", "conclusion": "The integration of phonemic transcriptions significantly improves the performance of multilingual LLMs on both Latin and non-Latin scripts, supporting the need for script-invariant representations in language models.", "key_contributions": ["Proposed the use of phonemic transcriptions for script-invariant representations.", "Introduced a Mixed-ICL retrieval strategy that utilizes both phonemic and orthographic scripts.", "Demonstrated significant performance improvements in multilingual contexts."], "limitations": "", "keywords": ["Multilingual LLMs", "Phonemic transcriptions", "In-context learning", "Performance improvements", "Latin and non-Latin scripts"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2411.05199", "pdf": "https://arxiv.org/pdf/2411.05199.pdf", "abs": "https://arxiv.org/abs/2411.05199", "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement", "authors": ["Leitian Tao", "Xiang Chen", "Tong Yu", "Tung Mai", "Ryan Rossi", "Yixuan Li", "Saayan Mitra"], "categories": ["cs.CL"], "comment": "TMLR 2025", "summary": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives.", "AI": {"tldr": "CodeLutra framework improves code generation with smaller LLMs by learning from both correct and incorrect outputs.", "motivation": "To enhance the efficiency of fine-tuning smaller, open-source LLMs for code generation by incorporating insights from incorrect attempts.", "method": "CodeLutra uses an iterative preference-based refinement approach, comparing successful and failed outputs to improve code generation.", "result": "On a data science coding task, the framework improved Llama-3-8B's accuracy from 28.2% to 48.6%, nearing GPT-4's performance with only 500 samples.", "conclusion": "CodeLutra offers a scalable and efficient method for improving code generation in smaller models, making them competitive with larger models without the need for massive datasets.", "key_contributions": ["Introduces a framework that utilizes both correct and incorrect code attempts for training.", "Implements iterative preference-based refinement for better output approximation.", "Demonstrates significant accuracy improvements with a minimal sample size."], "limitations": "", "keywords": ["Large Language Models", "Code Generation", "Fine-tuning", "Machine Learning", "Error Analysis"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2412.09587", "pdf": "https://arxiv.org/pdf/2412.09587.pdf", "abs": "https://arxiv.org/abs/2412.09587", "title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages", "authors": ["Chester Palen-Michel", "Maxwell Pickering", "Maya Kruse", "Jonne Sälevä", "Constantine Lignos"], "categories": ["cs.CL"], "comment": "Under review", "summary": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task.", "AI": {"tldr": "OpenNER 1.0 offers a standardized set of named entity recognition (NER) datasets across 52 languages, addressing various annotation issues and providing baseline performance results with multiple language models.", "motivation": "The need for a standardized resource in named entity recognition that supports multilingual and multi-ontology research.", "method": "Correction of annotation format issues and standardization of 36 NER corpora into a uniform structure, followed by performance evaluation using pretrained multilingual language models and large language models.", "result": "No single model outperformed others across all languages; discrepancies in large language model performance highlight further areas for improvement in NER tasks.", "conclusion": "OpenNER 1.0 serves as a foundational resource for advancing research in multilingual NER, yet challenges remain in achieving optimal performance from current models.", "key_contributions": ["Standardization of 36 NER datasets across 52 languages", "Baseline performance results from multiple language models", "Facilitation of future multilingual and multi-ontology NER research"], "limitations": "Further improvements are needed in large language models for NER tasks; no single best model identified.", "keywords": ["named entity recognition", "NER", "multilingual", "large language models", "OpenNER"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.14501", "pdf": "https://arxiv.org/pdf/2412.14501.pdf", "abs": "https://arxiv.org/abs/2412.14501", "title": "Do Large Language Models Advocate for Inferentialism?", "authors": ["Yuzuki Arai", "Sho Tsugawa"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims.", "AI": {"tldr": "This paper explores the application of inferential semantics to understand the language processing capabilities of large language models (LLMs), highlighting their anti-representationalist properties and proposing a new theory of truth aligned with LLMs' mechanisms.", "motivation": "The emergence of large language models (LLMs) introduces challenges in the philosophy of language, prompting a reevaluation of traditional semantics.", "method": "The paper analyzes LLMs using Robert Brandom's inferential semantics, contrasting it with traditional distributional semantics, and examines LLMs' properties and mechanisms like RLHF through the ISA approach.", "result": "LLMs exhibit anti-representationalist features in language processing, suggesting they may redefine traditional concepts in the philosophy of language.", "conclusion": "Inferential semantics offers valuable insights into LLM meaning generation, questioning strict semantic compositionality and externalism, though empirical investigations remain necessary.", "key_contributions": ["Application of inferential semantics to LLMs", "Development of a consensus theory of truth for LLMs", "Highlighting anti-representationalist properties of LLMs"], "limitations": "Highlighted tensions between inferentialism's philosophical tenets and LLMs' sub-symbolic processes.", "keywords": ["large language models", "inferential semantics", "philosophy of language", "anti-representationalism", "truth theory"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2501.19324", "pdf": "https://arxiv.org/pdf/2501.19324.pdf", "abs": "https://arxiv.org/abs/2501.19324", "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning", "authors": ["Baohao Liao", "Yuhui Xu", "Hanze Dong", "Junnan Li", "Christof Monz", "Silvio Savarese", "Doyen Sahoo", "Caiming Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.", "AI": {"tldr": "The paper presents Reward-Guided Speculative Decoding (RSD) to enhance inference efficiency in large language models by combining draft and target models based on a controlled bias for optimal output.", "motivation": "To improve inference efficiency in large language models (LLMs) and to optimize the trade-off between computational cost and output quality.", "method": "RSD combines a lightweight draft model with a more powerful target model, using a process reward model to evaluate intermediate decoding steps and decide on invoking the target model, aiming for efficiency and performance.", "result": "RSD achieves up to 4.4x fewer FLOPs in computation while yielding an average accuracy improvement of up to +3.5 compared to existing methods.", "conclusion": "RSD offers a robust and cost-effective approach for deploying LLMs efficiently in resource-intensive scenarios.", "key_contributions": ["Introduction of a framework that integrates draft and target models with a controlled bias.", "Theoretical demonstration of a threshold-based mixture strategy for optimal resource utilization.", "Significant efficiency gains in LLM decoding, outperforming existing methods in terms of computational cost and accuracy."], "limitations": "", "keywords": ["large language models", "efficiency", "speculative decoding", "reward model", "resource utilization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.15680", "pdf": "https://arxiv.org/pdf/2502.15680.pdf", "abs": "https://arxiv.org/abs/2502.15680", "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training", "authors": ["Jaydeep Borkar", "Matthew Jagielski", "Katherine Lee", "Niloofar Mireshghallah", "David A. Smith", "Christopher A. Choquette-Choo"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at the Findings of the Association for Computational\n  Linguistics (2025)", "summary": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.", "AI": {"tldr": "The study investigates the dynamic nature of PII memorization in LLMs and its implications for data privacy during training.", "motivation": "To understand how the memorization of personally identifiable information (PII) in large language models affects data privacy and user control over their information.", "method": "The study characterizes phenomena related to PII memorization during LLM training, focusing on how changes in dataset curation influence memorization dynamics.", "result": "Identified that similar-appearing PII can trigger memorization of previously seen data, adding new PII can significantly increase overall memorization, and removing PII can inadvertently lead to memorization of other PII.", "conclusion": "Model creators must be aware of privacy risks associated with PII when designing and training models to prevent unintended regurgitation of sensitive information.", "key_contributions": ["Characterization of assisted memorization in LLM training", "Quantification of increased memorization due to PII addition", "Identification of risks from PII removal during training"], "limitations": "", "keywords": ["PII memorization", "large language models", "data privacy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.21004", "pdf": "https://arxiv.org/pdf/2503.21004.pdf", "abs": "https://arxiv.org/abs/2503.21004", "title": "Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters", "authors": ["Mahmoud Alwakeel", "Emory Buck", "Jonathan G. Martin", "Imran Aslam", "Sudarshan Rajagopal", "Jian Pei", "Mihai V. Podgoreanu", "Christopher J. Lindsell", "An-Kwok Ian Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Pulmonary embolism (PE) registries accelerate practice improving research but\nrely on labor intensive manual abstraction of radiology reports. We examined\nwhether openly available large language models (LLMs) can automate concept\nextraction from computed tomography PE (CTPE) reports without loss of data\nquality. Four Llama 3 variants (3.0 8B, 3.1 8B, 3.1 70B, 3.3 70B) and one\nreviewer model, Phi 4 14B, were tested on 250 dual annotated CTPE reports from\neach of MIMIC IV and Duke University. Accuracy, positive predictive value (PPV)\nand negative predictive value (NPV) versus a human gold standard were measured\nacross model size, temperature and shot count. Mean accuracy rose with scale:\n0.83 (3.0 8B), 0.91 (3.1 8B) and 0.96 for both 70B variants; Phi 4 14B reached\n0.98. Accuracy differed by less than 0.03 between datasets, indicating external\nrobustness. In dual model concordance (L3 70B plus Phi 4 14B) PPV for PE\npresence was at least 0.95 and NPV at least 0.98, while location, thrombus\nburden, right heart strain and image quality artifacts each achieved PPV of at\nleast 0.90 and NPV of at least 0.95. Fewer than four percent of individual\nconcept annotations were discordant, and full agreement occurred in more than\nseventy five percent of reports. Large language models therefore provide a\nscalable, accurate solution for PE registry abstraction, and a dual model\nreview workflow can safeguard data quality with minimal human oversight.", "AI": {"tldr": "This study evaluates the efficacy of large language models (LLMs) in automating the extraction of concepts from pulmonary embolism (PE) reports, demonstrating high accuracy and minimal human input requirement.", "motivation": "To improve the efficiency and accuracy of extracting clinical data from radiology reports in PE registries, reducing the reliance on manual abstraction.", "method": "Four variants of Llama 3 and one reviewer model were tested on 250 dual-annotated CTPE reports from MIMIC IV and Duke University, measuring accuracy, positive predictive value (PPV), and negative predictive value (NPV) against human annotations.", "result": "Mean accuracy increased with model size, reaching up to 0.98 with the Phi 4 model. The dual model concordance achieved a PPV of at least 0.95 for PE presence and NPV of at least 0.98, with over 75% full agreement in reports.", "conclusion": "LLMs can automate and enhance the data abstraction process for PE registries efficiently, allowing for high-quality data collection with reduced human effort.", "key_contributions": ["Demonstrated high accuracy (up to 0.98) of LLMs in abstracting PE reports.", "Showed that dual model workflows can improve data quality with minimal oversight.", "Established the feasibility of using LLMs for large-scale medical data extraction."], "limitations": "Results were based on a limited set of CTPE reports from two institutions; generalizability to other types of medical reports needs further validation.", "keywords": ["large language models", "pulmonary embolism", "concept extraction", "radiology reports", "classification accuracy"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.20081", "pdf": "https://arxiv.org/pdf/2506.20081.pdf", "abs": "https://arxiv.org/abs/2506.20081", "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant. Based on our discoveries, we propose SACL, a framework that\nenriches textual information and reduces bias by augmenting code or structural\nknowledge with semantic information. Extensive experiments show that SACL\nsubstantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on\nHumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation\nperformance (e.g., by 4.88% Pass@1 on HumanEval).", "AI": {"tldr": "This paper presents SACL, a framework that improves code retrieval and generation by enriching textual information and addressing biases towards well-documented code.", "motivation": "To enhance code generation by retrieving relevant information more effectively and to address biases in current retrieval methods.", "method": "The authors conduct an analysis of code retrieval by systematically masking certain features, while proposing SACL to augment code and structural knowledge with semantic information.", "result": "SACL shows significant improvement in code retrieval performance, with increases in Recall@1 by 12.8% on HumanEval and 9.4% on MBPP, leading to a 4.88% improvement in Pass@1 on HumanEval.", "conclusion": "The findings validate that enhancing retrieval methods can lead to better code generation outcomes, while also reducing reliance on irrelevant documentation.", "key_contributions": ["Introduction of SACL framework for code retrieval enhancement.", "Systematic analysis of current retrieval methods and their biases.", "Demonstrated improvement in recall and generation performance metrics."], "limitations": "The study primarily focuses on well-documented code, and the effects of SACL on poorly documented code are not explored.", "keywords": ["Retrieval-Augmented Code Generation", "Semantic Information", "Code Retrieval", "HumanEval", "Bias in Code"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.20409", "pdf": "https://arxiv.org/pdf/2506.20409.pdf", "abs": "https://arxiv.org/abs/2506.20409", "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.", "AI": {"tldr": "This paper introduces TAPS, a solution for enhancing personalized tool use in goal-oriented dialogue agents by integrating user preferences into large language models.", "motivation": "To address the overlooked role of personalisation in guiding tool use within tool-augmented large language models.", "method": "The paper presents TAPS, which utilizes a structured tagging tool and an uncertainty-based tool detector to improve the personalization capabilities of LLMs.", "result": "TAPS significantly enhances the ability of LLMs to incorporate user preferences and achieves state-of-the-art performance on the NLSI task for open source models.", "conclusion": "The findings demonstrate that integrating user preferences can substantially improve the effectiveness of LLMs in goal-oriented tasks.", "key_contributions": ["Introduction of TAPS for personalized tool use in LLMs", "Identification of weaknesses in current LLM personalisation approaches", "Achievement of state-of-the-art performance on the NLSI task."], "limitations": "", "keywords": ["personalisation", "large language models", "dialogue agents", "tool use", "TAPS"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.20639", "pdf": "https://arxiv.org/pdf/2506.20639.pdf", "abs": "https://arxiv.org/abs/2506.20639", "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation", "authors": ["Shansan Gong", "Ruixiang Zhang", "Huangjie Zheng", "Jiatao Gu", "Navdeep Jaitly", "Lingpeng Kong", "Yizhe Zhang"], "categories": ["cs.CL"], "comment": "minor update", "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR bias during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.", "AI": {"tldr": "This paper explores the decoding behavior and reinforcement learning methods of diffusion large language models (dLLMs) in code generation, presenting a new model, DiffuCoder.", "motivation": "Current training and inference mechanisms for dLLMs in coding are under-explored, necessitating a systematic investigation to unlock their potential for code generation.", "method": "The paper introduces the DiffuCoder model trained on 130B tokens and analyzes its denoising processes, proposing a novel sampling scheme called coupled-GRPO to enhance RL training.", "result": "DiffuCoder outperforms existing models by +4.4% on the EvalPlus code generation benchmarks, revealing insights into dLLM generation and reducing AR bias during decoding.", "conclusion": "The study advances the understanding of dLLM decoding and proposes an effective, diffusion-native RL training framework.", "key_contributions": ["Introduction of DiffuCoder, a dLLM for code generation.", "Systematic investigation of dLLM denoising processes.", "Proposal of coupled-GRPO, a novel RL training scheme."], "limitations": "", "keywords": ["diffusion models", "large language models", "code generation"], "importance_score": 9, "read_time_minutes": 15}}
