{"id": "2506.15794", "pdf": "https://arxiv.org/pdf/2506.15794.pdf", "abs": "https://arxiv.org/abs/2506.15794", "title": "Veracity: An Open-Source AI Fact-Checking System", "authors": ["Taylor Lynn Curtis", "Maximilian Puelma Touzel", "William Garneau", "Manon Gruaz", "Mike Pinder", "Li Wei Wang", "Sukanya Krishna", "Luda Cohen", "Jean-Fran√ßois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of misinformation poses a significant threat to society,\nexacerbated by the capabilities of generative AI. This demo paper introduces\nVeracity, an open-source AI system designed to empower individuals to combat\nmisinformation through transparent and accessible fact-checking. Veracity\nleverages the synergy between Large Language Models (LLMs) and web retrieval\nagents to analyze user-submitted claims and provide grounded veracity\nassessments with intuitive explanations. Key features include multilingual\nsupport, numerical scoring of claim veracity, and an interactive interface\ninspired by familiar messaging applications. This paper will showcase\nVeracity's ability to not only detect misinformation but also explain its\nreasoning, fostering media literacy and promoting a more informed society."}
{"id": "2506.15830", "pdf": "https://arxiv.org/pdf/2506.15830.pdf", "abs": "https://arxiv.org/abs/2506.15830", "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics", "authors": ["Riccardo Di Sipio"], "categories": ["cs.CL", "quant-ph", "I.2; I.7"], "comment": "9 pages, 1 figure(s)", "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems."}
{"id": "2506.15841", "pdf": "https://arxiv.org/pdf/2506.15841.pdf", "abs": "https://arxiv.org/abs/2506.15841", "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents", "authors": ["Zijian Zhou", "Ao Qu", "Zhaoxuan Wu", "Sunghwan Kim", "Alok Prakash", "Daniela Rus", "Jinhua Zhao", "Bryan Kian Hsiang Low", "Paul Pu Liang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized."}
{"id": "2506.15846", "pdf": "https://arxiv.org/pdf/2506.15846.pdf", "abs": "https://arxiv.org/abs/2506.15846", "title": "Finance Language Model Evaluation (FLaME)", "authors": ["Glenn Matlin", "Mika Okamoto", "Huzaifa Pardawala", "Yang Yang", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Language Models (LMs) have demonstrated impressive capabilities with core\nNatural Language Processing (NLP) tasks. The effectiveness of LMs for highly\nspecialized knowledge-intensive tasks in finance remains difficult to assess\ndue to major gaps in the methodologies of existing evaluation frameworks, which\nhave caused an erroneous belief in a far lower bound of LMs' performance on\ncommon Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for\nthese FinNLP tasks, we present the first holistic benchmarking suite for\nFinancial Language Model Evaluation (FLaME). We are the first research paper to\ncomprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical\nstudy of 23 foundation LMs over 20 core NLP tasks in finance. We open-source\nour framework software along with all data and results."}
{"id": "2506.15834", "pdf": "https://arxiv.org/pdf/2506.15834.pdf", "abs": "https://arxiv.org/abs/2506.15834", "title": "Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study", "authors": ["Zachary D King", "Maryam Khalid", "Han Yu", "Kei Shibuya", "Khadija Zanna", "Marzieh Majd", "Ryan L Brown", "Yufei Shen", "Thomas Vaessen", "George Kypriotakis", "Christopher P Fagundes", "Akane Sano"], "categories": ["cs.HC"], "comment": null, "summary": "Mobile health (mHealth) systems help researchers monitor and care for\npatients in real-world settings. Studies utilizing mHealth applications use\nEcological Momentary Assessment (EMAs), passive sensing, and contextual\nfeatures to develop emotion recognition models, which rely on EMA responses as\nground truth. Due to this, it is crucial to consider EMA compliance when\nconducting a successful mHealth study. Utilizing machine learning is one\napproach that can solve this problem by sending EMAs based on the predicted\nlikelihood of a response. However, literature suggests that this approach may\nlead to prompting participants more frequently during emotions associated with\nresponsiveness, thereby narrowing the range of emotions collected. We propose a\nmulti-objective function that utilizes machine learning to identify optimal\ntimes for sending EMAs. The function identifies optimal moments by combining\npredicted response likelihood with model uncertainty in emotion predictions.\nUncertainty would lead the function to prioritize time points when the model is\nless confident, which often corresponds to underrepresented emotions. We\ndemonstrate that this objective function would result in EMAs being sent when\nparticipants are responsive and experiencing less commonly observed emotions.\nThe evaluation is conducted offline using two datasets: (1) 91 spousal\ncaregivers of individuals with Alzheimer's Disease and Related dementias\n(ADRD), (2) 45 healthy participants. Results show that the multi-objective\nfunction tends to be higher when participants respond to EMAs and report less\ncommonly observed emotions. This suggests that using the proposed objective\nfunction to guide EMA delivery could improve receptivity rates and capture a\nbroader range of emotions."}
{"id": "2506.15889", "pdf": "https://arxiv.org/pdf/2506.15889.pdf", "abs": "https://arxiv.org/abs/2506.15889", "title": "Entropy-Driven Pre-Tokenization for Byte-Pair Encoding", "authors": ["Yifan Hu", "Frank Liang", "Dachuan Zhao", "Jonathan Geuter", "Varshini Reddy", "Craig W. Schmidt", "Chris Tanner"], "categories": ["cs.CL"], "comment": null, "summary": "Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization\nmethod in modern language models due to its simplicity and strong empirical\nperformance across downstream tasks. However, applying BPE to unsegmented\nlanguages such as Chinese presents significant challenges, as its\nfrequency-driven merge operation is agnostic to linguistic boundaries. To\naddress this, we propose two entropy-informed pre-tokenization strategies that\nguide BPE segmentation using unsupervised information-theoretic cues. The first\napproach uses pointwise mutual information and left/right entropy to identify\ncoherent character spans, while the second leverages predictive entropy derived\nfrom a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both\nmethods on a subset of the PKU dataset and demonstrate substantial improvements\nin segmentation precision, recall, and F1 score compared to standard BPE. Our\nresults suggest that entropy-guided pre-tokenization not only enhances\nalignment with gold-standard linguistic units but also offers a promising\ndirection for improving tokenization quality in low-resource and multilingual\nsettings."}
{"id": "2506.15873", "pdf": "https://arxiv.org/pdf/2506.15873.pdf", "abs": "https://arxiv.org/abs/2506.15873", "title": "DeckFlow: Iterative Specification on a Multimodal Generative Canvas", "authors": ["Gregory Croisdale", "Emily Huang", "John Joon Young Chung", "Anhong Guo", "Xu Wang", "Austin Z. Henley", "Cyrus Omar"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI promises to allow people to create high-quality personalized\nmedia. Although powerful, we identify three fundamental design problems with\nexisting tooling through a literature review. We introduce a multimodal\ngenerative AI tool, DeckFlow, to address these problems. First, DeckFlow\nsupports task decomposition by allowing users to maintain multiple\ninterconnected subtasks on an infinite canvas populated by cards connected\nthrough visual dataflow affordances. Second, DeckFlow supports a specification\ndecomposition workflow where an initial goal is iteratively decomposed into\nsmaller parts and combined using feature labels and clusters. Finally, DeckFlow\nsupports generative space exploration by generating multiple prompt and output\nvariations, presented in a grid, that can feed back recursively into the next\ndesign iteration. We evaluate DeckFlow for text-to-image generation against a\nstate-of-practice conversational AI baseline for image generation tasks. We\nthen add audio generation and investigate user behaviors in a more open-ended\ncreative setting with text, image, and audio outputs."}
{"id": "2506.15894", "pdf": "https://arxiv.org/pdf/2506.15894.pdf", "abs": "https://arxiv.org/abs/2506.15894", "title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning", "authors": ["Sam Silver", "Jimin Sun", "Ivan Zhang", "Sara Hooker", "Eddie Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive mathematical\nreasoning capabilities, yet their performance remains brittle to minor\nvariations in problem description and prompting strategy. Furthermore,\nreasoning is vulnerable to sampling-induced errors which autoregressive models\nmust primarily address using self-correction via additionally-generated tokens.\nTo better understand self-correction capabilities of recent models, we conduct\nexperiments measuring models' ability to self-correct synthetic perturbations\nintroduced into their Chain of Thought (CoT) reasoning. We observe robust\nsingle-utterance intrinsic self-correction behavior across a range of\nopen-weight models and datasets, ranging from subtle, implicit corrections to\nexplicit acknowledgments and corrections of errors. Our findings suggest that\nLLMs, including those not finetuned for long CoT, may possess stronger\nintrinsic self-correction capabilities than commonly shown in the literature.\nThe presence of this ability suggests that recent \"reasoning\" model work\ninvolves amplification of traits already meaningfully present in models."}
{"id": "2506.15883", "pdf": "https://arxiv.org/pdf/2506.15883.pdf", "abs": "https://arxiv.org/abs/2506.15883", "title": "Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration", "authors": ["Jonathan Zong", "Isabella Pedraza Pineros", "Mengzhu Katie Chen", "Daniel Hajas", "Arvind Satyanarayan"], "categories": ["cs.HC"], "comment": null, "summary": "Drawing connections between interesting groupings of data and their\nreal-world meaning is an important, yet difficult, part of encountering a new\ndataset. A lay reader might see an interesting visual pattern in a chart but\nlack the domain expertise to explain its meaning. Or, a reader might be\nfamiliar with a real-world concept but struggle to express it in terms of a\ndataset's fields. In response, we developed semantic scaffolding, a technique\nfor using domain-specific information from large language models (LLMs) to\nidentify, explain, and formalize semantically meaningful data groupings. We\npresent groupings in two ways: as semantic bins, which segment a field into\ndomain-specific intervals and categories; and data highlights, which annotate\nsubsets of data records with their real-world meaning. We demonstrate and\nevaluate this technique in Olli, an accessible visualization tool that\nexemplifies tensions around explicitly defining groupings while respecting the\nagency of readers to conduct independent data exploration. We conducted a study\nwith 15 blind and low-vision (BLV) users and found that readers used semantic\nscaffolds to quickly understand the meaning of the data, but were often also\ncritically aware of its influence on their interpretation."}
{"id": "2506.15911", "pdf": "https://arxiv.org/pdf/2506.15911.pdf", "abs": "https://arxiv.org/abs/2506.15911", "title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents", "authors": ["Mohammad Amaan Sayeed", "Mohammed Talha Alam", "Raza Imam", "Shahab Saquib Sohail", "Amir Hussain"], "categories": ["cs.CL"], "comment": "Under-review at the 4th Muslims in Machine Learning (MusIML) Workshop\n  (ICML-25)", "summary": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the\nProphetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and\nholistic therapies, yet remain inaccessible to many and underutilized in modern\nAI systems. Existing language-model benchmarks focus narrowly on factual recall\nor user preference, leaving a gap in validating culturally grounded medical\nguidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that\naligns 30 carefully curated Prophetic-medicine questions with human-verified\nremedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three\nconfigurations: direct generation, retrieval-augmented generation, and a\nscientific self-critique filter. Each answer is then assessed by a secondary\nLLM serving as an agentic judge, yielding a single 3C3H quality score.\nRetrieval improves factual accuracy by 13%, while the agentic prompt adds\nanother 10% improvement through deeper mechanistic insight and safety\nconsiderations. Our results demonstrate that blending classical Islamic texts\nwith retrieval and self-evaluation enables reliable, culturally sensitive\nmedical question-answering."}
{"id": "2506.16008", "pdf": "https://arxiv.org/pdf/2506.16008.pdf", "abs": "https://arxiv.org/abs/2506.16008", "title": "ChatAR: Conversation Support using Large Language Model and Augmented Reality", "authors": ["Yuichiro Fujimoto"], "categories": ["cs.HC"], "comment": null, "summary": "Engaging in smooth conversations with others is a crucial social skill.\nHowever, differences in knowledge between conversation participants can\nsometimes hinder effective communication. To tackle this issue, this study\nproposes a real-time support system that integrates head-mounted display\n(HMD)-based augmented reality (AR) technology with large language models\n(LLMs). This system facilitates conversation by recognizing keywords during\ndialogue, generating relevant information using the LLM, reformatting it, and\npresenting it to the user via the HMD. A significant issue with this system is\nthat the user's eye movements may reveal to the conversation partner that they\nare reading the displayed text. This study also proposes a method for\npresenting information that takes into account appropriate eye movements during\nconversation. Two experiments were conducted to evaluate the effectiveness of\nthe proposed system. The first experiment revealed that the proposed\ninformation presentation method reduces the likelihood of the conversation\npartner noticing that the user is reading the displayed text. The second\nexperiment demonstrated that the proposed method led to a more balanced speech\nratio between the user and the conversation partner, as well as a increase in\nthe perceived excitement of the conversation."}
{"id": "2506.15925", "pdf": "https://arxiv.org/pdf/2506.15925.pdf", "abs": "https://arxiv.org/abs/2506.15925", "title": "Reranking-based Generation for Unbiased Perspective Summarization", "authors": ["Narutatsu Ri", "Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Generating unbiased summaries in real-world settings such as political\nperspective summarization remains a crucial application of Large Language\nModels (LLMs). Yet, existing evaluation frameworks rely on traditional metrics\nfor measuring key attributes such as coverage and faithfulness without\nverifying their applicability, and efforts to develop improved summarizers are\nstill nascent. We address these gaps by (1) identifying reliable metrics for\nmeasuring perspective summary quality, and (2) investigating the efficacy of\nLLM-based methods beyond zero-shot inference. Namely, we build a test set for\nbenchmarking metric reliability using human annotations and show that\ntraditional metrics underperform compared to language model-based metrics,\nwhich prove to be strong evaluators. Using these metrics, we show that\nreranking-based methods yield strong results, and preference tuning with\nsynthetically generated and reranking-labeled data further boosts performance.\nOur findings aim to contribute to the reliable evaluation and development of\nperspective summarization methods."}
{"id": "2506.16010", "pdf": "https://arxiv.org/pdf/2506.16010.pdf", "abs": "https://arxiv.org/abs/2506.16010", "title": "SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion", "authors": ["Xiangyang He", "Jiale Li", "Jiahao Chen", "Yang Yang", "Mingming Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Panel discussion allows the audience to learn different perspectives through\ninteractive discussions among experts moderated by a host and a Q&A session\nwith the audience. Despite its benefits, panel discussion in the real world is\ninaccessible to many who do not have the privilege to participate due to\ngeographical, financial, and time constraints. We present SimuPanel, which\nsimulates panel discussions among academic experts through LLM-based\nmulti-agent interaction. It enables users to define topics of interest for the\npanel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel\nemploys a host-expert architecture where each panel member is simulated by an\nagent with specialized expertise, and the panel is visualized in an immersive\n3D environment to enhance engagement. Traditional dialogue generation struggles\nto capture the depth and interactivity of real-world panel discussions. To\naddress this limitation, we propose a novel multi-agent interaction framework\nthat simulates authentic panel dynamics by modeling reasoning strategies and\npersonas of experts grounded in multimedia sources. This framework enables\nagents to dynamically recall and contribute to the discussion based on past\nexperiences from diverse perspectives. Our technical evaluation and the user\nstudy with university students show that SimuPanel was able to simulate more\nin-depth discussions and engage participants to interact with and reflect on\nthe discussions. As a first step in this direction, we offer design\nimplications for future avenues to improve and harness the power of panel\ndiscussion for multimedia learning."}
{"id": "2506.15978", "pdf": "https://arxiv.org/pdf/2506.15978.pdf", "abs": "https://arxiv.org/abs/2506.15978", "title": "A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension", "authors": ["Toan Nguyen Hai", "Ha Nguyen Viet", "Truong Quan Xuan", "Duc Do Minh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Vietnamese, the 20th most spoken language with over 102 million native\nspeakers, lacks robust resources for key natural language processing tasks such\nas text segmentation and machine reading comprehension (MRC). To address this\ngap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice\nReading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset\nincludes 15,942 documents for text segmentation and 16,347 synthetic\nmultiple-choice question-answer pairs generated with human quality assurance,\nensuring a reliable and diverse resource. Experiments show that mBERT\nconsistently outperforms monolingual models on both tasks, achieving an\naccuracy of 88.01% on MRC test set and an F1 score of 63.15\\% on text\nsegmentation test set. Our analysis reveals that multilingual models excel in\nNLP tasks for Vietnamese, suggesting potential applications to other\nunder-resourced languages. VSMRC is available at HuggingFace"}
{"id": "2506.16044", "pdf": "https://arxiv.org/pdf/2506.16044.pdf", "abs": "https://arxiv.org/abs/2506.16044", "title": "Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications", "authors": ["MH Farhadi", "Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Wei Xu", "Reza Abiri"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "With recent advancements in AI and computational tools, intelligent paradigms\nhave emerged to enhance fields like shared autonomy and human-machine teaming\nin healthcare. Advanced AI algorithms (e.g., reinforcement learning) can\nautonomously make decisions to achieve planning and motion goals. However, in\nhealthcare, where human intent is crucial, fully independent machine decisions\nmay not be ideal. This chapter presents a comprehensive review of\nhuman-centered shared autonomy AI frameworks, focusing on upper limb\nbiosignal-based machine interfaces and associated motor control systems,\nincluding computer cursors, robotic arms, and planar platforms. We examine\nmotor planning, learning (rehabilitation), and control, covering conceptual\nfoundations of human-machine teaming in reach-and-grasp tasks and analyzing\nboth theoretical and practical implementations. Each section explores how human\nand machine inputs can be blended for shared autonomy in healthcare\napplications. Topics include human factors, biosignal processing for intent\ndetection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,\nassistive robotics, and Large Language Models (LLMs) as the next frontier. We\npropose adaptive shared autonomy AI as a high-performance paradigm for\ncollaborative human-AI systems, identify key implementation challenges, and\noutline future directions, particularly regarding AI reasoning agents. This\nanalysis aims to bridge neuroscientific insights with robotics to create more\nintuitive, effective, and ethical human-machine teaming frameworks."}
{"id": "2506.15981", "pdf": "https://arxiv.org/pdf/2506.15981.pdf", "abs": "https://arxiv.org/abs/2506.15981", "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion", "authors": ["Markus Frohmann", "Gabriel Meseguer-Brocal", "Markus Schedl", "Elena V. Epure"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Findings", "summary": "The rapid advancement of AI-based music generation tools is revolutionizing\nthe music industry but also posing challenges to artists, copyright holders,\nand providers alike. This necessitates reliable methods for detecting such\nAI-generated content. However, existing detectors, relying on either audio or\nlyrics, face key practical limitations: audio-based detectors fail to\ngeneralize to new or unseen generators and are vulnerable to audio\nperturbations; lyrics-based methods require cleanly formatted and accurate\nlyrics, unavailable in practice. To overcome these limitations, we propose a\nnovel, practically grounded approach: a multimodal, modular late-fusion\npipeline that combines automatically transcribed sung lyrics and speech\nfeatures capturing lyrics-related information within the audio. By relying on\nlyrical aspects directly from audio, our method enhances robustness, mitigates\nsusceptibility to low-level artifacts, and enables practical applicability.\nExperiments show that our method, DE-detect, outperforms existing lyrics-based\ndetectors while also being more robust to audio perturbations. Thus, it offers\nan effective, robust solution for detecting AI-generated music in real-world\nscenarios. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."}
{"id": "2506.16107", "pdf": "https://arxiv.org/pdf/2506.16107.pdf", "abs": "https://arxiv.org/abs/2506.16107", "title": "From 600 Tools to 1 Console: A UX-Driven Transformation", "authors": ["Mariann Kornelia Smith", "Jacqueline Meijer-Irons", "Andrew Millar"], "categories": ["cs.HC"], "comment": null, "summary": "In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a\nsurvey to 10,000 Google Developers (Googlers) and uncovered that Google's\ninternal infrastructure tools were fragmented and inefficient, hindering\ndevelopers' productivity. Using user centered research and design methodologies\nthe team first created a story map and service blueprint to visualize the\nrelationship between internal applications, then formulated a strategic vision\nto consolidate tools, streamline workflows, and measure the impact of their\nwork. We secured executive buy-in and delivered incremental improvements."}
{"id": "2506.16024", "pdf": "https://arxiv.org/pdf/2506.16024.pdf", "abs": "https://arxiv.org/abs/2506.16024", "title": "From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation", "authors": ["Zhihan Guo", "Jiele Wu", "Wenqian Cui", "Yifei Zhang", "Minda Hu", "Yufei Wang", "Irwin King"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on long-form context in Large Language Models (LLMs)\nprimarily focuses on the understanding of long-contexts, the Open-ended Long\nText Generation (Open-LTG) remains insufficiently explored. Training a\nlong-context generation model requires curation of gold standard reference\ndata, which is typically nonexistent for informative Open-LTG tasks. However,\nprevious methods only utilize general assessments as reward signals, which\nlimits accuracy. To bridge this gap, we introduce ProxyReward, an innovative\nreinforcement learning (RL) based framework, which includes a dataset and a\nreward signal computation method. Firstly, ProxyReward Dataset generation is\naccomplished through simple prompts that enables the model to create\nautomatically, obviating extensive labeled data or significant manual effort.\nSecondly, ProxyReward Signal offers a targeted evaluation of information\ncomprehensiveness and accuracy for specific questions. The experimental results\nindicate that our method ProxyReward surpasses even GPT-4-Turbo. It can\nsignificantly enhance performance by 20% on the Open-LTG task when training\nwidely used open-source models, while also surpassing the LLM-as-a-Judge\napproach. Our work presents effective methods to enhance the ability of LLMs to\naddress complex open-ended questions posed by human."}
{"id": "2506.16168", "pdf": "https://arxiv.org/pdf/2506.16168.pdf", "abs": "https://arxiv.org/abs/2506.16168", "title": "On using AI for EEG-based BCI applications: problems, current challenges and future trends", "authors": ["Thomas Barbera", "Jacopo Burger", "Alessandro D'Amelio", "Simone Zini", "Simone Bianco", "Raffaella Lanzarotti", "Paolo Napoletano", "Giuseppe Boccignone", "Jose Luis Contreras-Vidal"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Imagine unlocking the power of the mind to communicate, create, and even\ninteract with the world around us. Recent breakthroughs in Artificial\nIntelligence (AI), especially in how machines \"see\" and \"understand\" language,\nare now fueling exciting progress in decoding brain signals from scalp\nelectroencephalography (EEG). Prima facie, this opens the door to revolutionary\nbrain-computer interfaces (BCIs) designed for real life, moving beyond\ntraditional uses to envision Brain-to-Speech, Brain-to-Image, and even a\nBrain-to-Internet of Things (BCIoT).\n  However, the journey is not as straightforward as it was for Computer Vision\n(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based\nBCIs, particularly in building powerful foundational models, presents unique\nand intricate hurdles that could affect their reliability.\n  Here, we unfold a guided exploration of this dynamic and rapidly evolving\nresearch area. Rather than barely outlining a map of current endeavors and\nresults, the goal is to provide a principled navigation of this hot and\ncutting-edge research landscape. We consider the basic paradigms that emerge\nfrom a causal perspective and the attendant challenges presented to AI-based\nmodels. Looking ahead, we then discuss promising research avenues that could\novercome today's technological, methodological, and ethical limitations. Our\naim is to lay out a clear roadmap for creating truly practical and effective\nEEG-based BCI solutions that can thrive in everyday environments."}
{"id": "2506.16029", "pdf": "https://arxiv.org/pdf/2506.16029.pdf", "abs": "https://arxiv.org/abs/2506.16029", "title": "EvoLM: In Search of Lost Language Model Training Dynamics", "authors": ["Zhenting Qi", "Fan Nie", "Alexandre Alahi", "James Zou", "Himabindu Lakkaraju", "Yilun Du", "Eric Xing", "Sham Kakade", "Hanlin Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern language model (LM) training has been divided into multiple stages,\nmaking it difficult for downstream developers to evaluate the impact of design\nchoices made at each stage. We present EvoLM, a model suite that enables\nsystematic and transparent analysis of LMs' training dynamics across\npre-training, continued pre-training, supervised fine-tuning, and reinforcement\nlearning. By training over 100 LMs with 1B and 4B parameters from scratch, we\nrigorously evaluate both upstream (language modeling) and downstream\n(problem-solving) reasoning capabilities, including considerations of both\nin-domain and out-of-domain generalization. Key insights highlight the\ndiminishing returns from excessive pre-training and post-training, the\nimportance and practices of mitigating forgetting during domain-specific\ncontinued pre-training, the crucial role of continued pre-training in bridging\npre-training and post-training phases, and various intricate trade-offs when\nconfiguring supervised fine-tuning and reinforcement learning. To facilitate\nopen research and reproducibility, we release all pre-trained and post-trained\nmodels, training datasets for all stages, and our entire training and\nevaluation pipeline."}
{"id": "2506.16199", "pdf": "https://arxiv.org/pdf/2506.16199.pdf", "abs": "https://arxiv.org/abs/2506.16199", "title": "Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)", "authors": ["Mohammad Naiseh", "Huseyin Dogan", "Stephen Giff", "Nan Jiang"], "categories": ["cs.HC"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) plays a critical role in fostering\nuser trust and understanding in AI-driven systems. However, the design of\neffective XAI interfaces presents significant challenges, particularly for UX\nprofessionals who may lack technical expertise in AI or machine learning.\nExisting explanation methods, such as SHAP, LIME, and counterfactual\nexplanations, often rely on complex technical language and assumptions that are\ndifficult for non-expert users to interpret. To address these gaps, we propose\na UX Research (UXR) Playbook for XAI - a practical framework aimed at\nsupporting UX professionals in designing accessible, transparent, and\ntrustworthy AI experiences. Our playbook offers actionable guidance to help\nbridge the gap between technical explainability methods and user centred\ndesign, empowering designers to create AI interactions that foster better\nunderstanding, trust, and responsible AI adoption."}
{"id": "2506.16037", "pdf": "https://arxiv.org/pdf/2506.16037.pdf", "abs": "https://arxiv.org/abs/2506.16037", "title": "Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3", "authors": ["Xinyue Huang", "Ziqi Lin", "Fang Sun", "Wenchao Zhang", "Kejian Tong", "Yunbo Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents a novel Retrieval-Augmented Generation (RAG) framework\ntailored for complex question answering tasks, addressing challenges in\nmulti-hop reasoning and contextual understanding across lengthy documents.\nBuilt upon LLaMA 3, the framework integrates a dense retrieval module with\nadvanced context fusion and multi-hop reasoning mechanisms, enabling more\naccurate and coherent response generation. A joint optimization strategy\ncombining retrieval likelihood and generation cross-entropy improves the\nmodel's robustness and adaptability. Experimental results show that the\nproposed system outperforms existing retrieval-augmented and generative\nbaselines, confirming its effectiveness in delivering precise, contextually\ngrounded answers."}
{"id": "2506.16312", "pdf": "https://arxiv.org/pdf/2506.16312.pdf", "abs": "https://arxiv.org/abs/2506.16312", "title": "When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard", "authors": ["Angxuan Chen"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigated the impact of a theory-driven, explainable Learning\nAnalytics Dashboard (LAD) on university students' human-AI collaborative\nacademic abstract writing task. Grounded in Self-Regulated Learning (SRL)\ntheory and incorporating Explainable AI (XAI) principles, our LAD featured a\nthree-layered design (Visual, Explainable, Interactive). In an experimental\nstudy, participants were randomly assigned to either an experimental group\n(using the full explainable LAD) or a control group (using a visual-only LAD)\nto collaboratively write an academic abstract with a Generative AI. While\nquantitative analysis revealed no significant difference in the quality of\nco-authored abstracts between the two groups, a significant and noteworthy\ndifference emerged in conceptual understanding: students in the explainable LAD\ngroup demonstrated a superior grasp of abstract writing principles, as\nevidenced by their higher scores on a knowledge test (p= .026). These findings\nhighlight that while basic AI-generated feedback may suffice for immediate task\ncompletion, the provision of explainable feedback is crucial for fostering\ndeeper learning, enhancing conceptual understanding, and developing\ntransferable skills fundamental to self-regulated learning in academic writing\ncontexts."}
{"id": "2506.16043", "pdf": "https://arxiv.org/pdf/2506.16043.pdf", "abs": "https://arxiv.org/abs/2506.16043", "title": "DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling", "authors": ["Fei Wang", "Xingchen Wan", "Ruoxi Sun", "Jiefeng Chen", "Sercan √ñ. Arƒ±k"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Inference-time scaling has proven effective in boosting large language model\n(LLM) performance through increased test-time computation. Yet, its practical\napplication is often hindered by reliance on external verifiers or a lack of\noptimization for realistic computational constraints. We propose DynScaling,\nwhich addresses these limitations through two primary innovations: an\nintegrated parallel-sequential sampling strategy and a bandit-based dynamic\nbudget allocation framework. The integrated sampling strategy unifies parallel\nand sequential sampling by constructing synthetic sequential reasoning chains\nfrom initially independent parallel responses, promoting diverse and coherent\nreasoning trajectories. The dynamic budget allocation framework formulates the\nallocation of computational resources as a multi-armed bandit problem,\nadaptively distributing the inference budget across queries based on the\nuncertainty of previously sampled responses, thereby maximizing computational\nefficiency. By combining these components, DynScaling effectively improves LLM\nperformance under practical resource constraints without the need for external\nverifiers. Experimental results demonstrate that DynScaling consistently\nsurpasses existing verifier-free inference scaling baselines in both task\nperformance and computational cost."}
{"id": "2506.16345", "pdf": "https://arxiv.org/pdf/2506.16345.pdf", "abs": "https://arxiv.org/abs/2506.16345", "title": "Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Bruna Capeleti", "Rafael Ferreira Mello", "Andr√© Freire", "Luciana Zaina"], "categories": ["cs.HC", "H.5.2"], "comment": "Paper accepted at the 20th IFIP TC13 International Conference on\n  Human-Computer Interaction (INTERACT) 2025", "summary": "Heuristic evaluation is a widely used method in Human-Computer Interaction\n(HCI) to inspect interfaces and identify issues based on heuristics. Recently,\nLarge Language Models (LLMs), such as GPT-4o, have been applied in HCI to\nassist in persona creation, the ideation process, and the analysis of\nsemi-structured interviews. However, considering the need to understand\nheuristics and the high degree of abstraction required to evaluate them, LLMs\nmay have difficulty conducting heuristic evaluation. However, prior research\nhas not investigated GPT-4o's performance in heuristic evaluation compared to\nHCI experts in web-based systems. In this context, this study aims to compare\nthe results of a heuristic evaluation performed by GPT-4o and human experts. To\nthis end, we selected a set of screenshots from a web system and asked GPT-4o\nto perform a heuristic evaluation based on Nielsen's Heuristics from a\nliterature-grounded prompt. Our results indicate that only 21.2% of the issues\nidentified by human experts were also identified by GPT-4o, despite it found 27\nnew issues. We also found that GPT-4o performed better for heuristics related\nto aesthetic and minimalist design and match between system and real world,\nwhereas it has difficulty identifying issues in heuristics related to\nflexibility, control, and user efficiency. Additionally, we noticed that GPT-4o\ngenerated several false positives due to hallucinations and attempts to predict\nissues. Finally, we highlight five takeaways for the conscious use of GPT-4o in\nheuristic evaluations."}
{"id": "2506.16052", "pdf": "https://arxiv.org/pdf/2506.16052.pdf", "abs": "https://arxiv.org/abs/2506.16052", "title": "A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text", "authors": ["Devesh Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of online communication platforms has created unprecedented\nopportunities for global connectivity while simultaneously enabling harmful\nbehaviors such as cyberbullying, which affects approximately 54.4\\% of\nteenagers according to recent research. This paper presents a hybrid\narchitecture that combines the contextual understanding capabilities of\ntransformer-based models with the pattern recognition strengths of broad\nlearning systems for effective cyberbullying detection. This approach\nintegrates a modified DeBERTa model augmented with Squeeze-and-Excitation\nblocks and sentiment analysis capabilities with a Gated Broad Learning System\n(GBLS) classifier, creating a synergistic framework that outperforms existing\napproaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +\nGBLS model achieved good performance on four English datasets: 79.3\\% accuracy\non HateXplain, 95.41\\% accuracy on SOSNet, 91.37\\% accuracy on Mendeley-I, and\n94.67\\% accuracy on Mendeley-II. Beyond performance gains, the framework\nincorporates comprehensive explainability mechanisms including token-level\nattribution analysis, LIME-based local interpretations, and confidence\ncalibration, addressing critical transparency requirements in automated content\nmoderation. Ablation studies confirm the meaningful contribution of each\narchitectural component, while failure case analysis reveals specific\nchallenges in detecting implicit bias and sarcastic content, providing valuable\ninsights for future improvements in cyberbullying detection systems."}
{"id": "2506.16468", "pdf": "https://arxiv.org/pdf/2506.16468.pdf", "abs": "https://arxiv.org/abs/2506.16468", "title": "Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury", "authors": ["Vlad Cnejevici", "Matthias Ponfick", "Raul C. S√Æmpetru", "Alessandro Del Vecchio"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "26 pages, 7 figures", "summary": "Restoring movement of a paralyzed foot is a key challenge in helping\nindividuals with neurological conditions such as spinal cord injury (SCI) to\nimprove their quality of life. Neuroprostheses based on functional electrical\nstimulation (FES) can restore the physiological range of motion by stimulating\nthe affected muscles using surface electrodes. We have previously shown that,\ndespite chronic motor-complete SCI, it is possible to capture paralyzed hand\nmovements in individuals with tetraplegia using spared and modulated motor unit\n(MU) activity decoded with non-invasive electromyography (EMG) sensors. This\nstudy investigated whether a wearable high-density surface EMG system could\ncapture and control paralyzed foot kinematics in closed-loop control with an\nFES system. We found that all our participants with SCI (2 with chronic SCI and\n3 with acute SCI) retained distinct spared EMG activity for at least three\nankle movements, which allowed them to reliably control a digital cursor using\ntheir spared tibialis anterior and triceps surae MU activity. Movement\nseparability was further reconfirmed by extracting task-modulated MU activity\nduring foot flexion/extension (3-7 modulated MUs/participant). Three\nparticipants were further able to modulate and maintain their foot\nflexion/extension EMG levels with an accuracy of >70%. Lastly, we show that\nreal-time control of a FES system using EMG from the affected limb can restore\nfoot movements in a highly intuitive way, significantly improving the lost or\npathological foot range of motion. Our system provides an intuitive approach\nfor closed-loop control of FES that has the potential to assist individuals\nwith SCI in regaining lost motor functions."}
{"id": "2506.16055", "pdf": "https://arxiv.org/pdf/2506.16055.pdf", "abs": "https://arxiv.org/abs/2506.16055", "title": "Knee-Deep in C-RASP: A Transformer Depth Hierarchy", "authors": ["Andy Yang", "Micha√´l Cadilhac", "David Chiang"], "categories": ["cs.CL", "cs.FL"], "comment": "27 pages, 4 figures", "summary": "It has been observed that transformers with greater depth (that is, more\nlayers) have more capabilities, but can we establish formally which\ncapabilities are gained with greater depth? We answer this question with a\ntheoretical proof followed by an empirical study. First, we consider\ntransformers that round to fixed precision except inside attention. We show\nthat this subclass of transformers is expressively equivalent to the\nprogramming language C-RASP and this equivalence preserves depth. Second, we\nprove that deeper C-RASP programs are more expressive than shallower C-RASP\nprograms, implying that deeper transformers are more expressive than shallower\ntransformers (within the subclass mentioned above). These results are\nestablished by studying a form of temporal logic with counting operators, which\nwas shown equivalent to C-RASP in previous work. Finally, we provide empirical\nevidence that our theory predicts the depth required for transformers without\npositional encodings to length-generalize on a family of sequential dependency\ntasks."}
{"id": "2506.16473", "pdf": "https://arxiv.org/pdf/2506.16473.pdf", "abs": "https://arxiv.org/abs/2506.16473", "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions."}
{"id": "2506.16064", "pdf": "https://arxiv.org/pdf/2506.16064.pdf", "abs": "https://arxiv.org/abs/2506.16064", "title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning", "authors": ["Duc Hieu Ho", "Chenglin Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated robust capabilities across\nvarious natural language tasks. However, producing outputs that are\nconsistently honest and helpful remains an open challenge. To overcome this\nchallenge, this paper tackles the problem through two complementary directions.\nIt conducts a comprehensive benchmark evaluation of ten widely used large\nlanguage models, including both proprietary and open-weight models from OpenAI,\nMeta, and Google. In parallel, it proposes a novel prompting strategy,\nself-critique-guided curiosity refinement prompting. The key idea behind this\nstrategy is enabling models to self-critique and refine their responses without\nadditional training. The proposed method extends the curiosity-driven prompting\nstrategy by incorporating two lightweight in-context steps including\nself-critique step and refinement step.\n  The experiment results on the HONESET dataset evaluated using the framework\n$\\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a\njudge of honesty and helpfulness, show consistent improvements across all\nmodels. The approach reduces the number of poor-quality responses, increases\nhigh-quality responses, and achieves relative gains in $\\mathrm{H}^2$ scores\nranging from 1.4% to 4.3% compared to curiosity-driven prompting across\nevaluated models. These results highlight the effectiveness of structured\nself-refinement as a scalable and training-free strategy to improve the\ntrustworthiness of LLMs outputs."}
{"id": "2506.16542", "pdf": "https://arxiv.org/pdf/2506.16542.pdf", "abs": "https://arxiv.org/abs/2506.16542", "title": "Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence", "authors": ["Nathalia Gomez", "S. Sue Batham", "Mathias Volonte", "Tiffany D. Do"], "categories": ["cs.HC"], "comment": "6 pages, To Appear in Companion Publication of the 2025 Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW Companion '25)", "summary": "Technical interviews are a critical yet stressful step in the hiring process\nfor computer science graduates, often hindered by limited access to practice\nopportunities. This formative qualitative study (n=20) explores whether a\nmultimodal AI system can realistically simulate technical interviews and\nsupport confidence-building among candidates. Participants engaged with an\nAI-driven mock interview tool featuring whiteboarding tasks and real-time\nfeedback. Many described the experience as realistic and helpful, noting\nincreased confidence and improved articulation of problem-solving decisions.\nHowever, challenges with conversational flow and timing were noted. These\nfindings demonstrate the potential of AI-driven technical interviews as\nscalable and realistic preparation tools, suggesting that future research could\nexplore variations in interviewer behavior and their potential effects on\ncandidate preparation."}
{"id": "2506.16066", "pdf": "https://arxiv.org/pdf/2506.16066.pdf", "abs": "https://arxiv.org/abs/2506.16066", "title": "Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI", "authors": ["Devesh Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "The growth of digital communication platforms has led to increased\ncyberbullying incidents worldwide, creating a need for automated detection\nsystems to protect users. The rise of code-mixed Hindi-English (Hinglish)\ncommunication on digital platforms poses challenges for existing cyberbullying\ndetection systems, which were designed primarily for monolingual text. This\npaper presents a framework for cyberbullying detection in Hinglish text using\nthe Multilingual Representations for Indian Languages (MURIL) architecture to\naddress limitations in current approaches. Evaluation across six benchmark\ndatasets -- Bohra \\textit{et al.}, BullyExplain, BullySentemo, Kumar \\textit{et\nal.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based\napproach outperforms existing multilingual models including RoBERTa and\nIndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies\nof 86.97\\% on Bohra, 84.62\\% on BullyExplain, 86.03\\% on BullySentemo, 75.41\\%\non Kumar datasets, 83.92\\% on HASOC 2021, and 94.63\\% on Mendeley dataset. The\nframework includes explainability features through attribution analysis and\ncross-linguistic pattern recognition. Ablation studies show that selective\nlayer freezing, appropriate classification head design, and specialized\npreprocessing for code-mixed content improve detection performance, while\nfailure analysis identifies challenges including context-dependent\ninterpretation, cultural understanding, and cross-linguistic sarcasm detection,\nproviding directions for future research in multilingual cyberbullying\ndetection."}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571.pdf", "abs": "https://arxiv.org/abs/2506.16571", "title": "Capturing Visualization Design Rationale", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.HC"], "comment": null, "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students."}
{"id": "2506.16123", "pdf": "https://arxiv.org/pdf/2506.16123.pdf", "abs": "https://arxiv.org/abs/2506.16123", "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning", "authors": ["Natapong Nitarach", "Warit Sirichotedumrong", "Panop Pitchayarthorn", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kunat Pipatanakul"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\napproach that incorporates insights from domain-specific expert financial\nreasoning to guide the reasoning traces of large language models. We\ninvestigate that there are three main prompting styles in FinNLP: (1) standard\nprompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an\nexplicit reasoning structure, such as the use of tags; and (3) structured CoT\nprompting--CoT prompting with explicit instructions or examples that define\nstructured reasoning steps. Previously, FinNLP has primarily focused on prompt\nengineering with either standard or unstructured CoT prompting. However,\nstructured CoT prompting has received limited attention in prior work.\nFurthermore, the design of reasoning structures in structured CoT prompting is\noften based on heuristics from non-domain experts. In this study, we\ninvestigate each prompting approach in FinNLP. We evaluate the three main\nprompting styles and FinCoT on CFA-style questions spanning ten financial\ndomains. We observe that FinCoT improves performance from 63.2% to 80.5% and\nQwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens\neight-fold compared to structured CoT prompting. Our findings show that\ndomain-aligned structured prompts not only improve performance and reduce\ninference costs but also yield more interpretable and expert-aligned reasoning\ntraces."}
{"id": "2506.16677", "pdf": "https://arxiv.org/pdf/2506.16677.pdf", "abs": "https://arxiv.org/abs/2506.16677", "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration", "authors": ["Hao Guo", "Wei Fan", "Shaohui Liu", "Feng Jiang", "Chunzhi Yi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Trust prediction is a key issue in human-robot collaboration, especially in\nconstruction scenarios where maintaining appropriate trust calibration is\ncritical for safety and efficiency. This paper introduces the\nPerformance-guided Physiological signal-based Trust Prediction (PPTP), a novel\nframework designed to improve trust assessment. We designed a human-robot\nconstruction scenario with three difficulty levels to induce different trust\nstates. Our approach integrates synchronized multimodal physiological signals\n(ECG, GSR, and EMG) with collaboration performance evaluation to predict human\ntrust levels. Individual physiological signals are processed using\ncollaboration performance information as guiding cues, leveraging the\nstandardized nature of collaboration performance to compensate for individual\nvariations in physiological responses. Extensive experiments demonstrate the\nefficacy of our cross-modality fusion method in significantly improving trust\nclassification performance. Our model achieves over 81% accuracy in three-level\ntrust classification, outperforming the best baseline method by 6.7%, and\nnotably reaches 74.3% accuracy in high-resolution seven-level classification,\nwhich is a first in trust prediction research. Ablation experiments further\nvalidate the superiority of physiological signal processing guided by\ncollaboration performance assessment."}
{"id": "2506.16151", "pdf": "https://arxiv.org/pdf/2506.16151.pdf", "abs": "https://arxiv.org/abs/2506.16151", "title": "Under the Shadow of Babel: How Language Shapes Reasoning in LLMs", "authors": ["Chenxi Wang", "Yixuan Zhang", "Lang Gao", "Zixiang Xu", "Zirui Song", "Yanbo Wang", "Xiuying Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Language is not only a tool for communication but also a medium for human\ncognition and reasoning. If, as linguistic relativity suggests, the structure\nof language shapes cognitive patterns, then large language models (LLMs)\ntrained on human language may also internalize the habitual logical structures\nembedded in different languages. To examine this hypothesis, we introduce\nBICAUSE, a structured bilingual dataset for causal reasoning, which includes\nsemantically aligned Chinese and English samples in both forward and reversed\ncausal forms. Our study reveals three key findings: (1) LLMs exhibit\ntypologically aligned attention patterns, focusing more on causes and\nsentence-initial connectives in Chinese, while showing a more balanced\ndistribution in English. (2) Models internalize language-specific preferences\nfor causal word order and often rigidly apply them to atypical inputs, leading\nto degraded performance, especially in Chinese. (3) When causal reasoning\nsucceeds, model representations converge toward semantically aligned\nabstractions across languages, indicating a shared understanding beyond surface\nform. Overall, these results suggest that LLMs not only mimic surface\nlinguistic forms but also internalize the reasoning biases shaped by language.\nRooted in cognitive linguistic theory, this phenomenon is for the first time\nempirically verified through structural analysis of model internals."}
{"id": "2506.16716", "pdf": "https://arxiv.org/pdf/2506.16716.pdf", "abs": "https://arxiv.org/abs/2506.16716", "title": "V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos", "authors": ["Qixin Wang", "Songtao Zhou", "Zeyu Jin", "Chenglin Guo", "Shikun Sun", "Xiaoyu Qin"], "categories": ["cs.HC"], "comment": "Accepted by IJCNN 2025", "summary": "Automatic video commentary systems are widely used on multimedia social media\nplatforms to extract factual information about video content. However, current\nsystems may overlook essential para-linguistic cues, including emotion and\nattitude, which are critical for fully conveying the meaning of visual content.\nThe absence of these cues can limit user understanding or, in some cases,\ndistort the video's original intent. Expressive speech effectively conveys\nthese cues and enhances the user's comprehension of videos. Building on these\ninsights, this paper explores the usage of vision-context-aware expressive\nspeech in enhancing users' understanding of videos in video commentary systems.\nFirstly, our formatting study indicates that semantic-only speech can lead to\nambiguity, and misaligned emotions between speech and visuals may distort\ncontent interpretation. To address this, we propose a method called\nvision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic\ncues from visuals using a vision-language model and leverages a\nknowledge-infused language model to guide the expressive speech model in\ngenerating context-aligned speech. User studies show that V-CASS enhances\nemotional and attitudinal resonance, as well as user audio-visual understanding\nand engagement, with 74.68% of participants preferring the system. Finally, we\nexplore the potential of our method in helping blind and low-vision users\nnavigate web videos, improving universal accessibility."}
{"id": "2506.16172", "pdf": "https://arxiv.org/pdf/2506.16172.pdf", "abs": "https://arxiv.org/abs/2506.16172", "title": "SGIC: A Self-Guided Iterative Calibration Framework for RAG", "authors": ["Guanhua Chen", "Yutong Yao", "Lidia S. Chao", "Xuebo Liu", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research in retrieval-augmented generation (RAG) has concentrated on\nretrieving useful information from candidate documents. However, numerous\nmethodologies frequently neglect the calibration capabilities of large language\nmodels (LLMs), which capitalize on their robust in-context reasoning prowess.\nThis work illustrates that providing LLMs with specific cues substantially\nimproves their calibration efficacy, especially in multi-round calibrations. We\npresent a new SGIC: Self-Guided Iterative Calibration Framework that employs\nuncertainty scores as a tool. Initially, this framework calculates uncertainty\nscores to determine both the relevance of each document to the query and the\nconfidence level in the responses produced by the LLMs. Subsequently, it\nreevaluates these scores iteratively, amalgamating them with prior responses to\nrefine calibration. Furthermore, we introduce an innovative approach for\nconstructing an iterative self-calibration training set, which optimizes LLMs\nto efficiently harness uncertainty scores for capturing critical information\nand enhancing response accuracy. Our proposed framework significantly improves\nperformance on both closed-source and open-weight LLMs."}
{"id": "2506.16851", "pdf": "https://arxiv.org/pdf/2506.16851.pdf", "abs": "https://arxiv.org/abs/2506.16851", "title": "\"Whoever needs to see it, will see it\": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok", "authors": ["Ankolika De", "Kelley Cotter", "Shaheen Kanthawala", "Haley McAtee", "Amy Ritchart", "Gahana Kadur"], "categories": ["cs.HC", "H.5.0"], "comment": "27 pages, Proc. ACM Hum.-Comput. Interact. 8", "summary": "Recent studies show that users often interpret social media algorithms as\nmystical or spiritual because of their unpredictability. This invites new\nquestions about how such perceptions affect the content that creators create\nand the communities they form online. In this study, 14 creators of algorithmic\nconspirituality content on TikTok were interviewed to explore their\ninterpretations and creation processes influenced by the platform's For You\nPage algorithm. We illustrate how creators' beliefs interact with TikTok's\nalgorithmic mediation to reinforce and shape their spiritual or relational\nthemes. Furthermore, we show how algorithmic conspirituality content impacts\nviewers, highlighting its role in generating significant emotional and\naffective labor for creators, stemming from complex relational dynamics\ninherent in this content creation. We discuss implications for design to\nsupport creators aimed at recognizing the unexpected spiritual and religious\nexperiences algorithms prompt, as well as supporting creators in effectively\nmanaging these challenges."}
{"id": "2506.16187", "pdf": "https://arxiv.org/pdf/2506.16187.pdf", "abs": "https://arxiv.org/abs/2506.16187", "title": "JETHICS: Japanese Ethics Understanding Evaluation Dataset", "authors": ["Masashi Takeshita", "Rafal Rzepka"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we propose JETHICS, a Japanese dataset for evaluating ethics\nunderstanding of AI models. JETHICS contains 78K examples and is built by\nfollowing the construction methods of the existing English ETHICS dataset. It\nincludes four categories based normative theories and concepts from ethics and\npolitical philosophy; and one representing commonsense morality. Our evaluation\nexperiments on non-proprietary large language models (LLMs) and on GPT-4o\nreveal that even GPT-4o achieves only an average score of about 0.7, while the\nbest-performing Japanese LLM attains around 0.5, indicating a relatively large\nroom for improvement in current LLMs."}
{"id": "2506.16874", "pdf": "https://arxiv.org/pdf/2506.16874.pdf", "abs": "https://arxiv.org/abs/2506.16874", "title": "Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools", "authors": ["Zhiqing Wang", "Haoxiang Fan", "Shiwei Wu", "Qiaoyi Chen", "Yongqi Liang", "Zhenhui Peng"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative Artificial Intelligence (GenAI) in K-6\nproject-based art courses presents both opportunities and challenges for\nenhancing creativity, engagement, and group collaboration. This study\nintroduces a four-phase field study, involving in total two experienced K-6 art\nteachers and 132 students in eight offline course sessions, to investigate the\nusage and impact of GenAI. Specifically, based on findings in Phases 1 and 2,\nwe developed AskArt, an interactive interface that combines DALL-E and GPT and\nis tailored to support elementary school students in their art projects, and\ndeployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in\nproviding background information, inspirations, and personalized guidance.\nHowever, challenges in query formulation for generating expected content were\nalso observed. Moreover, students employed varied collaboration strategies, and\nteachers noted increased engagement alongside concerns regarding misuse and\ninterface suitability. This study offers insights into the effective\nintegration of GenAI in elementary education, presents AskArt as a practical\ntool, and provides recommendations for educators and researchers to enhance\nproject-based learning with GenAI technologies."}
{"id": "2506.16190", "pdf": "https://arxiv.org/pdf/2506.16190.pdf", "abs": "https://arxiv.org/abs/2506.16190", "title": "Web(er) of Hate: A Survey on How Hate Speech Is Typed", "authors": ["Luna Wang", "Andrew Caines", "Alice Hutchings"], "categories": ["cs.CL"], "comment": null, "summary": "The curation of hate speech datasets involves complex design decisions that\nbalance competing priorities. This paper critically examines these\nmethodological choices in a diverse range of datasets, highlighting common\nthemes and practices, and their implications for dataset reliability. Drawing\non Max Weber's notion of ideal types, we argue for a reflexive approach in\ndataset creation, urging researchers to acknowledge their own value judgments\nduring dataset construction, fostering transparency and methodological rigour."}
{"id": "2506.17011", "pdf": "https://arxiv.org/pdf/2506.17011.pdf", "abs": "https://arxiv.org/abs/2506.17011", "title": "Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics", "authors": ["Bruno Campos"], "categories": ["cs.HC"], "comment": null, "summary": "This study compares the impact of \"juiciness\" on user engagement and\nshort-term information retention in interactive infographics. Juicy designs\ngenerally showed a slight advantage in overall user engagement scores compared\nto dry designs. Specifically, the juicy version of the Burcalories infographic\nhad the highest engagement score. However, the differences in engagement were\noften small. Regarding information retention, the results were mixed. The juicy\nversions of The Daily Routines of Famous Creative People and The Main Chakras\ninfographics showed marginally better average recall and more participants with\nhigher recall. Conversely, the dry version of Burcalories led to more correct\nanswers in multiple-choice questions. The study suggests that while juicy\ndesign elements can enhance user engagement and, in some cases, short-term\ninformation retention, their effectiveness depends on careful implementation.\nExcessive juiciness could be overwhelming or distracting, while\nwell-implemented juicy elements contributed to a more entertaining experience.\nThe findings emphasize the importance of balancing engaging feedback with\nclarity and usability."}
{"id": "2506.16247", "pdf": "https://arxiv.org/pdf/2506.16247.pdf", "abs": "https://arxiv.org/abs/2506.16247", "title": "Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports", "authors": ["Anindita Bhattacharya", "Tohida Rehman", "Debarshi Kumar Sanyal", "Samiran Chattopadhyay"], "categories": ["cs.CL"], "comment": "14 pages, 2 figures, 6 tables", "summary": "The findings section of a radiology report is often detailed and lengthy,\nwhereas the impression section is comparatively more compact and captures key\ndiagnostic conclusions. This research explores the use of advanced abstractive\nsummarization models to generate the concise impression from the findings\nsection of a radiology report. We have used the publicly available MIMIC-CXR\ndataset. A comparative analysis is conducted on leading pre-trained and\nopen-source large language models, including T5-base, BART-base,\nPEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network\nwith a coverage mechanism. To ensure a thorough assessment, multiple evaluation\nmetrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and\nBERTScore. By analyzing the performance of these models, this study identifies\ntheir respective strengths and limitations in the summarization of medical\ntext. The findings of this paper provide helpful information for medical\nprofessionals who need automated summarization solutions in the healthcare\nsector."}
{"id": "2506.17032", "pdf": "https://arxiv.org/pdf/2506.17032.pdf", "abs": "https://arxiv.org/abs/2506.17032", "title": "Toward Understanding Similarity of Visualization Techniques", "authors": ["Abdulhaq Adetunji Salako", "Christian Tominski"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "The literature describes many visualization techniques for different types of\ndata, tasks, and application contexts, and new techniques are proposed on a\nregular basis. Visualization surveys try to capture the immense space of\ntechniques and structure it with meaningful categorizations. Yet, it remains\ndifficult to understand the similarity of visualization techniques in general.\nWe approach this open research question from two angles. First, we follow a\nmodel-driven approach that is based on defining the signature of visualization\ntechniques and interpreting the similarity of signatures as the similarity of\ntheir associated techniques. Second, following an expert-driven approach, we\nasked visualization experts in a small online study for their ad-hoc intuitive\nassessment of the similarity of pairs visualization techniques. From both\napproaches, we gain insight into the similarity of a set of 13 basic and\nadvanced visualizations for different types of data. While our results are so\nfar preliminary and academic, they are first steps toward better understanding\nthe similarity of visualization techniques."}
{"id": "2506.16251", "pdf": "https://arxiv.org/pdf/2506.16251.pdf", "abs": "https://arxiv.org/abs/2506.16251", "title": "End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data", "authors": ["Aishwarya Pothula", "Bhavana Akkiraju", "Srihari Bandarupalli", "Charan D", "Santosh Kesiraju", "Anil Kumar Vuppala"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of high-quality annotated data presents a significant challenge\nin developing effective end-to-end speech-to-text translation (ST) systems,\nparticularly for low-resource languages. This paper explores the hypothesis\nthat weakly labeled data can be used to build ST models for low-resource\nlanguage pairs. We constructed speech-to-text translation datasets with the\nhelp of bitext mining using state-of-the-art sentence encoders. We mined the\nmultilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset\ncomprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,\nOdia-Hindi, and Telugu-Hindi. We created multiple versions of training data\nwith varying degrees of quality and quantity to investigate the effect of\nquality versus quantity of weakly labeled data on ST model performance. Results\ndemonstrate that ST systems can be built using weakly labeled data, with\nperformance comparable to massive multi-modal multilingual baselines such as\nSONAR and SeamlessM4T."}
{"id": "2506.17116", "pdf": "https://arxiv.org/pdf/2506.17116.pdf", "abs": "https://arxiv.org/abs/2506.17116", "title": "Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools", "authors": ["Samuel Rhys Cox", "Helena B√∏jer Djern√¶s", "Niels van Berkel"], "categories": ["cs.HC"], "comment": "Workshop paper presented at XAIxArts'25 - the third international\n  workshop on eXplainable AI for the Arts, held in conjunction with the ACM\n  Creativity and Cognition conference 2025, June 23rd, 2025. 3 pages", "summary": "In this workshop paper, we discuss the potential for measures of user-centric\nbenefits (such as emotional well-being) that could be explored when evaluating\nexplainable AI (XAI) systems within the arts. As a background to this, we draw\nfrom our recent review of creativity support tool (CST) evaluations, that found\na paucity of studies evaluating CSTs for user-centric measures that benefit the\nuser themselves. Specifically, we discuss measures of: (1) developing intrinsic\nabilities, (2) emotional well-being, (3) self-reflection, and (4)\nself-perception. By discussing these user-centric measures within the context\nof XAI and the arts, we wish to provoke discussion regarding the potential of\nsuch measures."}
{"id": "2506.16285", "pdf": "https://arxiv.org/pdf/2506.16285.pdf", "abs": "https://arxiv.org/abs/2506.16285", "title": "Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information", "authors": ["Hao-Chien Lu", "Jhen-Ke Lin", "Hong-Yun Lin", "Chung-Chun Wang", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Current automated speaking assessment (ASA) systems for use in multi-aspect\nevaluations often fail to make full use of content relevance, overlooking image\nor exemplar cues, and employ superficial grammar analysis that lacks detailed\nerror types. This paper ameliorates these deficiencies by introducing two novel\nenhancements to construct a hybrid scoring model. First, a multifaceted\nrelevance module integrates question and the associated image content,\nexemplar, and spoken response of an L2 speaker for a comprehensive assessment\nof content relevance. Second, fine-grained grammar error features are derived\nusing advanced grammar error correction (GEC) and detailed annotation to\nidentify specific error categories. Experiments and ablation studies\ndemonstrate that these components significantly improve the evaluation of\ncontent relevance, language use, and overall ASA performance, highlighting the\nbenefits of using richer, more nuanced feature sets for holistic speaking\nassessment."}
{"id": "2506.17196", "pdf": "https://arxiv.org/pdf/2506.17196.pdf", "abs": "https://arxiv.org/abs/2506.17196", "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance", "authors": ["Shambhavi Bhushan", "Danielle R Thomas", "Conrad Borchers", "Isha Raghuvanshi", "Ralph Abboud", "Erin Gatz", "Shivang Gupta", "Kenneth Koedinger"], "categories": ["cs.HC"], "comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript", "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases."}
{"id": "2506.16322", "pdf": "https://arxiv.org/pdf/2506.16322.pdf", "abs": "https://arxiv.org/abs/2506.16322", "title": "PL-Guard: Benchmarking Language Model Safety for Polish", "authors": ["Aleksandra Krasnodƒôbska", "Karolina Seweryn", "Szymon ≈Åukasik", "Wojciech Kusa"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to the 10th Workshop on Slavic Natural Language Processing", "summary": "Despite increasing efforts to ensure the safety of large language models\n(LLMs), most existing safety assessments and moderation tools remain heavily\nbiased toward English and other high-resource languages, leaving majority of\nglobal languages underexamined. To address this gap, we introduce a manually\nannotated benchmark dataset for language model safety classification in Polish.\nWe also create adversarially perturbed variants of these samples designed to\nchallenge model robustness. We conduct a series of experiments to evaluate\nLLM-based and classifier-based models of varying sizes and architectures.\nSpecifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based\nclassifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B\nmodel. We train these models using different combinations of annotated data and\nevaluate their performance, comparing it against publicly available guard\nmodels. Results demonstrate that the HerBERT-based classifier achieves the\nhighest overall performance, particularly under adversarial conditions."}
{"id": "2506.15794", "pdf": "https://arxiv.org/pdf/2506.15794.pdf", "abs": "https://arxiv.org/abs/2506.15794", "title": "Veracity: An Open-Source AI Fact-Checking System", "authors": ["Taylor Lynn Curtis", "Maximilian Puelma Touzel", "William Garneau", "Manon Gruaz", "Mike Pinder", "Li Wei Wang", "Sukanya Krishna", "Luda Cohen", "Jean-Fran√ßois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of misinformation poses a significant threat to society,\nexacerbated by the capabilities of generative AI. This demo paper introduces\nVeracity, an open-source AI system designed to empower individuals to combat\nmisinformation through transparent and accessible fact-checking. Veracity\nleverages the synergy between Large Language Models (LLMs) and web retrieval\nagents to analyze user-submitted claims and provide grounded veracity\nassessments with intuitive explanations. Key features include multilingual\nsupport, numerical scoring of claim veracity, and an interactive interface\ninspired by familiar messaging applications. This paper will showcase\nVeracity's ability to not only detect misinformation but also explain its\nreasoning, fostering media literacy and promoting a more informed society."}
{"id": "2506.16337", "pdf": "https://arxiv.org/pdf/2506.16337.pdf", "abs": "https://arxiv.org/abs/2506.16337", "title": "Generalizability of Media Frames: Corpus creation and analysis across countries", "authors": ["Agnese Daffara", "Sourabh Dattawad", "Sebastian Pad√≥", "Tanise Ceron"], "categories": ["cs.CL"], "comment": "8 pages + References (3 pages) and Appendix (4 pages). This paper was\n  submitted to StarSem 2025 and is currently under review", "summary": "Frames capture aspects of an issue that are emphasized in a debate by\ninterlocutors and can help us understand how political language conveys\ndifferent perspectives and ultimately shapes people's opinions. The Media Frame\nCorpus (MFC) is the most commonly used framework with categories and detailed\nguidelines for operationalizing frames. It is, however, focused on a few\nsalient U.S. news issues, making it unclear how well these frames can capture\nnews issues in other cultural contexts. To explore this, we introduce\nFrameNews-PT, a dataset of Brazilian Portuguese news articles covering\npolitical and economic news and annotate it within the MFC framework. Through\nseveral annotation rounds, we evaluate the extent to which MFC frames\ngeneralize to the Brazilian debate issues. We further evaluate how fine-tuned\nand zero-shot models perform on out-of-domain data. Results show that the 15\nMFC frames remain broadly applicable with minor revisions of the guidelines.\nHowever, some MFC frames are rarely used, and novel news issues are analyzed\nusing general 'fall-back' frames. We conclude that cross-cultural frame use\nrequires careful consideration."}
{"id": "2506.15860", "pdf": "https://arxiv.org/pdf/2506.15860.pdf", "abs": "https://arxiv.org/abs/2506.15860", "title": "User-Guided Force-Directed Graph Layout", "authors": ["Hasan Balci", "Augustin Luna"], "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "Visual analysis of relational data is essential for many real-world analytics\ntasks, with layout quality being key to interpretability. However, existing\nlayout algorithms often require users to navigate complex parameters to express\ntheir intent. We present a user-guided force-directed layout approach that\nenables intuitive control through freehand sketching. Our method uses classical\nimage analysis techniques to extract structural information from sketches,\nwhich is then used to generate positional constraints that guide the layout\nprocess. We evaluate the approach on various real and synthetic graphs ranging\nfrom small to medium scale, demonstrating its ability to produce layouts\naligned with user expectations. An implementation of our method along with\ndocumentation and a demo page is freely available on GitHub at\nhttps://github.com/sciluna/uggly."}
{"id": "2506.16343", "pdf": "https://arxiv.org/pdf/2506.16343.pdf", "abs": "https://arxiv.org/abs/2506.16343", "title": "Analyzing the Influence of Knowledge Graph Information on Relation Extraction", "authors": ["Cedric M√∂ller", "Ricardo Usbeck"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "We examine the impact of incorporating knowledge graph information on the\nperformance of relation extraction models across a range of datasets. Our\nhypothesis is that the positions of entities within a knowledge graph provide\nimportant insights for relation extraction tasks. We conduct experiments on\nmultiple datasets, each varying in the number of relations, training examples,\nand underlying knowledge graphs. Our results demonstrate that integrating\nknowledge graph information significantly enhances performance, especially when\ndealing with an imbalance in the number of training examples for each relation.\nWe evaluate the contribution of knowledge graph-based features by combining\nestablished relation extraction methods with graph-aware Neural Bellman-Ford\nnetworks. These features are tested in both supervised and zero-shot settings,\ndemonstrating consistent performance improvements across various datasets."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Under review for KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2506.16348", "pdf": "https://arxiv.org/pdf/2506.16348.pdf", "abs": "https://arxiv.org/abs/2506.16348", "title": "DISCIE -- Discriminative Closed Information Extraction", "authors": ["Cedric M√∂ller", "Ricardo Usbeck"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a novel method for closed information extraction. The\nmethod employs a discriminative approach that incorporates type and\nentity-specific information to improve relation extraction accuracy,\nparticularly benefiting long-tail relations. Notably, this method demonstrates\nsuperior performance compared to state-of-the-art end-to-end generative models.\nThis is especially evident for the problem of large-scale closed information\nextraction where we are confronted with millions of entities and hundreds of\nrelations. Furthermore, we emphasize the efficiency aspect by leveraging\nsmaller models. In particular, the integration of type-information proves\ninstrumental in achieving performance levels on par with or surpassing those of\na larger generative model. This advancement holds promise for more accurate and\nefficient information extraction techniques."}
{"id": "2506.16051", "pdf": "https://arxiv.org/pdf/2506.16051.pdf", "abs": "https://arxiv.org/abs/2506.16051", "title": "From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience", "authors": ["Zhiwei Li", "Carl Kesselman", "Tran Huy Nguyen", "Benjamin Yixing Xu", "Kyle Bolo", "Kimberley Yu"], "categories": ["cs.LG", "cs.DB", "cs.DL", "cs.HC"], "comment": null, "summary": "Reproducibility remains a central challenge in machine learning (ML),\nespecially in collaborative eScience projects where teams iterate over data,\nfeatures, and models. Current ML workflows are often dynamic yet fragmented,\nrelying on informal data sharing, ad hoc scripts, and loosely connected tools.\nThis fragmentation impedes transparency, reproducibility, and the adaptability\nof experiments over time. This paper introduces a data-centric framework for\nlifecycle-aware reproducibility, centered around six structured artifacts:\nDataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These\nartifacts formalize the relationships between data, code, and decisions,\nenabling ML experiments to be versioned, interpretable, and traceable over\ntime. The approach is demonstrated through a clinical ML use case of glaucoma\ndetection, illustrating how the system supports iterative exploration, improves\nreproducibility, and preserves the provenance of collaborative decisions across\nthe ML lifecycle."}
{"id": "2506.16370", "pdf": "https://arxiv.org/pdf/2506.16370.pdf", "abs": "https://arxiv.org/abs/2506.16370", "title": "Can structural correspondences ground real world representational content in Large Language Models?", "authors": ["Iwan Williams"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 produce compelling responses to a\nwide range of prompts. But their representational capacities are uncertain.\nMany LLMs have no direct contact with extra-linguistic reality: their inputs,\noutputs and training data consist solely of text, raising the questions (1) can\nLLMs represent anything and (2) if so, what? In this paper, I explore what it\nwould take to answer these questions according to a structural-correspondence\nbased account of representation, and make an initial survey of this evidence. I\nargue that the mere existence of structural correspondences between LLMs and\nworldly entities is insufficient to ground representation of those entities.\nHowever, if these structural correspondences play an appropriate role - they\nare exploited in a way that explains successful task performance - then they\ncould ground real world contents. This requires overcoming a challenge: the\ntext-boundedness of LLMs appears, on the face of it, to prevent them engaging\nin the right sorts of tasks."}
{"id": "2506.16202", "pdf": "https://arxiv.org/pdf/2506.16202.pdf", "abs": "https://arxiv.org/abs/2506.16202", "title": "AI labeling reduces the perceived accuracy of online content but has limited broader effects", "authors": ["Chuyao Wang", "Patrick Sturgis", "Daniel de Kadt"], "categories": ["cs.CY", "cs.HC", "stat.AP", "62P25, 91C99", "J.4; H.1.2"], "comment": "30 pages, 5 figures, 10 tables", "summary": "Explicit labeling of online content produced by artificial intelligence (AI)\nis a widely mooted policy for ensuring transparency and promoting public\nconfidence. Yet little is known about the scope of AI labeling effects on\npublic assessments of labeled content. We contribute new evidence on this\nquestion from a survey experiment using a high-quality nationally\nrepresentative probability sample (n = 3,861). First, we demonstrate that\nexplicit AI labeling of a news article about a proposed public policy reduces\nits perceived accuracy. Second, we test whether there are spillover effects in\nterms of policy interest, policy support, and general concerns about online\nmisinformation. We find that AI labeling reduces interest in the policy, but\nneither influences support for the policy nor triggers general concerns about\nonline misinformation. We further find that increasing the salience of AI use\nreduces the negative impact of AI labeling on perceived accuracy, while\none-sided versus two-sided framing of the policy has no moderating effect.\nOverall, our findings suggest that the effects of algorithm aversion induced by\nAI labeling of online content are limited in scope."}
{"id": "2506.16381", "pdf": "https://arxiv.org/pdf/2506.16381.pdf", "abs": "https://arxiv.org/abs/2506.16381", "title": "InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems", "authors": ["Kexin Huang", "Qian Tu", "Liwei Fan", "Chenchen Yang", "Dong Zhang", "Shimin Li", "Zhaoye Fei", "Qinyuan Cheng", "Xipeng Qiu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "19 pages, 9 figures", "summary": "In modern speech synthesis, paralinguistic information--such as a speaker's\nvocal timbre, emotional state, and dynamic prosody--plays a critical role in\nconveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)\nsystems rely on fixed style labels or inserting a speech prompt to control\nthese cues, which severely limits flexibility. Recent attempts seek to employ\nnatural-language instructions to modulate paralinguistic features,\nsubstantially improving the generalization of instruction-driven TTS models.\nAlthough many TTS systems now support customized synthesis via textual\ndescription, their actual ability to interpret and execute complex instructions\nremains largely unexplored. In addition, there is still a shortage of\nhigh-quality benchmarks and automated evaluation metrics specifically designed\nfor instruction-based TTS, which hinders accurate assessment and iterative\noptimization of these models. To address these limitations, we introduce\nInstructTTSEval, a benchmark for measuring the capability of complex\nnatural-language style control. We introduce three tasks, namely\nAcoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,\nincluding English and Chinese subsets, each with 1k test cases (6k in total)\npaired with reference audio. We leverage Gemini as an automatic judge to assess\ntheir instruction-following abilities. Our evaluation of accessible\ninstruction-following TTS systems highlights substantial room for further\nimprovement. We anticipate that InstructTTSEval will drive progress toward more\npowerful, flexible, and accurate instruction-following TTS."}
{"id": "2506.16310", "pdf": "https://arxiv.org/pdf/2506.16310.pdf", "abs": "https://arxiv.org/abs/2506.16310", "title": "Optimizing Multilingual Text-To-Speech with Accents & Emotions", "authors": ["Pranav Pawar", "Akshansh Dwivedi", "Jenish Boricha", "Himanshu Gohil", "Aditya Dubey"], "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "comment": "12 pages, 8 figures", "summary": "State-of-the-art text-to-speech (TTS) systems realize high naturalness in\nmonolingual environments, synthesizing speech with correct multilingual accents\n(especially for Indic languages) and context-relevant emotions still poses\ndifficulty owing to cultural nuance discrepancies in current frameworks. This\npaper introduces a new TTS architecture integrating accent along with\npreserving transliteration with multi-scale emotion modelling, in particularly\ntuned for Hindi and Indian English accent. Our approach extends the Parler-TTS\nmodel by integrating A language-specific phoneme alignment hybrid\nencoder-decoder architecture, and culture-sensitive emotion embedding layers\ntrained on native speaker corpora, as well as incorporating a dynamic accent\ncode switching with residual vector quantization. Quantitative tests\ndemonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction\nfrom 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native\nlisteners, surpassing METTS and VECL-TTS baselines. The novelty of the system\nis that it can mix code in real time - generating statements such as \"Namaste,\nlet's talk about <Hindi phrase>\" with uninterrupted accent shifts while\npreserving emotional consistency. Subjective evaluation with 200 users reported\na mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than\nexisting multilingual systems (p<0.01). This research makes cross-lingual\nsynthesis more feasible by showcasing scalable accent-emotion disentanglement,\nwith direct application in South Asian EdTech and accessibility software."}
{"id": "2506.16383", "pdf": "https://arxiv.org/pdf/2506.16383.pdf", "abs": "https://arxiv.org/abs/2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Work draft", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."}
{"id": "2506.16617", "pdf": "https://arxiv.org/pdf/2506.16617.pdf", "abs": "https://arxiv.org/abs/2506.16617", "title": "The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring", "authors": ["Soobin Chae", "Suhwan Lee", "Hanna Hauptmann", "Hajo A. Reijers", "Xixi Lu"], "categories": ["cs.AI", "cs.HC"], "comment": "Accepted at CAiSE'25", "summary": "Predictive Process Monitoring (PPM) often uses deep learning models to\npredict the future behavior of ongoing processes, such as predicting process\noutcomes. While these models achieve high accuracy, their lack of\ninterpretability undermines user trust and adoption. Explainable AI (XAI) aims\nto address this challenge by providing the reasoning behind the predictions.\nHowever, current evaluations of XAI in PPM focus primarily on functional\nmetrics (such as fidelity), overlooking user-centered aspects such as their\neffect on task performance and decision-making. This study investigates the\neffects of explanation styles (feature importance, rule-based, and\ncounterfactual) and perceived AI accuracy (low or high) on decision-making in\nPPM. We conducted a decision-making experiment, where users were presented with\nthe AI predictions, perceived accuracy levels, and explanations of different\nstyles. Users' decisions were measured both before and after receiving\nexplanations, allowing the assessment of objective metrics (Task Performance\nand Agreement) and subjective metrics (Decision Confidence). Our findings show\nthat perceived accuracy and explanation style have a significant effect."}
{"id": "2506.16388", "pdf": "https://arxiv.org/pdf/2506.16388.pdf", "abs": "https://arxiv.org/abs/2506.16388", "title": "HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection", "authors": ["Sani Abdullahi Sani", "Salim Abubakar", "Falalu Ibrahim Lawan", "Abdulhamid Abubakar", "Maryam Bala"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our approach to multi-label emotion detection in Hausa, a\nlow-resource African language, as part of SemEval Track A. We fine-tuned\nAfriBERTa, a transformer-based model pre-trained on African languages, to\nclassify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and\nsurprise. Our methodology involved data preprocessing, tokenization, and model\nfine-tuning using the Hugging Face Trainer API. The system achieved a\nvalidation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the\neffectiveness of transformer-based models for emotion detection in low-resource\nlanguages."}
{"id": "2506.16622", "pdf": "https://arxiv.org/pdf/2506.16622.pdf", "abs": "https://arxiv.org/abs/2506.16622", "title": "Modeling Public Perceptions of Science in Media", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstin", "David Jurgens"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content."}
{"id": "2506.16389", "pdf": "https://arxiv.org/pdf/2506.16389.pdf", "abs": "https://arxiv.org/abs/2506.16389", "title": "RiOT: Efficient Prompt Refinement with Residual Optimization Tree", "authors": ["Chenyi Zhou", "Zhengyan Shi", "Yuan Yao", "Lei Liang", "Huajun Chen", "Qiang Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have highlighted their\npotential across a variety of tasks, but their performance still heavily relies\non the design of effective prompts. Existing methods for automatic prompt\noptimization face two challenges: lack of diversity, limiting the exploration\nof valuable and innovative directions and semantic drift, where optimizations\nfor one task can degrade performance in others. To address these issues, we\npropose Residual Optimization Tree (RiOT), a novel framework for automatic\nprompt optimization. RiOT iteratively refines prompts through text gradients,\ngenerating multiple semantically diverse candidates at each step, and selects\nthe best prompt using perplexity. Additionally, RiOT incorporates the text\nresidual connection to mitigate semantic drift by selectively retaining\nbeneficial content across optimization iterations. A tree structure efficiently\nmanages the optimization process, ensuring scalability and flexibility.\nExtensive experiments across five benchmarks, covering commonsense,\nmathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT\noutperforms both previous prompt optimization methods and manual prompting."}
{"id": "2506.16697", "pdf": "https://arxiv.org/pdf/2506.16697.pdf", "abs": "https://arxiv.org/abs/2506.16697", "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools."}
{"id": "2506.16393", "pdf": "https://arxiv.org/pdf/2506.16393.pdf", "abs": "https://arxiv.org/abs/2506.16393", "title": "From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling", "authors": ["Yao Lu", "Zhaiyuan Ji", "Jiawei Du", "Yu Shanqing", "Qi Xuan", "Tianyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although the annotation paradigm based on Large Language Models (LLMs) has\nmade significant breakthroughs in recent years, its actual deployment still has\ntwo core bottlenecks: first, the cost of calling commercial APIs in large-scale\nannotation is very expensive; second, in scenarios that require fine-grained\nsemantic understanding, such as sentiment classification and toxicity\nclassification, the annotation accuracy of LLMs is even lower than that of\nSmall Language Models (SLMs) dedicated to this field. To address these\nproblems, we propose a new paradigm of multi-model cooperative annotation and\ndesign a fully automatic annotation framework AutoAnnotator based on this.\nSpecifically, AutoAnnotator consists of two layers. The upper-level\nmeta-controller layer uses the generation and reasoning capabilities of LLMs to\nselect SLMs for annotation, automatically generate annotation code and verify\ndifficult samples; the lower-level task-specialist layer consists of multiple\nSLMs that perform annotation through multi-model voting. In addition, we use\nthe difficult samples obtained by the secondary review of the meta-controller\nlayer as the reinforcement learning set and fine-tune the SLMs in stages\nthrough a continual learning strategy, thereby improving the generalization of\nSLMs. Extensive experiments show that AutoAnnotator outperforms existing\nopen-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.\nNotably, AutoAnnotator reduces the annotation cost by 74.15% compared to\ndirectly annotating with GPT-3.5-turbo, while still improving the accuracy by\n6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator."}
{"id": "2506.16702", "pdf": "https://arxiv.org/pdf/2506.16702.pdf", "abs": "https://arxiv.org/abs/2506.16702", "title": "Large Language Models as Psychological Simulators: A Methodological Guide", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research."}
{"id": "2506.16395", "pdf": "https://arxiv.org/pdf/2506.16395.pdf", "abs": "https://arxiv.org/abs/2506.16395", "title": "OJBench: A Competition Level Code Benchmark For Large Language Models", "authors": ["Zhexu Wang", "Yiping Liu", "Yejie Wang", "Wenyang He", "Bofei Gao", "Muxi Diao", "Yanxu Chen", "Kelin Fu", "Flood Sung", "Zhilin Yang", "Tianyu Liu", "Weiran Xu"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsignificant progress in math and code reasoning capabilities. However, existing\ncode benchmark are limited in their ability to evaluate the full spectrum of\nthese capabilities, particularly at the competitive level. To bridge this gap,\nwe introduce OJBench, a novel and challenging benchmark designed to assess the\ncompetitive-level code reasoning abilities of LLMs. OJBench comprises 232\nprogramming competition problems from NOI and ICPC, providing a more rigorous\ntest of models' reasoning skills. We conducted a comprehensive evaluation using\nOJBench on 37 models, including both closed-source and open-source models,\nreasoning-oriented and non-reasoning-oriented models. Our results indicate that\neven state-of-the-art reasoning-oriented models, such as o4-mini and\nGemini-2.5-pro-exp, struggle with highly challenging competition-level\nproblems. This highlights the significant challenges that models face in\ncompetitive-level code reasoning."}
{"id": "2310.07019", "pdf": "https://arxiv.org/pdf/2310.07019.pdf", "abs": "https://arxiv.org/abs/2310.07019", "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans and AI", "authors": ["Quan Ze Chen", "Amy X. Zhang"], "categories": ["cs.HC"], "comment": "Accepted at ACM Collective Intelligence 2025", "summary": "From moderating content within an online community to producing\nsocially-appropriate generative outputs, decision-making tasks -- conducted by\neither humans or AI -- often depend on subjective or socially-established\ncriteria. To ensure such decisions are consistent, prevailing processes\nprimarily make use of high-level rules and guidelines to ground decisions,\nsimilar to applying \"constitutions\" in the legal context. However,\ninconsistencies in specifying and interpreting constitutional grounding can\nlead to undesirable and even incorrect decisions being made. In this work, we\nintroduce \"case law grounding\" (CLG) -- an approach for grounding subjective\ndecision-making using past decisions, similar to how precedents are used in\ncase law. We present how this grounding approach can be implemented in both\nhuman and AI decision-making contexts, introducing both a human-led process and\na large language model (LLM) prompting setup. Evaluating with five groups and\ncommunities across two decision-making task domains, we find that decisions\nproduced with CLG were significantly more accurately aligned to ground truth in\n4 out of 5 groups, achieving a 16.0--23.3 %-points higher accuracy in the human\nprocess, and 20.8--32.9 %-points higher with LLMs. We also examined the impact\nof different configurations with the retrieval window size and binding nature\nof decisions and find that binding decisions and larger retrieval windows were\nbeneficial. Finally, we discuss the broader implications of using CLG to\naugment existing constitutional grounding when it comes to aligning human and\nAI decisions."}
{"id": "2506.16399", "pdf": "https://arxiv.org/pdf/2506.16399.pdf", "abs": "https://arxiv.org/abs/2506.16399", "title": "NepaliGPT: A Generative Language Model for the Nepali Language", "authors": ["Shushanta Pudasaini", "Aman Shakya", "Siddhartha Shrestha", "Sahil Bhatta", "Sunil Thapa", "Sushmita Palikhe"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "After the release of ChatGPT, Large Language Models (LLMs) have gained huge\npopularity in recent days and thousands of variants of LLMs have been released.\nHowever, there is no generative language model for the Nepali language, due to\nwhich other downstream tasks, including fine-tuning, have not been explored\nyet. To fill this research gap in the Nepali NLP space, this research proposes\n\\textit{NepaliGPT}, a generative large language model tailored specifically for\nthe Nepali language. This research introduces an advanced corpus for the Nepali\nlanguage collected from several sources, called the Devanagari Corpus.\nLikewise, the research introduces the first NepaliGPT benchmark dataset\ncomprised of 4,296 question-answer pairs in the Nepali language. The proposed\nLLM NepaliGPT achieves the following metrics in text generation: Perplexity of\n26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\\%, and causal\nconsistency of 85.41\\%."}
{"id": "2410.20564", "pdf": "https://arxiv.org/pdf/2410.20564.pdf", "abs": "https://arxiv.org/abs/2410.20564", "title": "Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors", "authors": ["Sadia Nowrin", "Keith Vertanen"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "To appear in PErvasive Technologies Related to Assistive Environments\n  (PETRA '25)", "summary": "Conversational systems rely heavily on speech recognition to interpret and\nrespond to user commands and queries. Despite progress on speech recognition\naccuracy, errors may still sometimes occur and can significantly affect the\nend-user utility of such systems. While visual feedback can help detect errors,\nit may not always be practical, especially for people who are blind or\nlow-vision. In this study, we investigate ways to improve error detection by\nmanipulating the audio output of the transcribed text based on the recognizer's\nconfidence level in its result. Our findings show that selectively slowing down\nthe audio when the recognizer exhibited uncertainty led to a 12% relative\nincrease in participants' ability to detect errors compared to uniformly\nslowing the audio. It also reduced the time it took participants to listen to\nthe recognition result and decide if there was an error by 11%."}
{"id": "2506.16411", "pdf": "https://arxiv.org/pdf/2506.16411.pdf", "abs": "https://arxiv.org/abs/2506.16411", "title": "When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework", "authors": ["Zhen Xu", "Shang Zhu", "Jue Wang", "Junlin Wang", "Ben Athiwaratkun", "Chi Wang", "James Zou", "Ce Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "under review", "summary": "We investigate the challenge of applying Large Language Models (LLMs) to long\ntexts. We propose a theoretical framework that distinguishes the failure modes\nof long context tasks into three categories: cross-chunk dependence (task\nnoise), confusion that grows with context size (model noise), and the imperfect\nintegration of partial results (aggregator noise). Under this view, we analyze\nwhen it is effective to use multi-agent chunking, i.e., dividing a length\nsequence into smaller chunks and aggregating the processed results of each\nchunk. Our experiments on tasks such as retrieval, question answering, and\nsummarization confirm both the theoretical analysis and the conditions that\nfavor multi-agent chunking. By exploring superlinear model noise growth with\ninput length, we also explain why, for large inputs, a weaker model configured\nwith chunk-based processing can surpass a more advanced model like GPT4o\napplied in a single shot. Overall, we present a principled understanding\nframework and our results highlight a direct pathway to handling long contexts\nin LLMs with carefully managed chunking and aggregator strategies."}
{"id": "2411.02263", "pdf": "https://arxiv.org/pdf/2411.02263.pdf", "abs": "https://arxiv.org/abs/2411.02263", "title": "AI Should Challenge, Not Obey", "authors": ["Advait Sarkar"], "categories": ["cs.HC"], "comment": "Advait Sarkar. 2024. AI Should Challenge, Not Obey. Commun. ACM 67,\n  10 (October 2024), 18-21. https://doi.org/10.1145/3649404", "summary": "Let's transform our robot secretaries into Socratic gadflies."}
{"id": "2506.16444", "pdf": "https://arxiv.org/pdf/2506.16444.pdf", "abs": "https://arxiv.org/abs/2506.16444", "title": "REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing", "authors": ["Kangqi Chen", "Andreas Kosmas Kakolyris", "Rakesh Nadig", "Manos Frouzakis", "Nika Mansouri Ghiasi", "Yu Liang", "Haiyu Mao", "Jisung Park", "Mohammad Sadrosadati", "Onur Mutlu"], "categories": ["cs.CL", "cs.AR", "cs.DB", "H.3.3; I.2.7"], "comment": "Extended version of our publication at the 52nd International\n  Symposium on Computer Architecture (ISCA-52), 2025", "summary": "Large Language Models (LLMs) face an inherent challenge: their knowledge is\nconfined to the data that they have been trained on. To overcome this issue,\nRetrieval-Augmented Generation (RAG) complements the static training-derived\nknowledge of LLMs with an external knowledge repository. RAG consists of three\nstages: indexing, retrieval, and generation. The retrieval stage of RAG becomes\na significant bottleneck in inference pipelines. In this stage, a user query is\nmapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)\nalgorithm searches for similar vectors in the database to identify relevant\nitems. Due to the large database sizes, ANNS incurs significant data movement\noverheads between the host and the storage system. To alleviate these\noverheads, prior works propose In-Storage Processing (ISP) techniques that\naccelerate ANNS by performing computations inside storage. However, existing\nworks that leverage ISP for ANNS (i) employ algorithms that are not tailored to\nISP systems, (ii) do not accelerate data retrieval operations for data selected\nby ANNS, and (iii) introduce significant hardware modifications, limiting\nperformance and hindering their adoption. We propose REIS, the first ISP system\ntailored for RAG that addresses these limitations with three key mechanisms.\nFirst, REIS employs a database layout that links database embedding vectors to\ntheir associated documents, enabling efficient retrieval. Second, it enables\nefficient ANNS by introducing an ISP-tailored data placement technique that\ndistributes embeddings across the planes of the storage system and employs a\nlightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that\nuses the existing computational resources inside the storage system. Compared\nto a server-grade system, REIS improves the performance (energy efficiency) of\nretrieval by an average of 13x (55x)."}
{"id": "2412.00630", "pdf": "https://arxiv.org/pdf/2412.00630.pdf", "abs": "https://arxiv.org/abs/2412.00630", "title": "Collective Creation of Intimacy: Exploring the Cosplay Commission Practice within the Otome Game Community in China", "authors": ["Yihao Zhou", "Haowei Xu", "Lili Zhang", "Shengdong Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Cosplay commission (cos-commission) is a new form of commodified romantic\ncompanionship within the Otome game community in China. To explore the\nmotivations, practices, experiences, and challenges, we conducted\nsemi-structured interviews with 15 participants in different roles. Our\nfindings reveal that cos-commission, as a hybrid activity, provides\nparticipants with a chance to collaboratively build meaningful connections. It\nalso offers a pathway for personal exploration and emotional recovery. However,\nthe vague boundary between performative roles and intimate interactions can\ngive rise to unexpected negative outcomes, such as attachment-driven\nentanglements and post-commission \"withdrawal symptoms.\" While digital\nplatforms facilitate communication in cos-commissions, they often lack\nsufficient safeguards. This preliminary work provides insights into the\nformation process of hybrid intimate relationship and its potential to foster\npersonalized, long-term support for mental well-being, and reveals potential\nprivacy and security challenges."}
{"id": "2506.16445", "pdf": "https://arxiv.org/pdf/2506.16445.pdf", "abs": "https://arxiv.org/abs/2506.16445", "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation", "authors": ["Haotian Xia", "Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long story generation remains a challenge for existing large language models\n(LLMs), primarily due to two main factors: (1) discourse coherence, which\nrequires plot consistency, logical coherence, and completeness in the long-form\ngeneration, and (2) narrative complexity, which requires an interwoven and\nengaging narrative. To address these challenges, we propose StoryWriter, a\nmulti-agent story generation framework, which consists of three main modules:\n(1) outline agent, which generates event-based outlines containing rich event\nplots, character, and event-event relationships. (2) planning agent, which\nfurther details events and plans which events should be written in each chapter\nto maintain an interwoven and engaging story. (3) writing agent, which\ndynamically compresses the story history based on the current event to generate\nand reflect new plots, ensuring the coherence of the generated story. We\nconduct both human and automated evaluation, and StoryWriter significantly\noutperforms existing story generation baselines in both story quality and\nlength. Furthermore, we use StoryWriter to generate a dataset, which contains\nabout $6,000$ high-quality long stories, with an average length of $8,000$\nwords. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning\non LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which\ndemonstrates advanced performance in long story generation."}
{"id": "2412.14195", "pdf": "https://arxiv.org/pdf/2412.14195.pdf", "abs": "https://arxiv.org/abs/2412.14195", "title": "A multimodal dataset for understanding the impact of mobile phones on remote online virtual education", "authors": ["Roberto Daza", "Alvaro Becerra", "Ruth Cobos", "Julian Fierrez", "Aythami Morales"], "categories": ["cs.HC", "cs.CV"], "comment": "Article under review in the journal Scientific Data. GitHub\n  repository of the dataset at: https://github.com/BiDAlab/IMPROVE", "summary": "This work presents the IMPROVE dataset, a multimodal resource designed to\nevaluate the effects of mobile phone usage on learners during online education.\nIt includes behavioral, biometric, physiological, and academic performance data\ncollected from 120 learners divided into three groups with different levels of\nphone interaction, enabling the analysis of the impact of mobile phone usage\nand related phenomena such as nomophobia. A setup involving 16 synchronized\nsensors -- including EEG, eye tracking, video cameras, smartwatches, and\nkeystroke dynamics -- was used to monitor learner activity during 30-minute\nsessions involving educational videos, document reading, and multiple-choice\ntests. Mobile phone usage events, including both controlled interventions and\nuncontrolled interactions, were labeled by supervisors and refined through a\nsemi-supervised re-labeling process. Technical validation confirmed signal\nquality, and statistical analyses revealed biometric changes associated with\nphone usage. The dataset is publicly available for research through GitHub and\nScience Data Bank, with synchronized recordings from three platforms (edBB,\nedX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and\naccompanied by a detailed guide."}
{"id": "2506.16476", "pdf": "https://arxiv.org/pdf/2506.16476.pdf", "abs": "https://arxiv.org/abs/2506.16476", "title": "Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection", "authors": ["Saad Almohaimeed", "Saleh Almohaimeed", "Damla Turgut", "Ladislau B√∂l√∂ni"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Implicit hate speech has recently emerged as a critical challenge for social\nmedia platforms. While much of the research has traditionally focused on\nharmful speech in general, the need for generalizable techniques to detect\nveiled and subtle forms of hate has become increasingly pressing. Based on\nlexicon analysis, we hypothesize that implicit hate speech is already present\nin publicly available harmful speech datasets but may not have been explicitly\nrecognized or labeled by annotators. Additionally, crowdsourced datasets are\nprone to mislabeling due to the complexity of the task and often influenced by\nannotators' subjective interpretations. In this paper, we propose an approach\nto address the detection of implicit hate speech and enhance generalizability\nacross diverse datasets by leveraging existing harmful speech datasets. Our\nmethod comprises three key components: influential sample identification,\nreannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental\nresults demonstrate the effectiveness of our approach in improving implicit\nhate detection, achieving a +12.9-point F1 score improvement compared to the\nbaseline."}
{"id": "2502.04599", "pdf": "https://arxiv.org/pdf/2502.04599.pdf", "abs": "https://arxiv.org/abs/2502.04599", "title": "Fuzzy Linkography: Automatic Graphical Summarization of Creative Activity Traces", "authors": ["Amy Smith", "Barrett R. Anderson", "Jasmine Tan Otto", "Isaac Karth", "Yuqian Sun", "John Joon Young Chung", "Melissa Roemmele", "Max Kreminski"], "categories": ["cs.HC"], "comment": "ACM C&C 2025. Code available at\n  https://github.com/mkremins/fuzzy-linkography", "summary": "Linkography -- the analysis of links between the design moves that make up an\nepisode of creative ideation or design -- can be used for both visual and\nquantitative assessment of creative activity traces. Traditional linkography,\nhowever, is time-consuming, requiring a human coder to manually annotate both\nthe design moves within an episode and the connections between them. As a\nresult, linkography has not yet been much applied at scale. To address this\nlimitation, we introduce fuzzy linkography: a means of automatically\nconstructing a linkograph from a sequence of recorded design moves via a\n\"fuzzy\" computational model of semantic similarity, enabling wider deployment\nand new applications of linkographic techniques. We apply fuzzy linkography to\nthree markedly different kinds of creative activity traces (text-to-image\nprompting journeys, LLM-supported ideation sessions, and researcher publication\nhistories) and discuss our findings, as well as strengths, limitations, and\npotential future applications of our approach."}
{"id": "2506.16502", "pdf": "https://arxiv.org/pdf/2506.16502.pdf", "abs": "https://arxiv.org/abs/2506.16502", "title": "Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples", "authors": ["Soumya Suvra Ghosal", "Vaibhav Singh", "Akash Ghosh", "Soumyabrata Pal", "Subhadip Baidya", "Sriparna Saha", "Dinesh Manocha"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward models are essential for aligning large language models (LLMs) with\nhuman preferences. However, most open-source multilingual reward models are\nprimarily trained on preference datasets in high-resource languages, resulting\nin unreliable reward signals for low-resource Indic languages. Collecting\nlarge-scale, high-quality preference data for these languages is prohibitively\nexpensive, making preference-based training approaches impractical. To address\nthis challenge, we propose RELIC, a novel in-context learning framework for\nreward modeling in low-resource Indic languages. RELIC trains a retriever with\na pairwise ranking objective to select in-context examples from auxiliary\nhigh-resource languages that most effectively highlight the distinction between\npreferred and less-preferred responses. Extensive experiments on three\npreference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art\nopen-source reward models demonstrate that RELIC significantly improves reward\nmodel accuracy for low-resource Indic languages, consistently outperforming\nexisting example selection methods. For example, on Bodo-a low-resource Indic\nlanguage-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%\nimprovement in accuracy over zero-shot prompting and state-of-the-art example\nselection method, respectively."}
{"id": "2502.05731", "pdf": "https://arxiv.org/pdf/2502.05731.pdf", "abs": "https://arxiv.org/abs/2502.05731", "title": "Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies", "authors": ["Sam Yu-Te Lee", "Cheng-Wei Hung", "Mei-Hua Yuan", "Kwan-Liu Ma"], "categories": ["cs.HC"], "comment": "IEEE PacificVis 2025 Information systems, Information systems\n  applications, Data Mining; Human-centered computing, Visualization,\n  Visualization systems and tools", "summary": "Environmental experts have developed the DPSIR (Driver, Pressure, State,\nImpact, Response) framework to systematically study and communicate key\nrelationships between society and the environment. Using this framework\nrequires experts to construct a DPSIR taxonomy from a corpus, annotate the\ndocuments, and identify DPSIR variables and relationships, which is laborious\nand inflexible. Automating it with conventional text mining faces technical\nchallenges, primarily because the taxonomy often begins with abstract\ndefinitions, which experts progressively refine and contextualize as they\nannotate the corpus. In response, we develop GreenMine, a system that supports\ninteractive text mining with prompt engineering. The system implements a\nprompting pipeline consisting of three simple and evaluable subtasks. In each\nsubtask, the DPSIR taxonomy can be defined in natural language and iteratively\nrefined as experts analyze the corpus. To support users evaluate the taxonomy,\nwe introduce an uncertainty score based on response consistency. Then, we\ndesign a radial uncertainty chart that visualizes uncertainties and corpus\ntopics, which supports interleaved evaluation and exploration. Using the\nsystem, experts can progressively construct the DPSIR taxonomy and annotate the\ncorpus with LLMs. Using real-world interview transcripts, we present a case\nstudy to demonstrate the capability of the system in supporting interactive\nmining of DPSIR relationships, and an expert review in the form of\ncollaborative discussion to understand the potential and limitations of the\nsystem. We discuss the lessons learned from developing the system and future\nopportunities for supporting interactive text mining in knowledge-intensive\ntasks for other application scenarios."}
{"id": "2506.16558", "pdf": "https://arxiv.org/pdf/2506.16558.pdf", "abs": "https://arxiv.org/abs/2506.16558", "title": "Automatic Speech Recognition Biases in Newcastle English: an Error Analysis", "authors": ["Dana Serditova", "Kevin Tang", "Jochen Steffens"], "categories": ["cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) systems struggle with regional dialects\ndue to biased training which favours mainstream varieties. While previous\nresearch has identified racial, age, and gender biases in ASR, regional bias\nremains underexamined. This study investigates ASR performance on Newcastle\nEnglish, a well-documented regional dialect known to be challenging for ASR. A\ntwo-stage analysis was conducted: first, a manual error analysis on a subsample\nidentified key phonological, lexical, and morphosyntactic errors behind ASR\nmisrecognitions; second, a case study focused on the systematic analysis of ASR\nrecognition of the regional pronouns ``yous'' and ``wor''. Results show that\nASR errors directly correlate with regional dialectal features, while social\nfactors play a lesser role in ASR mismatches. We advocate for greater dialectal\ndiversity in ASR training data and highlight the value of sociolinguistic\nanalysis in diagnosing and addressing regional biases."}
{"id": "2502.15242", "pdf": "https://arxiv.org/pdf/2502.15242.pdf", "abs": "https://arxiv.org/abs/2502.15242", "title": "Agonistic Image Generation: Unsettling the Hegemony of Intention", "authors": ["Andrew Shaw", "Andre Ye", "Ranjay Krishna", "Amy X. Zhang"], "categories": ["cs.HC"], "comment": "Accepted to ACM Fairness, Accountability, Transparency 2025 --\n  Athens, Greece", "summary": "Current image generation paradigms prioritize actualizing user intention -\n\"see what you intend\" - but often neglect the sociopolitical dimensions of this\nprocess. However, it is increasingly evident that image generation is\npolitical, contributing to broader social struggles over visual meaning. This\nsociopolitical aspect was highlighted by the March 2024 Gemini controversy,\nwhere Gemini faced criticism for inappropriately injecting demographic\ndiversity into user prompts. Although the developers sought to redress image\ngeneration's sociopolitical dimension by introducing diversity \"corrections,\"\ntheir opaque imposition of a standard for \"diversity\" ultimately proved\ncounterproductive. In this paper, we present an alternative approach: an image\ngeneration interface designed to embrace open negotiation along the\nsociopolitical dimensions of image creation. Grounded in the principles of\nagonistic pluralism (from the Greek agon, meaning struggle), our interface\nactively engages users with competing visual interpretations of their prompts.\nThrough a lab study with 29 participants, we evaluate our agonistic interface\non its ability to facilitate reflection - engagement with other perspectives\nand challenging dominant assumptions - a core principle that underpins\nagonistic contestation. We compare it to three existing paradigms: a standard\ninterface, a Gemini-style interface that produces \"diverse\" images, and an\nintention-centric interface suggesting prompt refinements. Our findings\ndemonstrate that the agonistic interface enhances reflection across multiple\nmeasures, but also that reflection depends on users perceiving the interface as\nboth appropriate and empowering; introducing diversity without grounding it in\nrelevant political contexts was perceived as inauthentic. Our results suggest\nthat diversity and user intention should not be treated as opposing values to\nbe balanced."}
{"id": "2506.16574", "pdf": "https://arxiv.org/pdf/2506.16574.pdf", "abs": "https://arxiv.org/abs/2506.16574", "title": "Weight Factorization and Centralization for Continual Learning in Speech Recognition", "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Modern neural network based speech recognition models are required to\ncontinually absorb new data without re-training the whole system, especially in\ndownstream applications using foundation models, having no access to the\noriginal training data. Continually training the models in a rehearsal-free,\nmultilingual, and language agnostic condition, likely leads to catastrophic\nforgetting, when a seemingly insignificant disruption to the weights can\ndestructively harm the quality of the models. Inspired by the ability of human\nbrains to learn and consolidate knowledge through the waking-sleeping cycle, we\npropose a continual learning approach with two distinct phases: factorization\nand centralization, learning and merging knowledge accordingly. Our experiments\non a sequence of varied code-switching datasets showed that the centralization\nstage can effectively prevent catastrophic forgetting by accumulating the\nknowledge in multiple scattering low-rank adapters."}
{"id": "2503.01769", "pdf": "https://arxiv.org/pdf/2503.01769.pdf", "abs": "https://arxiv.org/abs/2503.01769", "title": "Using Collective Dialogues and AI to Find Common Ground Between Israeli and Palestinian Peacebuilders", "authors": ["Andrew Konya", "Luke Thorburn", "Wasim Almasri", "Oded Adomi Leshem", "Ariel D. Procaccia", "Lisa Schirch", "Michiel A. Bakker"], "categories": ["cs.HC"], "comment": "Accepted at FAccT 2025", "summary": "A growing body of work has shown that AI-assisted methods -- leveraging large\nlanguage models, social choice methods, and collective dialogues -- can help\nnavigate polarization and surface common ground in controlled lab settings. But\nwhat can these approaches contribute in real-world contexts? We present a case\nstudy applying these techniques to find common ground between Israeli and\nPalestinian peacebuilders in the period following October 7th, 2023. From April\nto July 2024 an iterative deliberative process combining LLMs, bridging-based\nranking, and collective dialogues was conducted in partnership with the\nAlliance for Middle East Peace. Around 138 civil society peacebuilders\nparticipated including Israeli Jews, Palestinian citizens of Israel, and\nPalestinians from the West Bank and Gaza. The process resulted in a set of\ncollective statements, including demands to world leaders, with at least 84%\nagreement from participants on each side. In this paper, we document the\nprocess, results, challenges, and important open questions."}
{"id": "2506.16580", "pdf": "https://arxiv.org/pdf/2506.16580.pdf", "abs": "https://arxiv.org/abs/2506.16580", "title": "Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement", "authors": ["Tuan-Nam Nguyen", "Ngoc-Quan Pham", "Seymanur Akti", "Alexander Waibel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "We propose a first streaming accent conversion (AC) model that transforms\nnon-native speech into a native-like accent while preserving speaker identity,\nprosody and improving pronunciation. Our approach enables stream processing by\nmodifying a previous AC architecture with an Emformer encoder and an optimized\ninference mechanism. Additionally, we integrate a native text-to-speech (TTS)\nmodel to generate ideal ground-truth data for efficient training. Our streaming\nAC model achieves comparable performance to the top AC models while maintaining\nstable latency, making it the first AC system capable of streaming."}
{"id": "2503.14103", "pdf": "https://arxiv.org/pdf/2503.14103.pdf", "abs": "https://arxiv.org/abs/2503.14103", "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments using a Retrieval-Augmented Language Model", "authors": ["Jonas Oppenlaender"], "categories": ["cs.HC", "H.5.m"], "comment": "14 pages, 3 figures, 1 table", "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research."}
{"id": "2506.16584", "pdf": "https://arxiv.org/pdf/2506.16584.pdf", "abs": "https://arxiv.org/abs/2506.16584", "title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework", "authors": ["Nadav Kunievsky", "James A. Evans"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 68T05", "I.2.7; I.2.6; I.5.1"], "comment": null, "summary": "Understanding whether large language models (LLMs) possess a world model-a\nstructured understanding of the world that supports generalization beyond\nsurface-level patterns-is central to assessing their reliability, especially in\nhigh-stakes applications. We propose a formal framework for evaluating whether\nan LLM exhibits a sufficiently robust world model, defined as producing\nconsistent outputs across semantically equivalent prompts while distinguishing\nbetween prompts that express different intents. We introduce a new evaluation\napproach to measure this that decomposes model response variability into three\ncomponents: variability due to user purpose, user articulation, and model\ninstability. An LLM with a strong world model should attribute most of the\nvariability in its responses to changes in foundational purpose rather than\nsuperficial changes in articulation. This approach allows us to quantify how\nmuch of a model's behavior is semantically grounded rather than driven by model\ninstability or alternative wording. We apply this framework to evaluate LLMs\nacross diverse domains. Our results show how larger models attribute a greater\nshare of output variability to changes in user purpose, indicating a more\nrobust world model. This improvement is not uniform, however: larger models do\nnot consistently outperform smaller ones across all domains, and their\nadvantage in robustness is often modest. These findings highlight the\nimportance of moving beyond accuracy-based benchmarks toward semantic\ndiagnostics that more directly assess the structure and stability of a model's\ninternal understanding of the world."}
{"id": "2506.10932", "pdf": "https://arxiv.org/pdf/2506.10932.pdf", "abs": "https://arxiv.org/abs/2506.10932", "title": "Video-Mediated Emotion Disclosure: Expressions of Fear, Sadness, and Joy by People with Schizophrenia on YouTube", "authors": ["Jiaying Lizzy Liu", "Yan Zhang"], "categories": ["cs.HC", "cs.CY", "cs.MM"], "comment": "10 pages", "summary": "Individuals with schizophrenia frequently experience intense emotions and\noften turn to vlogging as a medium for emotional expression. While previous\nresearch has predominantly focused on text based disclosure, little is known\nabout how individuals construct narratives around emotions and emotional\nexperiences in video blogs. Our study addresses this gap by analyzing 200\nYouTube videos created by individuals with schizophrenia. Drawing on media\nresearch and self presentation theories, we developed a visual analysis\nframework to disentangle these videos. Our analysis revealed diverse practices\nof emotion disclosure through both verbal and visual channels, highlighting the\ndynamic interplay between these modes of expression. We found that the\ndeliberate construction of visual elements, including environmental settings\nand specific aesthetic choices, appears to foster more supportive and engaged\nviewer responses. These findings underscore the need for future large scale\nquantitative research examining how visual features shape video mediated\ncommunication on social media platforms. Such investigations would inform the\ndevelopment of care centered video sharing platforms that better support\nindividuals managing illness experiences."}
{"id": "2506.16594", "pdf": "https://arxiv.org/pdf/2506.16594.pdf", "abs": "https://arxiv.org/abs/2506.16594", "title": "A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications", "authors": ["Hanshu Rao", "Weisi Liu", "Haohan Wang", "I-Chan Huang", "Zhe He", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Synthetic data generation--mitigating data scarcity, privacy concerns, and\ndata quality challenges in biomedical fields--has been facilitated by rapid\nadvances of large language models (LLMs). This scoping review follows\nPRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and\n2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The\nreview systematically examines biomedical research and application trends in\nsynthetic data generation, emphasizing clinical applications, methodologies,\nand evaluations. Our analysis identifies data modalities of unstructured texts\n(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation\nmethods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model\n(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),\nhuman-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The\nanalysis addresses current limitations in what, where, and how health\nprofessionals can leverage synthetic data generation for biomedical domains.\nOur review also highlights challenges in adaption across clinical domains,\nresource and model accessibility, and evaluation standardizations."}
{"id": "2506.14777", "pdf": "https://arxiv.org/pdf/2506.14777.pdf", "abs": "https://arxiv.org/abs/2506.14777", "title": "WebXAII: an open-source web framework to study human-XAI interaction", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "S√©bastien Harispe"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature."}
{"id": "2506.16622", "pdf": "https://arxiv.org/pdf/2506.16622.pdf", "abs": "https://arxiv.org/abs/2506.16622", "title": "Modeling Public Perceptions of Science in Media", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstin", "David Jurgens"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content."}
{"id": "2404.15564", "pdf": "https://arxiv.org/pdf/2404.15564.pdf", "abs": "https://arxiv.org/abs/2404.15564", "title": "Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency", "authors": ["Jun Huang", "Yan Liu"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "CAI2024 Camera-ready Submission and Correction", "summary": "This paper proposes a new gradient-based XAI method called Guided\nAbsoluteGrad for saliency map explanations. We utilize both positive and\nnegative gradient magnitudes and employ gradient variance to distinguish the\nimportant areas for noise deduction. We also introduce a novel evaluation\nmetric named ReCover And Predict (RCAP), which considers the Localization and\nVisual Noise Level objectives of the explanations. We propose two propositions\nfor these two objectives and prove the necessity of evaluating them. We\nevaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the\nRCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset\nwith ResNet50 model; (2) International Skin Imaging Collaboration (ISIC)\ndataset with EfficientNet model; (3) the Places365 dataset with DenseNet161\nmodel. Our method surpasses other gradient-based approaches, showcasing the\nquality of enhanced saliency map explanations through gradient magnitude."}
{"id": "2506.16628", "pdf": "https://arxiv.org/pdf/2506.16628.pdf", "abs": "https://arxiv.org/abs/2506.16628", "title": "Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System", "authors": ["Jianlin Shi", "Brian T. Bucher"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite advances in machine learning (ML) and large language models (LLMs),\nrule-based natural language processing (NLP) systems remain active in clinical\nsettings due to their interpretability and operational efficiency. However,\ntheir manual development and maintenance are labor-intensive, particularly in\ntasks with large linguistic variability. To overcome these limitations, we\nproposed a novel approach employing LLMs solely during the rule-based systems\ndevelopment phase. We conducted the initial experiments focusing on the first\ntwo steps of developing a rule-based NLP pipeline: find relevant snippets from\nthe clinical note; extract informative keywords from the snippets for the\nrule-based named entity recognition (NER) component. Our experiments\ndemonstrated exceptional recall in identifying clinically relevant text\nsnippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.\nThis study sheds light on a promising new direction for NLP development,\nenabling semi-automated or automated development of rule-based systems with\nsignificantly faster, more cost-effective, and transparent execution compared\nwith deep learning model-based solutions."}
{"id": "2407.11823", "pdf": "https://arxiv.org/pdf/2407.11823.pdf", "abs": "https://arxiv.org/abs/2407.11823", "title": "Harmonizing Safety and Speed: A Human-Algorithm Approach to Enhance the FDA's Medical Device Clearance Policy", "authors": ["Mohammad Zhalechian", "Soroush Saghafian", "Omar Robles"], "categories": ["cs.LG", "cs.HC", "math.OC", "stat.ML"], "comment": null, "summary": "The United States Food and Drug Administration's (FDA's) Premarket\nNotification 510(k) pathway allows manufacturers to gain approval for a medical\ndevice by demonstrating its substantial equivalence to another legally marketed\ndevice. However, the inherent ambiguity of this regulatory procedure has led to\nhigh recall rates for many devices cleared through this pathway. This trend has\nraised significant concerns regarding the efficacy of the FDA's current\napproach, prompting a reassessment of the 510(k) regulatory framework. In this\npaper, we develop a combined human-algorithm approach to assist the FDA in\nimproving its 510(k) medical device clearance process by reducing the risk of\nrecalls and the workload imposed on the FDA. We first develop machine learning\nmethods to estimate the risk of recall of 510(k) medical devices based on the\ninformation available at submission time. We then propose a data-driven\nclearance policy that recommends acceptance, rejection, or deferral to FDA's\ncommittees for in-depth evaluation. We conduct an empirical study using a\nunique large-scale dataset of over 31,000 medical devices that we assembled\nbased on data sources from the FDA and Centers for Medicare and Medicaid\nService (CMS). A conservative evaluation of our proposed policy based on this\ndata shows a 32.9% improvement in the recall rate and a 40.5% reduction in the\nFDA's workload. Our analyses also indicate that implementing our policy could\nresult in significant annual cost savings of $1.7 billion, which highlights the\nvalue of using a holistic and data-driven approach to improve the FDA's current\n510(k) medical device evaluation pathway."}
{"id": "2506.16633", "pdf": "https://arxiv.org/pdf/2506.16633.pdf", "abs": "https://arxiv.org/abs/2506.16633", "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View", "authors": ["Fenghua Cheng", "Jinxiang Wang", "Sen Wang", "Zi Huang", "Xue Li"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "Multimodal reasoning is a process of understanding, integrating and inferring\ninformation across different data modalities. It has recently attracted surging\nacademic attention as a benchmark for Artificial Intelligence (AI). Although\nthere are various tasks for evaluating multimodal reasoning ability, they still\nhave limitations. Lack of reasoning on hierarchical visual clues at different\nlevels of granularity, e.g., local details and global context, is of little\ndiscussion, despite its frequent involvement in real scenarios. To bridge the\ngap, we introduce a novel and challenging task for multimodal reasoning, namely\nGeoGuess. Given a street view image, the task is to identify its location and\nprovide a detailed explanation. A system that succeeds in GeoGuess should be\nable to detect tiny visual clues, perceive the broader landscape, and associate\nwith vast geographic knowledge. Therefore, GeoGuess would require the ability\nto reason between hierarchical visual information and geographic knowledge. In\nthis work, we establish a benchmark for GeoGuess by introducing a specially\ncurated dataset GeoExplain which consists of\npanoramas-geocoordinates-explanation tuples. Additionally, we present a\nmultimodal and multilevel reasoning method, namely SightSense which can make\nprediction and generate comprehensive explanation based on hierarchy of visual\ninformation and external knowledge. Our analysis and experiments demonstrate\ntheir outstanding performance in GeoGuess."}
{"id": "2412.16402", "pdf": "https://arxiv.org/pdf/2412.16402.pdf", "abs": "https://arxiv.org/abs/2412.16402", "title": "The Landscape of College-level Data Visualization Courses, and the Benefits of Incorporating Statistical Thinking", "authors": ["Zach Branson", "Monica Paz Parra", "Ronald Yurko"], "categories": ["stat.OT", "cs.HC"], "comment": null, "summary": "Data visualization is a core part of statistical practice and is ubiquitous\nin many fields. Although there are numerous books on data visualization,\ninstructors in statistics and data science may be unsure how to teach data\nvisualization, because it is such a broad discipline. To give guidance on\nteaching data visualization from a statistical perspective, we make two\ncontributions. First, we conduct a survey of data visualization courses at top\ncolleges and universities in the United States, in order to understand the\nlandscape of data visualization courses. We find that most courses are not\ntaught by statistics and data science departments and do not focus on\nstatistical topics, especially those related to inference. Instead, most\ncourses focus on visual storytelling, aesthetic design, dashboard design, and\nother topics specialized for other disciplines. Second, we outline three\nteaching principles for incorporating statistical inference in data\nvisualization courses, and provide several examples that demonstrate how to\nfollow these principles. The dataset from our survey allows others to explore\nthe diversity of data visualization courses, and our teaching principles give\nguidance for encouraging statistical thinking when teaching data visualization."}
{"id": "2506.16640", "pdf": "https://arxiv.org/pdf/2506.16640.pdf", "abs": "https://arxiv.org/abs/2506.16640", "title": "Long-Context Generalization with Sparse Attention", "authors": ["Pavlo Vasylenko", "Marcos Treviso", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformer-based architectures traditionally employ softmax to compute\nattention weights, which produces dense distributions over all tokens in a\nsequence. While effective in many settings, this density has been shown to be\ndetrimental for tasks that demand precise focus on fixed-size patterns: as\nsequence length increases, non-informative tokens accumulate attention\nprobability mass, leading to dispersion and representational collapse. We show\nin this paper that sparse attention mechanisms using $\\alpha$-entmax can avoid\nthese issues, due to their ability to assign exact zeros to irrelevant tokens.\nFurthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows\n$\\alpha$-entmax with a learnable temperature parameter, allowing the attention\ndistribution to interpolate between sparse (pattern-focused) and dense\n(softmax-like) regimes. Finally, we show that the ability to locate and\ngeneralize fixed-size patterns can be further improved through a careful design\nof position encodings, which impacts both dense and sparse attention methods.\nBy integrating ASEntmax into standard transformer layers alongside proper\npositional encodings, we show that our models greatly outperform softmax,\nscalable softmax, and fixed-temperature $\\alpha$-entmax baselines on\nlong-context generalization."}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707.pdf", "abs": "https://arxiv.org/abs/2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."}
{"id": "2506.16655", "pdf": "https://arxiv.org/pdf/2506.16655.pdf", "abs": "https://arxiv.org/abs/2506.16655", "title": "Arch-Router: Aligning LLM Routing with Human Preferences", "authors": ["Co Tran", "Salman Paracha", "Adil Hafeez", "Shuguang Chen"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce \\textbf{Arch-Router}, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: \\texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}."}
{"id": "2506.10964", "pdf": "https://arxiv.org/pdf/2506.10964.pdf", "abs": "https://arxiv.org/abs/2506.10964", "title": "The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins", "authors": ["Rico H Herzog", "Till Degkwitz", "Trivik Verma"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Urban digital twins are increasingly perceived as a way to pool the growing\ndigital resources of cities for the purpose of a more sustainable and\nintegrated urban planning. Models and simulations are central to this\nundertaking: They enable \"what if?\" scenarios, create insights and describe\nrelationships between the vast data that is being collected. However, the\nprocess of integrating and subsequently using models in urban digital twins is\nan inherently complex undertaking. It raises questions about how to represent\nurban complexity, how to deal with uncertain assumptions and modeling\nparadigms, and how to capture underlying power relations. Existent approaches\nin the domain largely focus on monolithic and centralized solutions in the\ntradition of neoliberal city-making, oftentimes prohibiting pluralistic and\nopen interoperable models. Using a participatory design for participatory\nsystems approach together with the City of Hamburg, Germany, we find that an\nopen Urban Model Platform can function both as a public technological backbone\nfor modeling and simulation in urban digital twins and as a socio-technical\nframework for a collaborative and pluralistic representation of urban\nprocesses. Such a platform builds on open standards, allows for a decentralized\nintegration of models, enables communication between models and supports a\nmulti-model approach to representing urban systems."}
{"id": "2506.16678", "pdf": "https://arxiv.org/pdf/2506.16678.pdf", "abs": "https://arxiv.org/abs/2506.16678", "title": "Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations", "authors": ["Ananth Agarwal", "Jasper Jian", "Christopher D. Manning", "Shikhar Murty"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit a robust mastery of syntax when\nprocessing and generating text. While this suggests internalized understanding\nof hierarchical syntax and dependency relations, the precise mechanism by which\nthey represent syntactic structure is an open area within interpretability\nresearch. Probing provides one way to identify the mechanism of syntax being\nlinearly encoded in activations, however, no comprehensive study has yet\nestablished whether a model's probing accuracy reliably predicts its downstream\nsyntactic performance. Adopting a \"mechanisms vs. outcomes\" framework, we\nevaluate 32 open-weight transformer models and find that syntactic features\nextracted via probing fail to predict outcomes of targeted syntax evaluations\nacross English linguistic phenomena. Our results highlight a substantial\ndisconnect between latent syntactic representations found via probing and\nobservable syntactic behaviors in downstream tasks."}
{"id": "2506.11015", "pdf": "https://arxiv.org/pdf/2506.11015.pdf", "abs": "https://arxiv.org/abs/2506.11015", "title": "The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI", "authors": ["Barbara Oakley", "Michael Johnston", "Ken-Zen Chen", "Eulho Jung", "Terrence J. Sejnowski"], "categories": ["cs.CY", "cs.AI", "cs.HC", "q-bio.NC"], "comment": "50 pages, 8 figures", "summary": "In the age of generative AI and ubiquitous digital tools, human cognition\nfaces a structural paradox: as external aids become more capable, internal\nmemory systems risk atrophy. Drawing on neuroscience and cognitive psychology,\nthis paper examines how heavy reliance on AI systems and discovery-based\npedagogies may impair the consolidation of declarative and procedural memory --\nsystems essential for expertise, critical thinking, and long-term retention. We\nreview how tools like ChatGPT and calculators can short-circuit the retrieval,\nerror correction, and schema-building processes necessary for robust neural\nencoding. Notably, we highlight striking parallels between deep learning\nphenomena such as \"grokking\" and the neuroscience of overlearning and\nintuition. Empirical studies are discussed showing how premature reliance on AI\nduring learning inhibits proceduralization and intuitive mastery. We argue that\neffective human-AI interaction depends on strong internal models -- biological\n\"schemata\" and neural manifolds -- that enable users to evaluate, refine, and\nguide AI output. The paper concludes with policy implications for education and\nworkforce training in the age of large language models."}
{"id": "2506.16692", "pdf": "https://arxiv.org/pdf/2506.16692.pdf", "abs": "https://arxiv.org/abs/2506.16692", "title": "LegiGPT: Party Politics and Transport Policy with Large Language Model", "authors": ["Hyunsoo Yun", "Eun Hak Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Given the significant influence of lawmakers' political ideologies on\nlegislative decision-making, understanding their impact on policymaking is\ncritically important. We introduce a novel framework, LegiGPT, which integrates\na large language model (LLM) with explainable artificial intelligence (XAI) to\nanalyze transportation-related legislative proposals. LegiGPT employs a\nmulti-stage filtering and classification pipeline using zero-shot prompting\nwith GPT-4. Using legislative data from South Korea's 21st National Assembly,\nwe identify key factors - including sponsor characteristics, political\naffiliations, and geographic variables - that significantly influence\ntransportation policymaking. The LLM was used to classify\ntransportation-related bill proposals through a stepwise filtering process\nbased on keywords, phrases, and contextual relevance. XAI techniques were then\napplied to examine relationships between party affiliation and associated\nattributes. The results reveal that the number and proportion of conservative\nand progressive sponsors, along with district size and electoral population,\nare critical determinants shaping legislative outcomes. These findings suggest\nthat both parties contributed to bipartisan legislation through different forms\nof engagement, such as initiating or supporting proposals. This integrated\napproach provides a valuable tool for understanding legislative dynamics and\nguiding future policy development, with broader implications for infrastructure\nplanning and governance."}
{"id": "2506.12699", "pdf": "https://arxiv.org/pdf/2506.12699.pdf", "abs": "https://arxiv.org/abs/2506.12699", "title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation", "authors": ["Yashothara Shanmugarasa", "Ming Ding", "M. A. P Chamikara", "Thierry Rakotoarivelo"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are sophisticated artificial intelligence\nsystems that enable machines to generate human-like text with remarkable\nprecision. While LLMs offer significant technological progress, their\ndevelopment using vast amounts of user data scraped from the web and collected\nfrom extensive user interactions poses risks of sensitive information leakage.\nMost existing surveys focus on the privacy implications of the training data\nbut tend to overlook privacy risks from user interactions and advanced LLM\ncapabilities. This paper aims to fill that gap by providing a comprehensive\nanalysis of privacy in LLMs, categorizing the challenges into four main areas:\n(i) privacy issues in LLM training data, (ii) privacy challenges associated\nwith user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and\n(iv) privacy challenges involving LLM agents. We evaluate the effectiveness and\nlimitations of existing mitigation mechanisms targeting these proposed privacy\nchallenges and identify areas for further research."}
{"id": "2506.16712", "pdf": "https://arxiv.org/pdf/2506.16712.pdf", "abs": "https://arxiv.org/abs/2506.16712", "title": "ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models", "authors": ["Bin Chen", "Xinzge Gao", "Chuanrui Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative Reward Models (GRMs) provide greater flexibility than scalar\nreward models in capturing human preferences, but their effectiveness is\nlimited by poor reasoning capabilities. This often results in incomplete or\noverly speculative reasoning paths, leading to hallucinations or missing key\ninformation in complex tasks. We address this challenge with ReasonGRM, a\nthree-stage generative reward modeling framework. In the first stage, Zero-RL\nis used to generate concise, outcome-directed reasoning paths that reduce the\nlikelihood of critical omissions. In the second stage, we introduce a novel\nevaluation metric, $R^\\star$, which scores reasoning paths based on their\ngeneration likelihood. This favors paths that reach correct answers with\nminimal exploration, helping to reduce hallucination-prone data during\ntraining. In the final stage, the model is further refined through\nreinforcement learning on challenging examples to enhance its preference\ndiscrimination capabilities. Experiments on three public benchmarks show that\nReasonGRM achieves competitive or state-of-the-art performance, outperforming\nprevious best GRMs by 1.8\\% on average and surpassing proprietary models such\nas GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of\nreasoning-aware training and highlight the importance of high-quality rationale\nselection for reliable preference modeling."}
{"id": "2506.14854", "pdf": "https://arxiv.org/pdf/2506.14854.pdf", "abs": "https://arxiv.org/abs/2506.14854", "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "authors": ["Varun Mannam", "Zhenyu Shi"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring."}
{"id": "2506.16724", "pdf": "https://arxiv.org/pdf/2506.16724.pdf", "abs": "https://arxiv.org/abs/2506.16724", "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties", "authors": ["Xinyi Liu", "Weiguang Wang", "Hangfeng He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) for open-ended\ntasks, accurately assessing epistemic uncertainty, which reflects a model's\nlack of knowledge, has become crucial to ensuring reliable outcomes. However,\nquantifying epistemic uncertainty in such tasks is challenging due to the\npresence of aleatoric uncertainty, which arises from multiple valid answers.\nWhile bias can introduce noise into epistemic uncertainty estimation, it may\nalso reduce noise from aleatoric uncertainty. To investigate this trade-off, we\nconduct experiments on Visual Question Answering (VQA) tasks and find that\nmitigating prompt-introduced bias improves uncertainty quantification in\nGPT-4o. Building on prior work showing that LLMs tend to copy input information\nwhen model confidence is low, we further analyze how these prompt biases affect\nmeasured epistemic and aleatoric uncertainty across varying bias-free\nconfidence levels with GPT-4o and Qwen2-VL. We find that all considered biases\ninduce greater changes in both uncertainties when bias-free model confidence is\nlower. Moreover, lower bias-free model confidence leads to greater\nunderestimation of epistemic uncertainty (i.e. overconfidence) due to bias,\nwhereas it has no significant effect on the direction of changes in aleatoric\nuncertainty estimation. These distinct effects deepen our understanding of bias\nmitigation for uncertainty quantification and potentially inform the\ndevelopment of more advanced techniques."}
{"id": "2506.16738", "pdf": "https://arxiv.org/pdf/2506.16738.pdf", "abs": "https://arxiv.org/abs/2506.16738", "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization", "authors": ["Daejin Jo", "Jeeyoung Yun", "Byungseok Roh", "Sungwoong Kim"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "With the rapid progress of speech language models (SLMs), discrete speech\ntokens have emerged as a core interface between speech and text, enabling\nunified modeling across modalities. Recent speech tokenization approaches aim\nto isolate semantic information from low-level acoustics to better align with\nlanguage models. In particular, previous methods use SSL teachers such as\nHuBERT to extract semantic representations, which are then distilled into a\nsemantic quantizer to suppress acoustic redundancy as well as capture\ncontent-related latent structures. However, they still produce speech token\nsequences significantly longer than their textual counterparts, creating\nchallenges for efficient speech-language modeling. Reducing the frame rate is a\nnatural solution, but standard techniques, such as rigid average pooling across\nframes, can distort or dilute the semantic structure required for effective LM\nalignment. To address this, we propose LM-SPT, a speech tokenization method\nthat introduces a novel semantic distillation. Instead of directly matching\nteacher and student features via pooling, we reconstruct speech solely from\nsemantic tokens and minimize the discrepancy between the encoded\nrepresentations of the original and reconstructed waveforms, obtained from a\nfrozen automatic speech recognition (ASR) encoder. This indirect yet\ndata-driven supervision enables the tokenizer to learn discrete units that are\nmore semantically aligned with language models. LM-SPT further incorporates\narchitectural improvements to the encoder and decoder for speech tokenization,\nand supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.\nExperimental results show that LM-SPT achieves superior reconstruction fidelity\ncompared to baselines, and that SLMs trained with LM-SPT tokens achieve\ncompetitive performances on speech-to-text and consistently outperform\nbaselines on text-to-speech tasks."}
{"id": "2506.16755", "pdf": "https://arxiv.org/pdf/2506.16755.pdf", "abs": "https://arxiv.org/abs/2506.16755", "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly", "authors": ["Lance Ying", "Ryan Truong", "Katherine M. Collins", "Cedegao E. Zhang", "Megan Wei", "Tyler Brooke-Wilson", "Tan Zhi-Xuan", "Lionel Wong", "Joshua B. Tenenbaum"], "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 19 pages", "summary": "Drawing real world social inferences usually requires taking into account\ninformation from multiple modalities. Language is a particularly powerful\nsource of information in social settings, especially in novel situations where\nlanguage can provide both abstract information about the environment dynamics\nand concrete specifics about an agent that cannot be easily visually observed.\nIn this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a\nframework for drawing context-specific social inferences that integrate\nlinguistic and visual inputs. LIRAS frames multimodal social reasoning as a\nprocess of constructing structured but situation-specific agent and environment\nrepresentations - leveraging multimodal language models to parse language and\nvisual inputs into unified symbolic representations, over which a Bayesian\ninverse planning engine can be run to produce granular probabilistic judgments.\nOn a range of existing and new social reasoning tasks derived from cognitive\nscience experiments, we find that our model (instantiated with a comparatively\nlightweight VLM) outperforms ablations and state-of-the-art models in capturing\nhuman judgments across all domains."}
{"id": "2506.16756", "pdf": "https://arxiv.org/pdf/2506.16756.pdf", "abs": "https://arxiv.org/abs/2506.16756", "title": "SocialSim: Towards Socialized Simulation of Emotional Support Conversation", "authors": ["Zhuang Chen", "Yaru Cao", "Guanqun Bi", "Jincenzi Wu", "Jinfeng Zhou", "Xiyao Xiao", "Si Chen", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "AAAI 2025 Paper #32116 (Without Publication Edits)", "summary": "Emotional support conversation (ESC) helps reduce people's psychological\nstress and provide emotional value through interactive dialogues. Due to the\nhigh cost of crowdsourcing a large ESC corpus, recent attempts use large\nlanguage models for dialogue augmentation. However, existing approaches largely\noverlook the social dynamics inherent in ESC, leading to less effective\nsimulations. In this paper, we introduce SocialSim, a novel framework that\nsimulates ESC by integrating key aspects of social interactions: social\ndisclosure and social awareness. On the seeker side, we facilitate social\ndisclosure by constructing a comprehensive persona bank that captures diverse\nand authentic help-seeking scenarios. On the supporter side, we enhance social\nawareness by eliciting cognitive reasoning to generate logical and supportive\nresponses. Building upon SocialSim, we construct SSConv, a large-scale\nsynthetic ESC corpus of which quality can even surpass crowdsourced ESC data.\nWe further train a chatbot on SSConv and demonstrate its state-of-the-art\nperformance in both automatic and human evaluations. We believe SocialSim\noffers a scalable way to synthesize ESC, making emotional care more accessible\nand practical."}
{"id": "2506.16760", "pdf": "https://arxiv.org/pdf/2506.16760.pdf", "abs": "https://arxiv.org/abs/2506.16760", "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models", "authors": ["Lei Jiang", "Zixun Zhang", "Zizhou Wang", "Xiaobing Sun", "Zhen Li", "Liangli Zhen", "Xiaohua Xu"], "categories": ["cs.CL", "cs.CV"], "comment": "15 pages, 9 figures", "summary": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance\nacross multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass\nbuilt-in safety mechanisms to elicit restricted content generation. Existing\nblack-box jailbreak methods primarily rely on adversarial textual prompts or\nimage perturbations, yet these approaches are highly detectable by standard\ncontent filtering systems and exhibit low query and computational efficiency.\nIn this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),\na novel black-box jailbreak attack framework that decomposes malicious prompts\ninto semantically benign visual and textual fragments. By leveraging LVLMs'\ncross-modal reasoning abilities, CAMO covertly reconstructs harmful\ninstructions through multi-step reasoning, evading conventional detection\nmechanisms. Our approach supports adjustable reasoning complexity and requires\nsignificantly fewer queries than prior attacks, enabling both stealth and\nefficiency. Comprehensive evaluations conducted on leading LVLMs validate\nCAMO's effectiveness, showcasing robust performance and strong cross-model\ntransferability. These results underscore significant vulnerabilities in\ncurrent built-in safety mechanisms, emphasizing an urgent need for advanced,\nalignment-aware security and safety solutions in vision-language systems."}
{"id": "2506.16777", "pdf": "https://arxiv.org/pdf/2506.16777.pdf", "abs": "https://arxiv.org/abs/2506.16777", "title": "DistillNote: LLM-based clinical note summaries improve heart failure diagnosis", "authors": ["Heloisa Oss Boll", "Antonio Oss Boll", "Leticia Puttlitz Boll", "Ameen Abu Hanna", "Iacer Calixto"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research."}
{"id": "2506.16792", "pdf": "https://arxiv.org/pdf/2506.16792.pdf", "abs": "https://arxiv.org/abs/2506.16792", "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning", "authors": ["Muyang Zheng", "Yuanzhi Yao", "Changting Lin", "Rui Wang", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST."}
{"id": "2506.16912", "pdf": "https://arxiv.org/pdf/2506.16912.pdf", "abs": "https://arxiv.org/abs/2506.16912", "title": "From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts", "authors": ["Daniel Christoph", "Max Ploner", "Patrick Haller", "Alan Akbik"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to the First Workshop on Large Language Model Memorization\n  (L2M2), co-located with ACL 2025 in Vienna", "summary": "Sample efficiency is a crucial property of language models with practical\nimplications for training efficiency. In real-world text, information follows a\nlong-tailed distribution. Yet, we expect models to learn and recall frequent\nand infrequent facts. Sample-efficient models are better equipped to handle\nthis challenge of learning and retaining rare information without requiring\nexcessive exposure. This study analyzes multiple models of varying\narchitectures and sizes, all trained on the same pre-training data. By\nannotating relational facts with their frequencies in the training corpus, we\nexamine how model performance varies with fact frequency. Our findings show\nthat most models perform similarly on high-frequency facts but differ notably\non low-frequency facts. This analysis provides new insights into the\nrelationship between model architecture, size, and factual learning efficiency."}
{"id": "2506.16982", "pdf": "https://arxiv.org/pdf/2506.16982.pdf", "abs": "https://arxiv.org/abs/2506.16982", "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond", "authors": ["Antonin Berthon", "Mihaela van der Schaar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality."}
{"id": "2506.16990", "pdf": "https://arxiv.org/pdf/2506.16990.pdf", "abs": "https://arxiv.org/abs/2506.16990", "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the SDProc Workshop @ ACL 2025", "summary": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert."}
{"id": "2506.17001", "pdf": "https://arxiv.org/pdf/2506.17001.pdf", "abs": "https://arxiv.org/abs/2506.17001", "title": "PersonalAI: Towards digital twins in the graph form", "authors": ["Mikhail Menschikov", "Dmitry Evseev", "Ruslan Kostoev", "Ilya Perepechkin", "Ilnaz Salimov", "Victoria Dochkina", "Petr Anokhin", "Evgeny Burnaev", "Nikita Semenov"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies."}
{"id": "2506.17006", "pdf": "https://arxiv.org/pdf/2506.17006.pdf", "abs": "https://arxiv.org/abs/2506.17006", "title": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Kenneth R. Koedinger"], "categories": ["cs.CL", "cs.CY"], "comment": "Full research paper accepted at EC-TEL '25", "summary": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility."}
{"id": "2506.17019", "pdf": "https://arxiv.org/pdf/2506.17019.pdf", "abs": "https://arxiv.org/abs/2506.17019", "title": "Instituto de Telecomunica√ß√µes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning", "authors": ["Giuseppe Attanasio", "Sonal Sannigrahi", "Ben Peters", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 1 figure, IWSLT 2025", "summary": "This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on\nInstruction Following Speech Processing. We submit results for the Short Track,\ni.e., speech recognition, translation, and spoken question answering. Our model\nis a unified speech-to-text model that integrates a pre-trained continuous\nspeech encoder and text decoder through a first phase of modality alignment and\na second phase of instruction fine-tuning. Crucially, we focus on using\nsmall-scale language model backbones (< 2B) and restrict to high-quality, CC-BY\ndata along with synthetic data generation to supplement existing resources."}
{"id": "2506.17046", "pdf": "https://arxiv.org/pdf/2506.17046.pdf", "abs": "https://arxiv.org/abs/2506.17046", "title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models", "authors": ["Xiaolong Wang", "Zhaolu Kang", "Wangyuxuan Zhai", "Xinyue Lou", "Yunghwei Lai", "Ziyue Wang", "Yawen Wang", "Kaiyu Huang", "Yile Wang", "Peng Li", "Yang Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances across numerous vision-language tasks. Due to their strong image-text\nalignment capability, MLLMs can effectively understand image-text pairs with\nclear meanings. However, effectively resolving the inherent ambiguities in\nnatural language and visual contexts remains challenging. Existing multimodal\nbenchmarks typically overlook linguistic and visual ambiguities, relying mainly\non unimodal context for disambiguation and thus failing to exploit the mutual\nclarification potential between modalities. To bridge this gap, we introduce\nMUCAR, a novel and challenging benchmark designed explicitly for evaluating\nmultimodal ambiguity resolution across multilingual and cross-modal scenarios.\nMUCAR includes: (1) a multilingual dataset where ambiguous textual expressions\nare uniquely resolved by corresponding visual contexts, and (2) a\ndual-ambiguity dataset that systematically pairs ambiguous images with\nambiguous textual contexts, with each combination carefully constructed to\nyield a single, clear interpretation through mutual disambiguation. Extensive\nevaluations involving 19 state-of-the-art multimodal models--encompassing both\nopen-source and proprietary architectures--reveal substantial gaps compared to\nhuman-level performance, highlighting the need for future research into more\nsophisticated cross-modal ambiguity comprehension methods, further pushing the\nboundaries of multimodal reasoning."}
{"id": "2506.17077", "pdf": "https://arxiv.org/pdf/2506.17077.pdf", "abs": "https://arxiv.org/abs/2506.17077", "title": "Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025", "authors": ["Dominik Mach√°ƒçek", "Peter Pol√°k"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency."}
{"id": "2506.17080", "pdf": "https://arxiv.org/pdf/2506.17080.pdf", "abs": "https://arxiv.org/abs/2506.17080", "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs", "authors": ["Ricardo Rei", "Nuno M. Guerreiro", "Jos√© Pombal", "Jo√£o Alves", "Pedro Teixeirinha", "Amin Farajian", "Andr√© F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization."}
{"id": "2506.17088", "pdf": "https://arxiv.org/pdf/2506.17088.pdf", "abs": "https://arxiv.org/abs/2506.17088", "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "authors": ["Jiahao Cheng", "Tiancheng Su", "Jia Yuan", "Guoxiu He", "Jiawei Liu", "Xinqi Tao", "Jingwen Xie", "Huaxia Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect."}
{"id": "2506.17090", "pdf": "https://arxiv.org/pdf/2506.17090.pdf", "abs": "https://arxiv.org/abs/2506.17090", "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions", "authors": ["Murtaza Nazir", "Matthew Finlayson", "John X. Morris", "Xiang Ren", "Swabha Swayamdipta"], "categories": ["cs.CL"], "comment": null, "summary": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known."}
{"id": "2506.17121", "pdf": "https://arxiv.org/pdf/2506.17121.pdf", "abs": "https://arxiv.org/abs/2506.17121", "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?", "authors": ["Adithya Bhaskar", "Alexander Wettig", "Tianyu Gao", "Yihe Dong", "Danqi Chen"], "categories": ["cs.CL"], "comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong", "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."}
{"id": "2506.17180", "pdf": "https://arxiv.org/pdf/2506.17180.pdf", "abs": "https://arxiv.org/abs/2506.17180", "title": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models", "authors": ["Naiming Liu", "Richard Baraniuk", "Shashank Sonkar"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions\ndesigned to evaluate whether language models can determine if one statement\ncausally explains another. Each question present an assertion-reason pair and\nchallenge language models to distinguish between semantic relatedness and\ngenuine causal explanatory relationships. Through comprehensive evaluation of\n21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we\nidentify two fundamental findings. First, language models frequently confuse\nsemantic similarity with causality, relying on lexical and semantic overlap\ninstead of inferring actual causal explanatory relationships. Second, as\nparameter size increases, models tend to shift from being overly skeptical\nabout causal relationships to being excessively permissive in accepting them.\nDespite this shift, performance measured by the Matthews Correlation\nCoefficient plateaus at just 0.55, even for the best-performing models.Hence,\nCLEAR-3K provides a crucial benchmark for developing and evaluating genuine\ncausal reasoning in language models, which is an essential capability for\napplications that require accurate assessment of causal relationships."}
{"id": "2506.17188", "pdf": "https://arxiv.org/pdf/2506.17188.pdf", "abs": "https://arxiv.org/abs/2506.17188", "title": "Towards AI Search Paradigm", "authors": ["Yuchen Li", "Hengyi Cai", "Rui Kong", "Xinran Chen", "Jiamin Chen", "Jun Yang", "Haojie Zhang", "Jiayi Li", "Jiayi Wu", "Yiqun Chen", "Changle Qu", "Keyi Kong", "Wenwen Ye", "Lixin Su", "Xinyu Ma", "Long Xia", "Daiting Shi", "Jiashu Zhao", "Haoyi Xiong", "Shuaiqiang Wang", "Dawei Yin"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems."}
{"id": "2506.17209", "pdf": "https://arxiv.org/pdf/2506.17209.pdf", "abs": "https://arxiv.org/abs/2506.17209", "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency", "authors": ["Kathleen C. Fraser", "Hillary Dawkins", "Isar Nejadgholi", "Svetlana Kiritchenko"], "categories": ["cs.CL"], "comment": "to appear at LLMSEC 2025", "summary": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future."}
{"id": "2506.15655", "pdf": "https://arxiv.org/pdf/2506.15655.pdf", "abs": "https://arxiv.org/abs/2506.15655", "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence."}
{"id": "2506.15689", "pdf": "https://arxiv.org/pdf/2506.15689.pdf", "abs": "https://arxiv.org/abs/2506.15689", "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models", "authors": ["Liulu He", "Shenli Zhen", "Karwei Sun", "Yijiang Liu", "Yufei Zhao", "Chongkang Tan", "Huanrui Yang", "Yuan Du", "Li Du"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon."}
{"id": "2506.15697", "pdf": "https://arxiv.org/pdf/2506.15697.pdf", "abs": "https://arxiv.org/abs/2506.15697", "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks", "authors": ["Yi Liu", "Hongji Zhang", "Yunhao Zhou", "Zhengyuan Shi", "Changran Xu", "Qiang Xu"], "categories": ["cs.AR", "cs.CL", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "The integration of large language models (LLMs) into electronic design\nautomation (EDA) has significantly advanced the field, offering transformative\nbenefits, particularly in register transfer level (RTL) code generation and\nunderstanding. While previous studies have demonstrated the efficacy of\nfine-tuning LLMs for these generation-based tasks, embedding-based tasks, which\nare equally critical to EDA workflows, have been largely overlooked. These\ntasks, including natural language code search, RTL code functionality\nequivalence checking, and performance prediction, are essential for\naccelerating and optimizing the hardware design process. To address this gap,\nwe present DeepRTL2, a family of versatile LLMs that unifies both generation-\nand embedding-based tasks related to RTL. By simultaneously tackling a broad\nrange of tasks, DeepRTL2 represents the first model to provide a comprehensive\nsolution to the diverse challenges in EDA. Through extensive experiments, we\nshow that DeepRTL2 achieves state-of-the-art performance across all evaluated\ntasks."}
{"id": "2506.15704", "pdf": "https://arxiv.org/pdf/2506.15704.pdf", "abs": "https://arxiv.org/abs/2506.15704", "title": "Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding", "authors": ["Feiyu Yao", "Qian Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to support increasingly longer\ncontexts, the memory demand for key-value (KV) caches during decoding grows\nrapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe\nbandwidth. Sparse attention mechanisms alleviate this issue by computing\nattention weights only for selected key-value pairs. However, their indexing\ncomputation typically requires traversing all key vectors, resulting in\nsignificant computational and data transfer overhead. To reduce the cost of\nindex retrieval, existing methods often treat each decoding step as an\nindependent process, failing to exploit the temporal correlations embedded in\nhistorical decoding information. To this end, we propose LFPS(Learn From the\nPast for Sparse Indexing), an acceleration method that dynamically constructs\nsparse indexing candidates based on historical attention patterns. LFPS\ncaptures two prevalent trends in decoder attention -vertical patterns\n(attending to fixed positions) and slash patterns (attending to relative\npositions) -and incorporates a positional expansion strategy to effectively\npredict the Top-k indices for the current step. We validate LFPS on challenging\nlong-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as\nthe base model. Experimental results show that LFPS achieves up to 22.8$\\times$\nspeedup over full attention and 9.6$\\times$ speedup over exact Top-k retrieval\non an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,\nwhile preserving generation accuracy. These results demonstrate that LFPS\noffers a practical and efficient solution for decoding optimization in\nlong-context LLM inference."}
{"id": "2506.15714", "pdf": "https://arxiv.org/pdf/2506.15714.pdf", "abs": "https://arxiv.org/abs/2506.15714", "title": "Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention", "authors": ["Andrew Kiruluta"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We propose an innovative, learnable two-sided short-time Laplace transform\n(STLT) mechanism to supplant the traditional self attention in\ntransformer-based LLMs. Our STLT introduces trainable parameters for each\nLaplace node, enabling end-to-end learning of decay rates , oscillatory\nfrequencies, and window bandwidth T. This flexibility allows the model to\ndynamically adapt token relevance half lives and frequency responses during\ntraining. By selecting S learnable nodes and leveraging fast recursive\nconvolution, we achieve an effective complexity of in time and memory. We\nfurther incorporate an efficient FFT-based computation of the relevance matrix\nand an adaptive node allocation mechanism to dynamically adjust the number of\nactive Laplace nodes. Empirical results on language modeling (WikiText\\-103,\nProject Gutenberg), machine translation (WMT'14 En\\-De), and long document\nquestion answering (NarrativeQA) demonstrate that our learnable STLT achieves\nperplexities and scores on par with or better than existing efficient\ntransformers while naturally extending to context lengths exceeding 100k tokens\nor more limited only by available hardware. Ablation studies confirm the\nimportance of learnable parameters and adaptive node allocation. The proposed\napproach combines interpretability, through explicit decay and frequency\nparameters, with scalability and robustness, offering a pathway towards\nultra-long-sequence language modeling without the computational bottleneck of\nself-attention."}
{"id": "2506.15717", "pdf": "https://arxiv.org/pdf/2506.15717.pdf", "abs": "https://arxiv.org/abs/2506.15717", "title": "daDPO: Distribution-Aware DPO for Distilling Conversational Abilities", "authors": ["Zhengze Zhang", "Shiqi Wang", "Yiqun Shen", "Simin Guo", "Dahua Lin", "Xiaoliang Wang", "Nguyen Cam-Tu", "Fei Tan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious applications, but their conversational abilities decline sharply as\nmodel size decreases, presenting a barrier to their deployment in\nresource-constrained environments. Knowledge distillation with Direct\nPreference Optimization (dDPO) has emerged as a promising approach to enhancing\nthe conversational abilities of smaller models using a larger teacher model.\nHowever, current methods primarily focus on 'black-box' KD, which only uses the\nteacher's responses, overlooking the output distribution offered by the\nteacher. This paper addresses this gap by introducing daDPO (Distribution-Aware\nDPO), a unified method for preference optimization and distribution-based\ndistillation. We provide rigorous theoretical analysis and empirical\nvalidation, showing that daDPO outperforms existing methods in restoring\nperformance for pruned models and enhancing smaller LLM models. Notably, in\nin-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve\nnear-teacher performance (-7.3% preference rate compared to that of dDPO's\n-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model\n(14.0% win rate)."}
{"id": "2506.15724", "pdf": "https://arxiv.org/pdf/2506.15724.pdf", "abs": "https://arxiv.org/abs/2506.15724", "title": "MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference", "authors": ["Kunxi Li", "Zhonghua Jiang", "Zhouzhou Shen", "Zhaode Wang", "Chengfei Lv", "Shengyu Zhang", "Fan Wu", "Fei Wu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces MadaKV, a modality-adaptive key-value (KV) cache\neviction strategy designed to enhance the efficiency of multimodal large\nlanguage models (MLLMs) in long-context inference. In multimodal scenarios,\nattention heads exhibit varying preferences for different modalities, resulting\nin significant disparities in modality importance across attention heads.\nTraditional KV cache eviction methods, which are tailored for unimodal\nsettings, fail to capture modality-specific information, thereby yielding\nsuboptimal performance. MadaKV addresses these challenges through two key\ncomponents: modality preference adaptation and hierarchical compression\ncompensation. By dynamically sensing modality information within attention\nheads and adaptively retaining critical tokens, MadaKV achieves substantial\nreductions in KV cache memory footprint and model inference decoding latency\n(1.3 to 1.5 times improvement) while maintaining high accuracy across various\nmultimodal long-context tasks. Extensive experiments on representative MLLMs\nand the MileBench benchmark demonstrate the effectiveness of MadaKV compared to\nexisting KV cache eviction methods."}
{"id": "2506.15741", "pdf": "https://arxiv.org/pdf/2506.15741.pdf", "abs": "https://arxiv.org/abs/2506.15741", "title": "OAgents: An Empirical Study of Building Effective Agents", "authors": ["He Zhu", "Tianrui Qin", "King Zhu", "Heyuan Huang", "Yeyi Guan", "Jinxiang Xia", "Yi Yao", "Hanhao Li", "Ningning Wang", "Pai Liu", "Tianhao Peng", "Xin Gui", "Xiaowan Li", "Yuhui Liu", "Yuchen Eleanor Jiang", "Jun Wang", "Changwang Zhang", "Xiangru Tang", "Ge Zhang", "Jian Yang", "Minghao Liu", "Xitong Gao", "Wangchunshu Zhou", "Jiaheng Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI."}
{"id": "2506.15745", "pdf": "https://arxiv.org/pdf/2506.15745.pdf", "abs": "https://arxiv.org/abs/2506.15745", "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding", "authors": ["Minsoo Kim", "Kyuhong Shim", "Jungwook Choi", "Simyung Chang"], "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."}
{"id": "2506.15787", "pdf": "https://arxiv.org/pdf/2506.15787.pdf", "abs": "https://arxiv.org/abs/2506.15787", "title": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning", "authors": ["Lukas Helff", "Ahmad Omar", "Felix Friedrich", "Wolfgang Stammer", "Antonia W√ºst", "Tim Woydt", "Rupert Mitchell", "Patrick Schramowski", "Kristian Kersting"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR enables scalable, automated synthesis of\ninductive reasoning tasks with precisely controlled difficulty. For each task,\nSLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation\nprogram used by a symbolic judge to deterministically verify model outputs, and\n(iii) an instruction prompt for the reasoning task. Using SLR, we create\nSLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum\nlevels that progressively increase in relational, arithmetic, and recursive\ncomplexity. Large-scale evaluation reveals that contemporary LLMs readily\nproduce syntactically valid rules, yet often fail at correct logical inference.\nRecent reasoning LLMs do somewhat better, but incur substantial increases in\ntest-time compute, sometimes exceeding 15k completion tokens. Finally,\nlogic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity\nwith Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully\nautomated, requires no human annotation, ensures dataset novelty, and offers a\nscalable environment for probing and advancing LLMs' reasoning capabilities."}
{"id": "2506.15862", "pdf": "https://arxiv.org/pdf/2506.15862.pdf", "abs": "https://arxiv.org/abs/2506.15862", "title": "MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers", "authors": ["Jushaan Singh Kalra", "Xinran Zhao", "To Eun Kim", "Fengyu Cai", "Fernando Diaz", "Tongshuang Wu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "19 pages, 3 figures", "summary": "Retrieval-augmented Generation (RAG) is powerful, but its effectiveness\nhinges on which retrievers we use and how. Different retrievers offer distinct,\noften complementary signals: BM25 captures lexical matches; dense retrievers,\nsemantic similarity. Yet in practice, we typically fix a single retriever based\non heuristics, which fails to generalize across diverse information needs. Can\nwe dynamically select and integrate multiple retrievers for each individual\nquery, without the need for manual selection? In our work, we validate this\nintuition with quantitative analysis and introduce mixture of retrievers: a\nzero-shot, weighted combination of heterogeneous retrievers. Extensive\nexperiments show that such mixtures are effective and efficient: Despite\ntotaling just 0.8B parameters, this mixture outperforms every individual\nretriever and even larger 7B models by +10.8% and +3.9% on average,\nrespectively. Further analysis also shows that this mixture framework can help\nincorporate specialized non-oracle human information sources as retrievers to\nachieve good collaboration, with a 58.9% relative performance improvement over\nsimulated humans alone."}
{"id": "2506.15882", "pdf": "https://arxiv.org/pdf/2506.15882.pdf", "abs": "https://arxiv.org/abs/2506.15882", "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute", "authors": ["Sheng Liu", "Tianlang Chen", "Pan Lu", "Haotian Ye", "Yizheng Chen", "Lei Xing", "James Zou"], "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.SP"], "comment": "18 pages, 5 figures, Project website:\n  https://shengliu66.github.io/fractreason/", "summary": "Test-time compute has emerged as a powerful paradigm for improving the\nperformance of large language models (LLMs), where generating multiple outputs\nor refining individual chains can significantly boost answer accuracy. However,\nexisting methods like Best-of-N, majority voting, and self-reflection typically\napply reasoning in a uniform way across inputs, overlooking the fact that\ndifferent problems may require different levels of reasoning depth. In this\nwork, we propose Fractional Reasoning, a training-free and model-agnostic\nframework that enables continuous control over reasoning intensity at inference\ntime, going beyond the limitations of fixed instructional prompts. Our method\noperates by extracting the latent steering vector associated with deeper\nreasoning and reapplying it with a tunable scaling factor, allowing the model\nto tailor its reasoning process to the complexity of each input. This supports\ntwo key modes of test-time scaling: (1) improving output quality in\nbreadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing\nthe correctness of individual reasoning chains in depth-based strategies (e.g.,\nself-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that\nFractional Reasoning consistently improves performance across diverse reasoning\ntasks and models."}
{"id": "2506.15912", "pdf": "https://arxiv.org/pdf/2506.15912.pdf", "abs": "https://arxiv.org/abs/2506.15912", "title": "Early Attentive Sparsification Accelerates Neural Speech Transcription", "authors": ["Zifei Xu", "Sayeh Sharify", "Hesham Mostafa", "Tristan Webb", "Wanzin Yazar", "Xin Wang"], "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Transformer-based neural speech processing has achieved state-of-the-art\nperformance. Since speech audio signals are known to be highly compressible,\nhere we seek to accelerate neural speech transcription by time-domain signal\nsparsification early in the neural encoding stage, taking advantage of the\ninterpretability of the self-attention mechanism in transformer audio encoders.\nWith the Whisper family of models, we perform a systematic architecture search\nover the joint space of sparsification stage (a certain encoder layer) and\ncompression ratio (sparsity). We found that the best resulting solutions under\n1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity\nat an early encoding stage, and thereby achieve up to 1.6x runtime acceleration\nin English speech transcription tasks on Nvidia GPUs without any fine-tuning."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Under review for KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2506.15975", "pdf": "https://arxiv.org/pdf/2506.15975.pdf", "abs": "https://arxiv.org/abs/2506.15975", "title": "Multi-use LLM Watermarking and the False Detection Problem", "authors": ["Zihao Fu", "Chris Russell"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Digital watermarking is a promising solution for mitigating some of the risks\narising from the misuse of automatically generated text. These approaches\neither embed non-specific watermarks to allow for the detection of any text\ngenerated by a particular sampler, or embed specific keys that allow the\nidentification of the LLM user. However, simultaneously using the same\nembedding for both detection and user identification leads to a false detection\nproblem, whereby, as user capacity grows, unwatermarked text is increasingly\nlikely to be falsely detected as watermarked. Through theoretical analysis, we\nidentify the underlying causes of this phenomenon. Building on these insights,\nwe propose Dual Watermarking which jointly encodes detection and identification\nwatermarks into generated text, significantly reducing false positives while\nmaintaining high detection accuracy. Our experimental results validate our\ntheoretical findings and demonstrate the effectiveness of our approach."}
{"id": "2506.16015", "pdf": "https://arxiv.org/pdf/2506.16015.pdf", "abs": "https://arxiv.org/abs/2506.16015", "title": "Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning", "authors": ["Craig S. Wright"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LO", "math.LO", "68T27, 03B70, 68P20", "I.2.3; F.4.1; H.2.8"], "comment": "91 pages, 0 figures, includes mathematical appendix and formal\n  proofs. Designed as a foundational submission for a modular autonomous\n  epistemic reasoning system. Suitable for logic in computer science, AI\n  epistemology, and scientific informatics", "summary": "The exponential expansion of scientific literature has surpassed the\nepistemic processing capabilities of both human experts and current artificial\nintelligence systems. This paper introduces Bayesian Epistemology with Weighted\nAuthority (BEWA), a formally structured architecture that operationalises\nbelief as a dynamic, probabilistically coherent function over structured\nscientific claims. Each claim is contextualised, author-attributed, and\nevaluated through a system of replication scores, citation weighting, and\ntemporal decay. Belief updates are performed via evidence-conditioned Bayesian\ninference, contradiction processing, and epistemic decay mechanisms. The\narchitecture supports graph-based claim propagation, authorial credibility\nmodelling, cryptographic anchoring, and zero-knowledge audit verification. By\nformalising scientific reasoning into a computationally verifiable epistemic\nnetwork, BEWA advances the foundation for machine reasoning systems that\npromote truth utility, rational belief convergence, and audit-resilient\nintegrity across dynamic scientific domains."}
{"id": "2506.16078", "pdf": "https://arxiv.org/pdf/2506.16078.pdf", "abs": "https://arxiv.org/abs/2506.16078", "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations", "authors": ["Tianle Gu", "Kexin Huang", "Zongqi Wang", "Yixu Wang", "Jie Li", "Yuanqi Yao", "Yang Yao", "Yujiu Yang", "Yan Teng", "Yingchun Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Safety alignment is a key requirement for building reliable Artificial\nGeneral Intelligence. Despite significant advances in safety alignment, we\nobserve that minor latent shifts can still trigger unsafe responses in aligned\nmodels. We argue that this stems from the shallow nature of existing alignment\nmethods, which focus on surface-level refusal behaviors without sufficiently\naltering internal representations. Consequently, small shifts in hidden\nactivations can re-trigger harmful behaviors embedded in the latent space. To\nexplore the robustness of safety alignment to latent perturbations, we\nintroduce a probing method that measures the Negative Log-Likelihood of the\noriginal response generated by the model. This probe quantifies local\nsensitivity in the latent space, serving as a diagnostic tool for identifying\nvulnerable directions. Based on this signal, we construct effective jailbreak\ntrajectories, giving rise to the Activation Steering Attack (ASA). More\nimportantly, these insights offer a principled foundation for improving\nalignment robustness. To this end, we introduce Layer-wise Adversarial Patch\nTraining~(LAPT), a fine-tuning strategy that inject controlled perturbations\ninto hidden representations during training. Experimental results highlight\nthat LAPT strengthen alignment robustness without compromising general\ncapabilities. Our findings reveal fundamental flaws in current alignment\nparadigms and call for representation-level training strategies that move\nbeyond surface-level behavior supervision. Codes and results are available at\nhttps://github.com/Carol-gutianle/LatentSafety."}
{"id": "2506.16141", "pdf": "https://arxiv.org/pdf/2506.16141.pdf", "abs": "https://arxiv.org/abs/2506.16141", "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Junhao Cheng", "Ying Shan", "Xihui Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code released at: https://github.com/TencentARC/GRPO-CARE", "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs."}
{"id": "2506.16402", "pdf": "https://arxiv.org/pdf/2506.16402.pdf", "abs": "https://arxiv.org/abs/2506.16402", "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks", "authors": ["Xiaoya Lu", "Zeren Chen", "Xuhao Hu", "Yijin Zhou", "Weichen Zhang", "Dongrui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Flawed planning from VLM-driven embodied agents poses significant safety\nhazards, hindering their deployment in real-world household tasks. However,\nexisting static, non-interactive evaluation paradigms fail to adequately assess\nrisks within these interactive environments, since they cannot simulate dynamic\nrisks that emerge from an agent's actions and rely on unreliable post-hoc\nevaluations that ignore unsafe intermediate steps. To bridge this critical gap,\nwe propose evaluating an agent's interactive safety: its ability to perceive\nemergent risks and execute mitigation steps in the correct procedural order. We\nthus present IS-Bench, the first multi-modal benchmark designed for interactive\nsafety, featuring 161 challenging scenarios with 388 unique safety risks\ninstantiated in a high-fidelity simulator. Crucially, it facilitates a novel\nprocess-oriented evaluation that verifies whether risk mitigation actions are\nperformed before/after specific risk-prone steps. Extensive experiments on\nleading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current\nagents lack interactive safety awareness, and that while safety-aware\nChain-of-Thought can improve performance, it often compromises task completion.\nBy highlighting these critical limitations, IS-Bench provides a foundation for\ndeveloping safer and more reliable embodied AI systems."}
{"id": "2506.16412", "pdf": "https://arxiv.org/pdf/2506.16412.pdf", "abs": "https://arxiv.org/abs/2506.16412", "title": "Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse", "authors": ["Paulina DeVito", "Akhil Vallala", "Sean Mcmahon", "Yaroslav Hinda", "Benjamin Thaw", "Hanqi Zhuang", "Hari Kalva"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": "This work has been submitted to IEEE Transactions on Computational\n  Social Systems for possible publication", "summary": "Generative AI (GAI) technologies are quickly reshaping the educational\nlandscape. As adoption accelerates, understanding how students and educators\nperceive these tools is essential. This study presents one of the most\ncomprehensive analyses to date of stakeholder discourse dynamics on GAI in\neducation using social media data. Our dataset includes 1,199 Reddit posts and\n13,959 corresponding top-level comments. We apply sentiment analysis, topic\nmodeling, and author classification. To support this, we propose and validate a\nmodular framework that leverages prompt-based large language models (LLMs) for\nanalysis of online social discourse, and we evaluate this framework against\nclassical natural language processing (NLP) models. Our GPT-4o pipeline\nconsistently outperforms prior approaches across all tasks. For example, it\nachieved 90.6% accuracy in sentiment analysis against gold-standard human\nannotations. Topic extraction uncovered 12 latent topics in the public\ndiscourse with varying sentiment and author distributions. Teachers and\nstudents convey optimism about GAI's potential for personalized learning and\nproductivity in higher education. However, key differences emerged: students\noften voice distress over false accusations of cheating by AI detectors, while\nteachers generally express concern about job security, academic integrity, and\ninstitutional pressures to adopt GAI tools. These contrasting perspectives\nhighlight the tension between innovation and oversight in GAI-enabled learning\nenvironments. Our findings suggest a need for clearer institutional policies,\nmore transparent GAI integration practices, and support mechanisms for both\neducators and students. More broadly, this study demonstrates the potential of\nLLM-based frameworks for modeling stakeholder discourse within online\ncommunities."}
{"id": "2506.16447", "pdf": "https://arxiv.org/pdf/2506.16447.pdf", "abs": "https://arxiv.org/abs/2506.16447", "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models", "authors": ["Biao Yi", "Tiansheng Huang", "Sishuo Chen", "Tong Li", "Zheli Liu", "Zhixuan Chu", "Yiming Li"], "categories": ["cs.CR", "cs.CL"], "comment": "Accepted at ICLR 2025", "summary": "Backdoor unalignment attacks against Large Language Models (LLMs) enable the\nstealthy compromise of safety alignment using a hidden trigger while evading\nnormal safety auditing. These attacks pose significant threats to the\napplications of LLMs in the real-world Large Language Model as a Service\n(LLMaaS) setting, where the deployed model is a fully black-box system that can\nonly interact through text. Furthermore, the sample-dependent nature of the\nattack target exacerbates the threat. Instead of outputting a fixed label, the\nbackdoored LLM follows the semantics of any malicious command with the hidden\ntrigger, significantly expanding the target space. In this paper, we introduce\nBEAT, a black-box defense that detects triggered samples during inference to\ndeactivate the backdoor. It is motivated by an intriguing observation (dubbed\nthe probe concatenate effect), where concatenated triggered samples\nsignificantly reduce the refusal rate of the backdoored LLM towards a malicious\nprobe, while non-triggered samples have little effect. Specifically, BEAT\nidentifies whether an input is triggered by measuring the degree of distortion\nin the output distribution of the probe before and after concatenation with the\ninput. Our method addresses the challenges of sample-dependent targets from an\nopposite perspective. It captures the impact of the trigger on the refusal\nsignal (which is sample-independent) instead of sample-specific successful\nattack behaviors. It overcomes black-box access limitations by using multiple\nsampling to approximate the output distribution. Extensive experiments are\nconducted on various backdoor attacks and LLMs (including the closed-source\nGPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.\nBesides, we also preliminarily verify that BEAT can effectively defend against\npopular jailbreak attacks, as they can be regarded as 'natural backdoors'."}
{"id": "2506.16473", "pdf": "https://arxiv.org/pdf/2506.16473.pdf", "abs": "https://arxiv.org/abs/2506.16473", "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions."}
{"id": "2506.16552", "pdf": "https://arxiv.org/pdf/2506.16552.pdf", "abs": "https://arxiv.org/abs/2506.16552", "title": "Revela: Dense Retriever Learning via Language Modeling", "authors": ["Fengyu Cai", "Tong Chen", "Xinran Zhao", "Sihao Chen", "Hongming Zhang", "Sherry Tongshuang Wu", "Iryna Gurevych", "Heinz Koeppl"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Dense retrievers play a vital role in accessing external and specialized\nknowledge to augment language models (LMs). Training dense retrievers typically\nrequires annotated query-document pairs, which are costly and hard to obtain in\nspecialized domains such as code-motivating growing interest in self-supervised\nretriever learning. Since LMs are trained to capture token-level dependencies\nthrough a self-supervised learning objective (i.e., next-token prediction), we\ncan analogously cast retrieval as learning dependencies among chunks of tokens.\nThis analogy naturally leads to the question: How can we adapt self-supervised\nlearning objectives in the spirit of language modeling to train retrievers?\n  To answer this question, we introduce Revela, a unified and scalable training\nframework for self-supervised retriever learning via language modeling. Revela\nmodels semantic dependencies among documents by conditioning next-token\nprediction on both local and cross-document context through an in-batch\nattention mechanism. This attention is weighted by retriever-computed\nsimilarity scores, enabling the retriever to be optimized as part of language\nmodeling. We evaluate Revela on both general-domain (BEIR) and domain-specific\n(CoIR) benchmarks across various retriever backbones. At a comparable parameter\nscale, Revela outperforms the previous best method with absolute improvements\nof 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,\nrespectively, underscoring its effectiveness. Performance increases with model\nsize, highlighting both the scalability of our approach and its promise for\nself-supervised retriever learning."}
{"id": "2506.16575", "pdf": "https://arxiv.org/pdf/2506.16575.pdf", "abs": "https://arxiv.org/abs/2506.16575", "title": "Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System", "authors": ["Mustafa Akben", "Aaron Satko"], "categories": ["cs.AI", "cs.CL"], "comment": "Submitted for HICSS 2025 (Hawaii International Conference on System\n  Sciences); under review", "summary": "Large language models (LLMs) offer promising opportunities for organizational\nresearch. However, their built-in moderation systems can create problems when\nresearchers try to analyze harmful content, often refusing to follow certain\ninstructions or producing overly cautious responses that undermine validity of\nthe results. This is particularly problematic when analyzing organizational\nconflicts such as microaggressions or hate speech. This paper introduces an Elo\nrating-based method that significantly improves LLM performance for harmful\ncontent analysis In two datasets, one focused on microaggression detection and\nthe other on hate speech, we find that our method outperforms traditional LLM\nprompting techniques and conventional machine learning models on key measures\nsuch as accuracy, precision, and F1 scores. Advantages include better\nreliability when analyzing harmful content, fewer false positives, and greater\nscalability for large-scale datasets. This approach supports organizational\napplications, including detecting workplace harassment, assessing toxic\ncommunication, and fostering safer and more inclusive work environments."}
{"id": "2506.16697", "pdf": "https://arxiv.org/pdf/2506.16697.pdf", "abs": "https://arxiv.org/abs/2506.16697", "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools."}
{"id": "2506.16702", "pdf": "https://arxiv.org/pdf/2506.16702.pdf", "abs": "https://arxiv.org/abs/2506.16702", "title": "Large Language Models as Psychological Simulators: A Methodological Guide", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research."}
{"id": "2506.16962", "pdf": "https://arxiv.org/pdf/2506.16962.pdf", "abs": "https://arxiv.org/abs/2506.16962", "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs", "authors": ["Haoran Sun", "Yankai Jiang", "Wenjie Lou", "Yujie Zhang", "Wenjie Li", "Lilong Wang", "Mianxin Liu", "Lei Liu", "Xiaosong Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs"}
{"id": "2506.16975", "pdf": "https://arxiv.org/pdf/2506.16975.pdf", "abs": "https://arxiv.org/abs/2506.16975", "title": "Latent Concept Disentanglement in Transformer-based Language Models", "authors": ["Guan Zhe Hong", "Bhavya Vasudeva", "Vatsal Sharan", "Cyrus Rashtchian", "Prabhakar Raghavan", "Rina Panigrahy"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks."}
{"id": "2506.17052", "pdf": "https://arxiv.org/pdf/2506.17052.pdf", "abs": "https://arxiv.org/abs/2506.17052", "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers", "authors": ["Jingtong Su", "Julia Kempe", "Karen Ullrich"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet."}
{"id": "2506.17111", "pdf": "https://arxiv.org/pdf/2506.17111.pdf", "abs": "https://arxiv.org/abs/2506.17111", "title": "Are Bias Evaluation Methods Biased ?", "authors": ["Lina Berrayana", "Sean Rooney", "Luis Garc√©s-Erice", "Ioana Giurgiu"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Workshop GEM", "summary": "The creation of benchmarks to evaluate the safety of Large Language Models is\none of the key activities within the trusted AI community. These benchmarks\nallow models to be compared for different aspects of safety such as toxicity,\nbias, harmful behavior etc. Independent benchmarks adopt different approaches\nwith distinct data sets and evaluation methods. We investigate how robust such\nbenchmarks are by using different approaches to rank a set of representative\nmodels for bias and compare how similar are the overall rankings. We show that\ndifferent but widely used bias evaluations methods result in disparate model\nrankings. We conclude with recommendations for the community in the usage of\nsuch benchmarks."}
{"id": "2506.17113", "pdf": "https://arxiv.org/pdf/2506.17113.pdf", "abs": "https://arxiv.org/abs/2506.17113", "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation", "authors": ["Shoubin Yu", "Yue Zhang", "Ziyang Wang", "Jaehong Yoon", "Mohit Bansal"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "The first two authors contributed equally; Github link:\n  https://github.com/Yui010206/MEXA", "summary": "Combining pre-trained expert models offers substantial potential for scalable\nmultimodal reasoning, but building a unified framework remains challenging due\nto the increasing diversity of input modalities and task complexity. For\ninstance, medical diagnosis requires precise reasoning over structured clinical\ntables, while financial forecasting depends on interpreting plot-based data to\nmake informed predictions. To tackle this challenge, we introduce MEXA, a\ntraining-free framework that performs modality- and task-aware aggregation of\nmultiple expert models to enable effective multimodal reasoning across diverse\nand distinct domains. MEXA dynamically selects expert models based on the input\nmodality and the task-specific reasoning demands (i.e., skills). Each expert\nmodel, specialized in a modality task pair, generates interpretable textual\nreasoning outputs. MEXA then aggregates and reasons over these outputs using a\nLarge Reasoning Model (LRM) to produce the final answer. This modular design\nallows flexible and transparent multimodal reasoning across diverse domains\nwithout additional training overhead. We extensively evaluate our approach on\ndiverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D\nUnderstanding, and Medical QA. MEXA consistently delivers performance\nimprovements over strong multimodal baselines, highlighting the effectiveness\nand broad applicability of our expert-driven selection and aggregation in\ndiverse multimodal reasoning tasks."}
{"id": "2506.17208", "pdf": "https://arxiv.org/pdf/2506.17208.pdf", "abs": "https://arxiv.org/abs/2506.17208", "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems", "authors": ["Matias Martinez", "Xavier Franch"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies."}
{"id": "2305.14597", "pdf": "https://arxiv.org/pdf/2305.14597.pdf", "abs": "https://arxiv.org/abs/2305.14597", "title": "Voices of Her: Analyzing Gender Differences in the AI Publication World", "authors": ["Yiwen Ding", "Jiarui Liu", "Zhiheng Lyu", "Kun Zhang", "Bernhard Schoelkopf", "Zhijing Jin", "Rada Mihalcea"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While several previous studies have analyzed gender bias in research, we are\nstill missing a comprehensive analysis of gender differences in the AI\ncommunity, covering diverse topics and different development trends. Using the\nAI Scholar dataset of 78K researchers in the field of AI, we identify several\ngender differences: (1) Although female researchers tend to have fewer overall\ncitations than males, this citation difference does not hold for all\nacademic-age groups; (2) There exist large gender homophily in co-authorship on\nAI papers; (3) Female first-authored papers show distinct linguistic styles,\nsuch as longer text, more positive emotion words, and more catchy titles than\nmale first-authored papers. Our analysis provides a window into the current\ndemographic trends in our AI community, and encourages more gender equality and\ndiversity in the future. Our code and data are at\nhttps://github.com/causalNLP/ai-scholar-gender."}
{"id": "2312.04684", "pdf": "https://arxiv.org/pdf/2312.04684.pdf", "abs": "https://arxiv.org/abs/2312.04684", "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "authors": ["Zifan Xu", "Haozhu Wang", "Dmitriy Bespalov", "Xian Wu", "Peter Stone", "Yanjun Qi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks."}
{"id": "2402.09404", "pdf": "https://arxiv.org/pdf/2402.09404.pdf", "abs": "https://arxiv.org/abs/2402.09404", "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability", "authors": ["Siwei Yang", "Bingchen Zhao", "Cihang Xie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench."}
{"id": "2404.12041", "pdf": "https://arxiv.org/pdf/2404.12041.pdf", "abs": "https://arxiv.org/abs/2404.12041", "title": "A Survey of Automatic Hallucination Evaluation on Natural Language Generation", "authors": ["Siya Qi", "Lin Gui", "Yulan He", "Zheng Yuan"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages", "summary": "The proliferation of Large Language Models (LLMs) has introduced a critical\nchallenge: accurate hallucination evaluation that ensures model reliability.\nWhile Automatic Hallucination Evaluation (AHE) has emerged as essential, the\nfield suffers from methodological fragmentation, hindering both theoretical\nunderstanding and practical advancement. This survey addresses this critical\ngap through a comprehensive analysis of 74 evaluation methods, revealing that\n74% specifically target LLMs, a paradigm shift that demands new evaluation\nframeworks. We formulate a unified evaluation pipeline encompassing datasets\nand benchmarks, evidence collection strategies, and comparison mechanisms,\nsystematically documenting the evolution from pre-LLM to post-LLM\nmethodologies. Beyond taxonomical organization, we identify fundamental\nlimitations in current approaches and their implications for real-world\ndeployment. To guide future research, we delineate key challenges and propose\nstrategic directions, including enhanced interpretability mechanisms and\nintegration of application-specific evaluation criteria, ultimately providing a\nroadmap for developing more robust and practical hallucination evaluation\nsystems."}
{"id": "2406.04220", "pdf": "https://arxiv.org/pdf/2406.04220.pdf", "abs": "https://arxiv.org/abs/2406.04220", "title": "BEADs: Bias Evaluation Across Domains", "authors": ["Shaina Raza", "Mizanur Rahman", "Michael R. Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved natural language processing (NLP) applications. However, these models\noften inherit biases from their training data. While several datasets exist for\nbias detection, most are limited to one or two NLP tasks, typically\nclassification or evaluation, and lack comprehensive coverage across a broader\nrange of tasks. To address this gap, we introduce the Bias Evaluations Across\nDomains (BEADs) dataset, designed to support a wide range of NLP tasks,\nincluding text classification, token classification, bias quantification, and\nbenign language generation. A key contribution of this work is the\ngold-standard annotation provided by GPT-4 for scalability, with expert\nverification to ensure high reliability. BEADs can be used for both fine-tuning\nmodels (for classification and generation tasks) and evaluating LLM behavior.\nOur findings show that BEADs effectively surfaces various biases during model\nfine-tuning and helps reduce biases in language generation tasks while\nmaintaining output quality. The dataset also highlights prevalent demographic\nbiases in LLMs during evaluation. We release BEADs as a practical resource for\ndetecting and mitigating bias across domains, supporting the development of\nresponsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data:\nhttps://huggingface.co/datasets/shainar/BEAD"}
{"id": "2407.02397", "pdf": "https://arxiv.org/pdf/2407.02397.pdf", "abs": "https://arxiv.org/abs/2407.02397", "title": "Learning to Refine with Fine-Grained Natural Language Feedback", "authors": ["Manya Wadhwa", "Xinyu Zhao", "Junyi Jessy Li", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Code and models available at: https://github.com/ManyaWadhwa/DCR;\n  Findings of EMNLP 2024", "summary": "Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1) detection of\nbad generations; (2) fine-grained natural language critique generation; (3)\nrefining with fine-grained feedback. The first step can be implemented with a\nhigh-performing discriminative model and steps 2 and 3 can be implemented\neither via prompted or fine-tuned LLMs. A key property of the proposed Detect,\nCritique, Refine (\"DCR\") method is that the step 2 critique model can give\nfine-grained feedback about errors, made possible by offloading the\ndiscrimination to a separate model in step 1. We show that models of different\ncapabilities benefit from refining with DCR on the task of improving factual\nconsistency of document grounded summaries. Overall, DCR consistently\noutperforms existing end-to-end refinement approaches and current trained\nmodels not fine-tuned for factuality critiquing."}
{"id": "2407.09879", "pdf": "https://arxiv.org/pdf/2407.09879.pdf", "abs": "https://arxiv.org/abs/2407.09879", "title": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting", "authors": ["Sanchit Ahuja", "Kumar Tanmay", "Hardik Hansrajbhai Chauhan", "Barun Patra", "Kriti Aggarwal", "Luciano Del Corro", "Arindam Mitra", "Tejas Indulal Dhamecha", "Ahmed Awadallah", "Monojit Choudhary", "Vishrav Chaudhary", "Sunayana Sitaram"], "categories": ["cs.CL"], "comment": "20 pages, 12 tables, 5 figures", "summary": "Despite the remarkable success of large language models (LLMs) in English, a\nsignificant performance gap remains in non-English languages. To address this,\nwe introduce a novel approach for strategically constructing a multilingual\nsynthetic instruction tuning dataset, sPhinX. Unlike prior methods that\ndirectly translate fixed instruction-response pairs, sPhinX enhances diversity\nby selectively augmenting English instruction-response pairs with multilingual\ntranslations. Additionally, we propose LANGIT, a novel N-shot guided\nfine-tuning strategy, which further enhances model performance by incorporating\ncontextually relevant examples in each training sample. Our ablation study\nshows that our approach enhances the multilingual capabilities of Mistral-7B\nand Phi-3-Small improving performance by an average of 39.8% and 11.2%,\nrespectively, across multilingual benchmarks in reasoning, question answering,\nreading comprehension, and machine translation. Moreover, sPhinX maintains\nstrong performance on English LLM benchmarks while exhibiting minimal to no\ncatastrophic forgetting, even when trained on 51 languages."}
{"id": "2407.14701", "pdf": "https://arxiv.org/pdf/2407.14701.pdf", "abs": "https://arxiv.org/abs/2407.14701", "title": "Contextual modulation of language comprehension in a dynamic neural model of lexical meaning", "authors": ["Michael C. Stern", "Maria M. Pi√±ango"], "categories": ["cs.CL"], "comment": null, "summary": "We propose and computationally implement a dynamic neural model of lexical\nmeaning, and experimentally test its behavioral predictions. We demonstrate the\narchitecture and behavior of the model using as a test case the English lexical\nitem 'have', focusing on its polysemous use. In the model, 'have' maps to a\nsemantic space defined by two continuous conceptual dimensions, connectedness\nand control asymmetry, previously proposed to parameterize the conceptual\nsystem for language. The mapping is modeled as coupling between a neural node\nrepresenting the lexical item and neural fields representing the conceptual\ndimensions. While lexical knowledge is modeled as a stable coupling pattern,\nreal-time lexical meaning retrieval is modeled as the motion of neural\nactivation patterns between metastable states corresponding to semantic\ninterpretations or readings. Model simulations capture two previously reported\nempirical observations: (1) contextual modulation of lexical semantic\ninterpretation, and (2) individual variation in the magnitude of this\nmodulation. Simulations also generate a novel prediction that the by-trial\nrelationship between sentence reading time and acceptability should be\ncontextually modulated. An experiment combining self-paced reading and\nacceptability judgments replicates previous results and confirms the new model\nprediction. Altogether, results support a novel perspective on lexical\npolysemy: that the many related meanings of a word are metastable neural\nactivation states that arise from the nonlinear dynamics of neural populations\ngoverning interpretation on continuous semantic dimensions."}
{"id": "2408.01287", "pdf": "https://arxiv.org/pdf/2408.01287.pdf", "abs": "https://arxiv.org/abs/2408.01287", "title": "Deep Learning based Visually Rich Document Content Understanding: A Survey", "authors": ["Yihao Ding", "Soyeon Caren Han", "Jean Lee", "Eduard Hovy"], "categories": ["cs.CL", "cs.CV"], "comment": "Work in Progress", "summary": "Visually Rich Documents (VRDs) play a vital role in domains such as academia,\nfinance, healthcare, and marketing, as they convey information through a\ncombination of text, layout, and visual elements. Traditional approaches to\nextracting information from VRDs rely heavily on expert knowledge and manual\nannotation, making them labor-intensive and inefficient. Recent advances in\ndeep learning have transformed this landscape by enabling multimodal models\nthat integrate vision, language, and layout features through pretraining,\nsignificantly improving information extraction performance. This survey\npresents a comprehensive overview of deep learning-based frameworks for VRD\nContent Understanding (VRD-CU). We categorize existing methods based on their\nmodeling strategies and downstream tasks, and provide a comparative analysis of\nkey components, including feature representation, fusion techniques, model\narchitectures, and pretraining objectives. Additionally, we highlight the\nstrengths and limitations of each approach and discuss their suitability for\ndifferent applications. The paper concludes with a discussion of current\nchallenges and emerging trends, offering guidance for future research and\npractical deployment in real-world scenarios."}
{"id": "2408.06904", "pdf": "https://arxiv.org/pdf/2408.06904.pdf", "abs": "https://arxiv.org/abs/2408.06904", "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives", "authors": ["Zhihu Wang", "Shiwan Zhao", "Yu Wang", "Heyuan Huang", "Sitao Xie", "Yubo Zhang", "Jiaxin Shi", "Zhixing Wang", "Hongyan Li", "Junchi Yan"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; First three authors contributed equally", "summary": "The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving\ncomplex problems with large language models (LLMs). However, its application to\ndomain-specific tasks remains challenging, as LLMs often fail to decompose\ntasks accurately or execute subtasks effectively. This paper introduces the\nRe-TASK framework, a novel theoretical model that revisits LLM tasks from\ncapability, skill, and knowledge perspectives, drawing on the principles of\nBloom's Taxonomy and Knowledge Space Theory. While CoT provides a\nworkflow-centric perspective on tasks, Re-TASK introduces a Chain-of-Learning\n(CoL) paradigm that highlights task dependencies on specific capability items,\nfurther broken down into their constituent knowledge and skill components. To\naddress CoT failures, we propose a Re-TASK prompting strategy, which\nstrengthens task-relevant capabilities through targeted knowledge injection and\nskill adaptation. Experiments across diverse domains demonstrate the\neffectiveness of Re-TASK. In particular, we achieve improvements of 45.00% on\nYi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks. These results\nhighlight the potential of Re-TASK to significantly enhance LLM performance and\nits applicability in specialized domains. We release our code and data at\nhttps://github.com/Uylee/Re-TASK."}
{"id": "2408.14352", "pdf": "https://arxiv.org/pdf/2408.14352.pdf", "abs": "https://arxiv.org/abs/2408.14352", "title": "LogProber: Disentangling confidence from contamination in LLM responses", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical. In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."}
{"id": "2409.00128", "pdf": "https://arxiv.org/pdf/2409.00128.pdf", "abs": "https://arxiv.org/abs/2409.00128", "title": "Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management", "authors": ["Ziyan Cui", "Ning Li", "Huaikang Zhou"], "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "comment": "5 figures, 2 tables", "summary": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions."}
{"id": "2410.10855", "pdf": "https://arxiv.org/pdf/2410.10855.pdf", "abs": "https://arxiv.org/abs/2410.10855", "title": "Core Knowledge Deficits in Multi-Modal Language Models", "authors": ["Yijiang Li", "Qingying Gao", "Tianwei Zhao", "Bingyang Wang", "Haoran Sun", "Haiyun Lyu", "Robert D. Hawkins", "Nuno Vasconcelos", "Tal Golan", "Dezhi Luo", "Hokin Deng"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025. Project page at\n  https://williamium3000.github.io/core-knowledge and code is available at\n  https://github.com/williamium3000/core-knowledge", "summary": "While Multi-modal Large Language Models (MLLMs) demonstrate impressive\nabilities over high-level perception and reasoning, their robustness in the\nwild remains limited, often falling short on tasks that are intuitive and\neffortless for humans. We examine the hypothesis that these deficiencies stem\nfrom the absence of core knowledge--rudimentary cognitive abilities innate to\nhumans from early childhood. To explore the core knowledge representation in\nMLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core\nknowledge concepts grounded in developmental cognitive science. We evaluate 230\nmodels with 11 different prompts, leading to a total of 2,530 data points for\nanalysis. Our experiments uncover four key findings, collectively demonstrating\ncore knowledge deficits in MLLMs: they consistently underperform and show\nreduced, or even absent, scalability on low-level abilities relative to\nhigh-level ones. Finally, we propose Concept Hacking, a novel controlled\nevaluation method that reveals MLLMs fail to progress toward genuine core\nknowledge understanding, but instead rely on shortcut learning as they scale."}
{"id": "2410.11331", "pdf": "https://arxiv.org/pdf/2410.11331.pdf", "abs": "https://arxiv.org/abs/2410.11331", "title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments", "authors": ["Syed Abdul Gaffar Shakhadri", "Kruthika KR", "Rakshit Aralimatti"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "Paper in pdf format is 11 pages and contains 4 tables", "summary": "We introduce Shakti, a 2.5 billion parameter language model specifically\noptimized for resource-constrained environments such as edge devices, including\nsmartphones, wearables, and IoT systems. Shakti combines high-performance NLP\nwith optimized efficiency and precision, making it ideal for real-time AI\napplications where computational resources and memory are limited. With support\nfor vernacular languages and domain-specific tasks, Shakti excels in industries\nsuch as healthcare, finance, and customer service. Benchmark evaluations\ndemonstrate that Shakti performs competitively against larger models while\nmaintaining low latency and on-device efficiency, positioning it as a leading\nsolution for edge AI."}
{"id": "2410.13284", "pdf": "https://arxiv.org/pdf/2410.13284.pdf", "abs": "https://arxiv.org/abs/2410.13284", "title": "Learning to Route LLMs with Confidence Tokens", "authors": ["Yu-Neng Chuang", "Prathusha Kameswara Sarma", "Parikshit Gopalan", "John Boccio", "Sara Bolouki", "Xia Hu", "Helen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nseveral tasks and are increasingly deployed in real-world applications.\nHowever, especially in high-stakes settings, it becomes vital to know when the\noutput of an LLM may be unreliable. Depending on whether an answer is\ntrustworthy, a system can then choose to route the question to another expert,\nor otherwise fall back on a safe default behavior. In this work, we study the\nextent to which LLMs can reliably indicate confidence in their answers, and how\nthis notion of confidence can translate into downstream accuracy gains. We\npropose Self-Reflection with Error-based Feedback (Self-REF), a lightweight\ntraining strategy to teach LLMs to express confidence in whether their answers\nare correct in a reliable manner. Self-REF introduces confidence tokens into\nthe LLM, from which a confidence score can be extracted. Compared to\nconventional approaches such as verbalizing confidence and examining token\nprobabilities, we demonstrate empirically that confidence tokens show\nsignificant improvements in downstream routing and rejection learning tasks."}
{"id": "2410.15865", "pdf": "https://arxiv.org/pdf/2410.15865.pdf", "abs": "https://arxiv.org/abs/2410.15865", "title": "Principles of semantic and functional efficiency in grammatical patterning", "authors": ["Emily Cheng", "Francesca Franzon"], "categories": ["cs.CL"], "comment": null, "summary": "Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation-a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints, accounting for variable communicative need. Our analyses reveal\nthat grammatical organization provably inherits from perceptual attributes, and\nour measurements on a diverse language sample show that grammars prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding."}
{"id": "2411.04291", "pdf": "https://arxiv.org/pdf/2411.04291.pdf", "abs": "https://arxiv.org/abs/2411.04291", "title": "Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models", "authors": ["Saketh Bachu", "Erfan Shayegani", "Rohit Lal", "Trishna Chakraborty", "Arindam Dutta", "Chengyu Song", "Yue Dong", "Nael Abu-Ghazaleh", "Amit K. Roy-Chowdhury"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ICML 2025 as a spotlight poster", "summary": "Vision-language models (VLMs) have improved significantly in their\ncapabilities, but their complex architecture makes their safety alignment\nchallenging. In this paper, we reveal an uneven distribution of harmful\ninformation across the intermediate layers of the image encoder and show that\nskipping a certain set of layers and exiting early can increase the chance of\nthe VLM generating harmful responses. We call it as \"Image enCoder Early-exiT\"\nbased vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5,\nLLaVA-NeXT, and Llama 3.2, show that performing early exits from the image\nencoder significantly increases the likelihood of generating harmful outputs.\nTo tackle this, we propose a simple yet effective modification of the\nClipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing\nlayer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO).\nWe evaluate our L-PPO algorithm across three multimodal datasets and show that\nit consistently reduces the harmfulness caused by early exits."}
{"id": "2411.13100", "pdf": "https://arxiv.org/pdf/2411.13100.pdf", "abs": "https://arxiv.org/abs/2411.13100", "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control", "authors": ["Yunkee Chae", "Eunsik Shin", "Suntae Hwang", "Seungryeol Paik", "Kyogu Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999"}
{"id": "2411.16813", "pdf": "https://arxiv.org/pdf/2411.16813.pdf", "abs": "https://arxiv.org/abs/2411.16813", "title": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation", "authors": ["Svetlana Churina", "Kokil Jaidka"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support."}
{"id": "2411.19930", "pdf": "https://arxiv.org/pdf/2411.19930.pdf", "abs": "https://arxiv.org/abs/2411.19930", "title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models", "authors": ["Daixuan Cheng", "Shaohan Huang", "Ziyu Zhu", "Xintong Zhang", "Wayne Xin Zhao", "Zhongzhi Luan", "Bo Dai", "Zhenliang Zhang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://huggingface.co/AdaptLLM", "summary": "Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs via post-training, focusing on data synthesis,\ntraining pipeline, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models in enhancing domain-specific performance. (2) Training\nPipeline: Unlike general MLLMs that typically adopt a two-stage training\nparadigm, we find that a single-stage approach is more effective for domain\nadaptation. (3) Task Evaluation: We conduct extensive experiments in\nhigh-impact domains such as biomedicine, food, and remote sensing, by\npost-training a variety of MLLMs and then evaluating MLLM performance on\nvarious domain-specific tasks. Finally, we fully open-source our models, code,\nand data to encourage future research in this area."}
{"id": "2412.14860", "pdf": "https://arxiv.org/pdf/2412.14860.pdf", "abs": "https://arxiv.org/abs/2412.14860", "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling", "authors": ["Junyi Li", "Hwee Tou Ng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches."}
{"id": "2502.12685", "pdf": "https://arxiv.org/pdf/2502.12685.pdf", "abs": "https://arxiv.org/abs/2502.12685", "title": "Theoretical Guarantees for Minimum Bayes Risk Decoding", "authors": ["Yuki Ichihara", "Yuu Jinnai", "Kaito Ariu", "Tetsuro Morimura", "Eiji Uchibe"], "categories": ["cs.CL"], "comment": null, "summary": "Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing\nthe expected utility value of an underlying human distribution. While prior\nwork has shown the effectiveness of MBR decoding through empirical evaluation,\nfew studies have analytically investigated why the method is effective. As a\nresult of our analysis, we show that, given the size $n$ of the reference\nhypothesis set used in computation, MBR decoding approaches the optimal\nsolution with high probability at a rate of $O\\left(n^{-\\frac{1}{2}}\\right)$,\nunder certain assumptions, even though the language space $Y$ is significantly\nlarger $|Y|\\gg n$. This result helps to theoretically explain the strong\nperformance observed in several prior empirical studies on MBR decoding. In\naddition, we provide the performance gap for maximum-a-posteriori (MAP)\ndecoding and compare it to MBR decoding. The result of this paper indicates\nthat MBR decoding tends to converge to the optimal solution faster than MAP\ndecoding in several cases."}
{"id": "2502.12911", "pdf": "https://arxiv.org/pdf/2502.12911.pdf", "abs": "https://arxiv.org/abs/2502.12911", "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation", "authors": ["Zheng Yuan", "Hao Chen", "Zijin Hong", "Qinggang Zhang", "Feiran Huang", "Qing Li", "Xiao Huang"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes."}
{"id": "2502.14709", "pdf": "https://arxiv.org/pdf/2502.14709.pdf", "abs": "https://arxiv.org/abs/2502.14709", "title": "Group-Level Data Selection for Efficient Pretraining", "authors": ["Zichun Yu", "Fei Peng", "Jie Lei", "Arnold Overwijk", "Wen-tau Yih", "Chenyan Xiong"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we introduce Group-MATES, an efficient group-level data\nselection approach to optimize the speed-quality frontier of language model\npretraining. Specifically, Group-MATES parameterizes costly group-level\nselection with a relational data influence model. To train this model, we\nsample training trajectories of the language model and collect oracle data\ninfluences alongside. The relational data influence model approximates the\noracle data influence by weighting individual influence with relationships\namong training data. To enable efficient selection with our relational data\ninfluence model, we partition the dataset into small clusters using\nrelationship weights and select data within each cluster independently.\nExperiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves\n3.5%-9.4% relative performance gains over random selection across 22 downstream\ntasks, nearly doubling the improvements achieved by state-of-the-art individual\ndata selection baselines. Furthermore, Group-MATES reduces the number of tokens\nrequired to reach a certain downstream performance by up to 1.75x,\nsubstantially elevating the speed-quality frontier. Further analyses highlight\nthe critical role of relationship weights in the relational data influence\nmodel and the effectiveness of our cluster-based inference. Our code is\nopen-sourced at https://github.com/facebookresearch/Group-MATES."}
{"id": "2502.14802", "pdf": "https://arxiv.org/pdf/2502.14802.pdf", "abs": "https://arxiv.org/abs/2502.14802", "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models", "authors": ["Bernal Jim√©nez Guti√©rrez", "Yiheng Shu", "Weijian Qi", "Sizhe Zhou", "Yu Su"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025. Code and data are available at:\n  https://github.com/OSU-NLP-Group/HippoRAG", "summary": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG."}
{"id": "2502.14911", "pdf": "https://arxiv.org/pdf/2502.14911.pdf", "abs": "https://arxiv.org/abs/2502.14911", "title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models", "authors": ["Jann Railey Montalan", "Jimson Paulo Layacan", "David Demitri Africa", "Richell Isaiah Flores", "Michael T. Lopez II", "Theresa Denise Magsajo", "Anjanette Cayabyab", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities on widely benchmarked high-resource languages. However, linguistic\nnuances of under-resourced languages remain unexplored. We introduce Batayan, a\nholistic Filipino benchmark that systematically evaluates LLMs across three key\nnatural language processing (NLP) competencies: understanding, reasoning, and\ngeneration. Batayan consolidates eight tasks, three of which have not existed\nprior for Filipino corpora, covering both Tagalog and code-switched Taglish\nutterances. Our rigorous, native-speaker-driven adaptation and validation\nprocesses ensures fluency and authenticity to the complex morphological and\nsyntactic structures of Filipino, alleviating the pervasive translationese bias\nin existing Filipino corpora. We report empirical results on a variety of\nopen-source and commercial LLMs, highlighting significant performance gaps that\nsignal the under-representation of Filipino in pre-training corpora, the unique\nhurdles in modeling Filipino's rich morphology and construction, and the\nimportance of explicit Filipino language support. Moreover, we discuss the\npractical challenges encountered in dataset construction and propose principled\nsolutions for building culturally and linguistically-faithful resources in\nunder-represented languages. We also provide a public evaluation suite as a\nclear foundation for iterative, community-driven progress in Filipino NLP."}
{"id": "2502.18108", "pdf": "https://arxiv.org/pdf/2502.18108.pdf", "abs": "https://arxiv.org/abs/2502.18108", "title": "Uncertainty Quantification in Retrieval Augmented Question Answering", "authors": ["Laura Perez-Beltrachini", "Mirella Lapata"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval augmented Question Answering (QA) helps QA models overcome\nknowledge gaps by incorporating retrieved evidence, typically a set of\npassages, alongside the question at test time. Previous studies show that this\napproach improves QA performance and reduces hallucinations, without, however,\nassessing whether the retrieved passages are indeed useful at answering\ncorrectly. In this work, we propose to quantify the uncertainty of a QA model\nvia estimating the utility of the passages it is provided with. We train a\nlightweight neural model to predict passage utility for a target QA model and\nshow that while simple information theoretic metrics can predict answer\ncorrectness up to a certain extent, our approach efficiently approximates or\noutperforms more expensive sampling-based methods. Code and data are available\nat https://github.com/lauhaide/ragu."}
{"id": "2502.18443", "pdf": "https://arxiv.org/pdf/2502.18443.pdf", "abs": "https://arxiv.org/abs/2502.18443", "title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models", "authors": ["Jake Poznanski", "Aman Rangapur", "Jon Borchardt", "Jason Dunkelberger", "Regan Huff", "Daniel Lin", "Aman Rangapur", "Christopher Wilhelm", "Kyle Lo", "Luca Soldaini"], "categories": ["cs.CL"], "comment": null, "summary": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. Traditional open source tools often produce\nlower quality extractions compared to vision language models (VLMs), but\nreliance on the best VLMs can be prohibitively costly (e.g., over $6,240 USD\nper million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to\nproprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs\ninto clean, linearized plain text in natural reading order while preserving\nstructured content like sections, tables, lists, equations, and more. Our\ntoolkit runs a fine-tuned 7B vision language model (VLM) trained on\nolmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and can convert a million PDF pages for\nonly $176 USD. To aid comparison with existing systems, we also introduce\nolmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that\nremain challenging even for the best tools and VLMs, including formulas,\ntables, tiny fonts, old scans, and more. We find olmOCR outperforms even top\nVLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all\ncomponents of olmOCR: our fine-tuned VLM model, training code and data, an\nefficient inference pipeline that supports vLLM and SGLang backends, and\nbenchmark olmOCR-Bench."}
{"id": "2502.18452", "pdf": "https://arxiv.org/pdf/2502.18452.pdf", "abs": "https://arxiv.org/abs/2502.18452", "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response", "authors": ["Mollie Shichman", "Claire Bonial", "Austin Blodgett", "Taylor Hudson", "Francis Ferraro", "Rachel Rudinger"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 5 tables", "summary": "During Human Robot Interactions in disaster relief scenarios, Large Language\nModels (LLMs) have the potential for substantial physical reasoning to assist\nin mission objectives. However, these capabilities are often found only in\nlarger models, which are frequently not reasonable to deploy on robotic\nsystems. To meet our problem space requirements, we introduce a dataset and\npipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA)\nmodels. In our pipeline, domain experts and linguists combine their knowledge\nto make high-quality few-shot prompts used to generate synthetic data for\nfine-tuning. We hand-curate datasets for this few-shot prompting and for\nevaluation to improve LLM reasoning on both general and disaster-specific\nobjects. We concurrently run an ablation study to understand which kinds of\nsynthetic data most affect performance. We fine-tune several small\ninstruction-tuned models and find that ablated FRIDA models only trained on\nobjects' physical state and function data outperformed both the FRIDA models\ntrained on all synthetic data and the base models in our customized evaluation.\nWe demonstrate that the FRIDA pipeline is capable of instilling physical common\nsense with minimal data."}
{"id": "2503.01807", "pdf": "https://arxiv.org/pdf/2503.01807.pdf", "abs": "https://arxiv.org/abs/2503.01807", "title": "Large-Scale Data Selection for Instruction Tuning", "authors": ["Hamish Ivison", "Muru Zhang", "Faeze Brahman", "Pang Wei Koh", "Pradeep Dasigi"], "categories": ["cs.CL"], "comment": "Updated, new baselines, removed some typos", "summary": "Selecting high-quality training data from a larger pool is a crucial step\nwhen instruction-tuning language models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.\nAutomated data selection approaches for instruction-tuning are typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant of\nrepresentation-based data selection (RDS+), which uses weighted mean pooling of\npretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection."}
{"id": "2503.02832", "pdf": "https://arxiv.org/pdf/2503.02832.pdf", "abs": "https://arxiv.org/abs/2503.02832", "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "authors": ["Songming Zhang", "Xue Zhang", "Tong Zhang", "Bojie Hu", "Yufeng Chen", "Jinan Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main Conference, code available at:\n  https://github.com/songmzhang/AlignDistil", "summary": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization."}
{"id": "2503.05298", "pdf": "https://arxiv.org/pdf/2503.05298.pdf", "abs": "https://arxiv.org/abs/2503.05298", "title": "Coreference as an indicator of context scope in multimodal narrative", "authors": ["Nikolai Ilinykh", "Shalom Lappin", "Asad Sayeed", "Sharid Lo√°iciga"], "categories": ["cs.CL"], "comment": "19 pages, 4 tables. Accepted to GEM2 Workshop: Generation, Evaluation\n  & Metrics at ACL 2025", "summary": "We demonstrate that large multimodal language models differ substantially\nfrom humans in the distribution of coreferential expressions in a visual\nstorytelling task. We introduce a number of metrics to quantify the\ncharacteristics of coreferential patterns in both human- and machine-written\ntexts. Humans distribute coreferential expressions in a way that maintains\nconsistency across texts and images, interleaving references to different\nentities in a highly varied way. Machines are less able to track mixed\nreferences, despite achieving perceived improvements in generation quality.\nMaterials, metrics, and code for our study are available at\nhttps://github.com/GU-CLASP/coreference-context-scope."}
{"id": "2503.05328", "pdf": "https://arxiv.org/pdf/2503.05328.pdf", "abs": "https://arxiv.org/abs/2503.05328", "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models", "authors": ["Anar Yeginbergen", "Maite Oronoz", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems."}
{"id": "2503.05888", "pdf": "https://arxiv.org/pdf/2503.05888.pdf", "abs": "https://arxiv.org/abs/2503.05888", "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation", "authors": ["Bang Nguyen", "Tingting Du", "Mengxia Yu", "Lawrence Angrave", "Meng Jiang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Camera Ready - ACL 2025 Main", "summary": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items."}
{"id": "2503.10486", "pdf": "https://arxiv.org/pdf/2503.10486.pdf", "abs": "https://arxiv.org/abs/2503.10486", "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions", "authors": ["Gaurav Kumar Gupta", "Pranal Pande", "Nirajan Acharya", "Aniket Kumar Singh", "Suman Niroula"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare."}
{"id": "2503.11280", "pdf": "https://arxiv.org/pdf/2503.11280.pdf", "abs": "https://arxiv.org/abs/2503.11280", "title": "High-Dimensional Interlingual Representations of Large Language Models", "authors": ["Bryan Wilie", "Samuel Cahyawijaya", "Junxian He", "Pascale Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."}
{"id": "2503.16031", "pdf": "https://arxiv.org/pdf/2503.16031.pdf", "abs": "https://arxiv.org/abs/2503.16031", "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content", "authors": ["Sai Kartheek Reddy Kasu", "Shankar Biradar", "Sunil Saumya"], "categories": ["cs.CL"], "comment": "7 Pages, 2 figures, 7 tables", "summary": "In the evolving landscape of online discourse, misinformation increasingly\nadopts humorous tones to evade detection and gain traction. This work\nintroduces Deceptive Humor as a novel research direction, emphasizing how false\nnarratives, when coated in humor, can become more difficult to detect and more\nlikely to spread. To support research in this space, we present the Deceptive\nHumor Dataset (DHD) a collection of humor-infused comments derived from\nfabricated claims using the ChatGPT-4o model. Each entry is labeled with a\nSatire Level (from 1 for subtle satire to 3 for overt satire) and categorized\ninto five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their\ncode-mixed forms, making it a valuable resource for multilingual analysis. DHD\noffers a structured foundation for understanding how humor can serve as a\nvehicle for the propagation of misinformation, subtly enhancing its reach and\nimpact. Strong baselines are established to encourage further research and\nmodel development in this emerging area."}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154.pdf", "abs": "https://arxiv.org/abs/2504.05154", "title": "CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL"], "comment": "28 pages", "summary": "Language Models (LMs) are typically tuned with human preferences to produce\nhelpful responses, but the impact of preference tuning on the ability to handle\nculturally diverse queries remains understudied. In this paper, we\nsystematically analyze how native human cultural preferences can be\nincorporated into the preference learning process to train more culturally\naware LMs. We introduce CARE, a multilingual resource containing 3,490\nculturally specific questions and 31.7k responses with native judgments. We\ndemonstrate how a modest amount of high-quality native preferences improves\ncultural awareness across various LMs, outperforming larger generic preference\ndata. Our analyses reveal that models with stronger initial cultural\nperformance benefit more from alignment, leading to gaps among models developed\nin different regions with varying access to culturally relevant data. CARE will\nbe made publicly available at https://github.com/Guochry/CARE."}
{"id": "2504.07385", "pdf": "https://arxiv.org/pdf/2504.07385.pdf", "abs": "https://arxiv.org/abs/2504.07385", "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences."}
{"id": "2504.12345", "pdf": "https://arxiv.org/pdf/2504.12345.pdf", "abs": "https://arxiv.org/abs/2504.12345", "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "authors": ["Yutong Xia", "Ao Qu", "Yunhan Zheng", "Yihong Tang", "Dingyi Zhuang", "Yuxuan Liang", "Shenhao Wang", "Cathy Wu", "Lijun Sun", "Roger Zimmermann", "Jinhua Zhao"], "categories": ["cs.CL", "cs.CY", "cs.MA"], "comment": null, "summary": "Urban causal research is essential for understanding the complex, dynamic\nprocesses that shape cities and for informing evidence-based policies. However,\ncurrent practices are often constrained by inefficient and biased hypothesis\nformulation, challenges in integrating multimodal data, and fragile\nexperimental methodologies. Imagine a system that automatically estimates the\ncausal impact of congestion pricing on commute times by income group or\nmeasures how new green spaces affect asthma rates across neighborhoods using\nsatellite imagery and health reports, and then generates comprehensive,\npolicy-ready outputs, including causal estimates, subgroup analyses, and\nactionable recommendations. In this Perspective, we propose UrbanCIA, an\nLLM-driven conceptual framework composed of four distinct modular agents\nresponsible for hypothesis generation, data engineering, experiment design and\nexecution, and results interpretation with policy insights. We begin by\nexamining the current landscape of urban causal research through a structured\ntaxonomy of research topics, data sources, and methodological approaches,\nrevealing systemic limitations across the workflow. Next, we introduce the\ndesign principles and technological roadmap for the four modules in the\nproposed framework. We also propose evaluation criteria to assess the rigor and\ntransparency of these AI-augmented processes. Finally, we reflect on the\nbroader implications for human-AI collaboration, equity, and accountability. We\ncall for a new research agenda that embraces LLM-driven tools as catalysts for\nmore scalable, reproducible, and inclusive urban research."}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625.pdf", "abs": "https://arxiv.org/abs/2504.21625", "title": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability", "authors": ["Jiaming Wang", "Yunke Zhao", "Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions\nthrough an iterative feedback framework, which enables models to self-correct\nbased on specific requirement failures in each turn, better reflecting\nreal-world user-end usage patterns. Meanwhile, the benchmark implements a\ncomprehensive evaluation system with 38 capability tags organized across three\ndimensions: Intent Recognition, Granular Content Validation, and Output\nStructure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios."}
{"id": "2505.02819", "pdf": "https://arxiv.org/pdf/2505.02819.pdf", "abs": "https://arxiv.org/abs/2505.02819", "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe."}
{"id": "2505.07796", "pdf": "https://arxiv.org/pdf/2505.07796.pdf", "abs": "https://arxiv.org/abs/2505.07796", "title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "authors": ["Xingjin Wang", "Howe Tissue", "Lu Wang", "Linjing Li", "Daniel Dajun Zeng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICML2025 (Oral)", "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters."}
{"id": "2505.07968", "pdf": "https://arxiv.org/pdf/2505.07968.pdf", "abs": "https://arxiv.org/abs/2505.07968", "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Lucas A. Salas", "Jiang Gui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential in the field of health\ncare, yet they face great challenges in adapting to rapidly evolving medical\nknowledge. This can lead to outdated or contradictory treatment suggestions.\nThis study investigated how LLMs respond to evolving clinical guidelines,\nfocusing on concept drift and internal inconsistencies. We developed the\nDriftMedQA benchmark to simulate guideline evolution and assessed the temporal\nreliability of various LLMs. Our evaluation of seven state-of-the-art models\nacross 4,290 scenarios demonstrated difficulties in rejecting outdated\nrecommendations and frequently endorsing conflicting guidance. Additionally, we\nexplored two mitigation strategies: Retrieval-Augmented Generation and\npreference fine-tuning via Direct Preference Optimization. While each method\nimproved model performance, their combination led to the most consistent and\nreliable results. These findings underscore the need to improve LLM robustness\nto temporal shifts to ensure more dependable applications in clinical practice.\nThe dataset is available at https://huggingface.co/datasets/RDBH/DriftMed."}
{"id": "2505.13487", "pdf": "https://arxiv.org/pdf/2505.13487.pdf", "abs": "https://arxiv.org/abs/2505.13487", "title": "Detecting Prefix Bias in LLM-based Reward Models", "authors": ["Ashwin Kumar", "Yuzi He", "Aram H. Markosyan", "Bobbie Chern", "Imanol Arrieta-Ibarra"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a key\nparadigm for task-specific fine-tuning of language models using human\npreference data. While numerous publicly available preference datasets provide\npairwise comparisons of responses, the potential for biases in the resulting\nreward models remains underexplored. In this work, we introduce novel methods\nto detect and evaluate prefix bias -- a systematic shift in model preferences\ntriggered by minor variations in query prefixes -- in LLM-based reward models\ntrained on such datasets. We leverage these metrics to reveal significant\nbiases in preference models across racial and gender dimensions. Our\ncomprehensive evaluation spans diverse open-source preference datasets and\nreward model architectures, demonstrating susceptibility to this kind of bias\nregardless of the underlying model architecture. Furthermore, we propose a data\naugmentation strategy to mitigate these biases, showing its effectiveness in\nreducing the impact of prefix bias. Our findings highlight the critical need\nfor bias-aware dataset design and evaluation in developing fair and reliable\nreward models, contributing to the broader discourse on fairness in AI."}
{"id": "2505.14015", "pdf": "https://arxiv.org/pdf/2505.14015.pdf", "abs": "https://arxiv.org/abs/2505.14015", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications."}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637.pdf", "abs": "https://arxiv.org/abs/2505.16637", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li", "Sitong Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."}
{"id": "2505.18110", "pdf": "https://arxiv.org/pdf/2505.18110.pdf", "abs": "https://arxiv.org/abs/2505.18110", "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM", "authors": ["Zinuo Li", "Xian Zhang", "Yongxin Guo", "Mohammed Bennamoun", "Farid Boussaid", "Girish Dwivedi", "Luqi Gong", "Qiuhong Ke"], "categories": ["cs.CL"], "comment": null, "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released."}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436.pdf", "abs": "https://arxiv.org/abs/2505.18436", "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies."}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675.pdf", "abs": "https://arxiv.org/abs/2505.19675", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at KDD'25", "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523.pdf", "abs": "https://arxiv.org/abs/2505.21523", "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495.pdf", "abs": "https://arxiv.org/abs/2506.01495", "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC."}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404.pdf", "abs": "https://arxiv.org/abs/2506.02404", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qian-wen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community."}
{"id": "2506.06619", "pdf": "https://arxiv.org/pdf/2506.06619.pdf", "abs": "https://arxiv.org/abs/2506.06619", "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs", "authors": ["Jesse Woo", "Fateme Hashemi Chaleshtori", "Ana Marasoviƒá", "Kenneth Marino"], "categories": ["cs.CL"], "comment": "ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix", "summary": "A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work."}
{"id": "2506.06751", "pdf": "https://arxiv.org/pdf/2506.06751.pdf", "abs": "https://arxiv.org/abs/2506.06751", "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries according to contemporary language models", "authors": ["Mikhail Salnikov", "Dmitrii Korzh", "Ivan Lazichny", "Elvir Karimov", "Artyom Iudin", "Ivan Oseledets", "Oleg Y. Rogov", "Natalia Loukachevitch", "Alexander Panchenko", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": null, "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research."}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245.pdf", "abs": "https://arxiv.org/abs/2506.07245", "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement."}
{"id": "2506.08897", "pdf": "https://arxiv.org/pdf/2506.08897.pdf", "abs": "https://arxiv.org/abs/2506.08897", "title": "PlantBert: An Open Source Language Model for Plant Science", "authors": ["Hiba Khey", "Amine Lakhder", "Salma Rouichi", "Imane El Ghabi", "Kamal Hejjaoui", "Younes En-nahli", "Fahd Kalloubi", "Moez Amri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of transformer-based language models has catalyzed\nbreakthroughs in biomedical and clinical natural language processing; however,\nplant science remains markedly underserved by such domain-adapted tools. In\nthis work, we present PlantBert, a high-performance, open-source language model\nspecifically tailored for extracting structured knowledge from plant\nstress-response literature. Built upon the DeBERTa architecture-known for its\ndisentangled attention and robust contextual encoding-PlantBert is fine-tuned\non a meticulously curated corpus of expert-annotated abstracts, with a primary\nfocus on lentil (Lens culinaris) responses to diverse abiotic and biotic\nstressors. Our methodology combines transformer-based modeling with\nrule-enhanced linguistic post-processing and ontology-grounded entity\nnormalization, enabling PlantBert to capture biologically meaningful\nrelationships with precision and semantic fidelity. The underlying corpus is\nannotated using a hierarchical schema aligned with the Crop Ontology,\nencompassing molecular, physiological, biochemical, and agronomic dimensions of\nplant adaptation. PlantBert exhibits strong generalization capabilities across\nentity types and demonstrates the feasibility of robust domain adaptation in\nlow-resource scientific fields. By providing a scalable and reproducible\nframework for high-resolution entity recognition, PlantBert bridges a critical\ngap in agricultural NLP and paves the way for intelligent, data-driven systems\nin plant genomics, phenomics, and agronomic knowledge discovery. Our model is\npublicly released to promote transparency and accelerate cross-disciplinary\ninnovation in computational plant science."}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903.pdf", "abs": "https://arxiv.org/abs/2506.11903", "title": "GeistBERT: Breathing Life into German NLP", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license."}
{"id": "2506.12307", "pdf": "https://arxiv.org/pdf/2506.12307.pdf", "abs": "https://arxiv.org/abs/2506.12307", "title": "Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning", "authors": ["Xiaotian Zhang", "Yuan Wang", "Zhaopeng Feng", "Ruizhe Chen", "Zhijie Zhou", "Yan Zhang", "Hongxia Xu", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical Question-Answering (QA) encompasses a broad spectrum of tasks,\nincluding multiple choice questions (MCQ), open-ended text generation, and\ncomplex computational reasoning. Despite this variety, a unified framework for\ndelivering high-quality medical QA has yet to emerge. Although recent progress\nin reasoning-augmented large language models (LLMs) has shown promise, their\nability to achieve comprehensive medical understanding is still largely\nunexplored. In this paper, we present Med-U1, a unified framework for robust\nreasoning across medical QA tasks with diverse output formats, ranging from\nMCQs to complex generation and computation tasks. Med-U1 employs pure\nlarge-scale reinforcement learning with mixed rule-based binary reward\nfunctions, incorporating a length penalty to manage output verbosity. With\nmulti-objective reward optimization, Med-U1 directs LLMs to produce concise and\nverifiable reasoning chains. Empirical results reveal that Med-U1 significantly\nimproves performance across multiple challenging Med-QA benchmarks, surpassing\neven larger specialized and proprietary models. Furthermore, Med-U1\ndemonstrates robust generalization to out-of-distribution (OOD) tasks.\nExtensive analysis presents insights into training strategies, reasoning chain\nlength control, and reward design for medical LLMs. Our code is available here."}
{"id": "2506.13610", "pdf": "https://arxiv.org/pdf/2506.13610.pdf", "abs": "https://arxiv.org/abs/2506.13610", "title": "A Structured Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy", "authors": ["Abdullah Al Shafi", "Rowzatul Zannat", "Abdul Muntakim", "Mahmudul Hasan"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value (1 or 0), indicating whether a\nsymptom is associated with a disease (1 for presence, 0 for absence). Thereby,\nthis structured representation makes the dataset very useful for a wide range\nof applications, including machine learning-based disease prediction, clinical\ndecision support systems, and epidemiological studies. Although there are some\nadvancements in the field of disease-symptom datasets, there is a significant\ngap in structured datasets for the Bangla language. This dataset aims to bridge\nthat gap by facilitating the development of multilingual medical informatics\ntools and improving disease prediction models for underrepresented linguistic\ncommunities. Further developments should include region-specific diseases and\nfurther fine-tuning of symptom associations for better diagnostic performance"}
{"id": "2506.13681", "pdf": "https://arxiv.org/pdf/2506.13681.pdf", "abs": "https://arxiv.org/abs/2506.13681", "title": "Min-p, Max Exaggeration: A Critical Analysis of Min-p Sampling in Language Models", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Yegor Denisov-Blanch"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity."}
{"id": "2506.14028", "pdf": "https://arxiv.org/pdf/2506.14028.pdf", "abs": "https://arxiv.org/abs/2506.14028", "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications."}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111.pdf", "abs": "https://arxiv.org/abs/2506.14111", "title": "Essential-Web v1.0: 24T tokens of organized web data", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "include MegaMath-Web-Pro", "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}
{"id": "2310.17143", "pdf": "https://arxiv.org/pdf/2310.17143.pdf", "abs": "https://arxiv.org/abs/2310.17143", "title": "Techniques for supercharging academic writing with generative AI", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.CL"], "comment": "Published in: Nature Biomedical Engineering, 2025", "summary": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science."}
{"id": "2403.04311", "pdf": "https://arxiv.org/pdf/2403.04311.pdf", "abs": "https://arxiv.org/abs/2403.04311", "title": "Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry", "authors": ["Deepti Raghavan", "Keshav Santhanam", "Muhammad Shahir Rahman", "Nayani Modugula", "Luis Gaspar Schroeder", "Maximilien Cura", "Houjun Liu", "Pratiksha Thaker", "Philip Levis", "Matei Zaharia"], "categories": ["cs.AI", "cs.CL", "cs.DC", "cs.IR"], "comment": null, "summary": "Compound AI applications chain together subcomponents such as generative\nlanguage models, document retrievers, and embedding models. Applying\ntraditional systems optimizations such as parallelism and pipelining in\ncompound AI systems is difficult because each component has different\nconstraints in terms of the granularity and type of data that it ingests. New\ndata is often generated during intermediate computations, and text streams may\nbe split into smaller, independent fragments (such as documents to sentences)\nwhich may then be re-aggregated at later parts of the computation. Due to this\ncomplexity, existing systems to serve compound AI queries do not fully take\nadvantage of parallelism and pipelining opportunities.\n  We present Alto, a framework that automatically optimizes execution of\ncompound AI queries through streaming and parallelism. Bento introduces a new\nabstraction called nested ancestry, a metadata hierarchy that allows the system\nto correctly track partial outputs and aggregate data across the heterogeneous\nconstraints of the components of compound AI applications. This metadata is\nautomatically inferred from the programming model, allowing developers to\nexpress complex dataflow patterns without needing to reason manually about the\ndetails of routing and aggregation. Implementations of four applications in\nAlto outperform or match implementations in LangGraph, a popular existing AI\nprogramming framework. Alto implementations match or improve latency by between\n10-30%."}
{"id": "2406.12593", "pdf": "https://arxiv.org/pdf/2406.12593.pdf", "abs": "https://arxiv.org/abs/2406.12593", "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Yinwei Wei", "Trung Le", "Dragan Gasevic", "Yuan-Fang Li", "Thanh-Toan Do"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI", "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."}
{"id": "2407.17734", "pdf": "https://arxiv.org/pdf/2407.17734.pdf", "abs": "https://arxiv.org/abs/2407.17734", "title": "Cost-effective Instruction Learning for Pathology Vision and Language Analysis", "authors": ["Kaitao Chen", "Mianxin Liu", "Fang Yan", "Lei Ma", "Xiaoming Shi", "Lilong Wang", "Xiaosong Wang", "Lifeng Zhu", "Zhe Wang", "Mu Zhou", "Shaoting Zhang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology."}
{"id": "2408.08872", "pdf": "https://arxiv.org/pdf/2408.08872.pdf", "abs": "https://arxiv.org/abs/2408.08872", "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "authors": ["Le Xue", "Manli Shu", "Anas Awadalla", "Jun Wang", "An Yan", "Senthil Purushwalkam", "Honglu Zhou", "Viraj Prabhu", "Yutong Dai", "Michael S Ryoo", "Shrikant Kendre", "Jieyu Zhang", "Shaoyen Tseng", "Gustavo A Lujan-Moreno", "Matthew L Olson", "Musashi Hinck", "David Cobbley", "Vasudev Lal", "Can Qin", "Shu Zhang", "Chia-Chih Chen", "Ning Yu", "Juntao Tan", "Tulika Manoj Awalgaonkar", "Shelby Heinecke", "Huan Wang", "Yejin Choi", "Ludwig Schmidt", "Zeyuan Chen", "Silvio Savarese", "Juan Carlos Niebles", "Caiming Xiong", "Ran Xu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces BLIP-3, an open framework for developing Large\nMultimodal Models (LMMs). The framework comprises meticulously curated\ndatasets, a training recipe, model architectures, and a resulting suite of\nLMMs. We release 4B and 14B models, including both the pre-trained base model\nand the instruction fine-tuned ones. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\nmodels demonstrate competitive performance among open-source LMMs with similar\nmodel sizes. Our resulting LMMs demonstrate competitive performance among\nopen-source LMMs with similar model sizes, with the ability to comprehend\ninterleaved image-text inputs. Our training code, models, and all datasets used\nin this work, including the three largescale datasets we create and the\npreprocessed ones, will be open-sourced to better support the research\ncommunity."}
{"id": "2409.13609", "pdf": "https://arxiv.org/pdf/2409.13609.pdf", "abs": "https://arxiv.org/abs/2409.13609", "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension", "authors": ["Ting Liu", "Zunnan Xu", "Yue Hu", "Liangtao Shi", "Zhiqiang Wang", "Quanjun Yin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "EMNLP 2024 main", "summary": "Referring Expression Comprehension (REC), which aims to ground a local visual\nregion via natural language, is a task that heavily relies on multimodal\nalignment. Most existing methods utilize powerful pre-trained models to\ntransfer visual/linguistic knowledge by full fine-tuning. However, full\nfine-tuning the entire backbone not only breaks the rich prior knowledge\nembedded in the pre-training, but also incurs significant computational costs.\nMotivated by the recent emergence of Parameter-Efficient Transfer Learning\n(PETL) methods, we aim to solve the REC task in an effective and efficient\nmanner. Directly applying these PETL methods to the REC task is inappropriate,\nas they lack the specific-domain abilities for precise local visual perception\nand visual-language alignment. Therefore, we propose a novel framework of\nMultimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.\nSpecifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned\nprior, and Local Convolution Adapters to extract precise local semantics for\nbetter visual perception. Moreover, the Prior-Guided Text module is proposed to\nfurther utilize the prior for facilitating the cross-modal alignment.\nExperimental results on three widely-used benchmarks demonstrate that MaPPER\nachieves the best accuracy compared to the full fine-tuning and other PETL\nmethods with only 1.41% tunable backbone parameters. Our code is available at\nhttps://github.com/liuting20/MaPPER."}
{"id": "2410.08316", "pdf": "https://arxiv.org/pdf/2410.08316.pdf", "abs": "https://arxiv.org/abs/2410.08316", "title": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework", "authors": ["Yinuo Ren", "Tesi Xiao", "Michael Shavlovsky", "Lexing Ying", "Holakou Rahmanian"], "categories": ["cs.LG", "cs.CL", "math.OC"], "comment": "Published at UAI 2025", "summary": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose a Conditioned One-Shot fine-tuning framework\n(COS-DPO) that extends the Direct Preference Optimization technique, originally\ndeveloped for efficient LLM alignment with preference data, to accommodate the\nMOFT settings. By direct conditioning on the weight across auxiliary\nobjectives, our Weight-COS-DPO method enjoys an efficient one-shot training\nprocess for profiling the Pareto front and is capable of achieving\ncomprehensive trade-off solutions even in the post-training stage. Based on our\ntheoretical findings on the linear transformation properties of the loss\nfunction, we further propose the Temperature-COS-DPO method that augments the\ntemperature parameter to the model input, enhancing the flexibility of\npost-training control over the trade-offs between the main and auxiliary\nobjectives. We demonstrate the effectiveness and efficiency of the COS-DPO\nframework through its applications to various tasks, including the\nLearning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for\nlarge-scale ML deployments."}
{"id": "2410.18077", "pdf": "https://arxiv.org/pdf/2410.18077.pdf", "abs": "https://arxiv.org/abs/2410.18077", "title": "ALTA: Compiler-Based Analysis of Transformers", "authors": ["Peter Shaw", "James Cohan", "Jacob Eisenstein", "Kenton Lee", "Jonathan Berant", "Kristina Toutanova"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "TMLR 2025", "summary": "We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights."}
{"id": "2411.00412", "pdf": "https://arxiv.org/pdf/2411.00412.pdf", "abs": "https://arxiv.org/abs/2411.00412", "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation", "authors": ["Bohan Lyu", "Yadi Cao", "Duncan Watson-Parris", "Leon Bergen", "Taylor Berg-Kirkpatrick", "Rose Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "37 pages, 16 figures", "summary": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning."}
{"id": "2411.04105", "pdf": "https://arxiv.org/pdf/2411.04105.pdf", "abs": "https://arxiv.org/abs/2411.04105", "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning", "authors": ["Guan Zhe Hong", "Nishanth Dikkala", "Enming Luo", "Cyrus Rashtchian", "Xin Wang", "Rina Panigrahy"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Due to the size and complexity of modern large language models (LLMs), it has\nproven challenging to uncover the underlying mechanisms that models use to\nsolve reasoning problems. For instance, is their reasoning for a specific\nproblem localized to certain parts of the network? Do they break down the\nreasoning problem into modular components that are then executed as sequential\nsteps as we go deeper in the model? To better understand the reasoning\ncapability of LLMs, we study a minimal propositional logic problem that\nrequires combining multiple facts to arrive at a solution. By studying this\nproblem on Mistral and Gemma models, up to 27B parameters, we illuminate the\ncore components the models use to solve such logic problems. From a mechanistic\ninterpretability point of view, we use causal mediation analysis to uncover the\npathways and components of the LLMs' reasoning processes. Then, we offer\nfine-grained insights into the functions of attention heads in different\nlayers. We not only find a sparse circuit that computes the answer, but we\ndecompose it into sub-circuits that have four distinct and modular uses.\nFinally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and\nGemma-2-27B -- contain analogous but not identical mechanisms."}
{"id": "2411.05091", "pdf": "https://arxiv.org/pdf/2411.05091.pdf", "abs": "https://arxiv.org/abs/2411.05091", "title": "Watermarking Language Models through Language Models", "authors": ["Agnibh Dasgupta", "Abdullah Tanvir", "Xin Zhong"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Watermarking the outputs of large language models (LLMs) is critical for\nprovenance tracing, content regulation, and model accountability. Existing\napproaches often rely on access to model internals or are constrained by static\nrules and token-level perturbations. Moreover, the idea of steering generative\nbehavior via prompt-based instruction control remains largely underexplored. We\nintroduce a prompt-guided watermarking framework that operates entirely at the\ninput level and requires no access to model parameters or decoding logits. The\nframework comprises three cooperating components: a Prompting LM that\nsynthesizes watermarking instructions from user prompts, a Marking LM that\ngenerates watermarked outputs conditioned on these instructions, and a\nDetecting LM trained to classify whether a response carries an embedded\nwatermark. This modular design enables dynamic watermarking that adapts to\nindividual prompts while remaining compatible with diverse LLM architectures,\nincluding both proprietary and open-weight models. We evaluate the framework\nover 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral,\nLLaMA3, and DeepSeek. Experimental results show that watermark signals\ngeneralize across architectures and remain robust under fine-tuning, model\ndistillation, and prompt-based adversarial attacks, demonstrating the\neffectiveness and robustness of the proposed approach."}
{"id": "2411.05261", "pdf": "https://arxiv.org/pdf/2411.05261.pdf", "abs": "https://arxiv.org/abs/2411.05261", "title": "Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation", "authors": ["Yingying Fang", "Zihao Jin", "Shaojie Guo", "Jinda Liu", "Zhiling Yue", "Yijian Gao", "Junzhi Ning", "Zhi Li", "Simon Walsh", "Guang Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports."}
{"id": "2411.05943", "pdf": "https://arxiv.org/pdf/2411.05943.pdf", "abs": "https://arxiv.org/abs/2411.05943", "title": "Quantifying artificial intelligence through algorithmic generalization", "authors": ["Takuya Ito", "Murray Campbell", "Lior Horesh", "Tim Klinger", "Parikshit Ram"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": null, "summary": "The rapid development of artificial intelligence (AI) systems has created an\nurgent need for their scientific quantification. While their fluency across a\nvariety of domains is impressive, AI systems fall short on tests requiring\nalgorithmic reasoning -- a glaring limitation given the necessity for\ninterpretable and reliable technology. Despite a surge of reasoning benchmarks\nemerging from the academic community, no theoretical framework exists to\nquantify algorithmic reasoning in AI systems. Here, we adopt a framework from\ncomputational complexity theory to quantify algorithmic generalization using\nalgebraic expressions: algebraic circuit complexity. Algebraic circuit\ncomplexity theory -- the study of algebraic expressions as circuit models -- is\na natural framework to study the complexity of algorithmic computation.\nAlgebraic circuit complexity enables the study of generalization by defining\nbenchmarks in terms of the computational requirements to solve a problem.\nMoreover, algebraic circuits are generic mathematical objects; an arbitrarily\nlarge number of samples can be generated for a specified circuit, making it an\nideal experimental sandbox for the data-hungry models that are used today. In\nthis Perspective, we adopt tools from algebraic circuit complexity, apply them\nto formalize a science of algorithmic generalization, and address key\nchallenges for its successful application to AI science."}
{"id": "2411.09642", "pdf": "https://arxiv.org/pdf/2411.09642.pdf", "abs": "https://arxiv.org/abs/2411.09642", "title": "On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse", "authors": ["Alkis Kalavasis", "Anay Mehrotra", "Grigoris Velegkas"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "stat.ML"], "comment": "Accepted for presentation at the 57th Symposium on Theory of\n  Computing (STOC 2025)", "summary": "Specifying all desirable properties of a language model is challenging, but\ncertain requirements seem essential. Given samples from an unknown language,\nthe trained model should produce valid strings not seen in training and be\nexpressive enough to capture the language's full richness. Otherwise,\noutputting invalid strings constitutes \"hallucination,\" and failing to capture\nthe full range leads to \"mode collapse.\" We ask if a language model can meet\nboth requirements.\n  We investigate this within a statistical language generation setting building\non Gold and Angluin. Here, the model receives random samples from a\ndistribution over an unknown language K, which belongs to a possibly infinite\ncollection of languages. The goal is to generate unseen strings from K. We say\nthe model generates from K with consistency and breadth if, as training size\nincreases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in\nlanguage generation are possible. We answer this negatively: for a large class\nof language models, including next-token prediction models, this is impossible\nfor most collections of candidate languages. This contrasts with [KM24]'s\nresult, showing consistent generation without breadth is possible for any\ncountable collection of languages. Our finding highlights that generation with\nbreadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples\nneeded for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is\nachievable for any countable collection of languages when negative examples\n(strings outside K) are available alongside positive ones. This suggests that\npost-training feedback, which encodes negative examples, can be crucial in\nreducing hallucinations while limiting mode collapse."}
{"id": "2412.04628", "pdf": "https://arxiv.org/pdf/2412.04628.pdf", "abs": "https://arxiv.org/abs/2412.04628", "title": "Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts", "authors": ["Taneesh Gupta", "Rahul Madhavan", "Xuchao Zhang", "Nagarajan Natarajan", "Chetan Bansal", "Saravan Rajmohan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) has become a popular approach for\naligning language models using pairwise preferences. However, in practical\npost-training pipelines, on-policy generation typically yields multiple\ncandidate responses per prompt, which are scored by a reward model to guide\nlearning. In this setting, we propose $\\textbf{Multi-Preference Optimization\n(MPO)}$, a generalization of DPO that optimizes over entire sets of responses\nby extending the Bradley-Terry model to groupwise comparisons between chosen\nand rejected sets. To further enhance learning, MPO employs deviation-based\nweighting, which emphasizes outlier responses that deviate most from the mean\nreward, effectively inducing a self-paced curriculum. We theoretically prove\nthat MPO reduces alignment bias at a rate of\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}}\\right)$ with respect to the number of\nresponses per query. Empirically, MPO achieves state-of-the-art performance on\nthe UltraFeedback benchmark and yields up to $\\sim 17.5\\%$ improvement over the\nstate-of-the-art baseline in length-controlled win rate on AlpacaEval2,\nestablishing a new baseline for preference-based alignment"}
{"id": "2501.00912", "pdf": "https://arxiv.org/pdf/2501.00912.pdf", "abs": "https://arxiv.org/abs/2501.00912", "title": "AutoPresent: Designing Structured Visuals from Scratch", "authors": ["Jiaxin Ge", "Zora Zhiruo Wang", "Xuhui Zhou", "Yi-Hao Peng", "Sanjay Subramanian", "Qinyue Tan", "Maarten Sap", "Alane Suhr", "Daniel Fried", "Graham Neubig", "Trevor Darrell"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Designing structured visuals such as presentation slides is essential for\ncommunicative needs, necessitating both content creation and visual planning\nskills. In this work, we tackle the challenge of automated slide generation,\nwhere models produce slide presentations from natural language (NL)\ninstructions. We first introduce the SlidesBench benchmark, the first benchmark\nfor slide generation with 7k training and 585 testing examples derived from 310\nslide decks across 10 domains. SlidesBench supports evaluations that are\n(i)reference-based to measure similarity to a target slide, and\n(ii)reference-free to measure the design quality of generated slides alone. We\nbenchmark end-to-end image generation and program generation methods with a\nvariety of models, and find that programmatic methods produce higher-quality\nslides in user-interactable formats. Built on the success of program\ngeneration, we create AutoPresent, an 8B Llama-based model trained on 7k pairs\nof instructions paired with code for slide generation, and achieve results\ncomparable to the closed-source model GPT-4o. We further explore iterative\ndesign refinement where the model is tasked to self-refine its own output, and\nwe found that this process improves the slide's quality. We hope that our work\nwill provide a basis for future work on generating structured visuals."}
{"id": "2501.06589", "pdf": "https://arxiv.org/pdf/2501.06589.pdf", "abs": "https://arxiv.org/abs/2501.06589", "title": "Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping", "authors": ["Muru Zhang", "Mayank Mishra", "Zhongzhu Zhou", "William Brandon", "Jue Wang", "Yoon Kim", "Jonathan Ragan-Kelley", "Shuaiwen Leon Song", "Ben Athiwaratkun", "Tri Dao"], "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": "ICML 2025", "summary": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 29% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens. We release\nour code for training and inference for easier replication of experiments."}
{"id": "2501.15602", "pdf": "https://arxiv.org/pdf/2501.15602.pdf", "abs": "https://arxiv.org/abs/2501.15602", "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning", "authors": ["Zeyu Gan", "Yun Liao", "Yong Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "Published as a conference paper in ICML 2025", "summary": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability."}
{"id": "2502.01208", "pdf": "https://arxiv.org/pdf/2502.01208.pdf", "abs": "https://arxiv.org/abs/2502.01208", "title": "On Almost Surely Safe Alignment of Large Language Models at Inference-Time", "authors": ["Xiaotong Ji", "Shyam Sundhar Ramesh", "Matthieu Zimmer", "Ilija Bogunovic", "Jun Wang", "Haitham Bou Ammar"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce a novel inference-time alignment approach for LLMs that aims to\ngenerate safe responses almost surely, i.e., with probability approaching one.\nOur approach models the generation of safe responses as a constrained Markov\nDecision Process (MDP) within the LLM's latent space. We augment a safety state\nthat tracks the evolution of safety constraints and dynamically penalize unsafe\ngenerations to ensure the generation of safe responses. Consequently, we\ndemonstrate formal safety guarantees w.r.t. the given cost model upon solving\nthe MDP in the latent space with sufficiently large penalties. Building on this\nfoundation, we propose InferenceGuard, a practical implementation that safely\naligns LLMs without modifying the model weights. Empirically, we demonstrate\nthat InferenceGuard effectively balances safety and task performance,\noutperforming existing inference-time alignment methods in generating safe and\naligned responses. Our findings contribute to the advancement of safer LLM\ndeployment through alignment at inference-time, thus presenting a promising\nalternative to resource-intensive, overfitting-prone alignment techniques like\nRLHF."}
{"id": "2502.14321", "pdf": "https://arxiv.org/pdf/2502.14321.pdf", "abs": "https://arxiv.org/abs/2502.14321", "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems", "authors": ["Bingyu Yan", "Zhibo Zhou", "Litian Zhang", "Lian Zhang", "Ziyi Zhou", "Dezhuang Miao", "Zhoujun Li", "Chaozhuo Li", "Xiaoming Zhang"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large language model-based multi-agent systems have recently gained\nsignificant attention due to their potential for complex, collaborative, and\nintelligent problem-solving capabilities. Existing surveys typically categorize\nLLM-based multi-agent systems (LLM-MAS) according to their application domains\nor architectures, overlooking the central role of communication in coordinating\nagent behaviors and interactions. To address this gap, this paper presents a\ncomprehensive survey of LLM-MAS from a communication-centric perspective.\nSpecifically, we propose a structured framework that integrates system-level\ncommunication (architecture, goals, and protocols) with system internal\ncommunication (strategies, paradigms, objects, and content), enabling a\ndetailed exploration of how agents interact, negotiate, and achieve collective\nintelligence. Through an extensive analysis of recent literature, we identify\nkey components in multiple dimensions and summarize their strengths and\nlimitations. In addition, we highlight current challenges, including\ncommunication efficiency, security vulnerabilities, inadequate benchmarking,\nand scalability issues, and outline promising future research directions. This\nreview aims to help researchers and practitioners gain a clear understanding of\nthe communication mechanisms in LLM-MAS, thereby facilitating the design and\ndeployment of robust, scalable, and secure multi-agent systems."}
{"id": "2503.06680", "pdf": "https://arxiv.org/pdf/2503.06680.pdf", "abs": "https://arxiv.org/abs/2503.06680", "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation", "authors": ["Wei Li", "Xin Zhang", "Zhongxin Guo", "Shaoguang Mao", "Wen Luo", "Guangyue Peng", "Yangyu Huang", "Houfeng Wang", "Scarlett Li"], "categories": ["cs.SE", "cs.CL"], "comment": "V2, Accepted by ACL 2025 main conference", "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development."}
{"id": "2503.11702", "pdf": "https://arxiv.org/pdf/2503.11702.pdf", "abs": "https://arxiv.org/abs/2503.11702", "title": "LLM-Guided Indoor Navigation with Multimodal Map Understanding", "authors": ["Alberto Coffrini", "Paolo Barsocchi", "Francesco Furfari", "Antonino Crivello", "Alessio Ferrari"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 3 figures, 5 tables", "summary": "Indoor navigation presents unique challenges due to complex layouts and the\nunavailability of GNSS signals. Existing solutions often struggle with\ncontextual adaptation, and typically require dedicated hardware. In this work,\nwe explore the potential of a Large Language Model (LLM), i.e., ChatGPT, to\ngenerate natural, context-aware navigation instructions from indoor map images.\nWe design and evaluate test cases across different real-world environments,\nanalyzing the effectiveness of LLMs in interpreting spatial layouts, handling\nuser constraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 86.59% correct indications and a maximum of 97.14%. The proposed\nsystem achieves high accuracy and reasoning performance. These results have key\nimplications for AI-driven navigation and assistive technologies."}
{"id": "2503.23804", "pdf": "https://arxiv.org/pdf/2503.23804.pdf", "abs": "https://arxiv.org/abs/2503.23804", "title": "DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents", "authors": ["Shiyi Yang", "Zhibo Hu", "Xinshu Li", "Chen Wang", "Tong Yu", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "categories": ["cs.CR", "cs.CL", "cs.IR", "cs.MA"], "comment": null, "summary": "Large language model (LLM)-powered agents are increasingly used in\nrecommender systems (RSs) to achieve personalized behavior modeling, where the\nmemory mechanism plays a pivotal role in enabling the agents to autonomously\nexplore, learn and self-evolve from real-world interactions. However, this very\nmechanism, serving as a contextual repository, inherently exposes an attack\nsurface for potential adversarial manipulations. Despite its central role, the\nrobustness of agentic RSs in the face of such threats remains largely\nunderexplored. Previous works suffer from semantic mismatches or rely on static\nembeddings or pre-defined prompts, all of which hinder their applicability to\nsystems with dynamic memory states. This challenge is exacerbated by the\nblack-box nature of commercial RSs.\n  To tackle the above problems, in this paper, we present the first systematic\ninvestigation of memory-based vulnerabilities in LLM-powered recommender\nagents, revealing their security limitations and guiding efforts to strengthen\nsystem resilience and trustworthiness. Specifically, we propose a novel\nblack-box attack framework named DrunkAgent. DrunkAgent crafts semantically\nmeaningful adversarial textual triggers for target item promotions and\nintroduces a series of strategies to maximize the trigger effect by corrupting\nthe memory updates during the interactions. The triggers and strategies are\noptimized on a surrogate model, enabling DrunkAgent transferable and stealthy.\nExtensive experiments on real-world datasets across diverse agentic RSs,\nincluding collaborative filtering, retrieval augmentation and sequential\nrecommendations, demonstrate the generalizability, transferability and\nstealthiness of DrunkAgent."}
{"id": "2505.15517", "pdf": "https://arxiv.org/pdf/2505.15517.pdf", "abs": "https://arxiv.org/abs/2505.15517", "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets", "authors": ["Kaiyuan Chen", "Shuangyu Xie", "Zehan Ma", "Pannag R Sanketi", "Ken Goldberg"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning."}
{"id": "2505.16975", "pdf": "https://arxiv.org/pdf/2505.16975.pdf", "abs": "https://arxiv.org/abs/2505.16975", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/DorothyDUUU/SWE-Dev}{https://github.com/DorothyDUUU/SWE-Dev}."}
{"id": "2505.20246", "pdf": "https://arxiv.org/pdf/2505.20246.pdf", "abs": "https://arxiv.org/abs/2505.20246", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Shu Zhang", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Yao Xiao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning."}
{"id": "2505.20981", "pdf": "https://arxiv.org/pdf/2505.20981.pdf", "abs": "https://arxiv.org/abs/2505.20981", "title": "RefAV: Towards Planning-Centric Scenario Mining", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Project Page: https://cainand.github.io/RefAV/", "summary": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Lastly, we discuss our recent\nCVPR 2025 competition and share insights from the community. Our code and\ndataset are available at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html"}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147.pdf", "abs": "https://arxiv.org/abs/2506.03147", "title": "UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld-V1, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld-V1\nachieves impressive performance across diverse tasks, including image\nunderstanding, generation, manipulation, and perception. We fully open-source\nthe UniWorld-V1 framework, including model weights, training and evaluation\nscripts, and datasets to promote reproducibility and further research."}
{"id": "2506.05146", "pdf": "https://arxiv.org/pdf/2506.05146.pdf", "abs": "https://arxiv.org/abs/2506.05146", "title": "CIVET: Systematic Evaluation of Understanding in VLMs", "authors": ["Massimo Rizzoli", "Simone Alghisi", "Olha Khomyn", "Gabriel Roccabruna", "Seyed Mahed Mousavi", "Giuseppe Riccardi"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "While Vision-Language Models (VLMs) have achieved competitive performance in\nvarious tasks, their comprehension of the underlying structure and semantics of\na scene remains understudied. To investigate the understanding of VLMs, we\nstudy their capability regarding object properties and relations in a\ncontrolled and interpretable manner. To this scope, we introduce CIVET, a novel\nand extensible framework for systematiC evaluatIon Via controllEd sTimuli.\nCIVET addresses the lack of standardized systematic evaluation for assessing\nVLMs' understanding, enabling researchers to test hypotheses with statistical\nrigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of\nstimuli, free from annotation noise, dataset-specific biases, and uncontrolled\nscene complexity. Our findings reveal that 1) current VLMs can accurately\nrecognize only a limited set of basic object properties; 2) their performance\nheavily depends on the position of the object in the scene; 3) they struggle to\nunderstand basic relations among objects. Furthermore, a comparative evaluation\nwith human annotators reveals that VLMs still fall short of achieving\nhuman-level accuracy."}
{"id": "2506.05333", "pdf": "https://arxiv.org/pdf/2506.05333.pdf", "abs": "https://arxiv.org/abs/2506.05333", "title": "Kinetics: Rethinking Test-Time Scaling Laws", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nand increasingly important with more computing invested, for realizing the full\npotential of test-time scaling where, unlike training, accuracy has yet to\nsaturate as a function of computation, and continues to improve through\nincreased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics."}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707.pdf", "abs": "https://arxiv.org/abs/2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."}
{"id": "2506.13784", "pdf": "https://arxiv.org/pdf/2506.13784.pdf", "abs": "https://arxiv.org/abs/2506.13784", "title": "ScholarSearch: Benchmarking Scholar Searching Ability of LLMs", "authors": ["Junting Zhou", "Wang Li", "Yiyan Liao", "Nengyuan Zhang", "Tingjia Miao", "Zhihui Qi", "Yuhan Wu", "Tong Yang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed ScholarSearch, the first dataset specifically designed to evaluate the\ncomplex information retrieval capabilities of Large Language Models (LLMs) in\nacademic research. ScholarSearch possesses the following key characteristics:\nAcademic Practicality, where question content closely mirrors real academic\nlearning and research environments, avoiding deliberately misleading models;\nHigh Difficulty, with answers that are challenging for single models (e.g.,\nGrok DeepSearch or Gemini Deep Research) to provide directly, often requiring\nat least three deep searches to derive; Concise Evaluation, where limiting\nconditions ensure answers are as unique as possible, accompanied by clear\nsources and brief solution explanations, greatly facilitating subsequent audit\nand verification, surpassing the current lack of analyzed search datasets both\ndomestically and internationally; and Broad Coverage, as the dataset spans at\nleast 15 different academic disciplines. Through ScholarSearch, we expect to\nmore precisely measure and promote the performance improvement of LLMs in\ncomplex academic information retrieval tasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch"}
{"id": "2506.13923", "pdf": "https://arxiv.org/pdf/2506.13923.pdf", "abs": "https://arxiv.org/abs/2506.13923", "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models", "authors": ["Vaskar Nath", "Elaine Lau", "Anisha Gunjal", "Manasi Sharma", "Nikhil Baharte", "Sean Hendryx"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study the process through which reasoning models trained with\nreinforcement learning on verifiable rewards (RLVR) can learn to solve new\nproblems. We find that RLVR drives performance in two main ways: (1) by\ncompressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models\nlearn to solve new problems that they previously could not solve even at high\n$k$. We find that while capability gain exists across model scales, learning to\nsolve new problems is primarily driven through self-distillation. We\ndemonstrate these findings across model scales ranging from 0.5B to 72B\nparameters on >500,000 reasoning problems with prompts and verifiable final\nanswers across math, science, and code domains. We further show that we can\nsignificantly improve pass@$k$ rates by leveraging natural language guidance\nfor the model to consider within context while still requiring the model to\nderive a solution chain from scratch. Based of these insights, we derive\n$\\text{Guide}$ -- a new class of online training algorithms. $\\text{Guide}$\nadaptively incorporates hints into the model's context on problems for which\nall rollouts were initially incorrect and adjusts the importance sampling ratio\nfor the \"off-policy\" trajectories in order to optimize the policy for contexts\nin which the hints are no longer present. We describe variants of\n$\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and\n32B parameter models improves generalization over its vanilla counterpart with\nup to 4$\\%$ macro-average improvement across math benchmarks. We include\ncareful ablations to analyze $\\text{Guide}$'s components and theoretically\nanalyze Guide's learning efficiency."}
{"id": "2506.15538", "pdf": "https://arxiv.org/pdf/2506.15538.pdf", "abs": "https://arxiv.org/abs/2506.15538", "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework", "authors": ["Laura Kopf", "Nils Feldhus", "Kirill Bykov", "Philine Lou Bommer", "Anna Hedstr√∂m", "Marina M. -C. H√∂hne", "Oliver Eberle"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score)."}
{"id": "2506.15677", "pdf": "https://arxiv.org/pdf/2506.15677.pdf", "abs": "https://arxiv.org/abs/2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/."}
