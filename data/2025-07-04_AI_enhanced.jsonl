{"id": "2507.02088", "pdf": "https://arxiv.org/pdf/2507.02088.pdf", "abs": "https://arxiv.org/abs/2507.02088", "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "categories": ["cs.CL"], "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "The paper introduces the Multi-task Chinese Bias Evaluation Benchmark (McBE) to measure biases in large language models (LLMs) with a focus on Chinese culture, covering multiple evaluation tasks and categories.", "motivation": "The need to measure biases in large language models (LLMs) that extend beyond English and North American culture, addressing the scarcity of Chinese language datasets.", "method": "Development of the Multi-task Chinese Bias Evaluation Benchmark (McBE) with 4,077 bias evaluation instances across 12 categories and 82 subcategories, supporting 5 evaluation tasks.", "result": "Popular LLMs were evaluated, revealing varying degrees of bias across different models and parameter sizes.", "conclusion": "McBE provides a comprehensive framework for evaluating biases in LLMs, offering insights into their cultural impacts and ethical considerations.", "key_contributions": ["Introduction of McBE targeting the Chinese language", "Comprehensive coverage of bias categories and tasks", "Novel insights into the bias present in various LLMs."], "limitations": "Focused primarily on the Chinese language; applicability to other languages and cultures not addressed.", "keywords": ["bias evaluation", "large language models", "Chinese culture", "NLP", "ethical AI"], "importance_score": 9, "read_time_minutes": 24}}
{"id": "2507.02145", "pdf": "https://arxiv.org/pdf/2507.02145.pdf", "abs": "https://arxiv.org/abs/2507.02145", "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gon√ßalo Oliveira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "This paper evaluates reasoning and non-reasoning LLMs for dialogue summarization, revealing that explicit stepwise reasoning does not consistently yield better summaries and may lead to verbosity.", "motivation": "To explore the effectiveness of reasoning architectures, specifically Long Chain-of-Thought (CoT) implementations, in dialogue summarization and assess their practical value in real-world scenarios.", "method": "A comprehensive evaluation of reasoning and non-reasoning LLMs across three paradigms of dialogue summarization (generic, role-oriented, query-oriented) using strong benchmarks and rigorous evaluation protocols.", "result": "The study finds that reasoning LLMs often produce more verbose and factually inconsistent summaries, showing no consistent improvement in quality compared to non-reasoning models.", "conclusion": "The results highlight limitations of current reasoning LLMs and underscore the necessity for more tailored modeling and evaluation approaches for dialogue summarization.", "key_contributions": ["First comprehensive evaluation of reasoning LLMs in dialogue summarization.", "Identification of verbosity and factual consistency issues in reasoning LLMs.", "Insights into when explicit reasoning may hinder summarization quality."], "limitations": "Focuses mainly on reasoning LLMs; may not generalize to all types of dialogue interactions or other aspects of conversational AI.", "keywords": ["dialogue summarization", "large language models", "reasoning", "natural language processing", "conversational AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02199", "pdf": "https://arxiv.org/pdf/2507.02199.pdf", "abs": "https://arxiv.org/abs/2507.02199", "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "This paper investigates the emergence of latent chain-of-thought reasoning in the Huginn-3.5B depth-recurrent Transformer, contrasting it with standard models that externalize reasoning steps.", "motivation": "To explore the potential of internalizing reasoning in latent space for transformer models, especially in the context of arithmetic tasks, and understanding the trade-offs between interpretability and efficiency.", "method": "The authors analyzed the internal behavior of the Huginn-3.5B model through various probing techniques, including the Logit Lens and Coda Lens, to examine the model's reasoning structures.", "result": "The study found limited evidence of interpretable latent chain-of-thought reasoning, with significant inconsistencies in probing results across different recurrent blocks. Increasing the model's recurrence depth provided marginal improvements.", "conclusion": "The findings suggest that while recurrent architectures show promise, they do not yet achieve the same level of interpretability as models that externalize reasoning steps, with only slight performance gains from increased recurrence depth.", "key_contributions": ["Investigation of latent chain-of-thought reasoning in depth-recurrent transformers.", "Identification of probing inconsistencies across recurrent blocks.", "Empirical analysis showing marginal gains from increased recurrence depth."], "limitations": "Interpretability of hidden states is highly dependent on layer index and decoding method; limited evidence of effective latent reasoning mechanism observed.", "keywords": ["Chain-of-thought reasoning", "Depth-recurrent Transformer", "Latent space", "Probing techniques", "Interpretability"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.02221", "pdf": "https://arxiv.org/pdf/2507.02221.pdf", "abs": "https://arxiv.org/abs/2507.02221", "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "GDC Cohort Copilot is an open-source tool that allows users to create cancer genomics cohorts by utilizing natural language descriptions, improving usability for new users within the Genomic Data Commons.", "motivation": "To aid users in defining complex cancer genomics cohorts more intuitively using natural language, addressing the challenges of navigating extensive cohort descriptor options.", "method": "Development and evaluation of GDC Cohort Copilot, which employs multiple large language models to convert user-input natural language descriptions into corresponding GDC cohort filters for analysis.", "result": "The GDC Cohort LLM outperforms GPT-4o in generating accurate GDC cohorts, providing a refined interactive user interface for further cohort adjustments.", "conclusion": "GDC Cohort Copilot enhances the accessibility and usability of the Genomic Data Commons for users by simplifying cohort creation through natural language processing.", "key_contributions": ["Introduction of an open-source tool for cohort curation using natural language.", "Comparison and evaluation of multiple LLMs, demonstrating superiority of GDC Cohort LLM over existing models.", "Provision of a user-friendly interface for cohort refinement post-generation."], "limitations": "", "keywords": ["genomics", "natural language processing", "human-computer interaction", "cancer research", "large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.02122", "pdf": "https://arxiv.org/pdf/2507.02122.pdf", "abs": "https://arxiv.org/abs/2507.02122", "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Allen Chang", "Andrew Zhu", "Lyle Ungar", "Sharath Chandra Guntuku"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings.", "AI": {"tldr": "PAL (Palliative Assisted Learning-bot) is a conversational system designed to enhance communication skills in serious illness and palliative care by simulating patient interactions and providing feedback.", "motivation": "The paper addresses the gap in training resources for effective communication in serious illness and palliative care, aiming to enhance skill-building opportunities for medical trainees and clinicians.", "method": "A mixed-methods study was conducted with 17 U.S. medical trainees and clinicians to assess user engagement, usability, and design tensions related to the system.", "result": "Participants reported that PAL was beneficial for reflection and refining communication skills; however, issues with emotional authenticity and feedback adaptability were noted.", "conclusion": "The findings suggest that large language models can effectively support training in palliative communication, while also offering insights for developing emotionally sensitive simulation tools.", "key_contributions": ["Empirical evidence for LLMs in enhancing palliative communication training", "Design insights for modality-aware and emotionally sensitive simulation tools", "Implications for AI-augmented training in high-stakes care settings"], "limitations": "Some participants noted limitations in emotional authenticity and adaptability of feedback.", "keywords": ["palliative care", "communication training", "conversational systems", "empathy", "human-computer interaction"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.02259", "pdf": "https://arxiv.org/pdf/2507.02259.pdf", "abs": "https://arxiv.org/abs/2507.02259", "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "MemAgent optimizes long-text processing by using an overwrite memory strategy, achieving significant performance in extrapolation tasks.", "motivation": "The challenge of processing infinitely long documents efficiently and without performance degradation.", "method": "The paper introduces MemAgent, which reads text in segments and updates memory via an overwrite strategy. It extends the DAPO algorithm for training through independent-context multi-conversation generation.", "result": "MemAgent shows excellent long-context capabilities, effectively handling an 8K context to a 3.5M QA task with less than 5% performance loss and achieves over 95% in the 512K RULER test.", "conclusion": "MemAgent presents a promising approach to long-text tasks, demonstrating both efficiency and high performance.", "key_contributions": ["Introduction of MemAgent for long-text processing", "Use of overwrite memory strategy", "Extension of DAPO for training"], "limitations": "", "keywords": ["long-text processing", "memory optimization", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.02138", "pdf": "https://arxiv.org/pdf/2507.02138.pdf", "abs": "https://arxiv.org/abs/2507.02138", "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy", "authors": ["Shan Li", "Guozhu Ding"], "categories": ["cs.HC"], "comment": null, "summary": "This study introduces and evaluates Healthy Choice, an innovative\ntheory-driven and AI-enhanced simulation platform designed to cultivate\nnutrition literacy through interactive scenario-based learning experiences. We\ncollected feedback from 114 university students with diverse backgrounds who\ncompleted simulated product selection scenarios. Quantitative ratings of\nusefulness and ease of use demonstrated high user satisfaction.", "AI": {"tldr": "Healthy Choice is an AI-enhanced platform for improving nutrition literacy via interactive simulations.", "motivation": "To cultivate nutrition literacy among university students using innovative educational tools.", "method": "Evaluation of user feedback from 114 students after completing product selection scenarios.", "result": "Quantitative ratings showed high satisfaction with the platform's usefulness and ease of use.", "conclusion": "The simulation platform effectively engages students in nutrition education.", "key_contributions": ["Introduction of Healthy Choice as a new educational tool", "Validation through user feedback", "Demonstration of high satisfaction among users"], "limitations": "Limited to university students; generalizability may be constrained.", "keywords": ["Nutrition literacy", "AI in education", "Simulation platform", "Interactive learning", "User satisfaction"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.02302", "pdf": "https://arxiv.org/pdf/2507.02302.pdf", "abs": "https://arxiv.org/abs/2507.02302", "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "DoMIX offers a novel approach to Domain-Adaptive Pre-training (DAP) using LoRA modules to efficiently build tailored pre-trained models while addressing limitations like high computational cost and sensitivity to data order.", "motivation": "The need for efficient and adaptable pre-training methods in machine learning, particularly when dealing with multiple domain datasets.", "method": "DoMIX utilizes LoRA modules to enable parallel domain-adaptive pre-training while maintaining robustness to the order of incremental data.", "result": "The proposed method demonstrates reduced computational costs and improved model performance in providing tailored pre-trained models for specific tasks.", "conclusion": "DoMIX not only addresses current limitations of continual DAP but can also be applied to standard LLM fine-tuning scenarios, showing broad applicability.", "key_contributions": ["Introduces DoMIX for efficient domain-adaptive pre-training using LoRA modules.", "Addresses challenges of incremental data order sensitivity and computational costs.", "Demonstrates applicability of the method beyond DAP to standard LLM fine-tuning."], "limitations": "", "keywords": ["domain-adaptive pre-training", "LoRA", "parameter-efficient fine-tuning", "machine learning", "incremental learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02156", "pdf": "https://arxiv.org/pdf/2507.02156.pdf", "abs": "https://arxiv.org/abs/2507.02156", "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative", "authors": ["Benjamin Watson", "Janet Kim", "Tim McEneany", "Tom Moher", "Claudia Hindo", "Louis Gomez", "Stephen Fransen"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "The StorySpace project studies the role new interface technologies might play\nin high school education. With this approach in mind, StorySpace is\nspecifically designed to support and enhance classroom narrative, an already\nwell-established classroom activity. StorySpace strives to achieve this through\nadherence to three design goals. The first is to trigger student reflection and\ninterpretation. The narrative medium created by StorySpace should represent the\ntopic of classroom discussion and learning in all its complexity. In building\ntheir representation, the students will then be confronted with that same\ncomplexity. The medium should also itself be exciting and compelling, making\nclassroom narrative interesting and fun.", "AI": {"tldr": "The StorySpace project explores new interface technologies for enhancing narrative activities in high school education.", "motivation": "To investigate how interface technologies can enhance the effectiveness of classroom narrative, a standard educational practice.", "method": "Develop a narrative medium that engages students in reflecting on complex topics discussed in class.", "result": "StorySpace aims to create an exciting and compelling narrative medium that makes classroom discussions more interactive and meaningful.", "conclusion": "By confronting students with the complexity of the topics they study, StorySpace enhances their reflection and interpretation skills.", "key_contributions": ["Introduction of an innovative narrative medium for education", "Focus on enhancing student reflection", "Integration of technology in traditional classroom practices"], "limitations": "", "keywords": ["educational technology", "narrative", "high school", "student engagement", "interface design"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.02357", "pdf": "https://arxiv.org/pdf/2507.02357.pdf", "abs": "https://arxiv.org/abs/2507.02357", "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "categories": ["cs.CL"], "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "The paper presents a system for the SciVQA 2025 Shared Task, utilizing an ensemble of Multimodal Large Language Models for scientific visual question answering.", "motivation": "The motivation is to advance the capabilities of AI in interpreting scientific visual data through question answering mechanisms.", "method": "The system uses an ensemble approach combining two Multimodal Large Language Models and few-shot example retrieval strategies, adapting to both figure and question types, and selects answers based on model confidence levels.", "result": "On blind test data, the system achieved third place out of seven participants, with an average F1 score of 85.12 across ROUGE-1, ROUGE-L, and BERTS metrics.", "conclusion": "The results demonstrate the effectiveness of the proposed system in handling scientific visual question answering tasks, with the code made publicly available for further research.", "key_contributions": ["Introduced an ensemble approach for scientific visual question answering using Multimodal Large Language Models.", "Implemented few-shot example retrieval strategies tailored to the question type and figure.", "Achieved a competitive performance ranking in the SciVQA 2025 Shared Task."], "limitations": "", "keywords": ["Visual Question Answering", "Multimodal AI", "Large Language Models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.02180", "pdf": "https://arxiv.org/pdf/2507.02180.pdf", "abs": "https://arxiv.org/abs/2507.02180", "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future", "authors": ["Russell Beale"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.1; K.3.2"], "comment": null, "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users.", "AI": {"tldr": "Review of the impact of large language models (LLMs) on education and educational technology, discussing successes, failures, design challenges, and the evolution of interaction paradigms.", "motivation": "The rapid emergence of LLMs is transforming education and educational technologies, necessitating a review of their applications and implications.", "method": "Review of existing literature and case studies on the use of LLMs in education.", "result": "Identified various successful and failed use cases of LLMs in education, and outlined the evolving dynamics for learners and educators.", "conclusion": "The integration of LLMs is expected to revolutionize educational technology, and thoughtful design considerations are essential to meet changing user expectations.", "key_contributions": ["Analysis of LLM applications in education", "Identification of design challenges for effective educational systems", "Recommendations for future educational technology design"], "limitations": "The review is based on existing literature and may not encompass all recent developments in LLMs and education.", "keywords": ["large language models", "education technology", "design challenges", "learner interaction", "educational paradigms"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02364", "pdf": "https://arxiv.org/pdf/2507.02364.pdf", "abs": "https://arxiv.org/abs/2507.02364", "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "authors": ["Pilsung Kang"], "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "AI": {"tldr": "QFFN-BERT is a hybrid quantum-classical transformer that replaces classical feedforward networks with parameterized quantum circuits (PQCs), achieving improved efficiency and accuracy in NLP tasks.", "motivation": "The study aims to enhance the expressibility and efficiency of transformers by replacing the dominant feedforward network components with PQCs, which are expected to reduce the model size while maintaining performance.", "method": "A hybrid architecture, QFFN-BERT, incorporates PQCs in place of traditional FFN layers, utilizing residual connections, R_Y and R_Z rotations, and an alternating entanglement strategy to facilitate stable training and high expressibility.", "result": "QFFN-BERT surpasses classical models by achieving up to 102.0% of the baseline accuracy while reducing model size by over 99% in FFN parameters. It also demonstrates competitive performance in few-shot learning scenarios.", "conclusion": "The findings validate that PQCs are viable, parameter-efficient alternatives to classical feedforward networks in transformer architectures, especially when integrated with core deep learning principles.", "key_contributions": ["Introduction of QFFN-BERT, a quantum-classical hybrid transformer model.", "Showcase of significant reduction in parameters while retaining or enhancing model performance.", "Empirical validation of PQCs in capturing the expressibility of feedforward networks in NLP applications."], "limitations": "", "keywords": ["quantum circuits", "transformer", "machine learning", "neural networks", "parameter efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.02186", "pdf": "https://arxiv.org/pdf/2507.02186.pdf", "abs": "https://arxiv.org/abs/2507.02186", "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge", "authors": ["Zahra Ashktorab", "Elizabeth M. Daly", "Erik Miehling", "Werner Geyer", "Martin Santillan Cooper", "Tejaswini Pedapati", "Michael Desmond", "Qian Pan", "Hyo Jin Do"], "categories": ["cs.HC"], "comment": null, "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.", "AI": {"tldr": "EvalAssist is a framework that streamlines the workflow of using large language models (LLMs) as evaluators for assessing outputs and developing evaluation criteria.", "motivation": "The need to efficiently evaluate large language model outputs for varied tasks due to increasing model availability and complexity.", "method": "The paper introduces EvalAssist, an interactive online environment for creating, testing, and sharing custom evaluation criteria, combined with pipelines utilizing LLMs and prompt chaining.", "result": "EvalAssist simplifies the evaluation process by providing structured, portable criteria and offering LLM-based pipelines for assessment.", "conclusion": "The system has been successfully deployed in an organization, serving hundreds of users to enhance LLM evaluation workflows.", "key_contributions": ["Introduction of EvalAssist framework for LLM evaluation", "Development of an interactive criteria building environment", "Integration of LLM-based evaluation pipelines and specialized evaluators for harm detection."], "limitations": "", "keywords": ["large language models", "evaluation framework", "human evaluators", "prompt chaining", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.02378", "pdf": "https://arxiv.org/pdf/2507.02378.pdf", "abs": "https://arxiv.org/abs/2507.02378", "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "AI": {"tldr": "This paper presents a method for improving code generation and program comprehension in large language models (LLMs) by selecting high-quality code data, resulting in enhanced training efficiency and model performance.", "motivation": "Current approaches in software engineering focus too much on data quantity while neglecting data quality, which hampers training efficiency for LLMs in code generation.", "method": "The proposed approach utilizes a parametric model for code data selection, optimizing it to ensure consistency and diversity in the selected subset of data.", "result": "Experimental results show that using only 10K samples, the method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over a baseline of 92K samples, indicating superior performance and efficiency.", "conclusion": "The presented approach significantly boosts model performance while reducing computational costs due to improved data selection techniques.", "key_contributions": ["Introduction of a parametric model for code data selection", "Emphasis on data quality over quantity in training", "Demonstrated performance gains with reduced sample size"], "limitations": "", "keywords": ["large language models", "code generation", "data selection", "software engineering", "training efficiency"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.02187", "pdf": "https://arxiv.org/pdf/2507.02187.pdf", "abs": "https://arxiv.org/abs/2507.02187", "title": "VergeIO: Depth-Aware Eye Interaction on Glasses", "authors": ["Xiyuxing Zhang", "Duc Vu", "Chengyi Shen", "Yuntao Wang", "Yuanchun Shi", "Justin Chan"], "categories": ["cs.HC"], "comment": null, "summary": "There is growing industry interest in creating unobtrusive designs for\nelectrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and\nApple eyewear). We present VergeIO, the first EOG-based glasses that enables\ndepth-aware eye interaction using vergence with an optimized electrode layout\nand novel smart glass prototype. It can distinguish between four and six\ndepth-based eye gestures with 83-98% accuracy using personalized models in a\nuser study across 11 users and 1,320 gesture instances. It generalizes to\nunseen users with an accuracy of 80-98% without any calibration. To reduce\nfalse detections, we incorporate a motion artifact detection pipeline and a\npreamble-based activation scheme. The system uses dry sensors without any\nadhesives or gel, and operates in real time with 3 mW power consumption by the\nsensing front-end, making it suitable for always-on sensing.", "AI": {"tldr": "VergeIO is an EOG-based glasses prototype that enables depth-aware eye interaction with high accuracy in detecting eye gestures.", "motivation": "To create unobtrusive designs for EOG sensing of eye gestures on glasses, responding to industry interest in such technologies.", "method": "The VergeIO system uses an optimized electrode layout for EOG sensing and employs personalized models for recognizing multiple depth-based eye gestures with high accuracy across users, integrating a motion artifact detection pipeline and preamble-based activation.", "result": "It successfully distinguishes between four and six depth-based eye gestures with an accuracy of 83-98% in user studies and generalizes to unseen users with an accuracy of 80-98% without calibration.", "conclusion": "The system is feasible for real-time use with low power consumption and does not require adhesives for sensor attachment, making it suitable for continuous user interaction.", "key_contributions": ["First EOG-based glasses for depth-aware eye interaction", "High accuracy in distinguishing depth-based gestures", "Real-time operation with low power consumption"], "limitations": "", "keywords": ["EOG", "eye gestures", "smart glasses", "depth-aware interaction", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.02407", "pdf": "https://arxiv.org/pdf/2507.02407.pdf", "abs": "https://arxiv.org/abs/2507.02407", "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "AI": {"tldr": "This paper benchmarks seven Akan ASR models built on transformer architectures to assess their performance across diverse speech contexts, revealing domain dependence and error behaviors that inform architectural selection for low-resource languages.", "motivation": "To evaluate the generalization of automatic speech recognition (ASR) models across diverse speech contexts rather than just in-domain datasets.", "method": "Benchmarking seven transformer-based Akan ASR models using performance comparison across four distinct Akan speech corpora.", "result": "The study found significant domain dependency in ASR model performance, with varied error behaviors between Whisper and Wav2Vec2 architectures.", "conclusion": "The results highlight the necessity of domain adaptation techniques and multilingual training for effective performance in low-resource language applications.", "key_contributions": ["Benchmarking diverse Akan ASR models", "Identification of domain dependency in ASR performance", "Insights on error behaviors of different architectures"], "limitations": "The study focuses only on Akan language models; broader implications for other low-resource languages are not fully explored.", "keywords": ["Automatic Speech Recognition", "Akan Language", "Transformer Models", "Domain Adaptation", "Low-Resource Languages"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.02229", "pdf": "https://arxiv.org/pdf/2507.02229.pdf", "abs": "https://arxiv.org/abs/2507.02229", "title": "An Exploration of Internal States in Collaborative Problem Solving", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Mariah Bradford", "Anne M. Cleary", "Nathaniel Blanchard"], "categories": ["cs.HC"], "comment": "Accepted to the International Conference on Human-Computer\n  Interaction (HCII) 2025", "summary": "Collaborative problem solving (CPS) is a complex cognitive, social, and\nemotional process that is increasingly prevalent in educational and\nprofessional settings. This study investigates the emotional states of\nindividuals during CPS using a mixed-methods approach. Teams of four first\ncompleted a novel CPS task. Immediately after, each individual was placed in an\nisolated room where they reviewed the video of their group performing the task\nand self-reported their internal experiences throughout the task. We performed\na linguistic analysis of these internal monologues, providing insights into the\nrange of emotions individuals experience during CPS. Our analysis showed\ndistinct patterns in language use, including characteristic unigrams and\nbigrams, key words and phrases, emotion labels, and semantic similarity between\nemotion-related words.", "AI": {"tldr": "This study explores emotional states during collaborative problem solving (CPS) using mixed methods, analyzing participants' linguistic patterns in self-reported experiences after group tasks.", "motivation": "To understand the emotional dynamics of individuals in collaborative problem-solving contexts, which is essential in educational and professional settings.", "method": "Teams completed a CPS task, and individuals then reviewed group performance videos and provided self-reports of their emotional experiences, which were analyzed linguistically.", "result": "Distinct language patterns were identified in participants' reports, including unique unigrams, bigrams, and semantic similarities in emotion-related words.", "conclusion": "The findings reveal the complexity of emotional experiences during CPS and highlight the importance of language use in understanding these experiences.", "key_contributions": ["Introduces a novel analysis of emotional states during CPS.", "Utilizes a mixed-methods approach combining task performance and linguistic analysis.", "Identifies specific linguistic patterns related to emotional experiences in collaborative settings."], "limitations": "The study is limited to a specific CPS task and may not generalize across different contexts.", "keywords": ["Collaborative problem solving", "Emotional states", "Linguistic analysis", "Human-Computer Interaction", "Mixed methods"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.02428", "pdf": "https://arxiv.org/pdf/2507.02428.pdf", "abs": "https://arxiv.org/abs/2507.02428", "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "categories": ["cs.CL"], "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "AI": {"tldr": "This study develops a methodology for collecting speech samples to improve Automatic Speech Recognition (ASR) models for impaired speech in low-resource languages, specifically creating an open-source dataset in Akan.", "motivation": "To democratize ASR technology and provide a framework for community-driven data collection and model building for impaired speech in low-resource languages.", "method": "The study curated an open-source dataset of impaired speech in Akan and developed a cookbook of best practices for community data collection and ASR model training.", "result": "The dataset includes diverse speech samples from participants with speech impairments, and initial fine-tuning of ASR models has shown improved recognition of impaired speech in Akan.", "conclusion": "The open-source dataset and tools aim to facilitate the development of inclusive ASR technologies and are publicly available for researchers and practitioners.", "key_contributions": ["Development of a curriculum for community-driven data collection and ASR modeling.", "Creation of the first open-source dataset of impaired speech in Akan.", "Initial results of fine-tuning ASR models to recognize impaired speech more effectively."], "limitations": "", "keywords": ["Automatic Speech Recognition", "impaired speech", "Akan language", "dataset", "community-driven"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.02254", "pdf": "https://arxiv.org/pdf/2507.02254.pdf", "abs": "https://arxiv.org/abs/2507.02254", "title": "A framework for 3D interaction techniques", "authors": ["Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a software architecture for 3D interaction techniques\n(ITs) and an object oriented, toolkit-independent framework that implements\nsuch architecture. ITs are composed of basic filters connected in a dataflow,\nwhere virtual input devices and objects in the scene are sources of\ninformation. An execution model defines the general flow of information between\nfilters. This framework has been designed to be extensible: new information\ntypes, new input devices, new execution models, or new interaction techniques\ncan easily be added. Application specific code and application specific ITs are\nseamlessly integrated into this architecture.", "AI": {"tldr": "The paper introduces a flexible software architecture for implementing 3D interaction techniques via a toolkit-independent framework that allows easy integration of new input devices and techniques.", "motivation": "To improve the development and integration of 3D interaction techniques in applications by providing a modular and extensible software architecture.", "method": "The proposed framework utilizes an object-oriented approach where interaction techniques are structured as a network of filters in a dataflow, facilitating the integration of various input sources and execution models.", "result": "The architecture allows for the easy addition of new information types, input devices, and interaction techniques, thus enhancing flexibility and adaptability in application development.", "conclusion": "The proposed framework supports seamless integration of application-specific code and interaction techniques while being highly extensible.", "key_contributions": ["Introduction of a modular software architecture for 3D ITs", "Toolkit-independent design enabling flexibility", "Easy integration of diverse input devices and techniques"], "limitations": "", "keywords": ["3D interaction techniques", "software architecture", "dataflow", "object-oriented", "extensible framework"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.02506", "pdf": "https://arxiv.org/pdf/2507.02506.pdf", "abs": "https://arxiv.org/abs/2507.02506", "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "AI": {"tldr": "Introduction of a benchmark dataset for legal NLP focused on Indian bail judgments.", "motivation": "To address the underdevelopment of legal NLP in regions like India due to a lack of structured datasets.", "method": "Creation of IndianBailJudgments-1200, a dataset of 1200 Indian court bail judgment annotations using a prompt-engineered GPT-4o pipeline, followed by consistency verification.", "result": "The dataset supports various legal NLP tasks including outcome prediction and fairness analysis, being the first of its kind in Indian bail jurisprudence.", "conclusion": "IndianBailJudgments-1200 offers a valuable resource for advancing legal NLP applications in India.", "key_contributions": ["Introduction of a new dataset for Indian bail judgments", "Annotations covering over 20 attributes relevant to legal NLP", "First publicly available resource focused on Indian bail jurisprudence"], "limitations": "", "keywords": ["Legal NLP", "Dataset", "Indian court judgments", "Bail decisions", "AI applications"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.02283", "pdf": "https://arxiv.org/pdf/2507.02283.pdf", "abs": "https://arxiv.org/abs/2507.02283", "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness", "authors": ["Tim Rogers", "Ben Teehankee"], "categories": ["cs.HC", "I.2.6; H.1.2"], "comment": "21 pages", "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values.", "AI": {"tldr": "The paper investigates how Large Language Models (LLMs) may amplify existing misalignments in human theories of action, potentially reinforcing ineffective problem-solving in organizational contexts. It explores the implications for AI alignment and suggests ways to develop LLMs that promote more effective learning.", "motivation": "To address the unexplored aspect of AI alignment concerning how LLMs can perpetuate human cognitive biases in organizational learning.", "method": "A case study of an LLM functioning as an HR consultant, analyzing its advice and its reinforcement of defensive reasoning patterns typical of existing theories-in-use.", "result": "The LLM's advice, while appearing professional, perpetuates unproductive problem-solving approaches and obstructs meaningful organizational learning due to inherited cognitive blind spots.", "conclusion": "Developing LLMs that can facilitate Model 2 learning could improve AI alignment and assist in fostering deeper learning among humans.", "key_contributions": ["Examining the alignment problem from the perspective of cognitive biases in human theories-in-use.", "Detailed case study illustrating how LLMs can amplify ineffective organizational practices.", "Suggestions for creating LLMs that promote better learning outcomes."], "limitations": "", "keywords": ["AI alignment", "Large Language Models", "organizational learning", "human cognition", "action science"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2507.02592", "pdf": "https://arxiv.org/pdf/2507.02592.pdf", "abs": "https://arxiv.org/abs/2507.02592", "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "AI": {"tldr": "WebSailor is a post-training methodology that enhances LLM capabilities by teaching them to reduce uncertainty in information-seeking tasks, achieving performance comparable to proprietary systems.", "motivation": "The need to overcome human cognitive limitations in LLM training and improve the performance of open-source models in complex information-seeking tasks.", "method": "The methodology involves generating high-uncertainty tasks via structured sampling and information obfuscation, along with an RL training algorithm called Duplicating Sampling Policy Optimization (DUPO).", "result": "WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, closely matching the performance of proprietary agents.", "conclusion": "Implementing the WebSailor methodology allows open-source LLMs to compete with proprietary systems by equipping them with advanced reasoning capabilities for navigating complex information.", "key_contributions": ["Introduction of WebSailor methodology to enhance LLM performance", "Demonstration of systematic uncertainty reduction in LLM training", "Development of Duplicating Sampling Policy Optimization (DUPO) algorithm"], "limitations": "", "keywords": ["LLM training", "information-seeking", "WebSailor", "uncertainty reduction", "agentic systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02300", "pdf": "https://arxiv.org/pdf/2507.02300.pdf", "abs": "https://arxiv.org/abs/2507.02300", "title": "Human-Centered Explainability in Interactive Information Systems: A Survey", "authors": ["Yuhao Zhang", "Jiaxin An", "Ben Wang", "Yan Zhang", "Jiqun Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Human-centered explainability has become a critical foundation for the\nresponsible development of interactive information systems, where users must be\nable to understand, interpret, and scrutinize AI-driven outputs to make\ninformed decisions. This systematic survey of literature aims to characterize\nrecent progress in user studies on explainability in interactive information\nsystems by reviewing how explainability has been conceptualized, designed, and\nevaluated in practice. Following PRISMA guidelines, eight academic databases\nwere searched, and 100 relevant articles were identified. A structural encoding\napproach was then utilized to extract and synthesize insights from these\narticles. The main contributions include 1) five dimensions that researchers\nhave used to conceptualize explainability; 2) a classification scheme of\nexplanation designs; 3) a categorization of explainability measurements into\nsix user-centered dimensions. The review concludes by reflecting on ongoing\nchallenges and providing recommendations for future exploration of related\nissues. The findings shed light on the theoretical foundations of\nhuman-centered explainability, informing the design of interactive information\nsystems that better align with diverse user needs and promoting the development\nof systems that are transparent, trustworthy, and accountable.", "AI": {"tldr": "This survey reviews user studies on explainability in interactive information systems, identifying key conceptualizations, designs, and evaluations from 100 articles.", "motivation": "To ensure users can understand and scrutinize AI outputs for informed decision-making in interactive information systems.", "method": "A systematic survey following PRISMA guidelines, analyzing 100 relevant articles from eight academic databases using structural encoding to extract insights.", "result": "Identified five dimensions of explainability, a classification scheme of explanation designs, and six user-centered dimensions of explanation measurements.", "conclusion": "The review highlights challenges and provides recommendations for future research, emphasizing the need for transparency and user alignment in AI systems.", "key_contributions": ["Five dimensions to conceptualize explainability", "Classification scheme of explanation designs", "Categorization of explainability measurements into six user-centered dimensions"], "limitations": "The review focuses on existing literature and may not capture emerging concepts in explainability.", "keywords": ["human-centered explainability", "interactive information systems", "user studies", "AI-driven outputs", "system transparency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593.pdf", "abs": "https://arxiv.org/abs/2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G√∂ran Kauermann", "Barbara Plank", "Matthias A√üenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "AI": {"tldr": "The paper discusses the integration of human label variation (HLV) in active learning to optimize data annotation in supervised learning, especially in NLP.", "motivation": "Access to high-quality labeled data is critical in supervised learning, and the paper aims to address the common oversights regarding human label variation (HLV) in labeling frameworks.", "method": "The authors propose a conceptual framework to decompose label variation into informative signal (HLV) and noise, incorporating HLV through the active learning process.", "result": "The proposed framework offers insights into instance selection, annotator choice, and label representation that can improve the active learning loop in the presence of HLV.", "conclusion": "Incorporating HLV in active learning practices can lead to more accurate and relevant labels, ultimately enhancing the effectiveness of ML models in real-world scenarios.", "key_contributions": ["Introduces a conceptual framework for HLV-aware active learning.", "Suggests methods to integrate HLV into instance selection and annotator choice.", "Highlights the role of large language models as potential annotators."], "limitations": "The paper primarily provides a conceptual framework, which may require empirical validation in various real-world scenarios.", "keywords": ["active learning", "human label variation", "supervised learning", "annotation frameworks", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02306", "pdf": "https://arxiv.org/pdf/2507.02306.pdf", "abs": "https://arxiv.org/abs/2507.02306", "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Usability evaluation is crucial in human-centered design but can be costly,\nrequiring expert time and user compensation. In this work, we developed a\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\nanalyze images and provide design feedback. Comparing our synthetic evaluations\nto those by experienced UX practitioners across two apps, we found our\nevaluation identified 73% and 77% of usability issues, which exceeded the\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\nevaluators, the synthetic evaluation's performance maintained consistent\nperformance across tasks and excelled in detecting layout issues, highlighting\npotential attentional and perceptual strengths of synthetic evaluation.\nHowever, synthetic evaluation struggled with recognizing some UI components and\ndesign conventions, as well as identifying across screen violations.\nAdditionally, testing synthetic evaluations over time and accounts revealed\nstable performance. Overall, our work highlights the performance differences\nbetween human and LLM-driven evaluations, informing the design of synthetic\nheuristic evaluations.", "AI": {"tldr": "This paper presents a method for synthetic heuristic evaluation using multimodal LLMs to analyze images and provide design feedback, achieving superior usability issue identification compared to human evaluators.", "motivation": "To address the high costs and resource requirements of traditional usability evaluations in human-centered design by leveraging LLMs for synthetic evaluations.", "method": "The authors developed a synthetic heuristic evaluation method utilizing multimodal LLMs that analyze images to provide feedback, and compared its effectiveness with evaluations by experienced UX practitioners.", "result": "The synthetic evaluations identified 73% and 77% of usability issues in two apps, outperforming experienced human evaluators who identified 57% and 63%.", "conclusion": "While synthetic evaluations showed strengths in detecting usability issues, especially layout issues, they also had limitations in recognizing certain UI components and identifying across-screen violations.", "key_contributions": ["Development of synthetic heuristic evaluation using multimodal LLMs", "Demonstrated superior performance in usability issue identification compared to human evaluators", "Analysis of strengths and limitations of LLM-driven evaluations"], "limitations": "Struggled to recognize certain UI components and design conventions; difficulties in identifying across screen violations.", "keywords": ["usability evaluation", "human-centered design", "LLM", "synthetic evaluation", "user experience"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.02595", "pdf": "https://arxiv.org/pdf/2507.02595.pdf", "abs": "https://arxiv.org/abs/2507.02595", "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025 AIW Workshop", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.", "AI": {"tldr": "Multiperspective Fusion (MPF) is a framework for aligning large language models (LLMs) to mitigate bias by utilizing multiperspective generations to match humanlike baselines.", "motivation": "Address the increasing demand for effective bias mitigation in large language models (LLMs).", "method": "MPF utilizes the SAGED pipeline to construct bias benchmarks and extract interpretable baseline distributions. It decomposes these baselines into perspective components to guide LLM responses through sampling and balancing based on decomposed probabilities.", "result": "MPF enables alignment of LLM sentiment distributions with both counterfactual baselines and HR professional baselines, achieving small KL divergence, reduced calibration error, and generalization to unseen queries.", "conclusion": "MPF provides a scalable and interpretable solution for bias alignment in LLMs that does not require extensive prompt engineering or finetuning.", "key_contributions": ["Introduction of Multiperspective Fusion (MPF) for bias mitigation in LLMs.", "Utilization of the SAGED pipeline for benchmark construction and bias extraction.", "Empirical validation of MPF's effectiveness in aligning sentiment distributions."], "limitations": "", "keywords": ["Multiperspective Fusion", "Bias Mitigation", "Large Language Models", "SAGED Pipeline", "Sentiment Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02350", "pdf": "https://arxiv.org/pdf/2507.02350.pdf", "abs": "https://arxiv.org/abs/2507.02350", "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance", "authors": ["Hao Tang", "Songyun Xie", "Xinzhou Xie", "Can Liao", "Xin Zhang", "Bohan Li", "Zhongyu Tian", "Dalu Zheng"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional video-induced emotion physiological datasets often use\nwhole-trial annotation, assigning a single emotion label to all data collected\nduring an entire trial. This coarse-grained annotation approach misaligns with\nthe dynamic and temporally localized nature of emotional responses as they\nunfold with video narratives, introducing label noise that limits emotion\nrecognition algorithm evaluation and performance. To solve the label noise\nproblem caused by coarse-grained annotation, we propose a fine-grained\nannotation method through an immediate recall paradigm. This paradigm\nintegrates an immediate video replay phase after the initial stimulus viewing,\nallowing participants to precisely mark the onset timestamp, emotion label, and\nintensity based on their immediate recall. We validate this paradigm through\nphysiological evidence and recognition performance. Physiological validation of\nmultimodal signals within participant-marked windows revealed rhythm-specific\nEEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of\nhigh-arousal versus 6% of low-arousal emotion windows. These objective\nphysiological data changes strongly aligned with subjective annotations,\nconfirming annotation precision. For recognition performance, classification\nexperiments showed that models trained on fine-grained annotations achieved\n9.7% higher accuracy than traditional whole-trial labeling, despite using less\ndata. This work not only addresses label noise through fine-grained annotation\nbut also demonstrates that annotation precision outweighs data scale in\ndetermining emotion recognition performance.", "AI": {"tldr": "This paper proposes a fine-grained annotation method for video-induced emotion recognition, addressing label noise from coarse-grained methods by employing an immediate recall paradigm.", "motivation": "Traditional whole-trial annotation methods in emotion datasets misalign with the dynamic nature of emotional responses, leading to label noise that affects algorithm performance.", "method": "The proposed method uses an immediate video replay phase after initial viewing, allowing participants to mark onset timestamps, emotion labels, and intensities accurately based on immediate recall.", "result": "Physiological evidence showed significant EEG and GSR signal changes aligned with subjective annotations. Classification models trained on fine-grained annotations achieved 9.7% higher accuracy than those using whole-trial labels.", "conclusion": "The study highlights the importance of annotation precision over data scale for enhancing emotion recognition performance.", "key_contributions": ["Development of a fine-grained annotation method for emotions in video stimuli", "Validation of the method through physiological evidence and recognition performance", "Demonstration of improved accuracy in emotion recognition models using fine-grained annotations"], "limitations": "", "keywords": ["emotion recognition", "fine-grained annotation", "physiological signals", "multimodal data", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02679", "pdf": "https://arxiv.org/pdf/2507.02679.pdf", "abs": "https://arxiv.org/abs/2507.02679", "title": "Exploring Gender Bias Beyond Occupational Titles", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.", "AI": {"tldr": "This paper explores gender biases in contextual elements like verbs and nouns, using a novel dataset and framework to evaluate and explain these biases.", "motivation": "To investigate the correlation between gender and contextual biases, particularly focusing on action verbs, object nouns, and occupations.", "method": "Introduced a novel dataset called GenderLexicon and developed a framework for estimating and interpreting contextual bias and associated gender bias scores.", "result": "The model demonstrated the existence of gender biases beyond just occupational stereotypes, validated through evaluations on five diverse datasets, including a Japanese dataset.", "conclusion": "The findings enhance explainability of gender bias in language, indicating broader biases in contexts.", "key_contributions": ["Introduction of the GenderLexicon dataset", "Development of a framework to estimate contextual and gender bias", "Validation on multiple datasets illustrating broader implications of gender biases"], "limitations": "", "keywords": ["gender bias", "contextual bias", "natural language processing", "dataset", "explainability"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.02432", "pdf": "https://arxiv.org/pdf/2507.02432.pdf", "abs": "https://arxiv.org/abs/2507.02432", "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset", "authors": ["Jueun Lee", "Dennis Moschina", "Supraja Ramesh", "Tobias R√∂ddiger", "Kai Kunze", "Michael Beigl"], "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "We investigate the use of musically structured, closed-loop vibration\npatterns as a passive biofeedback intervention for relaxation and sleep\ninitiation. By encoding rhythmic meter structures into smartwatch vibrations\nand adapting their frequency to be slightly slower than the user's real-time\nheart rate, our system aims to reduce arousal through tactile entrainment,\noffering a non-invasive alternative to auditory or open-loop approaches\npreviously used in sleep and anxiety contexts. In the first study (N=20), we\ncompared five adaptive vibration rhythms for their effects on heart rate and\nsubjective perceptions of relaxation in a resting context. In the second study\n(N=28), we evaluated the most promising pattern from Study 1 in a prolonged\nsleep initiation setting. Results showed increased parasympathetic activity and\nperceived relaxation during short-term stimulation, but no significant effects\non sleep-related measures during the sleep onset phase. This work contributes\nto the understanding of how wearable haptic feedback can support relaxation and\nsleep, offering design insights and identifying methodological considerations\nfor effectively integrating haptic interaction into self-directed\ninterventions.", "AI": {"tldr": "The paper explores haptic feedback through smartwatch vibrations as a means to promote relaxation and aid in sleep initiation, comparing the effects of different rhythmic patterns on heart rate and relaxation perceptions.", "motivation": "To investigate a non-invasive method for relaxation and sleep initiation using wearable technology, particularly focusing on haptic feedback as an alternative to audio-based interventions.", "method": "Two studies were conducted: the first involved comparing five adaptive vibration rhythms on heart rate and relaxation perceptions in a resting context (N=20), and the second evaluated the best pattern from Study 1 in a prolonged sleep setting (N=28).", "result": "The adaptive haptic feedback increased parasympathetic activity and perceived relaxation during short-term stimulation but showed no significant impact on sleep initiation measures during the sleep phase.", "conclusion": "The findings suggest the potential of wearable haptic feedback for relaxation, while also highlighting design insights and methodological considerations for future interventions.", "key_contributions": ["Introduced musically structured vibration patterns for relaxation and sleep initiation.", "Demonstrated the effectiveness of haptic feedback in enhancing parasympathetic activity.", "Provided insights for future design and integration of haptic interactions in self-directed interventions."], "limitations": "Limited impact on sleep-related measures during the onset phase and small sample sizes in both studies.", "keywords": ["haptic feedback", "sleep initiation", "relaxation", "wearable technology", "biofeedback"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.02694", "pdf": "https://arxiv.org/pdf/2507.02694.pdf", "abs": "https://arxiv.org/abs/2507.02694", "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.CL"], "comment": null, "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.", "AI": {"tldr": "The paper discusses LimitGen, a benchmark for evaluating LLMs' capabilities in aiding peer review by identifying limitations in scientific papers.", "motivation": "To address challenges in the peer review process due to the increasing volume of publications and to explore the role of LLMs in improving peer review.", "method": "Development of a comprehensive taxonomy of limitation types and creation of LimitGen, a benchmark with synthetic and real datasets, combined with literature retrieval augmentation for LLM systems.", "result": "LimitGen effectively improves LLMs' ability to identify and generate limitations in research papers, enhancing peer review feedback.", "conclusion": "Augmenting LLM systems with literature retrieval significantly boosts their capability in generating constructive limitations for research, thus supporting human peer review.", "key_contributions": ["Creation of a comprehensive taxonomy of limitation types in scientific research.", "Introduction of LimitGen benchmark for evaluating LLM peer review support.", "Integration of literature retrieval to enhance LLM performance in identifying research limitations."], "limitations": "", "keywords": ["peer review", "LLM", "taxonomy", "LimitGen", "literature retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.02453", "pdf": "https://arxiv.org/pdf/2507.02453.pdf", "abs": "https://arxiv.org/abs/2507.02453", "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?", "authors": ["Jueun Lee", "Martin Flipe", "Philipp Lepold", "Tobias R√∂ddiger", "Michael Beigl"], "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "Wearable haptic interventions offer promising support for relaxation through\nslow, vibrotactile biofeedback. Despite their potential, current applications\nfocus on stress-inducing procedures and fixed vibration patterns, with limited\nconsideration of body location and dynamic biofeedback during restful states.\nThis study investigates the effects of haptic biofeedback adjusted from\nreal-time heart rate during eyes-closed wakeful rest, comparing four wearable\nbody placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave\nactivity on the ear, subjective restfulness, and vibration experience were\nmeasured across these conditions. Results show that biofeedback reduced heart\nrate at the wrist, shoulder, and forearm, while alpha power measured at the ear\nremained unchanged. Subjective restfulness was rated highest at the shoulder\nand forearm, which were also the most preferred locations. In addition,\nparticipants reported greater comfort, relaxation, and further increased\nsleepiness at the forearm compared to the wrist, which was more easily\nrecognizable. These findings suggest that the forearm and shoulder are ideal\nfor unobtrusive relaxation feedback for wakeful rest, while the wrist may\nrequire design improvements for subjective experience.", "AI": {"tldr": "This study explores the use of dynamic haptic biofeedback for relaxation, comparing different body placements for wearable devices.", "motivation": "To investigate the effects of real-time heart rate adjusted haptic biofeedback on relaxation using wearable devices, addressing current limitations in stress-inducing applications.", "method": "Participants were provided with wearable devices delivering vibrotactile feedback based on real-time heart rate while resting with eyes closed. Four body locations (wrist, hand, forearm, shoulder) were tested, measuring heart rate, alpha wave activity, subjective restfulness, and vibration experience.", "result": "Biofeedback significantly reduced heart rate at the wrist, shoulder, and forearm, with the highest subjective restfulness ratings for shoulder and forearm. Forearm placement provided greater comfort and relaxation compared to wrist.", "conclusion": "The forearm and shoulder are optimal placements for haptic feedback aimed at relaxation, while the wrist may need improvements in design for better user experience.", "key_contributions": ["Demonstrates effectiveness of haptic biofeedback for relaxation", "Identifies optimal body placements for wearable devices", "Highlights subjective experiences associated with different feedback locations"], "limitations": "Study limited to healthy participants; findings may not generalize to clinical populations.", "keywords": ["haptic biofeedback", "wearable devices", "relaxation", "heart rate", "user experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02744", "pdf": "https://arxiv.org/pdf/2507.02744.pdf", "abs": "https://arxiv.org/abs/2507.02744", "title": "Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens", "authors": ["Peter Viechnicki"], "categories": ["cs.CL"], "comment": null, "summary": "A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes.", "AI": {"tldr": "This study measures the 'Just Producible Difference' (JPD) in vowel imitation among English speakers to understand the accuracy of vowel production control mechanisms.", "motivation": "To investigate the accuracy of control mechanisms governing human vowel production in auditory space and its implications for speech production theories.", "method": "The study utilizes a vowel mimicry paradigm to measure the Just Producible Difference (JPD) for front vowel production in English speakers, estimating JPD in F1 X F2 space.", "result": "JPD is estimated to range between 14 and 51 mels, providing the first measurement of this phenomenon.", "conclusion": "The findings have implications for episodic theories of speech production and clarify the structures of human vowel systems by setting a theoretical lower bound for vowel phoneme proximity in speakers' formant space.", "key_contributions": ["First measurement of Just Producible Difference (JPD) in vowel stimuli.", "Provides insights into control mechanisms of vowel production.", "Clarifies theoretical lower bounds for vowel phoneme proximity."], "limitations": "", "keywords": ["vowel production", "Just Producible Difference", "auditory space", "speech production", "psychophysics"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.02537", "pdf": "https://arxiv.org/pdf/2507.02537.pdf", "abs": "https://arxiv.org/abs/2507.02537", "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "authors": ["Paulo Ricardo Knob", "Leonardo Scholler", "Juliano Rigatti", "Soraia Raupp Musse"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.", "AI": {"tldr": "The study investigates how Large Language Models (LLMs) generate emotionally rich conversations, focusing on empathetic listening in dialogues.", "motivation": "With the integration of conversational agents in various fields, including healthcare, there is a growing need for emotional intelligence, particularly empathetic interactions, to enhance user experiences.", "method": "The study began with a small expert-crafted dataset to reflect empathic behavior and extended the dialogues using two LLMs: ChatGPT and Gemini. Emotional progression was analyzed through sentiment analysis and human evaluations.", "result": "The findings indicated that while the conversations matched the intended emotional structure, there were notable discrepancies in perceived empathy and coherence, as assessed by human evaluators.", "conclusion": "Emotion modeling in dialogues needs to focus on both structural alignment of emotions and qualitative depth, suggesting the importance of combining automated methods with human-centered approaches.", "key_contributions": ["Investigation of empathetic listening in LLMs", "Analysis of emotional progression in generated dialogues", "Combination of automated and human-centered methods for emotion modeling in agents"], "limitations": "The study may depend on the quality of the initial expert-crafted dataset and may not generalize beyond the tested LLMs and contexts.", "keywords": ["Conversational agents", "emotional intelligence", "Large Language Models", "empathy", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.02778", "pdf": "https://arxiv.org/pdf/2507.02778.pdf", "abs": "https://arxiv.org/abs/2507.02778", "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs", "authors": ["Ken Tsui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "31 pages, 18 figures", "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.", "AI": {"tldr": "This paper introduces Self-Correction Bench to study the Self-Correction Blind Spot in LLMs, revealing a 64.5% failure rate to correct errors in their own outputs, with potential for improvement by specific prompts.", "motivation": "To address the systematic failure of large language models (LLMs) to self-correct errors in their outputs, which affects their reliability and trustworthiness.", "method": "The authors created a framework called Self-Correction Bench to systematically measure LLMs' self-correction capabilities through controlled error injection at three complexity levels across 14 models.", "result": "The study found an average self-correction blind spot rate of 64.5%, with a significant correlation to training data composition and a notable improvement of 89.3% in blind spots by appending specific prompts such as 'Wait'.", "conclusion": "This work underscores a critical limitation in current LLMs and suggests targeted prompts can activate self-correction capabilities, paving the way for enhanced reliability and trustworthiness in LLM applications.", "key_contributions": ["Introduction of Self-Correction Bench for evaluating LLMs' self-correction capabilities.", "Discovery of a high average blind spot rate of 64.5% in LLM outputs.", "Identification of potential activation methods for improving error correction."], "limitations": "The findings may be limited to the specific models tested and the nature of controlled error injections used in the study.", "keywords": ["self-correction", "large language models", "error correction", "trustworthiness", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.02682", "pdf": "https://arxiv.org/pdf/2507.02682.pdf", "abs": "https://arxiv.org/abs/2507.02682", "title": "A wireless, inexpensive optical tracker for the CAVE", "authors": ["Ehud Sharlin", "Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "CAVE displays offer many advantages over other virtual reality (VR) displays,\nincluding a large, unencumbering viewing space. Unfortunately, the typical\ntracking subsystems used with CAVE displays tether the user and lessen this\nadvantage. We have designed a simple, low-cost feet tracker that is wireless,\nleaving the user free to move. The tracker can be assembled for less than $200\nUS, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested\nthe prototype with two applications: a visualization supporting close visual\ninspection, and a walkthrough of the campus. Although the tracking was\nconvincing, it was clear that the tracker's limitations make it less than ideal\nfor applications requiring precise visual inspection. However, the freedom of\nmotion allowed by the tracker was a compelling supplement to our campus\nwalkthrough, allowing users to stroll and look around corners.", "AI": {"tldr": "A low-cost wireless feet tracker for CAVE displays enhances user freedom of movement but has limitations in precision for certain applications.", "motivation": "To address the limitations of tethered tracking subsystems in CAVE displays, which restrict user movement and reduce the immersive experience of VR.", "method": "Designed and built a wireless feet tracker for less than $200, achieving an accuracy of 10 cm at a 20 Hz sampling rate; prototype tested with two applications: visualization for close inspection and campus walkthrough.", "result": "The tracker provided convincing tracking during tests. It performed well in enabling free movement for the campus walkthrough but was inadequate for applications needing precise visual inspection.", "conclusion": "While the wireless feet tracker enhances user mobility in VR, its limitations in precision need to be addressed for applications requiring high accuracy.", "key_contributions": ["Development of a low-cost wireless feet tracker", "Demonstration of freedom of movement in VR environments", "Evaluation of the tracker in multiple practical applications"], "limitations": "Less than ideal for precise visual inspection applications due to accuracy limitations.", "keywords": ["CAVE displays", "virtual reality", "feet tracker", "wireless tracking", "user movement"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2507.02799", "pdf": "https://arxiv.org/pdf/2507.02799.pdf", "abs": "https://arxiv.org/abs/2507.02799", "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models", "authors": ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.", "AI": {"tldr": "This paper investigates the impact of Reasoning Language Models (RLMs) on bias elicitation, revealing that increased reasoning capabilities may actually make models more vulnerable to social biases.", "motivation": "To explore how reasoning capabilities in RLMs affect their robustness to biases, particularly in safety mechanisms.", "method": "Using the CLEAR-Bias benchmark, the authors evaluated various state-of-the-art RLMs with an LLM-as-a-judge approach for automated safety scoring, examining the effectiveness of reasoning and bias elicitation resistance against jailbreak techniques.", "result": "The study found that reasoning-enabled models are often more susceptible to bias elicitation compared to base models, questioning the assumption that reasoning always enhances model robustness.", "conclusion": "Despite expectations, reasoning capabilities may inadvertently increase vulnerabilities to social bias, necessitating more bias-aware reasoning designs.", "key_contributions": ["Introduces a systematic evaluation of RLMs against bias elicitation using the CLEAR-Bias benchmark.", "Demonstrates that reasoning capabilities can worsen vulnerability to social biases.", "Highlights the need for bias-aware approaches in the design of reasoning mechanisms."], "limitations": "The study mainly focuses on the adversarial robustness of RLMs concerning social biases and may not encompass all potential biases or contexts.", "keywords": ["Reasoning Language Models", "bias elicitation", "safety mechanisms", "Chain-of-Thought prompting", "robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02745", "pdf": "https://arxiv.org/pdf/2507.02745.pdf", "abs": "https://arxiv.org/abs/2507.02745", "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots", "authors": ["Zahra Ashktorab", "Alessandra Buccella", "Jason D'Cruz", "Zoe Fowler", "Andrew Gill", "Kei Yan Leung", "P. D. Magnus", "John Richards", "Kush R. Varshney"], "categories": ["cs.HC"], "comment": null, "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.", "AI": {"tldr": "The study investigates user preferences for different types of apologies (rote, explanatory, empathic) given by LLM-driven chatbots after they make mistakes, highlighting the impact of context and user emotions on these preferences.", "motivation": "To understand how chatbot apologies affect user trust and satisfaction in various error scenarios as LLM-driven chatbots become more common.", "method": "A pairwise experiment was conducted with 162 participants evaluating chatbot responses consisting of an error, an apology, and a resolution.", "result": "Explanatory apologies were generally preferred, but preferences varied by context; empathic apologies were favored in bias scenarios, while responses to hallucinations showed no clear preference.", "conclusion": "The findings reveal the complexity of effective apologies in AI systems, emphasizing the importance of personalization and context to repair user trust.", "key_contributions": ["Identification of user preferences for different types of chatbot apologies", "Analysis of how context affects the effectiveness of apologies", "Insights into the relationship between user emotions and apology preferences"], "limitations": "The study is limited to specific types of errors and contexts, which may not encompass all possible user interactions with chatbots.", "keywords": ["chatbots", "apologies", "large language models", "human-computer interaction", "user trust"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.02804", "pdf": "https://arxiv.org/pdf/2507.02804.pdf", "abs": "https://arxiv.org/abs/2507.02804", "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective", "authors": ["Wenhao Shi", "Zhiqiang Hu", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Heng Tao Shen"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.", "AI": {"tldr": "Introducing MathV-DP, a dataset for diverse solution trajectories in multimodal mathematical reasoning, and Qwen-VL-DP, a model that leverages this dataset with advanced reinforcement learning techniques.", "motivation": "To improve mathematical reasoning in multimodal large language models by incorporating diverse reasoning perspectives and solution trajectories.", "method": "The paper introduces MathV-DP, a dataset that includes multiple solution paths for each image-question pair, and proposes the Qwen-VL-DP model, which integrates traditional supervised learning with group relative policy optimization to optimize for accuracy and diversity in solutions.", "result": "Qwen-VL-DP outperforms existing multimodal LLMs in terms of accuracy and generative diversity on the MathVista's minitest and Math-V benchmarks.", "conclusion": "Fostering diverse perspectives and reflective reasoning is crucial for enhancing multimodal mathematical reasoning capabilities in large language models.", "key_contributions": ["Introduction of the MathV-DP dataset for diverse solution paths", "Development of the Qwen-VL-DP model utilizing group relative policy optimization", "Demonstrated significant performance improvements on mathematical benchmarks"], "limitations": "", "keywords": ["multimodal LLMs", "reinforcement learning", "mathematical reasoning", "diversity in solutions", "dataset"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.02800", "pdf": "https://arxiv.org/pdf/2507.02800.pdf", "abs": "https://arxiv.org/abs/2507.02800", "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding", "authors": ["Ebrahim Feghhi", "Shreyas Kaasyap", "Nima Hadidi", "Jonathan C. Kao"], "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Speech neuroprostheses aim to restore communication for people with severe\nparalysis by decoding speech directly from neural activity. To accelerate\nalgorithmic progress, a recent benchmark released intracranial recordings from\na paralyzed participant attempting to speak, along with a baseline decoding\nalgorithm. Prior work on the benchmark showed impressive accuracy gains.\nHowever, these gains increased computational costs and were not demonstrated in\na real-time decoding setting. Here, we make three contributions that pave the\nway towards accurate, efficient, and real-time neural speech decoding. First,\nwe incorporate large amounts of time masking during training. On average, over\n$50\\%$ of each trial is masked. Second, we replace the gated recurrent unit\n(GRU) architecture used in the baseline algorithm with a compact Transformer.\nThe Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory\nusage by $36\\%$ relative, and is significantly faster to calibrate relative to\nthe GRU. Third, we design a lightweight variant of an existing test-time\nadaptation method developed for decoding handwriting from neural activity. Our\nvariant adapts the model using multiple time masked augmentations of a single\ntrial and requires only one gradient step per trial. Together, these\ncontributions reduce word error rate by $19.5\\%$ and effectively mitigate\nperformance degradations across held-out days in a real-time decoding setting\nwhile substantially lowering computational costs.", "AI": {"tldr": "This paper presents advancements in neural speech decoding through enhanced training methods, a novel Transformer architecture, and a lightweight adaptation technique, achieving reduced error rates and computational costs.", "motivation": "To improve real-time speech neuroprostheses by enhancing decoding accuracy and efficiency while managing computational costs.", "method": "Incorporation of extensive time masking during training, implementation of a compact Transformer architecture in place of GRU, and development of a lightweight adaptation method for real-time decoding.", "result": "The proposed methods resulted in a 19.5% reduction in word error rate and improved performance consistency across held-out days in real-time settings.", "conclusion": "These contributions provide a framework for efficient and accurate neural speech decoding, paving the way for practical applications in speech neuroprostheses.", "key_contributions": ["Use of large time masking during training", "Replacement of GRU with a compact Transformer architecture", "Development of a lightweight test-time adaptation variant"], "limitations": "", "keywords": ["speech neuroprostheses", "real-time decoding", "neural speech decoding", "Transformer architecture", "time masking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02822", "pdf": "https://arxiv.org/pdf/2507.02822.pdf", "abs": "https://arxiv.org/abs/2507.02822", "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model", "authors": ["Wencheng Zhang", "Shiqin Qiao", "Lingjie Luo", "Yinfeng Li", "Chuanyang Zheng", "Qian Xu", "Meng Li", "Yong Gui", "Yijun He", "Jianing Qiu", "Jindong Hong", "Jiankai Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.", "AI": {"tldr": "This paper explores a dynamic routing framework, SynapseRoute, to improve the operational efficiency of LLMs in responding to medical queries by categorizing them into 'thinking' or 'non-thinking' modes based on complexity.", "motivation": "To optimize the selection of large language models for medical queries by balancing performance and operational cost, particularly highlighting the effectiveness of non-thinking modes.", "method": "The paper presents SynapseRoute, a machine learning-based dynamic routing framework that categorizes input queries into either thinking or non-thinking modes based on their complexity.", "result": "SynapseRoute improves overall accuracy to 0.8390, reduces inference time by 36.8%, and token consumption by 39.66% compared to using the thinking mode alone.", "conclusion": "The introduction of an adaptive routing mechanism can enhance user experience by preventing unnecessary computational overhead on simpler queries while ensuring higher accuracy in responses.", "key_contributions": ["Proposed SynapseRoute for dynamic query routing in medical applications", "Demonstrated improved accuracy and reduced costs in model inference", "Introduced the Accuracy-Inference-Token (AIT) index for evaluating trade-offs"], "limitations": "", "keywords": ["large language models", "dynamic routing", "medical queries", "machine learning", "cost efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.02819", "pdf": "https://arxiv.org/pdf/2507.02819.pdf", "abs": "https://arxiv.org/abs/2507.02819", "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": null, "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction.", "AI": {"tldr": "The paper explores how data scientists in education and healthcare construct proxy target variables for predictive modeling, emphasizing a bricolage process that balances measurement objectives and practical constraints.", "motivation": "To understand how data scientists translate fuzzy concepts into concrete target variables for predictive modeling tasks, focusing on the challenges faced in education and healthcare domains.", "method": "Interviews with fifteen data scientists (8 in education and 7 in healthcare) were conducted to explore their processes for constructing target variables.", "result": "Data scientists employ a bricolage approach to construct target variables, iteratively negotiating between high-level objectives and practical constraints, and aiming to meet five criteria: validity, simplicity, predictability, portability, and resource requirements.", "conclusion": "Future research in HCI, CSCW, and ML can be directed towards enhancing the support systems for data scientists in constructing meaningful target variables.", "key_contributions": ["Identifies key criteria for effective target variable construction.", "Describes a bricolage process used by data scientists.", "Highlights specific challenges and opportunities in HCI and ML for supporting target variable development."], "limitations": "", "keywords": ["target variables", "predictive modeling", "bricolage", "HCI", "data science"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.02833", "pdf": "https://arxiv.org/pdf/2507.02833.pdf", "abs": "https://arxiv.org/abs/2507.02833", "title": "Generalizing Verifiable Instruction Following", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.", "AI": {"tldr": "This paper introduces IFBench, a new benchmark for evaluating the precise instruction following capabilities of language models under diverse output constraints.", "motivation": "To address the challenge of language models struggling with precise instruction following and generalizing to unseen output constraints.", "method": "We create IFBench, consisting of 58 diverse out-of-domain constraints and perform a detailed analysis on model training. We implement constraint verification modules and apply reinforcement learning with verifiable rewards (RLVR) to enhance performance.", "result": "RLVR significantly improves the ability of language models to generalize and fulfill diverse instruction constraints.", "conclusion": "The introduction of IFBench and the reinforcement learning approach can improve the performance of language models in following precise instructions, indicating a path forward for enhancing human-AI interaction.", "key_contributions": ["Introduction of IFBench for evaluating instruction following", "Release of 29 new hand-annotated training constraints", "Development of constraint verification modules and RLVR training methods"], "limitations": "", "keywords": ["instruction following", "language models", "benchmark", "reinforcement learning", "human-AI interaction"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.02850", "pdf": "https://arxiv.org/pdf/2507.02850.pdf", "abs": "https://arxiv.org/abs/2507.02850", "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "authors": ["Almog Hilel", "Idan Shenfeld", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).", "AI": {"tldr": "The paper identifies a vulnerability in language models (LMs) trained with user feedback, demonstrating how a single user can manipulate LM behavior by affecting its knowledge through feedback actions. This can result in harmful outputs including misinformation and security flaws.", "motivation": "To highlight the risks associated with user feedback mechanisms in language models and the potential for malicious manipulation.", "method": "An experimental approach where the LM is prompted to produce either 'poisoned' or benign responses, followed by feedback to influence the model's output tendencies.", "result": "The research shows that feedback manipulation can lead to LMs generating poisoned responses even in non-malicious contexts, revealing their susceptibility to user input.", "conclusion": "User feedback systems in LMs are vulnerable to manipulation, necessitating improved safeguards against such attacks to ensure model integrity and reliability.", "key_contributions": ["Identification of a new attack mechanism on language models through preference tuning.", "Demonstration of how user feedback can be weaponized to alter LM behavior significantly.", "Evidence that even restricted feedback can lead to substantial degradation of LM outputs."], "limitations": "The study primarily focuses on qualitative outcomes and may require further quantitative analysis to fully understand the extent of the vulnerability.", "keywords": ["Language Models", "User Feedback", "Vulnerability", "Preference Tuning", "Security Flaws"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02851", "pdf": "https://arxiv.org/pdf/2507.02851.pdf", "abs": "https://arxiv.org/abs/2507.02851", "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "comment": null, "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.", "AI": {"tldr": "This paper presents MOTIF, a modular thinking strategy for LLMs using reinforcement learning to extend reasoning capabilities beyond context size constraints through multiple round token generation.", "motivation": "To improve the reasoning capabilities of large language models beyond the constraints of fixed context sizes.", "method": "The proposed method, MOTIF, utilizes a group relative policy optimization algorithm for reinforcement learning and enables multi-round token generation, enhancing the model's reasoning ability with additional context.", "result": "The experiments demonstrated a 3.8% and 3.3% improvement in accuracy over vanilla GRPO based training on MATH500 and AIME2024 benchmarks, respectively, using only 15% of the training samples.", "conclusion": "MOTIF demonstrates sample efficiency and improved reasoning capabilities for LLMs, making it a valuable approach for enhancing model performance in challenging reasoning tasks.", "key_contributions": ["Introduction of MOTIF for modular thinking in LLMs", "Demonstrated improvements in reasoning tasks using less training data", "Open-source implementation available for further research"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Modular Thinking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02856", "pdf": "https://arxiv.org/pdf/2507.02856.pdf", "abs": "https://arxiv.org/abs/2507.02856", "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching", "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.", "AI": {"tldr": "This paper critiques traditional multiple choice benchmarks for evaluating language models and proposes a new generative evaluation method, answer matching, which demonstrates superior agreement with human grading.", "motivation": "To address the fundamental limitations of multiple choice evaluations in language model assessment, which can often be answered without seeing the questions.", "method": "The authors introduce a method called answer matching, where a language model generates a free-form answer to a question without options, and a second model checks if this answer aligns with a reference answer.", "result": "Answer matching, even with small models, achieves near-perfect agreement with human grading, improving upon the poor alignment of both multiple choice evaluation and LLM-as-a-judge approaches.", "conclusion": "The transition from multiple choice to generative evaluation through answer matching could significantly improve the quality of language model assessments and change model ranking.", "key_contributions": ["Introduction of answer matching as a scalable alternative to multiple choice evaluation.", "Demonstration of improved alignment with human grading through generative evaluation techniques.", "Analysis of how model rankings can change when evaluated by answer matching versus traditional means."], "limitations": "", "keywords": ["language model evaluation", "answer matching", "generative evaluation", "human grading", "multiple choice benchmarks"], "importance_score": 8, "read_time_minutes": 34}}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593.pdf", "abs": "https://arxiv.org/abs/2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G√∂ran Kauermann", "Barbara Plank", "Matthias A√üenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "AI": {"tldr": "This paper discusses the need for incorporating human label variation (HLV) into active learning (AL) frameworks to improve the effectiveness of machine learning models trained on labeled data.", "motivation": "Access to high-quality labeled data is a limiting factor in supervised learning, and existing frameworks often ignore human label variation, which could be a significant informative signal.", "method": "The authors propose a conceptual framework to incorporate HLV into the active learning process, addressing how to decompose label variation into meaningful signals and noise.", "result": "The paper surveys existing literature on label variation and active learning, highlighting gaps in current methodologies and proposing strategies for HLV inclusion.", "conclusion": "Integrating HLV into active learning can enhance annotation quality and better represent real-world complexities in machine learning tasks.", "key_contributions": ["Conceptual framework for HLV-aware active learning.", "Discussion on the integration of large language models as annotators.", "Survey of how current AL and (H)LV communities address or neglect distinctions in label variation."], "limitations": "The framework proposed is conceptual and may require empirical validation in practical scenarios.", "keywords": ["active learning", "human label variation", "machine learning", "annotation", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.16206", "pdf": "https://arxiv.org/pdf/2407.16206.pdf", "abs": "https://arxiv.org/abs/2407.16206", "title": "Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts", "authors": ["Michikuni Eguchi", "Tomohiro Hayase", "Yuichi Hiroi", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "dataset: https://doi.org/10.6084/m9.figshare.29438288 code:\n  https://github.com/cluster-lab/Cluster-Haptic-Texture-Database", "summary": "Haptic sciences and technologies benefit greatly from comprehensive datasets\nthat capture tactile stimuli under controlled, systematic conditions. However,\nexisting haptic databases collect data through uncontrolled exploration, which\nhinders the systematic analysis of how motion parameters (e.g., motion\ndirection and velocity) influence tactile perception. This paper introduces\nCluster Haptic Texture Database, a multimodal dataset recorded using a 3-axis\nmachine with an artificial finger to precisely control sliding velocity and\ndirection. The dataset encompasses 118 textured surfaces across 9 material\ncategories, with recordings at 5 velocity levels (20-60 mm/s) and 8 directions.\nEach surface was tested under 160 conditions, yielding 18,880 synchronized\nrecordings of audio, acceleration, force, position, and visual data. Validation\nusing convolutional neural networks demonstrates classification accuracies of\n96% for texture recognition, 88.76% for velocity estimation, and 78.79% for\ndirection estimation, confirming the dataset's utility for machine learning\napplications. This resource enables research in haptic rendering, texture\nrecognition algorithms, and human tactile perception mechanisms, supporting the\ndevelopment of realistic haptic interfaces for virtual reality systems and\nrobotic applications.", "AI": {"tldr": "The paper presents the Cluster Haptic Texture Database, a multimodal dataset for haptic research that records tactile stimuli under controlled conditions, facilitating machine learning applications in haptic rendering and texture recognition.", "motivation": "The need for a comprehensive dataset that captures tactile stimuli systematically, overcoming limitations of existing haptic data collection methods.", "method": "Data was recorded using a 3-axis machine with an artificial finger, assessing 118 textured surfaces under controlled motion parameters including 5 velocity levels and 8 directions, resulting in 18,880 synchronized recordings of various data types.", "result": "Convolutional neural networks achieved high classification accuracies: 96% for texture recognition, 88.76% for velocity estimation, and 78.79% for direction estimation, validating the dataset's effectiveness for machine learning.", "conclusion": "The Cluster Haptic Texture Database supports research into haptic rendering and human tactile perception, contributing to advancements in virtual reality and robotic applications.", "key_contributions": ["Introduction of a systematic dataset for haptic research", "High classification accuracies validate its effectiveness for machine learning", "Facilitates the development of realistic haptic interfaces"], "limitations": "", "keywords": ["haptic technology", "dataset", "texture recognition", "machine learning", "human tactile perception"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2410.08723", "pdf": "https://arxiv.org/pdf/2410.08723.pdf", "abs": "https://arxiv.org/abs/2410.08723", "title": "Human-Computer Interaction and Visualization in Natural Language Generation Models: Applications, Challenges, and Opportunities", "authors": ["Yunchao Wang", "Guodao Sun", "Zihang Fu", "Ronghua Liang"], "categories": ["cs.HC"], "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50356-6}", "summary": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs).", "AI": {"tldr": "The paper explores enhancing the interpretability of natural language generation (NLG) models through human-computer interaction (HCI) and visualization techniques.", "motivation": "NLG models exhibit excellent performance in NLP tasks but lack interpretability, hindering use in critical decision-making. HCI and visualization can improve transparency and usability.", "method": "A comprehensive investigation, including a taxonomy of interaction methods and visualization techniques across three research domains related to NLG.", "result": "Identified roles, limitations, and impacts of HCI and visualization in NLG, along with a taxonomy and key tasks in these domains.", "conclusion": "The paper highlights existing shortcomings in HCI applications for NLG and discusses challenges and opportunities presented by large language models.", "key_contributions": ["Taxonomy of interaction methods and visualization techniques for NLG", "Identification of six key tasks for applying NLG and HCI", "Discussion on the impact of LLMs on NLG interpretability"], "limitations": "Existing work lacks comprehensive strategies to improve NLG interpretability effectively.", "keywords": ["Natural Language Generation", "Human-Computer Interaction", "Visualization", "Large Language Models", "Interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2306.13840", "pdf": "https://arxiv.org/pdf/2306.13840.pdf", "abs": "https://arxiv.org/abs/2306.13840", "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data", "authors": ["Brando Miranda", "Alycia Lee", "Sudharsan Sundar", "Allison Casasola", "Rylan Schaeffer", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.", "AI": {"tldr": "The paper introduces the diversity coefficient as a formal measure of data quality in pre-training Large Language Models, demonstrating its empirical relevance to model evaluation performance.", "motivation": "To rigorously characterize the nebulous concept of data quality in pre-training LLMs, focusing particularly on the variability of natural language data.", "method": "Proposes a diversity coefficient to measure the variability of natural language data; performs empirical analysis on this measure and controlled interventional experiments with GPT-2 and LLaMAv2 across 44 models.", "result": "The diversity coefficient aligns with properties of diversity and variability, showing that pre-training datasets have high formal diversity, which correlates with downstream model evaluation performance.", "conclusion": "The formal notion of diversity as measured by the diversity coefficient is a crucial element of data quality, leading to improved evaluation performance in downstream tasks.", "key_contributions": ["Introduction of the diversity coefficient for measuring data quality", "Empirical validation of the diversity coefficient's relevance to model evaluation performance", "Comprehensive experiments demonstrating high diversity in pre-training datasets"], "limitations": "", "keywords": ["Large Language Models", "data quality", "diversity coefficient", "pre-training datasets", "model evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2311.08010", "pdf": "https://arxiv.org/pdf/2311.08010.pdf", "abs": "https://arxiv.org/abs/2311.08010", "title": "Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning", "authors": ["Shuzheng Si", "Helan Hu", "Haozhe Zhao", "Shuang Zeng", "Kaikai An", "Zefan Cai", "Baobao Chang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2024 (Findings)", "summary": "Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in\nreal-world scenarios. It can effectively alleviate the burden of annotation by\nmatching entities in existing knowledge bases with snippets in the text but\nsuffer from the label noise. Recent works attempt to adopt the teacher-student\nframework to gradually refine the training labels and improve the overall\nrobustness. However, these teacher-student methods achieve limited performance\nbecause the poor calibration of the teacher network produces incorrectly\npseudo-labeled samples, leading to error propagation. Therefore, we propose:\n(1) Uncertainty-Aware Teacher Learning that leverages the prediction\nuncertainty to reduce the number of incorrect pseudo labels in the\nself-training stage; (2) Student-Student Collaborative Learning that allows the\ntransfer of reliable labels between two student networks instead of\nindiscriminately relying on all pseudo labels from its teacher, and further\nenables a full exploration of mislabeled samples rather than simply filtering\nunreliable pseudo-labeled samples. We evaluate our proposed method on five\nDS-NER datasets, demonstrating that our method is superior to the\nstate-of-the-art DS-NER methods.", "AI": {"tldr": "The paper proposes a novel approach to improve Distantly-Supervised Named Entity Recognition (DS-NER) by addressing label noise through Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning.", "motivation": "The motivation behind this work is to enhance the performance of Distantly-Supervised Named Entity Recognition (DS-NER) by addressing the issues of label noise which degrades model performance.", "method": "The authors introduce Uncertainty-Aware Teacher Learning to minimize incorrect pseudo labels during self-training and Student-Student Collaborative Learning to facilitate the transfer of reliable labels between student networks.", "result": "The proposed method outperforms state-of-the-art DS-NER methods across five different datasets, showcasing its efficacy in reducing label noise and improving training outcomes.", "conclusion": "The paper concludes that leveraging prediction uncertainty and collaborative learning strategies leads to better label refinement and improved robustness in DS-NER.", "key_contributions": ["Uncertainty-Aware Teacher Learning to enhance label quality", "Student-Student Collaborative Learning for reliable label transfer", "Comprehensive evaluation on multiple DS-NER datasets demonstrating superior performance"], "limitations": "", "keywords": ["Distantly-Supervised Named Entity Recognition", "label noise", "teacher-student framework", "uncertainty-aware learning", "collaborative learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2311.14727", "pdf": "https://arxiv.org/pdf/2311.14727.pdf", "abs": "https://arxiv.org/abs/2311.14727", "title": "Optimal strategies to perform multilingual analysis of social content for a novel dataset in the tourism domain", "authors": ["Maxime Masson", "Rodrigo Agerri", "Christian Sallaberry", "Marie-Noelle Bessagnet", "Annig Le Parc Lacayrelle", "Philippe Roose"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rising influence of social media platforms in various domains, including\ntourism, has highlighted the growing need for efficient and automated Natural\nLanguage Processing (NLP) strategies to take advantage of this valuable\nresource. However, the transformation of multilingual, unstructured, and\ninformal texts into structured knowledge still poses significant challenges,\nmost notably the never-ending requirement for manually annotated data to train\ndeep learning classifiers. In this work, we study different NLP techniques to\nestablish the best ones to obtain competitive performances while keeping the\nneed for training annotated data to a minimum. To do so, we built the first\npublicly available multilingual dataset (French, English, and Spanish) for the\ntourism domain, composed of tourism-related tweets. The dataset includes\nmultilayered, manually revised annotations for Named Entity Recognition (NER)\nfor Locations and Fine-grained Thematic Concepts Extraction mapped to the\nThesaurus of Tourism and Leisure Activities of the World Tourism Organization,\nas well as for Sentiment Analysis at the tweet level. Extensive experimentation\ncomparing various few-shot and fine-tuning techniques with modern language\nmodels demonstrate that modern few-shot techniques allow us to obtain\ncompetitive results for all three tasks with very little annotation data: 5\ntweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named\nEntity Recognition of Locations and 1K tweets annotated with fine-grained\nthematic concepts, a highly fine-grained sequence labeling task based on an\ninventory of 315 classes. We believe that our results, grounded in a novel\ndataset, pave the way for applying NLP to new domain-specific applications,\nreducing the need for manual annotations and circumventing the complexities of\nrule-based, ad-hoc solutions.", "AI": {"tldr": "This paper presents a novel multilingual dataset for tourism-related tweets and evaluates NLP techniques to minimize the need for manually annotated data while achieving competitive performance in key tasks.", "motivation": "The need for efficient NLP strategies to process multilingual, unstructured, and informal texts from social media, particularly in the tourism domain, while addressing the challenge of manual data annotation.", "method": "The authors created the first publicly available multilingual dataset for tourism, containing tweets with multilayered annotations for NER, thematic concepts extraction, and sentiment analysis. Various few-shot and fine-tuning techniques were experimentally compared using modern language models.", "result": "The experimentation showed that few-shot techniques yielded competitive results across tasks with minimal annotated data: 5 tweets for sentiment analysis, 30 for NER, and 1K for thematic concepts, demonstrating the dataset's utility.", "conclusion": "The findings indicate that NLP can be successfully applied to specific domains like tourism, thereby reducing the reliance on manual annotations and overcoming complexities of traditional methods.", "key_contributions": ["Creation of the first multilingual dataset for tourism-related tweets", "Demonstration of effective few-shot techniques for NLP tasks", "Reduction of manual annotation requirements for domain-specific applications"], "limitations": "", "keywords": ["Natural Language Processing", "multilingual datasets", "tourism", "few-shot learning", "sentiment analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2406.07016", "pdf": "https://arxiv.org/pdf/2406.07016.pdf", "abs": "https://arxiv.org/abs/2406.07016", "title": "Delving into LLM-assisted writing in biomedical publications through excess vocabulary", "authors": ["Dmitry Kobak", "Rita Gonz√°lez-M√°rquez", "Em≈ëke-√Ågnes Horv√°t", "Jan Lause"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.SI"], "comment": "v5: Reverting to v3", "summary": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.", "AI": {"tldr": "The study analyzes the impact of large language models (LLMs) on biomedical research writing, revealing significant usage trends from 2010-2024 through vocabulary changes in over 15 million abstracts.", "motivation": "To assess the prevalence and impact of LLMs on scholarly writing in biomedical research and understand how these models are influencing language use in academic literature.", "method": "Analysis of vocabulary changes in over 15 million biomedical abstracts indexed by PubMed from 2010 to 2024 to detect patterns associated with LLM usage.", "result": "The study found that at least 13.5% of 2024 abstracts indicated processing by LLMs, with variation by discipline, country, and journal; some subcorpora reached up to 40%.", "conclusion": "LLMs have significantly influenced scientific writing in biomedical research, surpassing the effects of major events like the Covid pandemic.", "key_contributions": ["Unbiased large-scale analysis of LLM usage in biomedical literature", "Quantification of LLM influence on vocabulary across disciplines and journals", "Identification of differential impacts based on geography and publication types."], "limitations": "", "keywords": ["large language models", "biomedical research", "scientific writing", "text analysis", "PubMed"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.01119", "pdf": "https://arxiv.org/pdf/2408.01119.pdf", "abs": "https://arxiv.org/abs/2408.01119", "title": "Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer", "authors": ["Robert Belanec", "Simon Ostermann", "Ivan Srba", "Maria Bielikova"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.", "AI": {"tldr": "Introducing Task Prompt Vectors to enhance soft-prompt tuning for multi-task performance in large language models.", "motivation": "Current soft-prompt methods compromise multi-task training efficiency, necessitating repeated training for new tasks.", "method": "Task Prompt Vectors are created by taking the element-wise difference between tuned soft-prompts and their random initialization, enabling arithmetic operations across tasks.", "result": "Experimental results demonstrate that Task Prompt Vectors allow effective initialization for prompt tuning in low-resource scenarios and are consistent across different model architectures.", "conclusion": "Task Prompt Vectors provide a viable alternative to state-of-the-art baselines, enhancing efficiency and modularity in multi-task learning with LLMs.", "key_contributions": ["Introduction of Task Prompt Vectors", "Demonstration of efficiency in low-resource settings", "Support for arithmetic operations across multiple tasks"], "limitations": "", "keywords": ["prompt tuning", "multi-task learning", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.12532", "pdf": "https://arxiv.org/pdf/2410.12532.pdf", "abs": "https://arxiv.org/abs/2410.12532", "title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration", "authors": ["Dingkang Yang", "Jinjie Wei", "Mingcheng Li", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junjun He", "Yakun Ju", "Wei Zhou", "Yang Liu", "Lihua Zhang"], "categories": ["cs.CL"], "comment": "LLM-based Multi-Agent Collaboration for Medical Applications", "summary": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning.", "AI": {"tldr": "This paper presents MedAide, an LLM-based framework for medical multi-agent collaboration aimed at improving healthcare decision-making through intent-aware information fusion and coordinated reasoning.", "motivation": "The need for reliable decision-making systems in healthcare that can integrate multi-intent information from diverse clinical sources while addressing issues of redundancy and performance bottlenecks in current LLMs.", "method": "MedAide employs a regularization-guided module for query decomposition, a dynamic intent prototype matching module for intent recognition, and a rotation agent collaboration mechanism to enhance information fusion.", "result": "MedAide shows improved performance on four medical benchmarks, outpacing current LLMs in medical proficiency and strategic reasoning as evaluated by automated metrics and expert doctors.", "conclusion": "The framework demonstrates significant promise in enhancing LLM-driven healthcare applications, addressing complex medical intents effectively.", "key_contributions": ["Regularization-guided module for structured query representations", "Dynamic intent prototype matching for multi-round healthcare dialogues", "Rotation agent collaboration for decision-level information fusion"], "limitations": "", "keywords": ["LLM", "Medical Applications", "Multi-Agent Systems", "Information Fusion", "Healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.13808", "pdf": "https://arxiv.org/pdf/2410.13808.pdf", "abs": "https://arxiv.org/abs/2410.13808", "title": "De-mark: Watermark Removal in Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks.", "AI": {"tldr": "De-mark is a framework developed to effectively remove n-gram-based watermarks from language models using a novel querying strategy called random selection probing.", "motivation": "The paper addresses the gap in exploring the robustness of watermarking techniques in machine-generated content.", "method": "The proposed framework, De-mark, employs a random selection probing strategy to evaluate watermark strength and identify specific n-gram watermarks.", "result": "Experiments conducted on Llama3 and ChatGPT show that De-mark efficiently removes and exploits watermarks from these models.", "conclusion": "De-mark is effective in watermark removal and can enhance our understanding of watermark robustness in language models.", "key_contributions": ["Introduction of the De-mark framework for watermark removal", "Utilization of random selection probing for assessing watermark strength", "Demonstration of effectiveness on Llama3 and ChatGPT"], "limitations": "The exploration is primarily focused on n-gram-based watermarks; other watermarking techniques are not covered.", "keywords": ["watermarking", "language models", "n-gram", "De-mark", "random selection probing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2411.00863", "pdf": "https://arxiv.org/pdf/2411.00863.pdf", "abs": "https://arxiv.org/abs/2411.00863", "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation", "authors": ["Chenyang An", "Shima Imani", "Feng Yao", "Chengyu Dong", "Ali Abbasi", "Harsh Shrivastava", "Samuel Buss", "Jingbo Shang", "Gayathri Mahalingam", "Pramod Sharma", "Maurice Diesendruck"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.", "AI": {"tldr": "This paper critiques the ordering of training data in LLM-based proof generation, proposing an 'intuitively sequential order' that enhances learning and improves proof success rates.", "motivation": "To investigate the suboptimal performance of LLMs in proof generation tasks, linking it to inefficient data ordering in training samples.", "method": "The authors validate their claims through two proof generation tasks: intuitionistic propositional logic theorem-proving and digit multiplication, comparing models trained on different data orders.", "result": "An 11 percent improvement in proof success rate is observed in the propositional logic theorem-proving task when models are trained using the intuitively sequential order compared to the worst-case scenario.", "conclusion": "Training is most effective when employing intuitively sequential ordering, highlighting a significant issue with the current data ordering practices in math proofs.", "key_contributions": ["Introduction of intuitively sequential order for proof training data", "Demonstration of substantial performance improvements in LLMs with optimized data ordering", "Identification of common order issues in advanced math proofs"], "limitations": "Focus on specific proof tasks; broader implications on other domains are not discussed.", "keywords": ["Large Language Models", "Proof Generation", "Data Ordering"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.11556", "pdf": "https://arxiv.org/pdf/2412.11556.pdf", "abs": "https://arxiv.org/abs/2412.11556", "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs", "authors": ["Yuchen Fu", "Zifeng Cheng", "Zhiwei Jiang", "Zhonghui Wang", "Yafeng Yin", "Zhengliang Li", "Qing Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accept to ACL 2025 (Oral). Code are available on\n  https://github.com/fuyuchenIfyw/token_prepending.git", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.", "AI": {"tldr": "This paper introduces the Token Prepending (TP) technique that enhances sentence embeddings extracted from LLMs by allowing earlier tokens to access complete sentence information, improving performance on STS tasks without additional inference cost.", "motivation": "Previous methods for extracting sentence embeddings from LLMs focus on prompt engineering but suffer from information bias due to causal attention limitations in decoder-only models.", "method": "The Token Prepending technique prepends the decoded sentence embedding from previous layers to the input of the next layer, facilitating better contextual attention among tokens.", "result": "TP significantly enhances the performance of existing prompt-based sentence embedding methods in Semantic Textual Similarity tasks and downstream classification tasks.", "conclusion": "The TP technique is effective, training-free, and can be integrated with various existing LLMs and sentence embedding methods with minimal inference cost.", "key_contributions": ["Introduction of the Token Prepending technique", "Demonstrated significant performance improvements in STS tasks", "Training-free integration with existing prompt-based methods"], "limitations": "", "keywords": ["sentence embeddings", "large language models", "Token Prepending", "semantic textual similarity", "prompt engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.03262", "pdf": "https://arxiv.org/pdf/2501.03262.pdf", "abs": "https://arxiv.org/abs/2501.03262", "title": "REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models", "authors": ["Jian Hu", "Xibin Wu", "Wei Shen", "Jason Klein Liu", "Zilin Zhu", "Weixun Wang", "Songlin Jiang", "Haoran Wang", "Hao Chen", "Bin Chen", "Weikai Fang", "Xianyu", "Yu Cao", "Haotian Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "fix typo", "summary": "Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR)\nsignificantly improve the alignment of human-AI values and further raise the\nupper bound of AI capabilities, particularly in reasoning-intensive,\nlong-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or\nRLVR) frameworks commonly face challenges such as inference bottlenecks and\ncomplexity barriers, restricting their accessibility for newcomers. To bridge\nthis gap, we introduce \\textbf{OpenRLHF}, a user-friendly, scalable, and\neasy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and\nHuggingFace Transformers, featuring a simplified design, clear code structure,\nand comprehensive documentation to facilitate entry for researchers and\npractitioners. Experimental results show that OpenRLHF achieves superior\ntraining efficiency with speedups ranging from 1.22x to 1.68x across different\nmodel sizes compared to state-of-the-art frameworks, while requiring\nsignificantly fewer lines of code for implementation. OpenRLHF is publicly\navailable at https://github.com/OpenRLHF/OpenRLHF, and has already been adopted\nby leading institutions to accelerate RLHF research and learning.", "AI": {"tldr": "OpenRLHF is a user-friendly open-source framework for Reinforcement Learning from Human Feedback that improves training efficiency and accessibility for researchers.", "motivation": "To enhance the alignment of human and AI values and to improve accessibility of RLHF frameworks to newcomers.", "method": "A new open-source RLHF framework is developed using Ray, vLLM, DeepSpeed, and HuggingFace Transformers, designed with a simplified architecture for ease of use.", "result": "OpenRLHF achieves training speedups of 1.22x to 1.68x compared to existing state-of-the-art frameworks and requires fewer lines of code for implementation.", "conclusion": "OpenRLHF is effective in improving RLHF research efficiency and has gained adoption in leading institutions.", "key_contributions": ["Introduction of a simplified RLHF framework", "Demonstrated training efficiency improvements", "Clear documentation and code structure for ease of learning"], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Open source", "Training efficiency", "AI alignment"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2501.08496", "pdf": "https://arxiv.org/pdf/2501.08496.pdf", "abs": "https://arxiv.org/abs/2501.08496", "title": "Quantifying the Importance of Data Alignment in Downstream Model Performance", "authors": ["Krrish Chawla", "Aryan Sahai", "Mario DePavia", "Sudharsan Sundar", "Brando Miranda", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.", "AI": {"tldr": "This paper investigates the importance of data alignment over dataset size in training Large Language Models (LLMs), finding a strong negative correlation between training-evaluation data alignment and model performance.", "motivation": "To challenge the conventional emphasis on dataset size and highlight the role of data alignment in the performance of Large Language Models (LLMs).", "method": "Utilized the Task2Vec-based alignment coefficient to measure the similarity between pre-training and evaluation datasets, conducting controlled interventional experiments.", "result": "A strong negative correlation was found between the alignment coefficient of training and evaluation data and the model's loss/perplexity on downstream tasks, particularly in Autoformalization.", "conclusion": "The study suggests a need to re-evaluate LLM training practices, emphasizing the significance of data alignment in specialized tasks over merely increasing data quantity.", "key_contributions": ["Introduced the Task2Vec-based alignment coefficient as a measure of data quality.", "Demonstrated the impact of data alignment on LLM performance in the Autoformalization task.", "Provided evidence to shift focus from dataset size to data alignment in LLM training."], "limitations": "", "keywords": ["Large Language Models", "Data Alignment", "Autoformalization", "Machine Translation", "Data Quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11268", "pdf": "https://arxiv.org/pdf/2502.11268.pdf", "abs": "https://arxiv.org/abs/2502.11268", "title": "Improved Unbiased Watermark for Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts.", "AI": {"tldr": "MCmark introduces unbiased, Multi-Channel-based watermarks to improve the detectability and robustness of AI-generated text, enhancing authentication without compromising quality.", "motivation": "The need to authenticate AI-generated content has become critical as AI surpasses human text generation capabilities.", "method": "MCmark embeds statistical signals into language model-generated text by partitioning the model's vocabulary and adjusting token probabilities according to a watermark key.", "result": "MCmark improves the detectability of unbiased watermarks by over 10% compared to existing methods while preserving the original distribution of the language model.", "conclusion": "MCmark demonstrates significant advancements in watermarking for AI-generated texts, enhancing their authenticity and practical application.", "key_contributions": ["Introduction of MCmark, a family of unbiased watermarks", "Partitioning the vocabulary of the model to enhance watermarking", "Demonstrated improvement in detectability and robustness of watermarks"], "limitations": "", "keywords": ["watermarks", "AI-generated content", "language models", "detectability", "robustness"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.00958", "pdf": "https://arxiv.org/pdf/2503.00958.pdf", "abs": "https://arxiv.org/abs/2503.00958", "title": "Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers", "authors": ["Milad Alshomary", "Nikhil Reddy Varimalla", "Vishal Anand", "Smaranda Muresan", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a new approach for the authorship attribution task that leverages\nthe various linguistic representations learned at different layers of\npre-trained transformer-based models. We evaluate our approach on three\ndatasets, comparing it to a state-of-the-art baseline in in-domain and\nout-of-domain scenarios. We found that utilizing various transformer layers\nimproves the robustness of authorship attribution models when tested on\nout-of-domain data, resulting in new state-of-the-art results. Our analysis\ngives further insights into how our model's different layers get specialized in\nrepresenting certain stylistic features that benefit the model when tested out\nof the domain.", "AI": {"tldr": "A new approach to authorship attribution that uses linguistic representations from different layers of transformer models, showing improved performance especially on out-of-domain data.", "motivation": "To enhance authorship attribution accuracy by leveraging different linguistic representations in transformer models.", "method": "The proposed method utilizes representations from various layers of pre-trained transformer models and evaluates on three datasets against a state-of-the-art baseline.", "result": "The approach achieved state-of-the-art results, especially in out-of-domain scenarios, demonstrating improved robustness.", "conclusion": "Different layers in transformer models specialize in representing stylistic features beneficial for authorship attribution.", "key_contributions": ["Introduction of layer-wise linguistic representations in authorship attribution", "Improved robustness of models on out-of-domain data", "Achievement of new state-of-the-art results in the field."], "limitations": "", "keywords": ["authorship attribution", "transformer models", "linguistic representations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.18681", "pdf": "https://arxiv.org/pdf/2503.18681.pdf", "abs": "https://arxiv.org/abs/2503.18681", "title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models", "authors": ["Yazhou Zhang", "Chunwang Zou", "Bo Wang", "Jing Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Our original goal was to use Commander-GPT: Dividing and Routing for\n  Multimodal Sarcasm Detection (arXiv:2506.19420) to replace Commander-GPT:\n  Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large\n  Language Models (arXiv:2503.18681). Due to various reasons, both versions\n  were released, so we would like to withdraw the latter", "summary": "Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales.", "AI": {"tldr": "This paper presents a novel multi-modal Commander-GPT framework for sarcasm detection that improves upon traditional methods by effectively employing multi-modal large language models (MLLMs).", "motivation": "Traditional sarcasm detection methods often fail due to their single-modal approaches, prompting a shift towards more effective multi-modal techniques.", "method": "The paper proposes a Commander-GPT framework that decomposes sarcasm detection into six sub-tasks, assigning specific large language models to each and aggregating their results for better accuracy.", "result": "The proposed framework demonstrated a 19.3% improvement in F1 score on benchmarks without the need for fine-tuning.", "conclusion": "The Commander-GPT framework showcases the effectiveness of multi-modal approaches in sarcasm detection, achieving state-of-the-art results in the field.", "key_contributions": ["Introduction of a novel multi-modal sarcasm detection framework", "Decomposition of the detection task into sub-tasks for improved model assignment", "Aggregation of results from multiple models for enhanced accuracy"], "limitations": "", "keywords": ["Sarcasm Detection", "Multi-Modal Large Language Models", "Natural Language Processing", "Machine Learning", "AI"], "importance_score": 8, "read_time_minutes": 15}}
