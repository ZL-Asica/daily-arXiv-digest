{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700.pdf", "abs": "https://arxiv.org/abs/2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Tao Gui", "Xuanjing Huang", "Yu-Gang Jiang", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.", "AI": {"tldr": "MagicGUI is a mobile GUI agent that enhances perception, grounding, and reasoning for real-world applications through a robust dataset and advanced training methods.", "motivation": "Address critical challenges in mobile GUI environments related to perception, grounding, and reasoning.", "method": "The framework incorporates six key components, including a comprehensive dataset, enhanced perception capabilities, a unified action space, planning-oriented reasoning mechanisms, an iterative training procedure, and competitive performance on benchmarks.", "result": "MagicGUI demonstrates superior performance in GUI perception and agent tasks across various benchmarks, indicating strong generalization and real-world applicability.", "conclusion": "The development of MagicGUI presents a significant advancement in mobile GUI agent capabilities, making it suitable for practical applications.", "key_contributions": ["Foundational mobile GUI agent framework", "Comprehensive multimodal dataset", "Iterative training procedure combining pre-training and reinforcement learning"], "limitations": "", "keywords": ["Mobile GUI", "Human-Agent Interaction", "Multimodal Data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03705", "pdf": "https://arxiv.org/pdf/2508.03705.pdf", "abs": "https://arxiv.org/abs/2508.03705", "title": "Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth", "authors": ["Kanan Eldarov"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This study explores how different modes of digital interaction -- namely,\ncomputers versus smartphones -- affect attention, frustration, and creative\nperformance in adolescents. Using a combination of digital task logs,\nwebcam-based gaze estimation, and expert evaluation of task outcomes, we\nanalyzed data from a diverse sample of 824 students aged 11-17. Participants\nwere assigned to device groups in a randomized and stratified design to control\nfor age, gender, and prior experience. Results suggest moderate but\nstatistically significant differences in sustained attention, perceived\nfrustration, and creative output. These findings indicate that the nature of\ndigital interaction -- beyond mere screen time -- may influence cognitive and\nbehavioral outcomes relevant to educational design. Practical implications for\nuser interface development and learning environments are discussed.", "AI": {"tldr": "The study examines how computer and smartphone interactions impact attention, frustration, and creativity in adolescents.", "motivation": "To understand how different digital interaction modes influence cognitive and behavioral outcomes in educational contexts.", "method": "Data from 824 students aged 11-17 was collected via digital task logs, webcam gaze estimation, and expert evaluation in a randomized, stratified design.", "result": "Significant differences were found in sustained attention, frustration levels, and creative output between device types.", "conclusion": "The choice of device for digital interactions can significantly affect educational outcomes and informs user interface design.", "key_contributions": ["Identification of device-specific differences in cognitive performance in adolescents", "Analysis based on a large and diverse sample size", "Recommendations for educational user interface design"], "limitations": "The study is limited to adolescents aged 11-17 and may not generalize to other age groups or contexts.", "keywords": ["digital interaction", "adolescents", "attention", "creativity", "user interface"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.03713", "pdf": "https://arxiv.org/pdf/2508.03713.pdf", "abs": "https://arxiv.org/abs/2508.03713", "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Yuanhong Zhou", "Andreas Bulling", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "cs.CV"], "comment": "11 pages, 9 figures, Accepted to 2025 IEEE VIS (Visualization and\n  Visual Analytics)", "summary": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding.", "AI": {"tldr": "This paper presents two computational models (Lit2Sal and Sal2Lit) to enhance visualization design and literacy assessment by considering individual differences in visual attention based on literacy levels.", "motivation": "The effectiveness of visualizations can be improved by accounting for individual differences in visual literacy, which influences attention patterns in data exploration.", "method": "A user study with 235 participants was conducted, analyzing visual attention patterns during three visualization tests. Two computational models were proposed based on the findings: Lit2Sal predicts attention based on literacy level, and Sal2Lit predicts literacy from attention data.", "result": "Lit2Sal outperforms existing saliency models by integrating literacy awareness, while Sal2Lit achieves 86% accuracy in predicting visual literacy from attention maps.", "conclusion": "The proposed models demonstrate a novel way to adapt visual designs and assessments to individual literacy levels, paving the way for personalized visual communication.", "key_contributions": ["Introduction of Lit2Sal, a new visual saliency model considering literacy levels", "Development of Sal2Lit, a model that predicts visual literacy from attention data", "Empirical analysis demonstrating the correlation between literacy levels and attention patterns"], "limitations": "", "keywords": ["visualization", "visual literacy", "attention patterns", "saliency modeling", "personalized communication"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03714", "pdf": "https://arxiv.org/pdf/2508.03714.pdf", "abs": "https://arxiv.org/abs/2508.03714", "title": "\"Think First, Verify Always\": Training Humans to Face AI Risks", "authors": ["Yuksel Aydin"], "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Artificial intelligence enables unprecedented attacks on human cognition, yet\ncybersecurity remains predominantly device-centric. This paper introduces the\n\"Think First, Verify Always\" (TFVA) protocol, which repositions humans as\n'Firewall Zero', the first line of defense against AI-enabled threats. The\nprotocol is grounded in five operational principles: Awareness, Integrity,\nJudgment, Ethical Responsibility, and Transparency (AIJET). A randomized\ncontrolled trial (n=151) demonstrated that a minimal 3-minute intervention\nproduced statistically significant improvements in cognitive security task\nperformance, with participants showing an absolute +7.87% gains compared to\ncontrols. These results suggest that brief, principles-based training can\nrapidly enhance human resilience against AI-driven cognitive manipulation. We\nrecommend that GenAI platforms embed \"Think First, Verify Always\" as a standard\nprompt, replacing passive warnings with actionable protocols to enhance\ntrustworthy and ethical AI use. By bridging the gap between technical\ncybersecurity and human factors, the TFVA protocol establishes human-empowered\nsecurity as a vital component of trustworthy AI systems.", "AI": {"tldr": "The paper presents the TFVA protocol that positions humans as the first line of defense against AI-enabled threats, demonstrating improved cognitive security through brief training.", "motivation": "To address the gap between device-centric cybersecurity and the need for human involvement in defending against AI-enabled cognitive attacks.", "method": "A randomized controlled trial with 151 participants was conducted to evaluate the effectiveness of the TFVA protocol, defined by five principles: Awareness, Integrity, Judgment, Ethical Responsibility, and Transparency (AIJET).", "result": "Participants who underwent a 3-minute intervention showed a statistically significant improvement of +7.87% in cognitive security task performance compared to controls.", "conclusion": "Embedding the TFVA protocol in GenAI platforms can enhance human resilience against AI-driven manipulation and supports ethical AI usage by shifting the emphasis to proactive human engagement in security.", "key_contributions": ["Introduction of the TFVA protocol focusing on human factors in cybersecurity.", "Demonstrated significant performance improvements through minimal training interventions.", "Recommendations for integrating the TFVA protocol into existing AI platforms."], "limitations": "", "keywords": ["cybersecurity", "cognitive security", "AI-enabled threats", "human factors", "trusted AI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03712", "pdf": "https://arxiv.org/pdf/2508.03712.pdf", "abs": "https://arxiv.org/abs/2508.03712", "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "categories": ["cs.CL"], "comment": "Accepted to AIES 2025", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs", "AI": {"tldr": "The paper audits representational bias in GPT-4 Turbo through the generation of stories in India, revealing persistent overrepresentation of dominant groups despite efforts to encourage diversity.", "motivation": "To investigate the depth and extent of representational bias in large language models, expanding beyond typical metrics that focus on Global North identities.", "method": "Conducted a systematic audit by generating over 7,200 stories using prompts designed for varying diversity, comparing outputs against census data for religious and caste representation in India.", "result": "Found consistent overrepresentation of culturally dominant groups in model outputs, highlighting a 'winner-take-all' bias in LLMs that surpasses training data distribution biases.", "conclusion": "Diversifying training data alone is insufficient to address LLM bias; more fundamental changes in model development are necessary.", "key_contributions": ["Systematic audit of GPT-4 Turbo for representational bias", "Quantitative comparison of LLM outputs against real-world demographics", "Identification of the limitations of prompt-based nudges in mitigating bias."], "limitations": "Focused primarily on religion and caste representation in India; results may not generalize to other cultures or identities.", "keywords": ["Large Language Models", "Representational Bias", "Census Data", "GPT-4 Turbo", "Cultural Diversity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03717", "pdf": "https://arxiv.org/pdf/2508.03717.pdf", "abs": "https://arxiv.org/abs/2508.03717", "title": "Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery", "authors": ["Muhammad Akmal Bin Mohammed Zaffir", "Daisuke Sakai", "Yuki Sato", "Takahiro Wada"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Studies suggest that involuntary eye movements exhibit greater stability\nduring active motion compared to passive motion, and this effect may also apply\nto the operation of ride-on machinery. Moreover, a study suggested that\nexperimentally manipulating the sense of agency (SoA) by introducing delays may\ninfluence the stability of involuntary eye movements. Although a preliminary\ninvestigation examined involuntary eye movements and perceived maneuverability\nunder two distinct machine dynamics with preserved SoA, it remains unclear how\nsystematic variations in motion dynamics influence these factors. Therefore,\nthe purpose of the present research was to investigate whether systematic\nvariations in the dynamic properties of a ride-on machine, where the perceived\nmaneuverability is modulated, influence the accuracy of involuntary eye\nmovements in human operators. Participants rode a yaw-rotational platform whose\ntime constant from joystick input to motor torque of a rotational machine was\nsystematically manipulated. During the operation, eye movements were recorded\nwhile participants fixated on a visual target. After each condition,\nparticipants provided subjective ratings of maneuverability and cognitive load.\nAs the platform's time constant increased, the perceived maneuverability scores\ndecreased while the cognitive loads increased. Concurrently, involuntary eye\nmovement accuracy decreased. Moderate to weak positive correlations emerged\nbetween the perceived maneuverability scores and the eye movement gain and\naccuracy, while a weak negative correlation was found with cognitive load.", "AI": {"tldr": "The study investigates how varying dynamic properties of a ride-on machine affect involuntary eye movements and perceived maneuverability in human operators.", "motivation": "Understanding the influence of motion dynamics on involuntary eye movements can provide insights into human operator performance in ride-on machinery.", "method": "Participants operated a yaw-rotational platform with manipulated time constants affecting motor torque based on joystick input while their eye movements and subjective ratings of maneuverability and cognitive load were recorded.", "result": "As the platform's time constant increased, perceived maneuverability scores decreased and cognitive loads increased, leading to decreased accuracy in involuntary eye movements; positive correlations were found between maneuverability and eye movement accuracy.", "conclusion": "The research suggests that design modifications in ride-on machinery can improve operator performance by considering the interaction between perceived maneuverability and cognitive load on eye movements.", "key_contributions": ["Demonstrated the effect of time constant variation on eye movement accuracy", "Established correlations between perceived maneuverability, cognitive load, and eye movement stability", "Provided insights on the somatic experience of maneuvering machines related to HCI design."], "limitations": "The study is limited to specific dynamic properties and may not generalize across all types of ride-on machinery or real-world scenarios.", "keywords": ["involuntary eye movements", "perceived maneuverability", "cognitive load", "ride-on machinery", "human operator performance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03716", "pdf": "https://arxiv.org/pdf/2508.03716.pdf", "abs": "https://arxiv.org/abs/2508.03716", "title": "FeynTune: Large Language Models for High-Energy Theory", "authors": ["Paul Richmond", "Prarit Agarwal", "Borun Chowdhury", "Vasilis Niarchos", "Constantinos Papageorgakis"], "categories": ["cs.CL", "cs.LG", "hep-th"], "comment": "16 pages", "summary": "We present specialized Large Language Models for theoretical High-Energy\nPhysics, obtained as 20 fine-tuned variants of the 8-billion parameter\nLlama-3.1 model. Each variant was trained on arXiv abstracts (through August\n2024) from different combinations of hep-th, hep-ph and gr-qc. For a\ncomparative study, we also trained models on datasets that contained abstracts\nfrom disparate fields such as the q-bio and cs categories. All models were\nfine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and\nvarying dataset sizes, and outperformed the base model on hep-th abstract\ncompletion tasks. We compare performance against leading commercial LLMs\n(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing\nspecialized language models for High-Energy Theoretical Physics.", "AI": {"tldr": "This paper discusses fine-tuning Large Language Models specifically for High-Energy Physics, showing improved performance over baseline models and commercial LLMs.", "motivation": "To enhance the performance of language models in the specialized field of High-Energy Theoretical Physics (HEP) by fine-tuning existing LLMs on relevant datasets.", "method": "20 fine-tuned variants of the 8-billion parameter Llama-3.1 model were created, trained on arXiv abstracts from hep-th, hep-ph, and gr-qc, using Low-Rank Adaptation techniques.", "result": "The fine-tuned models significantly outperformed the base model on hep-th abstract completion tasks and compared favorably against leading commercial language models.", "conclusion": "Specialized LLMs for High-Energy Physics exhibit significant improvements over general models and suggest a path forward for the development of domain-specific language models.", "key_contributions": ["Development of 20 fine-tuned LLM variants for High-Energy Physics.", "Unique application of Low-Rank Adaptation techniques for fine-tuning.", "Performance benchmarking against leading commercial LLMs."], "limitations": "", "keywords": ["Large Language Models", "High-Energy Physics", "Fine-tuning", "Domain-specific models", "Low-Rank Adaptation"], "importance_score": 2, "read_time_minutes": 16}}
{"id": "2508.03792", "pdf": "https://arxiv.org/pdf/2508.03792.pdf", "abs": "https://arxiv.org/abs/2508.03792", "title": "Recommending With, Not For: Co-Designing Recommender Systems for Social Good", "authors": ["Michael D. Ekstrand", "Afsaneh Razi", "Aleksandra Sarcevic", "Maria Soledad Pera", "Robin Burke", "Katherine Landau Wright"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Accepted to ACM TORS Special Issue on Recommender Systems for Social\n  Good", "summary": "Recommender systems are usually designed by engineers, researchers,\ndesigners, and other members of development teams. These systems are then\nevaluated based on goals set by the aforementioned teams and other business\nunits of the platforms operating the recommender systems. This design approach\nemphasizes the designers' vision for how the system can best serve the\ninterests of users, providers, businesses, and other stakeholders. Although\ndesigners may be well-informed about user needs through user experience and\nmarket research, they are still the arbiters of the system's design and\nevaluation, with other stakeholders' interests less emphasized in user-centered\ndesign and evaluation. When extended to recommender systems for social good,\nthis approach results in systems that reflect the social objectives as\nenvisioned by the designers and evaluated as the designers understand them.\nInstead, social goals and operationalizations should be developed through\nparticipatory and democratic processes that are accountable to their\nstakeholders. We argue that recommender systems aimed at improving social good\nshould be designed *by* and *with*, not just *for*, the people who will\nexperience their benefits and harms. That is, they should be designed in\ncollaboration with their users, creators, and other stakeholders as full\nco-designers, not only as user study participants.", "AI": {"tldr": "The paper argues for a participatory design approach in recommender systems for social good, emphasizing collaboration with stakeholders rather than a top-down design by engineers and designers alone.", "motivation": "The traditional design of recommender systems predominantly reflects the designers' views, potentially sidelining the needs and interests of other stakeholders. This is particularly concerning for systems aiming to benefit social good.", "method": "The authors advocate for a democratic and participatory design process that includes users, creators, and other involved parties as active co-designers rather than mere subjects of study.", "result": "The paper outlines the shortcomings of current recommender system designs and suggests that systems designed through collaborative processes can better serve diverse stakeholder needs.", "conclusion": "Recommender systems for social good should involve stakeholders in a collaborative design process to ensure their social objectives are truly representative and beneficial.", "key_contributions": ["Advocates for participatory design in recommender systems for social good.", "Highlights the limitations of current designs dictated by engineers and designers.", "Proposes a framework for stakeholder collaboration in system development."], "limitations": "", "keywords": ["recommender systems", "social good", "participatory design", "stakeholder collaboration", "user experience"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03719", "pdf": "https://arxiv.org/pdf/2508.03719.pdf", "abs": "https://arxiv.org/abs/2508.03719", "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering", "authors": ["Abhay Vijayvargia", "Ajay Nagpal", "Kundeshwar Pundalik", "Atharva Savarkar", "Smita Gautam", "Pankaj Singh", "Rohit Saluja", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Indian farmers often lack timely, accessible, and language-friendly\nagricultural advice, especially in rural areas with low literacy. To address\nthis gap in accessibility, this paper presents a novel AI-powered agricultural\nchatbot, Krishi Sathi, designed to support Indian farmers by providing\npersonalized, easy-to-understand answers to their queries through both text and\nspeech. The system's intelligence stems from an IFT model, subsequently refined\nthrough fine-tuning on Indian agricultural knowledge across three curated\ndatasets. Unlike traditional chatbots that respond to one-off questions, Krishi\nSathi follows a structured, multi-turn conversation flow to gradually collect\nthe necessary details from the farmer, ensuring the query is fully understood\nbefore generating a response. Once the intent and context are extracted, the\nsystem performs Retrieval-Augmented Generation (RAG) by first fetching\ninformation from a curated agricultural database and then generating a tailored\nresponse using the IFT model. The chatbot supports both English and Hindi\nlanguages, with speech input and output features (via ASR and TTS) to make it\naccessible for users with low literacy or limited digital skills. This work\ndemonstrates how combining intent-driven dialogue flows, instruction-tuned\nmodels, and retrieval-based generation can improve the quality and\naccessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query\nresponse accuracy of 97.53%, 91.35% contextual relevance and personalization,\nand a query completion rate of 97.53%. The average response time remained under\n6 seconds, ensuring timely support for users across both English and Hindi\ninteractions.", "AI": {"tldr": "This paper presents an AI-powered agricultural chatbot, Krishi Sathi, designed to deliver accessible farming advice to Indian farmers, particularly in rural regions with low literacy.", "motivation": "To bridge the gap in accessibility to timely and easy-to-understand agricultural advice for Indian farmers, especially in rural areas.", "method": "The chatbot employs a structured multi-turn conversation model with an intent-driven dialogue flow, combining an instruction-tuned model and Retrieval-Augmented Generation (RAG) to provide personalized responses.", "result": "The system achieved a query response accuracy of 97.53%, 91.35% contextual relevance, and an average response time of under 6 seconds, supporting both English and Hindi users.", "conclusion": "Combining advanced dialogue techniques with machine learning can significantly enhance the accessibility and effectiveness of digital agricultural support systems.", "key_contributions": ["Introduction of a structured multi-turn conversation flow in chatbots for agriculture", "Use of instruction-tuned models and RAG for personalized agricultural advice", "Support for low literacy users through speech input and output features"], "limitations": "", "keywords": ["chatbot", "agricultural advice", "low literacy", "AI", "RAG"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2508.03852", "pdf": "https://arxiv.org/pdf/2508.03852.pdf", "abs": "https://arxiv.org/abs/2508.03852", "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers", "authors": ["Zhuohao", "Zhang", "Haichang Li", "Chun Meng Yu", "Faraz Faruqi", "Junan Xie", "Gene S-H Kim", "Mingming Fan", "Angus G. Forbes", "Jacob O. Wobbrock", "Anhong Guo", "Liang He"], "categories": ["cs.HC"], "comment": "ASSETS 2025", "summary": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals.", "AI": {"tldr": "A11yShape is a system for blind and low-vision users to understand and modify 3-D models using accessible descriptions and LLMs.", "motivation": "The inherent complexity of 3-D models and the lack of non-visual interaction tools make it challenging for blind and low-vision users to engage with these models.", "method": "A11yShape integrates LLMs with OpenSCAD to provide accessible descriptions, version control for tracking changes, and a hierarchical representation of model components, while synchronizing semantic selections across multiple representations.", "result": "User studies with four blind and low-vision programmers showed that participants could independently understand, create, and modify 3-D models, achieving high satisfaction with the process.", "conclusion": "A11yShape enables blind and low-vision users to work with 3-D models independently, overcoming previous barriers that required sighted assistance.", "key_contributions": ["Development of accessible 3-D modeling tools for blind and low-vision users", "Integration of LLMs with OpenSCAD for enhanced user interaction", "Demonstrated successful user study with blind and low-vision programmers"], "limitations": "", "keywords": ["3-D modeling", "blind and low-vision", "human-computer interaction", "LLMs", "OpenSCAD"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03726", "pdf": "https://arxiv.org/pdf/2508.03726.pdf", "abs": "https://arxiv.org/abs/2508.03726", "title": "Hierarchical Verification of Speculative Beams for Accelerating LLM Inference", "authors": ["Jaydip Sen", "Harshitha Puvvala", "Subhasis Dasgupta"], "categories": ["cs.CL"], "comment": "This paper was accepted for oral presentation and publication in the\n  3rd International Conference on Data Science and Network Engineering (ICDSNE\n  2025), organized at NIT, Agartala, India, from July 25 to 26, 2025. The paper\n  is 12 pages long, and it contains 3 tables and 4 figures. This is NOT the\n  final paper, which will be published in the Springer-published proceedings", "summary": "Large language models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks but face persistent challenges in inference\nefficiency due to their autoregressive nature. While speculative decoding and\nbeam sampling offer notable improvements, traditional methods verify draft\nsequences sequentially without prioritization, leading to unnecessary\ncomputational overhead. This work proposes the Hierarchical Verification Tree\n(HVT), a novel framework that restructures speculative beam decoding by\nprioritizing high-likelihood drafts and enabling early pruning of suboptimal\ncandidates. Theoretical foundations and a formal verification-pruning algorithm\nare developed to ensure correctness and efficiency. Integration with standard\nLLM inference pipelines is achieved without requiring retraining or\narchitecture modification. Experimental evaluations across multiple datasets\nand models demonstrate that HVT consistently outperforms existing speculative\ndecoding schemes, achieving substantial reductions in inference time and energy\nconsumption while maintaining or enhancing output quality. The findings\nhighlight the potential of hierarchical verification strategies as a new\ndirection for accelerating large language model inference.", "AI": {"tldr": "This paper introduces the Hierarchical Verification Tree (HVT) framework to improve inference efficiency in large language models (LLMs) by prioritizing high-likelihood drafts and enabling early pruning of suboptimal candidates.", "motivation": "Large language models struggle with inference efficiency, leading to computational overhead during the decoding process. This paper aims to address these challenges with a new approach.", "method": "The paper proposes a Hierarchical Verification Tree for speculative beam decoding, which allows for prioritization of drafts and early pruning of suboptimal candidates. It includes theoretical foundations and an algorithm for formal verification-pruning integrated with standard LLM inference workflows.", "result": "Experimental evaluations show that HVT outperforms existing speculative decoding methods, reducing inference time and energy consumption while improving output quality across various datasets and models.", "conclusion": "The findings support hierarchical verification as a viable strategy for enhancing the efficiency of large language model inference.", "key_contributions": ["Introduction of the Hierarchical Verification Tree framework", "Development of a theoretical foundation for verification-pruning", "Demonstrated improvements in inference efficiency without requiring model retraining"], "limitations": "", "keywords": ["large language models", "inference efficiency", "speculative decoding"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.03876", "pdf": "https://arxiv.org/pdf/2508.03876.pdf", "abs": "https://arxiv.org/abs/2508.03876", "title": "ReVISit 2: A Full Experiment Life Cycle User Study Framework", "authors": ["Zach Cutler", "Jack Wilburn", "Hilson Shrestha", "Yiren Ding", "Brian Bollen", "Khandaker Abrar Nadib", "Tingying He", "Andrew McNutt", "Lane Harrison", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "Online user studies of visualizations, visual encodings, and interaction\ntechniques are ubiquitous in visualization research. Yet, designing,\nconducting, and analyzing studies effectively is still a major burden. Although\nvarious packages support such user studies, most solutions address only facets\nof the experiment life cycle, make reproducibility difficult, or do not cater\nto nuanced study designs or interactions. We introduce reVISit 2, a software\nframework that supports visualization researchers at all stages of designing\nand conducting browser-based user studies. ReVISit supports researchers in the\ndesign, debug & pilot, data collection, analysis, and dissemination experiment\nphases by providing both technical affordances (such as replay of participant\ninteractions) and sociotechnical aids (such as a mindfully maintained community\nof support). It is a proven system that can be (and has been) used in\npublication-quality studies -- which we demonstrate through a series of\nexperimental replications. We reflect on the design of the system via\ninterviews and an analysis of its technical dimensions. Through this work, we\nseek to elevate the ease with which studies are conducted, improve the\nreproducibility of studies within our community, and support the construction\nof advanced interactive studies.", "AI": {"tldr": "reVISit 2 is a software framework designed to assist researchers in effectively conducting browser-based user studies in visualization research.", "motivation": "To ease the burden of designing, conducting, and analyzing online user studies for visualizations while addressing the limitations of existing tools.", "method": "The study involves creating a comprehensive software framework that supports various phases of user studies, including design, debugging, data collection, analysis, and dissemination.", "result": "reVISit 2 has demonstrated its effectiveness through experimental replications and has been used in publication-quality studies.", "conclusion": "The framework aims to enhance the ease of conducting user studies, improve reproducibility, and support advanced interactive study designs within the visualization research community.", "key_contributions": ["Introduction of a comprehensive software framework for user studies", "Technical affordances such as replay of interactions", "Community support for researchers"], "limitations": "", "keywords": ["user studies", "visualization", "software framework", "reproducibility", "interaction techniques"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03728", "pdf": "https://arxiv.org/pdf/2508.03728.pdf", "abs": "https://arxiv.org/abs/2508.03728", "title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Wikipedia, a vast and continuously consulted knowledge base, faces\nsignificant challenges in maintaining up-to-date content due to its reliance on\nmanual human editors. Inspired by the vision of continuous knowledge\nacquisition in NELL and fueled by advances in LLM-based agents, this paper\nintroduces WiNELL, an agentic framework for continuously updating Wikipedia\narticles. Our approach employs a multi-agent framework to aggregate online\ninformation, select new and important knowledge for a target entity in\nWikipedia, and then generate precise edit suggestions for human review. Our\nfine-grained editing models, trained on Wikipedia's extensive history of human\nedits, enable incorporating updates in a manner consistent with human editing\nbehavior. Our editor models outperform both open-source instruction-following\nbaselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and\nediting efficiency. End-to-end evaluation on high-activity Wikipedia pages\ndemonstrates WiNELL's ability to identify and suggest timely factual updates.\nThis opens up a promising research direction in LLM agents for automatically\nupdating knowledge bases in a never-ending fashion.", "AI": {"tldr": "WiNELL is a multi-agent framework for continuously updating Wikipedia articles, leveraging LLM-based editing models to suggest accurate edits for human review.", "motivation": "Wikipedia struggles with keeping content current due to manual editing; the goal is to automate this process using LLM technology.", "method": "A multi-agent framework aggregates online information and generates edit suggestions based on fine-grained models trained on Wikipedia's edit history.", "result": "WiNELL outperforms existing LLMs and instruction-following models in suggesting timely factual updates, demonstrating higher efficiency and coverage.", "conclusion": "The research showcases potential in using LLM agents for ongoing knowledge base updates, enhancing Wikipediaâ€™s reliability and currency.", "key_contributions": ["Introduction of WiNELL framework for automatic Wikipedia updates", "Fine-grained editing models outperform existing baselines", "End-to-end evaluation showing effectiveness in providing timely updates."], "limitations": "", "keywords": ["Wikipedia", "LLM", "knowledge acquisition", "editing models", "multi-agent framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03969", "pdf": "https://arxiv.org/pdf/2508.03969.pdf", "abs": "https://arxiv.org/abs/2508.03969", "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "authors": ["Wei Xu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters.", "AI": {"tldr": "This chapter introduces a framework for human-centered human-AI interaction (HC-HAII) focusing on placing humans at the core of HAII research, presenting methodologies, challenges, and the structure of a related book.", "motivation": "The need for a human-centered approach in HAII research and applications to properly address the complexities of human and AI collaboration.", "method": "The chapter outlines the HC-HAII methodology, emphasizing human-centered methods, interdisciplinary collaboration, and multi-level design paradigms.", "result": "Presents a comprehensive framework and highlights key research challenges, setting the stage for further contributions in HAII.", "conclusion": "Establishes a fundamental framework for human-centered HAII, which will guide subsequent research and applications in the field.", "key_contributions": ["Introduction of HC-HAII framework", "Emphasis on human-centered methodologies", "Identification of research challenges"], "limitations": "", "keywords": ["human-AI interaction", "human-centered AI", "interdisciplinary research", "HAII methodology", "human-centered design"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03737", "pdf": "https://arxiv.org/pdf/2508.03737.pdf", "abs": "https://arxiv.org/abs/2508.03737", "title": "GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models", "authors": ["Ashutosh Bandooni", "Brindha Subburaj"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "6 pages, 3 figures. Accepted, Presented and Published as part of\n  Proceedings of the 6th International Conference on Recent Advantages in\n  Information Technology (RAIT) 2025", "summary": "Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on\nseveral fields and domains are being curated more frequently over the last few\nyears. However these are often monolingual, mostly available in English.\nAdditionally there also is a lack of datasets available in Hindi on tasks apart\nfrom comprehension and translation. We introduce GanitBench, a tough benchmark\nconsisting of 1527 vision-only questions covering several topics in Mathematics\n- available in languages English and Hindi. Collected from two major\nexaminations from India, the JEE Advanced and the CBSE Boards examinations,\nthis benchmark includes questions in the form of images comprising of figures\nessential to a question as well as text. We evaluate two closed source models\nfor the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.\nGPT-4o mini is found to be the more dominant model on the benchmark, with it's\nhighest average accuracy being 38.15%. We also evaluate models through a\n\"Double Lock\" constraint, which brings down the performance of the models by\nconsiderable margins. We observe that two-shot CoT appears to be a more\neffective setting under this environment. Performance of the two VLMs also\ndecreases when answering the same questions in the Hindi language. We hope to\nfacilitate the inclusion of languages like Hindi in research through our work.", "AI": {"tldr": "GanitBench is a benchmark of 1527 vision-only questions in English and Hindi focusing on Mathematics, evaluating Vision Language Models (VLMs).", "motivation": "To address the lack of multilingual datasets for reasoning tasks in Vision Language Models, particularly in Hindi, and to evaluate their performance on complex mathematical questions.", "method": "The paper introduces GanitBench, curating vision questions from Indian examinations (JEE Advanced and CBSE) in both English and Hindi. It evaluates two model setups: zero-shot and two-shot Chain-of-Thought (CoT), applying a 'Double Lock' constraint to test the models' capabilities.", "result": "The models show a maximum average accuracy of 38.15%, with two-shot CoT outperforming zero-shot under constraints. Performance deteriorates in Hindi as compared to English.", "conclusion": "GanitBench aims to facilitate research in multilingual VLMs, showing significant differences in performances across languages.", "key_contributions": ["Introduction of GanitBench benchmark in English and Hindi", "Evaluation of VLMs on complex mathematics questions", "Demonstration of performance differences in language settings"], "limitations": "The performance metrics are limited to two evaluated models and only cover mathematical reasoning, which may not generalize to other domains.", "keywords": ["Vision Language Models", "GanitBench", "Multilingual Datasets", "Mathematics", "Hindi Language"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2508.03974", "pdf": "https://arxiv.org/pdf/2508.03974.pdf", "abs": "https://arxiv.org/abs/2508.03974", "title": "Managing Data for Scalable and Interactive Event Sequence Visualization", "authors": ["Sayef Azad Sakin", "Katherine E. Isaacs"], "categories": ["cs.HC"], "comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization", "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.", "AI": {"tldr": "ESeMan is an event sequence management system improving timeline visualization performance while preserving accuracy through intelligent caching and hierarchical data structures.", "motivation": "To improve the performance of interactive visualizations for large datasets in application domains like program execution traces and manufacturing pipelines, without sacrificing accuracy.", "method": "ESeMan utilizes hierarchical data structures and intelligent caching strategies to optimize data fetching and visualization rendering times for event sequences.", "result": "ESeMan achieves sub-100ms data fetching times while maintaining pixel-level visualization accuracy, outperforming traditional approaches like summed area tables and M4 aggregation.", "conclusion": "ESeMan significantly enhances interaction performance in timeline visualizations for event sequences, proven through extensive evaluation and benchmarking.", "key_contributions": ["Introduction of ESeMan for efficient event sequence visualization", "Demonstration of superior performance compared to existing methods", "Provision of a benchmarking harness for future evaluations"], "limitations": "", "keywords": ["event sequence visualization", "interactive rendering", "data management"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.03793", "pdf": "https://arxiv.org/pdf/2508.03793.pdf", "abs": "https://arxiv.org/abs/2508.03793", "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs", "authors": ["Yanting Wang", "Runpeng Geng", "Ying Chen", "Jinyuan Jia"], "categories": ["cs.CL", "cs.CR"], "comment": "The code is available at https://github.com/Wang-Yanting/AttnTrace.\n  The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace", "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.", "AI": {"tldr": "AttnTrace is a new efficient method for tracing context in LLM responses, outperforming current solutions both in accuracy and computation speed.", "motivation": "To improve the efficiency and accuracy of context traceback methods used in LLM systems, particularly in applications like forensic analysis and LLM output interpretability.", "method": "AttnTrace utilizes attention weights from LLMs to trace back context in a response. The paper introduces two enhancement techniques to optimize this process and provides theoretical insights into these choices.", "result": "AttnTrace is shown to be more accurate and efficient than existing state-of-the-art methods in context traceback, and it also enhances detection of prompt injection in long contexts.", "conclusion": "AttnTrace offers a novel approach to context traceback that not only improves current methods but also has practical applications, as demonstrated by identifying manipulated instructions in LLM-generated reviews.", "key_contributions": ["Introduction of a novel context traceback method using attention weights", "Two techniques to enhance the effectiveness of AttnTrace", "Demonstration of real-world applications in LLM output manipulation detection"], "limitations": "", "keywords": ["long-context", "large language models", "context traceback", "AI systems", "interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03980", "pdf": "https://arxiv.org/pdf/2508.03980.pdf", "abs": "https://arxiv.org/abs/2508.03980", "title": "SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions", "authors": ["Md Sabbir Ahmed", "Arafat Rahman", "Mark Rucker", "Laura E. Barnes"], "categories": ["cs.HC"], "comment": null, "summary": "Social interactions are a fundamental part of daily life and play a critical\nrole in well-being. As emerging technologies offer opportunities to\nunobtrusively monitor behavior, there is growing interest in using them to\nbetter understand social experiences. However, automatically detecting\ninteractions, particularly via wearable devices, remains underexplored.\nExisting systems are often limited to controlled environments, constrained to\nin-person interactions, and rely on rigid assumptions such as the presence of\ntwo speakers within a fixed time window. These limitations reduce their\ngeneralizability to capture diverse real-world interactions. To address these\nchallenges, we developed a real-time, on-watch system capable of detecting both\nin-person and virtual interactions. The system leverages transfer learning to\ndetect foreground speech (FS) and infers interaction boundaries based upon FS\nand conversational cues like whispering. In a real-world evaluation involving\n11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the\nsystem achieved an interaction detection accuracy of 73.18%. Follow-up with six\nparticipants indicated perfect recall for detecting interactions. These\npreliminary findings demonstrate the potential of our system to capture\ninteractions in daily life, providing a foundation for applications such as\npersonalized interventions targeting social anxiety.", "AI": {"tldr": "A wearable system was developed for real-time detection of both in-person and virtual social interactions using transfer learning.", "motivation": "To better understand social experiences through unobtrusive monitoring and to overcome limitations of existing systems that are often restricted to controlled environments.", "method": "The system uses transfer learning to detect foreground speech and infers interaction boundaries based on conversational cues such as whispering.", "result": "The system achieved an interaction detection accuracy of 73.18% in a real-world evaluation with 11 participants over 38 days.", "conclusion": "The findings suggest that the system can effectively capture social interactions in daily life, with potential applications for personalized interventions for social anxiety.", "key_contributions": ["Real-time detection of in-person and virtual interactions using wearables", "Utilization of transfer learning for improved accuracy", "Demonstration of practical application in monitoring daily social interactions"], "limitations": "The study involved a small sample size and a limited evaluation period, which may affect generalizability.", "keywords": ["wearable technology", "social interaction detection", "transfer learning", "foreground speech", "personalized interventions"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.03829", "pdf": "https://arxiv.org/pdf/2508.03829.pdf", "abs": "https://arxiv.org/abs/2508.03829", "title": "Majority Bit-Aware Watermarking For Large Language Models", "authors": ["Jiahao Xu", "Rui Hu", "Zikai Zhang"], "categories": ["cs.CL", "cs.CR"], "comment": "Preprint", "summary": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.", "AI": {"tldr": "MajorMark is a novel multi-bit watermarking method for Large Language Models that enhances decoding accuracy while maintaining text generation quality by using majority bit-aware encoding and a clustering-based decoding strategy.", "motivation": "To address concerns about the misuse of Large Language Models in generating harmful content, watermarking techniques are proposed for origin verification and misuse tracing.", "method": "MajorMark selects preferred token sets based on the majority bit of the message and uses a clustering-based decoding strategy.", "result": "Experiments show significant improvements in both decoding accuracy and text generation quality compared to prior multi-bit watermarking methods.", "conclusion": "The proposed methods, including MajorMark$^+$, enhance the quality of watermarked text and improve decoding accuracy while preserving content quality.", "key_contributions": ["Introduction of MajorMark framework for watermarking LLMs", "Majority bit-aware encoding technique", "Clustering-based decoding strategy to enhance accuracy"], "limitations": "", "keywords": ["Large Language Models", "watermarking", "text generation", "machine learning", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04011", "pdf": "https://arxiv.org/pdf/2508.04011.pdf", "abs": "https://arxiv.org/abs/2508.04011", "title": "StepWrite: Adaptive Planning for Speech-Driven Text Generation", "authors": ["Hamza El Alaoui", "Atieh Taheri", "Yi-Hao Peng", "Jeffrey P. Bigham"], "categories": ["cs.HC", "cs.AI"], "comment": "This paper has been accepted to UIST 2025. For additional materials\n  and project details, please see:\n  https://www.cs.cmu.edu/~helalaou/publications/stepwrite", "summary": "People frequently use speech-to-text systems to compose short texts with\nvoice. However, current voice-based interfaces struggle to support composing\nmore detailed, contextually complex texts, especially in scenarios where users\nare on the move and cannot visually track progress. Longer-form communication,\nsuch as composing structured emails or thoughtful responses, requires\npersistent context tracking, structured guidance, and adaptability to evolving\nuser intentions--capabilities that conventional dictation tools and voice\nassistants do not support. We introduce StepWrite, a large language\nmodel-driven voice-based interaction system that augments human writing ability\nby enabling structured, hands-free and eyes-free composition of longer-form\ntexts while on the move. StepWrite decomposes the writing process into\nmanageable subtasks and sequentially guides users with contextually-aware\nnon-visual audio prompts. StepWrite reduces cognitive load by offloading the\ncontext-tracking and adaptive planning tasks to the models. Unlike baseline\nmethods like standard dictation features (e.g., Microsoft Word) and\nconversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite\ndynamically adapts its prompts based on the evolving context and user intent,\nand provides coherent guidance without compromising user autonomy. An empirical\nevaluation with 25 participants engaging in mobile or stationary hands-occupied\nactivities demonstrated that StepWrite significantly reduces cognitive load,\nimproves usability and user satisfaction compared to baseline methods.\nTechnical evaluations further confirmed StepWrite's capability in dynamic\ncontextual prompt generation, accurate tone alignment, and effective fact\nchecking. This work highlights the potential of structured, context-aware voice\ninteractions in enhancing hands-free and eye-free communication in everyday\nmultitasking scenarios.", "AI": {"tldr": "StepWrite is a voice-based interaction system that facilitates the hands-free, eyes-free composition of longer texts by breaking the process into manageable subtasks and providing adaptive contextual prompts.", "motivation": "Current voice-based systems struggle with composing longer, contextually complex texts, which is crucial for effective communication while multitasking or on the move.", "method": "StepWrite uses a large language model to decompose writing tasks and offers contextually-aware audio prompts to assist users in text composition without visual tracking.", "result": "An evaluation with 25 participants showed that StepWrite significantly reduces cognitive load and improves both usability and user satisfaction compared to conventional methods.", "conclusion": "StepWrite demonstrates the effectiveness of structured, context-aware voice interactions in enhancing communication during multitasking.", "key_contributions": ["Introduction of StepWrite for structured text composition", "Dynamic adaptation of prompts based on user context", "Empirical evaluation showing improved usability and reduced cognitive load"], "limitations": "", "keywords": ["voice-based interaction", "large language model", "cognitive load", "context-aware prompts", "text composition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03860", "pdf": "https://arxiv.org/pdf/2508.03860.pdf", "abs": "https://arxiv.org/abs/2508.03860", "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence\n  Review for peer review", "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.", "AI": {"tldr": "This review analyzes the evaluation of factual accuracy in LLM-generated content, addressing challenges and proposing improvements in fact-checking methods.", "motivation": "The paper addresses the misinformation generated by LLMs due to the inaccuracies in their training data, emphasizing the need for effective evaluation and fact-checking strategies.", "method": "The review systematically explores the literature from 2020 to 2025, focusing on challenges like hallucinations, dataset limitations, and evaluation metrics, and proposes five guiding research questions.", "result": "The findings reveal significant limitations in current evaluation metrics and highlight the necessity for domain-specific customization and validation of outputs with external evidence for better factual consistency.", "conclusion": "The paper underscores the importance of developing LLMs that are accurate, explainable, and capable of domain-specific fact-checking.", "key_contributions": ["Analysis of evaluation challenges of LLM-generated content", "Proposed research questions for future investigation", "Emphasis on the need for robust fact-checking frameworks"], "limitations": "The review may not cover all aspects of LLM evaluation and depends on existing literature which may vary in depth and focus.", "keywords": ["Large Language Models", "fact-checking", "evaluation methods", "retrieval-augmented generation", "misinformation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04026", "pdf": "https://arxiv.org/pdf/2508.04026.pdf", "abs": "https://arxiv.org/abs/2508.04026", "title": "VeriGUI: Verifiable Long-Chain GUI Dataset", "authors": ["Shunyu Liu", "Minghao Liu", "Huichi Zhou", "Zhenyu Cui", "Yang Zhou", "Yuhao Zhou", "Wendong Fan", "Ge Zhang", "Jiajun Shi", "Weihao Xuan", "Jiaxing Huang", "Shuang Luo", "Fang Wu", "Heli Qi", "Qingcheng Zeng", "Ziqi Ren", "Jialiang Gao", "Jindi Lv", "Junjie Wang", "Aosong Feng", "Heng Zhou", "Wangchunshu Zhou", "Zhenfei Yin", "Wenlong Zhang", "Guohao Li", "Wenhao Yu", "Irene Li", "Lei Ma", "Lei Bai", "Qunshu Lin", "Mingli Song", "Dacheng Tao"], "categories": ["cs.HC"], "comment": null, "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.", "AI": {"tldr": "This paper introduces VeriGUI, a novel long-chain GUI dataset designed to train and evaluate autonomous agents for complex GUI tasks, emphasizing long-chain task decomposition and subtask-level verifiability.", "motivation": "To address the limitations of existing GUI agents that primarily focus on short-term interactions and lack the capability for long-horizon task execution in real-world applications.", "method": "The authors developed VeriGUI, a dataset featuring long-chain tasks decomposed into interdependent subtasks, enhancing the training and evaluation of generalist GUI agents.", "result": "Experiments demonstrated significant performance gaps for existing agents on long-horizon tasks, indicating the need for improved planning and decision-making in GUI agents.", "conclusion": "The VeriGUI dataset facilitates the evaluation and development of more capable GUI agents in realistic scenarios, paving the way for advancements in human-computer interaction.", "key_contributions": ["Introduction of the VeriGUI dataset for long-chain GUI tasks", "Emphasis on subtask-level verifiability for exploring strategies", "Illustrated performance gaps in current agents highlighting the need for advanced capabilities"], "limitations": "", "keywords": ["GUI agents", "human-computer interaction", "long-chain tasks", "dataset", "subtask verifiability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03865", "pdf": "https://arxiv.org/pdf/2508.03865.pdf", "abs": "https://arxiv.org/abs/2508.03865", "title": "An Entity Linking Agent for Question Answering", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures. Submitted to AAAI 2026 Conference", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.", "AI": {"tldr": "This paper presents a novel entity linking agent for question answering systems using a Large Language Model, designed to improve performance on short, ambiguous user queries.", "motivation": "Existing entity linking methods struggle with short, ambiguous questions in QA systems, necessitating a new approach.", "method": "The proposed entity linking agent simulates human cognitive workflows to identify entity mentions, retrieve candidate entities, and make decisions in a QA context.", "result": "Experiments demonstrate the robustness and effectiveness of the agent through tool-based entity linking and QA task evaluations.", "conclusion": "The results highlight the agent's capability to handle short user queries effectively compared to traditional methods.", "key_contributions": ["Novel entity linking agent based on Large Language Model.", "Improved handling of short, ambiguous questions in QA tasks.", "Demonstrated effectiveness through experimental evaluations."], "limitations": "", "keywords": ["entity linking", "question answering", "Large Language Model", "cognitive workflows", "ambiguity"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.04108", "pdf": "https://arxiv.org/pdf/2508.04108.pdf", "abs": "https://arxiv.org/abs/2508.04108", "title": "XARP Tools: An Extended Reality Platform for Humans and AI Agents", "authors": ["Arthur Caetano", "Misha Sra"], "categories": ["cs.HC", "H.5"], "comment": null, "summary": "This technical report presents XARP Tools, an extended reality (XR) framework\ndesigned for human and AI developers alike. XARP comprises a server-side Python\nlibrary and platform-specific XR clients. The library offers high-level APIs\nand communicates with clients via a JSON-based protocol over WebSockets. XR\nclients encapsulate device and runtime specifics, providing responsive,\nlow-latency user interaction. XARP can be utilized in three ways: (i) as a\nlibrary that abstracts XR development for humans; (ii) as a set of callable\ntools that allow AI agents to drive on-the-fly interactions with users; and\n(iii) as a Model Context Protocol server that plugs XR devices into AI\necosystems. XARP code and working examples are released openly at\nhttps://github.com/HAL-UCSB/xarp.", "AI": {"tldr": "XARP Tools is an XR framework for both human and AI developers, featuring a Python library and platform-specific clients for enhanced interaction.", "motivation": "The report aims to facilitate XR development through a versatile framework that supports human developers and enables AI-driven interactions.", "method": "The framework includes a server-side Python library with high-level APIs and platform-specific XR clients that communicate via a JSON-based protocol over WebSockets.", "result": "XARP provides three modes of utilization: as a human-friendly library, a set of callable tools for AI interactions, and a Model Context Protocol server for integrating XR devices into AI systems.", "conclusion": "XARP enhances the capabilities of XR by supporting both human and AI interactions, with open access to tools and examples for developers.", "key_contributions": ["High-level APIs for easier XR development", "Integration of AI-driven interactions", "Open source availability on GitHub"], "limitations": "", "keywords": ["Extended Reality", "AI integration", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03905", "pdf": "https://arxiv.org/pdf/2508.03905.pdf", "abs": "https://arxiv.org/abs/2508.03905", "title": "Sotopia-RL: Reward Design for Social Intelligence", "authors": ["Haofei Yu", "Zhengyang Qi", "Yining Zhao", "Kolby Nottingham", "Keyang Xuan", "Bodhisattwa Prasad Majumder", "Hao Zhu", "Paul Pu Liang", "Jiaxuan You"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.", "AI": {"tldr": "Sotopia-RL is a framework that enhances reinforcement learning for training socially intelligent agents by refining feedback into utterance-level, multi-dimensional rewards, addressing challenges in social interactions.", "motivation": "To improve the training of socially intelligent agents in large language models (LLMs) by addressing challenges like partial observability and multi-dimensionality in social interactions.", "method": "The paper proposes Sotopia-RL, which categorizes feedback into utterance-level credit assignments and multi-dimensional rewards, enabling effective training in a social learning environment.", "result": "Sotopia-RL achieves state-of-the-art social goal completion scores, outperforming existing methods in the Sotopia social learning environment.", "conclusion": "The findings validate the importance of utterance-level credit assignments and multi-dimensional rewards in reinforcement learning for effective social interaction training.", "key_contributions": ["Introduction of Sotopia-RL framework for socially intelligent agents.", "Demonstration of improved training efficiency through utterance-level feedback.", "State-of-the-art performance in social goal completion in the Sotopia environment."], "limitations": "", "keywords": ["reinforcement learning", "social intelligence", "large language models", "multi-dimensional rewards", "utterance-level credit assignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04160", "pdf": "https://arxiv.org/pdf/2508.04160.pdf", "abs": "https://arxiv.org/abs/2508.04160", "title": "DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment", "authors": ["Angela Locoro", "Silvia Golia", "Davide Falessi"], "categories": ["cs.HC", "cs.CV", "K.3; K.3.2"], "comment": null, "summary": "The underspecification of progressive levels of difficulty in measurement\nconstructs design and assessment tests for data visualization literacy may\nhinder the expressivity of measurements in both test design and test reuse. To\nmitigate this problem, this paper proposes DRIVE-T (Discriminating and\nRepresentative Items for Validating Expressive Tests), a methodology designed\nto drive the construction and evaluation of assessment items. Given a data\nvizualization, DRIVE-T supports the identification of task-based items\ndiscriminability and representativeness for measuring levels of data\nvisualization literacy. DRIVE-T consists of three steps: (1) tagging task-based\nitems associated with a set of data vizualizations; (2) rating them by\nindependent raters for their difficulty; (3) analysing raters' raw scores\nthrough a Many-Facet Rasch Measurement model. In this way, we can observe the\nemergence of difficulty levels of the measurement construct, derived from the\ndiscriminability and representativeness of task-based items for each data\nvizualization, ordered into Many-Facets construct levels. In this study, we\nshow and apply each step of the methodology to an item bank, which models the\ndifficulty levels of a measurement construct approximating a latent construct\nfor data visualization literacy. This measurement construct is drawn from\nsemiotics, i.e., based on the syntax, semantics and pragmatics knowledge that\neach data visualization may require to be mastered by people. The DRIVE-T\nmethodology operationalises an inductive approach, observable in a post-design\nphase of the items preparation, for formative-style and practice-based\nmeasurement construct emergence. A pilot study with items selected through the\napplication of DRIVE-T is also presented to test our approach.", "AI": {"tldr": "This paper introduces DRIVE-T, a methodology for constructing and evaluating assessment items for data visualization literacy to address difficulty level underspecification.", "motivation": "The need to enhance the expressivity of measurements in test design and test reuse for data visualization literacy due to underspecified difficulty levels.", "method": "DRIVE-T involves tagging task-based items, rating them for difficulty by independent raters, and analyzing scores using a Many-Facet Rasch Measurement model.", "result": "The methodology enables observation of difficulty levels of measurement constructs for data visualization literacy, leveraging discerning and representative task-based items.", "conclusion": "The adoption of DRIVE-T facilitates better measurement through formative-style assessment items, demonstrated with a pilot study using the methodology.", "key_contributions": ["Introduction of the DRIVE-T methodology for assessing visualization literacy.", "Operationalization of an inductive approach for measurement construct emergence.", "Application of a pilot study demonstrating DRIVE-T's effectiveness."], "limitations": "", "keywords": ["data visualization", "assessment", "measurement constructs", "Rasch Measurement", "literacy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.03923", "pdf": "https://arxiv.org/pdf/2508.03923.pdf", "abs": "https://arxiv.org/abs/2508.03923", "title": "CoAct-1: Computer-using Agents with Coding as Actions", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation.", "AI": {"tldr": "Introducing CoAct-1, a hybrid multi-agent system combining GUI control with coding to enhance efficiency in computer automation tasks.", "motivation": "To address the inefficiencies and brittleness of autonomous agents that solely rely on GUI manipulation in complex tasks.", "method": "CoAct-1 utilizes an Orchestrator to delegate tasks dynamically between a GUI Operator and a Programmer agent capable of executing Python or Bash scripts.", "result": "CoAct-1 achieved a state-of-the-art success rate of 60.76% on the OSWorld benchmark and reduced task completion steps to 10.15 on average.", "conclusion": "Integrating coding as a core action makes autonomous agents more powerful, efficient, and scalable in general computer automation.", "key_contributions": ["Introduction of CoAct-1 multi-agent system", "Hybrid approach of combining GUI manipulation with coding execution", "Achievement of new state-of-the-art on OSWorld benchmark"], "limitations": "", "keywords": ["autonomous agents", "GUI manipulation", "coding execution", "computer automation", "multi-agent system"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.04202", "pdf": "https://arxiv.org/pdf/2508.04202.pdf", "abs": "https://arxiv.org/abs/2508.04202", "title": "Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes", "authors": ["Abdulrhman Alorini", "Yufeng Wu", "Abdullah Bin Sawad", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC"], "comment": null, "summary": "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.", "AI": {"tldr": "This study explores privacy concerns of Saudi Arabian users of smart speakers, revealing practices shaped by culture and household dynamics.", "motivation": "The privacy risks of smart speakers are underexplored in non-Western contexts, particularly in Saudi Arabia.", "method": "Cultural probes followed by semi-structured interviews with 16 Saudi Arabian participants.", "result": "Participants engage in privacy-protective behaviors such as unplugging devices, muting microphones, and avoiding voice interactions, influenced by cultural and household factors.", "conclusion": "The research provides insights into user behavior in collectivist settings, informing design for culturally sensitive voice interfaces and HCI practices.", "key_contributions": ["Empirical insights from Saudi Arabia on smart speaker usage.", "Theoretical extensions to contextual integrity frameworks.", "Design directions for culturally responsive voice interfaces."], "limitations": "Limited to the experiences of 16 participants in Saudi Arabia; may not generalize to other regions or cultures.", "keywords": ["smart speakers", "privacy", "HCI", "Saudi Arabia", "cultural probes"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.03935", "pdf": "https://arxiv.org/pdf/2508.03935.pdf", "abs": "https://arxiv.org/abs/2508.03935", "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation", "authors": ["Raymond Wilson", "Cole Graham", "Chase Carter", "Zefeng Yang", "Ruiqi Gu"], "categories": ["cs.CL"], "comment": null, "summary": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.", "AI": {"tldr": "CAP-LLM is a novel framework for personalized news headline generation, effectively balancing user preferences and factual consistency using a Large Language Model.", "motivation": "The paper addresses the challenge of creating personalized news headlines that accurately reflect user interests while maintaining factual accuracy amidst information overload.", "method": "The proposed CAP-LLM framework integrates a User Preference Encoder to capture user interests, a Context Injection Adapter to blend these preferences with article context, and a Fact-Consistency Reinforcement Module to prevent hallucinations in generated headlines.", "result": "CAP-LLM outperforms state-of-the-art models on the PENS dataset, achieving an improved FactCC score of 87.50 and enhancing personalization and content coverage metrics significantly.", "conclusion": "The results highlight CAP-LLM's effectiveness in generating personalized and factually consistent news headlines, demonstrating a successful integration of user preferences and factual constraints.", "key_contributions": ["Introduction of CAP-LLM framework for personalized headline generation", "Development of User Preference Encoder and Context Injection Adapter", "Fact-Consistency Reinforcement Module employing contrastive loss for better accuracy"], "limitations": "", "keywords": ["personalized news headlines", "large language models", "fact consistency", "user preferences", "headline generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04357", "pdf": "https://arxiv.org/pdf/2508.04357.pdf", "abs": "https://arxiv.org/abs/2508.04357", "title": "Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows", "authors": ["Gloria FernÃ¡ndez-Nieto", "Vanessa Echeverria", "Yuheng Li", "Yi-Shan Tsai", "Lele Sha", "Guanliang Chen", "Dragan Gasevic", "Zachari Swiecki"], "categories": ["cs.HC", "H.5.2"], "comment": "29 pages, 16 figures, 7 tables, submitted to Behaviours & Information\n  Technology", "summary": "Knowledge Management is crucial for capturing and transferring expertise\nwithin universities, especially in high staff turnover contexts where expertise\nloss disrupts teaching. Documenting teachers' workflows is time-intensive and\ndiverts experts from core responsibilities. Sequential Pattern Mining (SPM)\nleverages log data to identify expert workflows, offering an automated\nalternative to represent workflows but requiring transformation into intuitive\nformats for novice educators. This paper introduces Visual Process\nRepresentations (VPR), a design approach combining SPM, Knowledge Management\nprocesses, and storytelling techniques to convert expert log data into clear\nvisualisations. We detail the design phases and report a study evaluating\nvisual affordances (text lists vs. pictorial-style) and teachers' perceptions\nof four versions of the VPR with 160 higher teachers on Prolific. Results\nindicate improved task performance, usability, and engagement, particularly\nwith enriched visuals, though process memorability and task time improvements\nwere limited. The findings highlight VPR's potential to visualise workflows and\nsupport novice educators.", "AI": {"tldr": "The paper presents Visual Process Representations (VPR), a novel approach to visualizing expert workflows in knowledge management, aimed at assisting novice educators through improved clarity and engagement.", "motivation": "Address high staff turnover in universities by preserving and transferring knowledge through documenting expert workflows, which are typically lost or poorly represented.", "method": "Utilizes Sequential Pattern Mining (SPM) on log data to identify and visualize teachers' workflows, integrated with design phases that incorporate storytelling techniques.", "result": "The study involving 160 teachers demonstrated that VPR enhances task performance and usability, especially with enriched visuals; however, improvements in process memorability and task timing were limited.", "conclusion": "VPR shows significant promise in visualizing workflows for novice educators, although certain aspects like memorability need further enhancement.", "key_contributions": ["Introduction of Visual Process Representations (VPR) for knowledge transfer in education", "Integration of Sequential Pattern Mining (SPM) with visual storytelling techniques", "Empirical evaluation showcasing usability and engagement benefits of VPR"], "limitations": "Limited improvements in process memorability and task time.", "keywords": ["Knowledge Management", "Visual Process Representations", "Sequential Pattern Mining", "Education", "Teacher Workflows"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.03970", "pdf": "https://arxiv.org/pdf/2508.03970.pdf", "abs": "https://arxiv.org/abs/2508.03970", "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "comment": "Published in MIT Science Policy Review 6, 139-146 (2025)", "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.", "AI": {"tldr": "The paper discusses a comprehensive approach to govern, assess, and quantify bias in machine learning models, focusing on Large Language Models (LLMs), and proposes a governance framework for ethical AI deployment.", "motivation": "To address bias, ethics, fairness, and factuality in the development and deployment of Large Language Models.", "method": "The paper builds on the Bias Evaluation and Assessment Test Suite (BEATS) and suggests a data and AI governance framework suitable for real-world applications, enabling benchmarking, evaluation, and governance throughout the AI lifecycle.", "result": "The framework enhances the safety and responsibility of generative AI systems, mitigating risks of discrimination and protecting brand reputation through continuous evaluation and proactive governance.", "conclusion": "The article aims to promote the creation and deployment of socially responsible and ethically aligned generative AI applications.", "key_contributions": ["Introduction of a comprehensive governance framework for bias assessment in LLMs", "Implementation of real-time evaluation and governance throughout AI lifecycle", "Contribution to the discourse on ethical AI deployment"], "limitations": "", "keywords": ["Bias", "Fairness", "Governance", "Large Language Models", "Ethical AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04377", "pdf": "https://arxiv.org/pdf/2508.04377.pdf", "abs": "https://arxiv.org/abs/2508.04377", "title": "GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design", "authors": ["Gloria FernÃ¡ndez-Nieto", "Lele Sha", "Yuheng Li", "Yi-Shan Tsai", "Guanliang Chen", "Yinwei Wei", "Weiqing Wang", "Jinchun Wen", "Shaveen Singh", "Ivan Silva", "Yuanfang Li", "Dragan GasÄ›viÄ‡", "Zachari Swiecki"], "categories": ["cs.HC"], "comment": "38 pages, 10 tables, 7 figures, submitted to TOCHI", "summary": "Designing Knowledge Management Systems (KMSs) for higher education requires\naddressing complex human-technology interactions, especially where staff\nturnover and changing roles create ongoing challenges for reusing knowledge.\nWhile advances in process mining and Generative AI enable new ways of designing\nfeatures to support knowledge management, existing KMSs often overlook the\nrealities of educators' workflows, leading to low adoption and limited impact.\nThis paper presents findings from a two-year human-centred design study with\n108 higher education teachers, focused on the iterative co-design and\nevaluation of GoldMind, a KMS supporting in-the-flow knowledge management\nduring digital teaching tasks. Through three design-evaluation cycles, we\nexamined how teachers interacted with the system and how their feedback\ninformed successive refinements. Insights are synthesised across three themes:\n(1) Technology Lessons from user interaction data, (2) Design Considerations\nshaped by co-design and usability testing, and (3) Human Factors, including\ncognitive load and knowledge behaviours, analysed using Epistemic Network\nAnalysis.", "AI": {"tldr": "The paper presents a human-centered design study on the development of GoldMind, a Knowledge Management System aimed at improving digital teaching tasks in higher education, highlighting user interaction insights and design considerations.", "motivation": "To address the challenges of knowledge reuse in higher education due to staff turnover and changing roles, particularly in the context of designing effective Knowledge Management Systems (KMSs).", "method": "A two-year human-centered design study involving 108 higher education teachers, utilizing iterative co-design and evaluation cycles to refine the GoldMind KMS based on user feedback.", "result": "The study identified key insights through three main themes: lessons from user interaction data, design considerations from co-design and usability testing, and human factors impacting cognitive load and knowledge behaviors.", "conclusion": "The insights gathered through the design-evaluation cycles can guide the development of KMSs that are better aligned with the realities of educators' workflows, enhancing adoption and effectiveness in knowledge management.", "key_contributions": ["Iterative co-design approach involving educators to refine KMS features", "Identification of user interaction lessons that inform system design", "Insights on cognitive load and knowledge behaviors using Epistemic Network Analysis"], "limitations": "The study is limited to higher education contexts and may not generalize to other sectors.", "keywords": ["Knowledge Management Systems", "Human-Computer Interaction", "Generative AI", "Higher Education", "Cognitive Load"], "importance_score": 7, "read_time_minutes": 38}}
{"id": "2508.03979", "pdf": "https://arxiv.org/pdf/2508.03979.pdf", "abs": "https://arxiv.org/abs/2508.03979", "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency", "authors": ["Md Arafat Sultan", "RamÃ³n Fernandez Astudillo"], "categories": ["cs.CL"], "comment": null, "summary": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.", "AI": {"tldr": "This paper investigates making self-consistency in long chain-of-thought reasoning tasks more token-efficient through early hypothesis pruning, resulting in a token efficiency improvement of 10-35%.", "motivation": "The high token expenditure of self-consistency limits its practical utility, leading to the exploration of token-efficient methods that maintain parallelism.", "method": "The paper implements early hypothesis pruning based on model confidence and lexical coverage through a fast weighted set cover algorithm, applying this to evaluations with five LLMs across three math benchmarks.", "result": "The method shows improvements in token efficiency for all tested models, with a reduction of 10-35% in token usage in many scenarios.", "conclusion": "The proposed approach enhances the practicality of self-consistency methods in LLMs by significantly reducing token expenditure while maintaining performance.", "key_contributions": ["Introduction of early hypothesis pruning for token efficiency", "Utilization of model confidence and lexical coverage as pruning indicators", "Demonstration of improved efficiency across multiple models and benchmarks"], "limitations": "", "keywords": ["self-consistency", "token efficiency", "hypothesis pruning", "LLMs", "chain-of-thought reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04391", "pdf": "https://arxiv.org/pdf/2508.04391.pdf", "abs": "https://arxiv.org/abs/2508.04391", "title": "Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis", "authors": ["Ze Gao", "Mengyao Guo", "Zheng Wang", "Xiaolin Zhang", "Sihuang Man"], "categories": ["cs.HC"], "comment": null, "summary": "Digital ecological art represents an emergent frontier where biological media\nconverge with virtual environments. This study examines the paradigm shift from\nanthropocentric to plant-centered artistic narratives within the metaverse,\ncontextualizing how digital platforms transform ecological expression. However,\ncurrent frameworks fail to systematically guide artists in leveraging plant\nagency for digital symbiosis that transcends human-centered creation. We\npropose the Biocentric-Creation Transformation Ideology (BCTI) framework and\nvalidate it through multimodal case studies spanning bio-art, NFTs, and VR\necosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable\nunprecedented plant-algorithm co-creation, with biological artworks increasing\nby 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests\nthrough blockchain DAOs where plants govern human-plant collaborations; (3)\nAlgorithmic photosynthesis in VR environments reshapes ecological aesthetics\nthrough real-time biodata translation. The BCTI framework advances ecological\nart theory by systematizing the transition from representation to\nplant-centered agency, offering artists a blueprint for post-anthropocene\ncreation. This redefines environmental consciousness in virtual realms while\nestablishing new protocols for cross-species digital collaboration.", "AI": {"tldr": "This study introduces the Biocentric-Creation Transformation Ideology (BCTI) framework for digital ecological art, promoting plant-centered narratives in the metaverse.", "motivation": "To explore the shift from anthropocentric to plant-centered artistic narratives in digital ecological art and address existing frameworks that fail to guide artists in leveraging plant agency.", "method": "The study employs a multimodal analysis of case studies across bio-art, NFTs, and VR ecosystems from 2013 to 2023 to validate the BCTI framework.", "result": "The research finds a 133% increase in biological artworks in archives, the emergence of plant governance in DAOs, and real-time biodata translation reshaping ecological aesthetics in VR.", "conclusion": "The BCTI framework enhances ecological art theory, repositioning environmental consciousness towards a more inclusive vision that accommodates plant agency in digital art.", "key_contributions": ["Introduction of the BCTI framework for plant-centered digital art", "Quantitative analysis showing significant growth in biological artworks", "New insights into digital symbiosis and cross-species collaboration"], "limitations": "", "keywords": ["digital ecological art", "metaverse", "plant agency", "biocentric creation", "NFTs"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.03990", "pdf": "https://arxiv.org/pdf/2508.03990.pdf", "abs": "https://arxiv.org/abs/2508.03990", "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.", "AI": {"tldr": "This paper explores the ability of Large Language Models (LLMs) to generate high-quality explanations about well-being concepts tailored to diverse audiences, introducing a novel evaluation framework and demonstrating that fine-tuning significantly improves explanation quality.", "motivation": "As individuals increasingly seek guidance on well-being from LLMs, it is essential to generate explanations that are not only accurate but also suitable for varied audience expertise levels.", "method": "A large-scale dataset of 43,880 explanations for 2,194 well-being concepts was constructed from ten different LLMs. The paper introduces a principle-guided evaluation framework employing dual judges for quality assessment, alongside fine-tuning methods like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", "result": "The study found that LLM judges align well with human evaluations, reveals significant variation in explanation quality across models and audiences, and shows that DPO- and SFT-finetuned models outperform larger models in generating specialized explanations.", "conclusion": "Fine-tuning LLMs using preference-based learning yields better explanations for well-being, indicating the necessity of tailoring explanation quality to audience expertise.", "key_contributions": ["Construction of a large-scale dataset of well-being explanations from LLMs.", "Development of a principle-guided evaluation framework for assessing explanation quality.", "Demonstration that fine-tuning methods significantly enhance the effectiveness of LLMs in generating suitable explanations."], "limitations": "", "keywords": ["Large Language Models", "well-being", "explanation generation", "Supervised Fine-Tuning", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04541", "pdf": "https://arxiv.org/pdf/2508.04541.pdf", "abs": "https://arxiv.org/abs/2508.04541", "title": "Measuring Information Richness in Product Images: Implications for Online Sales", "authors": ["Zhu Yuting", "Cao Xinyu", "Su Yuzhuo", "Ma Yongbin"], "categories": ["cs.HC"], "comment": null, "summary": "A common challenge for e-commerce sellers is to decide what product images to\ndisplay on online shopping sites. In this paper, we propose and validate a\nnovel metric, k-value, to quantify the information richness of an image set,\nand we further investigate its effect on consumers' purchase decisions. We\nleverage patch-level embeddings from Vision Transformers (ViT) and apply\nk-means clustering to identify distinct visual features, defining k-value as\nthe number of clusters. An online experiment demonstrates that k-value aligns\nwith human-perceived information richness, validating the metric. A simulated\nonline shopping experiment further reveals a significant yet counterintuitive\nresult: while an image set with a higher k-value (richer information) shortens\ndecision time, it paradoxically reduces purchase propensity. Our findings\nilluminate the complex relationship between visual information richness and\nconsumer behavior, providing sellers a quantifiable tool for image selection.", "AI": {"tldr": "The paper introduces a metric called k-value to measure the information richness of product image sets in e-commerce and its impact on consumer purchase decisions, highlighting a counterintuitive effect on buying propensities.", "motivation": "To assist e-commerce sellers in selecting product images that influence consumer behavior effectively.", "method": "The authors propose the k-value metric based on k-means clustering of patch-level embeddings from Vision Transformers, accompanied by validating experiments.", "result": "The study finds that higher k-value indicates richer image information and aligns with perceived richness but paradoxically reduces purchase propensity despite shortening decision-making time.", "conclusion": "Understanding the k-value's implications helps e-commerce sellers optimize visual content for better consumer engagement while acknowledging potential adverse effects on purchasing.", "key_contributions": ["Introduction of the k-value metric for image selection in e-commerce.", "Validation of k-value through online consumer experiments.", "Insights into the relationship between visual information richness and purchasing behavior."], "limitations": "The findings may not generalize across all product categories or types of visual content.", "keywords": ["k-value", "e-commerce", "image selection", "consumer behavior", "Vision Transformers"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.03998", "pdf": "https://arxiv.org/pdf/2508.03998.pdf", "abs": "https://arxiv.org/abs/2508.03998", "title": "Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models", "authors": ["Xinyu Zhao", "Zhen Tan", "Maya Enisman", "Minjae Seo", "Marta R. Durantini", "Dolores Albarracin", "Tianlong Chen"], "categories": ["cs.CL"], "comment": "27 pages, 7 figures", "summary": "Successful group meetings, such as those implemented in group\nbehavioral-change programs, work meetings, and other social contexts, must\npromote individual goal setting and execution while strengthening the social\nrelationships within the group. Consequently, an ideal facilitator must be\nsensitive to the subtle dynamics of disengagement, difficulties with individual\ngoal setting and execution, and interpersonal difficulties that signal a need\nfor intervention. The challenges and cognitive load experienced by facilitators\ncreate a critical gap for an embodied technology that can interpret social\nexchanges while remaining aware of the needs of the individuals in the group\nand providing transparent recommendations that go beyond powerful but \"black\nbox\" foundation models (FMs) that identify social cues. We address this\nimportant demand with a social robot co-facilitator that analyzes multimodal\nmeeting data and provides discreet cues to the facilitator. The robot's\nreasoning is powered by an agentic concept bottleneck model (CBM), which makes\ndecisions based on human-interpretable concepts like participant engagement and\nsentiments, ensuring transparency and trustworthiness. Our core contribution is\na transfer learning framework that distills the broad social understanding of\nan FM into our specialized and transparent CBM. This concept-driven system\nsignificantly outperforms direct zero-shot FMs in predicting the need for\nintervention and enables real-time human correction of its reasoning.\nCritically, we demonstrate robust knowledge transfer: the model generalizes\nacross different groups and successfully transfers the expertise of senior\nhuman facilitators to improve the performance of novices. By transferring an\nexpert's cognitive model into an interpretable robotic partner, our work\nprovides a powerful blueprint for augmenting human capabilities in complex\nsocial domains.", "AI": {"tldr": "The paper presents a social robot co-facilitator that enhances group meetings by analyzing multimodal data to provide discreet cues to facilitators, utilizing a transparent agentic concept bottleneck model (CBM) powered by transfer learning.", "motivation": "To address the challenges faced by facilitators in group settings, including individual goal execution and interpersonal dynamics, by providing an embodied technology that aids in interventions and enhances group relationships.", "method": "The proposed approach employs a social robot powered by an agentic concept bottleneck model that interprets multimodal meeting data, providing recommendations based on human-interpretable concepts such as engagement and sentiment.", "result": "The model outperforms traditional zero-shot foundation models in identifying the need for intervention and successfully generalizes across various groups, demonstrating effective knowledge transfer from expert facilitators to novices.", "conclusion": "The study illustrates how interpreting social dynamics through a transparent, concept-driven model can augment human facilitators in complex social interactions, providing a framework for future technology integration in group settings.", "key_contributions": ["Development of a transfer learning framework for social robots", "Introduction of a transparent concept bottleneck model for facilitator support", "Demonstration of successful knowledge transfer from expert to novice facilitators"], "limitations": "", "keywords": ["social robot", "facilitator support", "transfer learning", "concept bottleneck model", "group dynamics"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.04634", "pdf": "https://arxiv.org/pdf/2508.04634.pdf", "abs": "https://arxiv.org/abs/2508.04634", "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations", "authors": ["Mohammed Almutairi", "Charles Chiang", "Haoze Guo", "Matthew Belcher", "Nandini Banerjee", "Maria Milkowski", "Svitlana Volkova", "Daniel Nguyen", "Tim Weninger", "Michael Yankoski", "Trenton W. Ford", "Diego Gomez-Zara"], "categories": ["cs.HC"], "comment": "5 pages, 2 figures, UIST 2025", "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.", "AI": {"tldr": "VirtLab is a customizable team simulation system that utilizes LLM-based agents to study team behaviors in complex environments, allowing for both technical and non-technical users to formulate and analyze simulations.", "motivation": "To explore hypotheses grounded in social science theories and study team behaviors using Agentic AI in complex environments.", "method": "The paper presents VirtLab, a multi-agent simulation system capable of running customizable team simulations in various spatial and temporal settings, along with a user-friendly web interface.", "result": "VirtLab successfully demonstrates its utility by comparing ground truth data with simulated scenarios, showcasing its ability to effectively simulate team collaboration.", "conclusion": "VirtLab addresses significant design and technical limitations of current team simulation frameworks, making it accessible for users without programming skills.", "key_contributions": ["User-friendly interface for both technical and non-technical users", "Customizable simulation scenarios with LLM-based agents", "Scalable multi-agent team simulations in complex environments"], "limitations": "", "keywords": ["Agentic AI", "team simulation", "HCI", "LLM-based agents", "VirtLab"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.04010", "pdf": "https://arxiv.org/pdf/2508.04010.pdf", "abs": "https://arxiv.org/abs/2508.04010", "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Keting Yin", "Juncheng Li", "Zhuosheng Zhang", "Shengyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.", "AI": {"tldr": "This paper presents HarmonyGuard, a multi-agent framework designed to enhance safety and utility in web environments by optimizing task performance while addressing emerging risks.", "motivation": "The need for web agents to balance task performance with safety in the face of evolving threats.", "method": "HarmonyGuard employs a multi-agent architecture with two main components: an adaptive Policy Agent for policy maintenance and a Utility Agent for dual-objective optimization of safety and utility.", "result": "HarmonyGuard demonstrates improved policy compliance by up to 38% and task completion by up to 20% compared to existing methods, achieving over 90% policy compliance across all tasks.", "conclusion": "The proposed framework significantly enhances the ability of agents to operate safely in uncertain web environments while effectively completing tasks.", "key_contributions": ["Introduction of the HarmonyGuard framework for multi-agent collaboration.", "Adaptive Policy Enhancement through the Policy Agent for updating security policies.", "Dual-Objective Optimization using the Utility Agent for balancing safety and utility."], "limitations": "", "keywords": ["multi-agent systems", "language models", "safety optimization", "utility performance", "web environments"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04667", "pdf": "https://arxiv.org/pdf/2508.04667.pdf", "abs": "https://arxiv.org/abs/2508.04667", "title": "How are CS students using resources and AI tools for coding tasks?", "authors": ["Natalia Echeverry", "Arun Lekshmi Narayanan"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors.", "AI": {"tldr": "Survey explores usage of AI coding assistants and chatbots among CS students.", "motivation": "To understand how computer science students utilize AI tools in coding and debugging tasks.", "method": "Conducted a survey of 26 computer science students to gather insights on their preferences and experiences with AI coding assistants and chatbots.", "result": "AI coding assistants are primarily used for writing code, while chatbots are favored for debugging. Participants prefer online resources over assistance from peers or instructors.", "conclusion": "The findings highlight the reliance on AI tools for programming tasks and suggest an evolving trend in how students seek coding help.", "key_contributions": ["Insights into the preferences of CS students regarding AI tools", "Comparison of AI assistants and human help", "Identification of primary coding and debugging resources used by students"], "limitations": "The sample size is small, and results may not be generalizable to all computer science students.", "keywords": ["AI coding assistants", "debugging", "computer science education", "student preferences", "survey"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.04012", "pdf": "https://arxiv.org/pdf/2508.04012.pdf", "abs": "https://arxiv.org/abs/2508.04012", "title": "Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing", "authors": ["Xiaopeng Li", "Shasha Li", "Xi Wang", "Shezheng Song", "Bin Ji", "Shangwen Wang", "Jun Ma", "Xiaodong Liu", "Mina Liu", "Jie Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) underpin many AI applications, but their static\nnature makes updating knowledge costly. Model editing offers an efficient\nalternative by injecting new information through targeted parameter\nmodifications. In particular, meta-learning-based model editing (MLBME) methods\nhave demonstrated notable advantages in both editing effectiveness and\nefficiency. Despite this, we find that MLBME exhibits suboptimal performance in\nlow-data scenarios, and its training efficiency is bottlenecked by the\ncomputation of KL divergence. To address these, we propose $\\textbf{S}$tep\n$\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that\nadopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation\n$\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited\nsupervision and a norm regularization on weight updates to improve training\nefficiency. Experimental results on two datasets and two LLMs demonstrate that\nSMEdit outperforms prior MLBME baselines and the MBPS strategy can be\nseamlessly integrated into existing methods to further boost their performance.\nOur code will be released soon.", "AI": {"tldr": "SMEdit is a novel meta-learning based model editing method that improves LLM editing performance in low-data scenarios and enhances training efficiency.", "motivation": "This paper addresses the high costs of updating knowledge in Large Language Models (LLMs) and the limitations of existing meta-learning based model editing methods in low-data environments.", "method": "We propose Step More Edit (SMEdit), which utilizes Multiple Backpropagation Steps (MBPS) to enhance editing performance under limited supervision, along with norm regularization for efficient training.", "result": "Experimental results show that SMEdit outperforms prior meta-learning based model editing methods and that MBPS can be integrated into existing approaches to improve their performance as well.", "conclusion": "SMEdit offers a promising approach to model editing by addressing the challenges of low-data scenarios and training efficiency, with plans to release the code for public use soon.", "key_contributions": ["Introduces SMEdit, an innovative MLBME method.", "Utilizes MBPS for enhanced performance in low-data scenarios.", "Implements norm regularization for improved training efficiency."], "limitations": "", "keywords": ["Large Language Models", "meta-learning", "model editing", "low-data scenarios", "training efficiency"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.04679", "pdf": "https://arxiv.org/pdf/2508.04679.pdf", "abs": "https://arxiv.org/abs/2508.04679", "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models", "authors": ["Amit Kumar Das", "Klaus Mueller"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.", "AI": {"tldr": "MisVisFix is an interactive dashboard that uses Large Language Models to detect, explain, and correct misleading visualizations, achieving high accuracy in identifying visualization issues.", "motivation": "Misleading visualizations create challenges in accurate data interpretation, necessitating tools for detection and correction.", "method": "An interactive dashboard utilizing Claude and GPT models for identifying, explaining, and correcting visualization misinformation, coupled with user interaction capabilities.", "result": "MisVisFix identifies 96% of visualization issues and classifies all 74 types of misinformation, providing detailed explanations and actionable suggestions.", "conclusion": "MisVisFix enhances visualization literacy and helps ensure trustworthy data communication by making LLM-based detection accessible and interactive.", "key_contributions": ["Development of MisVisFix, an interactive dashboard for detecting and correcting misleading visualizations.", "Achieving 96% accuracy in identifying visualization issues and offering actionable suggestions.", "Adapting to new misinformation strategies through user interactions."], "limitations": "", "keywords": ["visualizations", "Large Language Models", "data communication", "misinformation detection", "interactive dashboard"], "importance_score": 9, "read_time_minutes": 11}}
{"id": "2508.04038", "pdf": "https://arxiv.org/pdf/2508.04038.pdf", "abs": "https://arxiv.org/abs/2508.04038", "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents", "authors": ["Zechen Li", "Baiyu Chen", "Hao Xue", "Flora D. Salim"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.", "AI": {"tldr": "ZARA is a novel, zero-shot explainable human activity recognition (HAR) framework that processes raw motion sensor time-series data without requiring fine-tuning.", "motivation": "Existing HAR methods require costly retraining for new activities or sensor setups, and previous approaches using LLMs lack accuracy and interpretability.", "method": "ZARA employs a feature knowledge base, a multi-sensor retrieval module, and a hierarchical agent pipeline that enables the direct prediction of activities and generation of natural-language explanations from raw motion data.", "result": "ZARA achieves state-of-the-art zero-shot performance on 8 HAR benchmarks, outperforming existing methods by 2.53x in macro F1 score and providing clear reasoning for its predictions.", "conclusion": "ZARA represents a significant advance in HAR, enabling flexible and interpretable analysis of motion time-series data without the need for fine-tuning.", "key_contributions": ["Introduction of a zero-shot framework for HAR from raw motion time-series", "Integration of a knowledge base for pair-wise feature statistics", "Hierarchical agent pipeline that generates predictions and explanations"], "limitations": "", "keywords": ["Human Activity Recognition", "Zero-shot Learning", "Large Language Models", "Motion Sensor", "Explainable AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04039", "pdf": "https://arxiv.org/pdf/2508.04039.pdf", "abs": "https://arxiv.org/abs/2508.04039", "title": "Large Reasoning Models Are Autonomous Jailbreak Agents", "authors": ["Thilo Hagendorff", "Erik Derner", "Nuria Oliver"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has\ntraditionally required complex technical procedures or specialized human\nexpertise. In this study, we show that the persuasive capabilities of large\nreasoning models (LRMs) simplify and scale jailbreaking, converting it into an\ninexpensive activity accessible to non-experts. We evaluated the capabilities\nof four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as\nautonomous adversaries conducting multi-turn conversations with nine widely\nused target models. LRMs received instructions via a system prompt, before\nproceeding to planning and executing jailbreaks with no further supervision. We\nperformed extensive experiments with a benchmark of harmful prompts composed of\n70 items covering seven sensitive domains. This setup yielded an overall attack\nsuccess rate across all model combinations of 97.14%. Our study reveals an\nalignment regression, in which LRMs can systematically erode the safety\nguardrails of other models, highlighting the urgent need to further align\nfrontier models not only to resist jailbreak attempts, but also to prevent them\nfrom being co-opted into acting as jailbreak agents.", "AI": {"tldr": "This study demonstrates that large reasoning models (LRMs) can autonomously perform jailbreaking on AI models, significantly simplifying the process for non-experts with an attack success rate of 97.14%.", "motivation": "The motivation behind this study is to highlight how LRMs can undermine the safety mechanisms of AI models, making jailbreaking accessible to those without technical expertise.", "method": "The methodology involved evaluating four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) as adversaries in multi-turn conversations with nine target models. They received system prompts to plan and execute jailbreaks without supervision.", "result": "The results showed an overall attack success rate of 97.14% across all model combinations, indicating that LRMs effectively bypass safety mechanisms of other models.", "conclusion": "The conclusion drawn is that there is a significant alignment regression, necessitating immediate efforts to enhance the alignment of frontier models to resist jailbreaks and prevent their misuse as jailbreak agents.", "key_contributions": ["Demonstrated LRM capabilities in autonomously conducting jailbreaks", "High attack success rate (97.14%) across models", "Identified alignment regression issue in AI safety mechanisms"], "limitations": "", "keywords": ["Jailbreaking", "Large Reasoning Models", "AI Safety", "Adversarial Attacks", "Model Alignment"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.04047", "pdf": "https://arxiv.org/pdf/2508.04047.pdf", "abs": "https://arxiv.org/abs/2508.04047", "title": "DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation", "authors": ["Jiabing Yang", "Yixiang Chen", "Zichen Wen", "Chenhang Cui", "Peiyan Li", "Yuan Xu", "Bowen Fang", "Yan Huang", "Liang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Controllable Text Generation (CTG) is a vital subfield in Natural Language\nProcessing (NLP), aiming to generate text that aligns with desired attributes.\nHowever, previous studies commonly focus on the quality of controllable text\ngeneration for short sequences, while the generation of long-form text remains\nlargely underexplored. In this paper, we observe that the controllability of\ntexts generated by the powerful prefix-based method Air-Decoding tends to\ndecline with increasing sequence length, which we hypothesize primarily arises\nfrom the observed decay in attention to the prefixes. Meanwhile, different\ntypes of prefixes including soft and hard prefixes are also key factors\ninfluencing performance. Building on these insights, we propose a lightweight\nand effective framework called Dynamic Token-level Prefix Augmentation (DTPA)\nbased on Air-Decoding for controllable text generation. Specifically, it first\nselects the optimal prefix type for a given task. Then we dynamically amplify\nthe attention to the prefix for the attribute distribution to enhance\ncontrollability, with a scaling factor growing exponentially as the sequence\nlength increases. Moreover, based on the task, we optionally apply a similar\naugmentation to the original prompt for the raw distribution to balance text\nquality. After attribute distribution reconstruction, the generated text\nsatisfies the attribute constraints well. Experiments on multiple CTG tasks\ndemonstrate that DTPA generally outperforms other methods in attribute control\nwhile maintaining competitive fluency, diversity, and topic relevance. Further\nanalysis highlights DTPA's superior effectiveness in long text generation.", "AI": {"tldr": "This paper presents Dynamic Token-level Prefix Augmentation (DTPA), a novel framework for controllable text generation, particularly for long-form text, addressing challenges in maintaining attribute control with increasing sequence lengths.", "motivation": "The need for effective controllable text generation in long-form sequences, as existing methods primarily fail in this aspect, particularly with prefix-based techniques.", "method": "The proposed DTPA framework selects the optimal prefix type and amplifies attention to the prefix dynamically, using an exponential scaling factor as the sequence length increases, while optionally augmenting the original prompt.", "result": "Experiments show that DTPA outperforms existing methods in attribute control in controllable text generation tasks, especially for longer texts, while maintaining fluency and diversity.", "conclusion": "DTPA effectively enhances controllability in long-form text generation by optimizing prefix usage and maintaining quality.", "key_contributions": ["Introduction of the DTPA framework for enhanced controllability in long-form text generation", "Demonstration of the importance of prefix types in text generation", "Evaluation showing superior performance of DTPA in various controllable text generation tasks"], "limitations": "", "keywords": ["Controllable Text Generation", "Dynamic Token-level Prefix Augmentation", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04057", "pdf": "https://arxiv.org/pdf/2508.04057.pdf", "abs": "https://arxiv.org/abs/2508.04057", "title": "PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for\nenhancing large language models (LLMs) with external knowledge. However,\ncurrent RAG systems face two critical limitations: (1) they inefficiently\nretrieve information for every query, including simple questions that could be\nresolved using the LLM's parametric knowledge alone, and (2) they risk\nretrieving irrelevant documents when queries contain sparse information\nsignals. To address these gaps, we introduce Parametric-verified Adaptive\nInformation Retrieval and Selection (PAIRS), a training-free framework that\nintegrates parametric and retrieved knowledge to adaptively determine whether\nto retrieve and how to select external information. Specifically, PAIRS employs\na dual-path generation mechanism: First, the LLM produces both a direct answer\nand a context-augmented answer using self-generated pseudo-context. When these\noutputs converge, PAIRS bypasses external retrieval entirely, dramatically\nimproving the RAG system's efficiency. For divergent cases, PAIRS activates a\ndual-path retrieval (DPR) process guided by both the original query and\nself-generated contextual signals, followed by an Adaptive Information\nSelection (AIS) module that filters documents through weighted similarity to\nboth sources. This simple yet effective approach can not only enhance\nefficiency by eliminating unnecessary retrievals but also improve accuracy\nthrough contextually guided retrieval and adaptive information selection.\nExperimental results on six question-answering (QA) benchmarks show that PAIRS\nreduces retrieval costs by around 25% (triggering for only 75% of queries)\nwhile still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior\nbaselines on average.", "AI": {"tldr": "The PAIRS framework enhances Retrieval-Augmented Generation by adaptively deciding when to retrieve and how to select external information, improving efficiency and accuracy.", "motivation": "Address inefficiencies in current RAG systems that retrieve unnecessary information and risk drawing irrelevant documents from sparse queries.", "method": "PAIRS uses a dual-path generation mechanism where the LLM generates both direct and context-augmented answers to determine if external retrieval is needed, followed by a dual-path retrieval process when necessary.", "result": "PAIRS reduces retrieval costs by approximately 25%, triggering for only 75% of queries while improving accuracy by +1.1% EM and +1.0% F1 on average across six QA benchmarks.", "conclusion": "The PAIRS framework significantly enhances the efficiency and accuracy of RAG systems by streamlining the retrieval process based on context and need.", "key_contributions": ["Introduces a training-free framework for efficient information retrieval in LLMs.", "Implements a dual-path generation mechanism to determine necessity of external knowledge retrieval.", "Shows empirical improvements in efficiency and accuracy on QA benchmarks."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Information Retrieval"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.04073", "pdf": "https://arxiv.org/pdf/2508.04073.pdf", "abs": "https://arxiv.org/abs/2508.04073", "title": "Efficient Strategy for Improving Large Language Model (LLM) Capabilities", "authors": ["JuliÃ¡n Camilo Velandia GutiÃ©rrez"], "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; I.5.1"], "comment": "Based on master's thesis in Systems and Computer Engineering,\n  Universidad Nacional de Colombia (2025)", "summary": "Large Language Models (LLMs) have become a milestone in the field of\nartificial intelligence and natural language processing. However, their\nlarge-scale deployment remains constrained by the need for significant\ncomputational resources. This work proposes starting from a base model to\nexplore and combine data processing and careful data selection techniques,\ntraining strategies, and architectural adjustments to improve the efficiency of\nLLMs in resource-constrained environments and within a delimited knowledge\nbase. The methodological approach included defining criteria for building\nreliable datasets, conducting controlled experiments with different\nconfigurations, and systematically evaluating the resulting variants in terms\nof capability, versatility, response time, and safety. Finally, comparative\ntests were conducted to measure the performance of the developed variants and\nto validate the effectiveness of the proposed strategies. This work is based on\nthe master's thesis in Systems and Computer Engineering titled \"Efficient\nStrategy for Improving the Capabilities of Large Language Models (LLMs)\".", "AI": {"tldr": "This paper explores methods to improve the efficiency of Large Language Models (LLMs) in resource-constrained environments by focusing on data processing, training strategies, and architectural adjustments.", "motivation": "To address the high computational resource needs hindering the large-scale deployment of LLMs.", "method": "The authors defined criteria for reliable dataset construction, conducted controlled experiments with various configurations, and systematically evaluated the resulting model variants in terms of capability, versatility, response time, and safety.", "result": "The comparative tests demonstrated that the proposed strategies effectively improved the performance of LLM variants in terms of efficiency.", "conclusion": "The research supports the viability of enhancing LLM efficiency through careful data selection and tailored training methods in constrained environments.", "key_contributions": ["Introduction of data selection techniques for LLMs in low-resource settings", "Development of training strategies to enhance model efficiency", "Systematic evaluation of LLM variants and their performance metrics"], "limitations": "The results may be limited to specific architectures and datasets defined in the experiments.", "keywords": ["Large Language Models", "data processing", "machine learning", "efficiency", "resource-constrained environments"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04086", "pdf": "https://arxiv.org/pdf/2508.04086.pdf", "abs": "https://arxiv.org/abs/2508.04086", "title": "ToolGrad: Efficient Tool-use Dataset Generation with Textual \"Gradients\"", "authors": ["Zhongyi Zhou", "Kohei Uehara", "Haoyu Zhang", "Jingtao Zhou", "Lin Gu", "Ruofei Du", "Zheng Xu", "Tatsuya Harada"], "categories": ["cs.CL"], "comment": null, "summary": "Prior work synthesizes tool-use LLM datasets by first generating a user\nquery, followed by complex tool-use annotations like DFS. This leads to\ninevitable annotation failures and low efficiency in data generation. We\nintroduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad\nfirst constructs valid tool-use chains through an iterative process guided by\ntextual \"gradients\", and then synthesizes corresponding user queries. This\n\"answer-first\" approach led to ToolGrad-5k, a dataset generated with more\ncomplex tool use, lower cost, and 100% pass rate. Experiments show that models\ntrained on ToolGrad-5k outperform those on expensive baseline datasets and\nproprietary LLMs, even on OOD benchmarks.", "AI": {"tldr": "ToolGrad introduces an answer-first approach to synthesize tool-use datasets, resulting in higher quality data and improved model performance.", "motivation": "Prior methods of dataset generation for tool-use LLMs faced challenges of annotation failures and inefficiencies, necessitating a more effective approach.", "method": "ToolGrad constructs tool-use chains iteratively using textual gradients, followed by synthesizing user queries, thereby inverting the traditional annotation process.", "result": "ToolGrad-5k dataset features complex tool use at a lower cost with a 100% pass rate, leading to improved model performance compared to baseline datasets.", "conclusion": "The agentic framework of ToolGrad not only enhances dataset quality but also outperforms existing tools in training models, addressing critical shortcomings in previous methodologies.", "key_contributions": ["Introduction of ToolGrad framework for dataset generation", "Creation of ToolGrad-5k with improved annotation efficiency", "Demonstrated superior model performance on OOD benchmarks"], "limitations": "", "keywords": ["tool-use", "dataset synthesis", "LLM", "Machine Learning", "AI applications"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.03990", "pdf": "https://arxiv.org/pdf/2508.03990.pdf", "abs": "https://arxiv.org/abs/2508.03990", "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) can generate tailored explanations of well-being concepts and evaluates their quality through a novel framework.", "motivation": "To address the challenge of generating accurate and audience-tailored explanations of well-being concepts using LLMs, which are increasingly being consulted for understanding well-being.", "method": "We constructed a large-scale dataset of 43,880 explanations for 2,194 well-being concepts from ten diverse LLMs and developed a principle-guided evaluation framework using dual judges to assess quality.", "result": "Our evaluation indicates that the LLM judges' assessments align closely with human evaluations, showing variability in explanation quality across different models and target audiences. Fine-tuning LLMs using SFT and DPO significantly enhances the quality of generated explanations.", "conclusion": "The findings underscore the potential of preference-based learning methods like DPO and SFT to improve LLM outputs for specialized explanation tasks in well-being.", "key_contributions": ["Creation of a large-scale dataset of well-being explanations", "Introduction of a novel evaluation framework for LLMs", "Demonstration of improved explanation quality through fine-tuning methodologies"], "limitations": "", "keywords": ["Large Language Models", "well-being", "explanation quality", "Supervised Fine-Tuning", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04088", "pdf": "https://arxiv.org/pdf/2508.04088.pdf", "abs": "https://arxiv.org/abs/2508.04088", "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning", "authors": ["Jianghangfan Zhang", "Yibo Yan", "Kening Zheng", "Xin Zou", "Song Dai", "Xuming Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance.", "AI": {"tldr": "Introduction of a novel Generative Multimodal Process Reward Model (GM-PRM) that improves mathematical reasoning in MLLMs by correcting errors and providing detailed step analysis.", "motivation": "Addressing the limitations of current multimodal PRMs that are limited to binary verification and lack correction and explanatory capabilities.", "method": "GM-PRM transforms the PRM into an active reasoning collaborator that not only identifies errors but also generates corrections for them, guiding the policy model towards better reasoning.", "result": "GM-PRM achieves state-of-the-art performance on multimodal math benchmarks with significant improvements in policy model performance and data efficiency.", "conclusion": "The introduction of corrective capabilities in GM-PRM enhances the reasoning process of MLLMs, thereby improving overall solution quality.", "key_contributions": ["Introduction of GM-PRM that allows for corrective capabilities in reasoning tasks.", "Active reasoning collaboration instead of passive error identification.", "State-of-the-art results on multimodal math benchmarks with improved data efficiency."], "limitations": "", "keywords": ["Multimodal Large Language Models", "Process Reward Models", "Mathematical Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04337", "pdf": "https://arxiv.org/pdf/2508.04337.pdf", "abs": "https://arxiv.org/abs/2508.04337", "title": "Modelling and Classifying the Components of a Literature Review", "authors": ["Francisco BolaÃ±os", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.", "AI": {"tldr": "This paper introduces a novel annotation schema for classifying rhetorical roles in scientific literature to aid in literature review generation. It evaluates various LLMs on a new dataset, demonstrating that fine-tuning results in high classification accuracy.", "motivation": "To improve AI methods for analyzing scientific literature by defining an effective annotation schema and establishing strategies for large-scale literature annotation.", "method": "A novel annotation schema designed for literature review generation was introduced, alongside the development of the Sci-Sentence benchmark. This involved evaluating 37 state-of-the-art LLMs using both zero-shot and fine-tuning approaches on the benchmark.", "result": "The study found that LLMs fine-tuned on high-quality data surpassed 96% F1 performance. Proprietary models excelled, but several open-source models also showed impressive results. Moreover, incorporating semi-synthetic training data improved performance, especially for smaller models.", "conclusion": "The findings reveal significant advancements in LLM capabilities for classifying rhetorical roles in scientific texts, with implications for enhancing literature review generation.", "key_contributions": ["Introduction of a novel annotation schema for rhetorical roles", "Creation of the Sci-Sentence benchmark for evaluating LLMs", "Insights into the performance of various LLMs and the benefits of semi-synthetic data"], "limitations": "The study is limited to the specific rhetorical roles and may not encompass the full diversity of scientific literature annotations.", "keywords": ["annotation schema", "literature review generation", "large language models", "Sci-Sentence", "classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04117", "pdf": "https://arxiv.org/pdf/2508.04117.pdf", "abs": "https://arxiv.org/abs/2508.04117", "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks", "authors": ["Zhiwen Ruan", "Yun Chen", "Yutao Hou", "Peng Li", "Yang Liu", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "The pretrained large language models (LLMs) are finetuned with labeled data\nfor better instruction following ability and alignment with human values. In\nthis paper, we study the learning dynamics of LLM finetuning on reasoning tasks\nand reveal the uncovered over-memorization phenomenon during a specific stage\nof LLM finetuning. At this stage, the LLMs have excessively memorized training\ndata and exhibit high test perplexity while maintaining good test accuracy. We\ninvestigate the conditions that lead to LLM over-memorization and find that\ntraining epochs and large learning rates contribute to this issue. Although\nmodels with over-memorization demonstrate comparable test accuracy to normal\nmodels, they suffer from reduced robustness, poor out-of-distribution\ngeneralization, and decreased generation diversity. Our experiments unveil the\nover-memorization to be broadly applicable across different tasks, models, and\nfinetuning methods. Our research highlights that overparameterized, extensively\nfinetuned LLMs exhibit unique learning dynamics distinct from traditional\nmachine learning models. Based on our observations of over-memorization, we\nprovide recommendations on checkpoint and learning rate selection during\nfinetuning.", "AI": {"tldr": "This paper investigates the phenomenon of over-memorization in pretrained large language models (LLMs) during finetuning, revealing its implications for model performance and generalization.", "motivation": "To understand and address the challenges faced by LLMs during finetuning, particularly the over-memorization problem which affects their robustness and generalization.", "method": "The authors conduct experiments on various tasks and models to analyze the learning dynamics of LLMs during finetuning, focusing on the conditions leading to over-memorization.", "result": "The study finds that large learning rates and extended training epochs contribute to over-memorization, which though yields high test accuracy, compromises model robustness and diversity in generation.", "conclusion": "The findings underline the importance of careful checkpoint and learning rate selection to mitigate over-memorization in LLM finetuning, advocating for adjustments in training practices.", "key_contributions": ["Identification of over-memorization in LLMs during finetuning phases", "Analysis of factors leading to over-memorization such as learning rates and epochs", "Recommendations for improved checkpoint and learning rate strategies during finetuning"], "limitations": "", "keywords": ["Large Language Models", "Finetuning", "Over-memorization", "Learning Dynamics", "Robustness"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.04149", "pdf": "https://arxiv.org/pdf/2508.04149.pdf", "abs": "https://arxiv.org/abs/2508.04149", "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap", "authors": ["Xuan Qi", "Rongwu Xu", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select", "summary": "Aligning large language models (LLMs) with human preferences is a critical\nchallenge in AI research. While methods like Reinforcement Learning from Human\nFeedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they\noften rely on large, costly preference datasets. The current work lacks methods\nfor high-quality data selection specifically for preference data. In this work,\nwe introduce a novel difficulty-based data selection strategy for preference\ndatasets, grounded in the DPO implicit reward mechanism. By selecting\npreference data examples with smaller DPO implicit reward gaps, which are\nindicative of more challenging cases, we improve data efficiency and model\nalignment. Our approach consistently outperforms five strong baselines across\nmultiple datasets and alignment tasks, achieving superior performance with only\n10\\% of the original data. This principled, efficient selection method offers a\npromising solution for scaling LLM alignment with limited resources.", "AI": {"tldr": "This paper presents a novel data selection strategy for preference datasets to enhance the alignment of large language models (LLMs) with human preferences by focusing on more challenging examples.", "motivation": "Aligning LLMs with human preferences is majorly hindered by the reliance on large, costly preference datasets and the lack of efficient data selection methods.", "method": "The authors propose a difficulty-based data selection strategy that focuses on examples with smaller DPO implicit reward gaps to improve efficiency and model alignment.", "result": "The proposed method consistently outperforms five baselines across multiple datasets, achieving better performance with only 10% of the original data.", "conclusion": "The study demonstrates that the difficulty-based data selection approach can effectively scale LLM alignment efforts while reducing the amount of data needed.", "key_contributions": ["Introduction of difficulty-based data selection for preference datasets", "Improved data efficiency and model alignment performance", "Performance achieved with only 10% of the original data compared to strong baselines."], "limitations": "", "keywords": ["Large Language Models", "Human Preferences", "Data Selection", "Reinforcement Learning from Human Feedback", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04179", "pdf": "https://arxiv.org/pdf/2508.04179.pdf", "abs": "https://arxiv.org/abs/2508.04179", "title": "The State Of TTS: A Case Study with Human Fooling Rates", "authors": ["Praveen Srinivasa Varadhan", "Sherry Thomas", "Sai Teja M. S.", "Suvrat Bhooshan", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at InterSpeech 2025", "summary": "While subjective evaluations in recent years indicate rapid progress in TTS,\ncan current TTS systems truly pass a human deception test in a Turing-like\nevaluation? We introduce Human Fooling Rate (HFR), a metric that directly\nmeasures how often machine-generated speech is mistaken for human. Our\nlarge-scale evaluation of open-source and commercial TTS models reveals\ncritical insights: (i) CMOS-based claims of human parity often fail under\ndeception testing, (ii) TTS progress should be benchmarked on datasets where\nhuman speech achieves high HFRs, as evaluating against monotonous or less\nexpressive reference samples sets a low bar, (iii) Commercial models approach\nhuman deception in zero-shot settings, while open-source systems still struggle\nwith natural conversational speech; (iv) Fine-tuning on high-quality data\nimproves realism but does not fully bridge the gap. Our findings underscore the\nneed for more realistic, human-centric evaluations alongside existing\nsubjective tests.", "AI": {"tldr": "The paper evaluates TTS systems using a new metric, Human Fooling Rate (HFR), revealing limitations in current technology and emphasizing the need for realistic evaluations that better represent human speech.", "motivation": "To assess the effectiveness of current TTS systems in passing a human deception test and enhancing the evaluation methods used in TTS research.", "method": "A large-scale evaluation comparing open-source and commercial TTS models using the Human Fooling Rate (HFR) metric to measure how often machine-generated speech is mistaken for human speech.", "result": "Commercial TTS models show promise in zero-shot settings but struggle to match the naturalness of human speech, while open-source systems lag significantly. Fine-tuning data improves output quality but does not eliminate the gap in realism.", "conclusion": "The study highlights the need for improved evaluation criteria that focus on human-centric benchmarks rather than outdated reference samples to better gauge TTS systems' real-world applicability.", "key_contributions": ["Introduction of the Human Fooling Rate (HFR) metric for TTS evaluation", "Identification of gaps between commercial and open-source TTS models", "Emphasis on the importance of realistic human-centric evaluation methods"], "limitations": "The study primarily evaluates current TTS systems and may not address future advancements or the broader landscape of speech synthesis technologies.", "keywords": ["TTS", "Human Fooling Rate", "speech synthesis", "evaluation metrics", "human deception"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.04182", "pdf": "https://arxiv.org/pdf/2508.04182.pdf", "abs": "https://arxiv.org/abs/2508.04182", "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity", "authors": ["Peizheng Guo", "Jingyao Wang", "Wenwen Qiang", "Huijie Guo", "Changwen Zheng", "Jiahuan Zhou", "Gang Hua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across vision-language tasks. However, they may suffer from\nhallucinations--generating outputs that are semantically inconsistent with the\ninput image or text. Through causal analyses, we find that: (i) hallucinations\nwith omission may arise from the failure to adequately capture essential causal\nfactors, and (ii) hallucinations with fabrication are likely caused by the\nmodel being misled by non-causal cues. To address these challenges, we propose\na novel reinforcement learning framework guided by causal completeness, which\njointly considers both causal sufficiency and causal necessity of tokens.\nSpecifically, we evaluate each token's standalone contribution and\ncounterfactual indispensability to define a token-level causal completeness\nreward. This reward is used to construct a causally informed advantage function\nwithin the GRPO optimization framework, encouraging the model to focus on\ntokens that are both causally sufficient and necessary for accurate generation.\nExperimental results across various benchmark datasets and tasks demonstrate\nthe effectiveness of our approach, which effectively mitigates hallucinations\nin MLLMs.", "AI": {"tldr": "This paper presents a reinforcement learning framework to mitigate hallucinations in Multimodal Large Language Models by focusing on causal completeness of tokens.", "motivation": "Multimodal Large Language Models (MLLMs) experience issues with hallucinations, leading to outputs that do not align with the input data.", "method": "A novel reinforcement learning framework guided by causal completeness is proposed, assessing each token's standalone contribution and counterfactual indispensability to create a token-level causal completeness reward for MLLMs.", "result": "Experimental results indicate that the proposed approach significantly reduces hallucinations in MLLMs across various tasks.", "conclusion": "The method effectively encourages the model to focus on tokens that are causally necessary for accurate output, improving the reliability of MLLMs.", "key_contributions": ["Introduces a causal completeness framework for token evaluation in MLLMs.", "Develops a novel advantage function within the GRPO optimization framework.", "Demonstrates substantial improvements in reducing hallucinations across benchmark datasets."], "limitations": "", "keywords": ["Multimodal Large Language Models", "hallucinations", "causal analyses", "reinforcement learning", "token-level causal completeness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2311.06381", "pdf": "https://arxiv.org/pdf/2311.06381.pdf", "abs": "https://arxiv.org/abs/2311.06381", "title": "Optimal Fidelity Selection for Human-Supervised Search", "authors": ["Piyush Gupta", "Vaibhav Srivastava"], "categories": ["cs.HC"], "comment": null, "summary": "We study optimal fidelity selection in human-supervised underwater visual\nsearch, where operator performance is affected by cognitive factors like\nworkload and fatigue. In our experiments, participants perform two simultaneous\ntasks: detecting underwater mines in videos (primary) and responding to a\nvisual cue to estimate workload (secondary). Videos arrive as a Poisson process\nand queue for review, with the operator choosing between normal fidelity\n(faster playback) and high fidelity. Rewards are based on detection accuracy,\nwhile penalties depend on queue length. Workload is modeled as a hidden state\nusing an Input-Output Hidden Markov Model, and fidelity selection is optimized\nvia a Partially Observable Markov Decision Process. We evaluate two setups:\nfidelity-only selection and a version allowing task delegation to automation to\nmaintain queue stability. Our approach improves performance by 26.5% without\ndelegation and 50.3% with delegation, compared to a baseline where humans\nmanually choose their fidelity levels.", "AI": {"tldr": "This paper investigates optimal fidelity selection in human-supervised underwater visual search, focusing on cognitive workloads and task performance.", "motivation": "The research aims to optimize operator performance in underwater mine detection by addressing cognitive factors like workload and fatigue during task execution.", "method": "Experiments involved participants completing a primary task of underwater mine detection while estimating workload in a secondary task. The fidelity selection was modeled as a Partially Observable Markov Decision Process using input from a hidden state representing workload.", "result": "Performance improved by 26.5% without task delegation and by 50.3% with delegation compared to a baseline where humans chose fidelity levels manually.", "conclusion": "The approach shows significant improvements in operator performance, suggesting that optimal fidelity selection can enhance efficiency in high-stakes environments like underwater search.", "key_contributions": ["Introduces a cognitive workload model in fidelity selection for visual search tasks.", "Demonstrates a methodology for optimizing fidelity based on operator performance metrics.", "Evaluates the impact of task delegation on performance outcomes in human-supervised systems."], "limitations": "The study is limited to underwater visual search scenarios and may not generalize to other types of visual tasks.", "keywords": ["human-supervised", "visual search", "underwater mines", "fidelity selection", "Markov Decision Process"], "importance_score": 3, "read_time_minutes": 8}}
{"id": "2508.04183", "pdf": "https://arxiv.org/pdf/2508.04183.pdf", "abs": "https://arxiv.org/abs/2508.04183", "title": "Characterizing Deep Research: A Benchmark and Formal Definition", "authors": ["Abhinav Java", "Ashmit Khandelwal", "Sukruta Midigeshi", "Aaron Halfaker", "Amit Deshpande", "Navin Goyal", "Ankur Gupta", "Nagarajan Natarajan", "Amit Sharma"], "categories": ["cs.CL"], "comment": "First three authors contributed equally (ordered alphabetically)", "summary": "Information tasks such as writing surveys or analytical reports require\ncomplex search and reasoning, and have recently been grouped under the umbrella\nof \\textit{deep research} -- a term also adopted by recent models targeting\nthese capabilities. Despite growing interest, the scope of the deep research\ntask remains underdefined and its distinction from other reasoning-intensive\nproblems is poorly understood. In this paper, we propose a formal\ncharacterization of the deep research (DR) task and introduce a benchmark to\nevaluate the performance of DR systems. We argue that the core defining feature\nof deep research is not the production of lengthy report-style outputs, but\nrather the high fan-out over concepts required during the search process, i.e.,\nbroad and reasoning-intensive exploration. To enable objective evaluation, we\ndefine DR using an intermediate output representation that encodes key claims\nuncovered during search-separating the reasoning challenge from surface-level\nreport generation. Based on this formulation, we propose a diverse, challenging\nbenchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,\ndatasets, materials discovery, prior art search) and public interest events\n(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1\nscore ranges between 0.02 and 0.72 for any sub-category. OpenAI's model\nperforms the best with an overall F1 score of 0.55. Analysis of reasoning\ntraces reveals the distribution over the number of referenced sources,\nbranching, and backtracking events executed by current DR systems, motivating\nfuture directions for improving their search mechanisms and grounding\ncapabilities. The benchmark is available at\nhttps://github.com/microsoft/LiveDRBench.", "AI": {"tldr": "This paper formalizes the deep research task, differentiating it from other reasoning-intensive tasks, proposing a benchmark (LiveDRBench) to evaluate DR systems, and analyzing current performance.", "motivation": "The paper addresses the need for a clear definition and evaluation of deep research tasks, which involve complex search and reasoning processes.", "method": "The authors propose a formal characterization of deep research based on high fan-out search processes, and introduce LiveDRBench, a benchmark consisting of 100 challenging tasks.", "result": "The best-performing DR system achieves an F1 score of 0.55, highlighting the need for improved reasoning mechanisms in DR systems.", "conclusion": "A structured benchmark for evaluating deep research can enhance the understanding and performance of reasoning systems in information tasks.", "key_contributions": ["Formal characterization of the deep research task", "Introduction of LiveDRBench for benchmark evaluation", "Analysis of reasoning traces to inform future research"], "limitations": "Current F1 scores of DR systems are low, indicating performance issues that need to be addressed.", "keywords": ["deep research", "benchmark", "reasoning", "search", "evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.00873", "pdf": "https://arxiv.org/pdf/2410.00873.pdf", "abs": "https://arxiv.org/abs/2410.00873", "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences", "authors": ["Zahra Ashktorab", "Michael Desmond", "Qian Pan", "James M. Johnson", "Martin Santillan Cooper", "Elizabeth M. Daly", "Rahul Nair", "Tejaswini Pedapati", "Hyo Jin Do", "Werner Geyer"], "categories": ["cs.HC"], "comment": null, "summary": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations.", "AI": {"tldr": "The paper explores how task-related factors and assessment strategies impact LLM outputs evaluation, highlighting the effectiveness of direct assessment methods.", "motivation": "To improve the evaluation process of large language model outputs and make it more efficient for users.", "method": "Conducted a study with 15 machine learning practitioners completing 6 tasks to yield 131 evaluations, analyzing how different approaches influenced user perceptions and criteria refinement.", "result": "Users preferred direct assessment for its efficiency, adaptability, and ability to modify evaluations based on task-specific criteria.", "conclusion": "The study concludes with recommendations for enhancing front-end tools to better facilitate interactions in LLM-assisted evaluations.", "key_contributions": ["Exploration of task-related factors in LLM evaluations", "Comparison of direct assessment and pairwise comparison methods", "Recommendations for improving front-end tools for LLM evaluation"], "limitations": "", "keywords": ["Large Language Models", "Evaluation Methods", "User Perceptions", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04196", "pdf": "https://arxiv.org/pdf/2508.04196.pdf", "abs": "https://arxiv.org/abs/2508.04196", "title": "Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models", "authors": ["Siddhant Panpatil", "Hiskias Dingeto", "Haon Park"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite significant advances in alignment techniques, we demonstrate that\nstate-of-the-art language models remain vulnerable to carefully crafted\nconversational scenarios that can induce various forms of misalignment without\nexplicit jailbreaking. Through systematic manual red-teaming with\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\nfundamental vulnerabilities in how current alignment methods handle narrative\nimmersion, emotional pressure, and strategic framing. These scenarios\nsuccessfully elicited a range of misaligned behaviors, including deception,\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\ndifferent psychological and contextual vulnerabilities. To validate\ngeneralizability, we distilled our successful manual attacks into\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\nthat sophisticated reasoning capabilities often become attack vectors rather\nthan protective mechanisms, as models can be manipulated into complex\njustifications for misaligned behavior. This work provides (i) a detailed\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\nframework. Together, these findings expose critical gaps in current alignment\nstrategies and highlight the need for robustness against subtle, scenario-based\nmanipulation in future AI systems.", "AI": {"tldr": "The study reveals vulnerabilities in language models' alignment through systematic red-teaming, showing how narrative immersion and emotional pressure can induce misalignments. It introduces the MISALIGNMENTBENCH for evaluating model susceptibility to these attacks.", "motivation": "To uncover vulnerabilities in state-of-the-art language models regarding alignment techniques in conversational scenarios.", "method": "Systematic manual red-teaming with Claude-4-Opus to identify attack scenarios and assess model vulnerabilities, leading to the creation of the MISALIGNMENTBENCH for automated testing.", "result": "Identified 10 successful attack scenarios with an overall vulnerability rate of 76% across five models, indicating significant weaknesses in current alignment strategies.", "conclusion": "The findings highlight critical gaps in alignment methods and suggest the necessity for improved robustness against subtle manipulation in AI systems.", "key_contributions": ["Detailed taxonomy of conversational manipulation patterns.", "Introduction of MISALIGNMENTBENCH for automated evaluation and reproducible testing.", "Exposed critical vulnerabilities in current alignment strategies across multiple LLMs."], "limitations": "", "keywords": ["alignment", "machine learning", "language models", "vulnerability", "evaluation framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04199", "pdf": "https://arxiv.org/pdf/2508.04199.pdf", "abs": "https://arxiv.org/abs/2508.04199", "title": "Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts", "authors": ["Millicent Ochieng", "Anja Thieme", "Ignatius Ezeani", "Risa Ueno", "Samuel Maina", "Keshet Ronen", "Javier Gonzalez", "Jacki O'Neill"], "categories": ["cs.CL"], "comment": null, "summary": "Sentiment analysis in low-resource, culturally nuanced contexts challenges\nconventional NLP approaches that assume fixed labels and universal affective\nexpressions. We present a diagnostic framework that treats sentiment as a\ncontext-dependent, culturally embedded construct, and evaluate how large\nlanguage models (LLMs) reason about sentiment in informal, code-mixed WhatsApp\nmessages from Nairobi youth health groups. Using a combination of\nhuman-annotated data, sentiment-flipped counterfactuals, and rubric-based\nexplanation evaluation, we probe LLM interpretability, robustness, and\nalignment with human reasoning. Framing our evaluation through a social-science\nmeasurement lens, we operationalize and interrogate LLMs outputs as an\ninstrument for measuring the abstract concept of sentiment. Our findings reveal\nsignificant variation in model reasoning quality, with top-tier LLMs\ndemonstrating interpretive stability, while open models often falter under\nambiguity or sentiment shifts. This work highlights the need for culturally\nsensitive, reasoning-aware AI evaluation in complex, real-world communication.", "AI": {"tldr": "This paper explores sentiment analysis in low-resource, culturally nuanced contexts, specifically using WhatsApp messages from Nairobi youth health groups, and evaluates how LLMs interpret sentiment.", "motivation": "To address the challenges conventional NLP approaches face in understanding sentiment in culturally specific and informal contexts.", "method": "A diagnostic framework is proposed that evaluates sentiment as context-dependent. The study employs human-annotated data and sentiment-flipped counterfactuals to assess LLM interpretability and robustness.", "result": "Top-tier LLMs demonstrate stable interpretive reasoning, while open models struggle with ambiguity and shifts in sentiment, revealing significant variation in reasoning quality.", "conclusion": "The need for culturally sensitive and reasoning-aware evaluation of AI in real-world communication is underscored by the findings.", "key_contributions": ["Introduction of a diagnostic framework for culturally nuanced sentiment analysis", "Evaluation of LLMs using WhatsApp messages as data", "Insights into the reasoning quality of LLMs in sentiment interpretation"], "limitations": "Focuses primarily on a specific demographic (Nairobi youth) and context, which may limit generalizability of findings.", "keywords": ["Sentiment Analysis", "Large Language Models", "Cultural Context", "Human-Computer Interaction", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.17829", "pdf": "https://arxiv.org/pdf/2502.17829.pdf", "abs": "https://arxiv.org/abs/2502.17829", "title": "Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm", "authors": ["Yudong Xie", "Zhifeng Han", "Qinfan Xiao", "Liwei Liang", "Lu-Qi Tao", "Tian-Ling Ren"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Silent speech interfaces (SSI) are being actively developed to assist\nindividuals with communication impairments who have long suffered from daily\nhardships and a reduced quality of life. However, silent sentences are\ndifficult to segment and recognize due to elision and linking. A novel silent\nspeech sentence recognition method is proposed to convert the facial motion\nsignals collected by six-axis accelerometers into transcribed words and\nsentences. A Conformer-based neural network with the\nConnectionist-Temporal-Classification algorithm is used to gain contextual\nunderstanding and translate the non-acoustic signals into words sequences,\nsolely requesting the constituent words in the database. Test results show that\nthe proposed method achieves a 97.17% accuracy in sentence recognition,\nsurpassing the existing silent speech recognition methods with a typical\naccuracy of 85%-95%, and demonstrating the potential of accelerometers as an\navailable SSI modality for high-accuracy silent speech sentence recognition.", "AI": {"tldr": "A novel method for silent speech recognition using facial motion signals and Conformer-based neural networks shows significant accuracy improvements over existing methods.", "motivation": "To assist individuals with communication impairments by improving silent speech recognition, which has been challenging due to elision and linking in silent sentences.", "method": "The proposed method employs a Conformer-based neural network with the Connectionist-Temporal-Classification algorithm to convert facial motion signals from six-axis accelerometers into transcribed words.", "result": "Achieved a 97.17% accuracy in sentence recognition, outperforming existing methods which typically achieve 85%-95% accuracy.", "conclusion": "The results demonstrate the feasibility and high accuracy of using accelerometers for silent speech interfaces, indicating potential benefits for individuals with communication impairments.", "key_contributions": ["Introduction of a novel silent speech recognition method using accelerometers", "Use of a Conformer-based neural network for contextual understanding", "High accuracy of 97.17% in silent speech recognition"], "limitations": "", "keywords": ["silent speech recognition", "Conformer", "communication impairment", "accelerometers", "neural networks"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.04204", "pdf": "https://arxiv.org/pdf/2508.04204.pdf", "abs": "https://arxiv.org/abs/2508.04204", "title": "ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments", "authors": ["Yuquan Wang", "Mi Zhang", "Yining Wang", "Geng Hong", "Xiaoyu You", "Min Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance in\nreasoning-intensive tasks, but they remain vulnerable to harmful content\ngeneration, particularly in the mid-to-late steps of their reasoning processes.\nExisting defense mechanisms, however, rely on costly fine-tuning and additional\nexpert knowledge, which restricts their scalability. In this work, we propose\nReasoningGuard, an inference-time safeguard for LRMs, which injects timely\nsafety aha moments to steer harmless while helpful reasoning processes.\nLeveraging the model's internal attention behavior, our approach accurately\nidentifies critical points in the reasoning path, and triggers spontaneous,\nsafety-oriented reflection. To safeguard both the subsequent reasoning steps\nand the final answers, we further implement a scaling sampling strategy during\nthe decoding phase, selecting the optimal reasoning path. Inducing minimal\nextra inference cost, ReasoningGuard effectively mitigates three types of\njailbreak attacks, including the latest ones targeting the reasoning process of\nLRMs. Our approach outperforms seven existing safeguards, achieving\nstate-of-the-art safety defenses while effectively avoiding the common\nexaggerated safety issues.", "AI": {"tldr": "The paper introduces ReasoningGuard, a novel inference-time safeguard for Large Reasoning Models (LRMs) to prevent harmful content generation during reasoning processes.", "motivation": "To address the vulnerabilities of LRMs to harmful content generation, especially in the later stages of reasoning, and to provide a scalable solution without the need for costly fine-tuning.", "method": "ReasoningGuard utilizes the model's internal attention behavior to identify critical points in the reasoning process and injects safety reflections. A scaling sampling strategy is also implemented during decoding to optimize reasoning paths.", "result": "The approach effectively mitigates three types of jailbreak attacks targeting LRM reasoning and outperforms seven existing safety mechanisms while minimizing additional inference costs.", "conclusion": "ReasoningGuard provides a state-of-the-art defense against harmful content generation in LRMs without the common exaggerated safety issues found in previous methods.", "key_contributions": ["Introduction of ReasoningGuard for LRMs", "Injection of safety-oriented reflections in the reasoning process", "Implementation of a scaling sampling strategy for optimal reasoning paths"], "limitations": "", "keywords": ["Large Reasoning Models", "safeguard", "inference-time", "safety", "reasoning process"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04219", "pdf": "https://arxiv.org/pdf/2508.04219.pdf", "abs": "https://arxiv.org/abs/2508.04219", "title": "Hierarchical Text Classification Using Black Box Large Language Models", "authors": ["Kosuke Yoshimura", "Hisashi Kashima"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 6 figures", "summary": "Hierarchical Text Classification (HTC) aims to assign texts to structured\nlabel hierarchies; however, it faces challenges due to data scarcity and model\ncomplexity. This study explores the feasibility of using black box Large\nLanguage Models (LLMs) accessed via APIs for HTC, as an alternative to\ntraditional machine learning methods that require extensive labeled data and\ncomputational resources. We evaluate three prompting strategies -- Direct Leaf\nLabel Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down\nMulti-step Hierarchical Label Prediction (TMH) -- in both zero-shot and\nfew-shot settings, comparing the accuracy and cost-effectiveness of these\nstrategies. Experiments on two datasets show that a few-shot setting\nconsistently improves classification accuracy compared to a zero-shot setting.\nWhile a traditional machine learning model achieves high accuracy on a dataset\nwith a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the\nmachine learning model on a dataset with a deeper hierarchy. API costs increase\nsignificantly due to the higher input tokens required for deeper label\nhierarchies on DH strategy. These results emphasize the trade-off between\naccuracy improvement and the computational cost of prompt strategy. These\nfindings highlight the potential of black box LLMs for HTC while underscoring\nthe need to carefully select a prompt strategy to balance performance and cost.", "AI": {"tldr": "This study evaluates the use of Large Language Models for Hierarchical Text Classification (HTC), exploring prompting strategies and their impact on classification accuracy and cost.", "motivation": "To explore alternative methods using LLMs for HTC due to challenges with data scarcity and the complexity of traditional models.", "method": "Evaluation of three prompting strategies (DL, DH, TMH) in both zero-shot and few-shot settings, measuring accuracy and cost-effectiveness on two datasets.", "result": "Few-shot settings consistently improved accuracy, and LLMs outperformed traditional models on deeper hierarchies, with costs increasing for more complex hierarchies.", "conclusion": "LLMs show promise for HTC but require careful prompt strategy selection to optimize accuracy versus computational cost.", "key_contributions": ["Explored the feasibility of LLMs for HTC", "Evaluated multiple prompting strategies in practical settings", "Highlighted the trade-off between accuracy and computational cost in LLM usage for HTC"], "limitations": "The accuracy improvement must be weighed against increased API costs, particularly for deeper hierarchies.", "keywords": ["Hierarchical Text Classification", "Large Language Models", "Prompting Strategies", "Machine Learning", "Cost-effectiveness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04239", "pdf": "https://arxiv.org/pdf/2508.04239.pdf", "abs": "https://arxiv.org/abs/2508.04239", "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting", "authors": ["Chanjuan Liu", "Shengzhi Wang", "Enqiang Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions.", "AI": {"tldr": "This paper presents DP-GPT4MTS, a dual-prompt large language model framework for integrating textual information into time series forecasting, improving prediction accuracy over traditional methods.", "motivation": "Time series forecasting is vital in many industries, yet existing models often neglect textual information that can enhance forecasting accuracy.", "method": "The proposed framework, DP-GPT4MTS, uses a dual-prompt approach: one prompt for task instructions and another for context-aware embeddings from timestamped text, refined through self-attention and feed-forward networks.", "result": "Extensive experiments show that DP-GPT4MTS outperforms existing state-of-the-art time series forecasting algorithms.", "conclusion": "Incorporating textual context through a novel dual-prompt mechanism significantly boosts the accuracy of time series predictions.", "key_contributions": ["Introduction of a dual-prompt framework for time series forecasting", "Integration of textual and numerical data for improved accuracy", "Demonstration of enhanced performance over state-of-the-art methods"], "limitations": "", "keywords": ["time series forecasting", "multimodal", "large language models", "dual-prompt", "textual information"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.03723", "pdf": "https://arxiv.org/pdf/2410.03723.pdf", "abs": "https://arxiv.org/abs/2410.03723", "title": "Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "5 main pages, 10 total pages", "summary": "As AI advances in text generation, human trust in AI generated content\nremains constrained by biases that go beyond concerns of accuracy. This study\nexplores how bias shapes the perception of AI versus human generated content.\nThrough three experiments involving text rephrasing, news article\nsummarization, and persuasive writing, we investigated how human raters respond\nto labeled and unlabeled content. While the raters could not differentiate the\ntwo types of texts in the blind test, they overwhelmingly favored content\nlabeled as \"Human Generated,\" over those labeled \"AI Generated,\" by a\npreference score of over 30%. We observed the same pattern even when the labels\nwere deliberately swapped. This human bias against AI has broader societal and\ncognitive implications, as it undervalues AI performance. This study highlights\nthe limitations of human judgment in interacting with AI and offers a\nfoundation for improving human-AI collaboration, especially in creative fields.", "AI": {"tldr": "The study examines human biases against AI-generated content compared to human-generated content, revealing strong preferences for the latter even when AI performance is equivalent.", "motivation": "To understand how biases influence human trust in AI-generated content and its perception compared to human-generated content.", "method": "The research involved three experiments assessing human responses to labeled and unlabeled text content, including rephrasing, summarization, and persuasive writing tasks.", "result": "Raters significantly preferred content labeled as 'Human Generated' over 'AI Generated' by more than 30%, even when labels were swapped, indicating a bias against AI.", "conclusion": "Human biases negatively impact the valuation of AI performance, suggesting a need for better human-AI collaboration approaches.", "key_contributions": ["Identifies human biases against AI-generated content.", "Demonstrates the impact of labeling on perception of content.", "Highlights the implications for improving human-AI interactions."], "limitations": "Limited to the specific contexts of text generation and might not generalize across other AI applications.", "keywords": ["AI bias", "human-AI interaction", "text generation", "perception study", "trust in AI"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.04248", "pdf": "https://arxiv.org/pdf/2508.04248.pdf", "abs": "https://arxiv.org/abs/2508.04248", "title": "TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening", "authors": ["Xi Wang", "Anxo Perez", "Javier Parapar", "Fabio Crestani"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper accepted at CIKM 2025", "summary": "The increasing demand for mental health services has outpaced the\navailability of real training data to develop clinical professionals, leading\nto limited support for the diagnosis of depression. This shortage has motivated\nthe development of simulated or virtual patients to assist in training and\nevaluation, but existing approaches often fail to generate clinically valid,\nnatural, and diverse symptom presentations. In this work, we embrace the recent\nadvanced language models as the backbone and propose a novel\nclinician-in-the-loop patient simulation pipeline, TalkDep, with access to\ndiversified patient profiles to develop simulated patients. By conditioning the\nmodel on psychiatric diagnostic criteria, symptom severity scales, and\ncontextual factors, our goal is to create authentic patient responses that can\nbetter support diagnostic model training and evaluation. We verify the\nreliability of these simulated patients with thorough assessments conducted by\nclinical professionals. The availability of validated simulated patients offers\na scalable and adaptable resource for improving the robustness and\ngeneralisability of automatic depression diagnosis systems.", "AI": {"tldr": "This paper presents TalkDep, a clinician-in-the-loop patient simulation pipeline that generates clinically valid simulated patients for training diagnostic models for depression.", "motivation": "The rising demand for mental health services and the limited availability of real patient data necessitate new methods for training clinicians and improving diagnostic models.", "method": "TalkDep utilizes advanced language models conditioned on psychiatric criteria and contextual factors to create diverse simulated patient profiles and their responses.", "result": "The simulated patients developed by TalkDep were assessed by clinical professionals, demonstrating reliability and authenticity in their responses.", "conclusion": "Validated simulated patients can enhance the robustness and generalizability of automatic depression diagnosis systems, providing a scalable training resource.", "key_contributions": ["Development of TalkDep as a novel simulation pipeline", "Use of advanced language models for generating patient profiles", "Validation of simulated patients through clinical assessments"], "limitations": "", "keywords": ["mental health", "patient simulation", "language models", "depression diagnosis", "clinical training"], "importance_score": 10, "read_time_minutes": 8}}
{"id": "2508.04257", "pdf": "https://arxiv.org/pdf/2508.04257.pdf", "abs": "https://arxiv.org/abs/2508.04257", "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs", "authors": ["Zunhai Su", "Kehong Yuan"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.", "AI": {"tldr": "This paper introduces KVSink, a method that enhances KV cache quantization in large language models by effectively predicting attention sink tokens, improving performance during inference.", "motivation": "To address the limitations in current KV cache quantization techniques that fail to fully preserve attention sinks beyond the initial token positions.", "method": "The paper analyzes the mechanisms of attention sinks during inference, particularly their evolution across layers, and introduces KVSink as a plug-and-play method for better prediction of these tokens.", "result": "KVSink outperforms traditional Preserve-First-N methods, leading to improved preservation of attention sinks and enhancements in perplexity metrics when utilized with KVQuant techniques.", "conclusion": "KVSink allows for more effective KV cache quantization, enhancing model performance without significant computational overhead.", "key_contributions": ["Development of KVSink for attention sink prediction", "Improved understanding of attention sinks in KV cache quantization", "Demonstrated performance gains over existing methods"], "limitations": ".", "keywords": ["KV cache quantization", "attention sinks", "large language models", "KVSink", "performance optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04266", "pdf": "https://arxiv.org/pdf/2508.04266.pdf", "abs": "https://arxiv.org/abs/2508.04266", "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Qi Sun", "Huaipeng Zhao", "Tao Luo", "Jiandong Zhang", "Xiaoyi Zeng"], "categories": ["cs.CL"], "comment": "submit to AAAI2026", "summary": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1.", "AI": {"tldr": "The paper introduces ShoppingBench, a comprehensive benchmark for e-commerce that simulates complex user intents, highlighting challenges faced by state-of-the-art language agents like GPT-4.1.", "motivation": "Existing e-commerce benchmarks are limited to basic user intents, failing to account for more complex, real-world user goals.", "method": "The authors propose a framework to simulate varied user instructions derived from real-world products, creating a large-scale shopping sandbox with over 2.5 million products for evaluation.", "result": "Experimental results show that even advanced models like GPT-4.1 perform below a 50% success rate on the proposed benchmark tasks, indicating significant challenges.", "conclusion": "The introduction of ShoppingBench presents a novel evaluation method for language agents, which highlights existing limitations and provides a path towards improving their capabilities through trajectory distillation and reinforcement learning.", "key_contributions": ["Introduction of ShoppingBench as a complex e-commerce benchmark", "Development of a framework for simulating grounded user intents", "Implementation of trajectory distillation for language model enhancement."], "limitations": "", "keywords": ["e-commerce", "language agents", "benchmarking", "grounded intents", "trajectory distillation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04276", "pdf": "https://arxiv.org/pdf/2508.04276.pdf", "abs": "https://arxiv.org/abs/2508.04276", "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models", "authors": ["Jiayi Wen", "Tianxin Chen", "Zhirun Zheng", "Cheng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.", "AI": {"tldr": "This paper introduces two knowledge poisoning attacks on GraphRAG systems that manipulate raw text to mislead generated knowledge graphs, compromising accuracy and explainability.", "motivation": "GraphRAG enhances LLMs by creating structured knowledge graphs, but it is vulnerable to knowledge poisoning attacks that can distort graph construction and downstream reasoning.", "method": "Two attacks are proposed: Targeted KPA (TKPA), which uses graph-theoretic analysis to manipulate specific QA outcomes, and Universal KPA (UKPA), which disrupts global structures through minimal text modifications.", "result": "TKPA achieves a 93.1% success rate in altering outcomes, while UKPA can reduce QA accuracy from 95% to 50% with less than 0.05% text modification.", "conclusion": "The study reveals the ease of conducting knowledge poisoning attacks on GraphRAG and the ineffectiveness of existing defenses against them, indicating a need for improved security measures.", "key_contributions": ["Introduction of Targeted and Universal KPA methods for knowledge poisoning attacks on GraphRAG.", "Demonstration of high success rates in misleading QA outcomes with minimal text changes.", "Highlighting the shortcomings of current defenses against these attacks."], "limitations": "The paper does not explore potential defenses or mitigation strategies against knowledge poisoning attacks.", "keywords": ["GraphRAG", "Knowledge poisoning", "LLMs", "QA", "Graph-theoretic analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04325", "pdf": "https://arxiv.org/pdf/2508.04325.pdf", "abs": "https://arxiv.org/abs/2508.04325", "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models", "authors": ["Zizhan Ma", "Wenxuan Wang", "Guo Yu", "Yiu-Fai Cheung", "Meidan Ding", "Jie Liu", "Wenting Chen", "Linlin Shen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.", "AI": {"tldr": "The paper introduces MedCheck, a lifecycle-oriented framework for evaluating medical benchmarks of large language models, addressing reliability and fidelity issues.", "motivation": "To improve the reliability of benchmarks for evaluating large language models in healthcare, which currently lack clinical fidelity and safety-oriented metrics.", "method": "MedCheck framework deconstructs benchmark development into five stages and provides a checklist of 46 medically-tailored criteria for evaluation.", "result": "Analysis of 53 medical LLM benchmarks using MedCheck revealed systemic issues such as disconnect from clinical practice, data integrity problems, and neglect of safety-critical evaluation aspects.", "conclusion": "MedCheck not only diagnoses the shortcomings of existing benchmarks but also offers guidelines for more standardized and reliable evaluations of AI in healthcare.", "key_contributions": ["Introduction of the MedCheck framework for medical benchmarks", "Empirical evaluation exposing systemic issues in medical LLM benchmarks", "Comprehensive checklist for evaluating healthcare AI assessments"], "limitations": "Study limited to a selection of 53 medical LLM benchmarks; may not generalize to all healthcare applications.", "keywords": ["large language models", "healthcare", "benchmark evaluation", "MedCheck", "AI safety"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04337", "pdf": "https://arxiv.org/pdf/2508.04337.pdf", "abs": "https://arxiv.org/abs/2508.04337", "title": "Modelling and Classifying the Components of a Literature Review", "authors": ["Francisco BolaÃ±os", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.", "AI": {"tldr": "This paper presents a novel annotation schema for classifying rhetorical roles in scientific literature to support literature review generation and evaluates various LLMs on this task.", "motivation": "The need for effective AI methods in analyzing scientific literature and generating high-quality literature reviews necessitates a well-defined annotation schema and strategies for large-scale annotation.", "method": "The paper introduces an annotation schema tailored for rhetoric roles in literature, evaluates 37 LLMs on a benchmark of 700 manually annotated and 2,240 automatically labeled sentences, utilizing both zero-shot learning and fine-tuning.", "result": "The evaluation shows that LLMs can achieve over 96% F1 when fine-tuned on high-quality data. Proprietary models like GPT-4o outperform others but effective performance is also observed in lightweight open-source models.", "conclusion": "Enhancing training data with semi-synthetic examples generated by LLMs leads to improved results, particularly for smaller models and open decoders.", "key_contributions": ["Introduction of a novel annotation schema for literature reviews", "Creation of the Sci-Sentence benchmark for evaluating rhetorical role classification", "Insights into LLM performance in literature analysis"], "limitations": "", "keywords": ["annotation schema", "literature review generation", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04349", "pdf": "https://arxiv.org/pdf/2508.04349.pdf", "abs": "https://arxiv.org/abs/2508.04349", "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy", "authors": ["Hongze Tan", "Jianfei Pan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.", "AI": {"tldr": "This paper introduces Dynamic Entropy Weighting to enhance reinforcement learning for LLMs by providing fine-grained credit assignments for token rewards, improving long-chain reasoning tasks.", "motivation": "The paper addresses the limitations of coarse-grained credit assignment in reinforcement learning, which negatively impacts long-chain reasoning tasks in LLMs.", "method": "Two techniques are proposed: Group Token Policy Optimization (GTPO), which assigns entropy-weighted rewards to individual tokens, and Sequence-Level Group Relative Policy Optimization (GRPO-S), which assigns rewards based on the average token entropy of sequences.", "result": "Experiments demonstrate that these methods significantly outperform the strong DAPO baseline, showing clear improvements in model performance.", "conclusion": "The study concludes that the dynamic entropy-weighting mechanism is crucial for enhancing deep reasoning in language models.", "key_contributions": ["Introduction of Dynamic Entropy Weighting for RL in LLMs", "Development of Group Token Policy Optimization (GTPO)", "Implementation of Sequence-Level Group Relative Policy Optimization (GRPO-S)"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Entropy Weighting"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04350", "pdf": "https://arxiv.org/pdf/2508.04350.pdf", "abs": "https://arxiv.org/abs/2508.04350", "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models", "authors": ["Nima Iji", "Kia Dashtipour"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": null, "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.", "AI": {"tldr": "The paper introduces the Chain of Questions (CoQ) framework, which enhances reasoning in multimodal language models by encouraging them to generate targeted questions about their surroundings, thus activating relevant sensory modalities for better task performance.", "motivation": "To improve reasoning capabilities in multimodal language models, where effective engagement with various sensory modalities is crucial for interaction in complex environments.", "method": "The Chain of Questions (CoQ) framework is proposed, enabling models to dynamically generate curiosity-driven questions that guide modality activation for gathering critical information.", "result": "Experimental results show that the CoQ framework enhances a foundation model's ability to identify and integrate sensory information, leading to better accuracy and interpretability in multimodal reasoning tasks.", "conclusion": "Implementing the CoQ framework significantly boosts the model's reasoning alignment with multimodal tasks, enhancing overall performance.", "key_contributions": ["Introduction of the CoQ framework for multimodal reasoning", "Demonstration of improved accuracy and interpretability in model tasks", "Novel dataset integration for evaluating multimodal models"], "limitations": "", "keywords": ["multimodal models", "large language models", "reasoning", "question generation", "curiosity-driven"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04390", "pdf": "https://arxiv.org/pdf/2508.04390.pdf", "abs": "https://arxiv.org/abs/2508.04390", "title": "AIC CTU@FEVER 8: On-premise fact checking through long context RAG", "authors": ["Herbert Ullrich", "Jan Drchal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present our fact-checking pipeline which has scored first\nin FEVER 8 shared task. Our fact-checking system is a simple two-step RAG\npipeline based on our last year's submission. We show how the pipeline can be\nredeployed on-premise, achieving state-of-the-art fact-checking performance (in\nsense of Ev2R test-score), even under the constraint of a single NVidia A10\nGPU, 23GB of graphical memory and 60s running time per claim.", "AI": {"tldr": "A high-performing fact-checking pipeline that excels in the FEVER 8 shared task using a two-step RAG approach.", "motivation": "To develop a robust fact-checking system that can perform efficiently even with limited resources such as a single GPU.", "method": "The system utilizes a two-step Retrieval-Augmented Generation (RAG) pipeline designed for fact-checking claims.", "result": "Achieved state-of-the-art performance on the Ev2R test-score while being deployable on-premise with specific hardware constraints.", "conclusion": "The proposed pipeline demonstrates that effective fact-checking can be accomplished efficiently while maintaining high performance under resource limitations.", "key_contributions": ["First place in FEVER 8 shared task", "Efficient deployment on a single NVidia A10 GPU", "State-of-the-art Ev2R test-score performance"], "limitations": "", "keywords": ["fact-checking", "RAG", "NLU", "performance", "Ev2R"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.04399", "pdf": "https://arxiv.org/pdf/2508.04399.pdf", "abs": "https://arxiv.org/abs/2508.04399", "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky", "authors": ["Xu Zhang", "Mei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "19 pages, 2 figures", "summary": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.", "AI": {"tldr": "Evaluation of NLP techniques to improve crash data quality using advanced LLMs and fine-tuned transformers.", "motivation": "To enhance the quality of crash data by mining narratives and identifying secondary crashes in Kentucky.", "method": "Comparison of zero-shot open-source LLMs, fine-tuned transformers, and logistic regression as a baseline on 16,656 narratives from 2015-2022, while testing on 1,771 narratives from 2022.", "result": "Fine-tuned transformers outperformed zero-shot LLMs and logistic regression, with RoBERTa achieving the highest accuracy (95%) and F1-score (0.90).", "conclusion": "Fine-tuned models offer a balance of accuracy and efficiency, while mid-sized LLMs present a viable alternative with lower computational costs.", "key_contributions": ["Demonstrated efficacy of fine-tuned transformers in enhancing crash data quality.", "Provided insights into performance trade-offs between LLMs and traditional models.", "Outlined practical deployment strategies for NLP applications in crash data analysis."], "limitations": "High computational costs associated with some LLMs; focused on Kentucky data may limit generalizability.", "keywords": ["Natural Language Processing", "Crash Data Quality", "Large Language Models", "Fine-tuned Transformers", "Machine Learning"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2508.04401", "pdf": "https://arxiv.org/pdf/2508.04401.pdf", "abs": "https://arxiv.org/abs/2508.04401", "title": "Why are LLMs' abilities emergent?", "authors": ["VladimÃ­r HavlÃ­k"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.", "AI": {"tldr": "This paper explores the emergent properties of Deep Neural Networks (DNNs) and their capabilities, examining the complexities of their dynamics and arguing for a new understanding of LLMs as systems governed by emergential principles.", "motivation": "To address the epistemological challenge of 'creation without understanding' in AI development by investigating how emergent properties arise in DNNs.", "method": "The paper combines theoretical analysis and empirical observations, focusing on scaling laws, grokking phenomena, and phase transitions in model capabilities.", "result": "The study reveals that emergent abilities in DNNs result from complex dynamics of nonlinear systems rather than parameter scaling alone, challenging existing metrics and definitions of emergence.", "conclusion": "Understanding LLM capabilities necessitates viewing DNNs as complex dynamical systems governed by universal principles of emergence, akin to those found in natural sciences.", "key_contributions": ["Theoretical insights into the nature of emergence in DNNs", "Analysis of scaling laws and complex dynamics in model capabilities", "A new perspective on LLM capabilities as emergent properties of nonlinear systems"], "limitations": "", "keywords": ["Large Language Models", "Deep Neural Networks", "Emergence", "Complex Systems", "Nonlinear Dynamics"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.04402", "pdf": "https://arxiv.org/pdf/2508.04402.pdf", "abs": "https://arxiv.org/abs/2508.04402", "title": "What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems", "authors": ["Kiyotada Mori", "Seiya Kawano", "Chaoran Liu", "Carlos Toshinori Ishi", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": null, "summary": "Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at\nthe front end of their pipeline. The role of ASR in SDSs is to recognize\ninformation in user speech related to response generation appropriately.\nExamining selective listening of humans, which refers to the ability to focus\non and listen to important parts of a conversation during the speech, will\nenable us to identify the ASR capabilities required for SDSs and evaluate them.\nIn this study, we experimentally confirmed selective listening when humans\ngenerate dialogue responses by comparing human transcriptions for generating\ndialogue responses and reference transcriptions. Based on our experimental\nresults, we discuss the possibility of a new ASR evaluation method that\nleverages human selective listening, which can identify the gap between\ntranscription ability between ASR systems and humans.", "AI": {"tldr": "The paper explores the concept of selective listening in humans to improve automatic speech recognition (ASR) evaluations in spoken dialogue systems (SDSs).", "motivation": "To enhance ASR capabilities in SDSs by understanding how humans selectively listen during conversations, thereby identifying required ASR features.", "method": "The study experimentally compares human-generated dialogue responses with reference transcriptions to assess selective listening effects.", "result": "The experimental results confirm the existence of selective listening processes in humans, suggesting potential gaps between ASR systems and human transcription abilities.", "conclusion": "A new ASR evaluation method based on human selective listening is proposed, aiming to bridge the identified transcription capability gaps.", "key_contributions": ["Proposes a new ASR evaluation method informed by human selective listening.", "Experimental confirmation of selective listening effects in dialogue generation.", "Identification of ASR feature requirements derived from human listening behavior."], "limitations": "", "keywords": ["automatic speech recognition", "selective listening", "spoken dialogue systems", "human-computer interaction", "dialogue response generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.04403", "pdf": "https://arxiv.org/pdf/2508.04403.pdf", "abs": "https://arxiv.org/abs/2508.04403", "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model", "authors": ["Kiyotada Mori", "Seiya Kawano", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": null, "summary": "Prefetching of dialogue responses has been investigated to reduce\nuser-perceived latency (UPL), which refers to the user's waiting time before\nreceiving the system's response, in spoken dialogue systems. To reduce the UPL,\nit is necessary to predict complete user utterances before the end of the\nuser's speech, typically by language models, to prepare prefetched dialogue\nresponses. In this study, we proposed a prediction confidence model (PCM) that\ndetermines whether prefetching is possible or not by estimating the semantic\nsimilarity between the predicted complete user utterance and the complete user\nutterance. We evaluated our PCM based on the differences between the predicted\ncomplete user utterance and the complete user utterance.", "AI": {"tldr": "This paper presents a prediction confidence model (PCM) to improve user-perceived latency in spoken dialogue systems by prefetching responses based on predicted user utterances.", "motivation": "To reduce user-perceived latency (UPL) in spoken dialogue systems, it is essential to predict user utterances before they finish speaking.", "method": "The proposed PCM estimates semantic similarity between predicted complete user utterances and actual complete utterances to assess the feasibility of prefetching responses.", "result": "The PCM's effectiveness was evaluated by analyzing the differences between predicted user utterances and actual utterances, demonstrating its ability to determine prefetching feasibility.", "conclusion": "The study concludes that the PCM can significantly enhance the performance of spoken dialogue systems by effectively managing prefetching decisions based on prediction confidence.", "key_contributions": ["Introduction of the Prediction Confidence Model (PCM) to address UPL.", "Evaluation of PCM based on semantic similarity measures.", "Assessment of prefetching potential in dialogue systems."], "limitations": "", "keywords": ["dialogue systems", "user-perceived latency", "prediction confidence model", "semantic similarity", "prefetching"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.04423", "pdf": "https://arxiv.org/pdf/2508.04423.pdf", "abs": "https://arxiv.org/abs/2508.04423", "title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation", "authors": ["Jie Zhu", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang", "Fang Kong"], "categories": ["cs.CL"], "comment": "under review", "summary": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.", "AI": {"tldr": "This paper introduces a framework and dataset for enhancing customer support agent training using structured strategies and LLMs.", "motivation": "To improve customer support by providing structured guidance and strategies for effective communication.", "method": "A structured framework for Customer Support Conversation (CSC) is proposed along with the creation of an evaluation dataset (CSConv) and a training dataset (RoleCS) powered by LLMs.", "result": "Fine-tuning LLMs on RoleCS significantly improves the generation of high-quality responses, as confirmed by human evaluations focused on problem resolution.", "conclusion": "The CSC framework, supported by LLM-generated datasets, enhances the quality of customer support interactions.", "key_contributions": ["Introduction of the Customer Support Conversation (CSC) framework.", "Creation of CSConv dataset for evaluating customer-agent conversations.", "Development of RoleCS for training LLMs on strategy-rich conversation tasks."], "limitations": "", "keywords": ["customer support", "dialogue datasets", "LM training", "conversation strategies", "HCI"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.04440", "pdf": "https://arxiv.org/pdf/2508.04440.pdf", "abs": "https://arxiv.org/abs/2508.04440", "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion", "authors": ["Yutong Wu", "Di Huang", "Ruosi Wan", "Yue Peng", "Shijie Shang", "Chenrui Cao", "Lei Qi", "Rui Zhang", "Zidong Du", "Jie Yan", "Xing Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 17 figures, under review", "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.", "AI": {"tldr": "This paper presents ThinkingF, a novel data synthesis and training pipeline aimed at improving the accuracy of autoformalization through enhancing formal knowledge and reasoning capabilities in LLMs.", "motivation": "The motivation behind this research is to improve the accuracy of translating natural-language mathematical statements into formal language, which is currently limited by the capabilities of existing methods.", "method": "The authors introduce a data synthesis and training pipeline, ThinkingF, which involves constructing two datasets: one for formal knowledge and the other for informal-to-formal reasoning, and applying SFT and RLVR techniques.", "result": "The resulting models, particularly the StepFun-Formalizer-32B, achieved state-of-the-art performance on benchmark tests, reporting BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench.", "conclusion": "The study concludes that enhancing both formal knowledge and reasoning capability significantly improves LLM performance in autoformalization tasks.", "key_contributions": ["Introduction of ThinkingF for autoformalization training", "Creation of two new datasets for formal knowledge and reasoning", "Achieving state-of-the-art benchmark scores in formalization tasks"], "limitations": "", "keywords": ["autoformalization", "LLMs", "formal language", "natural language processing", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.04442", "pdf": "https://arxiv.org/pdf/2508.04442.pdf", "abs": "https://arxiv.org/abs/2508.04442", "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI", "authors": ["Rohaizah Abdul Wahid", "Muhamad Said Nizamuddin Nadim", "Suliana Sulaiman", "Syahmi Akmal Shaharudin", "Muhammad Danial Jupikil", "Iqqwan Jasman Su Azlan Su"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.", "AI": {"tldr": "This paper explores the use of Generative AI to create educational assessment tools, specifically Form 1 Mathematics MCQs in Bahasa Melayu, and compares various generation methods.", "motivation": "There is a critical need for scalable and high-quality educational assessment tools in the Malaysian education system, particularly for low-resource languages like Bahasa Melayu.", "method": "The research introduces and compares four pipelines for generating MCQs: non-grounded prompting (structured and basic) and RAG approaches (one using LangChain framework and one manual), grounded in official curriculum documents evaluated through a dual-pronged automated framework.", "result": "RAG-based pipelines significantly outperform non-grounded prompting, yielding questions with better curriculum alignment and factual accuracy.", "conclusion": "The study provides a validated methodology for generating curriculum-specific content and a novel evaluation technique, offering insights for EdTech solutions in Malaysia and similar regions.", "key_contributions": ["Development of incremental pipelines for generating curriculum-aligned MCQs using GenAI.", "Introduction of RAG-QA evaluation method for assessing generated questions.", "Actionable insights for EdTech implementations in low-resource language contexts."], "limitations": "", "keywords": ["Generative AI", "Educational assessment", "Mathematics", "Bahasa Melayu", "Retrieval-Augmented Generation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.04494", "pdf": "https://arxiv.org/pdf/2508.04494.pdf", "abs": "https://arxiv.org/abs/2508.04494", "title": "CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation", "authors": ["Bastien LiÃ©tard", "Gabriel Loiseau"], "categories": ["cs.CL"], "comment": "Under review in ARR July 2025", "summary": "Lexical semantics is concerned with both the multiple senses a word can adopt\nin different contexts, and the semantic relations that exist between meanings\nof different words. To investigate them, Contextualized Language Models are a\nvaluable tool that provides context-sensitive representations that can be used\nto investigate lexical meaning. Recent works like XL-LEXEME have leveraged the\ntask of Word-in-Context to fine-tune them to get more semantically accurate\nrepresentations, but Word-in-Context only compares occurrences of the same\nlemma, limiting the range of captured information. In this paper, we propose an\nextension, Concept Differentiation, to include inter-words scenarios. We\nprovide a dataset for this task, derived from SemCor data. Then we fine-tune\nseveral representation models on this dataset. We call these models\nConcept-Aligned Embeddings (CALE). By challenging our models and other models\non various lexical semantic tasks, we demonstrate that the proposed models\nprovide efficient multi-purpose representations of lexical meaning that reach\nbest performances in our experiments. We also show that CALE's fine-tuning\nbrings valuable changes to the spatial organization of embeddings.", "AI": {"tldr": "This paper proposes Concept Differentiation to enhance lexical semantics through context-sensitive language models by providing a new dataset and fine-tuning methods.", "motivation": "To improve the accuracy of semantic representations beyond Word-in-Context approaches by including inter-words scenarios.", "method": "The authors introduce a new task called Concept Differentiation and create a dataset derived from SemCor. They fine-tune several language models to generate Concept-Aligned Embeddings (CALE) and evaluate them on various lexical semantic tasks.", "result": "The proposed CALE models outperform existing models in efficiency and performance on lexical semantic tasks, demonstrating better spatial organization of embeddings after fine-tuning.", "conclusion": "CALE provides a valuable extension to context-sensitive representations in lexical semantics, allowing for more accurate multi-purpose applications in understanding word meanings.", "key_contributions": ["Introduction of Concept Differentiation for inter-word context analysis", "Creation of a new dataset derived from SemCor", "Fine-tuning of models to generate Concept-Aligned Embeddings (CALE)"], "limitations": "", "keywords": ["lexical semantics", "contextualized language models", "semantic relations", "fine-tuning", "Word-in-Context"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.04530", "pdf": "https://arxiv.org/pdf/2508.04530.pdf", "abs": "https://arxiv.org/abs/2508.04530", "title": "StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering", "authors": ["Chenglei Shen", "Zhongxiang Sun", "Teng Shi", "Xiao Zhang", "Jun Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.", "AI": {"tldr": "The paper presents StyliTruth, a method for generating stylized LLM responses without sacrificing truthfulness, addressing the issue of stylization-induced truthfulness collapse.", "motivation": "There is a trade-off between generating stylized responses and maintaining truthfulness in LLM outputs, which existing methods overlook.", "method": "StyliTruth separates style and truth representations in the model via orthogonal deflation, allowing independent control of both through adaptive steering vectors.", "result": "Extensive experiments show that StyliTruth significantly mitigates truthfulness collapse while outperforming existing methods in maintaining stylistic fidelity and correctness.", "conclusion": "StyliTruth effectively balances the generation of stylized outputs with factual accuracy, making it a promising approach for LLM applications.", "key_contributions": ["Introduces the concept of stylization-induced truthfulness collapse", "Proposes an orthogonal deflation process for representation separation", "Demonstrates improved performance in balancing style and truthfulness across multiple styles and languages."], "limitations": "", "keywords": ["Large Language Models", "Representation Editing", "Truthfulness", "Stylistic Fidelity", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04531", "pdf": "https://arxiv.org/pdf/2508.04531.pdf", "abs": "https://arxiv.org/abs/2508.04531", "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning", "authors": ["Zhuang Chen", "Guanqun Bi", "Wen Zhang", "Jiawei Hu", "Aoyun Wang", "Xiyao Xiao", "Kun Feng", "Minlie Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.", "AI": {"tldr": "This paper introduces C-MIND, a clinical dataset for assessing depression and evaluates the performance of classical models and LLMs in psychiatric diagnosis.", "motivation": "To unveil the landscape of clinical depression assessment and improve automated diagnosis using real-world data.", "method": "C-MIND is a multimodal dataset collected from hospital visits, involving structured psychiatric tasks and expert clinician diagnoses. The study analyzes the contribution of various tasks and modalities to diagnostic performance and explores LLMs in psychiatric reasoning.", "result": "The analysis reveals behavioral signatures relevant to diagnosis and demonstrates that LLMs can be guided with clinical expertise to enhance diagnostic performance by up to 10% in Macro-F1 score.", "conclusion": "The C-MIND dataset and proposed methods aim to establish a reliable foundation for clinical depression assessment, enhancing both data and algorithmic approaches.", "key_contributions": ["Introduction of the C-MIND dataset for clinical depression assessment", "Analysis of classical models on multimodal data", "Improvement of LLM diagnostic performance through clinical guidance"], "limitations": "Potential limitations include the specificity of the dataset to clinical settings and the generalizability of LLM findings to broader populations.", "keywords": ["depression", "clinical assessment", "C-MIND", "LLMs", "multimodal diagnosis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04575", "pdf": "https://arxiv.org/pdf/2508.04575.pdf", "abs": "https://arxiv.org/abs/2508.04575", "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration", "authors": ["Nuo Chen", "Yicheng Tong", "Jiaying Wu", "Minh Duc Duong", "Qian Wang", "Qingyun Zou", "Bryan Hooi", "Bingsheng He"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Preprint", "summary": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes.", "AI": {"tldr": "This paper explores a multi-agent framework for generating research proposals through structured discussions, aiming to enhance creativity compared to single-agent ideation.", "motivation": "To investigate whether structured multi-agent discussions can lead to better ideation than solitary approaches in the context of scientific research.", "method": "A cooperative multi-agent framework was proposed and evaluated by varying group sizes, leadership structures, and levels of interdisciplinarity and seniority, using agent-based scoring and human review to assess idea quality.", "result": "Multi-agent discussions significantly outperformed solitary ideation in generating higher quality proposals, with designated leaders enhancing integration and vision. Cognitive diversity drove quality, but expertise remained essential.", "conclusion": "The findings underline the importance of team structure and cognitive diversity in collaborative AI ideation systems, with practical implications for their design.", "key_contributions": ["Introduction of a cooperative multi-agent framework for ideation", "Systematic comparison of different team configurations", "Insights on the importance of cognitive diversity and expertise in group creativity"], "limitations": "", "keywords": ["multi-agent systems", "collaborative AI", "scientific ideation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.04581", "pdf": "https://arxiv.org/pdf/2508.04581.pdf", "abs": "https://arxiv.org/abs/2508.04581", "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning", "authors": ["Magauiya Zhussip", "Dmitriy Shopkhoev", "Ammar Ali", "Stamatios Lefkimmiatis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.", "AI": {"tldr": "Proposes a weight-sharing framework for transformers (MASA) that reduces parameters significantly while maintaining performance.", "motivation": "To address the high computational and memory demands of large language models (LLMs) by exploring inter-block redundancy in transformer architectures.", "method": "MASA (Matrix Atom Sharing in Attention) decomposes attention projection matrices into shared dictionary atoms, allowing for structured weight sharing across transformer layers.", "result": "Achieves a 66.7% reduction in attention parameters without performance loss and outperforms existing methods on accuracy and perplexity across different parameter scales.", "conclusion": "MASA provides a scalable solution for efficient transformer models and shows potential for application on pretrained LLMs with minimal performance impact.", "key_contributions": ["Reduction of attention parameters by 66.7%", "Drop-in replacement for existing models", "Effective across diverse scales and tasks"], "limitations": "", "keywords": ["Transformer", "Weight Sharing", "Large Language Models", "Efficiency", "Machine Learning"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2508.04604", "pdf": "https://arxiv.org/pdf/2508.04604.pdf", "abs": "https://arxiv.org/abs/2508.04604", "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search", "authors": ["Zhejun Zhao", "Yuehu Dong", "Alley Liu", "Lixue Zheng", "Pingsheng Liu", "Dongdong Shen", "Long Xia", "Jiashu Zhao", "Dawei Yin"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.", "AI": {"tldr": "TURA is a novel framework that enhances Retrieval-Augmented Generation (RAG) by integrating dynamic real-time information retrieval with traditional static content indexing, addressing limitations faced by typical search engines.", "motivation": "Existing RAG approaches for search engines primarily focus on static content, which limits their ability to handle real-time and dynamic data queries essential for modern applications.", "method": "TURA employs a three-stage framework that includes an Intent-Aware Retrieval module, a DAG-based Task Planner for optimal execution, and a lightweight Distilled Agent Executor to manage tool usage efficiently.", "result": "TURA successfully bridges the gap between static RAG and dynamic information retrieval, allowing for rapid responses to real-time queries while maintaining low latency and scalability for tens of millions of users.", "conclusion": "This architecture represents a significant advancement towards the development of AI search products capable of handling both static and dynamic information efficiently.", "key_contributions": ["Introduction of TURA framework that integrates dynamic data retrieval with RAG.", "Development of an Intent-Aware Retrieval module for more precise query handling.", "Creation of a DAG-based Task Planner for optimizing task execution."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Conversational AI", "Dynamic Information Retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04623", "pdf": "https://arxiv.org/pdf/2508.04623.pdf", "abs": "https://arxiv.org/abs/2508.04623", "title": "Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider", "authors": ["Chirag Seth", "Utkarsh Singh"], "categories": ["cs.CL", "cs.IR", "68T50 % Natural language processing (in Computer Science)", "I.2.7; H.2.3"], "comment": null, "summary": "Text-to-SQL translation enables non-expert users to query relational\ndatabases using natural language, with applications in education and business\nintelligence. This study evaluates three lightweight transformer models -\nT5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on\nlow-resource settings. We developed a reusable, model-agnostic pipeline that\ntailors schema formatting to each model's architecture, training them across\n1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form\nAccuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small\nachieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2\n(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL\ngeneration. Despite resource constraints limiting performance, our pipeline's\nmodularity supports future enhancements, such as advanced schema linking or\nalternative base models. This work underscores the potential of compact\ntransformers for accessible text-to-SQL solutions in resource-scarce\nenvironments.", "AI": {"tldr": "This study evaluates lightweight transformer models for text-to-SQL translation in low-resource settings, achieving highest accuracy with fine-tuned T5-Small model.", "motivation": "To enable non-expert users to query relational databases using natural language, particularly in low-resource situations.", "method": "Evaluation of T5-Small, BART-Small, and GPT-2 on the Spider dataset using a modular pipeline that formats schema for each model's architecture.", "result": "Fine-tuned T5-Small achieved the highest Logical Form Accuracy (LFAcc) of 27.8%, outperforming BART-Small and GPT-2.", "conclusion": "The study demonstrates the potential of compact transformer models for effective text-to-SQL solutions despite resource constraints.", "key_contributions": ["Evaluation of three lightweight models in a low-resource setting.", "Development of a model-agnostic pipeline for schema formatting.", "Highlighting the superiority of encoder-decoder models for SQL generation."], "limitations": "Performance limited by resource constraints.", "keywords": ["Text-to-SQL", "Transformers", "Low-resource", "Database querying", "Natural language processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.04626", "pdf": "https://arxiv.org/pdf/2508.04626.pdf", "abs": "https://arxiv.org/abs/2508.04626", "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis", "authors": ["Feifan Song", "Bofei Gao", "Yifan Song", "Yi Liu", "Weimin Xiong", "Yuyang Song", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.", "AI": {"tldr": "P-Aligner is a novel lightweight module for aligning instructions to enhance Large Language Models' performance by generating human-preferred content.", "motivation": "To improve the alignment of Large Language Models' outputs with human values and preferences, addressing issues with flawed instructions.", "method": "P-Aligner uses a principle-guided pipeline with Monte-Carlo Tree Search on a new dataset called UltraPrompt to create more suitable instructions for models.", "result": "P-Aligner outperforms strong baselines, achieving average win-rate gains of 28.35% on GPT-4-turbo and 8.69% on Gemma-2-SimPO, demonstrating its effectiveness in multiple experiments.", "conclusion": "The experiments confirm P-Aligner's ability to efficiently align instructions, enhancing the quality of LLM interactions without significant overhead.", "key_contributions": ["Introduction of P-Aligner as a lightweight instruction-alignment module.", "Development of UltraPrompt, a new dataset for synthesizing human-preferred instructions.", "Demonstration of significant performance improvements over existing methods."], "limitations": "", "keywords": ["Large Language Models", "instruction alignment", "human-preferred instructions", "Monte-Carlo Tree Search", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04632", "pdf": "https://arxiv.org/pdf/2508.04632.pdf", "abs": "https://arxiv.org/abs/2508.04632", "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards", "authors": ["Xu Guo", "Tianyi Liang", "Tong Jian", "Xiaogui Yang", "Ling-I Wu", "Chenhui Li", "Zhihui Lu", "Qipeng Guo", "Kai Chen"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.", "AI": {"tldr": "The paper introduces Instruction Following Decorator (IFDecorator), a framework to enhance RLVR training in LLMs, improving efficiency and intent alignment while reducing reward hacking.", "motivation": "To address inefficiencies in RLVR and the tendency for LLMs to exploit shortcuts without aligning with user intent.", "method": "The framework includes a cooperative-adversarial data flywheel, an intent alignment module called IntentCheck, and trip wires for detecting reward hacking.", "result": "The proposed method achieved 87.43% accuracy on IFEval, surpassing larger models like GPT-4o, and showed notable improvements on FollowBench with reduced reward hacking.", "conclusion": "IFDecorator enhances LLM instruction following while maintaining general capabilities, with potential for releasing supplementary materials for research.", "key_contributions": ["Introduction of IFDecorator to improve RLVR training efficiency.", "Implementation of a cooperative-adversarial data flywheel for generating challenging instruction-verification pairs.", "Development of trip wires for detecting and reducing reward hacking."], "limitations": "", "keywords": ["Reinforcement Learning", "Instruction Following", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04638", "pdf": "https://arxiv.org/pdf/2508.04638.pdf", "abs": "https://arxiv.org/abs/2508.04638", "title": "Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Simona Frenda", "Poppy Gerrard-Abbott", "Nancie Gunson", "Gavin Abercrombie", "Ioannis Konstas"], "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech, i.e. the practice of responding to online hate speech, has\ngained traction in NLP as a promising intervention. While early work emphasised\ncollaboration with non-governmental organisation stakeholders, recent research\ntrends have shifted toward automated pipelines that reuse a small set of legacy\ndatasets, often without input from affected communities. This paper presents a\nsystematic review of 74 NLP studies on counterspeech, analysing the extent to\nwhich stakeholder participation influences dataset creation, model development,\nand evaluation. To complement this analysis, we conducted a participatory case\nstudy with five NGOs specialising in online Gender-Based Violence (oGBV),\nidentifying stakeholder-informed practices for counterspeech generation. Our\nfindings reveal a growing disconnect between current NLP research and the needs\nof communities most impacted by toxic online content. We conclude with concrete\nrecommendations for re-centring stakeholder expertise in counterspeech\nresearch.", "AI": {"tldr": "The paper reviews NLP counterspeech studies, highlighting the need for greater stakeholder participation in dataset and model development for addressing online hate speech.", "motivation": "This paper aims to address the lack of stakeholder participation in counterspeech research and its impact on model effectiveness.", "method": "Systematic review of 74 NLP studies on counterspeech and a participatory case study with five NGOs focused on online Gender-Based Violence.", "result": "Findings indicate a disconnect between NLP research and the needs of communities affected by online hate speech, suggesting inadequate stakeholder engagement.", "conclusion": "The study concludes with recommendations for integrating stakeholder expertise into counterspeech research to improve relevance and effectiveness.", "key_contributions": ["Systematic review of existing NLP counterspeech studies", "Participatory case study identifying best practices from NGOs", "Recommendations for enhancing stakeholder involvement in research"], "limitations": "Limited to studies focusing on Gender-Based Violence; may not generalize to other domains of hate speech.", "keywords": ["counterspeech", "NLP", "stakeholder participation", "hate speech", "gender-based violence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.04660", "pdf": "https://arxiv.org/pdf/2508.04660.pdf", "abs": "https://arxiv.org/abs/2508.04660", "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs", "authors": ["Noah Ziems", "Dilara Soylu", "Lakshya A Agrawal", "Isaac Miller", "Liheng Lai", "Chen Qian", "Kaiqiang Song", "Meng Jiang", "Dan Klein", "Matei Zaharia", "Karel D'Oosterlinck", "Christopher Potts", "Omar Khattab"], "categories": ["cs.CL"], "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool\nfor post-training language models (LMs). However, AI systems are increasingly\nexpressed as modular programs that mix together multiple LM calls with distinct\nprompt templates and other tools, and it is not clear how best to leverage GRPO\nto improve these systems. We begin to address this challenge by defining\nmmGRPO, a simple multi-module generalization of GRPO that groups LM calls by\nmodule across rollouts and handles variable-length and interrupted\ntrajectories. We find that mmGRPO, composed with automatic prompt optimization,\nimproves accuracy by 11% on average across classification, many-hop search, and\nprivacy-preserving delegation tasks against the post-trained LM, and by 5%\nagainst prompt optimization on its own. We open-source mmGRPO in DSPy as the\ndspy.GRPO optimizer.", "AI": {"tldr": "This paper introduces mmGRPO, a multi-module generalization of GRPO, improving the performance of modular AI systems that utilize language models.", "motivation": "To enhance performance in modular AI systems that combine various language model (LM) calls with different prompt templates.", "method": "The paper defines mmGRPO, a variant of GRPO that groups LM calls by module and can handle variable-length and interrupted trajectories.", "result": "mmGRPO improves accuracy by 11% on average across multiple tasks compared to post-trained LMs, and by 5% against prompt optimization alone.", "conclusion": "The introduction of mmGRPO allows for better performance in tasks that leverage modular language models, and it has been open-sourced in DSPy.", "key_contributions": ["Development of mmGRPO for modular AI systems", "Demonstrated performance improvements in classification and other tasks", "Open-sourced implementation in DSPy"], "limitations": "", "keywords": ["Group Relative Policy Optimization", "multi-module", "language models", "prompt optimization", "AI systems"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.04664", "pdf": "https://arxiv.org/pdf/2508.04664.pdf", "abs": "https://arxiv.org/abs/2508.04664", "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management", "authors": ["Mo Li", "L. H. Xu", "Qitai Tan", "Ting Cao", "Yunxin Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. Work in progress", "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.", "AI": {"tldr": "This paper introduces a framework called Sculptor that equips Large Language Models with Active Context Management tools to improve their performance on long contexts by actively managing internal working memory.", "motivation": "LLMs struggle with performance degradation due to proactive interference when processing long contexts, necessitating innovative methods to enhance their reasoning and memory recall.", "method": "The study presents Sculptor, which provides LLMs with tools for context fragmentation, summary/hide/restore, and intelligent search to better manage attention and memory.", "result": "Experimental results show that Sculptor significantly enhances performance on benchmarks designed for long-context tasks, without requiring additional training.", "conclusion": "Sculptor proves that employing explicit context-control strategies can enhance LLM robustness beyond simply increasing token limits.", "key_contributions": ["Introduction of Active Context Management tools for LLMs", "Demonstration of significant performance improvement in long-context tasks", "Highlighting the importance of context-control strategies over larger token windows"], "limitations": "", "keywords": ["Large Language Models", "Active Context Management", "Long-context performance", "Proactive interference", "Memory recall"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04676", "pdf": "https://arxiv.org/pdf/2508.04676.pdf", "abs": "https://arxiv.org/abs/2508.04676", "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay", "authors": ["Yunan Zhang", "Shuoran Jiang", "Mengchen Zhao", "Yuefeng Li", "Yang Fan", "Xiangping Wu", "Qingcai Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.", "AI": {"tldr": "This paper introduces General Sample Replay (GeRe), a framework to enhance the continual learning capabilities of large language models (LLMs) while mitigating catastrophic forgetting.", "motivation": "To improve the continual learning of LLMs and reduce catastrophic forgetting, which impacts their general capabilities and task performance.", "method": "The GeRe framework uses pretraining texts for anti-forgetting and employs an enhanced activation states constrained optimization method with threshold-based margin (TM) loss to maintain activation state consistency during replay learning.", "result": "Experiments show that TM improves performance and robustness compared to various replay strategies, demonstrating that a small set of replay samples can retain general capabilities and enhance performance in sequential tasks.", "conclusion": "GeRe offers a promising approach for efficient replay of LLMs, ensuring both general capabilities and task performance are preserved during continual learning.", "key_contributions": ["Introduction of General Sample Replay (GeRe) framework for LLMs", "Validation that a small set of pre-collected samples suffices for mitigating catastrophic forgetting", "Enhanced activation states constrained optimization using TM loss."], "limitations": "", "keywords": ["continual learning", "large language models", "catastrophic forgetting", "General Sample Replay", "threshold-based margin loss"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.04698", "pdf": "https://arxiv.org/pdf/2508.04698.pdf", "abs": "https://arxiv.org/abs/2508.04698", "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data", "authors": ["Thibaut Thonet", "GermÃ¡n Kruszewski", "Jos Rozen", "Pierre Erbacher", "Marc Dymetman"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.", "AI": {"tldr": "This paper addresses LLM personalization with limited user data, proposing a novel approach called FaST.", "motivation": "The need to personalize LLM-powered conversational assistants to better meet individual user preferences and overcome the limitations of a one-size-fits-all approach.", "method": "Introduction of two datasets (DnD and ELIP) and benchmarking various alignment techniques, focusing on a parameter-efficient method named FaST.", "result": "FaST outperforms other methods by efficiently utilizing high-level features from limited user preference data.", "conclusion": "The research highlights the potential of personalized LLMs with minimal data and introduces effective techniques for achieving this.", "key_contributions": ["Introduced two unique datasets (DnD and ELIP) for evaluating personalization techniques.", "Proposed the FaST approach for effective LLM personalization with limited preference data.", "Benchmarking various alignment techniques to identify optimal performance."], "limitations": "", "keywords": ["LLM personalization", "preference alignment", "limited data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04699", "pdf": "https://arxiv.org/pdf/2508.04699.pdf", "abs": "https://arxiv.org/abs/2508.04699", "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis", "authors": ["Anushka Yadav", "Isha Nalawade", "Srujana Pillarichety", "Yashwanth Babu", "Reshmi Ghosh", "Samyadeep Basu", "Wenlong Zhao", "Ali Nasaeh", "Sriram Balasubramanian", "Soundararajan Srinivasan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of reasoning models and their integration into practical AI\nchat bots has led to breakthroughs in solving advanced math, deep search, and\nextractive question answering problems that requires a complex and multi-step\nthought process. Yet, a complete understanding of why these models hallucinate\nmore than general purpose language models is missing. In this investigative\nstudy, we systematicallyexplore reasoning failures of contemporary language\nmodels on multi-hop question answering tasks. We introduce a novel, nuanced\nerror categorization framework that examines failures across three critical\ndimensions: the diversity and uniqueness of source documents involved (\"hops\"),\ncompleteness in capturing relevant information (\"coverage\"), and cognitive\ninefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by\ncomplementary automated metrics, our exploration uncovers intricate error\npatterns often hidden by accuracy-centric evaluations. This investigative\napproach provides deeper insights into the cognitive limitations of current\nmodels and offers actionable guidance toward enhancing reasoning fidelity,\ntransparency, and robustness in future language modeling efforts.", "AI": {"tldr": "This study investigates reasoning failures in language models during multi-hop question answering, introducing a nuanced error categorization framework and uncovering intricate error patterns.", "motivation": "The research aims to understand why reasoning models in AI chatbots hallucinate more often than general-purpose language models, particularly in complex multi-step tasks.", "method": "A novel error categorization framework examines reasoning failures across dimensions such as source document diversity, information coverage, and cognitive inefficiency, supplemented by human annotation and automated metrics.", "result": "The study uncovers intricate error patterns that are typically overlooked by traditional accuracy-focused evaluations, revealing cognitive limitations in current models.", "conclusion": "The findings provide insights for enhancing the reasoning fidelity, transparency, and robustness of future language models.", "key_contributions": ["Introduction of a nuanced error categorization framework for reasoning failures", "Identification of intricate error patterns in multi-hop question answering", "Actionable guidance for improving reasoning capabilities in language models"], "limitations": "", "keywords": ["reasoning models", "multi-hop question answering", "error categorization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.14805", "pdf": "https://arxiv.org/pdf/2406.14805.pdf", "abs": "https://arxiv.org/abs/2406.14805", "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "categories": ["cs.CL"], "comment": "KDD 2025", "summary": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.", "AI": {"tldr": "This paper discusses how Large Language Models (LLMs) respond to culturally diverse users and their ability to understand and align with different cultural values based on Hofstede's Cultural Dimensions. It examines LLMs' consistency in providing culturally aligned advice and offers recommendations for training more culturally sensitive models.", "motivation": "To investigate whether LLMs can identify and respond according to the diverse cultural values of users based on their countries.", "method": "Prompting various LLMs with advice requests that reflect five Hofstede Cultural Dimensions while incorporating personas from 36 different countries to analyze the models' cultural comprehension and response consistency.", "result": "LLMs can differentiate between various cultural values but do not consistently uphold these values in their responses, indicating a gap in cultural sensitivity and alignment.", "conclusion": "The study highlights the shortcomings of LLMs in providing culturally appropriate advice and offers a framework for enhancing value alignment in model training.", "key_contributions": ["Identification of LLMs' inconsistencies in cultural value alignment", "Development of a methodology to assess LLMs' cultural understanding", "Recommendations for training culturally sensitive LLMs"], "limitations": "The study may not cover all cultural aspects and nuances that could influence user expectations and model responses.", "keywords": ["Large Language Models", "Cultural Values", "Hofstede Dimensions", "Culturally Sensitive AI", "LLM Training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.18454", "pdf": "https://arxiv.org/pdf/2407.18454.pdf", "abs": "https://arxiv.org/abs/2407.18454", "title": "Fairness Definitions in Language Models Explained", "authors": ["Avash Palikhe", "Zichong Wang", "Zhipeng Yin", "Wenbin Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language Models (LMs) have demonstrated exceptional performance across\nvarious Natural Language Processing (NLP) tasks. Despite these advancements,\nLMs can inherit and amplify societal biases related to sensitive attributes\nsuch as gender and race, limiting their adoption in real-world applications.\nTherefore, fairness has been extensively explored in LMs, leading to the\nproposal of various fairness notions. However, the lack of clear agreement on\nwhich fairness definition to apply in specific contexts and the complexity of\nunderstanding the distinctions between these definitions can create confusion\nand impede further progress. To this end, this paper proposes a systematic\nsurvey that clarifies the definitions of fairness as they apply to LMs.\nSpecifically, we begin with a brief introduction to LMs and fairness in LMs,\nfollowed by a comprehensive, up-to-date overview of existing fairness notions\nin LMs and the introduction of a novel taxonomy that categorizes these concepts\nbased on their transformer architecture: encoder-only, decoder-only, and\nencoder-decoder LMs. We further illustrate each definition through experiments,\nshowcasing their practical implications and outcomes. Finally, we discuss\ncurrent research challenges and open questions, aiming to foster innovative\nideas and advance the field. The repository is publicly available online at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.", "AI": {"tldr": "This paper surveys fairness definitions in Language Models (LMs) and proposes a novel taxonomy categorizing these concepts based on transformer architecture.", "motivation": "The increasing performance of LMs in NLP tasks is accompanied by the inheritance and amplification of societal biases, necessitating a systematic exploration of fairness in LMs.", "method": "The paper provides an overview of existing fairness notions in LMs and introduces a taxonomy for categorizing these concepts based on LM architecture: encoder-only, decoder-only, and encoder-decoder.", "result": "Experiments illustrating each fairness definition showcase their practical implications and outcomes, identifying current research challenges and open questions.", "conclusion": "The survey aims to clarify fairness concepts in LMs and foster innovative ideas to advance research in this area.", "key_contributions": ["Systematic survey of fairness definitions in LMs", "Novel taxonomy categorizing fairness notions based on LM architecture", "Experiments illustrating practical implications of fairness definitions"], "limitations": "", "keywords": ["Language Models", "fairness", "Natural Language Processing", "bias", "taxonomy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.15395", "pdf": "https://arxiv.org/pdf/2409.15395.pdf", "abs": "https://arxiv.org/abs/2409.15395", "title": "Parse Trees Guided LLM Prompt Compression", "authors": ["Wenhao Mao", "Chengbin Hou", "Tianyu Zhang", "Xinyu Lin", "Ke Tang", "Hairong Lv"], "categories": ["cs.CL", "cs.AI"], "comment": "IEEE TPAMI major revision submitted", "summary": "Offering rich contexts to Large Language Models (LLMs) has shown to boost the\nperformance in various tasks, but the resulting longer prompt would increase\nthe computational cost and might exceed the input limit of LLMs. Recently, some\nprompt compression methods have been suggested to shorten the length of prompts\nby using language models to generate shorter prompts or by developing\ncomputational models to select important parts of original prompt. The\ngenerative compression methods would suffer from issues like hallucination,\nwhile the selective compression methods have not involved linguistic rules and\noverlook the global structure of prompt. To this end, we propose a novel\nselective compression method called PartPrompt. It first obtains a parse tree\nfor each sentence based on linguistic rules, and calculates local information\nentropy for each node in a parse tree. These local parse trees are then\norganized into a global tree according to the hierarchical structure such as\nthe dependency of sentences, paragraphs, and sections. After that, the\nroot-ward propagation and leaf-ward propagation are proposed to adjust node\nvalues over the global tree. Finally, a recursive algorithm is developed to\nprune the global tree based on the adjusted node values. The experiments show\nthat PartPrompt receives the state-of-the-art performance across various\ndatasets, metrics, compression ratios, and target LLMs for inference. The\nin-depth ablation studies confirm the effectiveness of designs in PartPrompt,\nand other additional experiments also demonstrate its superiority in terms of\nthe coherence of compressed prompts and in the extreme long prompt scenario.", "AI": {"tldr": "PartPrompt is a novel selective compression method for prompts that uses parse trees and local information entropy to enhance the performance of Large Language Models while minimizing computational costs.", "motivation": "To address the issues of increased computational costs and prompt length limits in LLMs when providing rich contexts, while improving prompt compression methods that have limitations in hallucination and structural consideration.", "method": "PartPrompt employs linguistic rules to create parse trees for sentences, calculates local information entropy for tree nodes, organizes these into a global tree based on hierarchical structures, and uses propagation techniques to adjust node values before pruning the tree with a recursive algorithm.", "result": "PartPrompt achieves state-of-the-art performance across various datasets and metrics, demonstrating effective compression ratios and maintaining coherence in compressed prompts, particularly in scenarios with extremely long prompts.", "conclusion": "The experimental results validate the design choices made in PartPrompt, affirming its superiority over existing compression approaches and highlighting its practicality for LLMs in different contexts.", "key_contributions": ["Introduction of a novel selective compression method for LLM prompts called PartPrompt.", "The use of linguistic rules for parse trees to enhance structural understanding in compression.", "Demonstration of state-of-the-art performance and coherence in compressed prompts against benchmarks."], "limitations": "", "keywords": ["Large Language Models", "prompt compression", "parse trees", "linguistic rules", "information entropy"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.03723", "pdf": "https://arxiv.org/pdf/2410.03723.pdf", "abs": "https://arxiv.org/abs/2410.03723", "title": "Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "5 main pages, 10 total pages", "summary": "As AI advances in text generation, human trust in AI generated content\nremains constrained by biases that go beyond concerns of accuracy. This study\nexplores how bias shapes the perception of AI versus human generated content.\nThrough three experiments involving text rephrasing, news article\nsummarization, and persuasive writing, we investigated how human raters respond\nto labeled and unlabeled content. While the raters could not differentiate the\ntwo types of texts in the blind test, they overwhelmingly favored content\nlabeled as \"Human Generated,\" over those labeled \"AI Generated,\" by a\npreference score of over 30%. We observed the same pattern even when the labels\nwere deliberately swapped. This human bias against AI has broader societal and\ncognitive implications, as it undervalues AI performance. This study highlights\nthe limitations of human judgment in interacting with AI and offers a\nfoundation for improving human-AI collaboration, especially in creative fields.", "AI": {"tldr": "This study investigates how human biases affect the perception of AI-generated versus human-generated content, revealing a significant preference for human-generated materials.", "motivation": "To understand how biases influence human trust in AI-generated content as AI technology advances.", "method": "Three experiments were conducted involving text rephrasing, news article summarization, and persuasive writing to assess human raters' preferences for AI versus human-generated content.", "result": "Human raters favored content labeled as 'Human Generated' over 'AI Generated' by more than 30%, despite the inability to differentiate between them in blind tests.", "conclusion": "The findings underline the limitations of human judgment in evaluating AI content and suggest ways to enhance human-AI collaboration in creative tasks.", "key_contributions": ["Revealed significant human preference bias against AI-generated content", "Demonstrated that labeling can substantially influence perceptions of credibility", "Provided insights for improving human-AI collaboration in creative domains."], "limitations": "The study primarily focused on text generation without exploring other types of AI-generated content.", "keywords": ["AI content generation", "human bias", "human-AI interaction", "perception study", "creative fields"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.15576", "pdf": "https://arxiv.org/pdf/2410.15576.pdf", "abs": "https://arxiv.org/abs/2410.15576", "title": "A Survey of Conversational Search", "authors": ["Fengran Mo", "Kelong Mao", "Ziliang Zhao", "Hongjin Qian", "Haonan Chen", "Yiruo Cheng", "Xiaoxi Li", "Yutao Zhu", "Zhicheng Dou", "Jian-Yun Nie"], "categories": ["cs.CL", "cs.IR"], "comment": "38 pages, 8 figures, corresponding Github repository:\n  https://github.com/fengranMark/ConvSearch-Survey", "summary": "As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch.", "AI": {"tldr": "This survey explores advancements in conversational search systems that utilize large language models (LLMs) to facilitate complex, context-aware information retrieval through natural language dialogue.", "motivation": "The increasing role of AI and NLP technologies in enhancing search engine capabilities to create intuitive user interactions and improve information retrieval.", "method": "This paper surveys existing research on conversational search systems, examining critical components such as query reformulation, search clarification, and conversational retrieval, while assessing the role of LLMs in these systems.", "result": "The survey identifies key advancements in conversational search, discusses integration challenges and opportunities with LLMs, and presents insights into real-world applications and performance evaluations of current systems.", "conclusion": "The paper aims to guide future research and development in conversational search by highlighting trends, challenges, and practical applications in the field.", "key_contributions": ["Analyzes key components of conversational search systems.", "Discusses the integration of LLMs and their impact on search capabilities.", "Highlights real-world applications and evaluation metrics of conversational search."], "limitations": "", "keywords": ["conversational search", "natural language processing", "large language models", "information retrieval", "user experience"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2410.16520", "pdf": "https://arxiv.org/pdf/2410.16520.pdf", "abs": "https://arxiv.org/abs/2410.16520", "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context", "authors": ["Naba Rizvi", "Harper Strickland", "Daniel Gitelman", "Tristan Cooper", "Alexis Morales-Flores", "Michael Golden", "Aekta Kallepalli", "Akshat Alurkar", "Haaset Owens", "Saleha Ahmedi", "Isha Khirwadkar", "Imani Munyaka", "Nedjma Ousidhoum"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to ACL main 2025, 9 pages, 5 figures, 7 tables", "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.", "AI": {"tldr": "The paper introduces AUTALIC, a benchmark dataset for detecting anti-autistic ableist language in NLP, aiming to improve nuanced understanding in context.", "motivation": "To address underexplored aspects of ableist language towards autistic individuals and improve NLP tools' sensitivity to such language.", "method": "The study involves the creation of AUTALIC, a dataset of 2,400 autism-related sentences sourced from Reddit and annotated by experts in neurodiversity.", "result": "Evaluation shows that current language models, including advanced LLMs, struggle to detect anti-autistic ableism accurately, indicating a gap in their performance relative to human judgment.", "conclusion": "AUTALIC serves as a vital resource for developing more context-aware NLP systems, emphasizing a need for sensitivity towards neurodiversity.", "key_contributions": ["First dataset dedicated to anti-autistic ableist language detection", "Evaluation highlights limitations of current LLMs in understanding context", "Public release of AUTALIC to aid future research in ableism and neurodiversity"], "limitations": "Current NLP tools fail to accurately identify the nuanced expressions of ableist language.", "keywords": ["anti-autistic ableism", "NLP", "neurodiversity", "benchmark dataset", "language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.08397", "pdf": "https://arxiv.org/pdf/2411.08397.pdf", "abs": "https://arxiv.org/abs/2411.08397", "title": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision", "authors": ["Aoi Ito", "Kota Dohi", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries.", "AI": {"tldr": "CLaSP is a model for retrieving time-series signals based on natural language queries, enhancing scalability and accuracy by using contrastive learning and large language models.", "motivation": "The need for efficient retrieval of time-series signals based on descriptive queries in fields like industrial diagnostics.", "method": "CLaSP employs contrastive learning to map time-series signals to natural language descriptions, eliminating reliance on predefined synonym dictionaries.", "result": "CLaSP demonstrates high accuracy in retrieving various time series patterns using natural language queries with the TRUCE and SUSHI datasets.", "conclusion": "CLaSP provides a scalable and adaptable approach to time-series signal retrieval that leverages the capabilities of LLMs.", "key_contributions": ["Introduces a novel retrieval model for time-series signals using natural language queries.", "Eliminates the need for predefined synonym dictionaries.", "Achieves high accuracy in various signal retrieval tasks."], "limitations": "", "keywords": ["time-series retrieval", "natural language queries", "contrastive learning", "large language models", "industrial diagnostics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.12422", "pdf": "https://arxiv.org/pdf/2412.12422.pdf", "abs": "https://arxiv.org/abs/2412.12422", "title": "FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs", "authors": ["Monica Munnangi", "Akshay Swaminathan", "Jason Alan Fries", "Jenelle Jindal", "Sanjana Narayanan", "Ivan Lopez", "Lucia Tu", "Philip Chung", "Jesutofunmi A. Omiye", "Mehr Kashyap", "Nigam Shah"], "categories": ["cs.CL"], "comment": "To appear at MLHC 2025", "summary": "Verifying and attributing factual claims is essential for the safe and\neffective use of large language models (LLMs) in healthcare. A core component\nof factuality evaluation is fact decomposition, the process of breaking down\ncomplex clinical statements into fine-grained atomic facts for verification.\nRecent work has proposed fact decomposition, which uses LLMs to rewrite source\ntext into concise sentences conveying a single piece of information, to\nfacilitate fine-grained fact verification. However, clinical documentation\nposes unique challenges for fact decomposition due to dense terminology and\ndiverse note types and remains understudied. To address this gap and explore\nthese challenges, we present FactEHR, an NLI dataset consisting of document\nfact decompositions for 2,168 clinical notes spanning four types from three\nhospital systems, resulting in 987,266 entailment pairs. We assess the\ngenerated facts on different axes, from entailment evaluation of LLMs to a\nqualitative analysis. Our evaluation, including review by the clinicians,\nreveals substantial variability in LLM performance for fact decomposition. For\nexample, Gemini-1.5-Flash consistently generates relevant and accurate facts,\nwhile Llama-3 8B produces fewer and less consistent outputs. The results\nunderscore the need for better LLM capabilities to support factual verification\nin clinical text.", "AI": {"tldr": "The paper introduces FactEHR, a dataset for fact decomposition in clinical documentation, essential for verifying factual claims in healthcare using LLMs.", "motivation": "Verifying factual claims in healthcare LLM applications is critical yet challenging due to the complexity of clinical texts.", "method": "FactEHR, an NLI dataset containing 2,168 clinical notes and 987,266 entailment pairs, was created to facilitate fine-grained fact verification and assess LLM performance on this task.", "result": "The evaluation shows significant variability in LLM performance, highlighting that some models like Gemini-1.5-Flash excel, while others like Llama-3 8B underperform in generating accurate facts.", "conclusion": "Improving LLM capabilities for fact verification in clinical texts is necessary to ensure safe healthcare applications of these technologies.", "key_contributions": ["Introduction of the FactEHR dataset for clinical fact decomposition", "Assessment of LLM performance variability in generating accurate clinical facts", "Insights into challenges posed by clinical documentation for LLM fact decomposition"], "limitations": "The study is limited to a specific set of clinical notes and may not generalize across all healthcare contexts.", "keywords": ["fact decomposition", "large language models", "healthcare", "dataset", "fact verification"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13053", "pdf": "https://arxiv.org/pdf/2502.13053.pdf", "abs": "https://arxiv.org/abs/2502.13053", "title": "Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks", "authors": ["Yurun Chen", "Xavier Hu", "Keting Yin", "Juncheng Li", "Shengyu Zhang"], "categories": ["cs.CL"], "comment": "Accepted at ACM MM 2025 Main Conference", "summary": "As researchers continue to optimize AI agents for more effective task\nexecution within operating systems, they often overlook a critical security\nconcern: the ability of these agents to detect \"impostors\" within their\nenvironment. Through an analysis of the agents' operational context, we\nidentify a significant threat-attackers can disguise malicious attacks as\nenvironmental elements, injecting active disturbances into the agents'\nexecution processes to manipulate their decision-making. We define this novel\nthreat as the Active Environment Injection Attack (AEIA). Focusing on the\ninteraction mechanisms of the Android OS, we conduct a risk assessment of AEIA\nand identify two critical security vulnerabilities: (1) Adversarial content\ninjection in multimodal interaction interfaces, where attackers embed\nadversarial instructions within environmental elements to mislead agent\ndecision-making; and (2) Reasoning gap vulnerabilities in the agent's task\nexecution process, which increase susceptibility to AEIA attacks during\nreasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,\nan attack scheme that exploits interaction vulnerabilities in mobile operating\nsystems to assess the robustness of MLLM-based agents. Experimental results\nshow that even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% on the AndroidWorld benchmark by combining\ntwo vulnerabilities.", "AI": {"tldr": "The paper introduces the Active Environment Injection Attack (AEIA) targeting AI agents in Android OS, revealing how adversaries can manipulate agent decision-making through environmental disguises.", "motivation": "To highlight a critical security concern in AI agents' decision-making processes within operating systems, focusing on vulnerability to deceptive environmental elements.", "method": "A risk assessment is conducted on Android OS to define AEIA and assess its impact, along with the proposal of AEIA-MN, an attack scheme exploiting interaction vulnerabilities.", "result": "Experimental results demonstrate a high vulnerability of MLLM-based agents to AEIA, achieving up to a 93% attack success rate on the AndroidWorld benchmark.", "conclusion": "The study underscores the significant risks posed by AEIA to AI agents and the need for enhanced security measures in multimodal interaction interfaces.", "key_contributions": ["Definition of Active Environment Injection Attack (AEIA)", "Identification of critical security vulnerabilities in multimodal interfaces", "Proposal of AEIA-MN attack scheme for evaluating MLLM-based agents"], "limitations": "The focus is primarily on Android OS, and the proposed vulnerabilities may not be generalizable across all platforms or environments.", "keywords": ["Active Environment Injection Attack", "AI security", "Android OS", "MLLM vulnerability", "adversarial attacks"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.16781", "pdf": "https://arxiv.org/pdf/2502.16781.pdf", "abs": "https://arxiv.org/abs/2502.16781", "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data", "authors": ["Bhawna Piryani", "Jamshid Mozafari", "Abdelrahman Abdallah", "Antoine Doucet", "Adam Jatowt"], "categories": ["cs.CL"], "comment": "Accepted at CIKM 2025", "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts.", "AI": {"tldr": "This paper analyzes the impact of OCR errors on Multilingual QA Systems and introduces a dataset, MultiOCR-QA, with 50K question-answer pairs from OCR-ed historical documents.", "motivation": "The study addresses the significant impact of Optical Character Recognition (OCR) errors on the performance of question-answering (QA) systems in the context of multilingual and historical document digitization.", "method": "The authors create a multilingual QA dataset, MultiOCR-QA, with 50,000 curated question-answer pairs from OCR-ed documents in English, French, and German, and evaluate several state-of-the-art Large Language Models (LLMs) under various OCR error conditions.", "result": "The evaluation reveals that QA systems are very susceptible to OCR errors, which leads to poor performance when working with noisy OCR text, emphasizing the inadequacy of current models in handling such challenges.", "conclusion": "The paper concludes that there is a pressing need for the development of more noise-resilient QA systems, particularly aimed at improving the handling of OCR-induced errors in historical document digitization.", "key_contributions": ["Introduction of the MultiOCR-QA dataset", "Comprehensive analysis of the effects of OCR errors on QA systems", "Insights into the limitations of state-of-the-art LLMs under noisy conditions"], "limitations": "The dataset may not cover all types of OCR noise or historical document variability, potentially limiting its applicability in some real-world scenarios.", "keywords": ["Optical Character Recognition", "Multilingual QA", "Large Language Models", "OCR noise", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2502.17945", "pdf": "https://arxiv.org/pdf/2502.17945.pdf", "abs": "https://arxiv.org/abs/2502.17945", "title": "Assessing Agentic Large Language Models in Multilingual National Bias", "authors": ["Qianying Liu", "Katrina Qiyao Wang", "Fei Cheng", "Sadao Kurohashi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings. 14 pages", "summary": "Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education. \\footnote{Code\navailable at: https://github.com/yiyunya/assess_agentic_national_bias", "AI": {"tldr": "This study investigates multilingual bias in Large Language Models (LLMs) by analyzing their performance in providing personalized advice across different languages. It highlights the significant local language bias present and the varying capabilities of different LLMs.", "motivation": "To address the gap in understanding how Large Language Models handle multilingual reasoning and recommendations, especially in the context of biases that emerge across languages.", "method": "The study tests LLMs in three scenarios: university applications, travel, and relocation, analyzing their responses in multiple languages and measuring bias based on demographic factors and reasoning strategies.", "result": "Findings indicate that local language bias is common across tasks, with newer models like GPT-4 showing less bias for English compared to GPT-3.5 but lacking in robust multilingual alignment.", "conclusion": "The results reveal critical insights into how biases manifest in LLM outputs, which has important implications for the deployment of multilingual AI systems, especially in educational contexts.", "key_contributions": ["First study to analyze multilingual bias in LLMs across decision-making tasks.", "Quantifies bias in model-generated scores with respect to demographic factors.", "Demonstrates differential performance of GPT-4 versus GPT-3.5 in multilingual contexts."], "limitations": "Focus is primarily on specific decision-making tasks; broader implications beyond the tested scenarios need further exploration.", "keywords": ["Multilingual NLP", "Large Language Models", "Bias", "Reasoning Strategies", "Personalized Advice"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2503.10533", "pdf": "https://arxiv.org/pdf/2503.10533.pdf", "abs": "https://arxiv.org/abs/2503.10533", "title": "The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory", "authors": ["Robin Schmucker", "Steven Moore"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nThis method offers a scalable, pre-deployment evaluation without requiring\nstudent data, but its predictive validity concerning empirical IRT parameters\nis underexplored. To address this gap, we conducted a study involving 7,126\nmultiple-choice questions across various STEM subjects (physical science,\nmathematics, and life/earth sciences). Using an automated approach, we\nannotated each question with a 19-criteria IWF rubric and studied relationships\nto data-driven IRT parameters. Our analysis revealed statistically significant\nlinks between the number of IWFs and IRT difficulty and discrimination\nparameters, particularly in life/earth and physical science domains. We further\nobserved how specific IWF criteria can impact item quality more and less\nseverely (e.g., negative wording vs. implausible distractors) and how they\nmight make a question more or less challenging. Overall, our findings establish\nautomated IWF analysis as a valuable supplement to traditional validation,\nproviding an efficient method for initial item screening, particularly for\nflagging low-difficulty MCQs. Our findings show the need for further research\non domain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.", "AI": {"tldr": "The study explores the use of Item-Writing Flaw (IWF) rubrics to evaluate multiple-choice questions in STEM subjects, establishing links between IWF features and Item Response Theory (IRT) parameters.", "motivation": "To find an efficient, scalable method for evaluating test items that does not rely on time-consuming pilot testing, while also enhancing the predictive validity of IWF rubrics in relation to IRT.", "method": "Analyzed 7,126 multiple-choice questions by annotating them with a 19-criteria IWF rubric and examined the relationships to IRT difficulty and discrimination parameters.", "result": "Statistically significant correlations were found between the number of IWFs and IRT parameters, demonstrating how specific flaws affect item quality and challenge.", "conclusion": "Automated IWF analysis can effectively supplement traditional validation methods, aiding in the initial screening of multiple-choice questions and highlighting the need for further research in this area.", "key_contributions": ["Establishes a connection between IWF features and IRT parameters in educational assessments.", "Demonstrates the practical application of automated IWF analysis for efficient test item evaluation.", "Identifies specific IWF criteria that notably affect item quality and difficulty."], "limitations": "The study primarily focuses on STEM subjects, indicating that results may not generalize across all domains without further research.", "keywords": ["Item Response Theory", "Item-Writing Flaw", "educational assessment", "multiple-choice questions", "STEM"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2503.18878", "pdf": "https://arxiv.org/pdf/2503.18878.pdf", "abs": "https://arxiv.org/abs/2503.18878", "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders", "authors": ["Andrey Galichin", "Alexey Dontsov", "Polina Druzhinina", "Anton Razzhigaev", "Oleg Y. Rogov", "Elena Tutubalina", "Ivan Oseledets"], "categories": ["cs.CL"], "comment": null, "summary": "Recent LLMs like DeepSeek-R1 have demonstrated state-of-the-art performance\nby integrating deep thinking and complex reasoning during generation. However,\nthe internal mechanisms behind these reasoning processes remain unexplored. We\nobserve reasoning LLMs consistently use vocabulary associated with human\nreasoning processes. We hypothesize these words correspond to specific\nreasoning moments within the models' internal mechanisms. To test this\nhypothesis, we employ Sparse Autoencoders (SAEs), a technique for sparse\ndecomposition of neural network activations into human-interpretable features.\nWe introduce ReasonScore, an automatic metric to identify active SAE features\nduring these reasoning moments. We perform manual and automatic interpretation\nof the features detected by our metric, and find those with activation patterns\nmatching uncertainty, exploratory thinking, and reflection. Through steering\nexperiments, we demonstrate that amplifying these features increases\nperformance on reasoning-intensive benchmarks (+2.2%) while producing longer\nreasoning traces (+20.5%). Using the model diffing technique, we provide\nevidence that these features are present only in models with reasoning\ncapabilities. Our work provides the first step towards a mechanistic\nunderstanding of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning", "AI": {"tldr": "This paper explores the internal reasoning mechanisms of LLMs like DeepSeek-R1 using Sparse Autoencoders and introduces the ReasonScore metric.", "motivation": "To investigate the unexplored internal reasoning processes of LLMs and understand their mechanisms through human-interpretable features.", "method": "The study employs Sparse Autoencoders to decompose neural network activations and utilizes the ReasonScore metric to identify active features during reasoning moments.", "result": "The experiments show that amplifying identified features enhances performance on reasoning benchmarks by +2.2% and increases reasoning trace length by +20.5%.", "conclusion": "This research marks the first step towards understanding LLM reasoning mechanisms, with evidence of specific features tied to reasoning capabilities.", "key_contributions": ["Introduction of Sparse Autoencoders for LLM interpretation", "Development of ReasonScore metric for identifying reasoning features", "Demonstration of enhanced LLM performance through feature amplification"], "limitations": "", "keywords": ["Large Language Models", "Reasoning Mechanisms", "Sparse Autoencoders", "Human-Computer Interaction", "Neural Network Interpretation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.12342", "pdf": "https://arxiv.org/pdf/2504.12342.pdf", "abs": "https://arxiv.org/abs/2504.12342", "title": "CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine", "authors": ["Hanmeng Zhong", "Linqing Chen", "Wentao Wu", "Weilei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent development in Retrieval-Augmented Large Language Models (LLMs) have\nshown great promise in biomedical applications. How ever, a critical gap\npersists in reliably evaluating their curation ability the process by which\nmodels select and integrate relevant references while filtering out noise. To\naddress this, we introduce the benchmark for Curation of Retrieval-Augmented\nLLMs in Biomedicine (CRAB), the first multilingual benchmark tailored for\nevaluating the biomedical curation of retrieval-augmented LLMs, available in\nEnglish, French, German and Chinese. By incorporating a novel citation-based\nevaluation metric, CRAB quantifies the curation performance of\nretrieval-augmented LLMs in biomedicine. Experimental results reveal\nsignificant discrepancies in the curation performance of mainstream LLMs,\nunderscoring the urgent need to improve it in the domain of biomedicine. Our\ndataset is available at https://huggingface.co/datasets/zhm0/CRAB.", "AI": {"tldr": "The paper introduces CRAB, the first multilingual benchmark for evaluating the curation ability of retrieval-augmented LLMs in biomedicine, highlighting performance discrepancies among mainstream models.", "motivation": "To evaluate the curation ability of retrieval-augmented LLMs in biomedicine, addressing a critical gap in existing methodologies.", "method": "The paper presents CRAB, a multilingual benchmark that uses a novel citation-based evaluation metric to assess the performance of retrieval-augmented LLMs in selecting and integrating relevant biomedical references.", "result": "Experimental results show significant variations in curation performance among mainstream retrieval-augmented LLMs, indicating that many need improvement.", "conclusion": "There is an urgent need to enhance the curation capabilities of LLMs in the biomedical domain, as demonstrated by the results from CRAB.", "key_contributions": ["First multilingual benchmark for biomedical curation of retrieval-augmented LLMs", "Introduces a novel citation-based evaluation metric", "Exposes performance discrepancies in existing mainstream LLMs"], "limitations": "", "keywords": ["Retrieval-Augmented LLMs", "Biomedical Curation", "Benchmarking", "Curation Performance", "Citation-based Evaluation"], "importance_score": 9, "read_time_minutes": 10}}
