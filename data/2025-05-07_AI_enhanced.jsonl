{"id": "2505.03027", "pdf": "https://arxiv.org/pdf/2505.03027.pdf", "abs": "https://arxiv.org/abs/2505.03027", "title": "Revisiting Performance Models of Distal Pointing Tasks in Virtual Reality", "authors": ["Logan Lane", "Feiyu Lu", "Shakiba Davari", "Rob Teather", "Doug A. Bowman"], "categories": ["cs.HC"], "comment": null, "summary": "Performance models of interaction, such as Fitts Law, are important tools for\npredicting and explaining human motor performance and for designing\nhigh-performance user interfaces. Extensive prior work has proposed such models\nfor the 3D interaction task of distal pointing, in which the user points their\nhand or a device at a distant target in order to select it. However, there is\nno consensus on how to compute the index of difficulty for distal pointing\ntasks. We present a preliminary study suggesting that existing models may not\nbe sufficient to model distal pointing performance with current virtual reality\ntechnologies. Based on these results, we hypothesized that both the form of the\nmodel and the standard method for collecting empirical data for pointing tasks\nmight need to change in order to achieve a more accurate and valid distal\npointing model. In our main study, we used a new methodology to collect distal\npointing data and evaluated traditional models, purely ballistic models, and\ntwo-part models. Ultimately, we found that the best model used a simple\nFitts-Law-style index of difficulty with angular measures of amplitude and\nwidth.", "AI": {"tldr": "This study examines the efficacy of existing models for predicting distal pointing performance in 3D interaction, suggesting a need for new methodologies and model forms due to current VR technology limitations.", "motivation": "The study aims to address the lack of consensus on computing the index of difficulty for distal pointing tasks amidst evolving VR technologies.", "method": "A new methodology was employed to collect empirical data for distal pointing tasks, evaluating traditional models, ballistic models, and two-part models.", "result": "The study concluded that a modified Fitts-Law-style index of difficulty with angular measures of amplitude and width is superior for modeling distal pointing performance.", "conclusion": "Existing models may not suffice for current virtual reality applications, necessitating methodological changes for better modeling.", "key_contributions": ["Introduced a new methodology for collecting distal pointing data.", "Evaluated various models of interaction performance.", "Proposed an improved model for distal pointing using angular measures."], "limitations": "The study is preliminary and may require further testing in diverse contexts and technologies.", "keywords": ["Human-Computer Interaction", "Distal Pointing", "Fitts Law", "Virtual Reality", "Performance Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03073", "pdf": "https://arxiv.org/pdf/2505.03073.pdf", "abs": "https://arxiv.org/abs/2505.03073", "title": "Coupling the Heart to Musical Machines", "authors": ["Eric Easthope"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Biofeedback is being used more recently as a general control paradigm for\nhuman-computer interfaces (HCIs). While biofeedback especially from breath has\nseen increasing uptake as a controller for novel musical interfaces, new\ninterfaces for musical expression (NIMEs), the community has not given as much\nattention to the heart. The heart is just as intimate a part of music as breath\nand it is argued that the heart determines our perception of time and so\nindirectly our perception of music. Inspired by this I demonstrate a\nphotoplethysmogram (PPG)-based NIME controller using heart rate as a 1D control\nparameter to transform the qualities of sounds in real-time over a Bluetooth\nwireless HCI. I apply time scaling to \"warp\" audio buffers inbound to the sound\ncard, and play these transformed audio buffers back to the listener wearing the\nPPG sensor, creating a hypothetical perceptual biofeedback loop: changes in\nsound change heart rate to change PPG measurements to change sound. I discuss\nhow a sound-heart-PPG biofeedback loop possibly affords greater control and/or\nvariety of movements with a 1D controller, how controlling the space and/or\ntime scale of sound playback with biofeedback makes for possibilities in\nperformance ambience, and I briefly discuss generative latent spaces as a\npossible way to extend a 1D PPG control space.", "AI": {"tldr": "The paper presents a novel musical interface controlled by heart rate data via a photoplethysmogram (PPG), exploring the relationship between sound and heart rate in real-time to enhance musical expression.", "motivation": "To explore the potential of heart rate as a control parameter in musical interfaces, complementing existing biofeedback mechanisms utilizing breath.", "method": "The author developed a PPG-based musical interface that transforms audio in real-time based on heart rate changes, implementing a feedback loop between sound and physiological responses.", "result": "The PPG-based controller dynamically alters sound characteristics based on the user's heart rate, demonstrating how biofeedback can affect sound perception and performance ambience.", "conclusion": "The findings suggest the heart's role in musical interaction could offer a new dimension to human-computer interaction in music, potentially enriching performance experiences through a feedback loop.", "key_contributions": ["Introduction of heart rate as a control parameter in musical interfaces", "Implementation of a real-time audio processing technique using PPG data", "Discussion of generative latent spaces for extending control parameters"], "limitations": "The study is limited by the complexity of building a robust real-time system and the need for more empirical validation in varied musical contexts.", "keywords": ["biofeedback", "human-computer interaction", "music technology", "heart rate", "real-time audio processing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.03105", "pdf": "https://arxiv.org/pdf/2505.03105.pdf", "abs": "https://arxiv.org/abs/2505.03105", "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation", "authors": ["Xule Lin"], "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"], "comment": "62 pages (31 appendix pages for guidance), 2 figures", "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.", "AI": {"tldr": "The article introduces Cognitio Emergens (CE), a framework for understanding human-AI collaboration in scientific knowledge creation, emphasizing dynamic roles, capabilities, and the evolution of partnerships.", "motivation": "This paper addresses the limitations of existing models in capturing the dynamic and co-evolutionary nature of human-AI partnerships in scientific inquiry.", "method": "The CE framework integrates Agency Configurations, Epistemic Dimensions, and Partnership Dynamics to analyze and categorize the evolving relationships between humans and AI during knowledge creation.", "result": "CE reveals that scientific understanding emerges through ongoing negotiations of roles, values, and structures in human-AI collaborations, highlighting the risk of epistemic alienation.", "conclusion": "By framing human-AI scientific collaboration as co-evolutionary, the paper provides tools to foster meaningful human participation amidst transformative AI abilities in research.", "key_contributions": ["Introduction of the Cognitio Emergens framework for human-AI collaboration in science.", "Identification of dynamic Agency Configurations and Epistemic Dimensions in partnerships.", "Exploration of Partnership Dynamics and the concept of epistemic alienation."], "limitations": "The framework may not address all possible dimensions of human-AI interaction in diverse scientific fields.", "keywords": ["Human-AI collaboration", "Epistemic partnerships", "Cognitio Emergens", "Scientific knowledge creation", "Epistemic alienation"], "importance_score": 6, "read_time_minutes": 40}}
{"id": "2505.03117", "pdf": "https://arxiv.org/pdf/2505.03117.pdf", "abs": "https://arxiv.org/abs/2505.03117", "title": "Do ATCOs Need Explanations, and Why? Towards ATCO-Centered Explainable AI for Conflict Resolution Advisories", "authors": ["Katherine Fennedy", "Brian Hilburn", "Thaivalappil N. M. Nadirsha", "Sameer Alam", "Khanh-Duy Le", "Hua Li"], "categories": ["cs.HC"], "comment": "2025 ATRD US-Europe Air Transportation Research & Development\n  Symposium", "summary": "Interest in explainable artificial intelligence (XAI) is surging. Prior\nresearch has primarily focused on systems' ability to generate explanations,\noften guided by researchers' intuitions rather than end-users' needs.\nUnfortunately, such approaches have not yielded favorable outcomes when\ncompared to a black-box baseline (i.e., no explanation). To address this gap,\nthis paper advocates a human-centered approach that shifts focus to air traffic\ncontrollers (ATCOs) by asking a fundamental yet overlooked question: Do ATCOs\nneed explanations, and if so, why? Insights from air traffic management (ATM),\nhuman-computer interaction, and the social sciences were synthesized to provide\na holistic understanding of XAI challenges and opportunities in ATM. Evaluating\n11 ATM operational goals revealed a clear need for explanations when ATCOs aim\nto document decisions and rationales for future reference or report generation.\nConversely, ATCOs are less likely to seek them when their conflict resolution\napproach align with the artificial intelligence (AI) advisory. While this is a\npreliminary study, the findings are expected to inspire broader and deeper\ninquiries into the design of ATCO-centric XAI systems, paving the way for more\neffective human-AI interaction in ATM.", "AI": {"tldr": "This paper emphasizes a human-centered approach to explainable AI (XAI) in air traffic management, focusing on the needs of air traffic controllers (ATCOs) for explanations when making decisions.", "motivation": "To address the gap in XAI research which often overlooks end-user needs, specifically for air traffic controllers.", "method": "Synthesized insights from air traffic management, human-computer interaction, and social sciences, evaluating the need for explanations based on 11 operational goals in ATM.", "result": "The study found that ATCOs need explanations for documenting decisions but are less inclined to seek them when their decision-making aligns with AI recommendations.", "conclusion": "These preliminary findings support the idea that further investigations into ATCO-centric XAI systems are necessary for improved human-AI interactions in air traffic management.", "key_contributions": ["Introduction of a human-centered approach to XAI in ATM", "Evaluation of ATCOs' needs for explanations", "Identification of circumstances under which ATCOs seek explanations."], "limitations": "This is a preliminary study with limited scope and more extensive research is required.", "keywords": ["explainable AI", "human-centered design", "air traffic management", "decision-making", "human-computer interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.02847", "pdf": "https://arxiv.org/pdf/2505.02847.pdf", "abs": "https://arxiv.org/abs/2505.02847", "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.", "AI": {"tldr": "SAGE is an evaluation framework that assesses LLMs' understanding of human social cognition by simulating emotional changes and thoughts during conversations.", "motivation": "To address the challenge of evaluating how well large language models understand human emotions and social interactions.", "method": "SAGE simulates a Sentient Agent that reasons about its emotions and replies during multi-turn conversations, resulting in numerical emotion trajectories and interpretable thoughts.", "result": "Experiments indicate a strong correlation between the final Sentient emotion score and established psychological metrics, validating the framework's effectiveness in measuring social cognition.", "conclusion": "SAGE offers a scalable tool for evaluating empathetic capabilities of language models, revealing significant gaps in performance among current systems.", "key_contributions": ["Introduction of SAGE framework for evaluating LLMs' social cognition.", "Development of a public Sentient Leaderboard for comparing empathy metrics across models.", "Demonstration of SAGE's ability to uncover performance gaps not seen in traditional evaluation methods."], "limitations": "", "keywords": ["large language models", "human emotion", "social cognition", "evaluative framework", "empathetic agents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.03164", "pdf": "https://arxiv.org/pdf/2505.03164.pdf", "abs": "https://arxiv.org/abs/2505.03164", "title": "InfoVids: Reimagining the Viewer Experience with Alternative Visualization-Presenter Relationships", "authors": ["Ji Won Chung", "Tongyu Zhou", "Ivy Chen", "Kevin Hsu", "Ryan A. Rossi", "Alexa Siu", "Shunan Guo", "Franck Dernoncourt", "James Tompkin", "Jeff Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.", "AI": {"tldr": "This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualization in a human-centric way, enhancing viewer engagement.", "motivation": "To create a more equitable relationship between presenters and visualizations and improve the viewer experience during data presentations.", "method": "A mixed methods approach with comparative analysis conducted with 30 participants across 9 metrics to evaluate InfoVids against traditional 2D slides.", "result": "InfoVids reduced viewer attention splitting, shifted focus from visualization to presenter, and led to more engaging full-body performances.", "conclusion": "InfoVids can redefine presenter-visualization dynamics, leading to a more natural and interactive experiencing of data.", "key_contributions": ["Introduction of InfoVids as a new format for data presentation", "Empirical comparison of InfoVids and traditional 2D slides", "Insights into how layout and interactions influence viewer experience"], "limitations": "The study is based on a limited participant pool and specific presentation contexts.", "keywords": ["InfoVids", "data presentation", "viewer engagement", "HCI", "mixed methods"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.02850", "pdf": "https://arxiv.org/pdf/2505.02850.pdf", "abs": "https://arxiv.org/abs/2505.02850", "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Ananya Thakur", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "comment": null, "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale.", "AI": {"tldr": "This paper introduces a hierarchical concept map-based framework to automate the generation of high-quality multiple-choice questions (MCQs) that address misconceptions in high-school physics.", "motivation": "Generating high-quality MCQs is challenging and often requires extensive expertise and time, making manual creation impractical at scale. Existing automated approaches lack the ability to address cognitive diversity and common misconceptions.", "method": "The framework utilizes a hierarchical concept map to structure knowledge, guiding LLMs in generating MCQs and distractors that focus on common misconceptions. An automated pipeline retrieves relevant sections of the concept map, ensuring contextually relevant question generation, followed by validation to meet quality criteria.", "result": "The proposed method achieved a success rate of 75.20% in meeting quality criteria during expert evaluations, significantly outperforming baseline methods (approximately 37%). Student assessments showed a guess success rate of 28.05% for the new approach compared to 37.10% for baselines.", "conclusion": "The concept map-based framework enables effective assessment across various cognitive levels, quickly identifying conceptual gaps and allowing for targeted educational interventions at scale.", "key_contributions": ["Introduction of a structured knowledge framework for MCQ generation", "Effective incorporation of common misconceptions into distractor design", "Significant improvement in assessment quality over existing methods"], "limitations": "", "keywords": ["Multiple-Choice Questions", "Cognitive Levels", "Common Misconceptions", "Physics Education", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.03185", "pdf": "https://arxiv.org/pdf/2505.03185.pdf", "abs": "https://arxiv.org/abs/2505.03185", "title": "Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health", "authors": ["Jun Fang", "Yanuo Zhou", "Ka I Chan", "Jiajin Li", "Zeyi Sun", "Zhengnan Li", "Zicong Fu", "Hongjing Piao", "Haodong Xu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Ingestive behavior plays a critical role in health, yet many existing\ninterventions remain limited to static guidance or manual self-tracking. With\nthe increasing integration of sensors and perceptual computing, recent systems\nhave begun to support closed-loop interventions that dynamically sense user\nbehavior and provide feedback during or around ingestion episodes. In this\nsurvey, we review 136 studies that leverage sensor-enabled or\ninteraction-mediated approaches to influence eating behavior. We propose a\nbehavioral closed-loop paradigm comprising three core components: target\nbehaviors, sensing modalities, and feedback strategies. A taxonomy of sensing\nand intervention modalities is presented, organized along human- and\nenvironment-based dimensions. Our analysis also examines evaluation methods and\ndesign trends across different modality-behavior pairings. This review reveals\nprevailing patterns and critical gaps, offering design insights for future\nadaptive and context-aware ingestion health interventions.", "AI": {"tldr": "This survey reviews 136 studies on sensor-enabled interventions for eating behavior, proposing a behavioral closed-loop paradigm to enhance health interventions.", "motivation": "To address the limitations of static guidance and manual tracking in health interventions related to ingestive behavior.", "method": "Review and analysis of 136 studies on sensor-enabled and interaction-mediated approaches to influence eating behavior, proposing a taxonomy and evaluating design trends.", "result": "Identification of patterns and gaps in existing studies, providing insights into effective sensing modalities and feedback strategies for creating adaptive ingestion health interventions.", "conclusion": "The review highlights the need for more context-aware interventions and suggests design principles for future systems that leverage closed-loop feedback.", "key_contributions": ["Proposed a behavioral closed-loop paradigm for eating behavior interventions.", "Developed a taxonomy of sensing and intervention modalities.", "Evaluated design trends and gaps in current research on ingestive behavior interventions."], "limitations": "The review primarily focuses on existing studies and may not cover all innovative interventions, particularly outside the sensor-enabled realm.", "keywords": ["ingestive behavior", "health interventions", "sensing modalities", "feedback strategies", "adaptive systems"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851.pdf", "abs": "https://arxiv.org/abs/2505.02851", "title": "30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "8 pages (main content), 4 figures. Submitted to ACL BEA2025", "summary": "In this paper, we present 30 Day Me, a habit formation application that\nleverages Large Language Models (LLMs) to help users break down their goals\ninto manageable, actionable steps and track their progress. Central to the app\nis the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced\nfrom over 15K webpages, and enables runtime search of challenge ideas aligned\nwith user-defined goals. We showcase how LLMs can be harnessed to rapidly\nconstruct domain specific content corpora for behavioral and educational\npurposes, and propose a practical pipeline that incorporates effective LLM\nenhanced approaches for content generation and semantic deduplication.", "AI": {"tldr": "The paper presents 30 Day Me, a habit formation app utilizing LLMs to help users set and track actionable 30-day challenges.", "motivation": "To assist users in forming new habits by breaking down larger goals into smaller, actionable steps using LLM technology.", "method": "The 30DAYGEN system generates unique challenges and allows runtime search for ideas aligned with user goals, sourcing from a vast web corpus.", "result": "The app generates over 3,500 unique challenges and effectively uses LLMs for content creation and semantic deduplication.", "conclusion": "The proposed pipeline showcases how LLMs can enhance habit formation applications by generating tailored content", "key_contributions": ["Introduction of the 30DAYGEN system for generating unique challenges.", "Demonstration of LLM capabilities in content generation for behavioral changes.", "Proposal of a practical pipeline for semantic deduplication in challenge creation."], "limitations": "", "keywords": ["habit formation", "large language models", "content generation", "educational technology", "behavioral change"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.03364", "pdf": "https://arxiv.org/pdf/2505.03364.pdf", "abs": "https://arxiv.org/abs/2505.03364", "title": "DroidRetriever: An Autonomous Navigation and Information Integration System Facilitating Mobile Sensemaking", "authors": ["Yiheng Bian", "Yunpeng Song", "Guiyu Ma", "Rongrong Zhu", "Zhongmin Cai"], "categories": ["cs.HC"], "comment": null, "summary": "Users regularly rely on mobile applications for their daily information\nneeds, and mobile sensemaking is prevalent in various domains such as\neducation, healthcare, business intelligence, and emergency response, where\ntimely and context-aware information-processing and decision-making is\ncritical. However, valuable information is often scattered across the closed\necosystems within various applications, posing challenges for traditional\nsearch engines to retrieve data openly and in real-time. Additionally, due to\nlimitations such as mobile device screen sizes, language differences, and\nunfamiliarity with specific applications and domain knowledge, users have to\nfrequently switch between multiple applications and spend substantial time\nlocating and integrating the information. To address these challenges, we\npresent DroidRetriever, a system for cross-application information retrieval to\nfacilitate mobile sensemaking. DroidRetriever can automatically navigate to\nrelevant interfaces based on users' natural language commands, capture\nscreenshots, extract and integrate information, and finally present the\nresults. Our experimental results demonstrate that DroidRetriever can extract\nand integrate information with near-human accuracy while significantly reducing\nprocessing time. Furthermore, with minimal user intervention, DroidRetriever\neffectively corrects and completes various information retrieval tasks,\nsubstantially reducing the user's workload. Our summary of the motivations for\nintervention and the discussion of their necessity provide valuable\nimplications for future research. We will open-source our code upon acceptance\nof the paper.", "AI": {"tldr": "DroidRetriever is a proposed system for cross-application information retrieval aimed at enhancing mobile sensemaking by automating the access to relevant information across diverse mobile applications.", "motivation": "The prevalence of mobile applications for daily information needs and the challenges posed by scattered data in closed ecosystems necessitate an effective tool for real-time information retrieval and integration.", "method": "DroidRetriever utilizes natural language commands to navigate mobile interfaces, capture screenshots, extract pertinent information, and present integrated results with minimal user intervention.", "result": "DroidRetriever achieves near-human accuracy in information extraction and integration, significantly reducing processing time and user workload in comparison to traditional methods.", "conclusion": "The introduction of DroidRetriever represents a significant advancement in facilitating mobile sensemaking by improving the efficiency of cross-application information retrieval.", "key_contributions": ["Development of DroidRetriever for cross-application retrieval", "Demonstration of near-human accuracy in information integration", "Reduction of user workload through automation of tasks"], "limitations": "", "keywords": ["mobile applications", "information retrieval", "natural language processing", "cross-application integration", "mobile sensemaking"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.02854", "pdf": "https://arxiv.org/pdf/2505.02854.pdf", "abs": "https://arxiv.org/abs/2505.02854", "title": "Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets", "authors": ["Masumi Morishige", "Ryo Koshihara"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Reproducibility and reliability remain pressing challenges for generative AI\nsystems whose behavior can drift with each model update or prompt revision. We\nintroduce GPR-bench, a lightweight, extensible benchmark that operationalizes\nregression testing for general purpose use cases. GPR-bench couples an open,\nbilingual (English and Japanese) dataset covering eight task categories (e.g.,\ntext generation, code generation, and information retrieval) and 10 scenarios\nin each task categories (80 total test cases for each language) with an\nautomated evaluation pipeline that employs \"LLM-as-a-Judge\" scoring of\ncorrectness and conciseness. Experiments across three recent model versions -\ngpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default\nversus concise-writing instruction) reveal heterogeneous quality. Our results\nshow that newer models generally improve correctness, but the differences are\nmodest and not statistically significant, suggesting that GPR-bench may not be\nsufficiently challenging to differentiate between recent model versions. In\ncontrast, the concise-writing instruction significantly enhances conciseness\n(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with\nminimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of\nprompt engineering. Released under the MIT License, GPR- bench lowers the\nbarrier to initiating reproducibility monitoring and provides a foundation for\ncommunity-driven extensions, while also raising important considerations about\nbenchmark design for rapidly evolving language models.", "AI": {"tldr": "GPR-bench is a benchmark for regression testing generative AI systems using bilingual datasets and automated evaluation, addressing reproducibility and reliability challenges.", "motivation": "To tackle the pressing challenge of reproducibility and reliability in generative AI systems as model behavior can change with updates or prompt modifications.", "method": "GPR-bench combines an open dataset in English and Japanese with an automated evaluation system that rates correctness and conciseness using LLM-as-a-Judge.", "result": "Experiments showed that while newer models generally improve correctness, these differences are modest and not statistically significant. Concise writing instructions improved conciseness significantly with minimal impact on accuracy.", "conclusion": "GPR-bench aids reproducibility monitoring and offers a framework for future extensions, while also presenting challenges for benchmark design of evolving language models.", "key_contributions": ["Introduction of a lightweight and extensible benchmark for regression testing in generative AI.", "Bilingual dataset covering multiple task categories and scenarios.", "Insights into the effectiveness of prompt engineering on model performance."], "limitations": "GPR-bench may not sufficiently challenge to differentiate between recent model versions due to modest improvement in correctness.", "keywords": ["Generative AI", "Reproducibility", "Benchmarking", "Prompt Engineering", "Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.03423", "pdf": "https://arxiv.org/pdf/2505.03423.pdf", "abs": "https://arxiv.org/abs/2505.03423", "title": "AI-Based Feedback in Counselling Competence Training of Prospective Teachers", "authors": ["Tobias Hallmen", "Kathrin Gietl", "Karoline Hillesheim", "Moritz Bauermann", "Annemarie Friedrich", "Elisabeth André"], "categories": ["cs.HC"], "comment": null, "summary": "This study explores the use of AI-based feedback to enhance the counselling\ncompetence of prospective teachers. An iterative block seminar was designed,\nincorporating theoretical foundations, practical applications, and AI tools for\nanalysing verbal, paraverbal, and nonverbal communication. The seminar included\nrecorded simulated teacher-parent conversations, followed by AI-based feedback\nand qualitative interviews with students. The study investigated correlations\nbetween communication characteristics and conversation quality, student\nperceptions of AI-based feedback, and the training of AI models to identify\nconversation phases and techniques. Results indicated significant correlations\nbetween nonverbal and paraverbal features and conversation quality, and\nstudents positively perceived the AI feedback. The findings suggest that\nAI-based feedback can provide objective, actionable insights to improve teacher\ntraining programs. Future work will focus on refining verbal skill annotations,\nexpanding the dataset, and exploring additional features to enhance the\nfeedback system.", "AI": {"tldr": "The study examines AI-based feedback's effectiveness in improving counseling skills in teacher training through seminars and simulations.", "motivation": "To enhance the counseling competence of prospective teachers using AI tools for feedback on communication skills.", "method": "An iterative seminar design involving theoretical foundations, practical applications, and AI tools for analyzing communication. It included recorded teacher-parent conversations and qualitative interviews to assess the impact of AI feedback.", "result": "Significant correlations were found between nonverbal and paraverbal communication features and the quality of conversations. Students reported a positive perception of the AI feedback.", "conclusion": "AI-based feedback offers objective insights that can enhance teacher training programs, with plans for further refinement and expansion of the feedback system.", "key_contributions": ["Integration of AI feedback within teacher training.", "Identification of key communication features that correlate with conversation quality.", "Positive student perceptions of AI feedback in teacher training."], "limitations": "Future work is needed to refine verbal skill annotations and expand the dataset.", "keywords": ["AI in education", "teacher training", "communication skills", "feedback systems", "nonverbal communication"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.02858", "pdf": "https://arxiv.org/pdf/2505.02858.pdf", "abs": "https://arxiv.org/abs/2505.02858", "title": "Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models", "authors": ["Henry Tari", "Nojus Sereiva", "Rishabh Kaushal", "Thales Bertaglia", "Adriana Iamnitchi"], "categories": ["cs.CL", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2407.08323", "summary": "Social media datasets are essential for research on a variety of topics, such\nas disinformation, influence operations, hate speech detection, or influencer\nmarketing practices. However, access to social media datasets is often\nconstrained due to costs and platform restrictions. Acquiring datasets that\nspan multiple platforms, which is crucial for understanding the digital\necosystem, is particularly challenging. This paper explores the potential of\nlarge language models to create lexically and semantically relevant social\nmedia datasets across multiple platforms, aiming to match the quality of real\ndata. We propose multi-platform topic-based prompting and employ various\nlanguage models to generate synthetic data from two real datasets, each\nconsisting of posts from three different social media platforms. We assess the\nlexical and semantic properties of the synthetic data and compare them with\nthose of the real data. Our empirical findings show that using large language\nmodels to generate synthetic multi-platform social media data is promising,\ndifferent language models perform differently in terms of fidelity, and a\npost-processing approach might be needed for generating high-fidelity synthetic\ndatasets for research. In addition to the empirical evaluation of three state\nof the art large language models, our contributions include new fidelity\nmetrics specific to multi-platform social media datasets.", "AI": {"tldr": "This paper discusses how large language models can be used to create synthetic social media datasets across multiple platforms, highlighting their potential to match the fidelity of real data.", "motivation": "The paper seeks to address the challenges in accessing diverse social media datasets needed for various research areas due to costs and platform limitations.", "method": "The authors propose a methodology based on multi-platform topic-based prompting and leverage different language models to generate synthetic datasets derived from real social media posts across three platforms.", "result": "The study reveals that synthetic data generated by large language models can maintain lexical and semantic relevance compared to real data, with variability in performance across different models, suggesting the need for post-processing to enhance fidelity.", "conclusion": "Large language models offer a promising approach for generating multi-platform social media datasets, though specific metrics for fidelity need to be developed for better assessments.", "key_contributions": ["Methodology for generating synthetic social media datasets using large language models", "Empirical evaluation of fidelity of synthetic data versus real data", "Introduction of new fidelity metrics for multi-platform datasets"], "limitations": "The need for post-processing for high-fidelity dataset generation and variability in language model performance.", "keywords": ["Large language models", "Synthetic datasets", "Social media research"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.03440", "pdf": "https://arxiv.org/pdf/2505.03440.pdf", "abs": "https://arxiv.org/abs/2505.03440", "title": "manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality", "authors": ["Samuel Pantze", "Jean-Yves Tinevez", "Matthew McGinity", "Ulrik Günther"], "categories": ["cs.HC"], "comment": "7 pages, 6 figures, submitted to IEEE VIS 2025", "summary": "We propose manvr3d, a novel VR-ready platform for interactive\nhuman-in-the-loop cell tracking. We utilize VR controllers and eye-tracking\nhardware to facilitate rapid ground truth generation and proofreading for deep\nlearning-based cell tracking models. Life scientists reconstruct the\ndevelopmental history of organisms on the cellular level by analyzing 3D\ntime-lapse microscopy images acquired at high spatio-temporal resolution. The\nreconstruction of such cell lineage trees traditionally involves tracking\nindividual cells through all recorded time points, manually annotating their\npositions, and then linking them over time to create complete trajectories.\nDeep learning-based algorithms accelerate this process, yet depend heavily on\nmanually-annotated high-quality ground truth data and curation. Visual\nrepresentation of the image data in this process still relies primarily on 2D\nrenderings, which greatly limits spatial understanding and navigation. In this\nwork, we bridge the gap between deep learning-based cell tracking software and\n3D/VR visualization to create a human-in-the-loop cell tracking system. We lift\nthe incremental annotation, training and proofreading loop of the deep learning\nmodel into the 3rd dimension and apply natural user interfaces like hand\ngestures and eye tracking to accelerate the cell tracking workflow for life\nscientists.", "AI": {"tldr": "manvr3d is a VR-ready platform for enhancing human-in-the-loop cell tracking via 3D visualization and interaction.", "motivation": "The need for accurate and efficient cell tracking in life sciences is hindered by the limitations of traditional 2D visualization and manual annotation processes.", "method": "The platform integrates deep learning for cell tracking with VR technologies, allowing users to interactively generate ground truth and proofread annotations using VR controllers and eye tracking.", "result": "manvr3d improves the workflow for cell tracking, enabling faster and more intuitive data annotations, thus enhancing the quality of training data for deep learning algorithms.", "conclusion": "By creating a 3D interactive system, manvr3d significantly speeds up the cell tracking process, making it easier for life scientists to visualize and analyze complex cellular data.", "key_contributions": ["Integration of VR technology into cell tracking workflows", "Human-in-the-loop approach enhances data annotation", "Utilization of natural user interfaces for interaction"], "limitations": "", "keywords": ["VR", "cell tracking", "deep learning", "human-in-the-loop", "3D visualization"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.02859", "pdf": "https://arxiv.org/pdf/2505.02859.pdf", "abs": "https://arxiv.org/abs/2505.02859", "title": "Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI", "authors": ["Jonas Bokstaller", "Julia Altheimer", "Julian Dormehl", "Alina Buss", "Jasper Wiltfang", "Johannes Schneider", "Maximilian Röglinger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across various sectors applications of eXplainableAI (XAI) gained momentum as\nthe increasing black-boxedness of prevailing Machine Learning (ML) models\nbecame apparent. In parallel, Large Language Models (LLMs) significantly\ndeveloped in their abilities to understand human language and complex patterns.\nBy combining both, this paper presents a novel reference architecture for the\ninterpretation of XAI through an interactive chatbot powered by a fine-tuned\nLLM. We instantiate the reference architecture in the context of\nState-of-Health (SoH) prediction for batteries and validate its design in\nmultiple evaluation and demonstration rounds. The evaluation indicates that the\nimplemented prototype enhances the human interpretability of ML, especially for\nusers with less experience with XAI.", "AI": {"tldr": "This paper presents a novel architecture combining Explainable AI (XAI) and Large Language Models (LLMs) via an interactive chatbot to improve human interpretability of machine learning models, specifically in battery State-of-Health (SoH) prediction.", "motivation": "To address the challenges of black-box ML models and improve interpretability for users with less experience with XAI.", "method": "A reference architecture is designed and instantiated using a fine-tuned LLM, implemented as an interactive chatbot for interpreting XAI in battery SoH prediction.", "result": "The evaluation shows that the prototype significantly enhances human interpretability of ML, especially for users not well-versed in XAI.", "conclusion": "The proposed architecture effectively bridges the gap between complex ML models and user understanding, particularly for non-expert users.", "key_contributions": ["Development of a novel reference architecture for XAI interpretation using LLMs", "Implementation of an interactive chatbot to facilitate user comprehension", "Validation of the approach in the context of battery health prediction"], "limitations": "", "keywords": ["Explainable AI", "Large Language Models", "State-of-Health prediction", "Machine Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03492", "pdf": "https://arxiv.org/pdf/2505.03492.pdf", "abs": "https://arxiv.org/abs/2505.03492", "title": "Augmenting Human Cognition through Everyday AR", "authors": ["Xiaoan Liu"], "categories": ["cs.HC", "cs.AI"], "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'", "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.", "AI": {"tldr": "The paper discusses the evolution of AR into a cognitive tool that integrates AI to foster better human interactions with the environment.", "motivation": "To highlight the potential of always-on AR systems in enhancing cognitive capabilities and task performance through context-aware intelligence.", "method": "The paper presents a conceptual exploration of integrating multimodal LLMs with AR technologies to create immersive and intuitive user experiences.", "result": "The paper argues that a seamless integration of digital and physical elements through AR can significantly improve interaction quality and task efficiency.", "conclusion": "Always-on AR can transform how humans engage with digital information within their physical environments, suggesting a need for more research in this area.", "key_contributions": ["Integration of multimodal LLMs with AR for improved cognitive performance.", "Exploration of proactive context-sensitive interactions in AR.", "Proposition of AR as an intuitive tool for everyday tasks."], "limitations": "", "keywords": ["Augmented Reality", "Multimodal LLMs", "Human-Computer Interaction", "Context-aware Computing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862.pdf", "abs": "https://arxiv.org/abs/2505.02862", "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.", "AI": {"tldr": "This paper introduces a novel framework for jailbreak attacks on LLMs, employing cognitive biases to enhance attack effectiveness and propose new evaluation metrics for harmful outputs.", "motivation": "To expose the vulnerabilities of LLMs to jailbreak attacks that can undermine their safety mechanisms.", "method": "A novel framework called ICRT is developed, utilizing cognitive decomposition and relevance bias to craft malicious prompts, along with a new ranking-based harmfulness evaluation metric.", "result": "Experimental results demonstrate that the proposed ICRT framework consistently bypasses the safety mechanisms of mainstream LLMs and generates high-risk outputs.", "conclusion": "The insights gained from the study highlight the risks of jailbreak attacks and contribute to the development of better defense strategies for LLMs.", "key_contributions": ["Introduction of the ICRT framework for jailbreak attacks", "Utilization of cognitive biases for prompt optimization", "Development of a ranking-based harmfulness evaluation metric"], "limitations": "", "keywords": ["Large Language Models", "Jailbreak attacks", "Cognitive biases", "Harmfulness evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.03584", "pdf": "https://arxiv.org/pdf/2505.03584.pdf", "abs": "https://arxiv.org/abs/2505.03584", "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation", "authors": ["Lucas Anastasiou", "Anna De Liddo"], "categories": ["cs.HC", "cs.AI", "cs.CY", "I.2"], "comment": "5 pages, 3 figures", "summary": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.", "AI": {"tldr": "This paper presents BCause, a generative AI-based system that enhances public deliberation by transforming unstructured dialogue into structured democratic processes, promoting actionable outcomes.", "motivation": "To address issues in public deliberation such as scattered discourse and a lack of actionable policy outcomes.", "method": "Introducing BCause, a system that utilizes generative AI to organize public discussions through innovations like transforming transcripts into argumentative formats, geo-deliberation via a Telegram bot, and customizable reporting widgets.", "result": "BCause effectively converts unstructured discussions into structured, actionable insights into public issues, supporting better decision-making processes.", "conclusion": "The proposed system enhances democratic engagement by ensuring human participation and ethical oversight in conjunction with AI capabilities.", "key_contributions": ["Introduction of BCause for structured public deliberation", "Innovative use of AI to transform unstructured dialogue into meaningful discussions", "Development of geo-deliberation tools for localized issue reporting"], "limitations": "", "keywords": ["public deliberation", "generative AI", "human-machine collaboration", "democratic processes", "actionable policy"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865.pdf", "abs": "https://arxiv.org/abs/2505.02865", "title": "Accelerating Large Language Model Reasoning via Speculative Search", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality.", "AI": {"tldr": "A new framework, Speculative Search (SpecSearch), accelerates reasoning in large language models (LLMs) by optimizing thought generation while maintaining quality.", "motivation": "To reduce inference latency in tree-search-based reasoning methods for LLMs, which involves generating numerous intermediate thoughts.", "method": "SpecSearch uses a small model to collaborate with a large model for generating reasoning thoughts, featuring a quality-preserving rejection mechanism to filter out low-quality thoughts.", "result": "SpecSearch achieves up to 2.12x speedup in reasoning speed while preserving comparable reasoning quality to that of large models.", "conclusion": "The SpecSearch framework provides an effective solution for faster LLM reasoning without compromising output quality.", "key_contributions": ["Introduction of the SpecSearch framework for optimizing LLM reasoning", "Implementation of a quality-preserving rejection mechanism", "Demonstrated state-of-the-art speedup and quality preservation"], "limitations": "", "keywords": ["Speculative Search", "LLM reasoning", "thought generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03618", "pdf": "https://arxiv.org/pdf/2505.03618.pdf", "abs": "https://arxiv.org/abs/2505.03618", "title": "Scalable Class-Centric Visual Interactive Labeling", "authors": ["Matthias Matt", "Jana Sedlakova", "Jürgen Bernard", "Matthias Zeppelzauer", "Manuela Waldner"], "categories": ["cs.HC"], "comment": null, "summary": "Large unlabeled datasets demand efficient and scalable data labeling\nsolutions, in particular when the number of instances and classes is large.\nThis leads to significant visual scalability challenges and imposes a high\ncognitive load on the users. Traditional instance-centric labeling methods,\nwhere (single) instances are labeled in each iteration struggle to scale\neffectively in these scenarios. To address these challenges, we introduce cVIL,\na Class-Centric Visual Interactive Labeling methodology designed for\ninteractive visual data labeling. By shifting the paradigm from\nassigning-classes-to-instances to assigning-instances-to-classes, cVIL reduces\nlabeling effort and enhances efficiency for annotators working with large,\ncomplex and class-rich datasets. We propose a novel visual analytics labeling\ninterface built on top of the conceptual cVIL workflow, enabling improved\nscalability over traditional visual labeling. In a user study, we demonstrate\nthat cVIL can improve labeling efficiency and user satisfaction over\ninstance-centric interfaces. The effectiveness of cVIL is further demonstrated\nthrough a usage scenario, showcasing its potential to alleviate cognitive load\nand support experts in managing extensive labeling tasks efficiently.", "AI": {"tldr": "Introduction of cVIL, a Class-Centric Visual Interactive Labeling methodology aimed at improving labeling efficiency in large datasets.", "motivation": "Large unlabeled datasets necessitate scalable data labeling solutions, particularly when facing significant visual scalability challenges and user cognitive load.", "method": "cVIL shifts the labeling focus from assigning classes to instances to assigning instances to classes, enhancing user interaction and efficiency through a novel visual analytics labeling interface.", "result": "User studies show that cVIL improves labeling efficiency and user satisfaction compared to traditional instance-centric interfaces.", "conclusion": "cVIL alleviates cognitive load and supports efficient management of extensive labeling tasks, demonstrating significant advantages over existing methods.", "key_contributions": ["Introduction of cVIL methodology for visual data labeling.", "Development of a novel visual analytics interface that enhances scalability.", "Evidence from user study supporting improved efficiency and satisfaction."], "limitations": "", "keywords": ["data labeling", "visual analytics", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.02872", "pdf": "https://arxiv.org/pdf/2505.02872.pdf", "abs": "https://arxiv.org/abs/2505.02872", "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "authors": ["Cfir Avraham Hadar", "Omer Shubi", "Yoav Meiri", "Yevgeni Berzak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.", "AI": {"tldr": "The paper explores whether open-ended reading goals can be decoded from eye movements using LLMs, introducing new tasks and evaluation frameworks for goal classification and reconstruction.", "motivation": "To investigate how readers' text-specific goals influence reading behavior and whether these goals can be inferred from eye movement data.", "method": "The authors introduced goal classification and goal reconstruction tasks, employing large-scale eye tracking data and several multimodal LLMs to analyze eye movements in conjunction with text.", "result": "Experiments demonstrated significant success in both goal classification and goal reconstruction, indicating that LLMs can effectively decode reading goals from eye movements.", "conclusion": "The findings suggest that LLMs are capable of extracting meaningful information about a reader's intentions from their eye movement patterns, enhancing our understanding of reading behaviors.", "key_contributions": ["Introduction of goal classification and goal reconstruction tasks for reading research", "Development of new evaluation frameworks for these tasks", "Demonstration of LLMs' capability to decode text-specific reading goals from eye movements."], "limitations": "", "keywords": ["eye movements", "reading goals", "LLMs", "goal classification", "multimodal"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.03682", "pdf": "https://arxiv.org/pdf/2505.03682.pdf", "abs": "https://arxiv.org/abs/2505.03682", "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays", "authors": ["Christopher Flöter", "Sergej Geringer", "Guido Reina", "Daniel Weiskopf", "Timo Ropinski"], "categories": ["cs.HC", "cs.GR", "I.3.6; I.3.7"], "comment": "Temporal resolution reduction, frame rate reduction, foveated\n  rendering, virtual reality", "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery\nof the users' view. However, using foveated rendering to reduce temporal\nresolution, i.e., rendering frame rate, seems less explored. In this work, we\npresent the results of a user study investigating the perceptual effects of\nfoveated temporal resolution reduction, where only the temporal resolution\n(frame rate) is reduced in the periphery without affecting spatial quality\n(pixel density). In particular, we investigated the perception of temporal\nresolution artifacts caused by reducing the frame rate dependent on the\neccentricity of the user's gaze. Our user study with 15 participants was\nconducted in a virtual reality setting using a head-mounted display. Our\nresults indicate that it was possible to reduce average rendering costs, i.e.,\nthe number of rendered pixels, to a large degree before participants\nconsistently reported perceiving temporal artifacts.", "AI": {"tldr": "This study explores reducing frame rates in peripheral vision using foveated rendering in virtual reality and assesses the perception of temporal artifacts.", "motivation": "To investigate the perceptual effects of reducing temporal resolution (frame rates) in foveated rendering, an area less explored compared to spatial resolution reduction.", "method": "A user study was conducted with 15 participants in a virtual reality setting, utilizing a head-mounted display to evaluate perception of temporal artifacts based on gaze eccentricity and frame rate reductions.", "result": "The study found that significant reductions in rendering costs were achievable before temporal artifacts were consistently perceived by participants.", "conclusion": "Foveated temporal resolution reduction can successfully lower rendering demands while maintaining spatial quality without noticeable artifacts, suggesting its potential in optimizing virtual reality applications.", "key_contributions": ["First exploration of foveated temporal resolution in VR", "Demonstrated user tolerance to reduced frame rates in periphery", "Provided insights for optimizing rendering costs in virtual environments"], "limitations": "Only 15 participants; results may not generalize to all user groups or VR contexts.", "keywords": ["temporal resolution reduction", "frame rate reduction", "foveated rendering", "virtual reality", "perception"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.02983", "pdf": "https://arxiv.org/pdf/2505.02983.pdf", "abs": "https://arxiv.org/abs/2505.02983", "title": "Logits-Constrained Framework with RoBERTa for Ancient Chinese NER", "authors": ["Wenjie Hua", "Shenghan Xu"], "categories": ["cs.CL", "68T50", "I.2.7; I.5.1; I.5.4"], "comment": "5 pages, 2 figures, 6 tables. Accepted to EvaHan 2025 shared task on\n  Ancient Chinese NLP", "summary": "This paper presents a Logits-Constrained (LC) framework for Ancient Chinese\nNamed Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our\ntwo-stage model integrates GujiRoBERTa for contextual encoding and a\ndifferentiable decoding mechanism to enforce valid BMES label transitions.\nExperiments demonstrate that LC improves performance over traditional CRF and\nBiLSTM-based approaches, especially in high-label or large-data settings. We\nalso propose a model selection criterion balancing label complexity and dataset\nsize, providing practical guidance for real-world Ancient Chinese NLP tasks.", "AI": {"tldr": "A Logits-Constrained framework for Ancient Chinese Named Entity Recognition using GujiRoBERTa shows improved performance over traditional methods.", "motivation": "To enhance Named Entity Recognition for Ancient Chinese by integrating modern techniques and improving upon existing methods.", "method": "A two-stage model utilizing GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions.", "result": "The LC framework outperforms traditional CRF and BiLSTM approaches, particularly in scenarios with high label complexity and large datasets.", "conclusion": "The proposed model selection criterion aids in balancing between label complexity and dataset size, offering practical solutions for Ancient Chinese NLP tasks.", "key_contributions": ["Introduction of Logits-Constrained framework for NER", "Integration of GujiRoBERTa into the NER process", "Development of a model selection criterion for Ancient Chinese NLP tasks."], "limitations": "", "keywords": ["Named Entity Recognition", "GujiRoBERTa", "Ancient Chinese NLP"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005.pdf", "abs": "https://arxiv.org/abs/2505.03005", "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper", "AI": {"tldr": "RADLADS converts softmax attention transformers into efficient linear attention models with low training requirements, achieving competitive performance.", "motivation": "To address the inefficiencies of softmax attention transformers while maintaining high performance in downstream tasks.", "method": "Introduced RADLADS, a protocol for converting transformers to linear attention decoders, requiring significantly fewer tokens for training compared to standard methods.", "result": "Achieved state-of-the-art performance for linear attention models at various sizes with significantly reduced training costs and minimal token requirement.", "conclusion": "The RADLADS method demonstrates an efficient pathway for transitioning from traditional transformers to linear attention models without substantial loss in performance.", "key_contributions": ["Introduction of RADLADS protocol", "Development of new RWKV-variant architectures", "Models released on HuggingFace under open licenses"], "limitations": "", "keywords": ["linear attention", "transformers", "RWKV", "HuggingFace"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.03019", "pdf": "https://arxiv.org/pdf/2505.03019.pdf", "abs": "https://arxiv.org/abs/2505.03019", "title": "Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis", "authors": ["Albérick Euraste Djiré", "Abdoul Kader Kaboré", "Earl T. Barr", "Jacques Klein", "Tegawendé F. Bissyandé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) achieve remarkable performance through\ntraining on massive datasets, they can exhibit concerning behaviors such as\nverbatim reproduction of training data rather than true generalization. This\nmemorization phenomenon raises significant concerns about data privacy,\nintellectual property rights, and the reliability of model evaluations. This\npaper introduces PEARL, a novel approach for detecting memorization in LLMs.\nPEARL assesses how sensitive an LLM's performance is to input perturbations,\nenabling memorization detection without requiring access to the model's\ninternals. We investigate how input perturbations affect the consistency of\noutputs, enabling us to distinguish between true generalization and\nmemorization. Our findings, following extensive experiments on the Pythia open\nmodel, provide a robust framework for identifying when the model simply\nregurgitates learned information. Applied on the GPT 4o models, the PEARL\nframework not only identified cases of memorization of classic texts from the\nBible or common code from HumanEval but also demonstrated that it can provide\nsupporting evidence that some data, such as from the New York Times news\narticles, were likely part of the training data of a given model.", "AI": {"tldr": "PEARL is a new framework for detecting memorization in large language models (LLMs) through input perturbations, addressing privacy and generalization concerns.", "motivation": "The paper aims to tackle the memorization phenomenon in LLMs, which can lead to issues related to data privacy and the reliability of model evaluations.", "method": "PEARL assesses sensitivity of LLM performance to input perturbations to detect memorization without needing access to model internals.", "result": "Experiments on the Pythia model and GPT 4o demonstrated that PEARL can identify instances of memorization across various texts and provide evidence of training data inclusion.", "conclusion": "The PEARL framework facilitates the detection of memorization, enhancing understanding of LLM behavior and aiding in concerns regarding data usage and privacy.", "key_contributions": ["Introduction of PEARL for memorization detection", "Framework operates without model internals access", "Identification of memorization instances in various texts"], "limitations": "", "keywords": ["Large Language Models", "Memorization Detection", "PEARL", "Input Perturbations", "Data Privacy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.03025", "pdf": "https://arxiv.org/pdf/2505.03025.pdf", "abs": "https://arxiv.org/abs/2505.03025", "title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "authors": ["Steven Bedrick", "A. Seza Doğruöz", "Sergiu Nisioi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data sets are used across linguistic domains and NLP tasks,\nparticularly in scenarios where authentic data is limited (or even\nnon-existent). One such domain is that of clinical (healthcare) contexts, where\nthere exist significant and long-standing challenges (e.g., privacy,\nanonymization, and data governance) which have led to the development of an\nincreasing number of synthetic datasets. One increasingly important category of\nclinical dataset is that of clinical dialogues which are especially sensitive\nand difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some\nsituations, little theory exists to inform how they may be best used and\ngeneralized to new applications. In this paper, we provide an overview of how\nsynthetic datasets are created, evaluated and being used for dialogue related\ntasks in the medical domain. Additionally, we propose a novel typology for use\nin classifying types and degrees of data synthesis, to facilitate comparison\nand evaluation.", "AI": {"tldr": "Overview of synthetic data sets in clinical dialogues, their creation, evaluation, and a proposed typology for effective usage.", "motivation": "To address challenges related to the limited availability of authentic clinical dialogue data due to privacy and data governance issues.", "method": "The paper reviews existing methods for creating and evaluating synthetic datasets in medical dialogues and introduces a typology for classifying data synthesis types.", "result": "Synthetic datasets have shown sufficient performance in some dialogue tasks but lack theoretical guidance for optimal use across various applications.", "conclusion": "The proposed typology aims to enhance the understanding and evaluation of synthetic datasets in clinical contexts, promoting better application in real-world medical dialogue tasks.", "key_contributions": ["Comprehensive overview of synthetic datasets in clinical dialogue", "Introduction of a novel typology for data synthesis", "Discussion on the evaluation methods for synthetic datasets in healthcare"], "limitations": "The theoretical framework for optimizing the usage of synthetic datasets remains underdeveloped.", "keywords": ["synthetic data", "clinical dialogues", "data synthesis", "NLP", "healthcare"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.03030", "pdf": "https://arxiv.org/pdf/2505.03030.pdf", "abs": "https://arxiv.org/abs/2505.03030", "title": "UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output", "authors": ["Sicong Huang", "Jincheng He", "Shiyuan Huang", "Karthik Raja Anandan", "Arkajyoti Chakraborty", "Ian Lane"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Hallucinations pose a significant challenge for large language models when\nanswering knowledge-intensive queries. As LLMs become more widely adopted, it\nis crucial not only to detect if hallucinations occur but also to pinpoint\nexactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes, is a recent effort in this direction. This paper\ndescribes the UCSC system submission to the shared Mu-SHROOM task. We introduce\na framework that first retrieves relevant context, next identifies false\ncontent from the answer, and finally maps them back to spans in the LLM output.\nThe process is further enhanced by automatically optimizing prompts. Our system\nachieves the highest overall performance, ranking #1 in average position across\nall languages. We release our code and experiment results.", "AI": {"tldr": "The paper presents a framework for detecting and mapping hallucinations in LLM outputs, achieving top performance in the Mu-SHROOM multilingual task.", "motivation": "To effectively detect and locate hallucinations in large language model outputs, as their use grows in knowledge-intensive applications.", "method": "The framework retrieves relevant context, identifies erroneous content in the LLM output, and maps it back to specific spans, with an optimization of prompts to enhance the process.", "result": "The proposed system outperformed all other submissions, ranking #1 in average position across multiple languages during the SemEval 2025 Task 3, Mu-SHROOM.", "conclusion": "The implementation and results of our system demonstrate the effectiveness of our approach for hallucination detection in LLMs, and we provide our code for further research.", "key_contributions": ["Introduction of a framework for hallucination detection in LLM outputs.", "Achieving the highest performance in the Mu-SHROOM task across all languages.", "Release of code and experimental results for community use."], "limitations": "", "keywords": ["hallucinations", "large language models", "Mu-SHROOM", "multilingual", "prompt optimization"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2505.03052", "pdf": "https://arxiv.org/pdf/2505.03052.pdf", "abs": "https://arxiv.org/abs/2505.03052", "title": "Teaching Models to Understand (but not Generate) High-risk Data", "authors": ["Ryan Wang", "Matthew Finlayson", "Luca Soldaini", "Swabha Swayamdipta", "Robin Jia"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model developers typically filter out high-risk content -- such as\ntoxic or copyrighted text -- from their pre-training data to prevent models\nfrom generating similar outputs. However, removing such data altogether limits\nmodels' ability to recognize and appropriately respond to harmful or sensitive\ncontent. In this paper, we introduce Selective Loss to Understand but Not\nGenerate (SLUNG), a pre-training paradigm through which models learn to\nunderstand high-risk data without learning to generate it. Instead of uniformly\napplying the next-token prediction loss, SLUNG selectively avoids incentivizing\nthe generation of high-risk tokens while ensuring they remain within the\nmodel's context window. As the model learns to predict low-risk tokens that\nfollow high-risk ones, it is forced to understand the high-risk content.\nThrough our experiments, we show that SLUNG consistently improves models'\nunderstanding of high-risk data (e.g., ability to recognize toxic content)\nwithout increasing its generation (e.g., toxicity of model responses). Overall,\nour SLUNG paradigm enables models to benefit from high-risk text that would\notherwise be filtered out.", "AI": {"tldr": "The SLUNG paradigm allows language models to understand high-risk content without generating it, improving recognition of such content.", "motivation": "To enhance models' ability to recognize harmful or sensitive content without generating undesirable outputs by filtering out high-risk data from pre-training.", "method": "Introduces Selective Loss to Understand but Not Generate (SLUNG), which selectively avoids incentivizing the generation of high-risk tokens while retaining them in the context for recognition tasks during pre-training.", "result": "SLUNG improves models' understanding of high-risk data significantly (e.g., toxicity recognition) without increasing the generation of harmful responses.", "conclusion": "The SLUNG framework allows language models to benefit from high-risk text that would otherwise be discarded, enhancing their contextual understanding without compromising output safety.", "key_contributions": ["Introduces a novel pre-training paradigm (SLUNG) for language models", "Demonstrates improved understanding of high-risk data without harmful generation", "Provides experimental validation of the effectiveness of SLUNG in HCI applications."], "limitations": "", "keywords": ["language models", "high-risk content", "selective loss", "toxicity recognition", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.03053", "pdf": "https://arxiv.org/pdf/2505.03053.pdf", "abs": "https://arxiv.org/abs/2505.03053", "title": "Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text", "authors": ["Jennifer Healey", "Laurie Byrum", "Md Nadeem Akhtar", "Surabhi Bhargava", "Moumita Sinha"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, no figures, presented at CHI 2025 workshop for Human\n  Evaluation and Auditing of Language Models", "summary": "LLM evaluation is challenging even the case of base models. In real world\ndeployments, evaluation is further complicated by the interplay of task\nspecific prompts and experiential context. At scale, bias evaluation is often\nbased on short context, fixed choice benchmarks that can be rapidly evaluated,\nhowever, these can lose validity when the LLMs' deployed context differs. Large\nscale human evaluation is often seen as too intractable and costly. Here we\npresent our journey towards developing a semi-automated bias evaluation\nframework for free text responses that has human insights at its core. We\ndiscuss how we developed an operational definition of bias that helped us\nautomate our pipeline and a methodology for classifying bias beyond multiple\nchoice. We additionally comment on how human evaluation helped us uncover\nproblematic templates in a bias benchmark.", "AI": {"tldr": "This paper presents a semi-automated framework for evaluating bias in LLM free text responses, incorporating human insights and a refined methodology for bias classification.", "motivation": "To address the challenges of evaluating LLM bias in real-world deployments where context and prompts can significantly vary.", "method": "The authors developed an operational definition of bias to automate their evaluation pipeline and established a methodology that extends bias classification beyond fixed-choice benchmarks.", "result": "The framework highlighted problematic templates in existing bias benchmarks, enhancing the evaluation process of language models beyond limited conditions.", "conclusion": "Human insights proved essential in the development of a new methodology for bias classification, demonstrating the importance of a nuanced approach in LLM evaluation.", "key_contributions": ["Development of a semi-automated bias evaluation framework for LLMs", "Operational definition of bias for automating evaluation", "Identification of problematic templates in bias benchmarks through human insights"], "limitations": "", "keywords": ["LLM evaluation", "Bias evaluation", "Human evaluation", "Bias classification", "Language models"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2505.03059", "pdf": "https://arxiv.org/pdf/2505.03059.pdf", "abs": "https://arxiv.org/abs/2505.03059", "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS", "authors": ["Junlin Wang", "Roy Xie", "Shang Zhu", "Jue Wang", "Ben Athiwaratkun", "Bhuwan Dhingra", "Shuaiwen Leon Song", "Ce Zhang", "James Zou"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Building helpful and harmless large language models (LLMs) requires effective\nmodel alignment approach based on human instructions and feedback, which\nnecessitates high-quality human-labeled data. Constructing such datasets is\noften expensive and hard to scale, and may face potential limitations on\ndiversity and generalization. To address these challenges, we introduce Mixture\nof Agents Alignment (MoAA), that leverages the collective strengths of various\nlanguage models to provide high-quality data for model alignment. By employing\nMoAA, we enhance both supervised fine-tuning and preference optimization,\nleading to improved performance compared to using a single model alone to\ngenerate alignment data (e.g. using GPT-4o alone). Evaluation results show that\nour approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on\nArena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising\ndirection for model alignment through this new scalable and diverse synthetic\ndata recipe. Furthermore, we demonstrate that MoAA enables a self-improvement\npipeline, where models finetuned on MoA-generated data surpass their own\ninitial capabilities, providing evidence that our approach can push the\nfrontier of open-source LLMs without reliance on stronger external supervision.\nData and code will be released.", "AI": {"tldr": "Introduction of Mixture of Agents Alignment (MoAA) to generate high-quality datasets for LLM model alignment using multiple language models.", "motivation": "Building effective large language models (LLMs) requires high-quality human-labeled data, which is expensive and difficult to scale due to limitations on diversity and generalization.", "method": "Mixture of Agents Alignment (MoAA) leverages the strengths of various language models to generate high-quality alignment data, enhancing both supervised fine-tuning and preference optimization.", "result": "MoAA significantly improved the win rate of LLaMA-3.1-8B-Instruct on evaluation tasks, suggesting improved performance in alignment tasks compared to single-model approaches.", "conclusion": "MoAA facilitates a self-improvement pipeline for language models and lays a foundation for scalable and diverse synthetic data generation for model alignment.", "key_contributions": ["Introduction of MoAA for improved dataset generation.", "Demonstrated enhancement of LLAma model performance.", "Evidence of models exceeding initial capabilities through self-improvement."], "limitations": "", "keywords": ["Large Language Models", "Model Alignment", "Mixture of Agents", "Data Generation", "Synthetic Datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.03229", "pdf": "https://arxiv.org/pdf/2505.03229.pdf", "abs": "https://arxiv.org/abs/2505.03229", "title": "Survey of Abstract Meaning Representation: Then, Now, Future", "authors": ["Behrooz Mansouri"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a survey of Abstract Meaning Representation (AMR), a\nsemantic representation framework that captures the meaning of sentences\nthrough a graph-based structure. AMR represents sentences as rooted, directed\nacyclic graphs, where nodes correspond to concepts and edges denote\nrelationships, effectively encoding the meaning of complex sentences. This\nsurvey investigates AMR and its extensions, focusing on AMR capabilities. It\nthen explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by\nshowing traditional, current, and possible futures approaches. It also reviews\nvarious applications of AMR including text generation, text classification, and\ninformation extraction and information seeking. By analyzing recent\ndevelopments and challenges in the field, this survey provides insights into\nfuture directions for research and the potential impact of AMR on enhancing\nmachine understanding of human language.", "AI": {"tldr": "This survey discusses Abstract Meaning Representation (AMR), a graph-based semantic representation framework capturing sentence meanings and its implications on various NLP tasks.", "motivation": "To analyze and summarize the capabilities, parsing, and generation tasks of Abstract Meaning Representation (AMR) and its applications.", "method": "The paper presents a survey of AMR, focusing on the graph-based structure of sentences and reviewing traditional and current approaches to text-to-AMR parsing and AMR-to-text generation.", "result": "The survey showcases various applications of AMR in text generation, classification, and information extraction, while also identifying recent developments and challenges in the field.", "conclusion": "Insights provided may help shape future research directions and enhance machine understanding of human language through AMR.", "key_contributions": ["Comprehensive review of AMR framework and its capabilities", "Exploration of AMR applications in NLP tasks", "Identification of challenges and future research directions in AMR."], "limitations": "", "keywords": ["Abstract Meaning Representation", "graph-based structure", "NLP applications", "semantic representation", "information extraction"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.", "AI": {"tldr": "This study presents MedArabiQ, a benchmark dataset for evaluating Large Language Models in Arabic medical tasks, addressing data scarcity for effective healthcare applications.", "motivation": "Despite the promise of Large Language Models in healthcare, their effectiveness in the Arabic medical domain is uncharted due to insufficient high-quality datasets.", "method": "The dataset was created from past medical exams and public datasets, encompassing various tasks like multiple choice questions and patient-doctor Q&A. Several LLMs were evaluated including GPT-4o and Claude 3.5-Sonnet, with modifications for bias assessment.", "result": "Evaluation of five advanced LLMs showed the necessity for high-quality multilingual benchmarks to support LLM deployment in healthcare.", "conclusion": "The creation and release of MedArabiQ provides a crucial foundation for future research on improving the multilingual capabilities of LLMs for fair AI usage in healthcare.", "key_contributions": ["Introduction of MedArabiQ, the first Arabic medical benchmark dataset", "Comprehensive evaluation of multiple state-of-the-art LLMs in the Arabic medical context", "Insights into the need for multilingual benchmarks in healthcare AI applications"], "limitations": "The study is limited by the current implementations of LLMs and the inherent challenges in dataset creation for specific tasks.", "keywords": ["Large Language Models", "healthcare", "Arabic medical domain", "benchmark", "MedArabiQ"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.03293", "pdf": "https://arxiv.org/pdf/2505.03293.pdf", "abs": "https://arxiv.org/abs/2505.03293", "title": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback", "authors": ["Shijing Zhu", "Zhuang Chen", "Guanqun Bi", "Binghang Li", "Yaxi Deng", "Dazhen Wan", "Libiao Peng", "Xiyao Xiao", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "FangFang Li", "Minlie Huang"], "categories": ["cs.CL"], "comment": "in progress", "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.", "AI": {"tldr": "The paper introduces {\u0001}Arena, an interactive framework for assessing and optimizing LLM-based counselors in mental health, addressing limitations in existing evaluation methods.", "motivation": "To enhance the efficacy and safety of LLMs in providing mental health support through effective evaluation mechanisms.", "method": "The proposed framework includes realistic dialogues with psychologically profiled NPC clients, employs a tripartite evaluation perspective, and applies a closed-loop optimization method.", "result": "Experiments reveal substantial performance differences among LLMs and reflection-based optimizations yield up to a 141% improvement in counseling performance.", "conclusion": "{\u0001}Arena aims to serve as a foundational resource for improving LLM applications in mental healthcare, promoting reliability and human alignment.", "key_contributions": ["Introduction of realistic arena interactions for LLM evaluation", "Tripartite evaluation integrating various perspectives", "Closed-loop optimization for iterative improvement of LLM counselors"], "limitations": "", "keywords": ["Large language models", "mental health", "counseling", "evaluation", "optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320.pdf", "abs": "https://arxiv.org/abs/2505.03320", "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes.", "AI": {"tldr": "The paper presents a method called Recall with Reasoning (RwR) to enhance Mamba's ability to handle long-context memory by utilizing chain-of-thought prompts during fine-tuning.", "motivation": "To address Mamba's limitations in processing sequences that exceed training lengths effectively.", "method": "Recall with Reasoning (RwR) involves distilling chain-of-thought summarization from a teacher model and using these as prompts during fine-tuning.", "result": "RwR significantly improves Mamba's long-context performance on LONGMEMEVAL and HELMET datasets, outperforming Transformer/hybrid baselines while maintaining short-context efficiency.", "conclusion": "RwR provides a straightforward solution to enhance the long-context capabilities of Mamba without requiring changes to its architecture.", "key_contributions": ["Introduction of Recall with Reasoning (RwR) for long-context memory enhancement", "Demonstration of performance improvements on specific benchmarks", "Preservation of short-context efficacy alongside long-context improvements"], "limitations": "", "keywords": ["Mamba", "long-context memory", "chain-of-thought", "fine-tuning", "Recall with Reasoning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2409.10913", "pdf": "https://arxiv.org/pdf/2409.10913.pdf", "abs": "https://arxiv.org/abs/2409.10913", "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers", "authors": ["Pragnya Ramjee", "Mehak Chhokar", "Bhuvan Sachdeva", "Mahendra Meena", "Hamid Abdullah", "Aditya Vashistha", "Ruchit Nagar", "Mohit Jain"], "categories": ["cs.HC"], "comment": null, "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.", "AI": {"tldr": "ASHABot, a WhatsApp-based chatbot powered by LLMs, was designed to assist community health workers (CHWs) in India by addressing their information needs and offering a private channel for inquiries.", "motivation": "To enhance the capacity of community health workers in India who face challenges with limited training and medical knowledge.", "method": "Design and deployment of ASHABot, followed by interviews with CHWs and their supervisors and log analysis to evaluate engagement and information needs.", "result": "ASHABot enabled CHWs to ask sensitive questions confidentially, built trust in the information provided, and supported knowledge sharing between CHWs and their supervisors.", "conclusion": "Positioning LLMs as complementary resources can provide essential support in community healthcare without replacing human supervisors.", "key_contributions": ["Development and implementation of ASHABot for CHWs", "Demonstration of trust in LLMs as authoritative resources", "Identification of engagement factors and knowledge sharing dynamics"], "limitations": "Concerns about increased workload and accountability for supervisors.", "keywords": ["Community health workers", "Health informatics", "Chatbot", "LLM", "Supervision"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03406", "pdf": "https://arxiv.org/pdf/2505.03406.pdf", "abs": "https://arxiv.org/abs/2505.03406", "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation", "authors": ["Mohammad Shoaib Ansari", "Mohd Sohail Ali Khan", "Shubham Revankar", "Aditya Varma", "Anil S. Mokhade"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.", "AI": {"tldr": "This paper explores the use of Large Language Models (LLMs) for enhancing medical decision support via Retrieval-Augmented Generation (RAG) and QLoRA, focusing on scalability and practical challenges in healthcare applications.", "motivation": "To improve medical decision support and enhance response accuracy using LLMs integrated with hospital-specific data and fine-tuned for efficiency.", "method": "The study develops a system based on Llama 3.2-3B-Instruct, employing RAG to retrieve healthcare information and optimize memory and parameters through QLoRA.", "result": "The model shows improved performance on medical benchmarks, able to provide medical suggestions and better disease predictions from patients' data.", "conclusion": "While promising, the integration of LLMs in healthcare raises ethical considerations, requires clinical validation, and faces practical deployment challenges, indicating a need for future research in this domain.", "key_contributions": ["Development of a robust LLM-based medical decision support system", "Implementation of QLoRA for parameter and memory efficiency", "Discussion of ethical and practical implications for healthcare integration"], "limitations": "Integration challenges into current healthcare systems, need for clinical validation, and considerations for patient privacy and data security.", "keywords": ["Large Language Models", "Healthcare", "Retrieval-Augmented Generation", "Quantized Low-Rank Adaptation", "Medical Decision Support"], "importance_score": 10, "read_time_minutes": 12}}
{"id": "2410.00196", "pdf": "https://arxiv.org/pdf/2410.00196.pdf", "abs": "https://arxiv.org/abs/2410.00196", "title": "Motion Design Principles for Accessible Video-based Learning: Addressing Cognitive Challenges for Deaf and Hard of Hearing Learners", "authors": ["Si Cheng", "Haocong Cheng", "Suzy Su", "Lu Ming", "Sarah Masud", "Qi Wang", "Yun Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Deaf and Hard-of-Hearing (DHH) learners face unique challenges in video-based\nlearning due to the complex interplay between visual and auditory information\nin videos. Traditional approaches to making video content accessible primarily\nfocus on captioning, but these solutions often neglect the cognitive demands of\nprocessing both visual and textual information simultaneously. This paper\nintroduces a set of \\textit{Motion} design guidelines, aimed at mitigating\nthese cognitive challenges and improving video learning experiences for DHH\nlearners. Through a two-phase research, we identified five key challenges,\nincluding misaligned content and visual overload. We proposed five design\nprinciples accordingly. User study with 16 DHH participants showed that\nimproving visual-audio relevance and guiding visual attention significantly\nenhances the learning experience by reducing physical demand, alleviating\ntemporal pressure, and improving learning satisfaction. Our findings highlight\nthe potential of Motion design to transform educational content for DHH\nlearners, and we discuss implications for inclusive video learning tools.", "AI": {"tldr": "This paper presents Motion design guidelines to enhance video learning for Deaf and Hard-of-Hearing (DHH) learners by addressing cognitive challenges in processing visual and auditory information.", "motivation": "To improve video-based learning experiences for Deaf and Hard-of-Hearing learners by addressing the cognitive overload that arises from traditional captioning methods.", "method": "The paper identifies challenges through two phases of research and proposes Motion design principles based on user studies with DHH participants.", "result": "User studies revealed that improved visual-audio relevance and guidance in visual attention significantly enhance learning satisfaction and reduce cognitive load for DHH learners.", "conclusion": "Motion design has the potential to significantly improve educational content accessibility for DHH learners, suggesting a new direction for inclusive video learning tools.", "key_contributions": ["Introduction of Motion design guidelines for DHH learners", "Identification of key cognitive challenges in video learning", "Demonstration of improved learning outcomes through user studies"], "limitations": "Limited sample size in user study with only 16 participants.", "keywords": ["Deaf and Hard-of-Hearing", "video learning", "Motion design", "cognitive challenges", "inclusive education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.", "AI": {"tldr": "This paper introduces MedArabiQ, a benchmark dataset for evaluating LLMs in the Arabic medical domain, addressing the lack of high-quality datasets.", "motivation": "To explore the efficacy of LLMs in the Arabic medical domain, which has been overlooked due to the absence of suitable datasets.", "method": "The dataset was constructed from medical exams and public datasets, covering seven Arabic medical tasks, and evaluated using five LLMs including GPT-4o and Claude 3.5-Sonnet.", "result": "Evaluation revealed the necessity for high-quality multilingual benchmarks to ensure fair deployment of LLMs in healthcare applications.", "conclusion": "The creation of MedArabiQ provides a foundation for future research to improve LLM capabilities in the healthcare sector.", "key_contributions": ["Introduction of MedArabiQ dataset for Arabic medical tasks", "Evaluation of bias mitigation techniques in LLMs", "Highlighting the need for multilingual benchmarks in healthcare applications"], "limitations": "", "keywords": ["Large Language Models", "Healthcare", "Arabic Medical Domain", "Benchmark Dataset", "Multilingual Capabilities"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2410.03032", "pdf": "https://arxiv.org/pdf/2410.03032.pdf", "abs": "https://arxiv.org/abs/2410.03032", "title": "CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing", "authors": ["Xiaohan Ding", "Kaike Ping", "Uma Sushmitha Gunturi", "Buse Carik", "Sophia Stil", "Lance T Wilhelm", "Taufiq Daryanto", "James Hawdon", "Sang Won Lee", "Eugenia H Rho"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Online hate speech has become increasingly prevalent on social media\nplatforms, causing harm to individuals and society. While efforts have been\nmade to combat this issue through content moderation, the potential of\nuser-driven counterspeech as an alternative solution remains underexplored.\nExisting counterspeech methods often face challenges such as fear of\nretaliation and skill-related barriers. To address these challenges, we\nintroduce CounterQuill, an AI-mediated system that assists users in composing\neffective and empathetic counterspeech. CounterQuill provides a three-step\nprocess: (1) a learning session to help users understand hate speech and\ncounterspeech; (2) a brainstorming session that guides users in identifying key\nelements of hate speech and exploring counterspeech strategies; and (3) a\nco-writing session that enables users to draft and refine their counterspeech\nwith CounterQuill. We conducted a within-subjects user study with 20\nparticipants to evaluate CounterQuill in comparison to ChatGPT. Results show\nthat CounterQuill's guidance and collaborative writing process provided users a\nstronger sense of ownership over their co-authored counterspeech. Users\nperceived CounterQuill as a writing partner and thus were more willing to post\nthe co-written counterspeech online compared to the one written with ChatGPT.", "AI": {"tldr": "Introducing CounterQuill, an AI-mediated system to assist users in creating effective counterspeech against online hate speech.", "motivation": "The prevalence of online hate speech on social media platforms and the inadequacy of current counterspeech methods.", "method": "CounterQuill employs a three-step process: learning about hate speech, brainstorming counterspeech strategies, and co-writing the counterspeech with users.", "result": "Users found that CounterQuill provided them with a stronger sense of ownership over their counterspeech than ChatGPT, making them more willing to post their work online.", "conclusion": "CounterQuill effectively enhances user engagement in counterspeech through its collaborative approach and educational component.", "key_contributions": ["Introduces CounterQuill, an AI-assisted tool for counterspeech.", "Demonstrates the benefits of a collaborative writing process.", "Highlights the educational aspect of understanding hate speech and counterspeech."], "limitations": "The study involved only 20 participants, which may limit the generalizability of the findings.", "keywords": ["Hate speech", "Counterspeech", "AI mediation", "User engagement", "Social media"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.03452", "pdf": "https://arxiv.org/pdf/2505.03452.pdf", "abs": "https://arxiv.org/abs/2505.03452", "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation", "authors": ["Matan Orbach", "Ohad Eytan", "Benjamin Sznajder", "Ariel Gera", "Odellia Boni", "Yoav Kantor", "Gal Bloch", "Omri Levy", "Hadas Abraham", "Nitzan Barzilay", "Eyal Shnarch", "Michael E. Factor", "Shila Ofek-Koifman", "Paula Ta-Shma", "Assaf Toledo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.", "AI": {"tldr": "The paper benchmarks various hyper-parameter optimization (HPO) algorithms for Retrieval-Augmented Generation (RAG) across diverse datasets, revealing effective strategies for enhancing RAG performance.", "motivation": "To tackle the complexity and cost of finding optimal RAG configurations, and to address the lack of rigorous benchmarking of HPO frameworks for RAG.", "method": "A comprehensive study was conducted using 5 different HPO algorithms across 5 datasets, including a newly collected dataset for real-world product documentation, analyzing the largest HPO search space with two optimized evaluation metrics.", "result": "The study finds that RAG HPO can be performed efficiently with either greedy or iterative random search strategies, significantly boosting the performance of RAG models across all datasets analyzed.", "conclusion": "The findings suggest that optimizing models first is more effective than the traditional sequential optimization according to the RAG pipeline order.", "key_contributions": ["Benchmarking of 5 HPO algorithms for RAG", "Inclusion of a new dataset on product documentation", "Identification of effective optimization strategies for RAG performance enhancement."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Hyper-parameter optimization", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.16518", "pdf": "https://arxiv.org/pdf/2503.16518.pdf", "abs": "https://arxiv.org/abs/2503.16518", "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications", "authors": ["Dian Chen", "Han Jun Yoon", "Zelin Wan", "Nithin Alluru", "Sang Won Lee", "Richard He", "Terrence J. Moore", "Frederica F. Nelson", "Sunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Jin-Hee Cho"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.", "AI": {"tldr": "This survey analyzes Human-Machine Teaming (HMT) across several domains, presenting a taxonomy and discussing theoretical models and methodologies.", "motivation": "To explore the integration of AI-driven decision-making in collaborative environments like defense and healthcare, addressing key challenges and ethical considerations.", "method": "A comprehensive survey examining theoretical models such as reinforcement learning and interdependence theory, alongside methodologies in team cognition and ethical AI.", "result": "Identification of key challenges in HMT, including explainability and role allocation, with proposed directions for future research like trust-aware AI.", "conclusion": "The work provides a foundational framework for developing resilient and ethical Human-Machine Teaming systems across various domains.", "key_contributions": ["Comprehensive taxonomy of Human-Machine Teaming", "In-depth analysis of theoretical models including reinforcement learning", "Proposals for future research in cross-domain adaptation and ethical AI"], "limitations": "", "keywords": ["Human-Machine Teaming", "AI-driven decision-making", "ethical AI", "team cognition", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.03467", "pdf": "https://arxiv.org/pdf/2505.03467.pdf", "abs": "https://arxiv.org/abs/2505.03467", "title": "Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis", "authors": ["Shuang Zhou", "Jiashuo Wang", "Zidu Xu", "Song Wang", "David Brauer", "Lindsay Welton", "Jacob Cogan", "Yuen-Hei Chung", "Lei Tian", "Zaifu Zhan", "Yu Hou", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.", "AI": {"tldr": "This paper introduces ConfiDx, an uncertainty-aware LLM for explainable disease diagnosis that recognizes and explains diagnostic uncertainties to improve reliability in automatic diagnostic systems.", "motivation": "There is a critical need for trustworthy diagnostic systems that can explicitly identify and explain diagnostic uncertainties arising from insufficient clinical evidence.", "method": "ConfiDx was developed by fine-tuning open-source LLMs using diagnostic criteria and evaluated through richly annotated datasets that reflect various degrees of diagnostic ambiguity.", "result": "ConfiDx demonstrated superior performance in identifying diagnostic uncertainties and generating trustworthy explanations, outperforming existing systems on real-world datasets.", "conclusion": "This study is the first to jointly address the recognition and explanation of diagnostic uncertainty, contributing significantly to the enhancement of automatic diagnostic systems.", "key_contributions": ["Introduction of ConfiDx, an uncertainty-aware LLM for diagnostics", "Development of annotated datasets capturing diagnostic ambiguity", "Demonstration of improved reliability in automatic diagnostic explanations"], "limitations": "", "keywords": ["explainable AI", "diagnostic uncertainty", "large language model", "health informatics", "machine learning"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2504.01153", "pdf": "https://arxiv.org/pdf/2504.01153.pdf", "abs": "https://arxiv.org/abs/2504.01153", "title": "Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations", "authors": ["Mahjabin Nahar", "Eun-Ju Lee", "Jin Won Park", "Dongwon Lee"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.", "AI": {"tldr": "An experiment studying the effect of static and dynamic web search results on the perceived accuracy of LLM-generated content and users' confidence in their assessments.", "motivation": "To explore whether integrating web search results into LLMs helps users detect inaccuracies or hallucinations in the generated content.", "method": "An online experiment with 560 participants, comparing three conditions: static search results, dynamic participant-led searches, and a control group with no web search.", "result": "Participants exposed to web search results rated hallucinated LLM content as less accurate and had a more negative view of the LLM. Those in the dynamic search condition judged genuine content as more accurate and were more self-assured in their evaluations.", "conclusion": "Incorporating web search functionality in LLMs can enhance user accuracy assessments and confidence, particularly in dynamic search scenarios.", "key_contributions": ["Investigates how web search results influence user perceptions of LLM accuracy.", "Demonstrates differences in user confidence between static and dynamic searching.", "Provides practical implications for LLM design in real-world applications."], "limitations": "The study relies on self-reported measures, which may introduce bias.", "keywords": ["Large Language Models", "Web Search", "Hallucinations", "Human-Computer Interaction", "User Confidence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.03469", "pdf": "https://arxiv.org/pdf/2505.03469.pdf", "abs": "https://arxiv.org/abs/2505.03469", "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models", "authors": ["Bin Yu", "Hang Yuan", "Yuliang Wei", "Bailing Wang", "Weizhen Qi", "Kai Chen"], "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models.", "AI": {"tldr": "This paper presents LS-Mixture SFT, a method to fine-tune non-reasoning models using a mix of long and short Chain-of-Thought data to improve reasoning capabilities while minimizing verbosity.", "motivation": "To address the 'overthinking' problem in models fine-tuned with Chain-of-Thought reasoning data, which results in verbose responses.", "method": "The proposed LS-Mixture SFT combines long CoT reasoning datasets with short versions obtained through structure-preserved rewriting to fine-tune non-reasoning models.", "result": "LS-Mixture SFT improved model accuracy by 2.3% and reduced response length by 47.61% compared to traditional SFT methods.", "conclusion": "LS-Mixture SFT effectively enhances reasoning capabilities in non-reasoning models while mitigating verbosity, making it a viable approach for efficient fine-tuning.", "key_contributions": ["Proposes a novel LS-Mixture SFT approach for fine-tuning", "Demonstrates improvements in accuracy and response length", "Addresses the overthinking problem in reasoning models"], "limitations": "", "keywords": ["Supervised Fine-Tuning", "Chain-of-Thought", "Reasoning Models", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2504.09004", "pdf": "https://arxiv.org/pdf/2504.09004.pdf", "abs": "https://arxiv.org/abs/2504.09004", "title": "Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective", "authors": ["Shirley Zhang", "Bengisu Cagiltay", "Jennica Li", "Dakota Sullivan", "Bilge Mutlu", "Heather Kirkorian", "Kassem Fawaz"], "categories": ["cs.HC"], "comment": null, "summary": "Applications of Generative AI (GenAI), such as ChatGPT, have gained\npopularity among the public due to their ease of access, use, and support of\neducational and creative activities. Despite these benefits, GenAI poses unique\nrisks for families, such as lacking sufficient safeguards tailored to protect\nchildren under 16 years of age and not offering parental control features. This\nstudy explores families' use and co-use of GenAI, the perceived risks and\nopportunities of ChatGPT, and how parents mediate their children's use of\nGenAI. Through semi-structured interviews with 12 families, we identified ways\nfamilies used and mediated GenAI and factors that influenced parents' GenAI\nmediation strategies. We contextualize our findings with a modified model of\nfamily mediation strategies, drawing from previous family media and mediation\nframeworks. We provide insights for future research on family-GenAI\ninteractions and highlight the need for more robust protective measures on\nGenAI platforms for families.", "AI": {"tldr": "This study investigates families' interactions with Generative AI, particularly ChatGPT, exploring perceived risks and parental mediation strategies.", "motivation": "To understand the unique risks and opportunities presented by Generative AI for families and highlight the need for protective measures.", "method": "Semi-structured interviews with 12 families were conducted to gather insights on how they use and mediate the use of GenAI.", "result": "Identified various ways families engaged with GenAI and the factors influencing parental mediation strategies.", "conclusion": "The findings emphasize the need for improved safeguards tailored for families and suggest a model for future research on family interactions with GenAI.", "key_contributions": ["Identification of family mediation strategies for GenAI use", "Insights into parental perceptions of GenAI risks and opportunities", "Recommendations for improving protective measures on GenAI platforms"], "limitations": "", "keywords": ["Generative AI", "family mediation", "ChatGPT", "parental control", "safeguards"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.03473", "pdf": "https://arxiv.org/pdf/2505.03473.pdf", "abs": "https://arxiv.org/abs/2505.03473", "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents", "authors": ["Marta Boscariol", "Luana Bulla", "Lia Draetta", "Beatrice Fiumanò", "Emanuele Lenzi", "Leonardo Piano"], "categories": ["cs.CL"], "comment": null, "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.", "AI": {"tldr": "This paper evaluates the performance of LLMs (GPT and LLama3) in long-tail entity linking tasks, highlighting their effectiveness compared to traditional methods.", "motivation": "Entity Linking is critical for NLP applications, but linking long-tail entities is particularly challenging due to their underrepresentation in models and data.", "method": "The study assesses LLM performance in long-tail entity linking using MHERCL v0.1 benchmark, comparing results against ReLiK framework.", "result": "LLMs showed promising results in identifying and linking long-tail entities, indicating their potential to enhance traditional EL methods.", "conclusion": "LLMs can effectively assist in long-tail entity linking, bridging the gap between head and long-tail entities in NLP tasks.", "key_contributions": ["Evaluation of LLMs in long-tail entity linking", "Introduction of MHERCL v0.1 benchmark", "Comparison with state-of-the-art methods like ReLiK"], "limitations": "The study may have limitations in terms of dataset size and diversity of historical texts used for evaluation.", "keywords": ["Entity Linking", "Long-tail Entities", "Natural Language Processing", "LLMs", "Knowledge Bases"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.03481", "pdf": "https://arxiv.org/pdf/2505.03481.pdf", "abs": "https://arxiv.org/abs/2505.03481", "title": "Sentence Embeddings as an intermediate target in end-to-end summarisation", "authors": ["Maciej Zembrzuski", "Saad Mahamood"], "categories": ["cs.CL"], "comment": "10 pages, 1 figure, Year: 2019", "summary": "Current neural network-based methods to the problem of document summarisation\nstruggle when applied to datasets containing large inputs. In this paper we\npropose a new approach to the challenge of content-selection when dealing with\nend-to-end summarisation of user reviews of accommodations. We show that by\ncombining an extractive approach with externally pre-trained sentence level\nembeddings in an addition to an abstractive summarisation model we can\noutperform existing methods when this is applied to the task of summarising a\nlarge input dataset. We also prove that predicting sentence level embedding of\na summary increases the quality of an end-to-end system for loosely aligned\nsource to target corpora, than compared to commonly predicting probability\ndistributions of sentence selection.", "AI": {"tldr": "Proposes a new approach for document summarisation that combines extractive and abstractive techniques to improve summarisation of large input datasets, specifically for user reviews.", "motivation": "Current methods struggle with large datasets in document summarisation, particularly in user reviews of accommodations.", "method": "Combines an extractive approach with externally pre-trained sentence level embeddings alongside an abstractive summarisation model.", "result": "The new method outperforms existing techniques in summarising large datasets and enhances quality by predicting sentence level embeddings instead of sentence selection probabilities.", "conclusion": "The proposed approach significantly improves the end-to-end summation quality for loosely aligned source to target corpora.", "key_contributions": ["Introduction of a combined extractive and abstractive summarisation model", "Use of externally pre-trained sentence embeddings", "Improvement in summarisation quality for large datasets"], "limitations": "", "keywords": ["document summarisation", "neural networks", "sentence embeddings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.03531", "pdf": "https://arxiv.org/pdf/2505.03531.pdf", "abs": "https://arxiv.org/abs/2505.03531", "title": "Faster MoE LLM Inference for Extremely Large Models", "authors": ["Haoqi Yang", "Luohe Shi", "Qiwei Li", "Zuchao Li", "Ping Wang", "Bo Du", "Mengjia Shen", "Hai Zhao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.", "AI": {"tldr": "This paper explores the efficiency dynamics of fine-grained Sparse Mixture of Experts (MoE) models, demonstrating how reducing activated experts can enhance efficiency with minor performance loss.", "motivation": "The rise of fine-grained Mixture of Experts models in large language applications necessitates understanding their efficiency under different loads, as existing research is limited.", "method": "Analysis of efficiency dynamics in fine-grained MoE models under varying service loads, with experiments on the impact of different numbers of activated and total experts.", "result": "The study finds that reducing the number of activated experts enhances efficiency significantly (10% throughput increase), while reducing total experts leads to performance degradation.", "conclusion": "MoE inference optimization offers substantial potential for improvements, particularly in the development of fine-grained models.", "key_contributions": ["Identification of the efficiency trade-offs in fine-grained MoE models", "Proposal of methods leading to increased throughput", "Insight into performance implications of expert reduction strategies"], "limitations": "The results are primarily focused on specific scenarios and may not generalize to all applications of MoE models.", "keywords": ["Mixture of Experts", "fine-grained models", "efficiency", "performance", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563.pdf", "abs": "https://arxiv.org/abs/2505.03563", "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing", "authors": ["Cléa Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Taïk", "Elliot Creager", "Golnoosh Farnadi"], "categories": ["cs.CL"], "comment": null, "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.", "AI": {"tldr": "This paper presents a controlled paraphrasing framework for generating natural variations of prompts to evaluate the behavior of LLMs, demonstrating significant effects of prompt wording on model responses.", "motivation": "To address concerns about the stability and reliability of language model evaluations due to variations in prompt wording.", "method": "A controlled paraphrasing framework based on minimal linguistic transformations to create natural prompt variations, validated using the BBQ dataset through human annotations and automated checks.", "result": "Analysis reveals that subtle prompt modifications can lead to significant changes in LLM behavior, particularly in stereotype evaluation tasks.", "conclusion": "The findings underscore the necessity for robust, paraphrase-aware protocols when evaluating LLMs.", "key_contributions": ["Development of a controlled paraphrasing framework.", "Demonstration of the impact of prompt variations on LLM responses.", "Call for improved evaluation protocols considering paraphrasing."], "limitations": "", "keywords": ["paraphrasing", "large language models", "evaluations", "prompt variations", "natural language"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.", "AI": {"tldr": "This paper presents a philosophical perspective on NLP, leveraging the Hegelian Dialectic for LLM self-reflection and proposing dynamic temperature strategies for generation.", "motivation": "To connect computational methods with classical philosophy and improve the creativity and reasoning of LLMs.", "method": "Introduces Hegelian Dialectic for self-reflection in LLMs, explores dynamic and fixed-temperature strategies for idea generation, and utilizes a Multi-Agent Majority Voting (MAMV) strategy for idea validation.", "result": "The proposed methods show promise in generating novel ideas and enhancing reasoning capabilities in LLMs during problem-solving.", "conclusion": "The integration of philosophical approaches and temperature strategies can significantly improve LLMs' performance in creative generation and reasoning tasks.", "key_contributions": ["Philosophical approach using Hegelian Dialectic for LLM self-reflection.", "Dynamic annealing for temperature adjustment in idea generation.", "Implementation of Multi-Agent Majority Voting to assess idea validity."], "limitations": "", "keywords": ["NLP", "Hegelian Dialectic", "LLM", "creativity", "self-reflection"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.03675", "pdf": "https://arxiv.org/pdf/2505.03675.pdf", "abs": "https://arxiv.org/abs/2505.03675", "title": "Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula G Allen-Meares", "Eulalia P Abril", "Olga Garcia-Bedoya", "Carolyn A Dickens", "Andrew D. Boyd"], "categories": ["cs.CL"], "comment": null, "summary": "We explore the potential of ChatGPT (3.5-turbo and 4) to generate\nconversations focused on self-care strategies for African-American heart\nfailure patients -- a domain with limited specialized datasets. To simulate\npatient-health educator dialogues, we employed four prompting strategies:\ndomain, African American Vernacular English (AAVE), Social Determinants of\nHealth (SDOH), and SDOH-informed reasoning. Conversations were generated across\nkey self-care domains of food, exercise, and fluid intake, with varying turn\nlengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as\nage, gender, neighborhood, and socioeconomic status. Our findings show that\neffective prompt design is essential. While incorporating SDOH and reasoning\nimproves dialogue quality, ChatGPT still lacks the empathy and engagement\nneeded for meaningful healthcare communication.", "AI": {"tldr": "This paper investigates the use of ChatGPT to generate self-care strategy conversations for African-American heart failure patients, focusing on the role of prompt design in enhancing dialogue quality.", "motivation": "To address the lack of specialized datasets for creating effective health communication strategies for African-American heart failure patients.", "method": "Four prompting strategies were employed to simulate dialogues: domain-focused, AAVE, SDOH, and SDOH-informed reasoning, covering key self-care topics such as food, exercise, and fluid intake.", "result": "Effective prompt design significantly impacts dialogue quality, with SDOH incorporation enhancing conversations; however, ChatGPT struggles with empathy and engagement levels necessary for healthcare interactions.", "conclusion": "Although ChatGPT can improve dialogue in specific contexts, further refinements are needed to enhance emotional and engagement aspects of healthcare communication.", "key_contributions": ["Exploration of ChatGPT's capabilities in health communication", "Identification of effective prompting strategies", "Findings indicating the importance of empathy in health dialogues"], "limitations": "ChatGPT lacks the necessary empathy and engagement for meaningful healthcare conversations.", "keywords": ["ChatGPT", "self-care", "heart failure", "health communication", "African American"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.10707", "pdf": "https://arxiv.org/pdf/2503.10707.pdf", "abs": "https://arxiv.org/abs/2503.10707", "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models", "authors": ["Zhiyuan Wang", "Katharine E. Daniel", "Laura E. Barnes", "Philip I. Chow"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.", "AI": {"tldr": "This paper presents CALLM, a Context-Aware framework using LLMs and RAG to analyze mobile diary entries of cancer survivors for improved emotional state tracking and intervention opportunities.", "motivation": "Cancer survivors encounter distinct emotional challenges that influence their quality of life, necessitating effective tracking of emotional states for timely interventions.", "method": "The study analyzes mobile diary entries from 407 cancer survivors, establishing connections between context and emotional states, and employs CALLM to enhance emotion analysis using LLM and RAG techniques.", "result": "CALLM achieved balanced accuracies of 72.96% for positive affect, 73.29% for negative affect, 73.72% for emotion regulation desire, and 60.09% for intervention availability, outperforming existing models.", "conclusion": "The framework shows that contextual data from mobile diaries can be utilized to gain insights into emotional experiences and optimize intervention timing for personalized support.", "key_contributions": ["Development of the CALLM framework for emotional analysis", "Identification of context-emotion relationships in cancer survivors", "Achievement of high prediction accuracies for emotional states"], "limitations": "", "keywords": ["Cancer survivors", "Mobile diaries", "Emotion analysis", "Large Language Models", "Context-Aware"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688.pdf", "abs": "https://arxiv.org/abs/2505.03688", "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "IndicSQuAD is a new extractive QA dataset for nine major Indic languages, enhancing question-answering resources for low-resource languages.", "motivation": "To address the underrepresentation of Indic languages in question-answering systems despite their large native speaker base.", "method": "We adapted and extended translation techniques from the SQuAD dataset to create a comprehensive multi-lingual QA dataset, IndicSQuAD, while ensuring high linguistic fidelity and accurate answer-span alignment.", "result": "Baseline performance evaluations using language-specific BERT models show challenges faced in low-resource settings.", "conclusion": "IndicSQuAD provides a valuable resource for developing QA systems in Indic languages and suggests future directions for dataset expansion and multimodal integration.", "key_contributions": ["Introduction of IndicSQuAD, a multi-lingual QA dataset for nine Indic languages.", "Utilization of translation techniques to ensure linguistic fidelity in dataset creation.", "Evaluation of baseline performance highlights challenges in low-resource language QA systems."], "limitations": "Challenges inherent in low-resource settings that affect QA system performance.", "keywords": ["Indic languages", "question-answering", "machine learning", "NLP", "dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.03711", "pdf": "https://arxiv.org/pdf/2505.03711.pdf", "abs": "https://arxiv.org/abs/2505.03711", "title": "NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation", "authors": ["Baharul Islam", "Nasim Ahmad", "Ferdous Ahmed Barbhuiya", "Kuntal Dey"], "categories": ["cs.CL"], "comment": null, "summary": "We present our system submission for SemEval 2025 Task 5, which focuses on\ncross-lingual subject classification in the English and German academic\ndomains. Our approach leverages bilingual data during training, employing\nnegative sampling and a margin-based retrieval objective. We demonstrate that a\ndimension-as-token self-attention mechanism designed with significantly reduced\ninternal dimensions can effectively encode sentence embeddings for subject\nretrieval. In quantitative evaluation, our system achieved an average recall\nrate of 32.24% in the general quantitative setting (all subjects), 43.16% and\n31.53% of the general qualitative evaluation methods with minimal GPU usage,\nhighlighting their competitive performance. Our results demonstrate that our\napproach is effective in capturing relevant subject information under resource\nconstraints, although there is still room for improvement.", "AI": {"tldr": "This paper presents a system for cross-lingual subject classification in English and German using a dimension-as-token self-attention mechanism and bilingual data.", "motivation": "The goal is to improve subject classification in academic domains across languages with limited resources.", "method": "The system leverages bilingual data with negative sampling and a margin-based retrieval objective, using a self-attention mechanism with reduced internal dimensions for effective sentence embedding encoding.", "result": "Achieved an average recall rate of 32.24% in general quantitative settings, with 43.16% and 31.53% in qualitative evaluations, demonstrating competitive performance with minimal GPU usage.", "conclusion": "The approach effectively captures relevant subject information under resource constraints, although there is potential for further improvement.", "key_contributions": ["Dimension-as-token self-attention mechanism for sentence embeddings", "Leveraging bilingual data for effective cross-lingual classification", "Competitive performance metrics with minimal resource usage"], "limitations": "Still room for improvement in capturing subject information more effectively.", "keywords": ["Cross-lingual classification", "Subject retrieval", "Self-attention mechanism", "Bilingual data", "Academic domains"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733.pdf", "abs": "https://arxiv.org/abs/2505.03733", "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.", "AI": {"tldr": "WebGen-Bench is a benchmark for evaluating LLMs in generating code for multi-file websites, revealing challenges in accuracy through extensive testing.", "motivation": "To measure the capability of LLM-based agents in generating complex website codebases from scratch, addressing gaps in existing benchmarks.", "method": "Development of WebGen-Bench with diverse instructions for code generation, followed by creating test cases that assess the generated websites' functionality and automating the testing process using web-navigation agents.", "result": "The best LLM-based framework achieved only 27.8% accuracy on test cases, suggesting the benchmark's difficulty, while a modified model trained on specific instructions reached 38.2% accuracy.", "conclusion": "Despite advancements, LLM-based agents struggle with website code generation accuracy, indicating significant room for improvement in AI-assisted coding tasks.", "key_contributions": ["Introduction of WebGen-Bench as a new benchmark for LLM-based web code generation.", "Creation of 6,667 website-generation instructions for training LLMs.", "Evaluation of multiple LLM frameworks, highlighting varying performances."], "limitations": "The benchmark reveals that current LLMs have limited accuracy, indicating potential overfitting and underperformance in real-world applications.", "keywords": ["LLM", "web code generation", "benchmark", "test cases", "website development"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.03739", "pdf": "https://arxiv.org/pdf/2505.03739.pdf", "abs": "https://arxiv.org/abs/2505.03739", "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model", "authors": ["Zuwei Long", "Yunhang Shen", "Chaoyou Fu", "Heting Gao", "Lijiang Li", "Peixian Chen", "Mengdan Zhang", "Hang Shao", "Jian Li", "Jinlong Peng", "Haoyu Cao", "Ke Li", "Rongrong Ji", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio", "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.", "AI": {"tldr": "VITA-Audio is a large speech model that enhances real-time audio generation with minimal latency during streaming through a novel token prediction approach and progressive training techniques.", "motivation": "The need for natural human-computer interaction has increased the focus on speech-based systems, which currently face high latency in generating the first audio token during streaming.", "method": "VITA-Audio employs a Multiple Cross-modal Token Prediction (MCTP) module to generate multiple audio tokens in a single forward pass and introduces a four-stage progressive training strategy to optimize model performance and speed.", "result": "The model achieves a 3-5x speedup at the 7B parameter scale and outperforms similar sized models on benchmarks for automatic speech recognition, text-to-speech, and spoken question answering tasks.", "conclusion": "VITA-Audio is a reproducible, open-source model that enables real-time conversational capabilities with notably reduced latency, outperforming existing models in various speech tasks.", "key_contributions": ["Introduction of the Multiple Cross-modal Token Prediction (MCTP) module for faster audio generation.", "Four-stage progressive training strategy to minimize quality loss while accelerating the model.", "Demonstration of 3-5x inference speedup without sacrificing performance on ASR, TTS, and SQA tasks."], "limitations": "", "keywords": ["speech models", "human-computer interaction", "latency reduction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2401.16646", "pdf": "https://arxiv.org/pdf/2401.16646.pdf", "abs": "https://arxiv.org/abs/2401.16646", "title": "Incoherent Probability Judgments in Large Language Models", "authors": ["Jian-Qiao Zhu", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive Large Language Models (LLMs) trained for next-word prediction\nhave demonstrated remarkable proficiency at producing coherent text. But are\nthey equally adept at forming coherent probability judgments? We use\nprobabilistic identities and repeated judgments to assess the coherence of\nprobability judgments made by LLMs. Our results show that the judgments\nproduced by these models are often incoherent, displaying human-like systematic\ndeviations from the rules of probability theory. Moreover, when prompted to\njudge the same event, the mean-variance relationship of probability judgments\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\npropose that these deviations from rationality can be explained by linking\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\nthe Bayesian Sampler model of human probability judgments.", "AI": {"tldr": "The paper investigates the coherence of probability judgments by autoregressive LLMs, revealing systematic deviations from probability theory similar to human judgment errors.", "motivation": "To understand if LLMs are capable of coherent probability judgments, as they are with text generation.", "method": "Probabilistic identities and repeated judgments were used to analyze the coherence of LLMs' probability assessments.", "result": "LLMs often produce incoherent probability judgments, showing human-like systematic deviations and an inverted-U-shaped mean-variance relationship.", "conclusion": "These deviations can be linked to implicit Bayesian inference, offering parallels with human probability judgments through the Bayesian Sampler model.", "key_contributions": ["Assessment of LLMs' probability judgment coherence", "Comparison of LLMs' behaviors to human probability judgment errors", "Introduction of a Bayesian framework to explain LLM deviations"], "limitations": "", "keywords": ["Large Language Models", "Probability Judgments", "Bayesian Inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.14498", "pdf": "https://arxiv.org/pdf/2406.14498.pdf", "abs": "https://arxiv.org/abs/2406.14498", "title": "LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors", "authors": ["Sheikh Asif Imran", "Mohammad Nur Hossain Khan", "Subrata Biswas", "Bashima Islam"], "categories": ["cs.CL"], "comment": null, "summary": "Wearables generate rich motion data, yet current systems only classify what\nhappened - failing to support natural questions about why it happened or what\nit means. We introduce LLaSA (Large Language and Sensor Assistant), a compact\n13B model that enables ask-anything, open-ended question answering grounded in\nraw IMU data. LLaSA supports conversational, context-aware reasoning -\nexplaining the causes of sensor-detected behaviors and answering free-form\nquestions in real-world scenarios. It is tuned for scientific accuracy,\ncoherence, and response reliability. To advance this new task of sensor-based\nQA, we release three large-scale datasets: SensorCaps, OpenSQA, and\nTune-OpenSQA. Together, these resources define a new benchmark for\nsensor-language models. LLaSA consistently produces interpretable, causal\nanswers and outperforms commercial LLMs across both public and real-world\nsettings. Our code repository and datasets can be found at\nhttps://github.com/BASHLab/LLaSA.", "AI": {"tldr": "LLaSA is a new model for open-ended question answering based on IMU data, enabling explanations of sensor-detected behaviors in real-world scenarios.", "motivation": "Current wearable systems only classify what happened, lacking the capability to explain why it happened or its implications.", "method": "LLaSA, a 13B model, is tuned for conversational reasoning and grounded in raw IMU data, providing context-aware answers to free-form questions.", "result": "LLaSA consistently produces causal, interpretable answers and outperforms commercial LLMs, demonstrating scientific accuracy and response reliability.", "conclusion": "LLaSA sets a new benchmark for sensor-language models with three large-scale datasets, promoting advancements in sensor-based question answering.", "key_contributions": ["Introduction of LLaSA for open-ended sensor-based question answering", "Release of three large-scale datasets: SensorCaps, OpenSQA, Tune-OpenSQA", "Benchmark for evaluating sensor-language models"], "limitations": "", "keywords": ["Wearable Technology", "Question Answering", "Motion Data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.01122", "pdf": "https://arxiv.org/pdf/2408.01122.pdf", "abs": "https://arxiv.org/abs/2408.01122", "title": "CFBench: A Comprehensive Constraints-Following Benchmark for LLMs", "authors": ["Tao Zhang", "Chenglin Zhu", "Yanjun Shen", "Wenjing Luo", "Yan Zhang", "Hao Liang", "Tao Zhang", "Fan Yang", "Mingan Lin", "Yujing Qiao", "Weipeng Chen", "Bin Cui", "Wentao Zhang", "Zenan Zhou"], "categories": ["cs.CL"], "comment": "15 pages, 10 figures", "summary": "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench", "AI": {"tldr": "CFBench is a benchmark for evaluating Large Language Models (LLMs) on their ability to understand and follow natural language instructions through comprehensive constraints derived from real-world scenarios.", "motivation": "To address the limitations of existing evaluations that focus on narrow aspects of constraints, CFBench aims to provide a more holistic assessment of LLMs' performance in real-world applications by covering a wider range of constraints and scenarios.", "method": "CFBench features 1,000 curated samples across more than 200 real-life scenarios and over 50 NLP tasks, organized into 10 main constraint categories and over 25 subcategories. It uses a multi-dimensional methodology for evaluation that prioritizes user perception of requirement fulfillment.", "result": "Leading LLMs show significant limitations in following constraints accurately, highlighting areas for improvement and further research on influencing factors and enhancement strategies.", "conclusion": "CFBench sets a new standard for evaluating LLM performance in real-world contexts, aiming to enhance their usability in practical applications by better understanding constraints as perceived by users.", "key_contributions": ["Introduction of CFBench for comprehensive constraint evaluation of LLMs", "Creation of a systematic framework for 10 primary constraint categories and subcategories", "Public availability of data and code for further research and benchmark applications."], "limitations": "The benchmark may not cover all possible real-world scenarios and constraints; further enhancements may be needed as LLM capabilities evolve.", "keywords": ["Large Language Models", "MillBench", "natural language instructions", "constraint following", "NLP tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.14307", "pdf": "https://arxiv.org/pdf/2408.14307.pdf", "abs": "https://arxiv.org/abs/2408.14307", "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing", "authors": ["Yayati Jadhav", "Peter Pak", "Amir Barati Farimani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.", "AI": {"tldr": "This paper introduces a framework that utilizes Large Language Models (LLMs) for monitoring and controlling 3D printing processes, enabling automated defect detection and correction without human intervention.", "motivation": "The increasing complexity of 3D printing technologies, particularly in Fused Deposition Modeling (FDM), necessitates a robust solution for detecting and correcting defects to maintain product quality and reduce reliance on expert intervention.", "method": "The framework integrates pre-trained LLMs to evaluate print quality through image analysis of each printed layer, identifying defects and querying printer parameters to execute corrective actions.", "result": "The framework effectively identifies and corrects common 3D printing errors, surpassing traditional methods where human experts were involved, demonstrating improved accuracy and autonomy in quality control.", "conclusion": "The proposed LLM-based system can significantly enhance the efficiency and reliability of additive manufacturing processes by automating defect detection and correction.", "key_contributions": ["Introduction of an LLM-based monitoring framework for 3D printing", "Validation against human expert performance", "Demonstration of autonomy in defect correction"], "limitations": "Requires integration with specific 3D printer setups and may depend on the quality of the input data for image analysis.", "keywords": ["Large Language Models", "3D Printing", "Additive Manufacturing", "Defect Detection", "Automation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.00153", "pdf": "https://arxiv.org/pdf/2410.00153.pdf", "abs": "https://arxiv.org/abs/2410.00153", "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution", "authors": ["Haiyan Zhao", "Heng Zhao", "Bo Shen", "Ali Payani", "Fan Yang", "Mengnan Du"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.", "AI": {"tldr": "This paper introduces the Gaussian Concept Subspace (GCS) to improve the robustness of concept representation in large language models (LLMs) for probing tasks, demonstrating its effectiveness in emotion steering applications.", "motivation": "Understanding how semantic knowledge is encoded in LLMs requires effective probing of learned concepts, but existing methods suffer from variability and lack robustness.", "method": "The approach involves extending concept vectors into a Gaussian Concept Subspace (GCS) using linear probing classifiers, aiming to create a more stable representation of concepts.", "result": "GCS shows improved faithfulness and plausibility in representations across various LLMs, and its efficacy is validated through representation intervention tasks in applications like emotion steering.", "conclusion": "GCS concept vectors can balance improved steering performance while maintaining fluency in natural language generation tasks, providing a more reliable tool for probing LLMs.", "key_contributions": ["Introduction of Gaussian Concept Subspace (GCS) for LLMs", "Demonstration of robustness in concept representation", "Application of GCS in emotion steering tasks"], "limitations": "", "keywords": ["Large Language Models", "Probing", "Gaussian Concept Subspace", "Emotion Steering", "Natural Language Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.09580", "pdf": "https://arxiv.org/pdf/2410.09580.pdf", "abs": "https://arxiv.org/abs/2410.09580", "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Conversational Recommender Systems (CRS) proactively engage users in\ninteractive dialogues to elicit user preferences and provide personalized\nrecommendations. Existing methods train Reinforcement Learning (RL)-based agent\nwith greedy action selection or sampling strategy, and may suffer from\nsuboptimal conversational planning. To address this, we present a novel Monte\nCarlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a\nconversational agent (S-agent) and a conversational planner (S-planner).\nS-planner builds a conversational search tree with MCTS based on the initial\nactions proposed by S-agent to find conversation plans. The best conversation\nplans from S-planner are used to guide the training of S-agent, creating a\nself-training loop where S-agent can iteratively improve its capability for\nconversational planning. Furthermore, we propose an efficient variant SAPIENT-e\nfor trade-off between training efficiency and performance. Extensive\nexperiments on four benchmark datasets validate the effectiveness of our\napproach, showing that SAPIENT outperforms the state-of-the-art baselines.", "AI": {"tldr": "SAPIENT is a novel Conversational Recommender System using Monte Carlo Tree Search for improved conversational planning and personalized responses.", "motivation": "To address suboptimal conversational planning in existing Reinforcement Learning-based systems by enhancing the dialogue capabilities through structured search methods.", "method": "Introduces a two-component system: S-agent (the conversational agent) and S-planner (the conversational planner using MCTS) to build a conversational search tree for optimal dialogue strategies.", "result": "SAPIENT shows significant improvements over state-of-the-art methods in terms of conversational planning and user engagement across multiple benchmark datasets.", "conclusion": "The proposed framework and its variant demonstrate effective self-training and enhanced performance, making SAPIENT a robust solution for conversational recommender systems.", "key_contributions": ["Development of SAPIENT framework for CRS using MCTS.", "Introduction of a self-training loop for iterative improvement of conversation quality.", "Efficiency enhancements in SAPIENT-e variant for better training performance."], "limitations": "", "keywords": ["Conversational Recommender Systems", "Reinforcement Learning", "Monte Carlo Tree Search", "Conversational Planning", "Personalized Recommendations"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2411.00027", "pdf": "https://arxiv.org/pdf/2411.00027.pdf", "abs": "https://arxiv.org/abs/2411.00027", "title": "Personalization of Large Language Models: A Survey", "authors": ["Zhehao Zhang", "Ryan A. Rossi", "Branislav Kveton", "Yijia Shao", "Diyi Yang", "Hamed Zamani", "Franck Dernoncourt", "Joe Barrow", "Tong Yu", "Sungchul Kim", "Ruiyi Zhang", "Jiuxiang Gu", "Tyler Derr", "Hongjie Chen", "Junda Wu", "Xiang Chen", "Zichao Wang", "Subrata Mitra", "Nedim Lipka", "Nesreen Ahmed", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners.", "AI": {"tldr": "This paper introduces a taxonomy for personalized Large Language Models (LLMs) and bridges the gap between personalization in text generation and applications like recommendation systems, while addressing key challenges and open problems.", "motivation": "To unify existing research on personalized LLMs, which has been fragmented into text generation and application-focused segments.", "method": "The paper introduces a taxonomy for personalized LLM usage and systematically organizes the literature on personalization techniques, datasets, evaluation methods, and applications of personalized LLMs.", "result": "The authors consolidate and categorize research on personalization aspects of LLMs, providing a structured overview of techniques, challenges, and gaps in the literature.", "conclusion": "The proposed taxonomies and categorization aim to clarify the landscape of personalized LLMs, empowering researchers and practitioners with a comprehensive guide.", "key_contributions": ["Introduction of a taxonomy for personalized LLMs", "Formalization of personalization foundations and facets for LLMs", "Identification of challenges and open problems in the field."], "limitations": "", "keywords": ["personalized language models", "taxonomy", "human-computer interaction", "machine learning", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.18135", "pdf": "https://arxiv.org/pdf/2412.18135.pdf", "abs": "https://arxiv.org/abs/2412.18135", "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment", "authors": ["Binrui Zeng", "Bin Ji", "Xiaodong Liu", "Jie Yu", "Shasha Li", "Jun Ma", "Xiaopeng Li", "Shangwen Wang", "Xinran Hong", "Yongtao Tang"], "categories": ["cs.CL"], "comment": "8 pages, 4 figures, accepted to IJCNN 2025", "summary": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs.", "AI": {"tldr": "The paper introduces Layer-Specific Adaptive Quantization (LSAQ), a technique that dynamically adapts the quantization of Large Language Models (LLMs) for deployment on edge devices based on layer importance.", "motivation": "With the increasing use of Large Language Models on resource-constrained edge devices, existing quantization methods have limitations that prevent efficient deployment. There is a need for adaptive methods that can adjust to different computational resources.", "method": "Layer-Specific Adaptive Quantization (LSAQ) evaluates the importance of layers in LLMs by analyzing token sets from the inputs and outputs of these layers and calculating their Jaccard similarity, which informs real-time adjustments to quantization strategies.", "result": "Experimental results show that LSAQ outperforms existing quantization methods in handling perplexity and zero-shot tasks while enabling tailored quantization schemes for diverse scenarios.", "conclusion": "LSAQ presents a promising approach for the dynamic deployment of LLMs on edge devices, enhancing their applicability across various computational environments.", "key_contributions": ["Introduction of Layer-Specific Adaptive Quantization (LSAQ) for LLMs.", "Real-time adaptation of quantization based on layer importance.", "Demonstrated performance improvements over traditional quantization baselines."], "limitations": "", "keywords": ["Large Language Models", "quantization", "edge devices", "adaptive systems", "layer importance"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.", "AI": {"tldr": "This paper introduces a philosophical approach inspired by the Hegelian Dialectic for enabling self-reflection in LLMs to enhance creativity and reasoning abilities.", "motivation": "To connect computational methods in NLP with classical philosophy and improve LLMs' problem-solving performance.", "method": "The paper employs a self-dialectical approach for LLMs' self-reflection, a dynamic annealing method for generation temperature, and a Multi-Agent Majority Voting strategy to evaluate generated ideas.", "result": "The proposed methods show promising results in generating novel ideas and enhancing the reasoning abilities of LLMs during problem-solving.", "conclusion": "The philosophical approach combined with innovative techniques leads to improved creativity and reasoning in LLMs.", "key_contributions": ["Introduction of a Hegelian Dialectic inspired approach for LLM self-reflection", "Dynamic annealing approach for generation temperature", "Multi-Agent Majority Voting strategy for evaluating generated ideas"], "limitations": "", "keywords": ["NLP", "Hegelian Dialectic", "LLMs", "creativity", "self-reflection"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2501.15858", "pdf": "https://arxiv.org/pdf/2501.15858.pdf", "abs": "https://arxiv.org/abs/2501.15858", "title": "Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech", "authors": ["Eunjung Yeo", "Julie Liss", "Visar Berisha", "David Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "15 pages, 2 figure, 2 tables", "summary": "Purpose: Speech intelligibility is a critical outcome in the assessment and\nmanagement of dysarthria, yet most research and clinical practices have focused\non English, limiting their applicability across languages. This commentary\nintroduces a conceptual framework--and a demonstration of how it can be\nimplemented--leveraging artificial intelligence (AI) to advance cross-language\nintelligibility assessment of dysarthric speech. Method: We propose a\ntwo-tiered conceptual framework consisting of a universal speech model that\nencodes dysarthric speech into acoustic-phonetic representations, followed by a\nlanguage-specific intelligibility assessment model that interprets these\nrepresentations within the phonological or prosodic structures of the target\nlanguage. We further identify barriers to cross-language intelligibility\nassessment of dysarthric speech, including data scarcity, annotation\ncomplexity, and limited linguistic insights into dysarthric speech, and outline\npotential AI-driven solutions to overcome these challenges. Conclusion:\nAdvancing cross-language intelligibility assessment of dysarthric speech\nnecessitates models that are both efficient and scalable, yet constrained by\nlinguistic rules to ensure accurate and language-sensitive assessment. Recent\nadvances in AI provide the foundational tools to support this integration,\nshaping future directions toward generalizable and linguistically informed\nassessment frameworks.", "AI": {"tldr": "This paper presents a framework for improving speech intelligibility assessment of dysarthric speech across languages using AI.", "motivation": "The need for effective dysarthria assessment tools that are applicable across various languages due to a focus on English in existing research.", "method": "A two-tiered conceptual framework using a universal speech model for encoding dysarthric speech and a language-specific model for intelligibility assessment.", "result": "Identified common barriers such as data scarcity and proposed AI-driven solutions to facilitate cross-language intelligibility assessment.", "conclusion": "Effective assessment requires scalable models that adhere to linguistic rules, leveraging advances in AI for better outcomes.", "key_contributions": ["Introduces a universal speech model for dysarthric speech.", "Proposes a language-specific intelligibility assessment model.", "Identifies barriers and suggests AI-driven solutions for cross-language assessment."], "limitations": "Challenges include data scarcity, annotation complexity, and the need for comprehensive linguistic insights.", "keywords": ["Dysarthria", "Speech Intelligibility", "Artificial Intelligence", "Cross-Language Assessment", "Linguistics"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2502.00865", "pdf": "https://arxiv.org/pdf/2502.00865.pdf", "abs": "https://arxiv.org/abs/2502.00865", "title": "Predicting potentially abusive clauses in Chilean terms of services with natural language processing", "authors": ["Christoffer Loeffler", "Andrea Martínez Freile", "Tomás Rey Pizarro"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "39 pages, 2 figures, 8 tables, accepted for publication", "summary": "This study addresses the growing concern of information asymmetry in consumer\ncontracts, exacerbated by the proliferation of online services with complex\nTerms of Service that are rarely even read. Even though research on automatic\nanalysis methods is conducted, the problem is aggravated by the general focus\non English-language Machine Learning approaches and on major jurisdictions,\nsuch as the European Union. We introduce a new methodology and a substantial\ndataset addressing this gap. We propose a novel annotation scheme with four\ncategories and a total of 20 classes, and apply it on 50 online Terms of\nService used in Chile. Our evaluation of transformer-based models highlights\nhow factors like language- and/or domain-specific pre-training, few-shot sample\nsize, and model architecture affect the detection and classification of\npotentially abusive clauses. Results show a large variability in performance\nfor the different tasks and models, with the highest macro-F1 scores for the\ndetection task ranging from 79% to 89% and micro-F1 scores up to 96%, while\nmacro-F1 scores for the classification task range from 60% to 70% and micro-F1\nscores from 64% to 80%. Notably, this is the first Spanish-language multi-label\nclassification dataset for legal clauses, applying Chilean law and offering a\ncomprehensive evaluation of Spanish-language models in the legal domain. Our\nwork lays the ground for future research in method development for rarely\nconsidered legal analysis and potentially leads to practical applications to\nsupport consumers in Chile and Latin America as a whole.", "AI": {"tldr": "This study introduces a new methodology and dataset for analyzing online Terms of Service in Spanish, focusing on detecting and classifying abusive clauses under Chilean law using transformer-based models.", "motivation": "To address information asymmetry in consumer contracts exacerbated by complex online services and to provide insights beyond English-language models focusing on major jurisdictions.", "method": "A novel annotation scheme categorizing legal clauses applied to 50 Terms of Service in Chile, with evaluation of transformer-based models considering language-specific pre-training and other factors.", "result": "The evaluation shows high macro-F1 scores ranging from 79% to 89% for detection tasks and 60% to 70% for classification tasks, indicating variability in performance across tasks and models.", "conclusion": "This work establishes the first Spanish-language multi-label classification dataset for legal clauses, paving the way for further research and practical applications in consumer rights in Chile and Latin America.", "key_contributions": ["Introduction of a methodology for analyzing Spanish-language legal documents", "Creation of a comprehensive dataset for Terms of Service under Chilean law", "Evaluation of transformer-based models in a rarely considered legal domain"], "limitations": "Focus on Spanish language and Chilean context may limit applicability to other jurisdictions or languages.", "keywords": ["Machine Learning", "Legal analysis", "Terms of Service", "Spanish-language models", "Consumer rights"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2502.13685", "pdf": "https://arxiv.org/pdf/2502.13685.pdf", "abs": "https://arxiv.org/abs/2502.13685", "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "authors": ["Jusen Du", "Weigao Sun", "Disen Lan", "Jiaxi Hu", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical report, 16 pages", "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.", "AI": {"tldr": "The paper introduces Mixture-of-Memories (MoM), a linear sequence modeling architecture that enhances memory capacity and performance on recall-intensive tasks.", "motivation": "To improve performance in recall-intensive downstream tasks by addressing limitations of existing linear sequence modeling methods that use a single fixed-size memory state.", "method": "MoM employs multiple independent memory states, directed by a router network, to manage input tokens and enhance memory capacity while maintaining linear complexity.", "result": "MoM significantly outperforms current linear sequence models in recall-intensive tasks and achieves performance comparable to Transformer models.", "conclusion": "MoM retains linear-complexity advantages in training and constant-complexity in inference, showing promise for better memory efficiency in sequence modeling.", "key_contributions": ["Introduction of the Mixture-of-Memories architecture.", "Demonstrates enhanced performance on recall-intensive tasks compared to existing linear models.", "Maintains linear complexity in memory state computation for efficiency."], "limitations": "", "keywords": ["linear sequence modeling", "memory interference", "recall-intensive tasks", "Mixture-of-Memories", "neuroscience-inspired design"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2502.14338", "pdf": "https://arxiv.org/pdf/2502.14338.pdf", "abs": "https://arxiv.org/abs/2502.14338", "title": "English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports", "authors": ["Avinash Patil", "Aryan Jadon"], "categories": ["cs.CL", "cs.SE"], "comment": "8 Pages, 4 Figures, 3 Tables", "summary": "Accurate translation of bug reports is critical for efficient collaboration\nin global software development. In this study, we conduct the first\ncomprehensive evaluation of machine translation (MT) performance on bug\nreports, analyzing the capabilities of DeepL, AWS Translate, and large language\nmodels such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the\nVisual Studio Code GitHub repository, specifically focusing on reports labeled\nwith the english-please tag. To assess both translation quality and source\nlanguage identification accuracy, we employ a range of MT evaluation\nmetrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside\nclassification metrics such as accuracy, precision, recall, and F1-score. Our\nfindings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical\ntranslation quality, it does not lead in source language identification. Claude\nand Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively),\nand Gemini records the best precision (0.7414). AWS Translate shows the highest\naccuracy (0.4717) in identifying source languages. These results highlight that\nno single system dominates across all tasks, reinforcing the importance of\ntask-specific evaluations. This study underscores the need for domain\nadaptation when translating technical content and provides actionable insights\nfor integrating MT into bug-triaging workflows. The code and dataset for this\npaper are available at GitHub-https://github.com/av9ash/English-Please", "AI": {"tldr": "This study evaluates machine translation performance on bug reports in global software development, comparing models like ChatGPT and AWS Translate.", "motivation": "To improve collaboration in global software development through accurate translation of bug reports.", "method": "Comprehensive evaluation of machine translation models including DeepL, AWS Translate, and large language models, using various MT evaluation metrics and classification metrics.", "result": "ChatGPT excels in translation quality, while Almeida and Mistral achieve high F1-scores. AWS Translate leads in source language identification accuracy.", "conclusion": "No single system outperforms others in all tasks, indicating the need for task-specific evaluations and domain adaptation for translating technical content.", "key_contributions": ["First comprehensive evaluation of MT on bug reports", "Insights for integrating MT into bug triaging workflows", "Code and dataset shared for further research"], "limitations": "Focuses only on specific datasets (Visual Studio Code GitHub repository) and evaluation metrics; may not generalize to all bug reporting scenarios.", "keywords": ["Machine Translation", "Bug Reports", "Natural Language Processing", "Large Language Models", "Software Development"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.19187", "pdf": "https://arxiv.org/pdf/2502.19187.pdf", "abs": "https://arxiv.org/abs/2502.19187", "title": "BIG-Bench Extra Hard", "authors": ["Mehran Kazemi", "Bahare Fatemi", "Hritik Bansal", "John Palowitch", "Chrysovalantis Anastasiou", "Sanket Vaibhav Mehta", "Lalit K. Jain", "Virginia Aglietti", "Disha Jindal", "Peter Chen", "Nishanth Dikkala", "Gladys Tyen", "Xin Liu", "Uri Shalit", "Silvia Chiappa", "Kate Olszewska", "Yi Tay", "Vinh Q. Tran", "Quoc V. Le", "Orhan Firat"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.", "AI": {"tldr": "Introduction of a new benchmark, BIG-Bench Extra Hard (BBEH), for evaluating reasoning capabilities of LLMs.", "motivation": "To address the saturation of existing benchmarks (like BIG-Bench and BIG-Bench Hard) in evaluating LLM reasoning capabilities.", "method": "Development of the BIG-Bench Extra Hard benchmark by replacing tasks in BIG-Bench Hard with novel, significantly more difficult tasks while probing similar reasoning capabilities.", "result": "The best general-purpose model achieved a harmonic average accuracy of 9.8%, and the best reasoning-specialized model achieved 44.8%, indicating significant room for improvement in LLM reasoning.", "conclusion": "BBEH highlights the ongoing challenges in attaining robust general reasoning in LLMs and has been made publicly available.", "key_contributions": ["Introduction of BBEH as a new and challenging benchmark for LLM reasoning.", "Replacement of existing tasks with more difficult alternatives to better evaluate reasoning capabilities.", "Public release of BBEH for community use."], "limitations": "", "keywords": ["LLM", "reasoning benchmarks", "BIG-Bench Extra Hard"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.10707", "pdf": "https://arxiv.org/pdf/2503.10707.pdf", "abs": "https://arxiv.org/abs/2503.10707", "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models", "authors": ["Zhiyuan Wang", "Katharine E. Daniel", "Laura E. Barnes", "Philip I. Chow"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.", "AI": {"tldr": "The paper explores the emotional challenges faced by cancer survivors and introduces CALLM, a Context-Aware framework using LLMs to analyze mobile diary entries, aiming to improve emotional well-being through better understanding and intervention opportunities.", "motivation": "Cancer survivors often experience emotional challenges that impact their quality of life; tracking their emotional states can help improve self-awareness and well-being.", "method": "The study analyzes mobile diary entries from 407 cancer survivors, using CALLM, a Context-Aware framework that leverages LLMs and RAG to comprehend the context of entries and predict emotional states.", "result": "CALLM shows strong performance in recognizing emotional states and predicting factors relating to just-in-time interventions, achieving balanced accuracies of around 72-74% for various emotional dimensions.", "conclusion": "The research highlights the importance of contextual factors in diary entries for understanding emotions in cancer survivors and suggests that LLMs can significantly improve emotional analysis and intervention strategies.", "key_contributions": ["Development of CALLM for analyzing mobile diary entries", "Demonstration of strong accuracy in predicting emotional states", "Insights into the impact of context on emotional well-being"], "limitations": "The current model may still struggle with brief diary entries and lacks comprehensive contextual understanding for deeper emotional insights.", "keywords": ["Cancer Survivors", "Emotional Challenges", "Mobile Diaries", "Context-Aware Framework", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.13551", "pdf": "https://arxiv.org/pdf/2503.13551.pdf", "abs": "https://arxiv.org/abs/2503.13551", "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models", "authors": ["Teng Wang", "Zhangyi Jiang", "Zhenqi He", "Shenyang Tong", "Wenhan Yang", "Yanan Zheng", "Zeyu Li", "Zifan He", "Hailei Gong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.", "AI": {"tldr": "The paper introduces the Hierarchical Reward Model (HRM) to improve the reliability of reward modeling in reasoning tasks, addressing limitations of current models like the Process Reward Model (PRM).", "motivation": "Large Language Models face issues with reward hacking and high costs associated with annotating reasoning processes. This limits their effectiveness in training and evaluating reasoning capabilities.", "method": "The Hierarchical Reward Model (HRM) evaluates reasoning steps at different granularity levels and utilizes Hierarchical Node Compression (HNC) for data augmentation to enhance training data diversity and robustness.", "result": "Empirical evaluations indicate that HRM shows more stable and reliable evaluations compared to PRM on the PRM800K dataset, and demonstrates strong generalization across MATH500 and GSM8K datasets.", "conclusion": "HRM, enhanced with HNC, provides a more effective alternative for reward modeling, improving the overall assessment of multi-step reasoning tasks in LLMs.", "key_contributions": ["Introduction of the Hierarchical Reward Model (HRM) for better evaluation of reasoning steps.", "Hierarchical Node Compression (HNC) to reduce training data generation costs.", "Demonstrated improved stability and robustness in evaluations across multiple reasoning tasks."], "limitations": "The focus on multi-step reasoning might limit applicability in single-step contexts; further validation required on additional datasets.", "keywords": ["Large Language Models", "reward modeling", "reasoning tasks", "data augmentation", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279.pdf", "abs": "https://arxiv.org/abs/2503.17279", "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.", "AI": {"tldr": "This paper proposes Condition-Aware Sentence Embeddings (CASE), a method for generating sentence embeddings conditioned on context, showing improved performance over existing methods.", "motivation": "To improve sentence embeddings by considering context, which is crucial for understanding meaning and improving performance on semantic similarity tasks.", "method": "The proposed method, CASE, utilizes a Large Language Model to create embeddings conditioned on a sentence, learning a supervised nonlinear projection for dimensionality reduction.", "result": "CASE significantly outperforms previous C-STS methods on standard benchmark datasets and improves performance by subtracting the condition embedding from LLM-based embeddings.", "conclusion": "The study establishes that context-aware embeddings enhance the accuracy of semantic similarity measures in sentences, aided by an effective dimensionality reduction approach.", "key_contributions": ["Introduction of Condition-Aware Sentence Embeddings (CASE)", "Demonstrated improvement over existing C-STS methods", "Proposed a novel supervised dimensionality reduction technique"], "limitations": "", "keywords": ["sentence embeddings", "contextual embeddings", "semantic textual similarity", "dimensionality reduction", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991.pdf", "abs": "https://arxiv.org/abs/2503.18991", "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Weixin Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The three authors contributed equally to this work", "summary": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.", "AI": {"tldr": "HAIR is a novel approach for aligning large language models (LLMs) with human values, addressing key challenges related to safety datasets, alignment tax, jailbreak vulnerabilities, and reward adaptability.", "motivation": "The alignment of LLMs with human values is critical, yet it is hindered by challenges such as the scarcity of balanced safety datasets and other alignment issues.", "method": "HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning) involves creating a balanced safety Chain-of-Draft dataset using structured prompts and training category-specific reward models with Group Relative Policy Optimization to dynamically adjust to task difficulty.", "result": "HAIR outperforms all baseline methods in terms of safety while maintaining high usefulness levels across four harmlessness and four usefulness benchmarks.", "conclusion": "The introduction of HAIR provides a promising direction for improving the alignment of LLMs with human values.", "key_contributions": ["Introduction of HAIR, a novel alignment approach for LLMs", "Construction of a balanced safety Chain-of-Draft dataset", "Dynamic tuning of reward models based on task difficulty"], "limitations": "", "keywords": ["large language models", "alignment", "reinforcement learning", "safety", "introspective reasoning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2503.20953", "pdf": "https://arxiv.org/pdf/2503.20953.pdf", "abs": "https://arxiv.org/abs/2503.20953", "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance", "authors": ["Julia Ive", "Felix Jozsa", "Nick Jackson", "Paulina Bondaronek", "Ciaran Scott Hill", "Richard Dobson"], "categories": ["cs.CL"], "comment": null, "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.", "AI": {"tldr": "Development and assessment of an LLM-empowered chatbot for answering clinical guideline questions, demonstrating relevance and efficiency in providing medical information.", "motivation": "To enhance the speed and accuracy of information retrieval for clinical guidelines in healthcare using LLM-empowered chatbots.", "method": "Utilized the open-weight Llama-3.1-8B LLM to extract information from UCLH clinical guidelines, with performance assessed by seven doctors comparing chatbot answers to a gold standard.", "result": "The chatbot achieved ~73% relevant responses, a recall of 1.00 for guideline lines, and a satisfaction rate of approximately 78%, with an efficiency advantage over human respondents.", "conclusion": "The chatbot shows significant potential to improve access to clinical information for healthcare professionals, albeit with some minor precision issues.", "key_contributions": ["Development of a chatbot specifically trained on clinical guidelines", "Assessment of chatbot performance in a clinical context", "Demonstration of high recall and response efficiency compared to humans"], "limitations": "Occasional minor lapses in precision with ~14.5% of responses containing unnecessary information.", "keywords": ["LLM", "healthcare", "chatbot", "clinical guidelines", "information retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.07986", "pdf": "https://arxiv.org/pdf/2504.07986.pdf", "abs": "https://arxiv.org/abs/2504.07986", "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free", "authors": ["Runjin Chen", "Zhenyu Zhang", "Junyuan Hong", "Souvik Kundu", "Zhangyang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.", "AI": {"tldr": "This paper investigates redundancy in chain-of-thought (CoT) reasoning in large language models (LLMs) and introduces SEAL, a training-free approach that improves reasoning efficiency and accuracy.", "motivation": "To address the redundancy in CoT reasoning that increases inference latency and negatively impacts model performance.", "method": "The authors categorize internal reasoning structures into execution, reflection, and transition thoughts, and introduce a training-free method called SEAL, which calibrates the CoT process using a steering vector extracted from latent space.", "result": "SEAL improves accuracy by up to 11% and reduces reasoning tokens by 11.8% to 50.4% across multiple LLMs and benchmarks.", "conclusion": "The SEAL method enhances LLM performance by effectively calibrating reasoning processes and shows strong transferability across tasks.", "key_contributions": ["Introduction of the SEAL approach for CoT calibration", "Identification of thought types impacting model performance", "Empirical validation of SEAL with substantial efficiency improvements"], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought", "Reasoning efficiency", "AI training", "Machine Learning"], "importance_score": 8, "read_time_minutes": 12}}
