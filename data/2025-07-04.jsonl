{"id": "2507.02088", "pdf": "https://arxiv.org/pdf/2507.02088.pdf", "abs": "https://arxiv.org/abs/2507.02088", "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "categories": ["cs.CL"], "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs."}
{"id": "2507.02145", "pdf": "https://arxiv.org/pdf/2507.02145.pdf", "abs": "https://arxiv.org/abs/2507.02145", "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gon√ßalo Oliveira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization."}
{"id": "2507.02199", "pdf": "https://arxiv.org/pdf/2507.02199.pdf", "abs": "https://arxiv.org/abs/2507.02199", "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot."}
{"id": "2507.02221", "pdf": "https://arxiv.org/pdf/2507.02221.pdf", "abs": "https://arxiv.org/abs/2507.02221", "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds."}
{"id": "2507.02122", "pdf": "https://arxiv.org/pdf/2507.02122.pdf", "abs": "https://arxiv.org/abs/2507.02122", "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Allen Chang", "Andrew Zhu", "Lyle Ungar", "Sharath Chandra Guntuku"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings."}
{"id": "2507.02259", "pdf": "https://arxiv.org/pdf/2507.02259.pdf", "abs": "https://arxiv.org/abs/2507.02259", "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test."}
{"id": "2507.02138", "pdf": "https://arxiv.org/pdf/2507.02138.pdf", "abs": "https://arxiv.org/abs/2507.02138", "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy", "authors": ["Shan Li", "Guozhu Ding"], "categories": ["cs.HC"], "comment": null, "summary": "This study introduces and evaluates Healthy Choice, an innovative\ntheory-driven and AI-enhanced simulation platform designed to cultivate\nnutrition literacy through interactive scenario-based learning experiences. We\ncollected feedback from 114 university students with diverse backgrounds who\ncompleted simulated product selection scenarios. Quantitative ratings of\nusefulness and ease of use demonstrated high user satisfaction."}
{"id": "2507.02302", "pdf": "https://arxiv.org/pdf/2507.02302.pdf", "abs": "https://arxiv.org/abs/2507.02302", "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX."}
{"id": "2507.02156", "pdf": "https://arxiv.org/pdf/2507.02156.pdf", "abs": "https://arxiv.org/abs/2507.02156", "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative", "authors": ["Benjamin Watson", "Janet Kim", "Tim McEneany", "Tom Moher", "Claudia Hindo", "Louis Gomez", "Stephen Fransen"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "The StorySpace project studies the role new interface technologies might play\nin high school education. With this approach in mind, StorySpace is\nspecifically designed to support and enhance classroom narrative, an already\nwell-established classroom activity. StorySpace strives to achieve this through\nadherence to three design goals. The first is to trigger student reflection and\ninterpretation. The narrative medium created by StorySpace should represent the\ntopic of classroom discussion and learning in all its complexity. In building\ntheir representation, the students will then be confronted with that same\ncomplexity. The medium should also itself be exciting and compelling, making\nclassroom narrative interesting and fun."}
{"id": "2507.02357", "pdf": "https://arxiv.org/pdf/2507.02357.pdf", "abs": "https://arxiv.org/abs/2507.02357", "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "categories": ["cs.CL"], "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available."}
{"id": "2507.02180", "pdf": "https://arxiv.org/pdf/2507.02180.pdf", "abs": "https://arxiv.org/abs/2507.02180", "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future", "authors": ["Russell Beale"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.1; K.3.2"], "comment": null, "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users."}
{"id": "2507.02364", "pdf": "https://arxiv.org/pdf/2507.02364.pdf", "abs": "https://arxiv.org/abs/2507.02364", "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "authors": ["Pilsung Kang"], "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles."}
{"id": "2507.02186", "pdf": "https://arxiv.org/pdf/2507.02186.pdf", "abs": "https://arxiv.org/abs/2507.02186", "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge", "authors": ["Zahra Ashktorab", "Elizabeth M. Daly", "Erik Miehling", "Werner Geyer", "Martin Santillan Cooper", "Tejaswini Pedapati", "Michael Desmond", "Qian Pan", "Hyo Jin Do"], "categories": ["cs.HC"], "comment": null, "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users."}
{"id": "2507.02378", "pdf": "https://arxiv.org/pdf/2507.02378.pdf", "abs": "https://arxiv.org/abs/2507.02378", "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs."}
{"id": "2507.02187", "pdf": "https://arxiv.org/pdf/2507.02187.pdf", "abs": "https://arxiv.org/abs/2507.02187", "title": "VergeIO: Depth-Aware Eye Interaction on Glasses", "authors": ["Xiyuxing Zhang", "Duc Vu", "Chengyi Shen", "Yuntao Wang", "Yuanchun Shi", "Justin Chan"], "categories": ["cs.HC"], "comment": null, "summary": "There is growing industry interest in creating unobtrusive designs for\nelectrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and\nApple eyewear). We present VergeIO, the first EOG-based glasses that enables\ndepth-aware eye interaction using vergence with an optimized electrode layout\nand novel smart glass prototype. It can distinguish between four and six\ndepth-based eye gestures with 83-98% accuracy using personalized models in a\nuser study across 11 users and 1,320 gesture instances. It generalizes to\nunseen users with an accuracy of 80-98% without any calibration. To reduce\nfalse detections, we incorporate a motion artifact detection pipeline and a\npreamble-based activation scheme. The system uses dry sensors without any\nadhesives or gel, and operates in real time with 3 mW power consumption by the\nsensing front-end, making it suitable for always-on sensing."}
{"id": "2507.02407", "pdf": "https://arxiv.org/pdf/2507.02407.pdf", "abs": "https://arxiv.org/abs/2507.02407", "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs."}
{"id": "2507.02229", "pdf": "https://arxiv.org/pdf/2507.02229.pdf", "abs": "https://arxiv.org/abs/2507.02229", "title": "An Exploration of Internal States in Collaborative Problem Solving", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Mariah Bradford", "Anne M. Cleary", "Nathaniel Blanchard"], "categories": ["cs.HC"], "comment": "Accepted to the International Conference on Human-Computer\n  Interaction (HCII) 2025", "summary": "Collaborative problem solving (CPS) is a complex cognitive, social, and\nemotional process that is increasingly prevalent in educational and\nprofessional settings. This study investigates the emotional states of\nindividuals during CPS using a mixed-methods approach. Teams of four first\ncompleted a novel CPS task. Immediately after, each individual was placed in an\nisolated room where they reviewed the video of their group performing the task\nand self-reported their internal experiences throughout the task. We performed\na linguistic analysis of these internal monologues, providing insights into the\nrange of emotions individuals experience during CPS. Our analysis showed\ndistinct patterns in language use, including characteristic unigrams and\nbigrams, key words and phrases, emotion labels, and semantic similarity between\nemotion-related words."}
{"id": "2507.02428", "pdf": "https://arxiv.org/pdf/2507.02428.pdf", "abs": "https://arxiv.org/abs/2507.02428", "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "categories": ["cs.CL"], "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan."}
{"id": "2507.02254", "pdf": "https://arxiv.org/pdf/2507.02254.pdf", "abs": "https://arxiv.org/abs/2507.02254", "title": "A framework for 3D interaction techniques", "authors": ["Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a software architecture for 3D interaction techniques\n(ITs) and an object oriented, toolkit-independent framework that implements\nsuch architecture. ITs are composed of basic filters connected in a dataflow,\nwhere virtual input devices and objects in the scene are sources of\ninformation. An execution model defines the general flow of information between\nfilters. This framework has been designed to be extensible: new information\ntypes, new input devices, new execution models, or new interaction techniques\ncan easily be added. Application specific code and application specific ITs are\nseamlessly integrated into this architecture."}
{"id": "2507.02506", "pdf": "https://arxiv.org/pdf/2507.02506.pdf", "abs": "https://arxiv.org/abs/2507.02506", "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence."}
{"id": "2507.02283", "pdf": "https://arxiv.org/pdf/2507.02283.pdf", "abs": "https://arxiv.org/abs/2507.02283", "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness", "authors": ["Tim Rogers", "Ben Teehankee"], "categories": ["cs.HC", "I.2.6; H.1.2"], "comment": "21 pages", "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values."}
{"id": "2507.02592", "pdf": "https://arxiv.org/pdf/2507.02592.pdf", "abs": "https://arxiv.org/abs/2507.02592", "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."}
{"id": "2507.02300", "pdf": "https://arxiv.org/pdf/2507.02300.pdf", "abs": "https://arxiv.org/abs/2507.02300", "title": "Human-Centered Explainability in Interactive Information Systems: A Survey", "authors": ["Yuhao Zhang", "Jiaxin An", "Ben Wang", "Yan Zhang", "Jiqun Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Human-centered explainability has become a critical foundation for the\nresponsible development of interactive information systems, where users must be\nable to understand, interpret, and scrutinize AI-driven outputs to make\ninformed decisions. This systematic survey of literature aims to characterize\nrecent progress in user studies on explainability in interactive information\nsystems by reviewing how explainability has been conceptualized, designed, and\nevaluated in practice. Following PRISMA guidelines, eight academic databases\nwere searched, and 100 relevant articles were identified. A structural encoding\napproach was then utilized to extract and synthesize insights from these\narticles. The main contributions include 1) five dimensions that researchers\nhave used to conceptualize explainability; 2) a classification scheme of\nexplanation designs; 3) a categorization of explainability measurements into\nsix user-centered dimensions. The review concludes by reflecting on ongoing\nchallenges and providing recommendations for future exploration of related\nissues. The findings shed light on the theoretical foundations of\nhuman-centered explainability, informing the design of interactive information\nsystems that better align with diverse user needs and promoting the development\nof systems that are transparent, trustworthy, and accountable."}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593.pdf", "abs": "https://arxiv.org/abs/2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G√∂ran Kauermann", "Barbara Plank", "Matthias A√üenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation."}
{"id": "2507.02306", "pdf": "https://arxiv.org/pdf/2507.02306.pdf", "abs": "https://arxiv.org/abs/2507.02306", "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Usability evaluation is crucial in human-centered design but can be costly,\nrequiring expert time and user compensation. In this work, we developed a\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\nanalyze images and provide design feedback. Comparing our synthetic evaluations\nto those by experienced UX practitioners across two apps, we found our\nevaluation identified 73% and 77% of usability issues, which exceeded the\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\nevaluators, the synthetic evaluation's performance maintained consistent\nperformance across tasks and excelled in detecting layout issues, highlighting\npotential attentional and perceptual strengths of synthetic evaluation.\nHowever, synthetic evaluation struggled with recognizing some UI components and\ndesign conventions, as well as identifying across screen violations.\nAdditionally, testing synthetic evaluations over time and accounts revealed\nstable performance. Overall, our work highlights the performance differences\nbetween human and LLM-driven evaluations, informing the design of synthetic\nheuristic evaluations."}
{"id": "2507.02595", "pdf": "https://arxiv.org/pdf/2507.02595.pdf", "abs": "https://arxiv.org/abs/2507.02595", "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025 AIW Workshop", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning."}
{"id": "2507.02350", "pdf": "https://arxiv.org/pdf/2507.02350.pdf", "abs": "https://arxiv.org/abs/2507.02350", "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance", "authors": ["Hao Tang", "Songyun Xie", "Xinzhou Xie", "Can Liao", "Xin Zhang", "Bohan Li", "Zhongyu Tian", "Dalu Zheng"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional video-induced emotion physiological datasets often use\nwhole-trial annotation, assigning a single emotion label to all data collected\nduring an entire trial. This coarse-grained annotation approach misaligns with\nthe dynamic and temporally localized nature of emotional responses as they\nunfold with video narratives, introducing label noise that limits emotion\nrecognition algorithm evaluation and performance. To solve the label noise\nproblem caused by coarse-grained annotation, we propose a fine-grained\nannotation method through an immediate recall paradigm. This paradigm\nintegrates an immediate video replay phase after the initial stimulus viewing,\nallowing participants to precisely mark the onset timestamp, emotion label, and\nintensity based on their immediate recall. We validate this paradigm through\nphysiological evidence and recognition performance. Physiological validation of\nmultimodal signals within participant-marked windows revealed rhythm-specific\nEEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of\nhigh-arousal versus 6% of low-arousal emotion windows. These objective\nphysiological data changes strongly aligned with subjective annotations,\nconfirming annotation precision. For recognition performance, classification\nexperiments showed that models trained on fine-grained annotations achieved\n9.7% higher accuracy than traditional whole-trial labeling, despite using less\ndata. This work not only addresses label noise through fine-grained annotation\nbut also demonstrates that annotation precision outweighs data scale in\ndetermining emotion recognition performance."}
{"id": "2507.02679", "pdf": "https://arxiv.org/pdf/2507.02679.pdf", "abs": "https://arxiv.org/abs/2507.02679", "title": "Exploring Gender Bias Beyond Occupational Titles", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset."}
{"id": "2507.02432", "pdf": "https://arxiv.org/pdf/2507.02432.pdf", "abs": "https://arxiv.org/abs/2507.02432", "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset", "authors": ["Jueun Lee", "Dennis Moschina", "Supraja Ramesh", "Tobias R√∂ddiger", "Kai Kunze", "Michael Beigl"], "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "We investigate the use of musically structured, closed-loop vibration\npatterns as a passive biofeedback intervention for relaxation and sleep\ninitiation. By encoding rhythmic meter structures into smartwatch vibrations\nand adapting their frequency to be slightly slower than the user's real-time\nheart rate, our system aims to reduce arousal through tactile entrainment,\noffering a non-invasive alternative to auditory or open-loop approaches\npreviously used in sleep and anxiety contexts. In the first study (N=20), we\ncompared five adaptive vibration rhythms for their effects on heart rate and\nsubjective perceptions of relaxation in a resting context. In the second study\n(N=28), we evaluated the most promising pattern from Study 1 in a prolonged\nsleep initiation setting. Results showed increased parasympathetic activity and\nperceived relaxation during short-term stimulation, but no significant effects\non sleep-related measures during the sleep onset phase. This work contributes\nto the understanding of how wearable haptic feedback can support relaxation and\nsleep, offering design insights and identifying methodological considerations\nfor effectively integrating haptic interaction into self-directed\ninterventions."}
{"id": "2507.02694", "pdf": "https://arxiv.org/pdf/2507.02694.pdf", "abs": "https://arxiv.org/abs/2507.02694", "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.CL"], "comment": null, "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback."}
{"id": "2507.02453", "pdf": "https://arxiv.org/pdf/2507.02453.pdf", "abs": "https://arxiv.org/abs/2507.02453", "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?", "authors": ["Jueun Lee", "Martin Flipe", "Philipp Lepold", "Tobias R√∂ddiger", "Michael Beigl"], "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "Wearable haptic interventions offer promising support for relaxation through\nslow, vibrotactile biofeedback. Despite their potential, current applications\nfocus on stress-inducing procedures and fixed vibration patterns, with limited\nconsideration of body location and dynamic biofeedback during restful states.\nThis study investigates the effects of haptic biofeedback adjusted from\nreal-time heart rate during eyes-closed wakeful rest, comparing four wearable\nbody placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave\nactivity on the ear, subjective restfulness, and vibration experience were\nmeasured across these conditions. Results show that biofeedback reduced heart\nrate at the wrist, shoulder, and forearm, while alpha power measured at the ear\nremained unchanged. Subjective restfulness was rated highest at the shoulder\nand forearm, which were also the most preferred locations. In addition,\nparticipants reported greater comfort, relaxation, and further increased\nsleepiness at the forearm compared to the wrist, which was more easily\nrecognizable. These findings suggest that the forearm and shoulder are ideal\nfor unobtrusive relaxation feedback for wakeful rest, while the wrist may\nrequire design improvements for subjective experience."}
{"id": "2507.02744", "pdf": "https://arxiv.org/pdf/2507.02744.pdf", "abs": "https://arxiv.org/abs/2507.02744", "title": "Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens", "authors": ["Peter Viechnicki"], "categories": ["cs.CL"], "comment": null, "summary": "A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes."}
{"id": "2507.02537", "pdf": "https://arxiv.org/pdf/2507.02537.pdf", "abs": "https://arxiv.org/abs/2507.02537", "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "authors": ["Paulo Ricardo Knob", "Leonardo Scholler", "Juliano Rigatti", "Soraia Raupp Musse"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents."}
{"id": "2507.02778", "pdf": "https://arxiv.org/pdf/2507.02778.pdf", "abs": "https://arxiv.org/abs/2507.02778", "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs", "authors": ["Ken Tsui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "31 pages, 18 figures", "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness."}
{"id": "2507.02682", "pdf": "https://arxiv.org/pdf/2507.02682.pdf", "abs": "https://arxiv.org/abs/2507.02682", "title": "A wireless, inexpensive optical tracker for the CAVE", "authors": ["Ehud Sharlin", "Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "CAVE displays offer many advantages over other virtual reality (VR) displays,\nincluding a large, unencumbering viewing space. Unfortunately, the typical\ntracking subsystems used with CAVE displays tether the user and lessen this\nadvantage. We have designed a simple, low-cost feet tracker that is wireless,\nleaving the user free to move. The tracker can be assembled for less than $200\nUS, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested\nthe prototype with two applications: a visualization supporting close visual\ninspection, and a walkthrough of the campus. Although the tracking was\nconvincing, it was clear that the tracker's limitations make it less than ideal\nfor applications requiring precise visual inspection. However, the freedom of\nmotion allowed by the tracker was a compelling supplement to our campus\nwalkthrough, allowing users to stroll and look around corners."}
{"id": "2507.02799", "pdf": "https://arxiv.org/pdf/2507.02799.pdf", "abs": "https://arxiv.org/abs/2507.02799", "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models", "authors": ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design."}
{"id": "2507.02745", "pdf": "https://arxiv.org/pdf/2507.02745.pdf", "abs": "https://arxiv.org/abs/2507.02745", "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots", "authors": ["Zahra Ashktorab", "Alessandra Buccella", "Jason D'Cruz", "Zoe Fowler", "Andrew Gill", "Kei Yan Leung", "P. D. Magnus", "John Richards", "Kush R. Varshney"], "categories": ["cs.HC"], "comment": null, "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust."}
{"id": "2507.02804", "pdf": "https://arxiv.org/pdf/2507.02804.pdf", "abs": "https://arxiv.org/abs/2507.02804", "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective", "authors": ["Wenhao Shi", "Zhiqiang Hu", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Heng Tao Shen"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning."}
{"id": "2507.02800", "pdf": "https://arxiv.org/pdf/2507.02800.pdf", "abs": "https://arxiv.org/abs/2507.02800", "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding", "authors": ["Ebrahim Feghhi", "Shreyas Kaasyap", "Nima Hadidi", "Jonathan C. Kao"], "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Speech neuroprostheses aim to restore communication for people with severe\nparalysis by decoding speech directly from neural activity. To accelerate\nalgorithmic progress, a recent benchmark released intracranial recordings from\na paralyzed participant attempting to speak, along with a baseline decoding\nalgorithm. Prior work on the benchmark showed impressive accuracy gains.\nHowever, these gains increased computational costs and were not demonstrated in\na real-time decoding setting. Here, we make three contributions that pave the\nway towards accurate, efficient, and real-time neural speech decoding. First,\nwe incorporate large amounts of time masking during training. On average, over\n$50\\%$ of each trial is masked. Second, we replace the gated recurrent unit\n(GRU) architecture used in the baseline algorithm with a compact Transformer.\nThe Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory\nusage by $36\\%$ relative, and is significantly faster to calibrate relative to\nthe GRU. Third, we design a lightweight variant of an existing test-time\nadaptation method developed for decoding handwriting from neural activity. Our\nvariant adapts the model using multiple time masked augmentations of a single\ntrial and requires only one gradient step per trial. Together, these\ncontributions reduce word error rate by $19.5\\%$ and effectively mitigate\nperformance degradations across held-out days in a real-time decoding setting\nwhile substantially lowering computational costs."}
{"id": "2507.02822", "pdf": "https://arxiv.org/pdf/2507.02822.pdf", "abs": "https://arxiv.org/abs/2507.02822", "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model", "authors": ["Wencheng Zhang", "Shiqin Qiao", "Lingjie Luo", "Yinfeng Li", "Chuanyang Zheng", "Qian Xu", "Meng Li", "Yong Gui", "Yijun He", "Jianing Qiu", "Jindong Hong", "Jiankai Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost."}
{"id": "2507.02819", "pdf": "https://arxiv.org/pdf/2507.02819.pdf", "abs": "https://arxiv.org/abs/2507.02819", "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": null, "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction."}
{"id": "2507.02833", "pdf": "https://arxiv.org/pdf/2507.02833.pdf", "abs": "https://arxiv.org/abs/2507.02833", "title": "Generalizing Verifiable Instruction Following", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode."}
{"id": "2507.01968", "pdf": "https://arxiv.org/pdf/2507.01968.pdf", "abs": "https://arxiv.org/abs/2507.01968", "title": "Optimising task allocation to balance business goals and worker well-being for financial service workforces", "authors": ["Chris Duckworth", "Zlatko Zlatev", "James Sciberras", "Peter Hallett", "Enrico Gerding"], "categories": ["q-fin.GN", "cs.HC"], "comment": "Accepted in Journal of Modelling in Management", "summary": "Purpose: Financial service companies manage huge volumes of data which\nrequires timely error identification and resolution. The associated tasks to\nresolve these errors frequently put financial analyst workforces under\nsignificant pressure leading to resourcing challenges and increased business\nrisk. To address this challenge, we introduce a formal task allocation model\nwhich considers both business orientated goals and analyst well-being.\n  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to\nallocate and schedule tasks to analysts. The proposed solution is able to\nallocate tasks to analysts with appropriate skills and experience, while taking\ninto account staff well-being objectives.\n  Findings: We demonstrate our GA model outperforms baseline heuristics,\ncurrent working practice, and is applicable to a range of single and\nmulti-objective real-world scenarios. We discuss the potential for\nmetaheuristics (such as GAs) to efficiently find sufficiently good allocations\nwhich can provide recommendations for financial service managers in-the-loop.\n  Originality: A key gap in existing allocation and scheduling models, is fully\nconsidering worker well-being. This paper presents an allocation model which\nexplicitly optimises for well-being while still improving on current working\npractice for efficiency."}
{"id": "2507.02850", "pdf": "https://arxiv.org/pdf/2507.02850.pdf", "abs": "https://arxiv.org/abs/2507.02850", "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "authors": ["Almog Hilel", "Idan Shenfeld", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection)."}
{"id": "2507.02183", "pdf": "https://arxiv.org/pdf/2507.02183.pdf", "abs": "https://arxiv.org/abs/2507.02183", "title": "Computer Science Education in the Age of Generative AI", "authors": ["Russell Beale"], "categories": ["cs.CY", "cs.HC", "H.5.0; K.3.1; K.3.2"], "comment": null, "summary": "Generative AI tools - most notably large language models (LLMs) like ChatGPT\nand Codex - are rapidly revolutionizing computer science education. These tools\ncan generate, debug, and explain code, thereby transforming the landscape of\nprogramming instruction. This paper examines the profound opportunities that AI\noffers for enhancing computer science education in general, from coding\nassistance to fostering innovative pedagogical practices and streamlining\nassessments. At the same time, it highlights challenges including academic\nintegrity concerns, the risk of over-reliance on AI, and difficulties in\nverifying originality. We discuss what computer science educators should teach\nin the AI era, how to best integrate these technologies into curricula, and the\nbest practices for assessing student learning in an environment where AI can\ngenerate code, prototypes and user feedback. Finally, we propose a set of\npolicy recommendations designed to harness the potential of generative AI while\npreserving the integrity and rigour of computer science education. Empirical\ndata and emerging studies are used throughout to support our arguments."}
{"id": "2507.02851", "pdf": "https://arxiv.org/pdf/2507.02851.pdf", "abs": "https://arxiv.org/abs/2507.02851", "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "comment": null, "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively."}
{"id": "2507.02207", "pdf": "https://arxiv.org/pdf/2507.02207.pdf", "abs": "https://arxiv.org/abs/2507.02207", "title": "Public perspectives on the design of fusion energy facilities", "authors": ["Nathan Kawamoto", "Daniel Hoover", "Jonathan Xie", "Jacob Walters", "Katie Snyder", "Aditi Verma"], "categories": ["physics.soc-ph", "cs.HC", "physics.ed-ph", "physics.plasm-ph"], "comment": "33 pages", "summary": "As fusion energy technologies approach demonstration and commercial\ndeployment, understanding public perspectives on future fusion facilities will\nbe critical for achieving social license, especially because fusion energy\nfacilities, unlike large fission reactors, may be sited in closer proximity to\npeople and communities, due to distinct regulatory frameworks. In a departure\nfrom the 'decide-announce-defend' approach typically used to site energy\ninfrastructure, we develop a participatory design methodology for\ncollaboratively designing fusion energy facilities with prospective host\ncommunities. We present here our findings from a participatory design workshop\nthat brought together 22 community participants and 34 engineering students.\nOur analysis of the textual and visual data from this workshop shows a range of\ndesign values and decision-making criteria with 'integrity' and 'respect'\nranking highest among values and 'economic benefits' and 'environmental\nprotection/safety' ranking highest among decision-making criteria. Salient\ndesign themes that emerge across facility concepts include connecting the\nhistory and legacy of the community to the design of the facility, care for\nworkers, transparency and access to the facility, and health and safety of the\nhost community. Participants reported predominantly positive sentiments,\nexpressing joy and surprise as the workshop progressed from learning about\nfusion to designing the hypothetical facility. Our findings suggest that\ncarrying out participatory design in the early stages of technology development\ncan invite and make concrete public hopes and concerns, improve understanding\nof, and curiosity about, an emerging technology, build toward social license,\nand inform context-specific development of fusion energy facilities."}
{"id": "2507.02856", "pdf": "https://arxiv.org/pdf/2507.02856.pdf", "abs": "https://arxiv.org/abs/2507.02856", "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching", "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching."}
{"id": "2507.02320", "pdf": "https://arxiv.org/pdf/2507.02320.pdf", "abs": "https://arxiv.org/abs/2507.02320", "title": "Transformer-based EEG Decoding: A Survey", "authors": ["Haodong Zhang", "Hongqi Li"], "categories": ["cs.LG", "cs.HC"], "comment": "Submitted to IEEE Journals", "summary": "Electroencephalography (EEG) is one of the most common signals used to\ncapture the electrical activity of the brain, and the decoding of EEG, to\nacquire the user intents, has been at the forefront of brain-computer/machine\ninterfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods\nwith machine learning, the advent of deep learning approaches have gradually\nrevolutionized the field by providing an end-to-end long-cascaded architecture,\nwhich can learn more discriminative features automatically. Among these,\nTransformer is renowned for its strong handling capability of sequential data\nby the attention mechanism, and the application of Transformers in various EEG\nprocessing tasks is increasingly prevalent. This article delves into a relevant\nsurvey, summarizing the latest application of Transformer models in EEG\ndecoding since it appeared. The evolution of the model architecture is followed\nto sort and organize the related advances, in which we first elucidate the\nfundamentals of the Transformer that benefits EEG decoding and its direct\napplication. Then, the common hybrid architectures by integrating basic\nTransformer with other deep learning techniques\n(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial\nnetworks, diffusion models, etc.) is overviewed in detail. The research\nadvances of applying the modified intrinsic structures of customized\nTransformer have also been introduced. Finally, the current challenges and\nfuture development prospects in this rapidly evolving field are discussed. This\npaper aims to help readers gain a clear understanding of the current state of\nTransformer applications in EEG decoding and to provide valuable insights for\nfuture research endeavors."}
{"id": "2507.01984", "pdf": "https://arxiv.org/pdf/2507.01984.pdf", "abs": "https://arxiv.org/abs/2507.01984", "title": "Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features", "authors": ["Gautam Kishore Shahi"], "categories": ["cs.LG", "cs.CL", "cs.SI"], "comment": null, "summary": "Amid a tidal wave of misinformation flooding social media during elections\nand crises, extensive research has been conducted on misinformation detection,\nprimarily focusing on text-based or image-based approaches. However, only a few\nstudies have explored multimodal feature combinations, such as integrating text\nand images for building a classification model to detect misinformation. This\nstudy investigates the effectiveness of different multimodal feature\ncombinations, incorporating text, images, and social features using an early\nfusion approach for the classification model. This study analyzed 1,529 tweets\ncontaining both text and images during the COVID-19 pandemic and election\nperiods collected from Twitter (now X). A data enrichment process was applied\nto extract additional social features, as well as visual features, through\ntechniques such as object detection and optical character recognition (OCR).\nThe results show that combining unsupervised and supervised machine learning\nmodels improves classification performance by 15% compared to unimodal models\nand by 5% compared to bimodal models. Additionally, the study analyzes the\npropagation patterns of misinformation based on the characteristics of\nmisinformation tweets and the users who disseminate them."}
{"id": "2507.02400", "pdf": "https://arxiv.org/pdf/2507.02400.pdf", "abs": "https://arxiv.org/abs/2507.02400", "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand M√ºtsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian M√ºller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias K√∂lle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Z√∂llner", "J. Marius Z√∂llner"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the IEEE IAVVC 2025 Conference", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector."}
{"id": "2507.01991", "pdf": "https://arxiv.org/pdf/2507.01991.pdf", "abs": "https://arxiv.org/abs/2507.01991", "title": "FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports", "authors": ["Muhammad Bilal Zafar"], "categories": ["q-fin.CP", "cs.CL", "econ.GN", "q-fin.EC", "q-fin.GN"], "comment": "The FinAI-BERT model can be directly loaded via Hugging Face\n  Transformers (https://huggingface.co/bilalzafar/FinAI-BERT) for\n  sentence-level AI disclosure classification", "summary": "The proliferation of artificial intelligence (AI) in financial services has\nprompted growing demand for tools that can systematically detect AI-related\ndisclosures in corporate filings. While prior approaches often rely on keyword\nexpansion or document-level classification, they fall short in granularity,\ninterpretability, and robustness. This study introduces FinAI-BERT, a\ndomain-adapted transformer-based language model designed to classify AI-related\ncontent at the sentence level within financial texts. The model was fine-tuned\non a manually curated and balanced dataset of 1,586 sentences drawn from 669\nannual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect\nclassification performance (accuracy of 99.37 percent, F1 score of 0.993),\noutperforming traditional baselines such as Logistic Regression, Naive Bayes,\nRandom Forest, and XGBoost. Interpretability was ensured through SHAP-based\ntoken attribution, while bias analysis and robustness checks confirmed the\nmodel's stability across sentence lengths, adversarial inputs, and temporal\nsamples. Theoretically, the study advances financial NLP by operationalizing\nfine-grained, theme-specific classification using transformer architectures.\nPractically, it offers a scalable, transparent solution for analysts,\nregulators, and scholars seeking to monitor the diffusion and framing of AI\nacross financial institutions."}
{"id": "2507.02438", "pdf": "https://arxiv.org/pdf/2507.02438.pdf", "abs": "https://arxiv.org/abs/2507.02438", "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent."}
{"id": "2507.02000", "pdf": "https://arxiv.org/pdf/2507.02000.pdf", "abs": "https://arxiv.org/abs/2507.02000", "title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System", "authors": ["Yongsen Zheng", "Zongxuan Xie", "Guohua Wang", "Ziyao Liu", "Liang Lin", "Kwok-Yan Lam"], "categories": ["cs.IR", "cs.CL", "cs.MM"], "comment": null, "summary": "Unfairness is a well-known challenge in Recommender Systems (RSs), often\nresulting in biased outcomes that disadvantage users or items based on\nattributes such as gender, race, age, or popularity. Although some approaches\nhave started to improve fairness recommendation in offline or static contexts,\nthe issue of unfairness often exacerbates over time, leading to significant\nproblems like the Matthew effect, filter bubbles, and echo chambers. To address\nthese challenges, we proposed a novel framework, Hypergraph Contrastive\nMulti-Interest Learning for Fair Conversational Recommender System (HyFairCRS),\naiming to promote multi-interest diversity fairness in dynamic and interactive\nConversational Recommender Systems (CRSs). HyFairCRS first captures a wide\nrange of user interests by establishing diverse hypergraphs through contrastive\nlearning. These interests are then utilized in conversations to generate\ninformative responses and ensure fair item predictions within the dynamic\nuser-system feedback loop. Experiments on two CRS-based datasets show that\nHyFairCRS achieves a new state-of-the-art performance while effectively\nalleviating unfairness. Our code is available at\nhttps://github.com/zysensmile/HyFairCRS."}
{"id": "2507.02510", "pdf": "https://arxiv.org/pdf/2507.02510.pdf", "abs": "https://arxiv.org/abs/2507.02510", "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification", "authors": ["Ahmed G. Habashi", "Ahmed M. Azab", "Seif Eldawlatly", "Gamal M. Aly"], "categories": ["cs.LG", "cs.HC", "cs.NE"], "comment": null, "summary": "Cross-subject motor imagery (CS-MI) classification in brain-computer\ninterfaces (BCIs) is a challenging task due to the significant variability in\nElectroencephalography (EEG) patterns across different individuals. This\nvariability often results in lower classification accuracy compared to\nsubject-specific models, presenting a major barrier to developing\ncalibration-free BCIs suitable for real-world applications. In this paper, we\nintroduce a novel approach that significantly enhances cross-subject MI\nclassification performance through optimized preprocessing and deep learning\ntechniques. Our approach involves direct classification of Short-Time Fourier\nTransform (STFT)-transformed EEG data, optimized STFT parameters, and a\nbalanced batching strategy during training of a Convolutional Neural Network\n(CNN). This approach is uniquely validated across four different datasets,\nincluding three widely-used benchmark datasets leading to substantial\nimprovements in cross-subject classification, achieving 67.60% on the BCI\nCompetition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on\nDataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we\nsystematically investigate the classification performance using MI windows\nranging from the full 4-second window to 1-second windows. These results\nestablish a new benchmark for generalizable, calibration-free MI classification\nin addition to contributing a robust open-access dataset to advance research in\nthis domain."}
{"id": "2507.02004", "pdf": "https://arxiv.org/pdf/2507.02004.pdf", "abs": "https://arxiv.org/abs/2507.02004", "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery."}
{"id": "2507.02578", "pdf": "https://arxiv.org/pdf/2507.02578.pdf", "abs": "https://arxiv.org/abs/2507.02578", "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems", "authors": ["Zoe Pfister"], "categories": ["cs.SE", "cs.HC", "D.2.1"], "comment": "Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd\n  International Requirements Engineering Conference (RE), Doctor Symposium\n  Paper, 5 pages", "summary": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both\nphysical and computational capabilities, which can adjust in response to\nchanging parameters. Furthermore, they increasingly incorporate human-machine\ncollaboration, allowing them to benefit from the individual strengths of humans\nand machines. Human-Machine Teaming (HMT) represents the most advanced paradigm\nof human-machine collaboration, envisioning seamless teamwork between humans\nand machines. However, achieving effective and seamless HMT in adaptive CPS is\nchallenging. While adaptive CPS already benefit from feedback loops such as\nMAPE-K, there is still a gap in integrating humans into these feedback loops\ndue to different operational cadences of humans and machines. Further, HMT\nrequires constant monitoring of human operators, collecting potentially\nsensitive information about their actions and behavior. Respecting the privacy\nand human values of the actors of the CPS is crucial for the success of\nhuman-machine teams. This research addresses these challenges by: (1)\ndeveloping novel methods and processes for integrating HMT into adaptive CPS,\nfocusing on human-machine interaction principles and their incorporation into\nadaptive feedback loops found in CPS, and (2) creating frameworks for\nintegrating, verifying, and validating ethics and human values throughout the\nsystem lifecycle, starting from requirements engineering."}
{"id": "2507.02087", "pdf": "https://arxiv.org/pdf/2507.02087.pdf", "abs": "https://arxiv.org/abs/2507.02087", "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions", "authors": ["Eitan Anzenberg", "Arunava Samajpati", "Sivasankaran Chandrasekar", "Varun Kacholia"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes."}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593.pdf", "abs": "https://arxiv.org/abs/2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G√∂ran Kauermann", "Barbara Plank", "Matthias A√üenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation."}
{"id": "2507.02092", "pdf": "https://arxiv.org/pdf/2507.02092.pdf", "abs": "https://arxiv.org/abs/2507.02092", "title": "Energy-Based Transformers are Scalable Learners and Thinkers", "authors": ["Alexi Gladstone", "Ganesh Nanduru", "Md Mofijul Islam", "Peixuan Han", "Hyeonjeong Ha", "Aman Chadha", "Yilun Du", "Heng Ji", "Jundong Li", "Tariq Iqbal"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models."}
{"id": "2407.16206", "pdf": "https://arxiv.org/pdf/2407.16206.pdf", "abs": "https://arxiv.org/abs/2407.16206", "title": "Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts", "authors": ["Michikuni Eguchi", "Tomohiro Hayase", "Yuichi Hiroi", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "dataset: https://doi.org/10.6084/m9.figshare.29438288 code:\n  https://github.com/cluster-lab/Cluster-Haptic-Texture-Database", "summary": "Haptic sciences and technologies benefit greatly from comprehensive datasets\nthat capture tactile stimuli under controlled, systematic conditions. However,\nexisting haptic databases collect data through uncontrolled exploration, which\nhinders the systematic analysis of how motion parameters (e.g., motion\ndirection and velocity) influence tactile perception. This paper introduces\nCluster Haptic Texture Database, a multimodal dataset recorded using a 3-axis\nmachine with an artificial finger to precisely control sliding velocity and\ndirection. The dataset encompasses 118 textured surfaces across 9 material\ncategories, with recordings at 5 velocity levels (20-60 mm/s) and 8 directions.\nEach surface was tested under 160 conditions, yielding 18,880 synchronized\nrecordings of audio, acceleration, force, position, and visual data. Validation\nusing convolutional neural networks demonstrates classification accuracies of\n96% for texture recognition, 88.76% for velocity estimation, and 78.79% for\ndirection estimation, confirming the dataset's utility for machine learning\napplications. This resource enables research in haptic rendering, texture\nrecognition algorithms, and human tactile perception mechanisms, supporting the\ndevelopment of realistic haptic interfaces for virtual reality systems and\nrobotic applications."}
{"id": "2507.02135", "pdf": "https://arxiv.org/pdf/2507.02135.pdf", "abs": "https://arxiv.org/abs/2507.02135", "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency", "authors": ["Zongpu Zhang", "Pranab Dash", "Y. Charlie Hu", "Qiang Xu", "Jian Li", "Haibing Guan"], "categories": ["cs.OS", "cs.CL"], "comment": "equal contribution between Zhang and Dash", "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels."}
{"id": "2410.08723", "pdf": "https://arxiv.org/pdf/2410.08723.pdf", "abs": "https://arxiv.org/abs/2410.08723", "title": "Human-Computer Interaction and Visualization in Natural Language Generation Models: Applications, Challenges, and Opportunities", "authors": ["Yunchao Wang", "Guodao Sun", "Zihang Fu", "Ronghua Liang"], "categories": ["cs.HC"], "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50356-6}", "summary": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs)."}
{"id": "2507.02176", "pdf": "https://arxiv.org/pdf/2507.02176.pdf", "abs": "https://arxiv.org/abs/2507.02176", "title": "Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis", "authors": ["Marc-Andr√© Carbonneau", "Benjamin van Niekerk", "Hugo Seut√©", "Jean-Philippe Letendre", "Herman Kamper", "Julian Za√Ødi"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "summary": "Modeling voice identity is challenging due to its multifaceted nature. In\ngenerative speech systems, identity is often assessed using automatic speaker\nverification (ASV) embeddings, designed for discrimination rather than\ncharacterizing identity. This paper investigates which aspects of a voice are\ncaptured in such representations. We find that widely used ASV embeddings focus\nmainly on static features like timbre and pitch range, while neglecting dynamic\nelements such as rhythm. We also identify confounding factors that compromise\nspeaker similarity measurements and suggest mitigation strategies. To address\nthese gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm\npatterns. This work contributes to the ongoing challenge of assessing speaker\nidentity consistency in the context of ever-better voice cloning systems. We\npublicly release our code."}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."}
{"id": "2507.02200", "pdf": "https://arxiv.org/pdf/2507.02200.pdf", "abs": "https://arxiv.org/abs/2507.02200", "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT."}
{"id": "2407.06902", "pdf": "https://arxiv.org/pdf/2407.06902.pdf", "abs": "https://arxiv.org/abs/2407.06902", "title": "Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective", "authors": ["Shahana Ibrahim", "Panagiotis A. Traganitis", "Xiao Fu", "Georgios B. Giannakis"], "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "One of the primary catalysts fueling advances in artificial intelligence (AI)\nand machine learning (ML) is the availability of massive, curated datasets. A\ncommonly used technique to curate such massive datasets is crowdsourcing, where\ndata are dispatched to multiple annotators. The annotator-produced labels are\nthen fused to serve downstream learning and inference tasks. This annotation\nprocess often creates noisy labels due to various reasons, such as the limited\nexpertise, or unreliability of annotators, among others. Therefore, a core\nobjective in crowdsourcing is to develop methods that effectively mitigate the\nnegative impact of such label noise on learning tasks. This feature article\nintroduces advances in learning from noisy crowdsourced labels. The focus is on\nkey crowdsourcing models and their methodological treatments, from classical\nstatistical models to recent deep learning-based approaches, emphasizing\nanalytical insights and algorithmic developments. In particular, this article\nreviews the connections between signal processing (SP) theory and methods, such\nas identifiability of tensor and nonnegative matrix factorization, and novel,\nprincipled solutions of longstanding challenges in crowdsourcing -- showing how\nSP perspectives drive the advancements of this field. Furthermore, this article\ntouches upon emerging topics that are critical for developing cutting-edge\nAI/ML systems, such as crowdsourcing in reinforcement learning with human\nfeedback (RLHF) and direct preference optimization (DPO) that are key\ntechniques for fine-tuning large language models (LLMs)."}
{"id": "2507.02212", "pdf": "https://arxiv.org/pdf/2507.02212.pdf", "abs": "https://arxiv.org/abs/2507.02212", "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science."}
{"id": "2412.18716", "pdf": "https://arxiv.org/pdf/2412.18716.pdf", "abs": "https://arxiv.org/abs/2412.18716", "title": "Design and Evaluation of Privacy-Preserving Protocols for Agent-Facilitated Mobile Money Services in Kenya", "authors": ["Karen Sowon", "Collins W. Munyendo", "Lily Klucinec", "Eunice Maingi", "Gerald Suleh", "Lorrie Faith Cranor", "Giulia Fanti", "Conrad Tucker", "Assane Gueye"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Mobile Money (MoMo), a technology that allows users to complete financial\ntransactions using a mobile phone without requiring a bank account, is a common\nmethod for processing financial transactions in Africa and other developing\nregions. Users can deposit and withdraw money with the help of human agents.\nDuring deposit and withdraw operations, know-your-customer (KYC) processes\nrequire agents to access and verify customer information such as name and ID\nnumber, which can introduce privacy and security risks. In this work, we design\nalternative protocols for MoMo deposits/withdrawals that protect users' privacy\nwhile enabling KYC checks by redirecting the flow of sensitive information from\nthe agent to the MoMo provider. We evaluate the usability and efficiency of our\nproposed protocols in a role-play and semi-structured interview study with 32\nusers and 15 agents in Kenya. We find that users and agents prefer the new\nprotocols, due in part to convenient and efficient verification using\nbiometrics as well as better data privacy and access control. However, our\nstudy also surfaced challenges that need to be addressed before these protocols\ncan be deployed."}
{"id": "2507.02287", "pdf": "https://arxiv.org/pdf/2507.02287.pdf", "abs": "https://arxiv.org/abs/2507.02287", "title": "Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents", "authors": ["Lapo Santarlasci", "Armando Rungi", "Antonio Zinilli"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "This paper introduces Natural Language Processing for identifying ``true''\ngreen patents from official supporting documents. We start our training on\nabout 12.4 million patents that had been classified as green from previous\nliterature. Thus, we train a simple neural network to enlarge a baseline\ndictionary through vector representations of expressions related to\nenvironmental technologies. After testing, we find that ``true'' green patents\nrepresent about 20\\% of the total of patents classified as green from previous\nliterature. We show heterogeneity by technological classes, and then check that\n`true' green patents are about 1\\% less cited by following inventions. In the\nsecond part of the paper, we test the relationship between patenting and a\ndashboard of firm-level financial accounts in the European Union. After\ncontrolling for reverse causality, we show that holding at least one ``true''\ngreen patent raises sales, market shares, and productivity. If we restrict the\nanalysis to high-novelty ``true'' green patents, we find that they also yield\nhigher profits. Our findings underscore the importance of using text analyses\nto gauge finer-grained patent classifications that are useful for policymaking\nin different domains."}
{"id": "2503.08061", "pdf": "https://arxiv.org/pdf/2503.08061.pdf", "abs": "https://arxiv.org/abs/2503.08061", "title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation", "authors": ["DongHeun Han", "Byungmin Kim", "RoUn Lee", "KyeongMin Kim", "Hyoseok Hwang", "HyeongYeop Kang"], "categories": ["cs.RO", "cs.GR", "cs.HC", "cs.LG"], "comment": "11 pages, 11 figures. Accepted to SIGGRAPH Conference Papers '25.\n  Project page: https://han-dongheun.github.io/ForceGrip", "summary": "Realistic Hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. Demo videos are available as supplementary material and the code is\nprovided at https://han-dongheun.github.io/ForceGrip."}
{"id": "2507.02380", "pdf": "https://arxiv.org/pdf/2507.02380.pdf", "abs": "https://arxiv.org/abs/2507.02380", "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning", "authors": ["Fangru Zhou", "Jun Zhao", "Guoxin Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git."}
{"id": "2503.17046", "pdf": "https://arxiv.org/pdf/2503.17046.pdf", "abs": "https://arxiv.org/abs/2503.17046", "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences", "authors": ["Dongsheng Yang", "Qianying Liu", "Wataru Sato", "Takashi Minato", "Chaoran Liu", "Shin'ya Nishida"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses."}
{"id": "2507.02618", "pdf": "https://arxiv.org/pdf/2507.02618.pdf", "abs": "https://arxiv.org/abs/2507.02618", "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory", "authors": ["Kenneth Payne", "Baptiste Alloui-Cros"], "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": "29 pages, 27 tables, 4 figures", "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty."}
{"id": "2507.02652", "pdf": "https://arxiv.org/pdf/2507.02652.pdf", "abs": "https://arxiv.org/abs/2507.02652", "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yang Zhao", "Hongjin Qian", "Zhicheng Dou"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "9 pages", "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA."}
{"id": "2507.02659", "pdf": "https://arxiv.org/pdf/2507.02659.pdf", "abs": "https://arxiv.org/abs/2507.02659", "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding", "authors": ["Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Shaojie Zhuo", "Chen Feng", "Yicheng Lin", "Chenzheng Su", "Xiaopeng Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."}
{"id": "2507.02666", "pdf": "https://arxiv.org/pdf/2507.02666.pdf", "abs": "https://arxiv.org/abs/2507.02666", "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning", "authors": ["Junyu Wang", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech2025", "summary": "In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications."}
{"id": "2507.02737", "pdf": "https://arxiv.org/pdf/2507.02737.pdf", "abs": "https://arxiv.org/abs/2507.02737", "title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "authors": ["Artur Zolkowski", "Kei Nishimura-Gasparian", "Robert McCarthy", "Roland S. Zimmermann", "David Lindner"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future."}
{"id": "2507.02768", "pdf": "https://arxiv.org/pdf/2507.02768.pdf", "abs": "https://arxiv.org/abs/2507.02768", "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment", "authors": ["Ke-Han Lu", "Zhehuai Chen", "Szu-Wei Fu", "Chao-Han Huck Yang", "Sung-Feng Huang", "Chih-Kai Yang", "Chee-En Yu", "Chun-Wei Chen", "Wei-Chih Chen", "Chien-yu Huang", "Yi-Cheng Lin", "Yu-Xiang Lin", "Chi-An Fu", "Chun-Yi Kuan", "Wenze Ren", "Xuanjun Chen", "Wei-Ping Huang", "En-Pei Hu", "Tzu-Quan Lin", "Yuan-Kuei Wu", "Kuan-Po Huang", "Hsiao-Ying Huang", "Huang-Cheng Chou", "Kai-Wei Chang", "Cheng-Han Chiang", "Boris Ginsburg", "Yu-Chiang Frank Wang", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio", "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs."}
{"id": "2507.02790", "pdf": "https://arxiv.org/pdf/2507.02790.pdf", "abs": "https://arxiv.org/abs/2507.02790", "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding", "authors": ["Xiangfeng Wang", "Xiao Li", "Yadong Wei", "Xueyu Song", "Yang Song", "Xiaoqiang Xia", "Fangrui Zeng", "Zaiyi Chen", "Liu Liu", "Gu Xu", "Tong Xu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos."}
{"id": "2507.02834", "pdf": "https://arxiv.org/pdf/2507.02834.pdf", "abs": "https://arxiv.org/abs/2507.02834", "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning", "authors": ["Ruiyang Zhou", "Shuozhe Li", "Amy Zhang", "Liu Leqi"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most."}
{"id": "2507.02841", "pdf": "https://arxiv.org/pdf/2507.02841.pdf", "abs": "https://arxiv.org/abs/2507.02841", "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason", "authors": ["Kaiyi Zhang", "Ang Lv", "Jinpeng Li", "Yongbo Wang", "Feng Wang", "Haoyuan Hu", "Rui Yan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks."}
{"id": "2507.02844", "pdf": "https://arxiv.org/pdf/2507.02844.pdf", "abs": "https://arxiv.org/abs/2507.02844", "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "16 pages", "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack."}
{"id": "2507.02846", "pdf": "https://arxiv.org/pdf/2507.02846.pdf", "abs": "https://arxiv.org/abs/2507.02846", "title": "Legal Requirements Translation from Law", "authors": ["Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively."}
{"id": "2507.02858", "pdf": "https://arxiv.org/pdf/2507.02858.pdf", "abs": "https://arxiv.org/abs/2507.02858", "title": "Requirements Elicitation Follow-Up Question Generation", "authors": ["Yuchen Shen", "Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time."}
{"id": "2306.13840", "pdf": "https://arxiv.org/pdf/2306.13840.pdf", "abs": "https://arxiv.org/abs/2306.13840", "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data", "authors": ["Brando Miranda", "Alycia Lee", "Sudharsan Sundar", "Allison Casasola", "Rylan Schaeffer", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance."}
{"id": "2311.08010", "pdf": "https://arxiv.org/pdf/2311.08010.pdf", "abs": "https://arxiv.org/abs/2311.08010", "title": "Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning", "authors": ["Shuzheng Si", "Helan Hu", "Haozhe Zhao", "Shuang Zeng", "Kaikai An", "Zefan Cai", "Baobao Chang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2024 (Findings)", "summary": "Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in\nreal-world scenarios. It can effectively alleviate the burden of annotation by\nmatching entities in existing knowledge bases with snippets in the text but\nsuffer from the label noise. Recent works attempt to adopt the teacher-student\nframework to gradually refine the training labels and improve the overall\nrobustness. However, these teacher-student methods achieve limited performance\nbecause the poor calibration of the teacher network produces incorrectly\npseudo-labeled samples, leading to error propagation. Therefore, we propose:\n(1) Uncertainty-Aware Teacher Learning that leverages the prediction\nuncertainty to reduce the number of incorrect pseudo labels in the\nself-training stage; (2) Student-Student Collaborative Learning that allows the\ntransfer of reliable labels between two student networks instead of\nindiscriminately relying on all pseudo labels from its teacher, and further\nenables a full exploration of mislabeled samples rather than simply filtering\nunreliable pseudo-labeled samples. We evaluate our proposed method on five\nDS-NER datasets, demonstrating that our method is superior to the\nstate-of-the-art DS-NER methods."}
{"id": "2311.14727", "pdf": "https://arxiv.org/pdf/2311.14727.pdf", "abs": "https://arxiv.org/abs/2311.14727", "title": "Optimal strategies to perform multilingual analysis of social content for a novel dataset in the tourism domain", "authors": ["Maxime Masson", "Rodrigo Agerri", "Christian Sallaberry", "Marie-Noelle Bessagnet", "Annig Le Parc Lacayrelle", "Philippe Roose"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rising influence of social media platforms in various domains, including\ntourism, has highlighted the growing need for efficient and automated Natural\nLanguage Processing (NLP) strategies to take advantage of this valuable\nresource. However, the transformation of multilingual, unstructured, and\ninformal texts into structured knowledge still poses significant challenges,\nmost notably the never-ending requirement for manually annotated data to train\ndeep learning classifiers. In this work, we study different NLP techniques to\nestablish the best ones to obtain competitive performances while keeping the\nneed for training annotated data to a minimum. To do so, we built the first\npublicly available multilingual dataset (French, English, and Spanish) for the\ntourism domain, composed of tourism-related tweets. The dataset includes\nmultilayered, manually revised annotations for Named Entity Recognition (NER)\nfor Locations and Fine-grained Thematic Concepts Extraction mapped to the\nThesaurus of Tourism and Leisure Activities of the World Tourism Organization,\nas well as for Sentiment Analysis at the tweet level. Extensive experimentation\ncomparing various few-shot and fine-tuning techniques with modern language\nmodels demonstrate that modern few-shot techniques allow us to obtain\ncompetitive results for all three tasks with very little annotation data: 5\ntweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named\nEntity Recognition of Locations and 1K tweets annotated with fine-grained\nthematic concepts, a highly fine-grained sequence labeling task based on an\ninventory of 315 classes. We believe that our results, grounded in a novel\ndataset, pave the way for applying NLP to new domain-specific applications,\nreducing the need for manual annotations and circumventing the complexities of\nrule-based, ad-hoc solutions."}
{"id": "2406.07016", "pdf": "https://arxiv.org/pdf/2406.07016.pdf", "abs": "https://arxiv.org/abs/2406.07016", "title": "Delving into LLM-assisted writing in biomedical publications through excess vocabulary", "authors": ["Dmitry Kobak", "Rita Gonz√°lez-M√°rquez", "Em≈ëke-√Ågnes Horv√°t", "Jan Lause"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.SI"], "comment": "v5: Reverting to v3", "summary": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic."}
{"id": "2408.01119", "pdf": "https://arxiv.org/pdf/2408.01119.pdf", "abs": "https://arxiv.org/abs/2408.01119", "title": "Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer", "authors": ["Robert Belanec", "Simon Ostermann", "Ivan Srba", "Maria Bielikova"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks."}
{"id": "2410.12532", "pdf": "https://arxiv.org/pdf/2410.12532.pdf", "abs": "https://arxiv.org/abs/2410.12532", "title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration", "authors": ["Dingkang Yang", "Jinjie Wei", "Mingcheng Li", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junjun He", "Yakun Ju", "Wei Zhou", "Yang Liu", "Lihua Zhang"], "categories": ["cs.CL"], "comment": "LLM-based Multi-Agent Collaboration for Medical Applications", "summary": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning."}
{"id": "2410.13808", "pdf": "https://arxiv.org/pdf/2410.13808.pdf", "abs": "https://arxiv.org/abs/2410.13808", "title": "De-mark: Watermark Removal in Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks."}
{"id": "2411.00863", "pdf": "https://arxiv.org/pdf/2411.00863.pdf", "abs": "https://arxiv.org/abs/2411.00863", "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation", "authors": ["Chenyang An", "Shima Imani", "Feng Yao", "Chengyu Dong", "Ali Abbasi", "Harsh Shrivastava", "Samuel Buss", "Jingbo Shang", "Gayathri Mahalingam", "Pramod Sharma", "Maurice Diesendruck"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix."}
{"id": "2411.16765", "pdf": "https://arxiv.org/pdf/2411.16765.pdf", "abs": "https://arxiv.org/abs/2411.16765", "title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction", "authors": ["Shester Gueuwou", "Xiaodan Du", "Greg Shakhnarovich", "Karen Livescu", "Alexander H. Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "Fixed Figure 1. ACL 2025", "summary": "Sign language processing has traditionally relied on task-specific models,\nlimiting the potential for transfer learning across tasks. Pre-training methods\nfor sign language have typically focused on either supervised pre-training,\nwhich cannot take advantage of unlabeled data, or context-independent (frame or\nvideo segment) representations, which ignore the effects of relationships\nacross time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a\nself-supervised contextual representation model learned from approximately\n1,000 hours of American Sign Language video. SHuBERT adapts masked token\nprediction objectives to multi-stream visual sign language input, learning to\npredict multiple targets corresponding to clustered hand, face, and body pose\nstreams. SHuBERT achieves state-of-the-art performance across multiple tasks\nincluding sign language translation, isolated sign language recognition, and\nfingerspelling detection."}
{"id": "2412.05693", "pdf": "https://arxiv.org/pdf/2412.05693.pdf", "abs": "https://arxiv.org/abs/2412.05693", "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression", "authors": ["Michael R. Metel", "Boxing Chen", "Mehdi Rezagholizadeh"], "categories": ["cs.CL"], "comment": null, "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."}
{"id": "2412.11556", "pdf": "https://arxiv.org/pdf/2412.11556.pdf", "abs": "https://arxiv.org/abs/2412.11556", "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs", "authors": ["Yuchen Fu", "Zifeng Cheng", "Zhiwei Jiang", "Zhonghui Wang", "Yafeng Yin", "Zhengliang Li", "Qing Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accept to ACL 2025 (Oral). Code are available on\n  https://github.com/fuyuchenIfyw/token_prepending.git", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost."}
{"id": "2501.03262", "pdf": "https://arxiv.org/pdf/2501.03262.pdf", "abs": "https://arxiv.org/abs/2501.03262", "title": "REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models", "authors": ["Jian Hu", "Xibin Wu", "Wei Shen", "Jason Klein Liu", "Zilin Zhu", "Weixun Wang", "Songlin Jiang", "Haoran Wang", "Hao Chen", "Bin Chen", "Weikai Fang", "Xianyu", "Yu Cao", "Haotian Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "fix typo", "summary": "Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR)\nsignificantly improve the alignment of human-AI values and further raise the\nupper bound of AI capabilities, particularly in reasoning-intensive,\nlong-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or\nRLVR) frameworks commonly face challenges such as inference bottlenecks and\ncomplexity barriers, restricting their accessibility for newcomers. To bridge\nthis gap, we introduce \\textbf{OpenRLHF}, a user-friendly, scalable, and\neasy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and\nHuggingFace Transformers, featuring a simplified design, clear code structure,\nand comprehensive documentation to facilitate entry for researchers and\npractitioners. Experimental results show that OpenRLHF achieves superior\ntraining efficiency with speedups ranging from 1.22x to 1.68x across different\nmodel sizes compared to state-of-the-art frameworks, while requiring\nsignificantly fewer lines of code for implementation. OpenRLHF is publicly\navailable at https://github.com/OpenRLHF/OpenRLHF, and has already been adopted\nby leading institutions to accelerate RLHF research and learning."}
{"id": "2501.08496", "pdf": "https://arxiv.org/pdf/2501.08496.pdf", "abs": "https://arxiv.org/abs/2501.08496", "title": "Quantifying the Importance of Data Alignment in Downstream Model Performance", "authors": ["Krrish Chawla", "Aryan Sahai", "Mario DePavia", "Sudharsan Sundar", "Brando Miranda", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization."}
{"id": "2502.11268", "pdf": "https://arxiv.org/pdf/2502.11268.pdf", "abs": "https://arxiv.org/abs/2502.11268", "title": "Improved Unbiased Watermark for Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts."}
{"id": "2503.00958", "pdf": "https://arxiv.org/pdf/2503.00958.pdf", "abs": "https://arxiv.org/abs/2503.00958", "title": "Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers", "authors": ["Milad Alshomary", "Nikhil Reddy Varimalla", "Vishal Anand", "Smaranda Muresan", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a new approach for the authorship attribution task that leverages\nthe various linguistic representations learned at different layers of\npre-trained transformer-based models. We evaluate our approach on three\ndatasets, comparing it to a state-of-the-art baseline in in-domain and\nout-of-domain scenarios. We found that utilizing various transformer layers\nimproves the robustness of authorship attribution models when tested on\nout-of-domain data, resulting in new state-of-the-art results. Our analysis\ngives further insights into how our model's different layers get specialized in\nrepresenting certain stylistic features that benefit the model when tested out\nof the domain."}
{"id": "2503.18681", "pdf": "https://arxiv.org/pdf/2503.18681.pdf", "abs": "https://arxiv.org/abs/2503.18681", "title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models", "authors": ["Yazhou Zhang", "Chunwang Zou", "Bo Wang", "Jing Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Our original goal was to use Commander-GPT: Dividing and Routing for\n  Multimodal Sarcasm Detection (arXiv:2506.19420) to replace Commander-GPT:\n  Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large\n  Language Models (arXiv:2503.18681). Due to various reasons, both versions\n  were released, so we would like to withdraw the latter", "summary": "Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales."}
{"id": "2504.12816", "pdf": "https://arxiv.org/pdf/2504.12816.pdf", "abs": "https://arxiv.org/abs/2504.12816", "title": "SMARTe: Slot-based Method for Accountable Relational Triple extraction", "authors": ["Xue Wen Tan", "Stanley Kok"], "categories": ["cs.CL"], "comment": null, "summary": "Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research. Our code\nis available at https://github.com/Chen-XueWen/SMARTe."}
{"id": "2505.13886", "pdf": "https://arxiv.org/pdf/2505.13886.pdf", "abs": "https://arxiv.org/abs/2505.13886", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "63 pages, 23 figures, submitted to NeurIPS 2025", "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic."}
{"id": "2505.15075", "pdf": "https://arxiv.org/pdf/2505.15075.pdf", "abs": "https://arxiv.org/abs/2505.15075", "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs", "authors": ["Hao Wang", "Pinzhi Huang", "Jihan Yang", "Saining Xie", "Daisuke Kawahara"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "https://github.com/nlp-waseda/traveling-across-languages", "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models."}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618.pdf", "abs": "https://arxiv.org/abs/2505.22618", "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2506.00612", "pdf": "https://arxiv.org/pdf/2506.00612.pdf", "abs": "https://arxiv.org/abs/2506.00612", "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "authors": ["Running Yang", "Wenlong Deng", "Minghui Chen", "Yuyin Zhou", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs."}
{"id": "2506.08713", "pdf": "https://arxiv.org/pdf/2506.08713.pdf", "abs": "https://arxiv.org/abs/2506.08713", "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process."}
{"id": "2506.14634", "pdf": "https://arxiv.org/pdf/2506.14634.pdf", "abs": "https://arxiv.org/abs/2506.14634", "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Wei√ü", "Jessica Daikeler"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research."}
{"id": "2506.15830", "pdf": "https://arxiv.org/pdf/2506.15830.pdf", "abs": "https://arxiv.org/abs/2506.15830", "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics", "authors": ["Riccardo Di Sipio"], "categories": ["cs.CL", "quant-ph", "I.2; I.7"], "comment": "9 pages, 1 figure(s)", "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems."}
{"id": "2506.21191", "pdf": "https://arxiv.org/pdf/2506.21191.pdf", "abs": "https://arxiv.org/abs/2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts."}
{"id": "2506.23661", "pdf": "https://arxiv.org/pdf/2506.23661.pdf", "abs": "https://arxiv.org/abs/2506.23661", "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "categories": ["cs.CL"], "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack"}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606.pdf", "abs": "https://arxiv.org/abs/2507.00606", "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning. Our experiments show that MoR significantly\nenhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT\nprompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates\nthe need for task-specific prompts, offering a generalizable solution for\nrobust reasoning across diverse tasks."}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334.pdf", "abs": "https://arxiv.org/abs/2507.01334", "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains."}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352.pdf", "abs": "https://arxiv.org/abs/2507.01352", "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality."}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923.pdf", "abs": "https://arxiv.org/abs/2507.01923", "title": "Decision-Oriented Text Evaluation", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics."}
{"id": "2406.10576", "pdf": "https://arxiv.org/pdf/2406.10576.pdf", "abs": "https://arxiv.org/abs/2406.10576", "title": "Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient", "authors": ["Yuan Gao", "Zujing Liu", "Weizhong Zhang", "Bo Du", "Gui-Song Xia"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ACL2025 Main Accepted", "summary": "Recent Large-Language Models (LLMs) pruning methods typically operate at the\npost-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve efficiency, our method eliminates the back-propagation through the LLM\nper se during optimization, requiring only the forward pass of the LLM. We\nachieve this by learning an underlying Bernoulli distribution to sample binary\npruning masks, where we decouple the Bernoulli parameters from LLM loss,\nfacilitating efficient optimization via policy gradient estimator without\nback-propagation. Thus, our method can 1) support global and heterogeneous\npruning (i.e., automatically determine different redundancy for different\nlayers), and 2) optionally initialize with a metric-based method (for our\nBernoulli distributions). Extensive experiments conducted on LLaMA, LLaMA-2,\nLLaMA-3, Vicuna, and Mistral models using the C4 and WikiText2 datasets\ndemonstrate the promising performance of our method in efficiency and\neffectiveness. Code is available at\nhttps://github.com/ethanygao/backprop-free_LLM_pruning."}
{"id": "2410.00903", "pdf": "https://arxiv.org/pdf/2410.00903.pdf", "abs": "https://arxiv.org/abs/2410.00903", "title": "Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments", "authors": ["Kosuke Imai", "Kentaro Nakamura"], "categories": ["stat.AP", "cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence (GenAI). Specifically, we propose\nto use a deep generative model such as large language models (LLMs) to\nefficiently generate treatments and use their internal representation for\nsubsequent causal effect estimation. We show that the knowledge of this true\ninternal representation helps disentangle the treatment features of interest,\nsuch as specific sentiments and certain topics, from other possibly unknown\nconfounding features. Unlike existing methods, the proposed GenAI-Powered\nInference (GPI) methodology eliminates the need to learn causal representation\nfrom the data, and hence produces more accurate and efficient estimates. We\nformally establish the conditions required for the nonparametric identification\nof the average treatment effect, propose an estimation strategy that avoids the\nviolation of the overlap assumption, and derive the asymptotic properties of\nthe proposed estimator through the application of double machine learning.\nFinally, using an instrumental variables approach, we extend the proposed\nmethodology to the settings in which the treatment feature is based on human\nperception. The proposed GPI methodology is also applicable to text reuse where\nan LLM is used to regenerate existing texts. We conduct simulation and\nempirical studies, using the generated text data from an open-source LLM, Llama\n3, to illustrate the advantages of our estimator over state-of-the-art causal\nrepresentation learning algorithms."}
{"id": "2411.07618", "pdf": "https://arxiv.org/pdf/2411.07618.pdf", "abs": "https://arxiv.org/abs/2411.07618", "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "authors": ["Qingyu Yin", "Chak Tou Leong", "Hongbo Zhang", "Minjun Zhu", "Hanqi Yan", "Qiang Zhang", "Yulan He", "Wenjie Li", "Jun Wang", "Yue Zhang", "Linyi Yang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments."}
{"id": "2412.18530", "pdf": "https://arxiv.org/pdf/2412.18530.pdf", "abs": "https://arxiv.org/abs/2412.18530", "title": "On Characterizations for Language Generation: Interplay of Hallucinations, Breadth, and Stability", "authors": ["Alkis Kalavasis", "Anay Mehrotra", "Grigoris Velegkas"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "stat.ML"], "comment": "v2 improves exposition and simplifies proofs", "summary": "We study language generation in the limit - introduced by Kleinberg and\nMullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin\n[Ang79]. [KM24]'s main result is an algorithm for generating from any countable\nlanguage collection in the limit. While their algorithm eventually generates\nunseen strings from the target language $K$, it sacrifices coverage or breadth,\ni.e., its ability to generate a rich set of strings. Recent work introduces\ndifferent notions of breadth and explores when generation with breadth is\npossible, leaving a full characterization of these notions open. Our first set\nof results settles this by characterizing generation for existing notions of\nbreadth and their natural extensions. Interestingly, our lower bounds are very\nflexible and hold for many performance metrics beyond breadth - for instance,\nshowing that, in general, it is impossible to train generators which achieve a\nhigher perplexity or lower hallucination rate for $K$ compared to other\nlanguages. Next, we study language generation with breadth and stable\ngenerators - algorithms that eventually stop changing after seeing an arbitrary\nbut finite number of strings - and prove unconditional lower bounds for such\ngenerators, strengthening the results of [KMV25] and demonstrating that\ngeneration with many existing notions of breadth becomes equally hard, when\nstability is required. This gives a separation for generation with approximate\nbreadth, between stable and unstable generators, highlighting the rich\ninterplay between breadth, stability, and consistency in language generation."}
{"id": "2502.06106", "pdf": "https://arxiv.org/pdf/2502.06106.pdf", "abs": "https://arxiv.org/abs/2502.06106", "title": "Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks", "authors": ["Yueyan Li", "Wenhao Gao", "Caixia Yuan", "Xiaojie Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The study of mechanistic interpretability aims to reverse-engineer a model to\nexplain its behaviors. While recent studies have focused on the static\nmechanism of a certain behavior, the learning dynamics inside a model remain to\nbe explored. In this work, we develop an interpretable fine-tuning method for\nanalyzing the mechanism behind learning. We first introduce the concept of\nnode-level intrinsic dimensionality to describe the learning process of a model\nin a computational graph. Based on our theory, we propose circuit-tuning, a\ntwo-stage algorithm that iteratively builds the minimal subgraph for a specific\ntask and updates the key parameters in a heuristic way. Experimental results\nconfirm the existence of the intrinsic dimensionality at the node level and\ndemonstrate the effectiveness of our method for transparent and interpretable\nfine-tuning. We visualize and analyze the circuits before, during, and after\nfine-tuning, providing new insights into the self-organization mechanism of a\nneural network in the learning process."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880.pdf", "abs": "https://arxiv.org/abs/2505.21880", "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2506.12479", "pdf": "https://arxiv.org/pdf/2506.12479.pdf", "abs": "https://arxiv.org/abs/2506.12479", "title": "AI Flow: Perspectives, Scenarios, and Approaches", "authors": ["Hongjun An", "Wenhan Hu", "Sida Huang", "Siqi Huang", "Ruanjun Li", "Yuanzhi Liang", "Jiawei Shao", "Yiliang Song", "Zihan Wang", "Cheng Yuan", "Chi Zhang", "Hongyuan Zhang", "Wenhao Zhuang", "Xuelong Li"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.DC", "eess.SP"], "comment": "Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom", "summary": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems."}
{"id": "2506.17828", "pdf": "https://arxiv.org/pdf/2506.17828.pdf", "abs": "https://arxiv.org/abs/2506.17828", "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach", "authors": ["Xinnan Zhang", "Chenliang Li", "Siliang Zeng", "Jiaxiang Li", "Zhongruo Wang", "Kaixiang Lin", "Songtao Lu", "Alfredo Garcia", "Mingyi Hong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights."}
{"id": "2506.18959", "pdf": "https://arxiv.org/pdf/2506.18959.pdf", "abs": "https://arxiv.org/abs/2506.18959", "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents", "authors": ["Weizhi Zhang", "Yangning Li", "Yuanchen Bei", "Junyu Luo", "Guancheng Wan", "Liangwei Yang", "Chenxuan Xie", "Yuyao Yang", "Wei-Chieh Huang", "Chunyu Miao", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Yankai Chen", "Chunkit Chan", "Peilin Zhou", "Xinyang Zhang", "Chenwei Zhang", "Jingbo Shang", "Ming Zhang", "Yangqiu Song", "Irwin King", "Philip S. Yu"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."}
{"id": "2506.21506", "pdf": "https://arxiv.org/pdf/2506.21506.pdf", "abs": "https://arxiv.org/abs/2506.21506", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jim√©nez Guti√©rrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL"], "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web-2/", "summary": "Agentic search such as Deep Research systems-where agents autonomously browse\nthe web, synthesize information, and return comprehensive citation-backed\nanswers-represents a major shift in how users interact with web-scale\ninformation. While promising greater efficiency and cognitive offloading, the\ngrowing complexity and open-endedness of agentic search have outpaced existing\nevaluation benchmarks and methodologies, which largely assume short search\nhorizons and static answers. In this paper, we introduce Mind2Web 2, a\nbenchmark of 130 realistic, high-quality, and long-horizon tasks that require\nreal-time web browsing and extensive information synthesis, constructed with\nover 1000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of ten frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, highlighting its great potential. Altogether, Mind2Web\n2 provides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems."}
{"id": "2506.22049", "pdf": "https://arxiv.org/pdf/2506.22049.pdf", "abs": "https://arxiv.org/abs/2506.22049", "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Lu Yin", "Can Yang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS."}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551.pdf", "abs": "https://arxiv.org/abs/2507.01551", "title": "Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."}
