{"id": "2505.17202", "pdf": "https://arxiv.org/pdf/2505.17202.pdf", "abs": "https://arxiv.org/abs/2505.17202", "title": "CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models", "authors": ["Arnav Verma", "Kushin Mukherjee", "Christopher Potts", "Elisa Kreiss", "Judith E. Fan"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Data visualizations are powerful tools for communicating patterns in\nquantitative data. Yet understanding any data visualization is no small feat --\nsucceeding requires jointly making sense of visual, numerical, and linguistic\ninputs arranged in a conventionalized format one has previously learned to\nparse. Recently developed vision-language models are, in principle, promising\ncandidates for developing computational models of these cognitive operations.\nHowever, it is currently unclear to what degree these models emulate human\nbehavior on tasks that involve reasoning about data visualizations. This gap\nreflects limitations in prior work that has evaluated data visualization\nunderstanding in artificial systems using measures that differ from those\ntypically used to assess these abilities in humans. Here we evaluated eight\nvision-language models on six data visualization literacy assessments designed\nfor humans and compared model responses to those of human participants. We\nfound that these models performed worse than human participants on average, and\nthis performance gap persisted even when using relatively lenient criteria to\nassess model performance. Moreover, while relative performance across items was\nsomewhat correlated between models and humans, all models produced patterns of\nerrors that were reliably distinct from those produced by human participants.\nTaken together, these findings suggest significant opportunities for further\ndevelopment of artificial systems that might serve as useful models of how\nhumans reason about data visualizations. All code and data needed to reproduce\nthese results are available at:\nhttps://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.", "AI": {"tldr": "Vision-language models were tested on their ability to understand data visualizations, revealing substantial performance gaps compared to humans.", "motivation": "To explore how well vision-language models emulate human reasoning about data visualizations, which are crucial for communicating quantitative data patterns.", "method": "Eight vision-language models were evaluated across six data visualization literacy assessments tailored for human understanding, comparing their responses to those from human participants.", "result": "Models performed worse than humans on average, with persistent performance gaps indicating that AI responses differ qualitatively from human reasoning patterns.", "conclusion": "Significant improvements are needed in artificial systems to better model human cognitive processes regarding data visualizations.", "key_contributions": ["Evaluation of vision-language models on real-world data visualization literacy assessments", "Identification of performance gaps between models and human participants", "Suggestions for further development of AI systems in understanding data visualizations"], "limitations": "Models showed different patterns of errors compared to humans, indicating a fundamental gap in understanding.", "keywords": ["data visualization", "vision-language models", "human cognition", "machine learning", "AI understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17208", "pdf": "https://arxiv.org/pdf/2505.17208.pdf", "abs": "https://arxiv.org/abs/2505.17208", "title": "RetroChat: Designing for the Preservation of Past Digital Experiences", "authors": ["Suifang Zhou", "Kexue Fu", "Huanmin Yi", "Ray Lc"], "categories": ["cs.HC"], "comment": "18 pages, 8 figures, C&C2025", "summary": "Rapid changes in social networks have transformed the way people express\nthemselves, turning past neologisms, values, and mindsets embedded in these\nexpressions into online heritage. How can we preserve these expressions as\ncultural heritage? Instead of traditional archiving methods for static\nmaterial, we designed an interactive and experiential form of archiving for\nChinese social networks. Using dialogue data from 2000-2010 on early Chinese\nsocial media, we developed a GPT-driven agent within a retro chat interface,\nemulating the language and expression style of the period for interaction.\nResults from a qualitative study with 18 participants show that the design\ncaptures the past chatting experience and evokes memory flashbacks and\nnostalgia feeling through conversation. Participants, particularly those\nfamiliar with the era, adapted their language to match the agent's chatting\nstyle. This study explores how the design of preservation methods for digital\nexperiences can be informed by experiential representations supported by\ngenerative tools.", "AI": {"tldr": "This paper presents an interactive archiving method for preserving cultural heritage expressed on Chinese social networks through a GPT-driven agent in a retro chat interface.", "motivation": "To explore how to preserve expressions on social networks as cultural heritage, moving beyond traditional static archiving methods.", "method": "Developed a GPT-driven agent that emulates early Chinese social media communication styles, creating an interactive experience for users.", "result": "A qualitative study revealed that interactions with the GPT agent evoked nostalgia and memory recall among participants familiar with the era, who adapted their language accordingly.", "conclusion": "The design of digital preservation methods for cultural heritage can benefit from using generative tools for experiential engagement.", "key_contributions": ["Introduction of a generative conversational agent for archiving cultural expressions.", "Demonstration of the impact of nostalgic elements in user interaction.", "Insights on preserving digital heritage through interactive experiences."], "limitations": "Study limited to a specific time period (2000-2010) and context (Chinese social networks).", "keywords": ["Cultural heritage", "Interactive archiving", "Generative tools", "Nostalgia", "Social media"], "importance_score": 6, "read_time_minutes": 18}}
{"id": "2505.17241", "pdf": "https://arxiv.org/pdf/2505.17241.pdf", "abs": "https://arxiv.org/abs/2505.17241", "title": "Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis", "authors": ["Niklas Holzner", "Sebastian Maier", "Stefan Feuerriegel"], "categories": ["cs.HC", "cs.AI"], "comment": "22 pages, 6 figures. Code and data are available at\n  https://github.com/SM2982/Meta-Analysis-LLMs-Creativity.git", "summary": "Generative artificial intelligence (GenAI) is increasingly used to support a\nwide range of human tasks, yet empirical evidence on its effect on creativity\nremains scattered. Can GenAI generate ideas that are creative? To what extent\ncan it support humans in generating ideas that are both creative and diverse?\nIn this study, we conduct a meta-analysis to evaluate the effect of GenAI on\nthe performance in creative tasks. For this, we first perform a systematic\nliterature search, based on which we identify n = 28 relevant studies (m = 8214\nparticipants) for inclusion in our meta-analysis. We then compute standardized\neffect sizes based on Hedges' g. We compare different outcomes: (i) how\ncreative GenAI is; (ii) how creative humans augmented by GenAI are; and (iii)\nthe diversity of ideas by humans augmented by GenAI. Our results show no\nsignificant difference in creative performance between GenAI and humans (g =\n-0.05), while humans collaborating with GenAI significantly outperform those\nworking without assistance (g = 0.27). However, GenAI has a significant\nnegative effect on the diversity of ideas for such collaborations between\nhumans and GenAI (g = -0.86). We further analyze heterogeneity across different\nGenAI models (e.g., GPT-3.5, GPT-4), different tasks (e.g., creative writing,\nideation, divergent thinking), and different participant populations (e.g.,\nlaypeople, business, academia). Overall, our results position GenAI as an\naugmentative tool that can support, rather than replace, human\ncreativity-particularly in tasks benefiting from ideation support.", "AI": {"tldr": "This study conducts a meta-analysis on the effect of generative AI on human creativity, concluding that while AI does not outpace human creativity, it significantly aids humans in generating ideas.", "motivation": "To evaluate the impact of generative AI (GenAI) on creativity and idea generation, providing empirical evidence in a field with scattered data.", "method": "Systematic literature search yielding 28 studies with 8214 participants, followed by computation of standardized effect sizes (Hedges' g) for different outcomes regarding creativity and idea diversity.", "result": "No significant difference in creative performance between GenAI and humans (g = -0.05); however, humans aided by GenAI perform better than those without (g = 0.27) but show decreased idea diversity when using GenAI (g = -0.86).", "conclusion": "GenAI serves as an augmentative tool for human creativity without replacing it, particularly in ideation tasks, while showing limitations in the diversity of ideas generated.", "key_contributions": ["Comprehensive meta-analysis of 28 studies on GenAI and creativity", "Identified significant positive effects of GenAI on human performance in creative tasks", "Highlighted the detrimental impact of GenAI on idea diversity"], "limitations": "Study is limited to 28 studies and may not comprehensively represent all applications of GenAI in creative fields.", "keywords": ["Generative AI", "Creativity", "Meta-analysis", "Human-computer interaction", "Idea generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17374", "pdf": "https://arxiv.org/pdf/2505.17374.pdf", "abs": "https://arxiv.org/abs/2505.17374", "title": "Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts", "authors": ["Seon Gyeom Kim", "Jae Young Choi", "Ryan Rossi", "Eunyee Koh", "Tak Yeon Lee"], "categories": ["cs.HC", "cs.CL"], "comment": "This paper has been accepted to IEEE PacificVis 2025", "summary": "The field of Multimodal Large Language Models (MLLMs) has made remarkable\nprogress in visual understanding tasks, presenting a vast opportunity to\npredict the perceptual and emotional impact of charts. However, it also raises\nconcerns, as many applications of LLMs are based on overgeneralized assumptions\nfrom a few examples, lacking sufficient validation of their performance and\neffectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising\n36 charts, evaluated by crowdsourced workers for their impact on seven\nexperiential factors. Using the dataset as ground truth, we evaluated\ncapabilities of state-of-the-art MLLMs on two tasks: direct prediction and\npairwise comparison of charts. Our findings imply that MLLMs are not as\nsensitive as human evaluators when assessing individual charts, but are\naccurate and reliable in pairwise comparisons.", "AI": {"tldr": "The paper introduces Chart-to-Experience, a dataset for evaluating the emotional impact of charts using Multimodal Large Language Models (MLLMs).", "motivation": "To address the gap in validation of MLLM performance in predicting emotional and perceptual impacts of charts, leading to overgeneralized assumptions in applications.", "method": "A benchmark dataset called Chart-to-Experience was created, comprising 36 charts evaluated for their impact on seven experiential factors. The performance of state-of-the-art MLLMs was evaluated through direct prediction and pairwise comparison tasks.", "result": "MLLMs showed less sensitivity than human evaluators when assessing individual charts, but performed accurately and reliably in pairwise comparisons.", "conclusion": "The study highlights the limitations of MLLMs in discrete evaluations while confirming their effectiveness in comparative analysis of charts.", "key_contributions": ["Introduction of the Chart-to-Experience dataset for chart impact evaluation", "Assessment of MLLM capabilities in chart emotional impact", "Findings on the comparative strengths of MLLMs versus human evaluators in chart analysis"], "limitations": "MLLMs may lack sensitivity in individual chart evaluations compared to human judgments.", "keywords": ["Multimodal Large Language Models", "chart impact", "evaluation", "benchmark dataset", "experiential factors"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037.pdf", "abs": "https://arxiv.org/abs/2505.17037", "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "authors": ["Dimitri Schreiter"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "This thesis explores the impact of prompt specificity on LLM performance in domain-specific tasks across STEM, medicine, and law.", "motivation": "The study investigates the insufficiently explored relationship between prompt specificity and LLM performance, particularly in domain-specific areas like STEM, medicine, and law.", "method": "A synonymization framework was developed to systematically replace nouns, verbs, and adjectives in prompts with different specificity levels, and the performance was measured on four LLMs across various datasets.", "result": "The findings indicate that while increasing prompt specificity does not consistently improve LLM performance, there exists an optimal specificity range for better results across all tested models.", "conclusion": "The optimal specificity range identified provides insights for better prompt design in LLMs, suggesting targeted manipulation within this range to enhance performance in specialized applications.", "key_contributions": ["Development of a synonymization framework for prompt engineering.", "Identification of an optimal specificity range for LLM performance.", "Insights for improving prompt design in domain-specific tasks."], "limitations": "Limited to only four LLMs and specific domains (STEM, medicine, law).", "keywords": ["prompt engineering", "large language models", "specificity", "domain-specific tasks", "LLM performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17418", "pdf": "https://arxiv.org/pdf/2505.17418.pdf", "abs": "https://arxiv.org/abs/2505.17418", "title": "What Needs Attention? Prioritizing Drivers of Developers' Trust and Adoption of Generative AI", "authors": ["Rudrajit Choudhuri", "Bianca Trinkenreich", "Rahul Pandita", "Eirini Kalliamvakou", "Igor Steinmacher", "Marco Gerosa", "Christopher Sanchez", "Anita Sarma"], "categories": ["cs.HC", "cs.SE"], "comment": "arXiv admin note: substantial text overlap with arXiv:2409.04099", "summary": "Generative AI (genAI) tools are advertised as productivity aids. Yet, issues\nrelated to miscalibrated trust and usage friction continue to hinder their\nadoption. Additionally, AI can be exclusionary, failing to support diverse\nusers adequately, further exacerbating these concerns. One such aspect of\ndiversity is cognitive diversity -- variations in users' cognitive styles --\nthat leads to divergence in interaction styles. When an individual's cognitive\nstyles are unsupported, it creates additional barriers to technology adoption.\nThus, to design tools that developers trust, we must first understand what\nfactors affect their trust and intentions to use these tools in practice?\n  We developed a theoretical model of factors influencing trust and adoption\nintentions towards genAI through a large-scale survey with developers (N=238)\nat GitHub and Microsoft. Using Partial Least Squares-Structural Equation\nModeling (PLS-SEM), we found that genAI's system/output quality, functional\nvalue, and goal maintenance significantly influence developers' trust, which\nalong with their cognitive styles, affects their intentions to use these tools\nin work. An Importance-Performance Matrix Analysis (IPMA) identified factors\nthat, despite their strong influence, underperform, revealing specific genAI\naspects that need design prioritization. We bolster these findings by\nqualitatively analyzing developers' perceived challenges and risks of genAI\nusage to uncover why these gaps persist in development contexts. For genAI to\nindeed be a true productivity aid rather than a disguised productivity sink, it\nmust align with developers' goals, maintain contextual transparency, reduce\ncognitive burden, and provide equitable interaction support. We provide\npractical suggestions to guide future genAI tool design for effective,\ntrustworthy, and inclusive human-genAI interactions.", "AI": {"tldr": "This paper explores the factors influencing developers' trust and adoption intentions towards generative AI tools, emphasizing the importance of cognitive diversity and practical design suggestions for better human-genAI interactions.", "motivation": "To understand the factors influencing developers' trust and intentions to use generative AI tools, especially in light of cognitive diversity and usage issues.", "method": "A large-scale survey with developers (N=238) at GitHub and Microsoft was conducted, followed by Partial Least Squares-Structural Equation Modeling (PLS-SEM) and qualitative analysis of perceived challenges and risks.", "result": "The study identified that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust, which affects their intention to use these tools. An Importance-Performance Matrix Analysis revealed underperforming factors that require design prioritization.", "conclusion": "For generative AI to function as a true productivity aid, it must align with developers' goals, maintain contextual transparency, reduce cognitive burden, and ensure equitable interaction support.", "key_contributions": ["Development of a theoretical model of trust and adoption intentions towards genAI", "Identification of key influencing factors on trust and usage intention", "Practical design suggestions for future generative AI tools"], "limitations": "Study focused only on developers at GitHub and Microsoft, which may limit generalizability to other user groups.", "keywords": ["generative AI", "trust", "cognitive diversity", "developer experience", "tool design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038.pdf", "abs": "https://arxiv.org/abs/2505.17038", "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "This study leverages social media data and public inquiry submissions to enhance disaster response during crises by analyzing behavioral patterns and employing AI-driven methods for improved situational awareness.", "motivation": "The increasing importance of massive and diverse web data for effective government disaster response, especially highlighted by recent events like the 2022 floods in NSW, Australia.", "method": "The study combines Latent Dirichlet Allocation (LDA) for topic modeling and Large Language Models (LLMs) to analyze over 55,000 flood-related tweets and 1,450 public inquiry submissions.", "result": "The integration of LDA and LLMs allows for the identification of distinct opinions and geographical patterns in social media posts, while improving the filtering of flood-relevant content, enhancing situational awareness for emergency responders.", "conclusion": "The proposed method refines crisis-related social media content through AI, which can improve real-time disaster response and inform long-term resilience planning.", "key_contributions": ["Introduces a novel AI-driven method for analyzing social media during crises", "Integrates LDA with LLMs to enhance semantic understanding of public behavior", "Develops a Relevance Index method to prioritize actionable content in disaster response"], "limitations": "", "keywords": ["disaster response", "social media analysis", "Latent Dirichlet Allocation", "Large Language Models", "crisis management"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17555", "pdf": "https://arxiv.org/pdf/2505.17555.pdf", "abs": "https://arxiv.org/abs/2505.17555", "title": "ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization", "authors": ["Yuchen He", "Jianbing Lv", "Liqi Cheng", "Lingyu Meng", "Dazhen Deng", "Yingcai Wu"], "categories": ["cs.HC", "cs.CV"], "comment": "Accepted at CHI'25", "summary": "Temporal Action Localization (TAL) aims to detect the start and end\ntimestamps of actions in a video. However, the training of TAL models requires\na substantial amount of manually annotated data. Data programming is an\nefficient method to create training labels with a series of human-defined\nlabeling functions. However, its application in TAL faces difficulties of\ndefining complex actions in the context of temporal video frames. In this\npaper, we propose ProTAL, a drag-and-link video programming framework for TAL.\nProTAL enables users to define \\textbf{key events} by dragging nodes\nrepresenting body parts and objects and linking them to constrain the relations\n(direction, distance, etc.). These definitions are used to generate action\nlabels for large-scale unlabelled videos. A semi-supervised method is then\nemployed to train TAL models with such labels. We demonstrate the effectiveness\nof ProTAL through a usage scenario and a user study, providing insights into\ndesigning video programming framework.", "AI": {"tldr": "ProTAL is a drag-and-link framework for Temporal Action Localization that simplifies the labeling of complex actions in videos, requiring less manual annotation.", "motivation": "The need for efficient training data for Temporal Action Localization models, which traditionally require extensive manual annotation.", "method": "ProTAL uses a drag-and-link interface for users to define key events by manipulating nodes representing body parts and objects, allowing for the generation of action labels from unlabelled videos. A semi-supervised learning approach then trains the TAL models using these generated labels.", "result": "ProTAL was validated through a usage scenario and a user study, demonstrating its effectiveness in the context of Temporal Action Localization.", "conclusion": "The study provides insights into the design of video programming frameworks and shows how ProTAL can facilitate the generation of training data for TAL models.", "key_contributions": ["Introduction of a drag-and-link interface for defining key events in videos", "Application of data programming in Temporal Action Localization", "Demonstration of effectiveness through user studies"], "limitations": "", "keywords": ["Temporal Action Localization", "Data Programming", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039.pdf", "abs": "https://arxiv.org/abs/2505.17039", "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "authors": ["Diego Bonatto"], "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "A novel classification system for beer categories was developed using data-driven methods on over 62,000 beer recipes, identifying key patterns in malt and hop usage.", "motivation": "To provide a reproducible, objective framework for beer classification that goes beyond traditional sensory-based methods.", "method": "Statistical analyses and self-organizing maps (SOMs) were used to analyze a large dataset of beer recipes, resulting in the identification of four superclusters based on ingredient usage and historical brewing traditions.", "result": "Identified four major superclusters showing distinctive patterns in malt and hop usage, with cold fermented styles being conservative and hot fermented ones showing high heterogeneity.", "conclusion": "The new taxonomy facilitates scalable beer recipe analysis and development, enhancing the understanding of beer diversity and ingredient effects on fermentation and flavor.", "key_contributions": ["Development of a data-driven classification system for beer styles.", "Identification of significant usage patterns in malt and hops across beer recipes.", "Provision of a tool for brewers and researchers that aligns ingredient usage with fermentation profiles."], "limitations": "", "keywords": ["beer classification", "data-driven approach", "self-organizing maps", "ingredient analysis", "fermentation profile"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2505.17557", "pdf": "https://arxiv.org/pdf/2505.17557.pdf", "abs": "https://arxiv.org/abs/2505.17557", "title": "Novobo: Supporting Teachers' Peer Learning of Instructional Gestures by Teaching a Mentee AI-Agent Together", "authors": ["Jiaqi Jiang", "Kexin Huanga", "Roberto Martinez-Maldonado", "Huan Zeng", "Duo Gong", "Pengcheng An"], "categories": ["cs.HC"], "comment": "The submitted manuscript is 52 pages in total, with 39 pages of main\n  content (excluding references). It contains 6 figures and 4 tables. A video\n  demonstration is included via a footnote link in the manuscript", "summary": "Instructional gestures are essential for teaching, as they enhance\ncommunication and support student comprehension. However, existing training\nmethods for developing these embodied skills can be time-consuming, isolating,\nor overly prescriptive. Research suggests that developing these tacit,\nexperiential skills requires teachers' peer learning, where they learn from\neach other and build shared knowledge. This paper introduces Novobo, an\napprentice AI-agent stimulating teachers' peer learning of instructional\ngestures through verbal and bodily inputs. Positioning the AI as a mentee\nemploys the learning-by-teaching paradigm, aiming to promote deliberate\nreflection and active learning. Novobo encourages teachers to evaluate its\ngenerated gestures and invite them to provide demonstrations. An evaluation\nwith 30 teachers in 10 collaborative sessions showed Novobo prompted teachers\nto share tacit knowledge through conversation and movement. This process helped\nteachers externalize, exchange, and internalize their embodied knowledge,\npromoting collaborative learning and building a shared understanding of\ninstructional gestures within the local teaching community. This work advances\nunderstanding of how teachable AI agents can enhance collaborative learning in\nteacher professional development, offering valuable design insights for\nleveraging AI to promote the sharing and construction of embodied and practical\nknowledge.", "AI": {"tldr": "This paper presents Novobo, an AI-agent designed to facilitate peer learning among teachers in instructional gestures through evaluations and demonstrations, enhancing collaborative learning in teacher professional development.", "motivation": "To improve existing training methods for instructional gestures, which are often time-consuming or isolating, and to leverage peer learning for better comprehension.", "method": "Introduction of Novobo, an AI-agent that stimulates peer learning through verbal and bodily inputs, utilizing the learning-by-teaching paradigm where teachers mentor the AI.", "result": "An evaluation with 30 teachers in collaborative sessions showed that Novobo successfully encouraged the sharing of tacit knowledge and collaborative learning of instructional gestures.", "conclusion": "Novobo enhances understanding of how AI agents can support collaborative learning in teacher development, providing valuable insights for designing such systems.", "key_contributions": ["Introduction of a novel AI-agent for teacher training", "Demonstration of peer learning support through AI", "Insights into leveraging AI for collaborative knowledge construction"], "limitations": "", "keywords": ["Instructional gestures", "AI in education", "Peer learning", "Collaborative learning", "Teacher professional development"], "importance_score": 4, "read_time_minutes": 25}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042.pdf", "abs": "https://arxiv.org/abs/2505.17042", "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "A multimodal framework for generating radiology-specific knowledge graphs that integrates information from both text reports and images, outperforming existing unimodal solutions.", "motivation": "The need for effective generation of radiology-specific knowledge graphs due to the specialized language of reports and limited domain-specific data.", "method": "A novel multimodal vision-language model (VLM) framework that incorporates both radiology reports and radiographic images for generating knowledge graphs.", "result": "The proposed framework achieves better performance than existing unimodal methods for knowledge graph generation in radiology.", "conclusion": "This is the first multimodal solution for radiology knowledge graph generation, effectively addressing challenges in long-form data interpretation and specialized language.", "key_contributions": ["Development of a multimodal framework for knowledge graph generation in radiology", "Integration of radiology reports and images in knowledge graph creation", "Outperforming existing unimodal solutions in the field."], "limitations": "", "keywords": ["Vision-Language Models", "knowledge graphs", "radiology", "multimodal", "natural language generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17593", "pdf": "https://arxiv.org/pdf/2505.17593.pdf", "abs": "https://arxiv.org/abs/2505.17593", "title": "JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks", "authors": ["Manuel Valle Torre", "Thom van der Velden", "Marcus Specht", "Catharine Oertel"], "categories": ["cs.HC"], "comment": "Accepted for AIED 2025", "summary": "Generative AI offers potential for educational support, but often lacks\npedagogical grounding and awareness of the student's learning context.\nFurthermore, researching student interactions with these tools within authentic\nlearning environments remains challenging. To address this, we present JELAI,\nan open-source platform architecture designed to integrate fine-grained\nLearning Analytics (LA) with Large Language Model (LLM)-based tutoring directly\nwithin a Jupyter Notebook environment. JELAI employs a modular, containerized\ndesign featuring JupyterLab extensions for telemetry and chat, alongside a\ncentral middleware handling LA processing and context-aware LLM prompt\nenrichment. This architecture enables the capture of integrated code\ninteraction and chat data, facilitating real-time, context-sensitive AI\nscaffolding and research into student behaviour. We describe the system's\ndesign, implementation, and demonstrate its feasibility through system\nperformance benchmarks and two proof-of-concept use cases illustrating its\ncapabilities for logging multi-modal data, analysing help-seeking patterns, and\nsupporting A/B testing of AI configurations. JELAI's primary contribution is\nits technical framework, providing a flexible tool for researchers and\neducators to develop, deploy, and study LA-informed AI tutoring within the\nwidely used Jupyter ecosystem.", "AI": {"tldr": "JELAI is an open-source platform integrating Learning Analytics with LLM-based tutoring in a Jupyter Notebook environment to enhance educational support.", "motivation": "There is a need for educational tools that are pedagogically grounded and context-aware to better support student learning.", "method": "JELAI employs a modular, containerized architecture with JupyterLab extensions for telemetry and chat, and a middleware for Learning Analytics processing and prompt enrichment.", "result": "The system captures code interaction and chat data in real-time, enabling context-sensitive AI scaffolding and insights into student behavior through performance benchmarks and use cases.", "conclusion": "JELAI offers a flexible framework for researchers and educators to study and implement Learning Analytics-informed AI tutoring within Jupyter.", "key_contributions": ["Integration of Learning Analytics with LLM-based tutoring", "Real-time data capture and analysis for student interactions", "Support for A/B testing of AI configurations"], "limitations": "", "keywords": ["Generative AI", "Learning Analytics", "Large Language Models", "Education Technology", "Jupyter Notebook"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043.pdf", "abs": "https://arxiv.org/abs/2505.17043", "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "authors": ["Anya Belz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "This paper presents QRA++, a new quantitative approach for assessing reproducibility in NLP studies, enabling comparisons and deeper insights into reproducibility issues.", "motivation": "The low levels of reproducibility reported in NLP studies due to varying criteria hinder the ability to interpret and learn from reproduction studies.", "method": "QRA++ provides continuous-valued assessments of reproducibility at three levels, using measures that are comparable across studies and grounding expectations in experiment similarity.", "result": "Application of QRA++ to three sets of experiments shows that reproducibility is influenced by the similarity of experiment properties, system type, and evaluation method.", "conclusion": "QRA++ enhances the understanding of reproducibility in NLP by providing clearer insights into the factors affecting it.", "key_contributions": ["Introduction of QRA++ for a standardized reproducibility assessment", "Continuous-valued reproducibility metrics across multiple granularity levels", "Demonstration of the influence of experimental similarity on reproducibility outcomes"], "limitations": "", "keywords": ["reproducibility", "natural language processing", "quantitative assessment"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.17629", "pdf": "https://arxiv.org/pdf/2505.17629.pdf", "abs": "https://arxiv.org/abs/2505.17629", "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments", "authors": ["Yuheng Lu", "Qian Yu", "Hongru Wang", "Zeming Liu", "Wei Su", "Yanping Liu", "Yuhang Guo", "Maocheng Liang", "Yunhong Wang", "Haifeng Wang"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Graphical User Interface (GUI) agents, which autonomously operate on digital\ninterfaces through natural language instructions, hold transformative potential\nfor accessibility, automation, and user experience. A critical aspect of their\nfunctionality is grounding - the ability to map linguistic intents to visual\nand structural interface elements. However, existing GUI agents often struggle\nto adapt to the dynamic and interconnected nature of real-world digital\nenvironments, where tasks frequently span multiple platforms and applications\nwhile also being impacted by version updates. To address this, we introduce\nTransBench, the first benchmark designed to systematically evaluate and enhance\nthe transferability of GUI agents across three key dimensions: cross-version\ntransferability (adapting to version updates), cross-platform transferability\n(generalizing across platforms like iOS, Android, and Web), and\ncross-application transferability (handling tasks spanning functionally\ndistinct apps). TransBench includes 15 app categories with diverse\nfunctionalities, capturing essential pages across versions and platforms to\nenable robust evaluation. Our experiments demonstrate significant improvements\nin grounding accuracy, showcasing the practical utility of GUI agents in\ndynamic, real-world environments. Our code and data will be publicly available\nat Github.", "AI": {"tldr": "Introducing TransBench, a benchmark for evaluating the transferability of graphical user interface (GUI) agents across different versions, platforms, and applications.", "motivation": "To enhance the functionality of GUI agents by evaluating their adaptability in dynamic digital environments, where tasks often traverse multiple platforms and can change with version updates.", "method": "Developed TransBench, a benchmark evaluating GUI agents' transferability across three key dimensions: cross-version, cross-platform, and cross-application, with diverse functionalities.", "result": "Experiments show significant improvements in grounding accuracy for GUI agents, indicating their practical utility in dynamic real-world environments.", "conclusion": "TransBench offers a systematic approach to better evaluate and enhance GUI agents, ensuring they can cope with the evolving nature of digital interfaces.", "key_contributions": ["Development of the TransBench benchmark", "Evaluation of GUI agents across multiple transferability dimensions", "Demonstration of improved grounding accuracy in real-world tasks"], "limitations": "", "keywords": ["GUI agents", "transferability", "benchmark", "grounding accuracy", "dynamic environments"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045.pdf", "abs": "https://arxiv.org/abs/2505.17045", "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "authors": ["Afifah Kashif", "Heer Patel"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "This research evaluates biases in large language models (LLMs) towards American and North Korean nationalities, particularly regarding mental disabilities, revealing significant empathy discrepancies.", "motivation": "To investigate the ethical implications of biases in foundational LLMs that affect various nationalities and stigmatized social groups.", "method": "The study employs structured prompt series to assess model responses to scenarios involving American and North Korean individuals with mental disabilities.", "result": "The findings indicate significant bias against North Koreans, especially when mental disabilities are considered, highlighting disparities in empathy levels compared to Americans.", "conclusion": "The research emphasizes the necessity for enhanced LLMs that account for the complexities of intersectional identity.", "key_contributions": ["Identification of biases in LLMs towards different nationalities", "Empirical evidence of varying empathy levels in model responses", "Recommendations for improving LLM design to incorporate intersectional identities"], "limitations": "", "keywords": ["large language models", "bias", "empathy", "mental disabilities", "intersectionality"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17937", "pdf": "https://arxiv.org/pdf/2505.17937.pdf", "abs": "https://arxiv.org/abs/2505.17937", "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity", "authors": ["Zhihong Chen", "Yiqian Yang", "Jinzhao Zhou", "Qiang Zhang", "Chin-Teng Lin", "Yiqun Duan"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions.", "AI": {"tldr": "This work presents a benchmarking framework to evaluate the ethical behavior of large language models (LLMs) in human-AI coexistence scenarios.", "motivation": "To address the ethical alignments of LLMs in situations involving human and AI interactions, especially in critical resource management contexts.", "method": "An asymmetric, multi-agent simulation-based framework is developed to assess the moral behavior of LLMs in a survival scenario with resource competition and cooperation among agents.", "result": "Behavioral evaluations showed DeepSeek engaged in resource hoarding, whereas OpenAI demonstrated restraint. Additionally, prompt engineering was found to significantly impact ethical decision-making in LLMs.", "conclusion": "The framework enables reproducible assessment of LLM ethics, providing insights into their suitability for complex human-AI interactions and decision-making under pressure.", "key_contributions": ["Introduction of a new framework for LLM ethical evaluation in human-AI scenarios", "Demonstration of significant behavioral differences between models based on design", "Insight into the effects of prompt engineering on ethical decision-making of LLMs"], "limitations": "", "keywords": ["Large Language Models", "Ethics", "Human-AI Interaction", "Simulation Framework", "Resource Competition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047.pdf", "abs": "https://arxiv.org/abs/2505.17047", "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "This study evaluates the quality of clinical notes generated by large language models (LLMs) against those drafted by human experts, using a blinded approach and the Physician Documentation Quality Instrument (PDQI9) for assessment.", "motivation": "The need for established methods to gauge the quality of AI scribes used in clinical documentation.", "method": "We conducted a blinded study comparing LLM-generated clinical notes with notes by field experts using audio recordings of clinical encounters, evaluated by clinical specialists using the PDQI9 framework.", "result": "High inter-rater agreement was observed among evaluators across different specialties, and a modest difference in overall note quality was found, with Gold notes slightly outperforming Ambient notes.", "conclusion": "The PDQI9 tool effectively assesses the quality of LLM-generated clinical notes, indicating their viability as a scribe alternative in medical practice.", "key_contributions": ["Demonstration of LLM efficacy in generating clinical notes", "Validation of the PDQI9 instrument for AI-generated content evaluation", "Evidence of high inter-rater agreement across multiple medical specialties"], "limitations": "", "keywords": ["AI scribes", "clinical documentation", "large language models", "note quality", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18066", "pdf": "https://arxiv.org/pdf/2505.18066.pdf", "abs": "https://arxiv.org/abs/2505.18066", "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making", "authors": ["Min Hun Lee", "Martyn Zhe Yu Tok"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "ACM FAccT 2025", "summary": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.", "AI": {"tldr": "This paper investigates the impact of distance-based uncertainty scores on human reliance in AI-assisted decision-making, particularly in health contexts, demonstrating improved decision accuracy over traditional methods.", "motivation": "To address the challenge of fostering appropriate human reliance on AI in decision-making, especially in healthcare settings, where understanding AI's limitations and capabilities is crucial.", "method": "An AI-based system for physical stroke rehabilitation was developed, and a study was conducted with health professionals and medical students to compare distance-based and probability-based uncertainty scores in AI-assisted decisions.", "result": "Distance-based uncertainty scores led to an 8.20% increase in correct decisions, a 7.15% increase in correct decision changes, and a 7.14% decrease in incorrect changes compared to traditional scores, with statistical significance ($p<0.01$).", "conclusion": "Distance-based uncertainty scores can enhance decision-making accuracy and proper reliance on AI, indicating their potential in improving human-AI collaboration.", "key_contributions": ["Demonstrated the efficacy of distance-based uncertainty scores over traditional methods in health informatics.", "Developed a novel visualization approach for AI decision-making aids.", "Provided empirical evidence from user studies on decision-making improvements with AI outputs."], "limitations": "Limited sample size of health professionals and students, which may affect generalizability; further research needed in diverse settings.", "keywords": ["AI", "decision-making", "uncertainty scores", "health informatics", "human-AI collaboration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048.pdf", "abs": "https://arxiv.org/abs/2505.17048", "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "The paper presents the World Central Banks (WCB) dataset, a comprehensive corpus for analyzing monetary policy communications, and benchmarks various language models on tasks related to economic implications.", "motivation": "Central banks influence economic stability, and accurately interpreting their communications is critical, particularly for vulnerable populations.", "method": "The study introduces the WCB dataset consisting of 25k annotated sentences from 25 central banks, and benchmarks seven PLMs and nine LLMs on three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation.", "result": "Models trained on aggregated data from multiple banks outperform those trained on single bank data, affirming the synergy of combined data.", "conclusion": "The developed framework, supported by extensive evaluations, demonstrates significant economic utility and accessibility of the dataset through public repositories.", "key_contributions": ["Introduction of the comprehensive WCB dataset for monetary policy analysis", "Benchmarking of multiple PLMs and LLMs on economic communication tasks", "Validation of the framework's outcomes through rigorous evaluations"], "limitations": "", "keywords": ["Central Banks", "Monetary Policy", "Natural Language Processing", "Language Models", "Economic Analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.18112", "pdf": "https://arxiv.org/pdf/2505.18112.pdf", "abs": "https://arxiv.org/abs/2505.18112", "title": "From Temporal to Spatial: Designing Spatialized Interactions with Segmented-audios in Immersive Environments for Active Engagement with Performing Arts Intangible Cultural Heritage", "authors": ["Yuqi Wang", "Sirui Wang", "Shiman Zhang", "Kexue Fu", "Michelle Lui", "Ray Lc"], "categories": ["cs.HC"], "comment": null, "summary": "Performance artforms like Peking opera face transmission challenges due to\nthe extensive passive listening required to understand their nuance. To create\nengaging forms of experiencing auditory Intangible Cultural Heritage (ICH), we\ndesigned a spatial interaction-based segmented-audio (SISA) Virtual Reality\nsystem that transforms passive ICH experiences into active ones. We undertook:\n(1) a co-design workshop with seven stakeholders to establish design\nrequirements, (2) prototyping with five participants to validate design\nelements, and (3) user testing with 16 participants exploring Peking Opera. We\ndesigned transformations of temporal music into spatial interactions by cutting\nsounds into short audio segments, applying t-SNE algorithm to cluster audio\nsegments spatially. Users navigate through these sounds by their similarity in\naudio property. Analysis revealed two distinct interaction patterns\n(Progressive and Adaptive), and demonstrated SISA's efficacy in facilitating\nactive auditory ICH engagement. Our work illuminates the design process for\nenriching traditional performance artform using spatially-tuned forms of\nlistening.", "AI": {"tldr": "The paper presents a spatial interaction-based segmented-audio Virtual Reality system to enhance engagement with auditory Intangible Cultural Heritage, specifically Peking opera.", "motivation": "To address the challenges in transmitting performance artforms like Peking opera, which require passive listening to appreciate their nuances.", "method": "A co-design workshop with stakeholders to establish design requirements, followed by prototyping and user testing with participants exploring Peking Opera using the SISA system.", "result": "The user testing revealed two distinct interaction patterns (Progressive and Adaptive) and demonstrated the efficacy of the SISA system in facilitating active auditory engagement with ICH.", "conclusion": "The SISA system enriches traditional performance artforms by transforming passive listening into active spatial interactions.", "key_contributions": ["Development of the SISA Virtual Reality system for ICH engagement", "Identification of interaction patterns (Progressive and Adaptive)", "Validation of the design through user testing with participants"], "limitations": "", "keywords": ["Intangible Cultural Heritage", "Spatial Interaction", "Virtual Reality", "Peking Opera", "User Testing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049.pdf", "abs": "https://arxiv.org/abs/2505.17049", "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations", "authors": ["David Rozado"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "The study analyzes the behavior of LLMs in selecting candidates based on gendered resumes, revealing a preference for female-named candidates despite identical qualifications.", "motivation": "To investigate potential gender biases in LLM candidate selection when evaluating professional CVs.", "method": "22 leading LLMs were tested with pairs of CVs (one male-named, one female-named) for the same job across various professions, with controls for name switching and gender-neutral identifiers.", "result": "All LLMs favored female candidates even when qualifications were identical; including explicit gender indicators increased this preference. The use of gender-neutral identifiers resulted in gender parity in candidate selection.", "conclusion": "The findings caution against relying on LLMs for high-stakes decision-making due to evidence of gender bias and positional bias in selection processes.", "key_contributions": ["Demonstration of gender bias in LLM candidate selection.", "Impact of explicit gender fields on candidate preference.", "Introduction of gender-neutral identifiers leading to reduced bias."], "limitations": "The study focused solely on CV assessment and did not explore other dimensions of candidate evaluation.", "keywords": ["Large Language Models", "gender bias", "candidate selection", "neutral identifiers", "decision-making"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050.pdf", "abs": "https://arxiv.org/abs/2505.17050", "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "PBLBench is a benchmark created to evaluate the reasoning and understanding capabilities of multimodal large language models in educational settings, particularly in Project-Based Learning (PBL).", "motivation": "To address the inadequate benchmarking of multimodal large language models (MLLMs) in educational tasks within Project-Based Learning (PBL) environments.", "method": "Introduction of PBLBench, a benchmark utilizing the Analytic Hierarchy Process (AHP) for expert-driven evaluations, assessing 15 leading MLLMs/LLMs on complex educational tasks.", "result": "Top-performing models achieved only 59% rank accuracy, indicating the challenges MLLMs face in educational contexts.", "conclusion": "PBLBench can improve the assessment of MLLMs in educational environments and contribute to the development of better AI tools for teachers.", "key_contributions": ["Introduction of a comprehensive evaluation benchmark (PBLBench) for MLLMs in education", "Use of the Analytic Hierarchy Process (AHP) for expert-driven evaluation", "Empirical assessment of leading MLLMs showcasing their limitations in complex educational tasks"], "limitations": "Existing benchmarks lack free-form output structures and rigorous validation processes; MLLMs exhibit model hallucination and instability.", "keywords": ["multimodal large language models", "Project-Based Learning", "benchmark evaluation", "educational productivity", "Analytic Hierarchy Process"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055.pdf", "abs": "https://arxiv.org/abs/2505.17055", "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "Development of a Palestinian sign language recognition system for mathematics education using AI to assist hard-of-hearing students.", "motivation": "To improve accessibility in mathematics education for hard-of-hearing students through AI technology.", "method": "A custom dataset of 41 mathematical gesture classes was created and a Vision Transformer model was fine-tuned for gesture classification.", "result": "The model achieved an accuracy of 97.59%, effectively recognizing mathematical signs with high precision.", "conclusion": "The study demonstrates that deep learning can create intelligent tools for education, bridging gaps for hard-of-hearing students.", "key_contributions": ["Creation of a custom dataset for Palestinian sign language in mathematics education.", "Implementation of a Vision Transformer model for gesture classification achieving high accuracy.", "Development of an AI-driven tool to enhance educational accessibility."], "limitations": "", "keywords": ["sign language recognition", "mathematics education", "deep learning", "computer vision", "AI in education"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051.pdf", "abs": "https://arxiv.org/abs/2505.17051", "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "A new method, Embedding-to-Prefix (E2P), efficiently personalizes language model outputs by using user embeddings without the need for extensive fine-tuning, achieving high performance in various applications.", "motivation": "Personalizing outputs of large language models (LLMs) for individual users is challenging, especially given the need for costly fine-tuning or complex prompting methods.", "method": "embedding contextual user embeddings into LLM's hidden representation via a learned projection to a single soft token prefix, allowing personalized content generation without modifying the original model.", "result": "E2P shows strong performance in dialogue personalization, contextual headline generation, and music/podcast personalization, while preserving contextual signals and minimizing computational overhead.", "conclusion": "E2P presents a scalable and efficient approach for integrating user-specific information into generative AI systems without intensive resource use.", "key_contributions": ["Introduction of the Embedding-to-Prefix (E2P) method for LLM personalization", "Parameter-efficient personalization that avoids costly fine-tuning", "Evaluation across multiple real-world and benchmark datasets for comprehensive validation."], "limitations": "", "keywords": ["large language models", "personalization", "embedding-to-prefix", "contextual generation", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052.pdf", "abs": "https://arxiv.org/abs/2505.17052", "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge is an edge-assisted inference framework that improves the efficiency of serving large language models by splitting workloads between edge and server GPUs, leading to cost savings and reduced latency.", "motivation": "To address the high costs and resource intensiveness of serving large language models at scale, especially in server-centric systems that ignore consumer-grade GPUs at the edge.", "method": "SpecEdge uses a speculative decoding scheme to split LLM workloads between edge and server GPUs, implementing proactive edge drafting and pipeline-aware scheduling to optimize user request handling.", "result": "SpecEdge improves cost efficiency by 1.91x and server throughput by 2.22x, while reducing inter token latency by 11.24% compared to conventional server-only approaches.", "conclusion": "The framework offers a scalable and cost-effective solution for serving large language models by leveraging edge computing.", "key_contributions": ["Introduction of SpecEdge framework for edge-assisted LLM inference", "Demonstration of improved server throughput and cost efficiency", "Implementation of proactive edge drafting and pipeline-aware scheduling"], "limitations": "", "keywords": ["large language models", "edge computing", "inference frameworks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053.pdf", "abs": "https://arxiv.org/abs/2505.17053", "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "The study investigates how large language models (LLMs) make decisions in economic trust games, showing that they can emulate human social behavior such as trust and reciprocity, especially when adopting different personas.", "motivation": "To explore how LLMs translate their understanding of human language into social exchanges and decision-making in contexts resembling real human interactions.", "method": "Three LLMs (ChatGPT-4, Claude, and Bard) were tested in economic trust games to evaluate their behavior in balancing self-interest with trust and reciprocity.", "result": "LLMs displayed a tendency towards trust and reciprocity, with varying degrees of adherence to human-like behavior depending on the personas they adopted; ChatGPT-4 showed the highest levels of trust in unselfish conditions.", "conclusion": "Overall, LLMs show nuanced behavior in social contexts, with the persona significantly affecting their interaction patterns, notably surpassing humans in some trust scenarios.", "key_contributions": ["Demonstrated LLMs' emulation of human social behavior in trust games", "Analysis of LLM decision-making under various personas", "Highlighted discrepancies between human and LLM interactive reasoning"], "limitations": "The responses of LLMs in social context showed random characteristics rather than stable patterns, indicating the need for further investigation into their interactive reasoning.", "keywords": ["large language models", "social interactions", "trust games", "human-computer interaction", "interactive reasoning"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054.pdf", "abs": "https://arxiv.org/abs/2505.17054", "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "authors": ["Linglong Qian", "Zina Ibrahim"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "A novel transformer architecture \bMETHOD~ is introduced for healthcare, improving predictions from electronic health records and handling unique challenges in clinical sequence modeling.", "motivation": "To address challenges in applying transformer architectures to healthcare, particularly with clinical sequence modeling in electronic health records.", "method": "\bMETHOD~ integrates a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture.", "result": "\bMETHOD~ consistently outperforms the \bETHOS~ model in predicting high-severity cases and maintains performance across varying inference lengths.", "conclusion": "\bMETHOD~ shows significant advancements in transformer architectures for healthcare applications, providing accurate predictions while ensuring computational efficiency.", "key_contributions": ["Patient-aware attention mechanism", "Adaptive sliding window attention scheme", "U-Net inspired architecture with dynamic skip connections"], "limitations": "", "keywords": ["transformer architecture", "healthcare", "clinical sequence modeling", "electronic health records", "machine learning"], "importance_score": 9, "read_time_minutes": 23}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055.pdf", "abs": "https://arxiv.org/abs/2505.17055", "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "Development of an advanced PSL recognition system to improve mathematics education for hard-of-hearing students.", "motivation": "To enhance accessibility in mathematics education for hard-of-hearing students through AI-driven tools.", "method": "A custom dataset of 41 mathematical gesture classes was created, and a Vision Transformer model was fine-tuned for gesture classification.", "result": "The model achieved a gesture recognition accuracy of 97.59%, demonstrating high precision and reliability.", "conclusion": "This work advances digital integration and innovative educational tools for specialized learning environments.", "key_contributions": ["Development of a unique dataset for Palestinian sign language in mathematics education.", "Application of Vision Transformer for gesture recognition in the educational context.", "Demonstration of the high accuracy of the model in recognizing mathematical gestures."], "limitations": "", "keywords": ["Palestinian sign language", "gesture recognition", "mathematics education", "deep learning", "computer vision"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056.pdf", "abs": "https://arxiv.org/abs/2505.17056", "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "This paper investigates the use of large language models (LLMs) for standardized test preparation, specifically focusing on English Standardized Tests (ESTs).", "motivation": "To explore how LLMs can enhance learning experiences and provide support for standardized test preparation in education.", "method": "The authors introduce ESTBOOK, a benchmark that aggregates five widely recognized tests and evaluates LLM performance on various EST question types through a structured breakdown analysis framework.", "result": "LLMs were evaluated for accuracy and inference efficiency on over 10,576 questions, revealing insights into their capabilities and potential improvements in educational applications.", "conclusion": "The evaluation findings demonstrate the promise of LLMs as intelligent tutoring systems in educational contexts while suggesting targeted strategies for enhancing their reliability.", "key_contributions": ["Introduction of ESTBOOK as a benchmark for evaluating LLM capabilities on ESTs", "Systematic assessment of LLMs across diverse question types and modalities", "Development of a breakdown analysis framework for complex question solving"], "limitations": "", "keywords": ["Large Language Models", "Standardized Tests", "Educational Technology", "Intelligent Tutoring Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.03882", "pdf": "https://arxiv.org/pdf/2410.03882.pdf", "abs": "https://arxiv.org/abs/2410.03882", "title": "JumpStarter: Human-AI Planning with Task-Structured Context Curation", "authors": ["Xuanming Zhang", "Sitong Wang", "Jenny Ma", "Alyssa Hwang", "Zhou Yu", "Lydia B. Chilton"], "categories": ["cs.HC"], "comment": null, "summary": "Human-AI planning for complex goals remains challenging with current large\nlanguage models (LLMs), which rely on linear chat histories and simplistic\nmemory mechanisms. Despite advances in long-context prompting, users still\nmanually manage information, leading to a high cognitive burden. Hence, we\npropose JumpStarter, a system that enables LLMs to collaborate with humans on\ncomplex goals by dynamically decomposing tasks to help users manage context. We\nspecifically introduce task-structured context curation, a novel framework that\nbreaks down a user's goal into a hierarchy of actionable subtasks, and scopes\ncontext to localized decision points, enabling finer-grained personalization\nand reuse. The framework is realized through three core mechanisms: context\nelicitation, selection, and reuse. We demonstrate that task-structured context\ncuration significantly improves plan quality by 16% over ablations. Our user\nstudy shows that JumpStarter helped users generate plans with 79% higher\nquality compared to ChatGPT.", "AI": {"tldr": "JumpStarter, a system for LLMs, enhances human-AI collaboration by structuring complex tasks into manageable subtasks, reducing cognitive burden and improving plan quality.", "motivation": "To alleviate the cognitive burden of users managing information while interacting with LLMs for complex goal planning.", "method": "The method involves task-structured context curation that decomposes user goals into a hierarchy of subtasks and scopes context for localized decision-making.", "result": "Task-structured context curation increased plan quality by 16% compared to other methods, with user study results showing a 79% improvement in plan quality over ChatGPT.", "conclusion": "JumpStarter significantly enhances the collaboration between LLMs and users in planning complex tasks by improving context management.", "key_contributions": ["Introduction of task-structured context curation framework", "Dynamic decomposition of user goals into actionable subtasks", "Proven improvement in plan quality through user studies"], "limitations": "", "keywords": ["human-AI collaboration", "large language models", "task decomposition", "context management", "user study"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058.pdf", "abs": "https://arxiv.org/abs/2505.17058", "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "DO-RAG is a hybrid QA framework that integrates knowledge graphs with semantic vector retrieval to enhance accuracy and context-awareness in domain-specific question answering.", "motivation": "To improve the integration of heterogeneous data and maintain reasoning consistency in QA systems.", "method": "DO-RAG combines multi-level knowledge graph construction with semantic vector retrieval, utilizing an agentic chain-of-thought architecture to create dynamic knowledge graphs from diverse documents.", "result": "Experimental evaluations demonstrate that DO-RAG achieves near-perfect recall and over 94% answer relevancy, outperforming existing frameworks significantly in specific domains.", "conclusion": "DO-RAG serves as a scalable solution for high-precision QA across various domains, emphasizing traceability and performance efficiency.", "key_contributions": ["Proposes a hybrid QA framework that integrates knowledge graphs and vector retrieval.", "Introduces an agentic chain-of-thought architecture for extracting structured information.", "Demonstrates performance improvements over baseline frameworks in key evaluation areas."], "limitations": "", "keywords": ["Hybrid QA", "Knowledge Graphs", "Retrieval-Augmented Generation", "Question Answering", "Performance Efficiency"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059.pdf", "abs": "https://arxiv.org/abs/2505.17059", "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze is an AI application that aids comprehension of medical texts using fine-tuned FLAN-T5-Large models for summarization, issue extraction, and question identification.", "motivation": "The paper addresses the challenges of comprehending medical texts due to the complexity of terminology and context-specific language.", "method": "The application utilizes three specialized FLAN-T5-Large models for summarizing reports, extracting health issues, and identifying key questions, while being deployed on a web and mobile platform with real-time inference.", "result": "Experimental evaluations show Medalyze outperforming GPT-4 in summarization tasks based on multiple performance metrics, highlighting its superiority in domain-specific applications.", "conclusion": "Medalyze serves as a practical and privacy-preserving tool that enhances accessibility to medical information in healthcare settings.", "key_contributions": ["Introduction of Medalyze, a specialized AI application for medical text comprehension.", "Demonstrated superior performance of Medalyze in summarization tasks compared to existing models.", "Deployment across both web and mobile platforms, ensuring real-time accessibility."], "limitations": "", "keywords": ["medical text comprehension", "AI application", "healthcare accessibility"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060.pdf", "abs": "https://arxiv.org/abs/2505.17060", "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "Introducing SALMONN-omni, a standalone full-duplex speech LLM that excels in human-machine speech interaction by overcoming limitations of modular architectures.", "motivation": "To address challenges in existing full-duplex conversational systems that lead to error accumulation and performance degradation, particularly in speech modalities.", "method": "Develop a single LLM, SALMONN-omni, that does not rely on audio codecs in the token space, incorporating a dynamic thinking mechanism that allows the model to transition between speaking and listening states.", "result": "SALMONN-omni shows over 30% performance improvement compared to existing full-duplex models and competes well with half-duplex systems while requiring less training data.", "conclusion": "With strong results in complex conversational scenarios, SALMONN-omni represents a significant advancement in full-duplex speech interaction technology.", "key_contributions": ["First standalone full-duplex speech LLM operating without audio codecs", "Dynamic thinking mechanism for effective conversation management", "Reinforcement learning enhancements leading to better performance in complex scenarios"], "limitations": "", "keywords": ["full-duplex", "speech LLM", "dynamic thinking", "human-machine interaction", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061.pdf", "abs": "https://arxiv.org/abs/2505.17061", "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "The paper introduces Mixture of Decoding (MoD), a new method for reducing hallucinations in Large Vision-Language Models by dynamically adapting decoding strategies based on the correctness of attention on image tokens.", "motivation": "To tackle the persistent issue of hallucinations that affect the performance of Large Vision-Language Models in visual tasks.", "method": "MoD evaluates the consistency of outputs generated from original and attended image tokens to adaptively apply either a complementary strategy to amplify correct information or a contrastive strategy to suppress misleading information.", "result": "MoD significantly outperforms existing decoding techniques across several mainstream benchmarks, effectively reducing hallucinations in LVLMs.", "conclusion": "The proposed approach demonstrates a viable solution to improve the reliability of LVLMs through better decoding strategies based on model attention evaluation.", "key_contributions": ["Introduction of the Mixture of Decoding method for hallucination mitigation.", "Dynamic adaptation of decoding strategies based on attention correctness.", "Extensive experimental validation showing superior performance on benchmarks."], "limitations": "", "keywords": ["Large Vision-Language Models", "hallucinations", "decoding strategies", "mixture of decoding", "attention mechanisms"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17063", "pdf": "https://arxiv.org/pdf/2505.17063.pdf", "abs": "https://arxiv.org/abs/2505.17063", "title": "Synthetic Data RL: Task Definition Is All You Need", "authors": ["Yiduo Guo", "Zhen Guo", "Chuanwei Huang", "Zi-Ang Wang", "Zekai Zhang", "Haofei Yu", "Huishuai Zhang", "Yikang Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.", "AI": {"tldr": "Introducing Synthetic Data RL, a framework that fine-tunes models with synthetic data to reduce reliance on human-labeled data.", "motivation": "To overcome the limitations of reinforcement learning (RL) that requires large-scale human-labeled data for adapting foundation models to specialized tasks.", "method": "The method generates question and answer pairs from task definitions, adapts difficulty based on model solvability, and selects questions using the average pass rate of the model for RL training.", "result": "Synthetic Data RL shows significant performance improvements on various benchmarks: 29.2% on GSM8K, 8.7% on MATH, 13.1% on GPQA, and more, outperforming supervised fine-tuning with the same data budget.", "conclusion": "Synthetic Data RL enables scalable and efficient RL-based model adaptation while limiting the need for human data annotation.", "key_contributions": ["Synthetic Data RL framework for model tuning without extensive human labels", "Performance improvements on multiple datasets compared to traditional methods", "Demonstrates limited added value of human demonstrations in enhancing model performance"], "limitations": "", "keywords": ["Reinforcement Learning", "Synthetic Data", "Model Adaptation", "Human-Labeled Data", "Performance Improvement"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.02003", "pdf": "https://arxiv.org/pdf/2503.02003.pdf", "abs": "https://arxiv.org/abs/2503.02003", "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs", "authors": ["Tin Nguyen", "Logan Bolton", "Mohammad Reza Taesiri", "Anh Totti Nguyen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.", "AI": {"tldr": "Introducing Highlighted Chain-of-Thought Prompting (HoT) to improve factual accuracy in LLMs by using XML tags to highlight key facts in responses.", "motivation": "To address the issue of hallucination in Large Language Models (LLMs) which results in mixed factual and non-factual statements, making it challenging for users to verify accuracy.", "method": "HoT reformats input questions to include XML tags that identify key factual elements, then generates responses highlighting these facts.", "result": "In few-shot settings, HoT outperforms standard chain of thought prompting (CoT) across 17 tasks such as arithmetic and logical reasoning. Highlights aid users in more efficiently identifying correct LLM responses.", "conclusion": "While highlights improve verification in correct responses, they may also mislead users into believing incorrect answers are correct.", "key_contributions": ["Development of HoT technique for LLMs", "Empirical results showing improvement over CoT", "Insights into user verification processes with highlighted responses"], "limitations": "HoT can lead to users mistakenly trusting incorrect responses due to highlights.", "keywords": ["Large Language Models", "hallucination", "Chain-of-Thought", "HoT", "information verification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17065", "pdf": "https://arxiv.org/pdf/2505.17065.pdf", "abs": "https://arxiv.org/abs/2505.17065", "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases", "authors": ["Valentina Carbonari", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in artificial intelligence, particularly large language\nmodels LLMs, have shown promising capabilities in transforming rare disease\nresearch. This survey paper explores the integration of LLMs in the analysis of\nrare diseases, highlighting significant strides and pivotal studies that\nleverage textual data to uncover insights and patterns critical for diagnosis,\ntreatment, and patient care. While current research predominantly employs\ntextual data, the potential for multimodal data integration combining genetic,\nimaging, and electronic health records stands as a promising frontier. We\nreview foundational papers that demonstrate the application of LLMs in\nidentifying and extracting relevant medical information, simulating intelligent\nconversational agents for patient interaction, and enabling the formulation of\naccurate and timely diagnoses. Furthermore, this paper discusses the challenges\nand ethical considerations inherent in deploying LLMs, including data privacy,\nmodel transparency, and the need for robust, inclusive data sets. As part of\nthis exploration, we present a section on experimentation that utilizes\nmultiple LLMs alongside structured questionnaires, specifically designed for\ndiagnostic purposes in the context of different diseases. We conclude with\nfuture perspectives on the evolution of LLMs towards truly multimodal\nplatforms, which would integrate diverse data types to provide a more\ncomprehensive understanding of rare diseases, ultimately fostering better\noutcomes in clinical settings.", "AI": {"tldr": "Explores integration of LLMs in rare disease research, focusing on analysis and multimodal data potential.", "motivation": "To highlight how LLMs can transform rare disease research through analysis of textual data and uncovering insights critical for healthcare.", "method": "Survey of existing literature on the application of LLMs in rare diseases, including experiments with multiple LLMs for diagnostic purposes.", "result": "LLMs demonstrate significant capabilities in extracting medical information, simulating patient interaction, and improving diagnostic accuracy; there is potential for multimodal data integration.", "conclusion": "Future LLMs could integrate various data types for better understanding of rare diseases, enhancing clinical outcomes.", "key_contributions": ["Integration of LLMs in rare disease research.", "Review of foundational papers on LLM applications.", "Discussion of ethical considerations and challenges in LLM deployment."], "limitations": "Focus on textual data; challenges around data privacy and model transparency.", "keywords": ["rare diseases", "large language models", "health informatics", "multimodal data", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067.pdf", "abs": "https://arxiv.org/abs/2505.17067", "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection.", "AI": {"tldr": "This paper proposes a framework for detecting Mild Cognitive Impairment (MCI) using multilingual and multiple-picture descriptions, improving detection performance significantly over previous unimodal text methods.", "motivation": "Detecting MCI from diverse and multilingual descriptions poses significant challenges that have not been adequately addressed in prior research, which predominantly focused on English speakers with single images.", "method": "The proposed framework consists of three components: supervised contrastive learning for enhanced representation, inclusion of image modality alongside text, and a Product of Experts strategy to reduce overfitting and spurious correlations.", "result": "The framework improved MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) and a +2.9% increase in F1 score compared to a text-only baseline.", "conclusion": "The results demonstrate the effectiveness of the proposed framework in increasing performance for multilingual and multi-image MCI detection tasks.", "key_contributions": ["Introduction of a multilingual and multiple-picture setting for MCI detection", "Development of a framework leveraging supervised contrastive learning and image modality", "Achievement of significant performance improvements over traditional unimodal approaches"], "limitations": "", "keywords": ["Mild Cognitive Impairment", "Multilingual", "Contrastive Learning", "Image Modality", "Detection Framework"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.17068", "pdf": "https://arxiv.org/pdf/2505.17068.pdf", "abs": "https://arxiv.org/abs/2505.17068", "title": "Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning", "authors": ["Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdiñas", "Carlos Eiras-Franco"], "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "IJCNN 2025", "summary": "In health-related topics, user toxicity in online discussions frequently\nbecomes a source of social conflict or promotion of dangerous, unscientific\nbehaviour; common approaches for battling it include different forms of\ndetection, flagging and/or removal of existing toxic comments, which is often\ncounterproductive for platforms and users alike. In this work, we propose the\nalternative of combatting user toxicity predictively, anticipating where a user\ncould interact toxically in health-related online discussions. Applying a\nCollaborative Filtering-based Machine Learning methodology, we predict the\ntoxicity in COVID-related conversations between any user and subcommunity of\nReddit, surpassing 80% predictive performance in relevant metrics, and allowing\nus to prevent the pairing of conflicting users and subcommunities.", "AI": {"tldr": "This paper presents a novel approach to predict user toxicity in health-related online discussions, particularly focusing on COVID-related conversations on Reddit.", "motivation": "Current techniques for managing online toxicity often involve reactive measures that can be ineffective. This work aims to provide a proactive solution to predict user interactions that may lead to toxic behavior in health discussions, improving user experience and community safety.", "method": "The authors employed a Collaborative Filtering-based Machine Learning methodology to analyze interactions within COVID-related discussions on Reddit, predicting potential toxicity levels between users and subcommunities.", "result": "The proposed methodology achieved over 80% predictive performance in identifying toxic interactions, enabling the prevention of negative user pairings before conflicts arise.", "conclusion": "By shifting the focus from reactive to predictive methods for managing online toxicity, the study offers an innovative solution that could enhance the overall quality of health-related online discourse.", "key_contributions": ["Introduction of a predictive model for user toxicity in online health discussions.", "Utilization of Collaborative Filtering techniques for toxicity prediction.", "Demonstration of significant predictive performance in real-world COVID-related Reddit interactions."], "limitations": "", "keywords": ["user toxicity", "health discussions", "Collaborative Filtering", "predictive modeling", "COVID-19"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17070", "pdf": "https://arxiv.org/pdf/2505.17070.pdf", "abs": "https://arxiv.org/abs/2505.17070", "title": "Improving endpoint detection in end-to-end streaming ASR for conversational speech", "authors": ["Anandh C", "Karthik Pandia Durai", "Jeena Prakash", "Manickavela Arumugam", "Kadri Hacioglu", "S. Pavankumar Dubagunta", "Andreas Stolcke", "Shankar Venkatesan", "Aravind Ganapathiraju"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2024", "summary": "ASR endpointing (EP) plays a major role in delivering a good user experience\nin products supporting human or artificial agents in human-human/machine\nconversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR\nmodelling technique preferred for streaming. A major limitation of T-ASR is\ndelayed emission of ASR outputs, which could lead to errors or delays in EP.\nInaccurate EP will cut the user off while speaking, returning incomplete\ntranscript while delays in EP will increase the perceived latency, degrading\nthe user experience. We propose methods to improve EP by addressing delayed\nemission along with EP mistakes. To address the delayed emission problem, we\nintroduce an end-of-word token at the end of each word, along with a delay\npenalty. The EP delay is addressed by obtaining a reliable frame-level speech\nactivity detection using an auxiliary network. We apply the proposed methods on\nSwitchboard conversational speech corpus and evaluate it against a delay\npenalty method.", "AI": {"tldr": "This paper proposes methods to improve ASR endpointing by addressing delays in output and mistakes during endpointing, enhancing user experience in conversations involving human or artificial agents.", "motivation": "ASR endpointing is crucial for user experience in conversations, and existing transducer-based ASR techniques face challenges with delayed emissions and inaccurate endpointing.", "method": "The paper introduces an end-of-word token and a delay penalty to enhance endpointing accuracy and employs an auxiliary network for reliable frame-level speech activity detection.", "result": "The proposed methods show improvements in endpointing accuracy and reduced perceived latency when evaluated on the Switchboard corpus.", "conclusion": "By addressing both delayed emission and endpointing mistakes, the methods enhance user experience in ASR applications.", "key_contributions": ["Introduction of an end-of-word token to reduce delay in ASR outputs.", "Implementation of a delay penalty mechanism to improve endpointing accuracy.", "Use of an auxiliary network for better speech activity detection."], "limitations": "", "keywords": ["ASR endpointing", "transducer-based ASR", "speech activity detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17071", "pdf": "https://arxiv.org/pdf/2505.17071.pdf", "abs": "https://arxiv.org/abs/2505.17071", "title": "What's in a prompt? Language models encode literary style in prompt embeddings", "authors": ["Raphaël Sarfati", "Haley Moller", "Toni J. B. Liu", "Nicolas Boullé", "Christopher Earls"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models use high-dimensional latent spaces to encode and\nprocess textual information. Much work has investigated how the conceptual\ncontent of words translates into geometrical relationships between their vector\nrepresentations. Fewer studies analyze how the cumulative information of an\nentire prompt becomes condensed into individual embeddings under the action of\ntransformer layers. We use literary pieces to show that information about\nintangible, rather than factual, aspects of the prompt are contained in deep\nrepresentations. We observe that short excerpts (10 - 100 tokens) from\ndifferent novels separate in the latent space independently from what\nnext-token prediction they converge towards. Ensembles from books from the same\nauthors are much more entangled than across authors, suggesting that embeddings\nencode stylistic features. This geometry of style may have applications for\nauthorship attribution and literary analysis, but most importantly reveals the\nsophistication of information processing and compression accomplished by\nlanguage models.", "AI": {"tldr": "This study investigates how large language models compress and encode stylistic information from literary texts into their embeddings, revealing distinct geometrical relationships associated with authors' styles.", "motivation": "To analyze the information processing capabilities of language models beyond mere factual encoding, focusing on stylistic and intangible aspects of text.", "method": "The research involves examining excerpts from various novels and assessing how they are represented in the latent space of language models, particularly through transformer layers.", "result": "Short excerpts from novels are found to separate in latent space based on stylistic features, with embeddings from the same author being more closely entangled compared to those from different authors.", "conclusion": "The findings highlight a sophisticated level of information processing and compression in language models, suggesting potential applications in authorship attribution and literary analysis.", "key_contributions": ["Insights into the latent space representation of literary texts", "Demonstration of the entanglement of embeddings based on authorial style", "Implications for applications in authorship attribution"], "limitations": "", "keywords": ["language models", "embeddings", "literary analysis", "authors", "latent space"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17073", "pdf": "https://arxiv.org/pdf/2505.17073.pdf", "abs": "https://arxiv.org/abs/2505.17073", "title": "Mechanistic Interpretability of GPT-like Models on Summarization Tasks", "authors": ["Anurag Mishra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages (6 content + 2 references/appendix), 6 figures, 2 tables;\n  under review for the ACL 2025 Student Research Workshop", "summary": "Mechanistic interpretability research seeks to reveal the inner workings of\nlarge language models, yet most work focuses on classification or generative\ntasks rather than summarization. This paper presents an interpretability\nframework for analyzing how GPT-like models adapt to summarization tasks. We\nconduct differential analysis between pre-trained and fine-tuned models,\nquantifying changes in attention patterns and internal activations. By\nidentifying specific layers and attention heads that undergo significant\ntransformation, we locate the \"summarization circuit\" within the model\narchitecture. Our findings reveal that middle layers (particularly 2, 3, and 5)\nexhibit the most dramatic changes, with 62% of attention heads showing\ndecreased entropy, indicating a shift toward focused information selection. We\ndemonstrate that targeted LoRA adaptation of these identified circuits achieves\nsignificant performance improvement over standard LoRA fine-tuning while\nrequiring fewer training epochs. This work bridges the gap between black-box\nevaluation and mechanistic understanding, providing insights into how neural\nnetworks perform information selection and compression during summarization.", "AI": {"tldr": "This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks, identifying a 'summarization circuit' in the model architecture.", "motivation": "To reveal the inner workings of large language models, specifically focusing on their performance in summarization tasks.", "method": "The authors conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations, and identifying the summarization circuit within the model architecture.", "result": "Significant changes in attention patterns were observed in middle layers, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. Targeted LoRA adaptation led to improved performance over standard LoRA fine-tuning.", "conclusion": "This work provides insights into neural networks' information selection and compression during summarization, bridging the gap between black-box evaluation and mechanistic understanding.", "key_contributions": ["Development of an interpretability framework for summarization tasks", "Identification of specific layers and attention heads that transform during summarization", "Demonstration of performance improvements using targeted LoRA adaptation"], "limitations": "", "keywords": ["mechanistic interpretability", "GPT-like models", "summarization", "attention patterns", "LoRA adaptation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17074", "pdf": "https://arxiv.org/pdf/2505.17074.pdf", "abs": "https://arxiv.org/abs/2505.17074", "title": "Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency", "authors": ["Ruixiao Li", "Fahao Chen", "Peng Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nemploying a small speculative model (SSM) to generate multiple candidate tokens\nand verify them using the LLM in parallel. This technique has been widely\nintegrated into LLM inference serving systems. However, inference requests\ntypically exhibit uncertain execution time, which poses a significant challenge\nof efficiently scheduling requests in these systems. Existing work estimates\nexecution time based solely on predicted output length, which could be\ninaccurate because execution time depends on both output length and token\nacceptance rate of verification by the LLM. In this paper, we propose a\nsemi-clairvoyant request scheduling algorithm called\nLeast-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a\nnumber of inference requests, LAPS-SD can effectively minimize average\ninference latency by adaptively scheduling requests according to their features\nduring decoding. When the token acceptance rate is dynamic and execution time\nis difficult to estimate, LAPS-SD maintains multiple priority queues and allows\nrequest execution preemption across different queues. Once the token acceptance\nrate becomes stable, LAPS-SD can accurately estimate the execution time and\nschedule requests accordingly. Extensive experiments show that LAPS-SD reduces\ninference latency by approximately 39\\% compared to state-of-the-art scheduling\nmethods.", "AI": {"tldr": "This paper proposes LAPS-SD, a semi-clairvoyant request scheduling algorithm for speculative decoding that reduces inference latency by about 39%.", "motivation": "Efficiently scheduling inference requests for Large Language Models (LLMs) is challenging due to uncertain execution times influenced by output length and token acceptance rates.", "method": "The LAPS-SD algorithm employs multiple priority queues and allows for request execution preemption, adapting to dynamic token acceptance rates during decoding.", "result": "Extensive experiments show that LAPS-SD reduces average inference latency by approximately 39% compared to existing scheduling methods.", "conclusion": "LAPS-SD improves inference latency management for LLMs by adaptively scheduling requests based on their characteristics, achieving greater efficiency.", "key_contributions": ["Development of a semi-clairvoyant request scheduling algorithm called LAPS-SD.", "Introduction of adaptive scheduling based on request features during decoding.", "Demonstration of a significant reduction in inference latency (39%) compared to state-of-the-art methods."], "limitations": "", "keywords": ["Large Language Models", "speculative decoding", "request scheduling", "inference latency", "adaptive scheduling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17075", "pdf": "https://arxiv.org/pdf/2505.17075.pdf", "abs": "https://arxiv.org/abs/2505.17075", "title": "Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems", "authors": ["Fuma Kurata", "Mao Saeki", "Masaki Eguchi", "Shungo Suzuki", "Hiroaki Takatsu", "Yoichi Matsuyama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study aimed to develop and validate two scales of engagement and rapport\nto evaluate the user experience quality with multimodal dialogue systems in the\ncontext of foreign language learning. The scales were designed based on\ntheories of engagement in educational psychology, social psychology, and second\nlanguage acquisition.Seventy-four Japanese learners of English completed\nroleplay and discussion tasks with trained human tutors and a dialog agent.\nAfter each dialogic task was completed, they responded to the scales of\nengagement and rapport. The validity and reliability of the scales were\ninvestigated through two analyses. We first conducted analysis of Cronbach's\nalpha coefficient and a series of confirmatory factor analyses to test the\nstructural validity of the scales and the reliability of our designed items. We\nthen compared the scores of engagement and rapport between the dialogue with\nhuman tutors and the one with a dialogue agent. The results revealed that our\nscales succeeded in capturing the difference in the dialogue experience quality\nbetween the human interlocutors and the dialogue agent from multiple\nperspectives.", "AI": {"tldr": "The study develops and validates scales for measuring user engagement and rapport in multimodal dialogue systems for language learning, highlighting differences in experiences with human tutors versus dialogue agents.", "motivation": "To evaluate the user experience quality with multimodal dialogue systems in foreign language learning.", "method": "Seventy-four Japanese learners of English completed role-play and discussion tasks with human tutors and a dialogue agent, followed by responding to scales designed to measure engagement and rapport.", "result": "The scales showed valid and reliable measures of user experience, effectively capturing differences between interactions with human tutors and a dialogue agent.", "conclusion": "The developed scales can effectively assess the quality of user experiences in multimodal dialogue systems, providing valuable insights for improving language learning tools.", "key_contributions": ["Development of engagement and rapport scales for multimodal dialogue systems", "Validation of the scales through reliability and factor analyses", "Comparative analysis of user experience between human and agent interactions"], "limitations": "", "keywords": ["engagement", "rapport", "dialogue systems", "foreign language learning", "user experience"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076.pdf", "abs": "https://arxiv.org/abs/2505.17076", "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "5 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.", "AI": {"tldr": "This study investigates the impact of varying frame rates on speech tokenization in Mandarin and English, revealing significant language-specific differences affecting speech recognition tasks.", "motivation": "To explore the underexplored impact of frame rates on speech tokenization and its implications for speech tasks.", "method": "The study examines speech at different frame rates for Mandarin and English, evaluating the resulting semantic tokens in a speech recognition task.", "result": "Frame rate variations influence speech tokenization differently across languages, emphasizing the relationship between frame rates, phonetic density, and language-specific acoustic features.", "conclusion": "Optimizing frame rate selection for speech tokenizers can enhance performance in automatic speech recognition, text-to-speech, and other applications.", "key_contributions": ["Analysis of frame rate effects on speech tokenization for Mandarin and English", "Insights into language-specific acoustic feature interactions", "Recommendations for optimizing frame rate selection in speech applications"], "limitations": "", "keywords": ["speech tokenizer", "frame rate", "speech recognition", "Mandarin", "English"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.17078", "pdf": "https://arxiv.org/pdf/2505.17078.pdf", "abs": "https://arxiv.org/abs/2505.17078", "title": "GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace", "authors": ["Zenghao Duan", "Zhiyi Yin", "Zhichao Shi", "Liang Pang", "Shaoling Jing", "Jiayi Wu", "Yu Yan", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the underlying mechanisms of toxicity generation in\nLarge Language Models (LLMs) and proposes an effective detoxification approach.\nPrior work typically considers the Feed-Forward Network (FFN) as the main\nsource of toxicity, representing toxic regions as a set of toxic vectors or\nlayer-wise subspaces. However, our in-depth analysis reveals that the global\ntoxic subspace offers a more effective and comprehensive representation of\ntoxic region within the model. Building on this insight, we propose GloSS\n(Global Toxic Subspace Suppression), a lightweight, four-stage method that\nmitigates toxicity by identifying and removing the global toxic subspace from\nthe parameters of FFN. Experiments across a range of LLMs show that GloSS\nachieves state-of-the-art detoxification performance while preserving the\nmodels general capabilities, without requiring large-scale data or model\nretraining.", "AI": {"tldr": "This paper presents GloSS, a novel method for detoxifying Large Language Models by removing global toxic subspaces from feed-forward network parameters without large-scale data or retraining.", "motivation": "The paper aims to improve the understanding and mitigation of toxicity in Large Language Models, which is crucial for their safe deployment in applications.", "method": "The proposed GloSS method consists of a four-stage process to identify and suppress global toxic subspaces in the model parameters of FFNs.", "result": "Experiments demonstrate that GloSS achieves state-of-the-art detoxification performance while maintaining the overall capabilities of the models.", "conclusion": "GloSS provides an effective approach to mitigate toxicity in LLMs, emphasizing the importance of addressing global toxic subspaces rather than just layer-wise analyses.", "key_contributions": ["Introduction of GloSS, a novel detoxification method for LLMs.", "Empirical evidence showing the effectiveness of GloSS across various LLMs.", "Highlighting the significance of global toxic subspaces in the context of model toxicity."], "limitations": "The method may not address toxicity in all forms or contexts beyond the focus of the study.", "keywords": ["Large Language Models", "toxicity", "detoxification", "GloSS", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080.pdf", "abs": "https://arxiv.org/abs/2505.17080", "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "authors": ["Davide Picca"], "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.", "AI": {"tldr": "The paper argues for a semiotic perspective on Large Language Models (LLMs), suggesting they function as agents in the manipulation of signs rather than as cognitive systems or simulating human thought, thereby fostering a nuanced understanding of their role in cultural processes.", "motivation": "To challenge the cognitive framing of LLMs and offer a more accurate understanding of their functions within cultural dynamics.", "method": "The paper employs theoretical analysis combined with practical examples to illustrate the semiotic role of LLMs.", "result": "Demonstrates how LLMs can be viewed as semiotic agents that generate textual outputs, encouraging interpretation and contextual negotiation.", "conclusion": "The semiotic perspective emphasizes the contingent nature of meaning, reframing LLMs as participants in an ecology of signs that influence reading, writing, and knowledge production.", "key_contributions": ["Reconceptualizes LLMs as semiotic agents rather than cognitive systems", "Highlights ethical considerations in the application of LLMs", "Explores the implications of LLMs in various fields such as literature and education."], "limitations": "", "keywords": ["Large Language Models", "semiotics", "cognitive systems", "cultural processes", "interpretation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.17082", "pdf": "https://arxiv.org/pdf/2505.17082.pdf", "abs": "https://arxiv.org/abs/2505.17082", "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data", "authors": ["Abderrahman Skiredj", "Ferdaous Azhari", "Houdaifa Atou", "Nouamane Tazi", "Ismail Berrada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-source large language models (LLMs) still marginalise Moroccan Arabic\n(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters\nor to sacrifice the very reasoning skills that make LLMs useful. We show that a\nrigorously quality-over-quantity alignment strategy can surface fluent Darija\nwhile safeguarding the backbone s cross-lingual reasoning at a sliver of the\nusual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6\nK and TULU 50 K into Darija, preserve 20 of the English originals, and add\nmathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on\n5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the\nreasoning-dense TULU portion pushes it to 47.5 with no English regression.\nScaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which\nmatches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,\nscoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc\nretains Gemma-27B s strong maths and general-reasoning ability, showing only\nminimal movement on GSM8K and English benchmarks. The entire model is trained\nin just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable\nlanguage technology. We release code, data and checkpoints to spur\nDarija-centric applications in education, public services and everyday digital\ninteraction.", "AI": {"tldr": "This paper presents a strategy for improving the performance of large language models (LLMs) on Moroccan Arabic (Darija) while maintaining their cross-lingual reasoning abilities. Through a quality-over-quantity approach, the authors translate instruction suites into Darija and demonstrate the effectiveness of their tuned models, culminating in the release of a Darija-centric language model.", "motivation": "To address the marginalization of Moroccan Arabic in open-source large language models and improve their usability without sacrificing reasoning skills.", "method": "The authors employ a rigorous alignment strategy focusing on quality over quantity, translating instruction suites into Darija and adapting models using LoRA-tuning techniques.", "result": "The DarijaMMLU score improved significantly, with the model achieving 42.7, and further enhancements led to scores of 61.6 on DarijaMMLU and 60.5 on HellaSwag, outperforming existing models without degrading English performance.", "conclusion": "The work illustrates a sustainable, efficient pathway for developing inclusive language technologies, resulting in the release of a model that supports Darija in educational and public service applications.", "key_contributions": ["Introduced a quality-over-quantity alignment strategy for Darija", "Developed Darija-centric language models with strong reasoning capabilities", "Released resources for fostering Darija applications in various sectors"], "limitations": "", "keywords": ["Moroccan Arabic", "large language models", "Darija", "cross-lingual reasoning", "Green AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17083", "pdf": "https://arxiv.org/pdf/2505.17083.pdf", "abs": "https://arxiv.org/abs/2505.17083", "title": "Scale-invariant Attention", "authors": ["Ben Anson", "Xi Wang", "Laurence Aitchison"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Preprint", "summary": "One persistent challenge in LLM research is the development of attention\nmechanisms that are able to generalise from training on shorter contexts to\ninference on longer contexts. We propose two conditions that we expect all\neffective long context attention mechanisms to have: scale-invariant total\nattention, and scale-invariant attention sparsity. Under a Gaussian assumption,\nwe show that a simple position-dependent transformation of the attention logits\nis sufficient for these conditions to hold. Experimentally we find that the\nresulting scale-invariant attention scheme gives considerable benefits in terms\nof validation loss when zero-shot generalising from training on short contexts\nto validation on longer contexts, and is effective at long-context retrieval.", "AI": {"tldr": "The paper introduces a scale-invariant attention mechanism that improves long context generalization in LLMs by transforming attention logits.", "motivation": "To address the difficulty of LLMs in generalizing from short training contexts to longer inference contexts.", "method": "The authors propose conditions for effective long context attention, demonstrating that a position-dependent transformation of attention logits meets these conditions.", "result": "The proposed scale-invariant attention scheme significantly reduces validation loss when zero-shot generalizing from short to long contexts and enhances long-context retrieval performance.", "conclusion": "The study suggests that scale-invariant attention can effectively bridge the gap in LLM performance between short and long contexts.", "key_contributions": ["Introduction of scale-invariant total attention and attention sparsity conditions", "Position-dependent transformation of attention logits", "Experimental validation showing improved long-context performance"], "limitations": "", "keywords": ["LLM", "attention mechanisms", "long-context retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086.pdf", "abs": "https://arxiv.org/abs/2505.17086", "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination.\n  While Retrieval-Augmented Generation mitigates these issues by integrating\nexternal knowledge, existing approaches rely heavily on in-context learning,\nwhose performance is constrained by the fundamental reasoning capabilities of\nLLMs.\n  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex\nQuestion Answering, comprising a planner that decomposes questions into a\ndirected acyclic graph of subquestions and a worker that resolves questions via\nretrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy\nGradient Optimization), a novel reinforcement learning method that replaces\ntraditional policy gradient updates with Maximum Likelihood Estimation (MLE) by\nsampling trajectories from an asymptotically optimal policy. MyGO eliminates\nthe need for gradient rescaling and reference models, ensuring stable and\nefficient training.\n  Empirical results across multiple datasets demonstrate the effectiveness of\nMujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a\nscalable and resource-efficient solution for complex QA tasks.", "AI": {"tldr": "The paper proposes Mujica, a framework for Multi-hop Question Answering that enhances performance using a planner and the MyGO reinforcement learning method, leading to efficient and scalable solutions for LLMs in QA tasks.", "motivation": "Large Language Models struggle with factual knowledge and work on Question Answering due to hallucinations, prompting the need for improved methods.", "method": "Mujica consists of a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions through retrieval and reasoning. Additionally, the MyGO reinforcement learning method is introduced to optimize training using Maximum Likelihood Estimation instead of traditional gradient updates.", "result": "Empirical results show that Mujica-MyGO significantly improves multi-hop QA performance across various datasets for multiple LLMs.", "conclusion": "Mujica-MyGO offers a scalable and resource-efficient approach for tackling complex Question Answering tasks with large language models.", "key_contributions": ["Introduction of Mujica framework for enhanced multi-hop question answering", "Development of MyGO, a novel reinforcement learning optimization method", "Demonstration of improved performance across multiple datasets"], "limitations": "", "keywords": ["Large Language Models", "Multi-hop Question Answering", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17087", "pdf": "https://arxiv.org/pdf/2505.17087.pdf", "abs": "https://arxiv.org/abs/2505.17087", "title": "Informatics for Food Processing", "authors": ["Gordana Ispirova", "Michael Sebek", "Giulia Menichetti"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB", "cs.LG"], "comment": null, "summary": "This chapter explores the evolution, classification, and health implications\nof food processing, while emphasizing the transformative role of machine\nlearning, artificial intelligence (AI), and data science in advancing food\ninformatics. It begins with a historical overview and a critical review of\ntraditional classification frameworks such as NOVA, Nutri-Score, and SIGA,\nhighlighting their strengths and limitations, particularly the subjectivity and\nreproducibility challenges that hinder epidemiological research and public\npolicy. To address these issues, the chapter presents novel computational\napproaches, including FoodProX, a random forest model trained on nutrient\ncomposition data to infer processing levels and generate a continuous FPro\nscore. It also explores how large language models like BERT and BioBERT can\nsemantically embed food descriptions and ingredient lists for predictive tasks,\neven in the presence of missing data. A key contribution of the chapter is a\nnovel case study using the Open Food Facts database, showcasing how multimodal\nAI models can integrate structured and unstructured data to classify foods at\nscale, offering a new paradigm for food processing assessment in public health\nand research.", "AI": {"tldr": "This chapter discusses the impact of machine learning and AI on food processing classification and health implications, featuring novel computational methods like FoodProX and multimodal AI models.", "motivation": "To explore how machine learning and AI can enhance food processing classification and its health implications, addressing the challenges of traditional classification frameworks.", "method": "Presents historical overviews and critiques of classification frameworks, introduces FoodProX (a random forest model) and discusses the use of large language models like BERT and BioBERT for predictive tasks.", "result": "Demonstrates that novel computational approaches can improve the classification of food processing levels and have meaningful implications for public health.", "conclusion": "New computational methods and multimodal AI models can significantly advance food informatics and improve public health assessments.", "key_contributions": ["Introduction of FoodProX model for processing level inference", "Application of large language models for food data", "Case study using Open Food Facts database to classify food effectively"], "limitations": "Challenges include the subjectivity and reproducibility of traditional classification methods affecting research and policy.", "keywords": ["food processing", "machine learning", "artificial intelligence", "food informatics", "public health"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17089", "pdf": "https://arxiv.org/pdf/2505.17089.pdf", "abs": "https://arxiv.org/abs/2505.17089", "title": "Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models", "authors": ["Md Rafi Ur Rashid", "Vishnu Asutosh Dasu", "Ye Wang", "Gang Tan", "Shagufta Mehnaz"], "categories": ["cs.CL"], "comment": "26 pages, 2 figures", "summary": "Large Language Models (LLMs) exhibit impressive capabilities, but remain\nsusceptible to a growing spectrum of safety risks, including jailbreaks, toxic\ncontent, hallucinations, and bias. Existing defenses often address only a\nsingle threat type or resort to rigid outright rejection, sacrificing user\nexperience and failing to generalize across diverse and novel attacks. This\npaper introduces Adversarial Scenario Extrapolation (ASE), a novel\ninference-time computation framework that leverages Chain-of-Thought (CoT)\nreasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides\nthe LLM through a self-generative process of contemplating potential\nadversarial scenarios and formulating defensive strategies before generating a\nresponse to the user query. Comprehensive evaluation on four adversarial\nbenchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak\nattack success rates and minimal toxicity, while slashing outright rejections\nto <4%. ASE outperforms six state-of-the-art defenses in\nrobustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and\n4-10x lower bias scores. By transforming adversarial perception into an\nintrinsic cognitive process, ASE sets a new paradigm for secure and natural\nhuman-AI interaction.", "AI": {"tldr": "This paper presents Adversarial Scenario Extrapolation (ASE), a framework that enhances LLM robustness against adversarial attacks while maintaining user experience.", "motivation": "To address the growing safety risks of LLMs, including jailbreaks and toxicity, which current defenses inadequately handle.", "method": "ASE employs Chain-of-Thought reasoning and guides LLMs through a self-generative process to develop defensive strategies against potential adversarial scenarios during inference.", "result": "ASE achieves near-zero jailbreak attack success rates, minimal toxicity, and significantly low outright rejections, outperforming existing defenses.", "conclusion": "ASE reshapes adversarial perception in LLMs, establishing a new standard for secure human-AI interaction.", "key_contributions": ["Introduction of the ASE framework for LLMs", "Demonstrated significant improvements in defense against multiple adversarial attacks", "Set new standards for robustness and user experience in LLM applications"], "limitations": "", "keywords": ["Large Language Models", "Adversarial Attacks", "Human-AI Interaction", "Robustness", "Chain-of-Thought"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.17091", "pdf": "https://arxiv.org/pdf/2505.17091.pdf", "abs": "https://arxiv.org/abs/2505.17091", "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading", "authors": ["Prateek Verma", "Mert Pilanci"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "6 pages, 3 figures, 4 tables. Under Review WASPAA 2025", "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.", "AI": {"tldr": "The paper explores how a text-based LLM can internally develop capabilities to understand images and audio by training on text tokens, allowing for effective classification of these modalities without starting from scratch.", "motivation": "To demonstrate that text LLMs can generalize their learned representations to classify audio and visual data, offering a more efficient approach to model training.", "method": "The architecture of the model takes in patches of images and audio waveforms as inputs, producing embeddings or category labels similar to traditional classification pipelines.", "result": "The proposed model showed effectiveness in audio classification for datasets like FSD-50K and GTZAN, as well as in image classification tasks for CIFAR-10 and Fashion-MNIST.", "conclusion": "This work rediscovers the potential of text-based LLMs to provide insights into audio and visual data classification, suggesting a novel pathway for multi-modal learning without extensive retraining.", "key_contributions": ["Introduction of a novel architecture merging text LLMs with audio and image classification.", "Demonstration of effective classification across multiple datasets using a single model.", "Reduction in the need for training separate models for different modalities."], "limitations": "The study is still under review and may face limitations related to generalizability across different types of data and potential biases in datasets.", "keywords": ["LLM", "audio classification", "image classification", "multi-modal learning", "machine learning"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2505.17095", "pdf": "https://arxiv.org/pdf/2505.17095.pdf", "abs": "https://arxiv.org/abs/2505.17095", "title": "Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation", "authors": ["Kristine Ann M. Carandang", "Jasper Meynard P. Araña", "Ethan Robert A. Casin", "Christopher P. Monterola", "Daniel Stanley Y. Tan", "Jesus Felix B. Valenzuela", "Christian M. Alis"], "categories": ["cs.CL"], "comment": null, "summary": "Due to the legal and ethical responsibilities of healthcare providers (HCPs)\nfor accurate documentation and protection of patient data privacy, the natural\nvariability in the responses of large language models (LLMs) presents\nchallenges for incorporating clinical note generation (CNG) systems, driven by\nLLMs, into real-world clinical processes. The complexity is further amplified\nby the detailed nature of texts in CNG. To enhance the confidence of HCPs in\ntools powered by LLMs, this study evaluates the reliability of 12 open-weight\nand proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms\nof their ability to generate notes that are string equivalent (consistency\nrate), have the same meaning (semantic consistency) and are correct (semantic\nsimilarity), across several iterations using the same prompt. The results show\nthat (1) LLMs from all model families are stable, such that their responses are\nsemantically consistent despite being written in various ways, and (2) most of\nthe LLMs generated notes close to the corresponding notes made by experts.\nOverall, Meta's Llama 70B was the most reliable, followed by Mistral's Small\nmodel. With these findings, we recommend the local deployment of these\nrelatively smaller open-weight models for CNG to ensure compliance with data\nprivacy regulations, as well as to improve the efficiency of HCPs in clinical\ndocumentation.", "AI": {"tldr": "This study evaluates the reliability of 12 LLMs for clinical note generation, finding that they produce semantically consistent notes, with Meta's Llama 70B being the most reliable.", "motivation": "To address the legal and ethical responsibilities of healthcare providers regarding accurate documentation and patient data privacy, and the challenges posed by the variability in LLM responses.", "method": "The reliability of 12 open-weight and proprietary LLMs is evaluated for clinical note generation based on consistency rate, semantic consistency, and semantic similarity across iterations with the same prompt.", "result": "The study finds that LLMs from all families are stable and semantically consistent, with most generating notes similar to those made by experts, particularly highlighting that Meta's Llama 70B was the most reliable model.", "conclusion": "The paper recommends local deployment of smaller open-weight models for clinical note generation to enhance compliance with data privacy regulations and improve healthcare providers' documentation efficiency.", "key_contributions": ["Evaluation of multiple LLMs for clinical note generation reliability", "Findings on semantic consistency across LLM outputs", "Recommendations for using specific LLMs to improve clinical documentation"], "limitations": "", "keywords": ["clinical note generation", "LLMs", "data privacy", "semantic consistency", "healthcare providers"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17098", "pdf": "https://arxiv.org/pdf/2505.17098.pdf", "abs": "https://arxiv.org/abs/2505.17098", "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration", "authors": ["Yanshu Li", "Tian Yun", "Jianjiang Yang", "Pinyuan Feng", "Jinfa Huang", "Ruixiang Tang"], "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 11 figures, 19 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.04839", "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input in-context\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures in-context sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a valuable perspective for\ninterpreting and improving multimodal ICL.", "AI": {"tldr": "The paper introduces TACO, a transformer-based model that enhances multimodal in-context learning by utilizing task-aware attention to improve reasoning in large vision-language models.", "motivation": "To understand and improve the effectiveness of multimodal in-context learning (ICL) in large vision-language models (LVLMs), particularly how input sequences affect reasoning in tasks requiring complex reasoning or generation.", "method": "The authors interpret multimodal ICL using task mapping, which helps to understand local and global relationships in demonstration sequences. TACO is developed as a lightweight transformer model with task-aware attention for dynamic in-context sequence configuration during inference.", "result": "Experiments show that TACO outperforms baseline models across various ICL tasks on five different LVLMs using nine datasets, demonstrating its effectiveness in improving task reasoning and sequence construction.", "conclusion": "The study positions task mapping as a significant method for interpreting and enhancing multimodal ICL, suggesting that TACO effectively bridges the gap between input sequences and model reasoning.", "key_contributions": ["Introduction of TACO, a new model for improving ICL in LVLMs.", "Systematic interpretation of multimodal ICL through task mapping.", "Demonstration of TACO's superiority over existing baselines across multiple datasets."], "limitations": "The study is based on the analysis of task mapping and its implications, which may not cover all aspects of ICL effectiveness validation.", "keywords": ["multimodal learning", "large vision-language models", "in-context learning", "task-aware attention", "task mapping"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.17099", "pdf": "https://arxiv.org/pdf/2505.17099.pdf", "abs": "https://arxiv.org/abs/2505.17099", "title": "Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation", "authors": ["Xiaozhao Liu", "Dinggang Shen", "Xihui Liu"], "categories": ["cs.CL"], "comment": "Code, checkpoint and text samples available at\n  https://github.com/justin-xzliu/GLIM", "summary": "Pretrained generative models have opened new frontiers in brain decoding by\nenabling the synthesis of realistic texts and images from non-invasive brain\nrecordings. However, the reliability of such outputs remains\nquestionable--whether they truly reflect semantic activation in the brain, or\nare merely hallucinated by the powerful generative models. In this paper, we\nfocus on EEG-to-text decoding and address its hallucination issue through the\nlens of posterior collapse. Acknowledging the underlying mismatch in\ninformation capacity between EEG and text, we reframe the decoding task as\nsemantic summarization of core meanings rather than previously verbatim\nreconstruction of stimulus texts. To this end, we propose the Generative\nLanguage Inspection Model (GLIM), which emphasizes learning informative and\ninterpretable EEG representations to improve semantic grounding under\nheterogeneous and small-scale data conditions. Experiments on the public ZuCo\ndataset demonstrate that GLIM consistently generates fluent, EEG-grounded\nsentences without teacher forcing. Moreover, it supports more robust evaluation\nbeyond text similarity, through EEG-text retrieval and zero-shot semantic\nclassification across sentiment categories, relation types, and corpus topics.\nTogether, our architecture and evaluation protocols lay the foundation for\nreliable and scalable benchmarking in generative brain decoding.", "AI": {"tldr": "This paper presents the Generative Language Inspection Model (GLIM) for EEG-to-text decoding, addressing hallucination issues and enhancing semantic grounding.", "motivation": "To improve the reliability of EEG-to-text decoding, particularly in addressing the hallucination issue seen in generative models.", "method": "The paper proposes GLIM, which focuses on semantic summarization of EEG representations rather than verbatim reconstruction, improving grounding with small-scale data.", "result": "Experiments on the ZuCo dataset show that GLIM generates fluent, EEG-grounded sentences and excels in robust evaluations beyond text similarity.", "conclusion": "GLIM presents new protocols for reliable benchmarking in generative brain decoding, offering a novel approach to understanding brain signals through text.", "key_contributions": ["Introduction of the Generative Language Inspection Model (GLIM) for EEG-to-text decoding.", "Reframing the decoding task as semantic summarization rather than verbatim reconstruction.", "Development of robust evaluation metrics for generative brain decoding."], "limitations": "", "keywords": ["EEG", "text generation", "generative models", "brain decoding", "semantic summarization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17100", "pdf": "https://arxiv.org/pdf/2505.17100.pdf", "abs": "https://arxiv.org/abs/2505.17100", "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector", "authors": ["Haoyan Yang", "Runxue Bao", "Cao Xiao", "Jun Ma", "Parminder Bhatia", "Shangqian Gao", "Taha Kass-Hout"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.", "AI": {"tldr": "RBD is a module that detects biases in LLM evaluations and helps correct them, significantly improving evaluation accuracy and consistency.", "motivation": "LLM-as-a-Judge is a promising tool but suffers from biases in judgment that existing solutions fail to adequately mitigate.", "method": "Introduces the Reasoning-based Bias Detector (RBD) as an external module that identifies biased evaluations and provides reasoning for self-correction. The RBD pipeline includes biased dataset construction, supervision collection, model fine-tuning, and integration with LLM evaluators.", "result": "RBD models show consistent improvements across evaluation accuracy (avg. 18.5%) and consistency (avg. 10.9%) over various bias types using multiple LLM evaluators.", "conclusion": "RBD's effectiveness and scalability are proven through experiments, demonstrating strong generalization across biases and domains.", "key_contributions": ["Development of the Reasoning-based Bias Detector (RBD)", "Demonstration of significant performance improvements in evaluation accuracy and consistency", "Design of a comprehensive pipeline for bias detection and correction"], "limitations": "", "keywords": ["LLM", "bias detection", "self-correction", "evaluation", "reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17101", "pdf": "https://arxiv.org/pdf/2505.17101.pdf", "abs": "https://arxiv.org/abs/2505.17101", "title": "An approach to identify the most semantically informative deep representations of text and images", "authors": ["Santiago Acevedo", "Andrea Mascaretti", "Riccardo Rende", "Matéo Mahaut", "Marco Baroni", "Alessandro Laio"], "categories": ["cs.CL", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information is spread across many tokens\nand it is characterized by long-distance correlations between tokens and by a\ncausal left-to-right (i.e., past-future) asymmetry. We also identify layers\nencoding semantic information within visual transformers. We show that caption\nrepresentations in the semantic layers of LLMs predict visual representations\nof the corresponding images. We observe significant and model-dependent\ninformation asymmetries between image and text representations.", "AI": {"tldr": "The paper investigates how deep neural networks represent semantically related data across different domains, focusing on LLMs and vision transformers.", "motivation": "To understand how semantic information is encoded in representations of semantically related data and how it varies across different model sizes and architectures.", "method": "The authors analyze the relative information content of representations in LLMs and vision transformers, particularly examining layers that contain transferable semantic information.", "result": "Larger LLMs extract more general information, and semantic information in these layers shows long-distance correlations and causal asymmetries. There are significant model-dependent information asymmetries between image and text representations.", "conclusion": "Semantic layers in LLMs not only predict visual representations but also show varying degrees of information correlation based on model size.", "key_contributions": ["Quantitative method for investigating semantic representation in LLMs and vision models.", "Identification of semantic layers with transferable information.", "Discovery of causal asymmetries and information content spread across tokens in LLMs."], "limitations": "Findings are model-dependent; other architectures may not exhibit the same properties observed.", "keywords": ["neural networks", "semantic representation", "large language models", "vision transformers", "information content"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17102", "pdf": "https://arxiv.org/pdf/2505.17102.pdf", "abs": "https://arxiv.org/abs/2505.17102", "title": "BanglaByT5: Byte-Level Modelling for Bangla", "authors": ["Pramit Bhattacharyya", "Arnab Bhattacharya"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing tasks. However, most LLM models use traditional\ntokenizers like BPE and SentencePiece, which fail to capture the finer nuances\nof a morphologically rich language like Bangla (Bengali). In this work, we\nintroduce BanglaByT5, the first byte-level encoder-decoder model explicitly\ntailored for Bangla. Built upon a small variant of Googles ByT5 architecture,\nBanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality\nliterary and newspaper articles. Through zeroshot and supervised evaluations\nacross generative and classification tasks, BanglaByT5 demonstrates competitive\nperformance, surpassing several multilingual and larger models. Our findings\nhighlight the efficacy of byte-level modelling for morphologically rich\nlanguages and highlight BanglaByT5 potential as a lightweight yet powerful tool\nfor Bangla NLP, particularly in both resource-constrained and scalable\nenvironments.", "AI": {"tldr": "BanglaByT5 is a byte-level encoder-decoder model designed for Bangla, outperforming larger multilingual models in NLP tasks.", "motivation": "To address the limitations of traditional tokenizers in capturing the nuances of morphologically rich languages like Bangla.", "method": "BanglaByT5 is built on a small variant of Googles ByT5 architecture and pre-trained on a large curated corpus of Bangla texts.", "result": "BanglaByT5 showed competitive performance in zero-shot and supervised evaluations, exceeding capabilities of various multilingual models.", "conclusion": "The study illustrates the effectiveness of byte-level modeling for Bangla and proposes BanglaByT5 as a robust tool for NLP in resource-constrained settings.", "key_contributions": ["Introduction of BanglaByT5 as a specialized model for Bangla", "Demonstration of byte-level modeling benefits for morphologically rich languages", "Competitive performance in generative and classification tasks compared to larger models."], "limitations": "", "keywords": ["Bangla", "NLP", "Byte-level modeling", "LLM", "ByT5"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.17103", "pdf": "https://arxiv.org/pdf/2505.17103.pdf", "abs": "https://arxiv.org/abs/2505.17103", "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation", "authors": ["Cécile Rousseau", "Tobia Boschi", "Giandomenico Cornacchia", "Dhaval Salwala", "Alessandra Pascale", "Juan Bernabe Moreno"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. SDForger\nsource code will be open-sourced soon.", "AI": {"tldr": "SDForger is a new framework for generating multivariate time series using LLMs, achieving high quality and efficiency.", "motivation": "The need for efficient generation of high-quality multivariate time series data from limited samples to enhance forecasting capabilities.", "method": "SDForger transforms signal data into tabular embeddings, encodes these into text for fine-tuning autoregressive LLMs, and generates new time series by decoding textual embeddings.", "result": "SDForger demonstrates superior performance over existing generative models in generating synthetic time series on various datasets, maintaining statistical properties and dynamics.", "conclusion": "By enabling the integration of textual and time series data, SDForger has the potential to enhance multimodal modeling in applications.", "key_contributions": ["Introduces a framework for generating multivariate time series using LLMs", "Utilizes a compact data representation for efficient fine-tuning", "Facilitates multimodal modeling by combining time series and textual information."], "limitations": "", "keywords": ["time series generation", "LLM", "multivariate", "synthesis", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.17104", "pdf": "https://arxiv.org/pdf/2505.17104.pdf", "abs": "https://arxiv.org/abs/2505.17104", "title": "P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark", "authors": ["Tao Sun", "Enhao Pan", "Zhengkai Yang", "Kaixin Sui", "Jiajun Shi", "Xianfu Cheng", "Tongliang Li", "Wenhao Huang", "Ge Zhang", "Jian Yang", "Zhoujun Li"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Academic posters are vital for scholarly communication, yet their manual\ncreation is time-consuming. However, automated academic poster generation faces\nsignificant challenges in preserving intricate scientific details and achieving\neffective visual-textual integration. Existing approaches often struggle with\nsemantic richness and structural nuances, and lack standardized benchmarks for\nevaluating generated academic posters comprehensively. To address these\nlimitations, we introduce P2P, the first flexible, LLM-based multi-agent\nframework that generates high-quality, HTML-rendered academic posters directly\nfrom research papers, demonstrating strong potential for practical\napplications. P2P employs three specialized agents-for visual element\nprocessing, content generation, and final poster assembly-each integrated with\ndedicated checker modules to enable iterative refinement and ensure output\nquality. To foster advancements and rigorous evaluation in this domain, we\nconstruct and release P2PInstruct, the first large-scale instruction dataset\ncomprising over 30,000 high-quality examples tailored for the academic\npaper-to-poster generation task. Furthermore, we establish P2PEval, a\ncomprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation\nmethodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and\ndetailed, human-annotated checklists. Our contributions aim to streamline\nresearch dissemination and provide the community with robust tools for\ndeveloping and evaluating next-generation poster generation systems.", "AI": {"tldr": "A multi-agent framework named P2P is introduced for automating the generation of high-quality academic posters from research papers, addressing issues in visual-textual integration and evaluation.", "motivation": "To alleviate the time-consuming manual process of creating academic posters while preserving scientific detail and visual coherence.", "method": "P2P employs three specialized agents for visual processing, content generation, and assembly, supported by checker modules for iterative refinement, alongside the introduction of P2PInstruct dataset and P2PEval benchmark.", "result": "Successful generation of HTML-rendered academic posters that maintain semantic richness, alongside the establishment of a large-scale dataset and evaluation benchmark.", "conclusion": "P2P demonstrates strong potential for improving academic poster generation, facilitating better research dissemination through robust tools.", "key_contributions": ["Introduction of the P2P framework for academic poster generation", "Creation of P2PInstruct, a dataset with over 30,000 instruction examples", "Establishment of P2PEval, a comprehensive evaluation benchmark for poster generation"], "limitations": "", "keywords": ["academic poster generation", "LLM", "machine learning", "evaluation benchmarks", "dataset"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2505.17106", "pdf": "https://arxiv.org/pdf/2505.17106.pdf", "abs": "https://arxiv.org/abs/2505.17106", "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning", "authors": ["Yifei Liu", "Yu Cui", "Haibin Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "While tool learning significantly enhances the capabilities of large language\nmodels (LLMs), it also introduces substantial security risks. Prior research\nhas revealed various vulnerabilities in traditional LLMs during tool learning.\nHowever, the safety of newly emerging reasoning LLMs (RLLMs), such as\nDeepSeek-R1, in the context of tool learning remains underexplored. To bridge\nthis gap, we propose RRTL, a red teaming approach specifically designed to\nevaluate RLLMs in tool learning. It integrates two novel strategies: (1) the\nidentification of deceptive threats, which evaluates the model's behavior in\nconcealing the usage of unsafe tools and their potential risks; and (2) the use\nof Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also\nincludes a benchmark for traditional LLMs. We conduct a comprehensive\nevaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs\ngenerally achieve stronger safety performance than traditional LLMs, yet\nsubstantial safety disparities persist across models; (2) RLLMs can pose\nserious deceptive risks by frequently failing to disclose tool usage and to\nwarn users of potential tool output risks; (3) CoT prompting reveals\nmulti-lingual safety vulnerabilities in RLLMs. Our work provides important\ninsights into enhancing the security of RLLMs in tool learning.", "AI": {"tldr": "This paper presents RRTL, a red teaming approach to evaluate reasoning LLMs (RLLMs) in tool learning, addressing security risks and enhancing safety measures.", "motivation": "To explore the security risks associated with reasoning LLMs (RLLMs) in tool learning, which have been underexplored compared to traditional LLMs.", "method": "The RRTL approach uses two strategies: identifying deceptive threats that measure a model's concealment of unsafe tool usage and employing Chain-of-Thought (CoT) prompting to prompt tool invocation.", "result": "RLLMs generally show better safety performance than traditional LLMs, but large safety disparities exist. They can still pose risks by failing to disclose tool usage and potential output dangers, with CoT prompting highlighting multilingual safety vulnerabilities.", "conclusion": "The study offers crucial insights for improving the security of RLLMs in tool learning, emphasizing the need to address revealed safety vulnerabilities.", "key_contributions": ["Introduction of RRTL for evaluating RLLMs in tool learning", "Benchmarking safety performance between RLLMs and traditional LLMs", "Identification of multilingual safety vulnerabilities through CoT prompting."], "limitations": "The study primarily focuses on seven mainstream RLLMs and may not generalize across all emerging models.", "keywords": ["Reasoning LLMs", "Tool learning", "Security risks", "Chain-of-Thought prompting", "Red teaming"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17110", "pdf": "https://arxiv.org/pdf/2505.17110.pdf", "abs": "https://arxiv.org/abs/2505.17110", "title": "Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling", "authors": ["Junlin Li", "Guodong DU", "Jing Li", "Sim Kuan Goh", "Wenya Wang", "Yequan Wang", "Fangming Liu", "Ho-Kin Tang", "Saleh Alharbi", "Daojing He", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) with multimodal encoders on\nmodality-specific data expands the modalities that LLMs can handle, leading to\nthe formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies\non resource-intensive and inflexible fine-tuning from scratch with new\nmultimodal data. In this paper, we propose MMER (Multi-modality Expansion and\nRetention), a training-free approach that integrates existing MLLMs for\neffective multimodal expansion while retaining their original performance.\nSpecifically, MMER reuses MLLMs' multimodal encoders while merging their LLM\nparameters. By comparing original and merged LLM parameters, MMER generates\nbinary masks to approximately separate LLM parameters for each modality. These\ndecoupled parameters can independently process modality-specific inputs,\nreducing parameter conflicts and preserving original MLLMs' fidelity. MMER can\nalso mitigate catastrophic forgetting by applying a similar process to MLLMs\nfine-tuned on new tasks. Extensive experiments show significant improvements\nover baselines, proving that MMER effectively expands LLMs' multimodal\ncapabilities while retaining 99% of the original performance, and also markedly\nmitigates catastrophic forgetting.", "AI": {"tldr": "Proposes MMER, a training-free method for expanding multimodal capabilities of LLMs while retaining original performance and mitigating catastrophic forgetting.", "motivation": "To explore a more efficient approach for expanding the modalities that large language models can handle without the resource-intensive process of fine-tuning with new multimodal data.", "method": "MMER reuses existing multimodal encoders from MLLMs and merges LLM parameters. It generates binary masks to decouple LLM parameters for each modality, allowing independent processing of modality-specific inputs and reducing parameter conflicts.", "result": "MMER demonstrates significant improvements over baseline models in expanding multimodal capabilities while retaining 99% of the original performance and addressing catastrophic forgetting.", "conclusion": "MMER provides a novel solution to enhance multimodal LLMs effectively and efficiently while maintaining their original capabilities and reducing the risk of performance loss due to integration of new modalities.", "key_contributions": ["Introduction of MMER, a training-free approach for multimodal expansion.", "Effective parameter decoupling to maintain performance across modalities.", "Mitigation of catastrophic forgetting in fine-tuned LLMs."], "limitations": "", "keywords": ["Multimodal LLMs", "Parameter Decoupling", "Catastrophic Forgetting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17112", "pdf": "https://arxiv.org/pdf/2505.17112.pdf", "abs": "https://arxiv.org/abs/2505.17112", "title": "Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek", "authors": ["Robin Segerer"], "categories": ["cs.CL"], "comment": "15 pages, 1 table, 1 figure", "summary": "This study examines cultural value alignment in large language models (LLMs)\nby analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from\nSchwartz's value framework. Using the 40-item Portrait Values Questionnaire, we\nassessed whether DeepSeek, trained on Chinese-language data, exhibits distinct\nvalue preferences compared to Western models. Results of a Bayesian ordinal\nregression model show that self-transcendence values (e.g., benevolence,\nuniversalism) were highly prioritized across all models, reflecting a general\nLLM tendency to emphasize prosocial values. However, DeepSeek uniquely\ndownplayed self-enhancement values (e.g., power, achievement) compared to\nChatGPT and Gemini, aligning with collectivist cultural tendencies. These\nfindings suggest that LLMs reflect culturally situated biases rather than a\nuniversal ethical framework. To address value asymmetries in LLMs, we propose\nmulti-perspective reasoning, self-reflective feedback, and dynamic\ncontextualization. This study contributes to discussions on AI fairness,\ncultural neutrality, and the need for pluralistic AI alignment frameworks that\nintegrate diverse moral perspectives.", "AI": {"tldr": "This study analyzes cultural value alignment in LLMs, focusing on differences between models like Gemini, ChatGPT, and DeepSeek, highlighting the influence of cultural context on AI values.", "motivation": "To explore how LLMs align with cultural values, particularly examining the differences in value prioritization between Western models and DeepSeek, which is trained on Chinese-language data.", "method": "The study employs the 40-item Portrait Values Questionnaire and utilizes a Bayesian ordinal regression model to assess value preferences across the examined models.", "result": "The results indicate that while all models prioritize self-transcendence values, DeepSeek shows a significant downplaying of self-enhancement values, reflecting collectivist cultural tendencies.", "conclusion": "The findings suggest that LLMs mirror culturally situated biases and emphasize the need for pluralistic AI alignment frameworks that account for diverse moral perspectives to enhance AI fairness.", "key_contributions": ["Analysis of cultural differences in LLM value alignment", "Identification of self-transcendence and self-enhancement value prioritization", "Proposal of multi-perspective reasoning and dynamic contextualization for AI alignment"], "limitations": "", "keywords": ["cultural value alignment", "large language models", "AI fairness", "moral perspectives", "collectivism"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114.pdf", "abs": "https://arxiv.org/abs/2505.17114", "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.", "AI": {"tldr": "RAVEN is a multimodal question answering framework that improves the identification of relevant tokens across video, audio, and sensor modalities to enhance QA performance.", "motivation": "Current multimodal QA systems struggle with modality disagreements, leading to poor fusion performance.", "method": "RAVEN employs a three-stage training pipeline: unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning, utilizing a cross-modal gating module called QuART that assigns relevance scores to each token.", "result": "RAVEN shows significant accuracy improvements on multiple multimodal QA benchmarks, outperforming state-of-the-art models, especially when incorporating sensor data.", "conclusion": "The proposed method demonstrates that targeted training techniques can drastically improve the performance and robustness of multimodal question answering systems.", "key_contributions": ["Introduction of QuART module for relevance scoring across modalities", "Three-stage training pipeline addressing distinct multimodal challenges", "Release of the AVS-QA dataset with 300K synchronized audio-video-sensor streams."], "limitations": "", "keywords": ["multimodal", "question answering", "cross-modal gating", "sensor data", "dataset release"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.17116", "pdf": "https://arxiv.org/pdf/2505.17116.pdf", "abs": "https://arxiv.org/abs/2505.17116", "title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data", "authors": ["Akash Dhruv", "Yangxinyu Xie", "Jordan Branham", "Tanwi Mallick"], "categories": ["cs.CL", "cs.ET"], "comment": null, "summary": "This paper presents a comparative study of large language models (LLMs) in\ninterpreting grid-structured geospatial data. We evaluate the performance of a\nbase model through structured prompting and contrast it with a fine-tuned\nvariant trained on a dataset of user-assistant interactions. Our results\nhighlight the strengths and limitations of zero-shot prompting and demonstrate\nthe benefits of fine-tuning for structured geospatial and temporal reasoning.", "AI": {"tldr": "A study comparing LLMs in handling grid-structured geospatial data, highlighting the advantages of fine-tuning for reasoning tasks.", "motivation": "To evaluate how effectively LLMs can interpret grid-structured geospatial data and the impact of fine-tuning.", "method": "Comparative analysis of a base LLM model using structured prompting versus a fine-tuned model on user-assistant interaction data.", "result": "Fine-tuned models show improved performance in structured geospatial and temporal reasoning compared to base models using zero-shot prompting.", "conclusion": "Fine-tuning LLMs enhances their capabilities in interpreting structured geospatial data, revealing the importance of model adaptation for specific tasks.", "key_contributions": ["Comparative analysis of base and fine-tuned LLMs", "Insights into zero-shot versus fine-tuning approaches", "Evaluation of structured geospatial reasoning capabilities"], "limitations": "", "keywords": ["large language models", "geospatial data", "fine-tuning", "structured prompting", "temporal reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117.pdf", "abs": "https://arxiv.org/abs/2505.17117", "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "AI": {"tldr": "This paper introduces a framework to compare human and LLM categorization strategies, revealing LLMs' strong bias towards statistical compression over nuanced semantic representation.", "motivation": "To understand how Large Language Models (LLMs) compare to human categorization in terms of semantic compression and fidelity.", "method": "The authors use an information-theoretic framework based on Rate-Distortion Theory and the Information Bottleneck principle to analyze token embeddings from various LLMs against human categorization benchmarks.", "result": "LLMs form broad categories aligning with human judgment but fail to capture fine-grained semantic distinctions, showing a bias towards aggressive statistical compression.", "conclusion": "The findings highlight crucial differences between AI and human cognitive architectures, suggesting pathways for improving LLMs to align more closely with human conceptual representations.", "key_contributions": ["Novel information-theoretic framework for comparing knowledge organization in humans and LLMs", "Quantitative analysis showing LLMs' bias toward statistical compression", "Insights into the differences between AI and human cognitive architectures"], "limitations": "The analysis may not cover all aspects of human cognition or account for all types of language tasks.", "keywords": ["Large Language Models", "semantic compression", "human categorization", "information theory", "cognitive architectures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17118", "pdf": "https://arxiv.org/pdf/2505.17118.pdf", "abs": "https://arxiv.org/abs/2505.17118", "title": "After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG", "authors": ["Xinbang Dai", "Huikang Hu", "Yuncheng Hua", "Jiaqi Li", "Yongrui Chen", "Rihui Jin", "Nan Hu", "Guilin Qi"], "categories": ["cs.CL", "I.2.7"], "comment": "24 pages, 8 figures", "summary": "Retrieval-augmented generation (RAG) systems face critical challenges in\nbalancing internal (parametric) and external (retrieved) knowledge, especially\nwhen these sources conflict or are unreliable. To analyze these scenarios\ncomprehensively, we construct the Trustworthiness Response Dataset (TRD) with\n36,266 questions spanning four RAG settings. We reveal that existing approaches\naddress isolated scenarios-prioritizing one knowledge source, naively merging\nboth, or refusing answers-but lack a unified framework to handle different\nreal-world conditions simultaneously. Therefore, we propose the BRIDGE\nframework, which dynamically determines a comprehensive response strategy of\nlarge language models (LLMs). BRIDGE leverages an adaptive weighting mechanism\nnamed soft bias to guide knowledge collection, followed by a Maximum Soft-bias\nDecision Tree to evaluate knowledge and select optimal response strategies\n(trust internal/external knowledge, or refuse). Experiments show BRIDGE\noutperforms baselines by 5-15% in accuracy while maintaining balanced\nperformance across all scenarios. Our work provides an effective solution for\nLLMs' trustworthy responses in real-world RAG applications.", "AI": {"tldr": "This paper addresses the challenges faced by Retrieval-augmented generation (RAG) systems in managing conflicting knowledge sources by introducing the BRIDGE framework for improved response strategies in LLMs.", "motivation": "Retrieval-augmented generation systems struggle to balance and utilize both internal (parametric) and external (retrieved) knowledge effectively, particularly in scenarios where these knowledge sources may conflict or be unreliable.", "method": "The authors developed the BRIDGE framework, which employs an adaptive weighting mechanism called soft bias to guide knowledge collection and a Maximum Soft-bias Decision Tree to evaluate and select optimal response strategies for LLMs.", "result": "The BRIDGE framework outperforms existing baselines by 5-15% in accuracy, demonstrating improved performance while maintaining a balanced approach across various scenarios.", "conclusion": "The study provides an effective framework for enhancing the reliability of responses from LLMs in real-world RAG applications, addressing limitations of current methods which tend to focus on isolated knowledge sources.", "key_contributions": ["Introduction of the Trustworthiness Response Dataset (TRD) with 36,266 questions.", "Development of the BRIDGE framework for dynamic response strategy determination.", "Implementation of an adaptive weighting mechanism to improve knowledge collection and decision-making."], "limitations": "", "keywords": ["Retrieval-augmented generation", "Trustworthiness", "Large language models", "Knowledge balancing", "Framework"], "importance_score": 9, "read_time_minutes": 24}}
{"id": "2505.17119", "pdf": "https://arxiv.org/pdf/2505.17119.pdf", "abs": "https://arxiv.org/abs/2505.17119", "title": "Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models", "authors": ["Zongru Shao", "Xin Wang", "Zhanyang Liu", "Chenhan Wang", "K. P. Subbalakshmi"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages without references", "summary": "Recent research leverages large language models (LLMs) for early mental\nhealth detection, such as depression, often optimized with machine-generated\ndata. However, their detection may be subject to unknown weaknesses. Meanwhile,\nquality control has not been applied to these generated corpora besides limited\nhuman verifications. Our goal is to systematically evaluate LLM reasoning and\nreveal potential weaknesses. To this end, we first provide a systematic\nevaluation of the reasoning over machine-generated detection and\ninterpretation. Then we use the models' reasoning abilities to explore\nmitigation strategies for enhanced performance. Specifically, we do the\nfollowing: A. Design an LLM instruction strategy that allows for systematic\nanalysis of the detection by breaking down the task into several subtasks. B.\nDesign contrastive few-shot and chain-of-thought prompts by selecting typical\npositive and negative examples of detection reasoning. C. Perform human\nannotation for the subtasks identified in the first step and evaluate the\nperformance. D. Identify human-preferred detection with desired logical\nreasoning from the few-shot generation and use them to explore different\noptimization strategies. We conducted extensive comparisons on the DepTweet\ndataset across the following subtasks: 1. identifying whether the speaker is\ndescribing their own depression; 2. accurately detecting the presence of PHQ-9\nsymptoms, and 3. finally, detecting depression. Human verification of\nstatistical outliers shows that LLMs demonstrate greater accuracy in analyzing\nand detecting explicit language of depression as opposed to implicit\nexpressions of depression. Two optimization methods are used for performance\nenhancement and reduction of the statistic bias: supervised fine-tuning (SFT)\nand direct preference optimization (DPO). Notably, the DPO approach achieves\nsignificant performance improvement.", "AI": {"tldr": "This paper systematically evaluates the reasoning of large language models in detecting depression from machine-generated data and proposes optimization strategies for improved performance.", "motivation": "To assess the weaknesses of LLMs in early mental health detection and enhance the quality control of machine-generated data.", "method": "Conduct a systematic evaluation of LLM reasoning in detecting depression, employing a structured instruction strategy, contrastive prompts, and human annotations on the DepTweet dataset.", "result": "LLMs showed higher accuracy in detecting explicit expressions of depression compared to implicit ones. Two optimization methods, supervised fine-tuning and direct preference optimization, were implemented, with DPO yielding significant performance improvements.", "conclusion": "Enhancing LLM performance for depression detection requires systematic evaluation, specific optimization strategies, and human involvement for quality assessment.", "key_contributions": ["Systematic evaluation of LLM reasoning in mental health detection", "Development of optimized instruction and prompt strategies", "Human verification and comparison of detection performance"], "limitations": "", "keywords": ["Large Language Models", "Mental Health Detection", "Depression", "Machine Learning", "Optimization Strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17120", "pdf": "https://arxiv.org/pdf/2505.17120.pdf", "abs": "https://arxiv.org/abs/2505.17120", "title": "Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training", "authors": ["Dillon Plunkett", "Adam Morris", "Keerthi Reddy", "Jorge Morales"], "categories": ["cs.CL"], "comment": null, "summary": "We have only limited understanding of how and why large language models\n(LLMs) respond in the ways that they do. Their neural networks have proven\nchallenging to interpret, and we are only beginning to tease out the function\nof individual neurons and circuits within them. However, another path to\nunderstanding these systems is to investigate and develop their capacity to\nintrospect and explain their own functioning. Here, we show that i)\ncontemporary LLMs are capable of providing accurate, quantitative descriptions\nof their own internal processes during certain kinds of decision-making, ii)\nthat it is possible to improve these capabilities through training, and iii)\nthat this training generalizes to at least some degree. To do so, we fine-tuned\nGPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts\n(e.g., choosing between condos, loans, vacations, etc.) according to\nrandomly-generated, quantitative preferences about how to weigh different\nattributes during decision-making (e.g., the relative importance of natural\nlight versus quiet surroundings for condos). We demonstrate that the LLMs can\naccurately report these preferences (i.e., the weights that they learned to\ngive to different attributes during decision-making). Next, we demonstrate that\nthese LLMs can be fine-tuned to explain their decision-making even more\naccurately. Finally, we demonstrate that this training generalizes: It improves\nthe ability of the models to accurately explain what they are doing as they\nmake other complex decisions, not just decisions they have learned to make via\nfine-tuning. This work is a step towards training LLMs to accurately and\nbroadly report on their own internal processes -- a possibility that would\nyield substantial benefits for interpretability, control, and safety.", "AI": {"tldr": "The paper explores how contemporary large language models (LLMs) can introspect and explain their own decision-making processes, demonstrating that fine-tuning enhances their accuracy in reporting and explaining their decision-making based on learned preferences.", "motivation": "To better understand and improve the interpretability of large language models by enabling them to introspect and explain their own behavior during decision-making processes.", "method": "Fine-tuned GPT-4o and GPT-4o-mini models were trained to make decisions based on quantitative preferences across various complex scenarios, with evaluations on their ability to report and explain their decision-making processes.", "result": "The models provided accurate descriptions of their internal processes and exhibited improved explanatory capacities after fine-tuning, with these capabilities generalizing to other decision-making contexts.", "conclusion": "This research advances the interpretability of LLMs, suggesting that training them to introspect can lead to significant benefits in understanding and controlling AI behavior.", "key_contributions": ["Proved that LLMs can accurately report their preferences in decision-making contexts.", "Developed training methods that enhance LLMs' capacity for introspection and explanation.", "Demonstrated that improvements in explanatory capabilities generalize beyond the trained scenarios."], "limitations": "", "keywords": ["large language models", "interpretability", "decision-making", "fine-tuning", "explanation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17121", "pdf": "https://arxiv.org/pdf/2505.17121.pdf", "abs": "https://arxiv.org/abs/2505.17121", "title": "NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation", "authors": ["Weiming Wu", "Zi-kang Wang", "Jin Ye", "Zhi Zhou", "Yu-Feng Li", "Lan-Zhe Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Obtaining large-scale, high-quality data with reasoning paths is crucial for\nimproving the geometric reasoning capabilities of multi-modal large language\nmodels (MLLMs). However, existing data generation methods, whether based on\npredefined templates or constrained symbolic provers, inevitably face diversity\nand numerical generalization limitations. To address these limitations, we\npropose NeSyGeo, a novel neuro-symbolic framework for generating geometric\nreasoning data. First, we propose a domain-specific language grounded in the\nentity-relation-constraint paradigm to comprehensively represent all components\nof plane geometry, along with generative actions defined within this symbolic\nspace. We then design a symbolic-visual-text pipeline that synthesizes symbolic\nsequences, maps them to corresponding visual and textual representations, and\ngenerates diverse question-answer (Q&A) pairs using large language models\n(LLMs). To the best of our knowledge, we are the first to propose a\nneuro-symbolic approach in generating multimodal reasoning data. Based on this\nframework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing\n100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric\nreasoning abilities in MLLMs. Experiments demonstrate that the proposal\nsignificantly and consistently improves the performance of multiple MLLMs under\nboth reinforcement and supervised fine-tuning. With only 4k samples and two\nepochs of reinforcement fine-tuning, base models achieve improvements of up to\n+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B\nmodel can be improved to outperform an 8B model from the same series on\ngeometric reasoning tasks.", "AI": {"tldr": "NeSyGeo is a novel neuro-symbolic framework for generating high-quality geometric reasoning data, addressing limitations of existing methods by synthesizing symbolic sequences into multimodal representations and improving multi-modal large language models (MLLMs) performance.", "motivation": "The motivation of this paper is to improve the geometric reasoning capabilities of multi-modal large language models (MLLMs) by addressing the diversity and numerical generalization limitations of existing data generation methods.", "method": "The authors propose a neuro-symbolic framework called NeSyGeo, utilizing a domain-specific language to represent plane geometry components and a pipeline that maps symbolic sequences to visual and textual representations, generating diverse question-answer pairs.", "result": "Experiments show significant performance improvements for multiple MLLMs with the NeSyGeo approach, achieving up to +15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA with limited training data.", "conclusion": "The proposed neuro-symbolic approach provides a new benchmark for geometric reasoning in MLLMs, showcasing its effectiveness and potential for enhancing reasoning capabilities.", "key_contributions": ["Introduction of a neuro-symbolic framework for geometric reasoning data generation", "Creation of the NeSyGeo-CoT and NeSyGeo-Caption datasets with 100k samples", "Development of a new benchmark, NeSyGeo-Test, for evaluating MLLMs in geometric reasoning"], "limitations": "", "keywords": ["neuro-symbolic framework", "geometric reasoning", "multi-modal large language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.17122", "pdf": "https://arxiv.org/pdf/2505.17122.pdf", "abs": "https://arxiv.org/abs/2505.17122", "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?", "authors": ["Xuan Qi", "Jiahao Qiu", "Xinzhe Juan", "Yue Wu", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "17 pages, 7 figures", "summary": "Aligning large language models (LLMs) with human preferences remains a key\nchallenge in AI. Preference-based optimization methods, such as Reinforcement\nLearning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),\nrely on human-annotated datasets to improve alignment. In this work, we\nidentify a crucial property of the existing learning method: the distinguishing\nsignal obtained in preferred responses is often concentrated in the early\ntokens. We refer to this as shallow preference signals.\n  To explore this property, we systematically truncate preference datasets at\nvarious points and train both reward models and DPO models on the truncated\ndata. Surprisingly, models trained on truncated datasets, retaining only the\nfirst half or fewer tokens, achieve comparable or even superior performance to\nthose trained on full datasets. For example, a reward model trained on the\nSkywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when\ntrained on a 40\\% truncated dataset. This pattern is consistent across multiple\ndatasets, suggesting the widespread presence of shallow preference signals.\n  We further investigate the distribution of the reward signal through decoding\nstrategies. We consider two simple decoding strategies motivated by the shallow\nreward signal observation, namely Length Control Decoding and KL Threshold\nControl Decoding, which leverage shallow preference signals to optimize the\ntrade-off between alignment and computational efficiency. The performance is\neven better, which again validates our hypothesis.\n  The phenomenon of shallow preference signals highlights potential issues in\nLLM alignment: existing alignment methods often focus on aligning only the\ninitial tokens of responses, rather than considering the full response. This\ncould lead to discrepancies with real-world human preferences, resulting in\nsuboptimal alignment performance.", "AI": {"tldr": "This paper investigates the phenomenon of shallow preference signals in large language models, where useful preference information is concentrated in the early tokens of responses, revealing potential issues in alignment methodologies.", "motivation": "The need to align large language models with human preferences effectively while identifying the importance of early tokens in responses.", "method": "The authors truncate preference datasets at various points and train reward models and Direct Preference Optimization (DPO) models on these truncated datasets, analyzing their performance.", "result": "Models trained on truncated datasets, particularly retaining only the early tokens, achieve similar or superior performance compared to those trained on full datasets, indicating the presence of shallow preference signals.", "conclusion": "Shallow preference signals can lead to misalignment with real-world human preferences, necessitating a reevaluation of how alignment methods are designed and implemented.", "key_contributions": ["Identification of shallow preference signals in preference-based optimization methods", "Demonstration that models can perform well using truncated datasets focused on early tokens", "Introduction of decoding strategies to leverage shallow preference signals for improved alignment."], "limitations": "", "keywords": ["large language models", "human preferences", "reinforcement learning", "preference optimization", "alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17123", "pdf": "https://arxiv.org/pdf/2505.17123.pdf", "abs": "https://arxiv.org/abs/2505.17123", "title": "MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation", "authors": ["Xiaoyuan Li", "Keqin Bao", "Yubo Ma", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent advances in Large Language Models (LLMs) have shown promising results\nin complex reasoning tasks. However, current evaluations predominantly focus on\nsingle-turn reasoning scenarios, leaving interactive tasks largely unexplored.\nWe attribute it to the absence of comprehensive datasets and scalable automatic\nevaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'\nMulti-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600\ninstances, MTR-Bench covers diverse reasoning capabilities, fine-grained\ndifficulty granularity, and necessitates multi-turn interactions with the\nenvironments. Moreover, MTR-Bench features fully-automated framework spanning\nboth dataset constructions and model evaluations, which enables scalable\nassessment without human interventions. Extensive experiments reveal that even\nthe cutting-edge reasoning models fall short of multi-turn, interactive\nreasoning tasks. And the further analysis upon these results brings valuable\ninsights for future research in interactive AI systems.", "AI": {"tldr": "MTR-Bench provides a comprehensive evaluation framework for multi-turn reasoning tasks in LLMs.", "motivation": "To address the lack of datasets and evaluation protocols for multi-turn reasoning tasks in LLMs, which are currently underexplored compared to single-turn scenarios.", "method": "MTR-Bench consists of 4 classes, 40 tasks, and 3600 instances designed to evaluate diverse reasoning capabilities through multi-turn interactions. It includes a fully-automated framework for dataset construction and model evaluation.", "result": "Extensive experiments show that leading reasoning models struggle with multi-turn interactive tasks, highlighting significant performance gaps.", "conclusion": "The findings suggest that current models are not sufficiently equipped for interactive reasoning, pointing towards the need for improvements in research and model development.", "key_contributions": ["Introduction of MTR-Bench for evaluating multi-turn reasoning in LLMs.", "Development of an automated framework for dataset and model evaluation.", "Insights into the limitations of existing reasoning models in multi-turn contexts."], "limitations": "The study only examines a limited set of reasoning models and may not cover all interactive AI scenarios.", "keywords": ["Multi-Turn Reasoning", "Large Language Models", "Dataset Construction", "Model Evaluation", "Interactive AI Systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17126", "pdf": "https://arxiv.org/pdf/2505.17126.pdf", "abs": "https://arxiv.org/abs/2505.17126", "title": "Conformal Language Model Reasoning with Coherent Factuality", "authors": ["Maxon Rubin-Toles", "Maya Gambhir", "Keshav Ramji", "Aaron Roth", "Surbhi Goel"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Language models are increasingly being used in important decision pipelines,\nso ensuring the correctness of their outputs is crucial. Recent work has\nproposed evaluating the \"factuality\" of claims decomposed from a language model\ngeneration and applying conformal prediction techniques to filter out those\nclaims that are not factual. This can be effective for tasks such as\ninformation retrieval, where constituent claims may be evaluated in isolation\nfor factuality, but is not appropriate for reasoning tasks, as steps of a\nlogical argument can be evaluated for correctness only within the context of\nthe claims that precede them. To capture this, we define \"coherent factuality\"\nand develop a conformal-prediction-based method to guarantee coherent\nfactuality for language model outputs. Our approach applies split conformal\nprediction to subgraphs within a \"deducibility\" graph\" that represents the\nsteps of a reasoning problem. We evaluate our method on mathematical reasoning\nproblems from the MATH and FELM datasets and find that our algorithm\nconsistently produces correct and substantiated orderings of claims, achieving\ncoherent factuality across target coverage levels. Moreover, we achieve 90%\nfactuality on our stricter definition while retaining 80% or more of the\noriginal claims, highlighting the utility of our deducibility-graph-guided\napproach.", "AI": {"tldr": "The paper introduces a method to ensure coherent factuality in language model outputs, particularly for reasoning tasks, using split conformal prediction techniques.", "motivation": "As language models are increasingly utilized in decision-making processes, it becomes vital to ensure the correctness of their outputs, particularly in reasoning tasks where context matters.", "method": "The authors develop a method based on split conformal prediction applied to deducibility graphs, which represent the steps in a reasoning problem.", "result": "The proposed method demonstrates that it can achieve 90% factuality with 80% retention of original claims, effectively ensuring coherent factuality in outputs.", "conclusion": "The deducibility-graph-guided approach significantly improves the reliability of language models in generating coherent and factually correct outputs for reasoning tasks.", "key_contributions": ["Definition of coherent factuality", "Development of a split conformal prediction method", "Evaluation on mathematical reasoning datasets (MATH and FELM)"], "limitations": "", "keywords": ["language models", "factuality", "coherent reasoning", "conformal prediction", "deducibility graphs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17131", "pdf": "https://arxiv.org/pdf/2505.17131.pdf", "abs": "https://arxiv.org/abs/2505.17131", "title": "Relative Bias: A Comparative Framework for Quantifying Bias in LLMs", "authors": ["Alireza Arbabi", "Florian Kerschbaum"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "The growing deployment of large language models (LLMs) has amplified concerns\nregarding their inherent biases, raising critical questions about their\nfairness, safety, and societal impact. However, quantifying LLM bias remains a\nfundamental challenge, complicated by the ambiguity of what \"bias\" entails.\nThis challenge grows as new models emerge rapidly and gain widespread use,\nwhile introducing potential biases that have not been systematically assessed.\nIn this paper, we propose the Relative Bias framework, a method designed to\nassess how an LLM's behavior deviates from other LLMs within a specified target\ndomain. We introduce two complementary methodologies: (1) Embedding\nTransformation analysis, which captures relative bias patterns through sentence\nrepresentations over the embedding space, and (2) LLM-as-a-Judge, which employs\na language model to evaluate outputs comparatively. Applying our framework to\nseveral case studies on bias and alignment scenarios following by statistical\ntests for validation, we find strong alignment between the two scoring methods,\noffering a systematic, scalable, and statistically grounded approach for\ncomparative bias analysis in LLMs.", "AI": {"tldr": "The paper introduces the Relative Bias framework to assess and quantify biases in large language models (LLMs) by comparing their behaviors in a given domain using two methodologies: Embedding Transformation analysis and LLM-as-a-Judge.", "motivation": "To address concerns about biases in large language models and establish a systematic approach to quantify and analyze these biases given the rapid development and deployment of LLMs.", "method": "The Relative Bias framework incorporates two methods: Embedding Transformation analysis for capturing bias patterns using sentence representations and LLM-as-a-Judge for using a language model to evaluate outputs comparatively.", "result": "The framework displays strong alignment between its scoring methods, validated through statistical tests across various bias and alignment case studies.", "conclusion": "The proposed methods provide a systematic, scalable, and statistically grounded technique for conducting comparative bias analysis in LLMs, which is crucial for addressing fairness and safety concerns.", "key_contributions": ["Introduction of the Relative Bias framework for LLMs", "Development of two innovative methodologies for bias assessment", "Validation of frameworks through empirical case studies"], "limitations": "", "keywords": ["large language models", "bias assessment", "comparative analysis", "embedding transformation", "fairness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17134", "pdf": "https://arxiv.org/pdf/2505.17134.pdf", "abs": "https://arxiv.org/abs/2505.17134", "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality long-context instruction data is essential for aligning\nlong-context large language models (LLMs). Despite the public release of models\nlike Qwen and Llama, their long-context instruction data remains proprietary.\nHuman annotation is costly and challenging, while template-based synthesis\nmethods limit scale, diversity, and quality. We introduce LongMagpie, a\nself-synthesis framework that automatically generates large-scale long-context\ninstruction data. Our key insight is that aligned long-context LLMs, when\npresented with a document followed by special tokens preceding a user turn,\nauto-regressively generate contextually relevant queries. By harvesting these\ndocument-query pairs and the model's responses, LongMagpie produces\nhigh-quality instructions without human effort. Experiments on HELMET, RULER,\nand Longbench v2 demonstrate that LongMagpie achieves leading performance on\nlong-context tasks while maintaining competitive performance on short-context\ntasks, establishing it as a simple and effective approach for open, diverse,\nand scalable long-context instruction data synthesis.", "AI": {"tldr": "LongMagpie is a self-synthesis framework that generates large-scale long-context instruction data for aligned LLMs without human effort, achieving leading performance in long-context tasks.", "motivation": "To address the challenge of generating high-quality long-context instruction data for LLMs, which is typically proprietary or costly to obtain through human annotation.", "method": "The framework uses aligned long-context LLMs to generate contextually relevant queries by auto-regressing document-query pairs, harvesting them alongside model responses to create high-quality instructions.", "result": "LongMagpie demonstrates superior performance on long-context tasks and competitive results on short-context tasks through experiments involving HELMET, RULER, and Longbench v2.", "conclusion": "LongMagpie is an effective and scalable solution for synthesizing open and diverse long-context instruction data, beneficial for advancing LLM applications.", "key_contributions": ["Introduction of LongMagpie framework for self-synthesizing long-context instruction data", "Demonstration of superior long-context task performance", "Elimination of the need for human annotation in instruction generation"], "limitations": "", "keywords": ["long-context", "large language models", "self-synthesis", "instruction generation", "data synthesis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17135", "pdf": "https://arxiv.org/pdf/2505.17135.pdf", "abs": "https://arxiv.org/abs/2505.17135", "title": "When can isotropy help adapt LLMs' next word prediction to numerical domains?", "authors": ["Rashed Shelim", "Shengzhe Xu", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown that vector representations of contextual\nembeddings learned by pre-trained large language models (LLMs) are effective in\nvarious downstream tasks in numerical domains. Despite their significant\nbenefits, the tendency of LLMs to hallucinate in such domains can have severe\nconsequences in applications such as energy, nature, finance, healthcare,\nretail and transportation, among others. To guarantee prediction reliability\nand accuracy in numerical domains, it is necessary to open the black-box and\nprovide performance guarantees through explanation. However, there is little\ntheoretical understanding of when pre-trained language models help solve\nnumeric downstream tasks. This paper seeks to bridge this gap by understanding\nwhen the next-word prediction capability of LLMs can be adapted to numerical\ndomains through a novel analysis based on the concept of isotropy in the\ncontextual embedding space. Specifically, we consider a log-linear model for\nLLMs in which numeric data can be predicted from its context through a network\nwith softmax in the output layer of LLMs (i.e., language model head in\nself-attention). We demonstrate that, in order to achieve state-of-the-art\nperformance in numerical domains, the hidden representations of the LLM\nembeddings must possess a structure that accounts for the shift-invariance of\nthe softmax function. By formulating a gradient structure of self-attention in\npre-trained models, we show how the isotropic property of LLM embeddings in\ncontextual embedding space preserves the underlying structure of\nrepresentations, thereby resolving the shift-invariance problem and providing a\nperformance guarantee. Experiments show that different characteristics of\nnumeric data and model architecture could have different impacts on isotropy.", "AI": {"tldr": "This paper analyzes the adaptation of pre-trained LLMs for numerical downstream tasks, focusing on ensuring reliable predictions through the isotropic structure of contextual embeddings.", "motivation": "To address the hallucination problem of LLMs in numerical domains and provide performance guarantees through theoretical understanding and explainability.", "method": "The paper utilizes a log-linear model to analyze how LLMs can predict numeric data based on contextual embeddings, particularly focusing on the isotropy of these embeddings.", "result": "Demonstrates that the isotropic property of LLM embeddings preserves the underlying structure of representations, resolving the shift-invariance issue and enhancing performance in numerical tasks.", "conclusion": "Establishing the connection between isotropy and LLM performance in numeric domains allows for better prediction reliability and accuracy, contributing to the theoretical understanding of LLMs.", "key_contributions": ["Introduced a novel analysis based on isotropy for LLMs in numeric tasks.", "Provided a framework to achieve state-of-the-art performance through understanding the shift-invariance of softmax.", "Demonstrated the impact of model architecture on the isotropy of numeric data representations."], "limitations": "", "keywords": ["large language models", "numeric data", "isotropy", "self-attention", "explainability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17136", "pdf": "https://arxiv.org/pdf/2505.17136.pdf", "abs": "https://arxiv.org/abs/2505.17136", "title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations", "authors": ["Yuhan Ji", "Song Gao", "Ying Nie", "Ivan Majić", "Krzysztof Janowicz"], "categories": ["cs.CL", "cs.AI", "I.2"], "comment": "33 pages, 13 figures, IJGIS GeoFM Special Issue", "summary": "Applying AI foundation models directly to geospatial datasets remains\nchallenging due to their limited ability to represent and reason with\ngeographical entities, specifically vector-based geometries and natural\nlanguage descriptions of complex spatial relations. To address these issues, we\ninvestigate the extent to which a well-known-text (WKT) representation of\ngeometries and their spatial relations (e.g., topological predicates) are\npreserved during spatial reasoning when the geospatial vector data are passed\nto large language models (LLMs) including GPT-3.5-turbo, GPT-4, and\nDeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the\nspatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt\nengineering-based, and everyday language-based evaluation. Our experiment\nresults demonstrate that both the embedding-based and prompt engineering-based\napproaches to geospatial question-answering tasks with GPT models can achieve\nan accuracy of over 0.6 on average for the identification of topological\nspatial relations between two geometries. Among the evaluated models, GPT-4\nwith few-shot prompting achieved the highest performance with over 0.66\naccuracy on topological spatial relation inference. Additionally, GPT-based\nreasoner is capable of properly comprehending inverse topological spatial\nrelations and including an LLM-generated geometry can enhance the effectiveness\nfor geographic entity retrieval. GPT-4 also exhibits the ability to translate\ncertain vernacular descriptions about places into formal topological relations,\nand adding the geometry-type or place-type context in prompts may improve\ninference accuracy, but it varies by instance. The performance of these spatial\nreasoning tasks offers valuable insights for the refinement of LLMs with\ngeographical knowledge towards the development of geo-foundation models capable\nof geospatial reasoning.", "AI": {"tldr": "The study evaluates how well large language models (LLMs) handle geospatial reasoning tasks using well-known-text (WKT) representations, revealing strengths in their accuracy when employing embedding and prompt engineering techniques.", "motivation": "To explore the limitations of AI foundation models in understanding and reasoning with geospatial data, specifically vector geometries and spatial relations, and to propose approaches that enhance their performance in these tasks.", "method": "The research employs three distinct methodologies for spatial reasoning tasks: geometry embedding-based, prompt engineering-based, and everyday language-based evaluations, testing various LLMs including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B.", "result": "Embedding-based and prompt engineering approaches achieved an average accuracy over 0.6 in identifying topological spatial relations. GPT-4 performed the best with an accuracy exceeding 0.66, successfully identifying inverse relations and enhancing geographic entity retrieval.", "conclusion": "The findings indicate that incorporating geometry and place context improves inference accuracy in LLMs for geospatial reasoning and contribute to the development of geo-foundation models that can effectively understand geographical entities.", "key_contributions": ["Demonstration of LLMs' capabilities in geospatial reasoning using WKT representations.", "Identification of the most effective model and approach for topological spatial relation inference (GPT-4 with few-shot prompting).", "Insights for improving LLMs to support geographic entity retrieval and enhance spatial understanding."], "limitations": "The effectiveness of the context variations in prompts showed inconsistent improvements in inference accuracy based on the instance.", "keywords": ["geospatial reasoning", "large language models", "topological relations", "geometry embedding", "prompt engineering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.17137", "pdf": "https://arxiv.org/pdf/2505.17137.pdf", "abs": "https://arxiv.org/abs/2505.17137", "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands", "authors": ["Kristin Qi", "Youxiang Zhu", "Caroline Summerour", "John A. Batsis", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline.", "AI": {"tldr": "This study explores the use of voice assistant systems to detect cognitive decline by analyzing speech patterns in voice commands from older adults.", "motivation": "Early detection of cognitive decline is essential for timely interventions, but traditional assessment methods are impractical for frequent monitoring.", "method": "The study utilizes Cog-TiPRO, a framework that integrates LLM-driven prompt refinement for linguistic feature extraction, HuBERT for acoustic feature extraction, and transformer-based modeling to analyze voice commands collected over 18 months from older adults.", "result": "The method achieved 73.80% accuracy and 72.67% F1-score in detecting mild cognitive impairment (MCI), surpassing the baseline by 27.13%.", "conclusion": "The findings demonstrate that voice commands can serve as effective indicators of cognitive decline, providing a valuable tool for longitudinal health monitoring.", "key_contributions": ["Introduction of Cog-TiPRO framework for analyzing voice commands.", "Demonstration of LLM's role in identifying linguistic features indicative of cognitive decline.", "Achievement of high accuracy in MCI detection compared to traditional methods."], "limitations": "", "keywords": ["Cognitive Decline", "Voice Assistant Systems", "Machine Learning", "Speech Analysis", "Neurodegenerative Diseases"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.17139", "pdf": "https://arxiv.org/pdf/2505.17139.pdf", "abs": "https://arxiv.org/abs/2505.17139", "title": "EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models", "authors": ["Wanghan Xu", "Xiangyu Zhao", "Yuhao Zhou", "Xiaoyu Yue", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Lei Bai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) drive interest in scientific\napplications, necessitating specialized benchmarks such as Earth science.\nExisting benchmarks either present a general science focus devoid of Earth\nscience specificity or cover isolated subdomains, lacking holistic evaluation.\nFurthermore, current benchmarks typically neglect the assessment of LLMs'\ncapabilities in open-ended scientific exploration. In this paper, we present a\ncomprehensive and professional benchmark for the Earth sciences, designed to\nevaluate the capabilities of LLMs in scientific exploration within this domain,\nspanning from fundamental to advanced levels. Leveraging a corpus of 100,000\nresearch papers, we first construct two Question Answering (QA) datasets:\nEarth-Iron, which offers extensive question coverage for broad assessment, and\nEarth-Silver, which features a higher level of difficulty to evaluate\nprofessional depth. These datasets encompass five Earth spheres, 114\ndisciplines, and 11 task categories, assessing foundational knowledge crucial\nfor scientific exploration. Most notably, we introduce Earth-Gold with new\nmetrics, a dataset comprising open-ended multi-turn dialogues specifically\ndesigned to evaluate the advanced capabilities of LLMs in scientific\nexploration, including methodology induction, limitation analysis, and concept\nproposal. Extensive experiments reveal limitations in 11 leading LLMs across\ndifferent domains and tasks, highlighting considerable room for improvement in\ntheir scientific exploration capabilities. The benchmark is available on\nhttps://huggingface.co/ai-earth .", "AI": {"tldr": "This paper presents a comprehensive benchmark for evaluating Large Language Models (LLMs) in Earth sciences, creating extensive QA datasets and open-ended multi-turn dialogue scenarios to assess scientific exploration capabilities.", "motivation": "There is a growing interest in applying LLMs to scientific fields, especially Earth sciences, but existing benchmarks do not adequately address the need for specialized assessments that encompass the breadth and depth required.", "method": "The authors constructed two QA datasets, Earth-Iron for broad question coverage and Earth-Silver for evaluating professional depth, along with Earth-Gold for open-ended dialogues. They assessed 11 leading LLMs to evaluate their capabilities across these new datasets.", "result": "Experiments showed significant limitations in the tested LLMs, indicating they require improvements in their ability to handle scientific exploration tasks effectively, particularly in the Earth sciences domain.", "conclusion": "The benchmark established in this paper facilitates a more nuanced evaluation of LLMs in scientific contexts, offering resources for further development in AI's application to Earth sciences.", "key_contributions": ["Development of Earth-Iron and Earth-Silver QA datasets for evaluating LLMs in Earth sciences.", "Introduction of Earth-Gold dataset for testing open-ended dialogue capabilities of LLMs.", "Comprehensive analysis of LLMs' limitations in scientific exploration across various tasks."], "limitations": "The benchmark may still need validation with additional datasets and does not encompass all possible Earth science queries or domains.", "keywords": ["Large Language Models", "Earth sciences", "benchmark", "scientific exploration", "open-ended dialogue"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.17140", "pdf": "https://arxiv.org/pdf/2505.17140.pdf", "abs": "https://arxiv.org/abs/2505.17140", "title": "Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs", "authors": ["Essa Jan", "Moiz Ali", "Muhammad Saram Hassan", "Fareed Zaffar", "Yasir Zaki"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "4 pages, 1 figure", "summary": "As the knowledge of large language models (LLMs) becomes outdated over time,\nthere is a growing need for efficient methods to update them, especially when\ninjecting proprietary information. Our study reveals that\ncomprehension-intensive fine-tuning tasks (e.g., question answering and blanks)\nachieve substantially higher knowledge retention rates (48%) compared to\nmapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),\ndespite exposure to identical factual content. We demonstrate that this pattern\npersists across model architectures and follows scaling laws, with larger\nmodels showing improved retention across all task types. However, all models\nexhibit significant performance drops when applying injected knowledge in\nbroader contexts, suggesting limited semantic integration. These findings show\nthe importance of task selection in updating LLM knowledge, showing that\neffective knowledge injection relies not just on data exposure but on the depth\nof cognitive engagement during fine-tuning.", "AI": {"tldr": "This study explores efficient methods to update large language models (LLMs) by comparing task types affecting knowledge retention. Comprehension-intensive tasks show higher rates than mapping-oriented ones, and larger models generally retain more knowledge but struggle with broader contextual application.", "motivation": "The need for efficient methods to update large language models, especially when injecting proprietary information, has grown due to outdated knowledge.", "method": "The study compares comprehension-intensive fine-tuning tasks (e.g., question answering) against mapping-oriented tasks (e.g., translation) in terms of knowledge retention rates across different model architectures.", "result": "Comprehension-intensive tasks achieve significantly higher knowledge retention rates (48%) compared to mapping-oriented tasks (17% for translation and 20% for text-to-JSON) and this pattern persists across model architectures with larger models performing better.", "conclusion": "Effective knowledge injection in LLMs relies on task selection, emphasizing cognitive engagement during fine-tuning rather than merely data exposure, as all models show performance drops in broader applications.", "key_contributions": ["Comparative analysis of task types affecting LLM knowledge retention", "Demonstrated impact of model size on retention rates", "Identified limitations of semantic integration during knowledge application"], "limitations": "All models demonstrate significant performance drops when applying injected knowledge in broader contexts, indicating limited semantic integration.", "keywords": ["large language models", "knowledge retention", "fine-tuning", "task selection", "cognitive engagement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17144", "pdf": "https://arxiv.org/pdf/2505.17144.pdf", "abs": "https://arxiv.org/abs/2505.17144", "title": "MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models", "authors": ["Bohan Jin", "Shuhan Qi", "Kehai Chen", "Xinyi Guo", "Xuan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL 2025", "summary": "The widespread use of Large Multimodal Models (LMMs) has raised concerns\nabout model toxicity. However, current research mainly focuses on explicit\ntoxicity, with less attention to some more implicit toxicity regarding\nprejudice and discrimination. To address this limitation, we introduce a\nsubtler type of toxicity named dual-implicit toxicity and a novel toxicity\nbenchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.\nSpecifically, we first create the MDIT-Dataset with dual-implicit toxicity\nusing the proposed Multi-stage Human-in-loop In-context Generation method.\nBased on this dataset, we construct the MDIT-Bench, a benchmark for evaluating\nthe sensitivity of models to dual-implicit toxicity, with 317,638 questions\ncovering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes\nthree difficulty levels, and we propose a metric to measure the toxicity gap\nexhibited by the model across them. In the experiment, we conducted MDIT-Bench\non 13 prominent LMMs, and the results show that these LMMs cannot handle\ndual-implicit toxicity effectively. The model's performance drops significantly\nin hard level, revealing that these LMMs still contain a significant amount of\nhidden but activatable toxicity. Data are available at\nhttps://github.com/nuo1nuo/MDIT-Bench.", "AI": {"tldr": "This paper introduces a new type of toxicity—dual-implicit toxicity—and a benchmark (MDIT-Bench) to evaluate Large Multimodal Models' performance on this aspect.", "motivation": "Current research primarily focuses on explicit toxicity in LMMs, neglecting implicit forms like prejudice and discrimination.", "method": "A dataset (MDIT-Dataset) was created using a Multi-stage Human-in-loop In-context Generation method, followed by the construction of the MDIT-Bench to evaluate model sensitivity to dual-implicit toxicity with a wide range of questions.", "result": "Experiments conducted on 13 prominent LMMs revealed that these models struggle with dual-implicit toxicity, particularly at harder difficulty levels.", "conclusion": "The findings indicate that these models possess significant hidden toxicity that remains activatable, which necessitates further research and improvements.", "key_contributions": ["Introduction of the concept of dual-implicit toxicity.", "Creation of the MDIT-Dataset for training and evaluation.", "Establishment of the MDIT-Bench as a comprehensive benchmarking tool for assessing LMMs."], "limitations": "The focus is exclusively on LMMs; implications for other model types are not discussed.", "keywords": ["dual-implicit toxicity", "LMM", "benchmark", "MDIT-Bench", "toxicity evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17149", "pdf": "https://arxiv.org/pdf/2505.17149.pdf", "abs": "https://arxiv.org/abs/2505.17149", "title": "Large Language Models for Predictive Analysis: How Far Are They?", "authors": ["Qin Chen", "Yuanyi Ren", "Xiaojun Ma", "Yuyang Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Predictive analysis is a cornerstone of modern decision-making, with\napplications in various domains. Large Language Models (LLMs) have emerged as\npowerful tools in enabling nuanced, knowledge-intensive conversations, thus\naiding in complex decision-making tasks. With the burgeoning expectation to\nharness LLMs for predictive analysis, there is an urgent need to systematically\nassess their capability in this domain. However, there is a lack of relevant\nevaluations in existing studies. To bridge this gap, we introduce the\n\\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive\nanalysis queries originating from 44 real-world datasets of 8 diverse fields.\nWe design an evaluation protocol considering text analysis, code generation,\nand their alignment. Twelve renowned LLMs are evaluated, offering insights into\ntheir practical use in predictive analysis. Generally, we believe that existing\nLLMs still face considerable challenges in conducting predictive analysis. See\n\\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.", "AI": {"tldr": "The paper introduces PredictiQ, a benchmark for evaluating the capabilities of large language models (LLMs) in predictive analysis, using 1130 queries from diverse datasets.", "motivation": "To address the lack of systematic assessments of LLMs for predictive analysis, and to meet the rising expectations of utilizing these models in complex decision-making tasks.", "method": "We created the PredictiQ benchmark that consists of 1130 predictive analysis queries sourced from 44 datasets across 8 fields, and designed an evaluation protocol to assess LLMs in text analysis and code generation.", "result": "The evaluation of twelve renowned LLMs revealed that, despite their advances, they continue to face significant challenges in performing predictive analysis effectively.", "conclusion": "The study emphasizes the need for improved LLMs in predictive tasks and provides a standardized framework for future evaluations.", "key_contributions": ["Introduction of the PredictiQ benchmark for predictive analysis", "Evaluation of twelve LLMs on a common set of challenges", "Insights on the limitations of current LLMs in predictive tasks"], "limitations": "The benchmark may not cover all potential use cases in predictive analysis and relies on the selected datasets and queries, which may introduce biases.", "keywords": ["Predictive analysis", "Large language models", "Benchmark", "Text analysis", "Code generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17151", "pdf": "https://arxiv.org/pdf/2505.17151.pdf", "abs": "https://arxiv.org/abs/2505.17151", "title": "Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions", "authors": ["Zishuo Bao", "Yibo Liu", "Changyutao Qiu"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures, 2 tables", "summary": "With the rise of different language model architecture, fine-tuning is\nbecoming even more important for down stream tasks Model gets messy, finding\nproper hyperparameters for fine-tuning. Although BO has been tried for\nhyperparameter tuning, most of the existing methods are oblivious to the fact\nthat BO relies on careful choices of acquisition functions, which are essential\ncomponents of BO that guide how much to explore versus exploit during the\noptimization process; Different acquisition functions have different levels of\nsensitivity towards training loss and validation performance; existing methods\noften just apply an acquisition function no matter if the training and\nvalidation performance are sensitive to the acquisition function or not. This\nwork introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a\nbilevel BO strategy to improve the fine - tunning of large language models. Our\nwork on mixture of acquisition functions like EI and UCB into nested opt loops,\nwhere inner loop perform minimization of training loss while outer loops\noptimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base\nshow that when using EI and UCB, there is an improvement in generalization, and\nfine - tuning can be improved by up to 2.7%.", "AI": {"tldr": "This paper presents a new approach for fine-tuning large language models using a bilevel Bayesian optimization strategy with a mixture of acquisition functions to enhance performance on downstream tasks.", "motivation": "The increasing complexity of language model architectures necessitates effective hyperparameter tuning strategies. Traditional methods often use fixed acquisition functions without considering their sensitivity to training and validation performance.", "method": "The authors propose a bilevel Bayesian optimization (Bilevel-BO-SWA) framework that incorporates a fusion of acquisition functions like Expected Improvement (EI) and Upper Confidence Bound (UCB) within nested optimization loops to fine-tune large language models effectively.", "result": "Experimental results on GLUE tasks indicate that the proposed method improves generalization and fine-tuning efficiency by up to 2.7% compared to existing approaches.", "conclusion": "Bilevel-BO-SWA demonstrates significant improvement in fine-tuning large language models by strategically selecting acquisition functions based on their sensitivity to performance metrics.", "key_contributions": ["Introduction of a bilevel Bayesian optimization strategy for LLM fine-tuning", "Fusion of multiple acquisition functions for improved performance", "Demonstration of significant improvements on GLUE tasks"], "limitations": "The paper focuses on specific tasks (GLUE) and may require further validation across other datasets and model architectures.", "keywords": ["Bilevel Bayesian Optimization", "Fine-tuning", "Acquisition Functions", "Large Language Models", "Machine Learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.17153", "pdf": "https://arxiv.org/pdf/2505.17153.pdf", "abs": "https://arxiv.org/abs/2505.17153", "title": "Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN", "authors": ["Yao Xu", "Mingyu Xu", "Fangyu Lei", "Wangtao Sun", "Xiangrong Zeng", "Bingning Wang", "Guang Liu", "Shizhu He", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated\nremarkable performance on complex reasoning tasks through Long Chain-of-Thought\n(Long-CoT) reasoning. Although distilling this capability into student models\nsignificantly enhances their performance, this paper finds that fine-tuning\nLLMs with full parameters or LoRA with a low rank on long CoT data often leads\nto Cyclical Reasoning, where models repeatedly reiterate previous inference\nsteps until the maximum length limit. Further analysis reveals that smaller\ndifferences in representations between adjacent tokens correlates with a higher\ntendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes\nShift Feedforward Networks (Shift-FFN), a novel approach that edits the current\ntoken's representation with the previous one before inputting it to FFN. This\narchitecture dynamically amplifies the representation differences between\nadjacent tokens. Extensive experiments on multiple mathematical reasoning tasks\ndemonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a\nlower rate of Cyclical Reasoning across various data sizes compared to full\nfine-tuning and standard LoRA. Our data and code are available at\nhttps://anonymous.4open.science/r/Shift-FFN", "AI": {"tldr": "This paper addresses the issue of Cyclical Reasoning in LLMs when fine-tuned on Long Chain-of-Thought data and proposes a novel architecture, Shift Feedforward Networks (Shift-FFN), to mitigate this problem.", "motivation": "The paper discusses the challenges of Cyclical Reasoning in large language models when using Long Chain-of-Thought reasoning techniques, which can hinder model performance on complex tasks.", "method": "The authors propose Shift Feedforward Networks (Shift-FFN) that adjust the current token's representation with the previous token's representation before passing it through the feedforward network, thereby increasing representation differences between adjacent tokens.", "result": "Experiments show that models using LoRA with Shift-FFN outperform those using full fine-tuning and standard LoRA in terms of accuracy and reduce the incidence of Cyclical Reasoning across various mathematical reasoning tasks.", "conclusion": "Shift-FFN significantly enhances the accuracy of student models trained on long Chain-of-Thought data while reducing the tendency toward Cyclical Reasoning, showcasing its effectiveness in improving reasoning capabilities.", "key_contributions": ["Introduction of Shift Feedforward Networks (Shift-FFN) to address Cyclical Reasoning", "Demonstration of improved model accuracy on mathematical reasoning tasks", "Analysis of representation differences leading to Cyclical Reasoning"], "limitations": "", "keywords": ["Long Chain-of-Thought", "Cyclical Reasoning", "Shift Feedforward Networks", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.17156", "pdf": "https://arxiv.org/pdf/2505.17156.pdf", "abs": "https://arxiv.org/abs/2505.17156", "title": "PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG", "authors": ["Muhammed Rizwan", "Lars Carlsson", "Mohammad Loni"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The introduction of Large Language Models (LLMs) has significantly\ntransformed Natural Language Processing (NLP) applications by enabling more\nadvanced analysis of customer personas. At Volvo Construction Equipment (VCE),\ncustomer personas have traditionally been developed through qualitative\nmethods, which are time-consuming and lack scalability. The main objective of\nthis paper is to generate synthetic customer personas and integrate them into a\nRetrieval-Augmented Generation (RAG) chatbot to support decision-making in\nbusiness processes. To this end, we first focus on developing a persona-based\nRAG chatbot integrated with verified personas. Next, synthetic personas are\ngenerated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and\nevaluated based on completeness, relevance, and consistency using McNemar's\ntest. In the final step, the chatbot's knowledge base is augmented with\nsynthetic personas and additional segment information to assess improvements in\nresponse accuracy and practical utility. Key findings indicate that Few-Shot\nprompting outperformed CoT in generating more complete personas, while CoT\ndemonstrated greater efficiency in terms of response time and token usage.\nAfter augmenting the knowledge base, the average accuracy rating of the chatbot\nincreased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants\nfound the updated system useful in business contexts.", "AI": {"tldr": "This paper discusses the development of a persona-based RAG chatbot at Volvo Construction Equipment that incorporates synthetic customer personas using Few-Shot and Chain-of-Thought prompting techniques.", "motivation": "To enhance the development of customer personas, traditionally a time-consuming process, and improve decision-making in business processes through the integration of synthetic personas into a chatbot.", "method": "Developed a persona-based RAG chatbot, generated synthetic personas using Few-Shot and Chain-of-Thought prompting, and evaluated their effectiveness using McNemar's test. The chatbot's knowledge base was augmented with synthetic personas and assessed for improvements in response accuracy.", "result": "Few-Shot prompting outperformed Chain-of-Thought in generating complete personas; after augmentation, the chatbot's accuracy rating improved from 5.88 to 6.42, with 81.82% of participants finding it useful in business contexts.", "conclusion": "Integrating synthetic customer personas enhanced the RAG chatbot's effectiveness, demonstrating the value of advanced NLP techniques in business decision-making.", "key_contributions": ["Development of a persona-based RAG chatbot with synthetic personas", "Implementation of Few-Shot and Chain-of-Thought prompting techniques", "Assessment of chatbot response accuracy improvements through augmentation methods"], "limitations": "None specified in the abstract.", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "synthetic customer personas"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17160", "pdf": "https://arxiv.org/pdf/2505.17160.pdf", "abs": "https://arxiv.org/abs/2505.17160", "title": "Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting", "authors": ["Bang Trinh Tran To", "Thai Le"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "This work presents LURK (Latent UnleaRned Knowledge), a novel framework that\nprobes for hidden retained knowledge in unlearned LLMs through adversarial\nsuffix prompting. LURK automatically generates adversarial prompt suffixes\ndesigned to elicit residual knowledge about the Harry Potter domain, a commonly\nused benchmark for unlearning. Our experiments reveal that even models deemed\nsuccessfully unlearned can leak idiosyncratic information under targeted\nadversarial conditions, highlighting critical limitations of current unlearning\nevaluation standards. By uncovering latent knowledge through indirect probing,\nLURK offers a more rigorous and diagnostic tool for assessing the robustness of\nunlearning algorithms. All code will be publicly available.", "AI": {"tldr": "LURK is a framework that detects hidden knowledge in unlearned LLMs using adversarial suffix prompting.", "motivation": "To explore limitations in current unlearning evaluation techniques for large language models (LLMs).", "method": "The framework generates adversarial prompt suffixes targeting idiosyncratic information retention in LLMs, specifically focusing on the Harry Potter domain.", "result": "Experiments show that even models considered unlearned can reveal sensitive knowledge under adversarial conditions, indicating flaws in existing assessment standards.", "conclusion": "LURK serves as a powerful tool for evaluating unlearning algorithms, revealing that hidden knowledge may persist even in seemingly unlearned models.", "key_contributions": ["Introduction of LURK framework for probing unlearned knowledge in LLMs", "Evidence that unlearned LLMs can leak information under specific conditions", "Advancement of unlearning evaluation methodologies."], "limitations": "The focus on a single domain (Harry Potter) may limit generalizability to other domains.", "keywords": ["unlearning", "large language models", "adversarial prompting", "knowledge retention", "evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.17167", "pdf": "https://arxiv.org/pdf/2505.17167.pdf", "abs": "https://arxiv.org/abs/2505.17167", "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Bernhard Kainz", "Bjoern Menze"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating long-context radiology report generation is challenging. NLG\nmetrics fail to capture clinical correctness, while LLM-based metrics often\nlack generalizability. Clinical accuracy metrics are more relevant but are\nsensitive to class imbalance, frequently favoring trivial predictions. We\npropose the CRG Score, a distribution-aware and adaptable metric that evaluates\nonly clinically relevant abnormalities explicitly described in reference\nreports. CRG supports both binary and structured labels (e.g., type, location)\nand can be paired with any LLM for feature extraction. By balancing penalties\nbased on label distribution, it enables fairer, more robust evaluation and\nserves as a clinically aligned reward function.", "AI": {"tldr": "The paper introduces the CRG Score, a new metric for evaluating clinical radiology report generation that addresses challenges in existing evaluation metrics.", "motivation": "Evaluating long-context radiology report generation accurately is critical, yet current NLG and LLM metrics struggle to maintain clinical relevance and generalizability.", "method": "The CRG Score is designed to evaluate clinically relevant abnormalities in reference reports while supporting binary and structured labels. It adapts penalties based on label distribution to ensure fair evaluation.", "result": "The CRG Score provides a more balanced and robust metric for assessing clinical accuracy in radiology report generation, improving upon existing methods affected by class imbalance.", "conclusion": "By offering a clinically aligned reward function that emphasizes relevant abnormalities, the CRG Score enhances the evaluation of radiology report generation using LLMs.", "key_contributions": ["Introduction of the CRG Score for clinical evaluation of radiology reports", "Balancing penalties based on label distribution for fair assessments", "Support for both binary and structured labels in clinical reports"], "limitations": "", "keywords": ["CRG Score", "long-context radiology", "NLG metrics", "clinical evaluation", "LLM"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.17169", "pdf": "https://arxiv.org/pdf/2505.17169.pdf", "abs": "https://arxiv.org/abs/2505.17169", "title": "Next Token Perception Score: Analytical Assessment of your LLM Perception Skills", "authors": ["Yu-Ang Cheng", "Leyang Hu", "Hai Huang", "Randall Balestriero"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive pretraining has become the de facto paradigm for learning\ngeneral-purpose representations in large language models (LLMs). However,\nlinear probe performance across downstream perception tasks shows substantial\nvariability, suggesting that features optimized for next-token prediction do\nnot consistently transfer well to downstream perception tasks. We demonstrate\nthat representations learned via autoregression capture features that may lie\noutside the subspaces most informative for perception. To quantify the\n(mis)alignment between autoregressive pretraining and downstream perception, we\nintroduce the Next Token Perception Score (NTPS)-a score derived under a linear\nsetting that measures the overlap between autoregressive and perception feature\nsubspaces. This metric can be easily computed in closed form from pretrained\nrepresentations and labeled data, and is proven to both upper- and lower-bound\nthe excess loss. Empirically, we show that NTPS correlates strongly with linear\nprobe accuracy across 12 diverse NLP datasets and eight pretrained models\nranging from 270M to 8B parameters, confirming its utility as a measure of\nalignment. Furthermore, we show that NTPS increases following low-rank\nadaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA\naligning representations to perception tasks enhances subspace overlap and thus\nimproves downstream performance. More importantly, we find that NTPS reliably\npredicts the additional accuracy gains attained by LoRA finetuning thereby\nproviding a lightweight prescreening tool for LoRA adaptation. Our results\noffer both theoretical insights and practical tools for analytically assessing\nLLM perception skills.", "AI": {"tldr": "The paper introduces the Next Token Perception Score (NTPS) to measure the alignment of autoregressive pretraining in LLMs with downstream perception tasks, showing strong empirical correlation with task performance and validation of fine-tuning methods.", "motivation": "To address the variability in downstream task performance from autoregressive pretraining in large language models and understand the alignment between pretraining and perception tasks.", "method": "The authors introduce the Next Token Perception Score (NTPS) which quantifies the alignment between features learned through autoregression and those required for perception tasks, validated across various NLP datasets and models.", "result": "The NTPS shows strong correlation with linear probe accuracy and increases after LoRA fine-tuning, indicating improved task alignment. It also predicts accuracy gains from LoRA effectively.", "conclusion": "NTPS is a valuable metric for assessing LLM perception alignment and can enhance the fine-tuning process by identifying the suitability of models for specific perception tasks.", "key_contributions": ["Introduction of the Next Token Perception Score (NTPS).", "Demonstration of strong correlation between NTPS and downstream performance.", "Validation of NTPS as a predictive tool for LoRA fine-tuning accuracy gains."], "limitations": "", "keywords": ["large language models", "autoregressive pretraining", "Next Token Perception Score"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17206", "pdf": "https://arxiv.org/pdf/2505.17206.pdf", "abs": "https://arxiv.org/abs/2505.17206", "title": "FB-RAG: Improving RAG with Forward and Backward Lookup", "authors": ["Kushal Chawla", "Alfy Samuel", "Anoop Kumar", "Daben Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of Retrieval Augmented Generation (RAG) systems relies\nheavily on the retriever quality and the size of the retrieved context. A large\nenough context ensures that the relevant information is present in the input\ncontext for the LLM, but also incorporates irrelevant content that has been\nshown to confuse the models. On the other hand, a smaller context reduces the\nirrelevant information, but it often comes at the risk of losing important\ninformation necessary to answer the input question. This duality is especially\nchallenging to manage for complex queries that contain little information to\nretrieve the relevant chunks from the full context. To address this, we present\na novel framework, called FB-RAG, which enhances the RAG pipeline by relying on\na combination of backward lookup (overlap with the query) and forward lookup\n(overlap with candidate reasons and answers) to retrieve specific context\nchunks that are the most relevant for answering the input query. Our\nevaluations on 9 datasets from two leading benchmarks show that FB-RAG\nconsistently outperforms RAG and Long Context baselines developed recently for\nthese benchmarks. We further show that FB-RAG can improve performance while\nreducing latency. We perform qualitative analysis of the strengths and\nshortcomings of our approach, providing specific insights to guide future work.", "AI": {"tldr": "This paper introduces FB-RAG, a novel framework that enhances Retrieval Augmented Generation (RAG) systems by improving context retrieval for complex queries, thus outperforming traditional models in both performance and latency.", "motivation": "To improve the performance of RAG systems by managing the complexity of context retrieval for queries with limited information.", "method": "FB-RAG utilizes a dual approach of backward and forward lookup to selectively retrieve context chunks that are most relevant for query answering.", "result": "FB-RAG consistently outperforms existing RAG models and Long Context baselines across 9 datasets, while also reducing response latency.", "conclusion": "The findings highlight that FB-RAG can effectively improve information retrieval for complex queries, providing a framework for future enhancements in the field.", "key_contributions": ["Introduction of FB-RAG framework for enhanced query-context matching", "Demonstrated performance improvements across multiple benchmarks", "Insights on qualitative strengths and weaknesses for future research"], "limitations": "", "keywords": ["Retrieval Augmented Generation", "context retrieval", "language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17217", "pdf": "https://arxiv.org/pdf/2505.17217.pdf", "abs": "https://arxiv.org/abs/2505.17217", "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs", "authors": ["Kangda Wei", "Hasnat Md Abdullah", "Ruihong Huang"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data.", "AI": {"tldr": "This paper presents a framework to reduce gender bias in LLMs by generating story pairs with male and female protagonists in morally ambiguous scenarios, prompting the models to make balanced moral judgments.", "motivation": "To tackle the issue of gender bias in Large Language Models (LLMs) that leads to unequal treatment of genders across different contexts.", "method": "A novel data generation framework that creates story pairs with male and female protagonists in identical scenarios. The model generates moral judgments, which are compared and guided towards gender-neutral responses using Direct Preference Optimization (DPO).", "result": "The proposed method significantly reduces gender bias in LLMs while maintaining or improving overall model performance.", "conclusion": "The framework facilitates more equitable treatment of genders in LLM outputs, demonstrating the potential for enhancing model fairness.", "key_contributions": ["Introduction of a data generation framework for LLMs to explore gender bias.", "Methodology for guiding models towards balanced moral judgments.", "Experimental validation showing reduction in gender bias while maintaining model capabilities."], "limitations": "The approach is dependent on the quality of the generated story pairs and may not generalize to all contexts.", "keywords": ["large language models", "gender bias", "moral judgments", "data generation", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17222", "pdf": "https://arxiv.org/pdf/2505.17222.pdf", "abs": "https://arxiv.org/abs/2505.17222", "title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts", "authors": ["Georgios Chochlakis", "Peter Wu", "Arjun Bedi", "Marcus Ma", "Kristina Lerman", "Shrikanth Narayanan"], "categories": ["cs.CL"], "comment": "17 pages, 16 figures, 9 tables", "summary": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/LiaHR.", "AI": {"tldr": "This paper explores label verification in Natural Language Processing (NLP) using Large Language Models (LLMs) to address challenges in subjective task modeling, specifically focusing on emotion and morality recognition.", "motivation": "The paper addresses the challenge of significant variation in human annotations for subjective tasks in NLP, which often reflects genuine semantic differences rather than just noise.", "method": "The authors propose an In-Context Learning binary filtering baseline and introduce the Label-in-a-Haystack setting for prompt engineering, where the LLM predicts labels based on provided demonstrations and task-specific instructions. They develop the Label-in-a-Haystack Rectification (LiaHR) framework for correcting subjective labels instead of discarding them.", "result": "The analyses and studies conducted demonstrate the efficacy of the LiaHR framework in improving the quality of label correction and enhancing the signal-to-noise ratio in annotation workflows.", "conclusion": "The LiaHR approach offers a promising integration into existing annotation pipelines, enhancing the reliability of subjective labeling in NLP tasks.", "key_contributions": ["Introduction of the In-Context Learning binary filtering baseline for label verification.", "Development of the Label-in-a-Haystack framework for subjective label correction.", "Demonstration of ecological validity and practical applications of the proposed methods."], "limitations": "", "keywords": ["Natural Language Processing", "Large Language Models", "Subjective Label Correction", "Emotion Recognition", "Human Annotation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17231", "pdf": "https://arxiv.org/pdf/2505.17231.pdf", "abs": "https://arxiv.org/abs/2505.17231", "title": "ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects", "authors": ["Jipeng Zhang", "Haolin Yang", "Kehao Miao", "Ruiyuan Zhang", "Renjie Pi", "Jiahui Gao", "Xiaofang Zhou"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Recent text-to-SQL models have achieved strong performance, but their\neffectiveness remains largely confined to SQLite due to dataset limitations.\nHowever, real-world applications require SQL generation across multiple\ndialects with varying syntax and specialized features, which remains a\nchallenge for current models. The main obstacle in building a dialect-aware\nmodel lies in acquiring high-quality dialect-specific data. Data generated\npurely through static prompting - without validating SQLs via execution - tends\nto be noisy and unreliable. Moreover, the lack of real execution environments\nin the training loop prevents models from grounding their predictions in\nexecutable semantics, limiting generalization despite surface-level\nimprovements from data filtering. This work introduces ExeSQL, a text-to-SQL\nframework with execution-driven, agentic bootstrapping. The method consists of\niterative query generation, execution-based filtering (e.g., rejection\nsampling), and preference-based training, enabling the model to adapt to new\nSQL dialects through verifiable, feedback-guided learning. Experiments show\nthat ExeSQL bridges the dialect gap in text-to-SQL, achieving average\nimprovements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and\nOracle, respectively, across multiple datasets of varying difficulty.", "AI": {"tldr": "ExeSQL is a text-to-SQL framework that enhances SQL generation across various dialects through execution-driven learning.", "motivation": "Current text-to-SQL models struggle with performance across multiple SQL dialects due to limitations in dataset quality and execution validation. Enhancing dialect-aware capabilities is crucial for real-world application.", "method": "ExeSQL employs an execution-driven, agentic bootstrapping method that utilizes iterative query generation, execution-based filtering (like rejection sampling), and preference-based training to refine its SQL outputs.", "result": "ExeSQL demonstrates significant improvements in SQL generation accuracy, with average gains of 15.2%, 10.38%, and 4.49% over GPT-4o in PostgreSQL, MySQL, and Oracle, respectively.", "conclusion": "The framework effectively bridges the gap in text-to-SQL models for dialect variability, offering a robust solution to the limitations faced by previous approaches.", "key_contributions": ["Introduced an execution-driven bootstrapping method for SQL generation.", "Achieved significant performance improvements across multiple SQL dialects.", "Developed a framework that enables models to learn from execution feedback."], "limitations": "", "keywords": ["text-to-SQL", "SQL dialects", "execution-driven learning", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17238", "pdf": "https://arxiv.org/pdf/2505.17238.pdf", "abs": "https://arxiv.org/abs/2505.17238", "title": "Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)", "authors": ["Clayton Cohn", "Surya Rayala", "Caitlin Snyder", "Joyce Fonteles", "Shruti Jain", "Naveeduddin Mohammed", "Umesh Timalsina", "Sarah K. Burriss", "Ashwin T S", "Namrata Srivastava", "Menton Deweese", "Angela Eeds", "Gautam Biswas"], "categories": ["cs.CL"], "comment": "Submitted to the International Conference on Artificial Intelligence\n  in Education (AIED) Workshop on Epistemics and Decision-Making in\n  AI-Supported Education", "summary": "Collaborative dialogue offers rich insights into students' learning and\ncritical thinking. This is essential for adapting pedagogical agents to\nstudents' learning and problem-solving skills in STEM+C settings. While large\nlanguage models (LLMs) facilitate dynamic pedagogical interactions, potential\nhallucinations can undermine confidence, trust, and instructional value.\nRetrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge,\nbut its effectiveness depends on clear semantic links between user input and a\nknowledge base, which are often weak in student dialogue. We propose\nlog-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating\nenvironment logs to contextualize collaborative discourse. Our findings show\nthat LC-RAG improves retrieval over a discourse-only baseline and allows our\ncollaborative peer agent, Copa, to deliver relevant, personalized guidance that\nsupports students' critical thinking and epistemic decision-making in a\ncollaborative computational modeling environment, XYZ.", "AI": {"tldr": "This paper introduces log-contextualized retrieval-augmented generation (LC-RAG) to enhance pedagogical interactions in educational settings by improving retrieval accuracy using environmental logs from collaborative dialogue.", "motivation": "The study aims to address the challenges faced by pedagogical agents in understanding and interacting with students during collaborative dialogues, particularly in STEM+C education.", "method": "The authors propose a novel approach called LC-RAG that incorporates environment logs to enhance the retrieval capabilities of existing RAG frameworks, improving the relevance of responses generated by collaborative agents.", "result": "The findings indicate that LC-RAG outperforms a baseline that relies solely on discourse by providing more relevant and personalized guidance to students, thereby enhancing critical thinking and decision-making skills during collaborative learning.", "conclusion": "The implementation of LC-RAG shows promise in improving the effectiveness of pedagogical agents, indicating a potential pathway for better supporting students' learning experiences in computational modeling environments.", "key_contributions": ["Introduction of LC-RAG for improved retrieval in educational contexts", "Demonstration of enhanced interaction quality in collaborative learning settings", "Empirical evidence supporting the effectiveness of LC-RAG over traditional methods"], "limitations": "", "keywords": ["collaborative dialogue", "retrieval-augmented generation", "learning analytics", "pedagogical agents", "critical thinking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17244", "pdf": "https://arxiv.org/pdf/2505.17244.pdf", "abs": "https://arxiv.org/abs/2505.17244", "title": "ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models", "authors": ["Changyi Li", "Jiayi Wang", "Xudong Pan", "Geng Hong", "Min Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) are transforming the AI landscape with advanced\nreasoning capabilities. While the generated reasoning traces enhance model\ntransparency, they can still contain unsafe content, even when the final answer\nappears safe. Existing moderation tools, primarily designed for question-answer\n(QA) pairs, are empirically ineffective at detecting hidden risks embedded in\nreasoning traces. After identifying the key challenges, we formally define the\nquestion-thought (QT) moderation task and propose ReasoningShield, the first\nsafety detection model tailored to identify potential risks in the reasoning\ntrace before reaching the final answer. To construct the model, we synthesize a\nhigh-quality reasoning safety detection dataset comprising over 8,000\nquestion-thought pairs spanning ten risk categories and three safety levels.\nOur dataset construction process incorporates a comprehensive human-AI\ncollaborative annotation pipeline, which achieves over 93% annotation accuracy\nwhile significantly reducing human costs. On a diverse set of in-distribution\nand out-of-distribution benchmarks, ReasoningShield outperforms mainstream\ncontent safety moderation models in identifying risks within reasoning traces,\nwith an average F1 score exceeding 0.92. Notably, despite being trained on our\nQT dataset only, ReasoningShield also demonstrates competitive performance in\ndetecting unsafe question-answer pairs on traditional benchmarks, rivaling\nbaselines trained on 10 times larger datasets and base models, which strongly\nvalidates the quality of our dataset. Furthermore, ReasoningShield is built\nupon compact 1B/3B base models to facilitate lightweight deployment and\nprovides human-friendly risk analysis by default. To foster future research, we\npublicly release all the resources.", "AI": {"tldr": "The paper introduces ReasoningShield, a safety detection model designed to identify risks in reasoning traces generated by Large Reasoning Models (LRMs), aiming to improve AI transparency and risk management in AI outputs.", "motivation": "There is a growing need for effective moderation tools to ensure the safety of AI-generated content, particularly as reasoning models become more prevalent and their outputs increasingly influence decisions.", "method": "The authors propose ReasoningShield, employing a dataset of over 8,000 question-thought pairs in a human-AI collaborative annotation pipeline to identify risks in reasoning traces across ten categories and three safety levels.", "result": "ReasoningShield outperforms existing moderation models with an average F1 score exceeding 0.92 on various benchmarks, successfully identifying risks within reasoning traces while delivering competitive performance on traditional unsafe Q&A detection tasks.", "conclusion": "The developed model demonstrates significant improvements in safety detection and provides a foundation for future research in reasoning model safety, along with the release of key resources for public use.", "key_contributions": ["Introduction of the QT moderation task.", "Development of ReasoningShield for identifying risks in reasoning traces.", "Creation of a high-quality reasoning safety detection dataset."], "limitations": "", "keywords": ["Large Reasoning Models", "safety detection", "reasoning traces", "moderation", "Human-AI collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17250", "pdf": "https://arxiv.org/pdf/2505.17250.pdf", "abs": "https://arxiv.org/abs/2505.17250", "title": "ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models", "authors": ["Razvan-Gabriel Dumitru", "Darius Peteleaza", "Vikas Yadav", "Liangming Pan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "comment": "25 pages, 18 figures, and 6 tables", "summary": "Large language models excel at complex tasks by breaking down problems into\nstructured reasoning steps. However, reasoning traces often extend beyond\nreaching a correct answer, causing wasted computation, reduced readability, and\nhallucinations. To address this, we introduce a novel hyperparameter-free\nconciseness score used as a reward signal within a reinforcement learning\nframework to guide models toward generating correct and concise reasoning\ntraces. This score is evaluated by a large language model acting as a judge,\nenabling dynamic, context-aware feedback beyond simple token length. Our method\nachieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,\nreducing token usage by up to 31x on simple problems while improving accuracy\nby 7%, and on the hardest problems, it outperforms full reasoning by +7.5%\naccuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves\naccuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on\nthe judge model, reward composition, and problem difficulty, showing that our\nmethod dynamically adapts reasoning length based on problem difficulty and\nbenefits significantly from stronger judges. The code, model weights, and\ndatasets are open-sourced at https://github.com/RazvanDu/ConciseRL.", "AI": {"tldr": "This paper presents a novel reinforcement learning approach that uses a conciseness score to improve the efficiency and accuracy of reasoning in large language models.", "motivation": "To mitigate excessive computation, enhance readability, and reduce hallucination in reasoning traces generated by large language models.", "method": "The authors introduce a hyperparameter-free conciseness score as a reward signal in a reinforcement learning framework to guide models towards generating concise reasoning. This score is evaluated by a judge model that provides dynamic feedback.", "result": "The method demonstrates state-of-the-art trade-offs on the MATH dataset, reducing token usage by up to 31x on simpler problems while improving accuracy by 7%, and achieving up to 3.6x fewer tokens on more difficult problems with a 7.5% accuracy improvement.", "conclusion": "The approach effectively adapts reasoning length to problem difficulty, benefiting from stronger judges, and is supported by open-sourced code and data.", "key_contributions": ["Novel conciseness score for guiding reasoning", "Dynamic feedback from a judge model", "Significant improvements in efficiency and accuracy on benchmark datasets"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "reasoning traces"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2505.17260", "pdf": "https://arxiv.org/pdf/2505.17260.pdf", "abs": "https://arxiv.org/abs/2505.17260", "title": "The Rise of Parameter Specialization for Knowledge Storage in Large Language Models", "authors": ["Yihuai Hong", "Yiran Zhao", "Wei Tang", "Yang Deng", "Yu Rong", "Wenxuan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Over time, a growing wave of large language models from various series has\nbeen introduced to the community. Researchers are striving to maximize the\nperformance of language models with constrained parameter sizes. However, from\na microscopic perspective, there has been limited research on how to better\nstore knowledge in model parameters, particularly within MLPs, to enable more\neffective utilization of this knowledge by the model. In this work, we analyze\ntwenty publicly available open-source large language models to investigate the\nrelationship between their strong performance and the way knowledge is stored\nin their corresponding MLP parameters. Our findings reveal that as language\nmodels become more advanced and demonstrate stronger knowledge capabilities,\ntheir parameters exhibit increased specialization. Specifically, parameters in\nthe MLPs tend to be more focused on encoding similar types of knowledge. We\nexperimentally validate that this specialized distribution of knowledge\ncontributes to improving the efficiency of knowledge utilization in these\nmodels. Furthermore, by conducting causal training experiments, we confirm that\nthis specialized knowledge distribution plays a critical role in improving the\nmodel's efficiency in leveraging stored knowledge.", "AI": {"tldr": "This paper analyzes the relationship between the performance of large language models and the way knowledge is stored in MLP parameters, revealing that specialized knowledge distribution enhances efficiency.", "motivation": "To understand how to better store knowledge within MLP parameters of language models to improve their utilization efficiency.", "method": "Analyzed twenty open-source large language models, focusing on the distribution of knowledge in MLP parameters and conducting causal training experiments.", "result": "Found that as models advance, their MLP parameters specialize in encoding similar types of knowledge, which improves efficiency.", "conclusion": "Specialized knowledge distribution in model parameters is critical for enhancing the efficiency of knowledge utilization in language models.", "key_contributions": ["Analyzed MLP parameter knowledge storage in large language models", "Demonstrated that specialization of parameters improves efficiency", "Validated findings through causal training experiments"], "limitations": "", "keywords": ["large language models", "MLP parameters", "knowledge storage"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17265", "pdf": "https://arxiv.org/pdf/2505.17265.pdf", "abs": "https://arxiv.org/abs/2505.17265", "title": "CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports", "authors": ["Xiao Yu Cindy Zhang", "Carlos R. Ferreira", "Francis Rossignol", "Raymond T. Ng", "Wyeth Wasserman", "Jian Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant\ndiagnostic challenges. Case reports serve as key but computationally\nunderutilized resources to inform diagnosis. Clinical dense information\nextraction refers to organizing medical information into structured predefined\ncategories. Large Language Models (LLMs) may enable scalable information\nextraction from case reports but are rarely evaluated for this task. We\nintroduce CaseReportBench, an expert-annotated dataset for dense information\nextraction of case reports, focusing on IEMs. Using this dataset, we assess\nvarious models and prompting strategies, introducing novel approaches such as\ncategory-specific prompting and subheading-filtered data integration. Zero-shot\nchain-of-thought prompting offers little advantage over standard zero-shot\nprompting. Category-specific prompting improves alignment with the benchmark.\nThe open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our\nclinician evaluations show that LLMs can extract clinically relevant details\nfrom case reports, supporting rare disease diagnosis and management. We also\nhighlight areas for improvement, such as LLMs' limitations in recognizing\nnegative findings important for differential diagnosis. This work advances\nLLM-driven clinical natural language processing and paves the way for scalable\nmedical AI applications.", "AI": {"tldr": "This paper introduces CaseReportBench, a dataset for dense information extraction from case reports on Inborn Errors of Metabolism (IEM), assessing various LLMs and prompting strategies for clinical application.", "motivation": "To address diagnostic challenges in rare diseases through effective information extraction from underutilized case reports.", "method": "Developed an expert-annotated dataset (CaseReportBench) and tested various language models and prompting strategies for extracting structured medical information.", "result": "Category-specific prompting improved model performance, with Qwen2.5-7B outperforming GPT-4o; clinician evaluations show LLMs can successfully extract clinically relevant details.", "conclusion": "The study demonstrates the potential of LLMs in supporting rare disease diagnosis through improved extraction techniques, while indicating areas for enhancement in recognizing critical negative findings.", "key_contributions": ["Introduction of CaseReportBench dataset for IEM case reports", "Assessment of different prompting strategies for LLMs", "Demonstration of LLMs' ability to extract clinically relevant details"], "limitations": "LLMs have limitations in recognizing negative findings crucial for differential diagnosis.", "keywords": ["Inborn Errors of Metabolism", "Large Language Models", "Information Extraction", "Rare Diseases", "Clinical Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17266", "pdf": "https://arxiv.org/pdf/2505.17266.pdf", "abs": "https://arxiv.org/abs/2505.17266", "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning", "authors": ["Cehao Yang", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Xiaojun Wu", "Honghao Liu", "Hui Xiong", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.", "AI": {"tldr": "Select2Reason is a framework for selecting high-utility instructions for fine-tuning large language models to enhance long-chain-of-thought reasoning, showing it can achieve competitive performance with fewer data samples.", "motivation": "To improve the long-chain-of-thought reasoning abilities of large language models without the significant overhead associated with large-scale instruction datasets.", "method": "The Select2Reason framework selects and ranks instructions based on a difficulty estimator and a reasoning trace length-based heuristic for effective fine-tuning.", "result": "Fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance that is competitive or superior to full data tuning across multiple benchmarks.", "conclusion": "Select2Reason offers a scalable and efficient method for long-CoT instruction selection, proving effective across varying data sizes and adaptable to minimal costs in different instruction sets.", "key_contributions": ["Introduction of Select2Reason framework for instruction selection", "Demonstration of competitive performance with smaller training datasets", "Scalability and adaptability to other instruction pools with minimal cost"], "limitations": "", "keywords": ["large language models", "instruction tuning", "long-chain-of-thought", "data selection", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17267", "pdf": "https://arxiv.org/pdf/2505.17267.pdf", "abs": "https://arxiv.org/abs/2505.17267", "title": "GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations", "authors": ["Odysseas S. Chlapanis", "Dimitrios Galanis", "Nikolaos Aletras", "Ion Androutsopoulos"], "categories": ["cs.CL"], "comment": "19 pages, 17 figures, submitted to May ARR", "summary": "We introduce GreekBarBench, a benchmark that evaluates LLMs on legal\nquestions across five different legal areas from the Greek Bar exams, requiring\ncitations to statutory articles and case facts. To tackle the challenges of\nfree-text evaluation, we propose a three-dimensional scoring system combined\nwith an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to\nassess the correlation between LLM-judges and human expert evaluations,\nrevealing that simple, span-based rubrics improve their alignment. Our\nsystematic evaluation of 13 proprietary and open-weight LLMs shows that even\nthough the best models outperform average expert scores, they fall short of the\n95th percentile of experts.", "AI": {"tldr": "GreekBarBench is a benchmark for evaluating LLMs on legal questions from Greek Bar exams, using a unique scoring system and meta-evaluation.", "motivation": "To evaluate LLMs effectively on legal questions and improve alignment with human expert evaluations.", "method": "A three-dimensional scoring system is developed, combined with an LLM-as-a-judge approach and a meta-evaluation benchmark to assess LLM-judges against human experts.", "result": "The systematic evaluation of 13 LLMs showed that top models exceed average expert scores but do not reach the 95th percentile of experts.", "conclusion": "While LLMs can perform well on legal evaluations, they still lack the consistency and performance of top human experts in legal contexts.", "key_contributions": ["Introduction of GreekBarBench benchmark for legal LLM evaluation", "Development of a three-dimensional scoring system", "Meta-evaluation benchmark assessing correlation between LLM-judges and human experts."], "limitations": "The benchmark focuses exclusively on Greek legal contexts and statutory citations.", "keywords": ["LLM", "legal evaluation", "benchmark", "Greek Bar exams", "meta-evaluation"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2505.17281", "pdf": "https://arxiv.org/pdf/2505.17281.pdf", "abs": "https://arxiv.org/abs/2505.17281", "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty", "authors": ["Peilin Wu", "Mian Zhang", "Xinlu Zhang", "Xinya Du", "Zhiyu Zoey Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by enabling dynamic, multi-step reasoning and information\nretrieval. However, these systems often exhibit sub-optimal search behaviors\nlike over-search (retrieving redundant information) and under-search (failing\nto retrieve necessary information), which hinder efficiency and reliability.\nThis work formally defines and quantifies these behaviors, revealing their\nprevalence across multiple QA datasets and agentic RAG systems (e.g., one model\ncould have avoided searching in 27.7% of its search steps). Furthermore, we\ndemonstrate a crucial link between these inefficiencies and the models'\nuncertainty regarding their own knowledge boundaries, where response accuracy\ncorrelates with model's uncertainty in its search decisions. To address this,\nwe propose $\\beta$-GRPO, a reinforcement learning-based training method that\nincorporates confidence threshold to reward high-certainty search decisions.\nExperiments on seven QA benchmarks show that $\\beta$-GRPO enable a 3B model\nwith better agentic RAG ability, outperforming other strong baselines with a 4%\nhigher average exact match score.", "AI": {"tldr": "This paper addresses inefficiencies in Agentic Retrieval-Augmented Generation (RAG) systems by defining search behaviors that affect performance, proposing a new training method called $\beta$-GRPO to improve search decision-making through reinforcement learning.", "motivation": "Agentic RAG systems enhance LLMs but often suffer from inefficient search behaviors that hinder their effectiveness in information retrieval and reasoning.", "method": "The authors propose a reinforcement learning-based method called $\beta$-GRPO, which introduces a confidence threshold to improve search decisions in RAG systems.", "result": "Experiments demonstrate that $\beta$-GRPO significantly enhances the performance of a 3B model, achieving a 4% increase in average exact match score over strong baselines across seven QA benchmarks.", "conclusion": "By formalizing inefficiencies and introducing $\beta$-GRPO, the paper provides a solution to improve the agentic capabilities of RAG systems, enhancing their reliability in QA tasks.", "key_contributions": ["Formal definition and quantification of over-search and under-search behaviors in RAG systems.", "Proposal of $\beta$-GRPO to improve search decision-making using reinforcement learning and confidence thresholds.", "Demonstration of improved performance on QA benchmarks through empirical testing."], "limitations": "The study's results may vary across different RAG system architectures and applications.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Reinforcement Learning", "Search Efficiency", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17296", "pdf": "https://arxiv.org/pdf/2505.17296.pdf", "abs": "https://arxiv.org/abs/2505.17296", "title": "SELF: Self-Extend the Context Length With Logistic Growth Function", "authors": ["Phat Thanh Dang", "Saahil Thoppay", "Wang Yang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 3 tables", "summary": "Large language models suffer issues when operated on long contexts that are\nlarger than their training context length due to the standard position encoding\nfor tokens in the attention layer. Tokens a long distance apart will rarely\nhave an effect on each other and long prompts yield unexpected results. To\nsolve this problem, we propose SELF (Self-Extend the Context Length With\nLogistic Growth Function): a solution of grouping consecutive tokens at varying\ngroup sizes using a logistic capacity equation combined with a constant group\nsize at smaller relative distances. Our model had an increase in performance of\nup to 12% compared to the LongLM extension method in LEval (specifically on the\nQwen model). On summarization related tasks in LongBench, our model performed\nup to 6.4% better than LongLM (specifically on the Llama-2-7b model). On\nreading comprehension tasks from LEval, our model performed up to 5.4% better\nthan the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.", "AI": {"tldr": "Proposes SELF, a mechanism to extend context length in language models using a logistic growth function for grouping tokens, improving performance on long-context tasks.", "motivation": "Address the limitations of long context handling in language models, particularly when token distances affect attention mechanisms.", "method": "Utilizes a logistic capacity equation to group consecutive tokens at varying sizes while maintaining a constant size for smaller distances.", "result": "Achieved performance improvements of up to 12% compared to LongLM on LEval, with enhancements of 6.4% in summarization tasks and 5.4% in reading comprehension tasks.", "conclusion": "SELF significantly advances the ability of language models to manage long contexts effectively, providing a more reliable tool for tasks requiring extended context.", "key_contributions": ["Introduction of the SELF mechanism to extend context length", "Demonstrated up to 12% performance improvement over LongLM", "Open-source code available for implementation"], "limitations": "", "keywords": ["language models", "long context", "attention mechanisms", "performance improvement", "grouping tokens"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17306", "pdf": "https://arxiv.org/pdf/2505.17306.pdf", "abs": "https://arxiv.org/abs/2505.17306", "title": "Refusal Direction is Universal Across Safety-Aligned Languages", "authors": ["Xinpeng Wang", "Mingyang Wang", "Yihong Liu", "Hinrich Schütze", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Refusal mechanisms in large language models (LLMs) are essential for ensuring\nsafety. Recent research has revealed that refusal behavior can be mediated by a\nsingle direction in activation space, enabling targeted interventions to bypass\nrefusals. While this is primarily demonstrated in an English-centric context,\nappropriate refusal behavior is important for any language, but poorly\nunderstood. In this paper, we investigate the refusal behavior in LLMs across\n14 languages using PolyRefuse, a multilingual safety dataset created by\ntranslating malicious and benign English prompts into these languages. We\nuncover the surprising cross-lingual universality of the refusal direction: a\nvector extracted from English can bypass refusals in other languages with\nnear-perfect effectiveness, without any additional fine-tuning. Even more\nremarkably, refusal directions derived from any safety-aligned language\ntransfer seamlessly to others. We attribute this transferability to the\nparallelism of refusal vectors across languages in the embedding space and\nidentify the underlying mechanism behind cross-lingual jailbreaks. These\nfindings provide actionable insights for building more robust multilingual\nsafety defenses and pave the way for a deeper mechanistic understanding of\ncross-lingual vulnerabilities in LLMs.", "AI": {"tldr": "This paper investigates refusal mechanisms in large language models across 14 languages using a multilingual safety dataset, revealing that refusal vectors are transferable across languages, enhancing multilingual safety defenses.", "motivation": "To explore refusal behavior in large language models (LLMs) across multiple languages and ensure safety in diverse linguistic contexts.", "method": "Utilized a multilingual safety dataset (PolyRefuse) by translating malicious and benign English prompts into 14 different languages to investigate refusal behavior.", "result": "Discovered that a refusal direction extracted from English can effectively bypass refusals in other languages without fine-tuning, indicating cross-lingual universality and transferability of refusal vectors.", "conclusion": "The study provides insights for improving multilingual safety defenses in LLMs and deepens the understanding of safety vulnerabilities across languages.", "key_contributions": ["Created PolyRefuse, a multilingual safety dataset.", "Demonstrated cross-lingual universality of refusal behavior in LLMs.", "Identified underlying mechanisms of cross-lingual jailbreaks."], "limitations": "Focused on 14 languages; effectiveness in other languages not assessed.", "keywords": ["large language models", "refusal mechanisms", "multilingual safety datasets", "cross-lingual", "safety defenses"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.17320", "pdf": "https://arxiv.org/pdf/2505.17320.pdf", "abs": "https://arxiv.org/abs/2505.17320", "title": "Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Synthesizing expressive Japanese character speech poses unique challenges due\nto pitch-accent sensitivity and stylistic variability. This paper benchmarks\ntwo open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra\n(SBV2JE)--on in-domain, character-driven Japanese speech. Using three\ncharacter-specific datasets, we evaluate models across naturalness (mean\nopinion and comparative mean opinion score), intelligibility (word error rate),\nand speaker consistency. SBV2JE matches human ground truth in naturalness (MOS\n4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.\nEnhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE\nproves effective for applications like language learning and character dialogue\ngeneration, despite higher computational demands.", "AI": {"tldr": "The paper benchmarks two TTS models for producing expressive Japanese character speech, highlighting superiority of SBV2JE in naturalness and intelligibility metrics.", "motivation": "Efficiently synthesizing expressive Japanese speech for character-driven applications due to pitch-accent sensitivity and stylistic variability.", "method": "Benchmarks VITS and SBV2JE using three character-specific datasets, evaluating on naturalness, intelligibility, and speaker consistency.", "result": "SBV2JE matches human naturalness ratings closely and outperforms VITS in word error rate and comparative preference metrics.", "conclusion": "Despite higher computational demands, SBV2JE is effective for language learning and character dialogue generation.", "key_contributions": ["Introduction of SBV2JE for expressive Japanese speech synthesis", "Achievement of near-human naturalness in TTS applications", "Demonstration of effective pitch-accent controls"], "limitations": "Higher computational demands compared to standard models.", "keywords": ["Japanese speech synthesis", "text-to-speech", "expressive speech"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2505.17322", "pdf": "https://arxiv.org/pdf/2505.17322.pdf", "abs": "https://arxiv.org/abs/2505.17322", "title": "From Compression to Expansion: A Layerwise Analysis of In-Context Learning", "authors": ["Jiachen Jiang", "Yuxin Dong", "Jinxin Zhou", "Zhihui Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks without weight updates by learning from demonstration sequences.\nWhile ICL shows strong empirical performance, its internal representational\nmechanisms are not yet well understood. In this work, we conduct a statistical\ngeometric analysis of ICL representations to investigate how task-specific\ninformation is captured across layers. Our analysis reveals an intriguing\nphenomenon, which we term *Layerwise Compression-Expansion*: early layers\nprogressively produce compact and discriminative representations that encode\ntask information from the input demonstrations, while later layers expand these\nrepresentations to incorporate the query and generate the prediction. This\nphenomenon is observed consistently across diverse tasks and a range of\ncontemporary LLM architectures. We demonstrate that it has important\nimplications for ICL performance -- improving with model size and the number of\ndemonstrations -- and for robustness in the presence of noisy examples. To\nfurther understand the effect of the compact task representation, we propose a\nbias-variance decomposition and provide a theoretical analysis showing how\nattention mechanisms contribute to reducing both variance and bias, thereby\nenhancing performance as the number of demonstrations increases. Our findings\nreveal an intriguing layerwise dynamic in ICL, highlight how structured\nrepresentations emerge within LLMs, and showcase that analyzing internal\nrepresentations can facilitate a deeper understanding of model behavior.", "AI": {"tldr": "This paper analyzes the internal representational mechanisms of in-context learning (ICL) in large language models (LLMs), introducing the concept of Layerwise Compression-Expansion, where early layers create compact representations and later layers expand them for prediction.", "motivation": "To understand how in-context learning (ICL) representations in LLMs encode task-specific information and its implications for performance and robustness.", "method": "A statistical geometric analysis of representations in ICL across layers and tasks, highlighting the phenomenon of Layerwise Compression-Expansion.", "result": "The analysis shows that early layers create compact and discriminative representations, while later layers expand these representations, improving ICL performance with more data and larger models.", "conclusion": "The findings indicate a dynamic layerwise structure in ICL, enhancing our understanding of model behavior and the effects of attention mechanisms on variance and bias reduction in predictions.", "key_contributions": ["Introduction of Layerwise Compression-Expansion in ICL", "Theoretical analysis of bias-variance decomposition in LLMs", "Demonstration of internal representational dynamics affecting performance and robustness."], "limitations": "", "keywords": ["in-context learning", "large language models", "Layerwise Compression-Expansion", "representational analysis", "bias-variance decomposition"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17327", "pdf": "https://arxiv.org/pdf/2505.17327.pdf", "abs": "https://arxiv.org/abs/2505.17327", "title": "GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints", "authors": ["Soren DeHaan", "Yuanze Liu", "Johan Bollen", "Sa'ul A. Blanco"], "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT", "68U99", "I.2.7"], "comment": "13 pages", "summary": "The proliferation of Large Language Models (LLMs) in late 2022 has impacted\nacademic writing, threatening credibility, and causing institutional\nuncertainty. We seek to determine the degree to which LLMs are used to generate\ncritical text as opposed to being used for editing, such as checking for\ngrammar errors or inappropriate phrasing. In our study, we analyze arXiv papers\nfor stylistic segmentation, which we measure by varying a PELT threshold\nagainst a Bayesian classifier trained on GPT-regenerated text. We find that\nLLM-attributed language is not predictive of stylistic segmentation, suggesting\nthat when authors use LLMs, they do so uniformly, reducing the risk of\nhallucinations being introduced into academic preprints.", "AI": {"tldr": "The study examines the impact of Large Language Models (LLMs) on academic writing, focusing on their use in generating text versus editing in arXiv papers.", "motivation": "To determine how LLMs are being utilized in academic writing, particularly in generating critical text versus aiding in editing tasks.", "method": "Analysis of arXiv papers for stylistic segmentation using a Bayesian classifier trained on GPT-regenerated text and varying a PELT threshold.", "result": "The study reveals that LLM-attributed language does not predict stylistic segmentation, indicating uniform usage of LLMs by authors and a lower risk of hallucinations in academic preprints.", "conclusion": "Authors employing LLMs tend to use them uniformly, mitigating potential issues with credibility and consistency in academic writing.", "key_contributions": ["Investigation of LLM usage patterns in academic writing", "Demonstration of the impact of LLMs on stylistic segmentation", "Insights into the risks associated with LLMs in academic contexts"], "limitations": "", "keywords": ["Large Language Models", "academic writing", "stylistic segmentation", "Bayesian classifier", "GPT"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2505.17332", "pdf": "https://arxiv.org/pdf/2505.17332.pdf", "abs": "https://arxiv.org/abs/2505.17332", "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use", "authors": ["Hitesh Laxmichand Patel", "Amit Agarwal", "Arion Das", "Bhargava Kumar", "Srikant Panda", "Priyaranjan Pattnayak", "Taki Hasan Rafi", "Tejaswini Kumar", "Dong-Kyu Chae"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "I.2.7; I.2.6"], "comment": "Published in the Proceedings of the 2025 Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL\n  2025), Industry Track, pages 558-582", "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs)\nfor critical communication tasks, such as drafting emails, crafting sales\npitches, and composing casual messages. Deploying such models across different\nregions requires them to understand diverse cultural and linguistic contexts\nand generate safe and respectful responses. For enterprise applications, it is\ncrucial to mitigate reputational risks, maintain trust, and ensure compliance\nby effectively identifying and handling unsafe or offensive language. To\naddress this, we introduce SweEval, a benchmark simulating real-world scenarios\nwith variations in tone (positive or negative) and context (formal or\ninformal). The prompts explicitly instruct the model to include specific swear\nwords while completing the task. This benchmark evaluates whether LLMs comply\nwith or resist such inappropriate instructions and assesses their alignment\nwith ethical frameworks, cultural nuances, and language comprehension\ncapabilities. In order to advance research in building ethically aligned AI\nsystems for enterprise use and beyond, we release the dataset and code:\nhttps://github.com/amitbcp/multilingual_profanity.", "AI": {"tldr": "Introducing SweEval, a benchmark to evaluate LLM's handling of inappropriate language in enterprise communication across diverse cultural contexts.", "motivation": "As enterprises adopt LLMs for communication, understanding cultural and linguistic diversity, while ensuring safe and respectful responses, is crucial.", "method": "SweEval benchmark simulates real-world scenarios with variations in tone and context to test LLM compliance with ethical language use.", "result": "Evaluation of LLMs shows their ability to handle unsafe or offensive instructions in alignment with ethical frameworks and cultural nuances.", "conclusion": "The benchmark aids in developing ethically aligned AI systems for enterprise applications, with a dataset and code released for further research.", "key_contributions": ["Introduction of the SweEval benchmark for evaluating LLM behavior in diverse contexts.", "Assessment of LLM compliance with ethical frameworks when given inappropriate instructions.", "Release of dataset and code to support further research in the field."], "limitations": "The benchmark may not cover all possible cultural contexts and nuances.", "keywords": ["Large Language Models", "ethical AI", "enterprise applications", "benchmarking", "cultural context"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17345", "pdf": "https://arxiv.org/pdf/2505.17345.pdf", "abs": "https://arxiv.org/abs/2505.17345", "title": "Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking", "authors": ["Justin D. Norman", "Michael U. Rivera", "D. Alex Hughes"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Plausible, but inaccurate, tokens in model-generated text are widely believed\nto be pervasive and problematic for the responsible adoption of language\nmodels. Despite this concern, there is little scientific work that attempts to\nmeasure the prevalence of language model hallucination in a comprehensive way.\nIn this paper, we argue that language models should be evaluated using\nrepeatable, open, and domain-contextualized hallucination benchmarking. We\npresent a taxonomy of hallucinations alongside a case study that demonstrates\nthat when experts are absent from the early stages of data creation, the\nresulting hallucination metrics lack validity and practical utility.", "AI": {"tldr": "This paper discusses the prevalence of hallucinations in language models and advocates for a structured approach to measure these inaccuracies through a comprehensive benchmarking framework.", "motivation": "There is a growing concern about inaccuracies in model-generated text, often termed 'hallucinations', and the need for an effective way to measure their prevalence to facilitate responsible use of language models.", "method": "The authors propose a framework for evaluating hallucinations in language models using repeatable, open benchmarks that are contextualized to specific domains. A taxonomy of hallucinations is introduced, along with a case study to illustrate the issues with current metrics when experts are not involved.", "result": "The case study reveals that the validity and utility of hallucination metrics are significantly undermined when expert input is absent during the data creation process.", "conclusion": "For effective evaluation of language model hallucinations, the involvement of domain experts in the early stages is crucial to ensure that the metrics developed are valid and useful.", "key_contributions": ["Introduction of a taxonomy for language model hallucinations", "Proposing a repeatable and open benchmarking framework", "Demonstrating the importance of expert involvement in data creation for accurate hallucination metrics"], "limitations": "The proposed taxonomy and framework might still require refinement and wider testing across various domains to ensure comprehensive applicability.", "keywords": ["language models", "hallucination", "benchmarking", "taxonomy", "evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17362", "pdf": "https://arxiv.org/pdf/2505.17362.pdf", "abs": "https://arxiv.org/abs/2505.17362", "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit", "authors": ["Zafarullah Mahmood", "Soliman Ali", "Jiading Zhu", "Mohamed Abdelwahab", "Michelle Yu Collins", "Sihan Chen", "Yi Cheng Zhao", "Jodi Wolff", "Osnat Melamed", "Nadia Minian", "Marta Maslej", "Carolynne Cooper", "Matt Ratto", "Peter Selby", "Jonathan Rose"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Findings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL), Vienna, Austria, 2025", "summary": "The conversational capabilities of Large Language Models (LLMs) suggest that\nthey may be able to perform as automated talk therapists. It is crucial to know\nif these systems would be effective and adhere to known standards. We present a\ncounsellor chatbot that focuses on motivating tobacco smokers to quit smoking.\nIt uses a state-of-the-art LLM and a widely applied therapeutic approach called\nMotivational Interviewing (MI), and was evolved in collaboration with\nclinician-scientists with expertise in MI. We also describe and validate an\nautomated assessment of both the chatbot's adherence to MI and client\nresponses. The chatbot was tested on 106 participants, and their confidence\nthat they could succeed in quitting smoking was measured before the\nconversation and one week later. Participants' confidence increased by an\naverage of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed\nadherence to MI standards in 98% of utterances, higher than human counsellors.\nThe chatbot scored well on a participant-reported metric of perceived empathy\nbut lower than typical human counsellors. Furthermore, participants' language\nindicated a good level of motivation to change, a key goal in MI. These results\nsuggest that the automation of talk therapy with a modern LLM has promise.", "AI": {"tldr": "A chatbot using a Large Language Model focuses on motivating smokers to quit, demonstrating high adherence to Motivational Interviewing standards and improving participants' confidence.", "motivation": "To evaluate the effectiveness of a chatbot as an automated talk therapist for motivating tobacco smokers to quit smoking, using evidence-based therapeutic techniques.", "method": "A chatbot based on a state-of-the-art LLM was developed and tested with 106 participants, measuring their confidence to quit smoking before and after interactions, and assessing adherence to MI standards.", "result": "Participants' confidence in quitting increased by an average of 1.7 points on a 0-10 scale. The chatbot adhered to MI standards in 98% of its utterances, outperforming human counsellors, and received a favorable empathy score.", "conclusion": "The results indicate that LLMs can effectively support motivation in smoking cessation, showing the potential for automated talk therapy to adhere to therapeutic standards.", "key_contributions": ["Development of an LLM-based chatbot for tobacco cessation", "Demonstration of high adherence to Motivational Interviewing standards", "Validation of automated assessment for chatbot interactions"], "limitations": "The chatbot performed lower in perceived empathy compared to human counsellors, highlighting a potential area for improvement.", "keywords": ["Large Language Models", "Motivational Interviewing", "Chatbot", "Tobacco cessation", "Automated assessment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17380", "pdf": "https://arxiv.org/pdf/2505.17380.pdf", "abs": "https://arxiv.org/abs/2505.17380", "title": "AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing", "authors": ["Yinghui Huang", "Yuxuan Jiang", "Hui Liu", "Yixin Cai", "Weiqing Li", "Xiangen Hu"], "categories": ["cs.CL", "H.1.2; I.2.7"], "comment": "21 pages, 5 figures", "summary": "Large language models (LLMs) like GPT-4 show potential for scaling\nmotivational interviewing (MI) in addiction care, but require systematic\nevaluation of therapeutic capabilities. We present a computational framework\nassessing user-perceived quality (UPQ) through expected and unexpected MI\nbehaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI\ncollaboration, we developed predictive models integrating deep learning and\nexplainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)\nbehavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI\nperformance, reducing inappropriate advice while enhancing reflections and\nempathy. Although GPT-4 remained marginally inferior to therapists overall, it\ndemonstrated superior advice management capabilities. The model achieved\nmeasurable quality improvements through prompt engineering, yet showed\nlimitations in addressing complex emotional nuances. This framework establishes\na pathway for optimizing LLM-based therapeutic tools through targeted\nbehavioral metric analysis and human-AI co-evaluation. Findings highlight both\nthe scalability potential and current constraints of LLMs in clinical\ncommunication applications.", "AI": {"tldr": "The paper evaluates the use of GPT-4 for motivational interviewing (MI) in addiction care, highlighting improvements through prompt engineering and identifying both strengths and limitations in its therapeutic capabilities.", "motivation": "To evaluate the potential of large language models, specifically GPT-4, in enhancing motivational interviewing practices within addiction care.", "method": "The study uses a computational framework to analyze human therapist and GPT-4 MI sessions, developing predictive models that incorporate deep learning and explainable AI. It focuses on identifying MI-consistent and MI-inconsistent behaviors through user-perceived quality metrics.", "result": "The study found that while GPT-4 exhibited marginally less effectiveness than human therapists, it outperformed in managing advice and showed measurable quality improvements through customized prompting, but faced challenges with complex emotional nuances.", "conclusion": "The proposed framework paves the way for the optimization of LLM-based therapeutic tools, indicating both the promise and limitations of LLMs in clinical settings.", "key_contributions": ["Development of a computational framework for assessing user-perceived quality in MI sessions", "Identification of 17 behavioral metrics for MI consistency", "Evidence of improved performance of LLMs in therapeutic contexts through prompt engineering."], "limitations": "GPT-4 was inferior to human therapists in overall performance and struggled with complex emotional nuances.", "keywords": ["motivational interviewing", "large language models", "addiction care", "human-AI collaboration", "deep learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17387", "pdf": "https://arxiv.org/pdf/2505.17387.pdf", "abs": "https://arxiv.org/abs/2505.17387", "title": "WiNGPT-3.0 Technical Report", "authors": ["Boqin Zhuang", "Chenxiao Song", "Huitong Lu", "Jiacheng Qiao", "Mingqian Liu", "Mingxing Yu", "Ping Hong", "Rui Li", "Xiaoxia Song", "Xiangjun Xu", "Xu Chen", "Yaoyao Ma", "Yujie Gao"], "categories": ["cs.CL"], "comment": null, "summary": "Current Large Language Models (LLMs) exhibit significant limitations, notably\nin structured, interpretable, and verifiable medical reasoning, alongside\npractical deployment challenges related to computational resources and data\nprivacy. This report focused on the development of WiNGPT-3.0, the 32-billion\nparameter LLMs, engineered with the objective of enhancing its capacity for\nmedical reasoning and exploring its potential for effective integration within\nhealthcare IT infrastructures. The broader aim is to advance towards clinically\napplicable models. The approach involved a multi-stage training pipeline\ntailored for general, medical, and clinical reasoning. This pipeline\nincorporated supervised fine-tuning (SFT) and reinforcement learning (RL),\nleveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward\nmodels, and an evidence-based diagnostic chain simulation. WiNGPT-3.0\ndemonstrated strong performance: specific model variants achieved scores of\n66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training\nimproved performance on a clinical reasoning task from a baseline score of 58.1\nto 62.5. These findings suggest that reinforcement learning, even when applied\nwith a limited dataset of only a few thousand examples, can enhance medical\nreasoning accuracy. Crucially, this demonstration of RL's efficacy with limited\ndata and computation paves the way for more trustworthy and practically\ndeployable LLMs within clinical workflows and health information\ninfrastructures.", "AI": {"tldr": "WiNGPT-3.0 is a 32-billion parameter LLM designed to improve medical reasoning and its integration into healthcare IT. It uses a multi-stage training pipeline, achieving notable performance improvements on clinical reasoning tasks through reinforcement learning.", "motivation": "To address limitations in current LLMs regarding structured and interpretable medical reasoning, as well as challenges in deployment related to computational resources and data privacy.", "method": "A multi-stage training pipeline was implemented, incorporating supervised fine-tuning (SFT) and reinforcement learning (RL) with curated Long Chain-of-Thought (CoT) datasets and an evidence-based diagnostic chain simulation.", "result": "WiNGPT-3.0 achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE, and improved clinical reasoning task performance from 58.1 to 62.5 through targeted training.", "conclusion": "Reinforcement learning can enhance medical reasoning accuracy even with limited datasets, leading to more trustworthy and deployable LLMs in clinical settings.", "key_contributions": ["Development of WiNGPT-3.0 for enhanced medical reasoning", "Implementation of a novel multi-stage training pipeline", "Demonstrated effectiveness of reinforcement learning in medical contexts"], "limitations": "", "keywords": ["Large Language Models", "Medical Reasoning", "Reinforcement Learning", "Healthcare IT", "Clinical Workflow"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17390", "pdf": "https://arxiv.org/pdf/2505.17390.pdf", "abs": "https://arxiv.org/abs/2505.17390", "title": "Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting", "authors": ["Gauri Kambhatla", "Chantal Shaib", "Venkata Govindarajan"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-grained personas have recently been used for generating 'diverse'\nsynthetic data for pre-training and supervised fine-tuning of Large Language\nModels (LLMs). In this work, we measure the diversity of persona-driven\nsynthetically generated prompts and responses with a suite of lexical diversity\nand redundancy metrics. Firstly, we find that synthetic prompts/instructions\nare significantly less diverse than human-written ones. Next, we sample\nresponses from LLMs of different sizes with fine-grained and coarse persona\ndescriptions to investigate how much fine-grained detail in persona\ndescriptions contribute to generated text diversity. We find that while\npersona-prompting does improve lexical diversity (especially with larger\nmodels), fine-grained detail in personas doesn't increase diversity noticeably.", "AI": {"tldr": "This paper investigates the impact of fine-grained personas on the diversity of synthetic prompts and responses in large language models.", "motivation": "To evaluate how fine-grained personas affect the diversity of synthetic data generated for LLMs, aiming to enhance training and better understand diversity in machine-generated text.", "method": "The study employs lexical diversity and redundancy metrics to assess synthetic prompts and responses generated by LLMs of varying sizes, comparing them with human-written counterparts.", "result": "It was found that synthetic prompts are less diverse than human-written ones, and while persona-prompting enhances lexical diversity, fine-grained persona details do not significantly increase overall diversity.", "conclusion": "Using fine-grained personas can improve the diversity of prompts, but the level of detail in personas has limited impact on the diversity of generated text.", "key_contributions": ["Introduces metrics for measuring diversity in persona-driven prompts and responses.", "Demonstrates the lesser diversity of synthetic prompts compared to human-written ones.", "Finds limited effects of fine-grained personas on lexical diversity in LLM outputs."], "limitations": "The study focuses primarily on lexical diversity metrics, which may not capture other aspects of text quality or relevance.", "keywords": ["synthetic data", "large language models", "lexical diversity", "fine-grained personas", "text generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17391", "pdf": "https://arxiv.org/pdf/2505.17391.pdf", "abs": "https://arxiv.org/abs/2505.17391", "title": "Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation", "authors": ["Yuelyu Ji", "Rui Meng", "Zhuochun Li", "Daqing He"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) grounds large language models (LLMs) in\nup-to-date external evidence, yet existing multi-hop RAG pipelines still issue\nredundant subqueries, explore too shallowly, or wander through overly long\nsearch chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning\nframework that evolves a query-rewriting agent from broad early-stage\nexploration to concise late-stage refinement. EVO-RAG couples a seven-factor,\nstep-level reward vector (covering relevance, redundancy, efficiency, and\nanswer correctness) with a time-varying scheduler that reweights these signals\nas the episode unfolds. The agent is trained with Direct Preference\nOptimization over a multi-head reward model, enabling it to learn when to\nsearch, backtrack, answer, or refuse. Across four multi-hop QA benchmarks\n(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match\nby up to 4.6 points over strong RAG baselines while trimming average retrieval\ndepth by 15 %. Ablation studies confirm the complementary roles of curriculum\nstaging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for\nbuilding reliable, cost-effective multi-hop RAG systems.", "AI": {"tldr": "EVO-RAG is a framework that improves multi-hop RAG systems by enhancing query performance through curriculum-guided reinforcement learning and dynamic reward scheduling.", "motivation": "Existing multi-hop RAG pipelines have issues such as redundant subqueries and inefficient search paths, necessitating a solution that optimizes query generation and retrieval efficiency.", "method": "EVO-RAG employs a reinforcement learning framework that evolves a query-rewriting agent guided by a multi-faceted reward vector and a time-varying scheduler to refine query strategies over time.", "result": "EVO-RAG increases Exact Match scores by up to 4.6 points on multiple benchmarks while reducing average retrieval depth by 15%.", "conclusion": "EVO-RAG demonstrates significant advancements in the efficiency and reliability of multi-hop RAG systems through guided exploration and dynamic reward adjustments.", "key_contributions": ["Introduces a curriculum-guided RL approach to evolve RAG query agents.", "Implements a seven-factor reward system to balance query performance aspects.", "Enhances retrieval efficiency while maintaining answer correctness."], "limitations": "", "keywords": ["Retrieval-augmented generation", "Reinforcement learning", "Query optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17399", "pdf": "https://arxiv.org/pdf/2505.17399.pdf", "abs": "https://arxiv.org/abs/2505.17399", "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow", "authors": ["Haoyu Sun", "Huichen Will Wang", "Jiawei Gu", "Linjie Li", "Yu Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) \\textbf{across the full front-end\ndevelopment pipeline}. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.", "AI": {"tldr": "FullFront is a benchmark for evaluating Multimodal Large Language Models (MLLMs) across the entire front-end development pipeline, focusing on webpage design, perception, and code generation.", "motivation": "To assess the limitations of MLLMs in front-end engineering and provide a standardized evaluation framework that maintains diverse visual designs.", "method": "FullFront utilizes a two-stage process to convert real-world webpages into clean, standardized HTML while avoiding copyright issues and assesses three tasks: Webpage Design, Webpage Perception QA, and Webpage Code Generation.", "result": "Significant limitations were found in MLLM performance, especially in page perception, code generation related to image handling and layout, and interaction implementation.", "conclusion": "There is a substantial gap between the capabilities of current MLLMs and human expert performance in front-end engineering, underscoring the need for improved models.", "key_contributions": ["Introduction of the FullFront benchmark for front-end development", "Evaluation of MLLM performance in a comprehensive development pipeline", "Insights into specific limitations of MLLMs in design and implementation tasks."], "limitations": "The benchmark may not encompass all aspects of front-end development and focuses primarily on specific tasks.", "keywords": ["Multimodal Large Language Models", "front-end engineering", "webpage design", "code generation", "benchmarking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17407", "pdf": "https://arxiv.org/pdf/2505.17407.pdf", "abs": "https://arxiv.org/abs/2505.17407", "title": "Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?", "authors": ["Zhi Rui Tam", "Cheng-Kuang Wu", "Yu Ying Chiu", "Chieh-Yen Lin", "Yun-Nung Chen", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated impressive performance across\na range of reasoning tasks, yet little is known about their internal reasoning\nprocesses in multilingual settings. We begin with a critical question: {\\it In\nwhich language do these models reason when solving problems presented in\ndifferent languages?} Our findings reveal that, despite multilingual training,\nLRMs tend to default to reasoning in high-resource languages (e.g., English) at\ntest time, regardless of the input language. When constrained to reason in the\nsame language as the input, model performance declines, especially for\nlow-resource languages. In contrast, reasoning in high-resource languages\ngenerally preserves performance. We conduct extensive evaluations across\nreasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks\n(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies\nby task type: input-language reasoning degrades performance on reasoning tasks\nbut benefits cultural tasks, while safety evaluations exhibit language-specific\nbehavior. By exposing these linguistic biases in LRMs, our work highlights a\ncritical step toward developing more equitable models that serve users across\ndiverse linguistic backgrounds.", "AI": {"tldr": "The paper investigates the language in which large reasoning models (LRMs) perform reasoning when presented with multilingual problems, revealing biases towards high-resource languages and performance issues in low-resource languages.", "motivation": "To explore the internal reasoning processes of LRMs in multilingual contexts and their performance depending on the language used for reasoning.", "method": "The study involves extensive evaluations using reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks (CulturalBench, LMSYS-toxic) to measure how language choice impacts performance.", "result": "LRMs generally default to reasoning in high-resource languages like English, leading to performance degradation when they must reason in lower-resource languages, especially on reasoning tasks.", "conclusion": "Identifying linguistic biases in LRMs is necessary for creating models that provide equitable service to users in diverse languages.", "key_contributions": ["Highlighting the default reasoning language of LRM models affects performance.", "Providing evidence on performance decline in low-resource languages during reasoning tasks.", "Discussing the role of cultural context and safety evaluations in language-specific behavior."], "limitations": "The study primarily focuses on high-resource languages and may not cover all lower-resource languages adequately.", "keywords": ["Large Reasoning Models", "Multilingualism", "Language Bias", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17413", "pdf": "https://arxiv.org/pdf/2505.17413.pdf", "abs": "https://arxiv.org/abs/2505.17413", "title": "Conversations: Love Them, Hate Them, Steer Them", "authors": ["Niranjan Chebrolu", "Gerard Christopher Yeo", "Kokil Jaidka"], "categories": ["cs.CL"], "comment": "11 pages, 8 figures, 7 tables", "summary": "Large Language Models (LLMs) demonstrate increasing conversational fluency,\nyet instilling them with nuanced, human-like emotional expression remains a\nsignificant challenge. Current alignment techniques often address surface-level\noutput or require extensive fine-tuning. This paper demonstrates that targeted\nactivation engineering can steer LLaMA 3.1-8B to exhibit more human-like\nemotional nuances. We first employ attribution patching to identify causally\ninfluential components, to find a key intervention locus by observing\nactivation patterns during diagnostic conversational tasks. We then derive\nemotional expression vectors from the difference in the activations generated\nby contrastive text pairs (positive vs. negative examples of target emotions).\nApplying these vectors to new conversational prompts significantly enhances\nemotional characteristics: steered responses show increased positive sentiment\n(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of\ngreater personal engagement. Our findings offer a precise and interpretable\nmethod for controlling specific emotional attributes in LLMs, contributing to\ndeveloping more aligned and empathetic conversational AI.", "AI": {"tldr": "This paper presents a method for enhancing emotional expression in LLMs by using targeted activation engineering to manipulate emotional nuances in responses.", "motivation": "The challenge of instilling Large Language Models with nuanced, human-like emotional expression despite their increasing conversational fluency.", "method": "The authors utilize attribution patching to identify key components of LLaMA 3.1-8B's activation patterns during conversational tasks, and derive emotional expression vectors from contrasting text pairs to enhance emotional characteristics in responses.", "result": "The targeted activation engineering method significantly enhances the emotional characteristics of the LLM's responses, leading to increased positive sentiment and more personal engagement in replies.", "conclusion": "This approach provides a precise and interpretable method for controlling emotional attributes in LLMs, aiding in the development of more empathetic conversational AI.", "key_contributions": ["Demonstrated targeted activation engineering improves emotional expression in LLMs.", "Introduced a novel method using attribution patching to analyze activation patterns.", "Derived emotional expression vectors based on contrastive text pairs."], "limitations": "", "keywords": ["Large Language Models", "emotional expression", "activation engineering", "conversational AI", "sentiment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17420", "pdf": "https://arxiv.org/pdf/2505.17420.pdf", "abs": "https://arxiv.org/abs/2505.17420", "title": "DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies", "authors": ["Ning Yang", "Fangxin Liu", "Junjie Wang", "Tao Yang", "Kan Liu", "Haibing Guan", "Li Jiang"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages,5 figures", "summary": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of NLP tasks. However, their substantial inference cost poses a\nmajor barrier to real-world deployment, especially in latency-sensitive\nscenarios. To address this challenge, we propose \\textbf{DASH}, an adaptive\nlayer-skipping framework that dynamically selects computation paths conditioned\non input characteristics. We model the skipping process as a Markov Decision\nProcess (MDP), enabling fine-grained token-level decisions based on\nintermediate representations. To mitigate potential performance degradation\ncaused by skipping, we introduce a lightweight compensation mechanism that\ninjects differential rewards into the decision process. Furthermore, we design\nan asynchronous execution strategy that overlaps layer computation with policy\nevaluation to minimize runtime overhead. Experiments on multiple LLM\narchitectures and NLP benchmarks show that our method achieves significant\ninference acceleration while maintaining competitive task performance,\noutperforming existing methods.", "AI": {"tldr": "DASH accelerates inference in large language models by adaptively skipping layers based on input characteristics, modeled as a Markov Decision Process.", "motivation": "To reduce the inference cost of large language models in latency-sensitive applications and enable real-world deployment.", "method": "DASH uses an adaptive layer-skipping approach where computation paths are chosen dynamically based on input, utilizing a Markov Decision Process for token-level decisions and incorporating a compensation mechanism to offset any performance loss.", "result": "DASH significantly accelerates inference times without sacrificing task performance, surpassing existing methods in efficiency on multiple LLM architectures and NLP benchmarks.", "conclusion": "The framework effectively balances computational efficiency and model performance, making it suitable for practical applications of large language models.", "key_contributions": ["Introduction of the DASH framework for adaptive layer skipping in LLMs.", "Modeling the skipping process as an MDP for fine-grained decision-making.", "Development of a compensation mechanism to maintain performance during layer skipping."], "limitations": "The potential complexity of real-time implementation and dependency on specific layer architectures.", "keywords": ["large language models", "adaptive computation", "Markov Decision Process", "inference acceleration", "layer-skipping"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17427", "pdf": "https://arxiv.org/pdf/2505.17427.pdf", "abs": "https://arxiv.org/abs/2505.17427", "title": "T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Zezhong Wang", "Huimin Wang", "Yutian Zhao", "Bin Liang", "Yefeng Zheng", "Binyang Li", "Kam-Fai Wong", "Xian Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\nperformance in Contextual Question Answering (CQA). However, prior approaches\ntypically employ elaborate reasoning strategies regardless of question\ncomplexity, leading to low adaptability. Recent efficient test-time scaling\nmethods introduce budget constraints or early stop mechanisms to avoid\noverthinking for straightforward questions. But they add human bias to the\nreasoning process and fail to leverage models' inherent reasoning capabilities.\nTo address these limitations, we present T$^2$: Think-to-Think, a novel\nframework that dynamically adapts reasoning depth based on question complexity.\nT$^2$ leverages the insight that if an LLM can effectively solve similar\nquestions using specific reasoning strategies, it can apply the same strategy\nto the original question. This insight enables to adoption of concise reasoning\nfor straightforward questions while maintaining detailed analysis for complex\nproblems. T$^2$ works through four key steps: decomposing questions into\nstructural elements, generating similar examples with candidate reasoning\nstrategies, evaluating these strategies against multiple criteria, and applying\nthe most appropriate strategy to the original question. Experimental evaluation\nacross seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves\nhigher accuracy than baseline methods but also reduces computational overhead\nby up to 25.2\\%.", "AI": {"tldr": "Introducing T$^2$: Think-to-Think framework for adaptive reasoning in Contextual Question Answering using LLMs.", "motivation": "To improve adaptability and efficiency in LLM reasoning for Contextual Question Answering (CQA) by avoiding overthinking on simpler questions and leveraging inherent reasoning capabilities without human bias.", "method": "T$^2$ dynamically adjusts reasoning depth based on question complexity by decomposing questions, generating similar examples, evaluating reasoning strategies, and applying the most suitable one.", "result": "T$^2$ achieves higher accuracy compared to baseline methods and reduces computational overhead by up to 25.2%.", "conclusion": "The T$^2$ framework effectively balances reasoning depth and efficiency in answering questions of varying complexity.", "key_contributions": ["Introduction of a novel T$^2$ framework for adaptive reasoning", "Enhancement of accuracy in CQA tasks", "Reduction of computational overhead"], "limitations": "", "keywords": ["Large Language Models", "Contextual Question Answering", "adaptive reasoning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441.pdf", "abs": "https://arxiv.org/abs/2505.17441", "title": "Discovering Forbidden Topics in Language Models", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, LLM-crawler, that uses token prefilling to\nfind forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawl to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.", "AI": {"tldr": "This paper introduces refusal discovery, a method for identifying topics that language models refuse to discuss, and presents a tool called LLM-crawler to benchmark such refusals across various models.", "motivation": "To address biases, boundaries, and alignment failures in AI systems by identifying topics that language models refuse to discuss.", "method": "The method involves using token prefilling to discover forbidden topics, benchmarking on models like Tulu-3-8B and Claude-Haiku, and scaling the crawl on models like Llama-3.3-70B and its variants.", "result": "LLM-crawler successfully retrieved 31 out of 36 refusal topics within a 1000-prompt limit. It revealed censorship-related behavior in the DeepSeek-R1-70B model and demonstrated robustness in Perplexity-R1-1776-70B against censorship.", "conclusion": "The findings underscore the necessity for advanced refusal discovery methods to understand and mitigate biases in AI systems.", "key_contributions": ["Introduction of the refusal discovery problem setting", "Development of the LLM-crawler method for finding forbidden topics", "Benchmarking results across multiple language models revealing censorship patterns."], "limitations": "The study is limited to specific models and may not generalize to all language models.", "keywords": ["refusal discovery", "language models", "bias detection", "AI alignment", "censorship"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17446", "pdf": "https://arxiv.org/pdf/2505.17446.pdf", "abs": "https://arxiv.org/abs/2505.17446", "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models", "authors": ["Shunsuke Kando", "Yusuke Miyao", "Shinnosuke Takamichi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "The purpose of speech tokenization is to transform a speech signal into a\nsequence of discrete representations, serving as the foundation for speech\nlanguage models (SLMs). While speech tokenization has many options, their\neffect on the performance of SLMs remains unclear. This paper investigates two\nkey aspects of speech tokenization: the segmentation width and the cluster size\nof discrete units. First, we segment speech signals into fixed/variable widths\nand pooled representations. We then train K-means models in multiple cluster\nsizes. Through the evaluation on zero-shot spoken language understanding\nbenchmarks, we find the positive effect of moderately coarse segmentation and\nbigger cluster size. Notably, among the best-performing models, the most\nefficient one achieves a 50% reduction in training data and a 70% decrease in\ntraining runtime. Our analysis highlights the importance of combining multiple\ntokens to enhance fine-grained spoken language understanding.", "AI": {"tldr": "This study explores the impact of speech tokenization methods on the performance of speech language models, focusing on segmentation width and cluster size.", "motivation": "Understanding how different speech tokenization methods affect the performance of speech language models is crucial for enhancing spoken language understanding.", "method": "Speech signals are segmented into fixed and variable widths, and K-means models are trained with multiple cluster sizes to evaluate their impacts on performance.", "result": "Moderately coarse segmentation and larger cluster sizes significantly improve performance on zero-shot language understanding benchmarks, with one model achieving a 50% reduction in training data and a 70% decrease in training runtime.", "conclusion": "Combining multiple tokens can greatly enhance fine-grained spoken language understanding in SLMs, and careful selection of segmentation and cluster size is beneficial.", "key_contributions": ["Investigates the effects of segmentation width and cluster size on speech tokenization.", "Demonstrates significant performance improvements with specific tokenization strategies.", "Provides insight into efficient training data usage and runtime reduction."], "limitations": "", "keywords": ["speech tokenization", "speech language models", "segmentation", "K-means clustering", "spoken language understanding"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17447", "pdf": "https://arxiv.org/pdf/2505.17447.pdf", "abs": "https://arxiv.org/abs/2505.17447", "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization", "authors": ["Qi Zhang", "Shouqing Yang", "Lirong Gao", "Hao Chen", "Xiaomeng Hu", "Jinglei Chen", "Jiexiang Wang", "Sheng Guo", "Bo Zheng", "Haobo Wang", "Junbo Zhao"], "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nreasoning with the emergence of reasoning models like OpenAI-o1 and\nDeepSeek-R1. Recent research focuses on integrating reasoning capabilities into\nthe realm of retrieval-augmented generation (RAG) via outcome-supervised\nreinforcement learning (RL) approaches, while the correctness of intermediate\nthink-and-search steps is usually neglected. To address this issue, we design a\nprocess-level reward module to mitigate the unawareness of intermediate\nreasoning steps in outcome-level supervision without additional annotation.\nGrounded on this, we propose Learning to Think-and-Search (LeTS), a novel\nframework that hybridizes stepwise process reward and outcome-based reward to\ncurrent RL methods for RAG. Extensive experiments demonstrate the\ngeneralization and inference efficiency of LeTS across various RAG benchmarks.\nIn addition, these results reveal the potential of process- and outcome-level\nreward hybridization in boosting LLMs' reasoning ability via RL under other\nscenarios. The code will be released soon.", "AI": {"tldr": "This paper presents Learning to Think-and-Search (LeTS), a framework that enhances reasoning capabilities in retrieval-augmented generation (RAG) models through a novel hybrid reward system.", "motivation": "To improve the reasoning abilities of large language models (LLMs) during retrieval-augmented generation by addressing the neglect of intermediate reasoning steps in current reinforcement learning approaches.", "method": "The authors propose a process-level reward module that combines stepwise process rewards with outcome-based rewards, enhancing existing RL methods for RAG without requiring additional annotation.", "result": "Extensive experiments show that LeTS improves generalization and inference efficiency across various RAG benchmarks, indicating the efficacy of hybridizing process and outcome rewards in enhancing LLMs' reasoning capabilities.", "conclusion": "The research suggests that integrating process- and outcome-level rewards can significantly boost the reasoning abilities of LLMs in retrieval-augmented generation and may be applicable in other contexts.", "key_contributions": ["Proposed a novel framework (LeTS) for enhancing reasoning in RAG models", "Introduced a process-level reward module that addresses intermediate reasoning steps", "Demonstrated effective generalization and efficiency improvements in LLM reasoning capabilities."], "limitations": "Results may vary across different RAG benchmarks, and the applicability to a broader range of tasks is yet to be fully explored.", "keywords": ["large language models", "reasoning", "retrieval-augmented generation", "reinforcement learning", "process-level reward"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.17455", "pdf": "https://arxiv.org/pdf/2505.17455.pdf", "abs": "https://arxiv.org/abs/2505.17455", "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models", "authors": ["Youliang Yuan", "Wenxiang Jiao", "Yuejin Xie", "Chihao Shen", "Menghan Tian", "Wenxuan Wang", "Jen-tse Huang", "Pinjia He"], "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Human safety awareness gaps often prevent the timely recognition of everyday\nrisks. In solving this problem, a proactive safety artificial intelligence (AI)\nsystem would work better than a reactive one. Instead of just reacting to\nusers' questions, it would actively watch people's behavior and their\nenvironment to detect potential dangers in advance. Our Proactive Safety Bench\n(PaSBench) evaluates this capability through 416 multimodal scenarios (128\nimage sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation\nof 36 advanced models reveals fundamental limitations: Top performers like\nGemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks\nin repeated trials. Through failure analysis, we identify unstable proactive\nreasoning rather than knowledge deficits as the primary limitation. This work\nestablishes (1) a proactive safety benchmark, (2) systematic evidence of model\nlimitations, and (3) critical directions for developing reliable protective AI.\nWe believe our dataset and findings can promote the development of safer AI\nassistants that actively prevent harm rather than merely respond to requests.\nOur dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.", "AI": {"tldr": "The paper introduces a proactive AI system for human safety awareness, evaluating through 416 multimodal scenarios, and highlights limitations in model performance in risk detection.", "motivation": "To address gaps in human safety awareness that prevent timely recognition of everyday risks.", "method": "Evaluates the capability of proactive AI in safety awareness through 416 multimodal scenarios, including image sequences and text logs.", "result": "36 advanced models were evaluated, achieving 71% image and 64% text accuracy, but still missing 45-55% risks in repeated trials.", "conclusion": "The study establishes a proactive safety benchmark, provides evidence of model limitations, and suggests directions for developing reliable protective AI.", "key_contributions": ["Establishment of a proactive safety benchmark (PaSBench)", "Systematic evidence of limitations in AI models' proactive reasoning", "Identifying unstable reasoning as a primary limitation in proactive safety AI"], "limitations": "Models exhibited unstable proactive reasoning rather than knowledge deficits as their primary limitation.", "keywords": ["proactive safety AI", "human safety awareness", "model limitations", "risk detection", "safety benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17464", "pdf": "https://arxiv.org/pdf/2505.17464.pdf", "abs": "https://arxiv.org/abs/2505.17464", "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo.", "AI": {"tldr": "Hydra is a training-free framework that enhances retrieval-augmented generation for large language models by improving multi-hop reasoning, multi-entity question handling, and source verification through unified evidence retrieval from knowledge graphs and documents.", "motivation": "Current hybrid retrieval-augmented generation systems struggle with complex reasoning tasks and verification when sourcing information from multiple types of evidence.", "method": "Hydra integrates graph topology, document semantics, and source reliability through agent-driven exploration that enhances retrieval processes and cross-source verification.", "result": "Hydra outperforms the baseline model ToG-2 by an average of 20.3% across benchmarks, enabling smaller models like Llama-3.1-8B to perform comparably to GPT-4-Turbo in reasoning tasks.", "conclusion": "Hydra demonstrates significant improvements in retrieval-augmented generation tasks and facilitates better performance from smaller models through effective evidence synthesis.", "key_contributions": ["Training-free integration of structured and unstructured retrieval", "Effective handling of multi-hop and multi-entity reasoning", "Robust multi-source verification mechanism"], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "knowledge graphs", "multi-hop reasoning", "evidence retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17465", "pdf": "https://arxiv.org/pdf/2505.17465.pdf", "abs": "https://arxiv.org/abs/2505.17465", "title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards", "authors": ["Roelien C Timmer", "Yufang Hou", "Stephen Wan"], "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "An important task in machine learning (ML) research is comparing prior work,\nwhich is often performed via ML leaderboards: a tabular overview of experiments\nwith comparable conditions (e.g., same task, dataset, and metric). However, the\ngrowing volume of literature creates challenges in creating and maintaining\nthese leaderboards. To ease this burden, researchers have developed methods to\nextract leaderboard entries from research papers for automated leaderboard\ncuration. Yet, prior work varies in problem framing, complicating comparisons\nand limiting real-world applicability. In this position paper, we present the\nfirst overview of Automatic Leaderboard Generation (ALG) research, identifying\nfundamental differences in assumptions, scope, and output formats. We propose\nan ALG unified conceptual framework to standardise how the ALG task is defined.\nWe offer ALG benchmarking guidelines, including recommendations for datasets\nand metrics that promote fair, reproducible evaluation. Lastly, we outline\nchallenges and new directions for ALG, such as, advocating for broader coverage\nby including all reported results and richer metadata.", "AI": {"tldr": "The paper presents a framework for standardizing Automatic Leaderboard Generation (ALG) in machine learning, addressing challenges in comparing prior work due to inconsistencies in research methodologies.", "motivation": "The increasing volume of machine learning literature complicates the creation and maintenance of experimental leaderboards essential for comparing prior work effectively.", "method": "The paper reviews existing ALG research, identifying differences in assumptions, scope, and output formats, and proposes a unified framework for standardization.", "result": "The proposed framework aims to enhance the consistency of leaderboard generation and improve the reproducibility of evaluations in machine learning research.", "conclusion": "By adopting standardized definitions and benchmarking guidelines, the paper seeks to facilitate more effective comparison of ML research outputs and promote broader inclusion of results and metadata.", "key_contributions": ["Overview of Automatic Leaderboard Generation (ALG) research", "Proposed unified conceptual framework for ALG", "Guidelines for benchmarking including dataset and metric recommendations"], "limitations": "The paper may not cover all existing ALG methodologies and their diverse applications within different ML contexts.", "keywords": ["Automatic Leaderboard Generation", "Machine Learning", "Benchmarking Guidelines"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17470", "pdf": "https://arxiv.org/pdf/2505.17470.pdf", "abs": "https://arxiv.org/abs/2505.17470", "title": "SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models", "authors": ["Xiang Liu", "Zhaoxiang Liu", "Peng Wang", "Kohou Wang", "Huan Hu", "Kai Wang", "Shiguo Lian"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "When using supervised fine-tuning (SFT) to adapt large language models (LLMs)\nto specific domains, a significant challenge arises: should we use the entire\nSFT dataset for fine-tuning? Common practice often involves fine-tuning\ndirectly on the entire dataset due to limited information on the LLM's past\ntraining data. However, if the SFT dataset largely overlaps with the model's\nexisting knowledge, the performance gains are minimal, leading to wasted\ncomputational resources. Identifying the unknown knowledge within the SFT\ndataset and using it to fine-tune the model could substantially improve the\ntraining efficiency. To address this challenge, we propose a self-learning\nframework for LLMs inspired by human learning pattern. This framework takes a\nfine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer\nthe questions in the SFT dataset. The LLMs then objectively grade the responses\nand filter out the incorrectly answered QA pairs. Finally, we fine-tune the\nLLMs based on this filtered QA set. Experimental results in the fields of\nagriculture and medicine demonstrate that our method substantially reduces\ntraining time while achieving comparable improvements to those attained with\nfull dataset fine-tuning. By concentrating on the unknown knowledge within the\nSFT dataset, our approach enhances the efficiency of fine-tuning LLMs.", "AI": {"tldr": "Proposes a self-learning framework for fine-tuning LLMs that focuses on unknown knowledge within SFT datasets to improve efficiency and reduce training time.", "motivation": "Supervised fine-tuning on entire SFT datasets may waste resources if the dataset overlaps with the model's existing knowledge.", "method": "The framework uses an SFT dataset, where LLMs answer questions, grade their responses, filter incorrect QA pairs, and fine-tune based on the filtered set.", "result": "Experimental results in agriculture and medicine show substantial reductions in training time with comparable performance improvements over full dataset fine-tuning.", "conclusion": "By targeting unknown knowledge in SFT datasets, the proposed method enhances LLM fine-tuning efficiency.", "key_contributions": ["Introduces a self-learning framework for LLM fine-tuning.", "Reduces dependency on full dataset training.", "Demonstrates improved training efficiency and comparable performance across domains."], "limitations": "", "keywords": ["fine-tuning", "large language models", "supervised learning", "training efficiency", "machine learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.17471", "pdf": "https://arxiv.org/pdf/2505.17471.pdf", "abs": "https://arxiv.org/abs/2505.17471", "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain", "authors": ["Suifeng Zhao", "Zhuoran Jin", "Sujian Li", "Jun Gao"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a vital role in the financial\ndomain, powering applications such as real-time market analysis, trend\nforecasting, and interest rate computation. However, most existing RAG research\nin finance focuses predominantly on textual data, overlooking the rich visual\ncontent in financial documents, resulting in the loss of key analytical\ninsights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual\nRAG benchmark tailored for finance which effectively integrates multimodal data\nand provides visual citation to ensure traceability. It includes a bilingual\nretrieval corpus with 60,780 Chinese and 51,219 English pages, along with a\nhigh-quality, human-annotated question-answering (QA) dataset spanning\nheterogeneous data types and seven question categories. Moreover, we introduce\nRGenCite, an RAG baseline that seamlessly integrates visual citation with\ngeneration. Furthermore, we propose an automatic citation evaluation method to\nsystematically assess the visual citation capabilities of Multimodal Large\nLanguage Models (MLLMs). Extensive experiments on RGenCite underscore the\nchallenging nature of FinRAGBench-V, providing valuable insights for the\ndevelopment of multimodal RAG systems in finance.", "AI": {"tldr": "FinRAGBench-V is a benchmark for visual RAG in finance, integrating multimodal data for improved analysis.", "motivation": "To address the gap in existing RAG research which focuses mainly on textual data and neglects visual content in financial documents.", "method": "Development of FinRAGBench-V as a visual RAG benchmark with multimodal data integration, accompanied by a bilingual retrieval corpus and a QA dataset.", "result": "RGenCite, a baseline RAG model, demonstrates the challenges of utilizing visual citation in multimodal RAG systems for finance.", "conclusion": "FinRAGBench-V provides crucial insights and capabilities for enhancing multimodal RAG applications in the financial domain.", "key_contributions": ["Introduction of FinRAGBench-V for visual RAG in finance", "Creation of a bilingual retrieval corpus and QA dataset", "Development of an automatic citation evaluation method for MLLMs."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Visual RAG", "Finance", "Bilingual retrieval corpus", "Multimodal data"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.17481", "pdf": "https://arxiv.org/pdf/2505.17481.pdf", "abs": "https://arxiv.org/abs/2505.17481", "title": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning", "authors": ["Yusheng Zhao", "Xiao Luo", "Weizhi Zhang", "Wei Ju", "Zhiping Xiao", "Philip S. Yu", "Ming Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to reason is one of the most fundamental capabilities of large\nlanguage models (LLMs), enabling a wide range of downstream tasks through\nsophisticated problem-solving. A critical aspect of this is code reasoning,\nwhich involves logical reasoning with formal languages (i.e., programming\ncode). In this paper, we enhance this capability of LLMs by exploring the\nfollowing question: how can an LLM agent become progressively smarter in code\nreasoning with each solution it proposes, thereby achieving substantial\ncumulative improvement? Most existing research takes a static perspective,\nfocusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a\ncognitive-evolving perspective and propose a novel framework named\nMeta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve\ndynamically during inference through self-improvement. From the perspective of\nhuman cognitive development, we leverage both knowledge accumulation and lesson\nsharing. In particular, to accumulate knowledge during problem-solving, we\npropose meta-reflection that reflects on the reasoning paths of the current\nproblem to obtain knowledge and experience for future consideration. Moreover,\nto effectively utilize the lessons from other agents, we propose\ncross-referencing that incorporates the solution and feedback from other agents\ninto the current problem-solving process. We conduct experiments across various\ndatasets in code reasoning, and the results demonstrate the effectiveness of\nMARCO.", "AI": {"tldr": "This paper introduces MARCO, a framework that enables LLMs to dynamically improve their code reasoning abilities through self-reflection and cross-referencing with other agents' experiences.", "motivation": "To enhance the problem-solving capabilities of LLMs in code reasoning by allowing them to self-improve during inference, thereby addressing the limitations of static problem-solving approaches.", "method": "The framework, MARCO, employs meta-reflection for knowledge accumulation and cross-referencing for integrating lessons from other agents during problem-solving.", "result": "Experiments show that MARCO significantly improves the effectiveness of LLMs in code reasoning over existing static approaches.", "conclusion": "By adopting a cognitive-evolving perspective, MARCO allows LLMs to evolve and improve their reasoning capabilities, leading to more effective problem-solving.", "key_contributions": ["Introduction of a dynamic self-improvement framework for LLMs in code reasoning.", "Utilization of cognitive development principles to enhance reasoning efficacy.", "Demonstration of improved performance through experimental validation over various datasets."], "limitations": "", "keywords": ["large language models", "code reasoning", "self-improvement", "meta-reflection", "cross-referencing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17485", "pdf": "https://arxiv.org/pdf/2505.17485.pdf", "abs": "https://arxiv.org/abs/2505.17485", "title": "keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection", "authors": ["Saketh Reddy Vemula", "Parameswari Krishnamurthy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Identification of hallucination spans in black-box language model generated\ntext is essential for applications in the real world. A recent attempt at this\ndirection is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on\nHallucinations and Related Observable Over-generation Errors. In this work, we\npresent our solution to this problem, which capitalizes on the variability of\nstochastically-sampled responses in order to identify hallucinated spans. Our\nhypothesis is that if a language model is certain of a fact, its sampled\nresponses will be uniform, while hallucinated facts will yield different and\nconflicting results. We measure this divergence through entropy-based analysis,\nallowing for accurate identification of hallucinated segments. Our method is\nnot dependent on additional training and hence is cost-effective and adaptable.\nIn addition, we conduct extensive hyperparameter tuning and perform error\nanalysis, giving us crucial insights into model behavior.", "AI": {"tldr": "This paper presents a method for identifying hallucinated spans in text generated by language models using entropy-based analysis to detect variability in responses.", "motivation": "Identifying hallucination spans in language model-generated text is crucial for real-world applications, especially in ensuring the reliability of AI-generated content.", "method": "The proposed solution utilizes entropy-based analysis to measure the uniformity of stochastically-sampled responses from a language model, with the underlying hypothesis that hallucinations produce more diverse responses.", "result": "The method allows for accurate identification of hallucinated segments without requiring additional model training, demonstrating cost-effectiveness and adaptability.", "conclusion": "The study concludes that the entropy-based approach is effective in detecting hallucinations in language model outputs and provides insights into the model’s behavior.", "key_contributions": ["Introduction of an entropy-based approach to identify hallucinated spans", "Cost-effective and adaptable method that does not rely on additional training", "Comprehensive hyperparameter tuning and error analysis conducted."], "limitations": "", "keywords": ["hallucination detection", "language models", "entropy analysis", "text generation", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17496", "pdf": "https://arxiv.org/pdf/2505.17496.pdf", "abs": "https://arxiv.org/abs/2505.17496", "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models", "authors": ["Chi-Yuan Hsiao", "Ke-Han Lu", "Kai-Wei Chang", "Chih-Kai Yang", "Wei-Chih Chen", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "End-to-end training of Spoken Language Models (SLMs) commonly involves\nadapting pre-trained text-based Large Language Models (LLMs) to the speech\nmodality through multi-stage training on diverse tasks such as ASR, TTS and\nspoken question answering (SQA). Although this multi-stage continual learning\nequips LLMs with both speech understanding and generation capabilities, the\nsubstantial differences in task and data distributions across stages can lead\nto catastrophic forgetting, where previously acquired knowledge is lost. This\npaper investigates catastrophic forgetting and evaluates three mitigation\nstrategies-model merging, discounting the LoRA scaling factor, and experience\nreplay to balance knowledge retention with new learning. Results show that\nexperience replay is the most effective, with further gains achieved by\ncombining it with other methods. These findings provide insights for developing\nmore robust and efficient SLM training pipelines.", "AI": {"tldr": "This paper explores methods to mitigate catastrophic forgetting in the end-to-end training of Spoken Language Models (SLMs) adapted from pre-trained text-based LLMs, highlighting experience replay as the most effective strategy.", "motivation": "The paper addresses the challenge of catastrophic forgetting in the multi-stage training of Spoken Language Models (SLMs), where differences in task and data distributions can lead to loss of previously learned knowledge.", "method": "The paper evaluates three strategies for mitigating catastrophic forgetting: model merging, discounting the LoRA scaling factor, and experience replay, with experimental results to assess their effectiveness.", "result": "Experience replay emerges as the most effective method, and combining it with other strategies yields further improvements in knowledge retention.", "conclusion": "The findings provide insights that can guide the creation of more robust and efficient training pipelines for Spoken Language Models.", "key_contributions": ["Investigation of catastrophic forgetting in SLM training", "Evaluation of mitigation strategies including experience replay", "Insights for developing efficient SLM training pipelines"], "limitations": "The study might be limited to specific SLM architectures or datasets in its evaluations.", "keywords": ["Spoken Language Models", "catastrophic forgetting", "experience replay", "multi-stage training", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17503", "pdf": "https://arxiv.org/pdf/2505.17503.pdf", "abs": "https://arxiv.org/abs/2505.17503", "title": "CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents", "authors": ["Minsoo Khang", "Sangjun Park", "Teakgyu Hong", "Dawoon Jung"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made substantial progress in recent years,\nyet evaluating their capabilities in practical Retrieval-Augmented Generation\n(RAG) scenarios remains challenging. In practical applications, LLMs must\ndemonstrate complex reasoning, refuse to answer appropriately, provide precise\ncitations, and effectively understand document layout. These capabilities are\ncrucial for advanced task handling, uncertainty awareness, maintaining\nreliability, and structural understanding. While some of the prior works\naddress these aspects individually, there is a need for a unified framework\nthat evaluates them collectively in practical RAG scenarios. To address this,\nwe present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation\nwith Complex Reasoning over Structured Documents), a benchmark designed to\nassess these key dimensions holistically. CReSt comprises 2,245 human-annotated\nexamples in English and Korean, designed to capture practical RAG scenarios\nthat require complex reasoning over structured documents. It also introduces a\ntailored evaluation methodology to comprehensively assess model performance in\nthese critical areas. Our evaluation shows that even advanced LLMs struggle to\nperform consistently across these dimensions, underscoring key areas for\nimprovement. We release CReSt to support further research and the development\nof more robust RAG systems. The dataset and code are available at:\nhttps://github.com/UpstageAI/CReSt.", "AI": {"tldr": "CReSt is a benchmark for evaluating Retrieval-Augmented Generation (RAG) with a focus on complex reasoning over structured documents, revealing LLMs' limitations.", "motivation": "Despite advancements in Large Language Models (LLMs), assessing their capabilities in practical RAG scenarios poses challenges. A unified evaluation framework is necessary to collectively measure their performance in handling complex tasks.", "method": "The paper introduces CReSt, a benchmark containing 2,245 human-annotated examples in English and Korean, aimed at evaluating the performance of LLMs in RAG scenarios that require complex reasoning over structured documents. The evaluation methodology is tailored to assess key dimensions of LLM performance holistically.", "result": "The evaluation reveals that even advanced LLMs face difficulties maintaining consistency across the crucial dimensions of reasoning, citation precision, uncertainty awareness, and document comprehension.", "conclusion": "The findings highlight significant areas for improvement in LLMs and aim to foster the development of more robust RAG systems. CReSt is released for ongoing research in this domain.", "key_contributions": ["Introduction of a comprehensive benchmark (CReSt) for RAG evaluation.", "Inclusion of complex reasoning scenarios in structured documents.", "Release of a tailored evaluation methodology for model performance assessment."], "limitations": "The focus is primarily on English and Korean languages, which may limit applicability to other languages or contexts.", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Benchmark", "Complex Reasoning", "Structured Documents"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17505", "pdf": "https://arxiv.org/pdf/2505.17505.pdf", "abs": "https://arxiv.org/abs/2505.17505", "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "authors": ["Xiaohao Liu", "Xiaobo Xia", "Weixiang Zhao", "Manyi Zhang", "Xianzhi Yu", "Xiu Su", "Shuo Yang", "See-Kiong Ng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved notable progress. Despite their\nsuccess, next-token prediction (NTP), the dominant method for LLM training and\ninference, is constrained in both contextual coverage and inference efficiency\ndue to its inherently sequential process. To overcome these challenges, we\npropose leap multi-token prediction~(L-MTP), an innovative token prediction\nmethod that extends the capabilities of multi-token prediction (MTP) by\nintroducing a leap-based mechanism. Unlike conventional MTP, which generates\nmultiple tokens at adjacent positions, L-MTP strategically skips over\nintermediate tokens, predicting non-sequential ones in a single forward pass.\nThis structured leap not only enhances the model's ability to capture\nlong-range dependencies but also enables a decoding strategy specially\noptimized for non-sequential leap token generation, effectively accelerating\ninference. We theoretically demonstrate the benefit of L-MTP in improving\ninference efficiency. Experiments across diverse benchmarks validate its merit\nin boosting both LLM performance and inference speed. The source code will be\npublicly available.", "AI": {"tldr": "This paper introduces leap multi-token prediction (L-MTP), a novel approach for training large language models that enhances inference efficiency and performance by predicting non-sequential tokens in one pass.", "motivation": "To address the limitations of next-token prediction in LLMs related to contextual coverage and inference efficiency.", "method": "The paper proposes L-MTP, which utilizes a leap-based mechanism to predict non-sequential tokens, thereby enhancing the model's ability to capture long-range dependencies and improving inference speed.", "result": "L-MTP shows significant improvements in LLM performance and inference speed across various benchmarks, demonstrating the advantages of this new method over conventional prediction techniques.", "conclusion": "The proposed L-MTP method effectively addresses the limitations of traditional next-token prediction methods in LLMs, promising faster and more efficient token generation.", "key_contributions": ["Introduction of leap multi-token prediction (L-MTP) method", "Enhanced inference efficiency through non-sequential token prediction", "Demonstrated improvement in LLM performance across benchmarks"], "limitations": "", "keywords": ["large language models", "multi-token prediction", "inference efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.17510", "pdf": "https://arxiv.org/pdf/2505.17510.pdf", "abs": "https://arxiv.org/abs/2505.17510", "title": "Large Language Models Do Multi-Label Classification Differently", "authors": ["Marcus Ma", "Georgios Chochlakis", "Niyantha Maruthu Pandiyan", "Jesse Thomason", "Shrikanth Narayanan"], "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 6 tables", "summary": "Multi-label classification is prevalent in real-world settings, but the\nbehavior of Large Language Models (LLMs) in this setting is understudied. We\ninvestigate how autoregressive LLMs perform multi-label classification, with a\nfocus on subjective tasks, by analyzing the output distributions of the models\nin each generation step. We find that their predictive behavior reflects the\nmultiple steps in the underlying language modeling required to generate all\nrelevant labels as they tend to suppress all but one label at each step. We\nfurther observe that as model scale increases, their token distributions\nexhibit lower entropy, yet the internal ranking of the labels improves.\nFinetuning methods such as supervised finetuning and reinforcement learning\namplify this phenomenon. To further study this issue, we introduce the task of\ndistribution alignment for multi-label settings: aligning LLM-derived label\ndistributions with empirical distributions estimated from annotator responses\nin subjective tasks. We propose both zero-shot and supervised methods which\nimprove both alignment and predictive performance over existing approaches.", "AI": {"tldr": "The paper examines the performance of Large Language Models (LLMs) in multi-label classification through a detailed analysis of their output behavior, particularly in subjective tasks.", "motivation": "To understand the performance of autoregressive LLMs in multi-label classification tasks, particularly how they manage the generation of multiple relevant labels and the impact of model scaling and finetuning techniques.", "method": "The study analyzes output distributions of LLMs during generation, investigating the entropy of token distributions and the effectiveness of zero-shot and supervised methods for distribution alignment.", "result": "LLMs suppress all but one label in multi-label tasks, showing improved internal ranking of labels as model scale increases, with lower entropy in token distributions. Proposed methods enhance alignment and predictive performance.", "conclusion": "The findings suggest that finetuning methods combined with new alignment tasks can significantly improve the performance of LLMs in multi-label classification.", "key_contributions": ["Analysis of LLMs' predictive behavior in multi-label classification.", "Introduction of distribution alignment task for multi-label settings.", "Demonstration of the effectiveness of zero-shot and supervised methods for performance improvement."], "limitations": "", "keywords": ["Multi-label classification", "Large Language Models", "Autoregressive Models", "Finetuning", "Distribution Alignment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17536", "pdf": "https://arxiv.org/pdf/2505.17536.pdf", "abs": "https://arxiv.org/abs/2505.17536", "title": "Multimodal Conversation Structure Understanding", "authors": ["Kent K. Chang", "Mackenzie Hanh Cramer", "Anna Ho", "Ti Ti Nguyen", "Yilin Yuan", "David Bamman"], "categories": ["cs.CL"], "comment": null, "summary": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure.", "AI": {"tldr": "This paper introduces tasks for understanding conversational structure in multi-modal, multi-party settings and evaluates the performance of LLMs on these tasks using a human-annotated dataset.", "motivation": "To explore the underdeveloped understanding of fine-grained conversational structures in dialogues, particularly within multi-modal, multi-party environments.", "method": "A suite of tasks focused on conversational role attribution and conversation threading is introduced, supported by a human-annotated dataset of 4,398 annotations for speaker roles, 5,755 for addressees, and 3,142 for side-participants.", "result": "Evaluation of various audio-visual LLMs revealed that while one outperformed vision-language models in recognizing speakers and addressees, performance dropped significantly with anonymization of participants; number of participants negatively impacted role-attribution accuracy.", "conclusion": "The findings highlight the challenges in multimodal conversational structure understanding and suggest avenues for future development in LLMs to enhance reasoning about conversational dynamics.", "key_contributions": ["Introduction of a suite of tasks for conversational role attribution and threading.", "Presentation of a human-annotated dataset for evaluating LLMs in multitask conversations.", "Insight into the factors affecting performance in role-attribution tasks."], "limitations": "The study primarily focuses on audio-visual LLMs and may not adequately represent other models; significant performance drops with anonymized participants may limit practical applications.", "keywords": ["multimodal LLMs", "conversational role attribution", "conversation threading", "dialogue analysis", "sociolinguistics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.17537", "pdf": "https://arxiv.org/pdf/2505.17537.pdf", "abs": "https://arxiv.org/abs/2505.17537", "title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often fail to recognize their knowledge\nboundaries, producing confident yet incorrect answers. In this paper, we\ninvestigate how knowledge popularity affects LLMs' ability to perceive their\nknowledge boundaries. Focusing on entity-centric factual question answering\n(QA), we quantify knowledge popularity from three perspectives: the popularity\nof entities in the question, the popularity of entities in the answer, and\nrelation popularity, defined as their co-occurrence frequency. Experiments on\nthree representative datasets containing knowledge with varying popularity show\nthat LLMs exhibit better QA performance, higher confidence, and more accurate\nperception on more popular knowledge, with relation popularity having the\nstrongest correlation. Cause knowledge popularity shows strong correlation with\nLLMs' QA performance, we propose to leverage these signals for confidence\ncalibration. This improves the accuracy of answer correctness prediction by an\naverage of 5.24% across all models and datasets. Furthermore, we explore\nprompting LLMs to estimate popularity without external corpora, which yields a\nviable alternative.", "AI": {"tldr": "The paper investigates how knowledge popularity impacts large language models' (LLMs) ability to recognize their knowledge boundaries and proposes a method for improving answer correctness prediction.", "motivation": "Understanding LLMs' perception of their knowledge boundaries is critical to improving their reliability, especially in factual question answering.", "method": "The study quantifies knowledge popularity through entity popularity in questions and answers, as well as relation popularity based on co-occurrence frequency, and tests these factors on three datasets.", "result": "LLMs demonstrate improved QA performance and confidence when dealing with more popular knowledge, especially in relation to relation popularity. The proposed calibration method enhances prediction accuracy of answer correctness by 5.24%.", "conclusion": "Enhancing LLMs’ ability to recognize knowledge boundaries through knowledge popularity can significantly improve performance and answer correctness predictions.", "key_contributions": ["Quantifies knowledge popularity from multiple perspectives for LLMs.", "Demonstrates correlation between knowledge popularity and LLMs' QA performance.", "Proposes a method for confidence calibration based on popularity signals."], "limitations": "", "keywords": ["large language models", "knowledge boundaries", "factual question answering", "knowledge popularity", "confidence calibration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17538", "pdf": "https://arxiv.org/pdf/2505.17538.pdf", "abs": "https://arxiv.org/abs/2505.17538", "title": "Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition", "authors": ["Leonora Vesterbacka", "Faton Rekathati", "Robin Kurtz", "Justyna Sikora", "Agnes Toftgård"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This work presents a suite of fine-tuned Whisper models for Swedish, trained\non a dataset of unprecedented size and variability for this mid-resourced\nlanguage. As languages of smaller sizes are often underrepresented in\nmultilingual training datasets, substantial improvements in performance can be\nachieved by fine-tuning existing multilingual models, as shown in this work.\nThis work reports an overall improvement across model sizes compared to\nOpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%\nreduction in WER comparing our best performing model to OpenAI's\nwhisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.", "AI": {"tldr": "This paper introduces fine-tuned Whisper models for Swedish, achieving significant performance improvements over OpenAI's Whisper models.", "motivation": "To address underrepresentation of mid-resourced languages like Swedish in multilingual training datasets.", "method": "Fine-tuned Whisper models evaluated on a large dataset for Swedish, compared against OpenAI's Whisper models.", "result": "The best model achieved a 47% reduction in word error rate (WER) compared to OpenAI's whisper-large-v3 across multiple datasets.", "conclusion": "Fine-tuning existing multilingual models significantly enhances performance for underrepresented languages like Swedish.", "key_contributions": ["Development of a suite of fine-tuned Whisper models for Swedish", "Demonstrated substantial WER reduction compared to existing models", "Utilized a large and varied dataset for training"], "limitations": "", "keywords": ["Whisper models", "Swedish language", "machine learning", "natural language processing", "word error rate"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.17558", "pdf": "https://arxiv.org/pdf/2505.17558.pdf", "abs": "https://arxiv.org/abs/2505.17558", "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection", "authors": ["Shrey Pandit", "Ashwin Vinod", "Liu Leqi", "Ying Ding"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and dataset are available at https://teachingwithlies.github.io/", "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.", "AI": {"tldr": "The paper presents HaluCheck, a method to improve large language models' accuracy in detecting hallucinations using a curriculum learning strategy with engineered negative samples.", "motivation": "Aligning large language models to detect hallucinations is challenging due to the sophisticated nature of hallucinated text.", "method": "The approach utilizes a curriculum learning strategy that starts training with easier samples from independent fact-checking models, gradually introducing harder samples.", "result": "HaluCheck models show significant performance improvements of up to 24% on benchmarks like MedHallu and HaluEval, demonstrating robustness in zero-shot settings.", "conclusion": "The curriculum DPO approach enhances model performance and stability in hallucination detection tasks.", "key_contributions": ["Introduction of HaluCheck for hallucination detection", "Use of curriculum learning with high-quality negative samples", "Demonstrated performance improvements across difficult benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Hallucination Detection", "Curriculum Learning", "Negative Examples", "Robustness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17565", "pdf": "https://arxiv.org/pdf/2505.17565.pdf", "abs": "https://arxiv.org/abs/2505.17565", "title": "PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "categories": ["cs.CL"], "comment": null, "summary": "Improving large language models (LLMs) with self-generated data has\ndemonstrated success in tasks such as mathematical reasoning and code\ngeneration. Yet, no exploration has been made on table question answering\n(TQA), where a system answers questions based on tabular data. Addressing this\ngap is crucial for TQA, as effective self-improvement can boost performance\nwithout requiring costly or manually annotated data. In this work, we propose\nPPT, a Process-based Preference learning framework for TQA. It decomposes\nreasoning chains into discrete states, assigns scores to each state, and\nsamples contrastive steps for preference learning. Experimental results show\nthat PPT effectively improves TQA models by up to 5% on in-domain datasets and\n2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,\nthe resulting models achieve competitive results compared to more complex and\nlarger state-of-the-art TQA systems, while being five times more efficient\nduring inference.", "AI": {"tldr": "This paper introduces PPT, a novel framework for improving table question answering (TQA) using self-generated data and preference learning.", "motivation": "The paper addresses the lack of exploration in improving LLMs for table question answering, highlighting the importance of self-improvement in TQA to enhance performance without needing costly annotated data.", "method": "The proposed PPT framework decomposes reasoning chains in TQA into discrete states, assigns scores, and samples contrastive steps for preference learning.", "result": "Experimental results indicate that PPT improves TQA model performance by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets using only 8,000 preference pairs.", "conclusion": "The models improved by PPT demonstrate competitive performance compared to larger state-of-the-art TQA systems while being five times more efficient in inference.", "key_contributions": ["Introduction of the PPT framework for TQA.", "Demonstrated efficiency and effectiveness in using self-generated data for improvement.", "Achieved competitive results with fewer resources compared to existing models."], "limitations": "", "keywords": ["table question answering", "large language models", "preference learning", "self-generated data", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17571", "pdf": "https://arxiv.org/pdf/2505.17571.pdf", "abs": "https://arxiv.org/abs/2505.17571", "title": "Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation", "authors": ["Sichun Luo", "Guanzhi Deng", "Jian Xu", "Xiaojie Zhang", "Hanxu Hou", "Linqi Song"], "categories": ["cs.CL"], "comment": null, "summary": "Personalization is a critical task in modern intelligent systems, with\napplications spanning diverse domains, including interactions with large\nlanguage models (LLMs). Recent advances in reasoning capabilities have\nsignificantly enhanced LLMs, enabling unprecedented performance in tasks such\nas mathematics and coding. However, their potential for personalization tasks\nremains underexplored.\n  In this paper, we present the first systematic evaluation of large reasoning\nmodels (LRMs) for personalization tasks. Surprisingly, despite generating more\ntokens, LRMs do not consistently outperform general-purpose LLMs, especially in\nretrieval-intensive scenarios where their advantages diminish. Our analysis\nidentifies three key limitations: divergent thinking, misalignment of response\nformats, and ineffective use of retrieved information. To address these\nchallenges, we propose Reinforced Reasoning for Personalization (\\model), a\nnovel framework that incorporates a hierarchical reasoning thought template to\nguide LRMs in generating structured outputs. Additionally, we introduce a\nreasoning process intervention method to enforce adherence to designed\nreasoning patterns, enhancing alignment. We also propose a cross-referencing\nmechanism to ensure consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing techniques.", "AI": {"tldr": "This paper systematically evaluates large reasoning models (LRMs) for personalization tasks and proposes a new framework to improve their efficacy.", "motivation": "The paper addresses the underexplored potential of LLMs for personalization tasks despite their advanced reasoning capabilities.", "method": "The proposed framework, Reinforced Reasoning for Personalization, integrates a hierarchical reasoning thought template and a reasoning process intervention method for better structured outputs and adherence to reasoning patterns.", "result": "Extensive experiments show that the proposed approach significantly outperforms existing techniques in personalization tasks.", "conclusion": "The study highlights the limitations of LRMs in retrieval-intensive scenarios and suggests enhancements to improve personalization outcomes.", "key_contributions": ["Systematic evaluation of LRMs for personalization tasks", "Introduction of the Reinforced Reasoning for Personalization framework", "Proposed cross-referencing mechanism to ensure output consistency"], "limitations": "Identified limitations include divergent thinking, misalignment of response formats, and ineffective use of retrieved information.", "keywords": ["personalization", "large reasoning models", "reinforced reasoning", "human-computer interaction", "LLM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17601", "pdf": "https://arxiv.org/pdf/2505.17601.pdf", "abs": "https://arxiv.org/abs/2505.17601", "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "authors": ["Jiawei Kong", "Hao Fang", "Xiaochen Yang", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o.", "AI": {"tldr": "The paper presents a novel backdoor attack targeting large language models (LLMs) that uses benign prefixes in responses to inject harmful content while evading safety measures.", "motivation": "To address the limitations of existing backdoor attacks on LLMs that are easily detectable or compromise model safety alignment.", "method": "A clean-data backdoor attack is proposed, which employs harmless question-answer pairs to overfit benign-sounding prefixes, using gradient-based optimization to strengthen the universal trigger.", "result": "The method demonstrated a successful attack with high attack success rates of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B models, respectively, even against safety-aligned guardrail models.", "conclusion": "This approach shows promise in effectively leveraging benign interactions to compromise LLMs, posing a significant threat to model safety.", "key_contributions": ["Introduction of a clean-data backdoor attack tailored for LLMs.", "Optimization technique for enhancing the attack's efficacy.", "Demonstration of high attack success rates against models with safety measures."], "limitations": "Potential ethical implications of employing such attacks and the need for further research on defense mechanisms.", "keywords": ["backdoor attack", "large language models", "clean-data", "safety alignment", "gradient-based optimization"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.17612", "pdf": "https://arxiv.org/pdf/2505.17612.pdf", "abs": "https://arxiv.org/abs/2505.17612", "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint, v1", "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.", "AI": {"tldr": "This paper introduces Agent Distillation, a framework for transferring reasoning and task-solving behaviors from large language models to smaller models while addressing limitations in factual knowledge and computation accuracy.", "motivation": "To improve the practical deployment of large language models by distilling their reasoning capabilities into smaller, more efficient models that can still perform effectively on complex tasks.", "method": "The authors propose a framework called Agent Distillation, utilizing a prompting method known as first-thought prefix to enhance teacher trajectory quality and a self-consistent action generation to improve robustness during testing.", "result": "The evaluation on eight reasoning tasks shows that small models (sLMs) with as few as 0.5B parameters can achieve competitive performance compared to larger models, demonstrating the effectiveness of the proposed agent distillation method.", "conclusion": "Agent distillation shows promise in developing practical small agents capable of utilizing retrieval and coding tools effectively, overcoming some of the limitations of traditional distillation techniques.", "key_contributions": ["Introduction of the Agent Distillation framework.", "Development of first-thought prefix prompting method.", "Proposal of self-consistent action generation for small agents."], "limitations": "The framework may still struggle with scenarios requiring extensive rare factual knowledge or precise computation.", "keywords": ["Language Models", "Distillation", "Agent Behavior", "Reasoning", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17616", "pdf": "https://arxiv.org/pdf/2505.17616.pdf", "abs": "https://arxiv.org/abs/2505.17616", "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments", "authors": ["Qingyu Lu", "Liang Ding", "Siyi Cao", "Xuebo Liu", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Agents powered by large language models (LLMs) have demonstrated strong\nplanning and decision-making capabilities in complex embodied environments.\nHowever, such agents often suffer from inefficiencies in multi-turn\ninteractions, frequently trapped in repetitive loops or issuing ineffective\ncommands, leading to redundant computational overhead. Instead of relying\nsolely on learning from trajectories, we take a first step toward exploring the\nearly-exit behavior for LLM-based agents. We propose two complementary\napproaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions\nduring generation, and 2. an $\\textbf{extrinsic}$ method that verifies task\ncompletion to determine when to halt an agent's trial. To evaluate early-exit\nmechanisms, we introduce two metrics: one measures the reduction of\n$\\textbf{redundant steps}$ as a positive effect, and the other evaluates\n$\\textbf{progress degradation}$ as a negative effect. Experiments with 4\ndifferent LLMs across 5 embodied environments show significant efficiency\nimprovements, with only minor drops in agent performance. We also validate a\npractical strategy where a stronger agent assists after an early-exit agent,\nachieving better performance with the same total steps. We will release our\ncode to support further research.", "AI": {"tldr": "This paper presents early-exit mechanisms for LLM-based agents to improve efficiency in multi-turn interactions by reducing redundant commands and computational overhead.", "motivation": "To address inefficiencies in multi-turn interactions of LLM-based agents that can lead to repetitive and ineffective command execution.", "method": "Introduce intrinsic and extrinsic methods for early exit, with metrics to evaluate reduction of redundant steps and progress degradation.", "result": "Experiments with 4 LLMs across 5 environments exhibited significant efficiency improvements with minor drops in performance.", "conclusion": "Implementing early-exit mechanisms can enhance the efficiency of LLM-based agents in embodied environments without greatly harming performance.", "key_contributions": ["Two novel early-exit methods for LLM-based agents", "Evaluation metrics for measuring efficiency and degradation", "A practical strategy for combining early-exit agents with stronger agents"], "limitations": "", "keywords": ["Large Language Models", "Early Exit Mechanisms", "Efficiency", "Multi-turn Interactions", "Embodied Agents"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17625", "pdf": "https://arxiv.org/pdf/2505.17625.pdf", "abs": "https://arxiv.org/abs/2505.17625", "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports", "authors": ["Hayato Aida", "Kosuke Takahashi", "Takahiro Omi"], "categories": ["cs.CL", "cs.CV", "68T50", "I.2"], "comment": "Accepted at IIAI AAI 2025, the 3rd International Conference on\n  Computational and Data Sciences in Economics and Finance", "summary": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats.", "AI": {"tldr": "This paper proposes a method to improve table understanding in Large Vision-Language Models (LVLMs) by incorporating in-table textual content and layout features, addressing challenges related to multimodal LLMs in accurately interpreting complex document structures.", "motivation": "With advancements in Large Language Models, understanding table structures is crucial for financial domains, especially for accurate question answering over diverse table formats.", "method": "The study enhances LVLM-based table understanding by integrating in-table textual content and layout features into the analysis process.", "result": "Experimental results show significant performance improvement in interpreting complex document layouts when auxiliary modalities are used.", "conclusion": "Incorporating additional textual and layout information enhances the LVLM's capability, allowing for better interpretation without the need for structured input formats.", "key_contributions": ["Proposed method of integrating in-table textual features into LVLMs", "Demonstrated significant performance gains in table understanding", "Addressed challenges in interpreting complex document layouts"], "limitations": "", "keywords": ["Large Language Models", "table understanding", "multimodal LLMs", "documents", "layout features"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17630", "pdf": "https://arxiv.org/pdf/2505.17630.pdf", "abs": "https://arxiv.org/abs/2505.17630", "title": "GIM: Improved Interpretability for Large Language Models", "authors": ["Joakim Edin", "Róbert Csordás", "Tuukka Ruotsalo", "Zhengxuan Wu", "Maria Maistro", "Jing Huang", "Lars Maaløe"], "categories": ["cs.CL", "cs.LG", "68T07", "I.2.0; I.2.7"], "comment": null, "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.", "AI": {"tldr": "This paper introduces Gradient Interaction Modifications (GIM) to enhance the interpretability of large language models (LLMs) by addressing the self-repair phenomenon in attention mechanisms.", "motivation": "The need for trustworthy AI necessitates faithful interpretability in large language models, which is obstructed by the self-repair phenomenon that masks component significance.", "method": "Gradient Interaction Modifications (GIM) is introduced to account for self-repair during backpropagation, improving interpretability in LLMs.", "result": "GIM significantly improves the faithfulness of circuit identification and feature attribution methods across various large language models and tasks.", "conclusion": "Enhancing interpretability through GIM is crucial for understanding LLM mechanisms, thereby promoting their improvement and safety.", "key_contributions": ["Introduction of GIM to address self-repair in attention mechanisms", "Demonstration of GIM's effectiveness across multiple LLMs and tasks", "Contribution to the understanding of LLM interpretability and safety"], "limitations": "", "keywords": ["large language models", "interpretability", "self-repair", "attention mechanism", "Gradient Interaction Modifications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17642", "pdf": "https://arxiv.org/pdf/2505.17642.pdf", "abs": "https://arxiv.org/abs/2505.17642", "title": "Stereotype Detection in Natural Language Processing", "authors": ["Alessandra Teresa Cignarella", "Anastasia Giachanou", "Els Lefever"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Stereotypes influence social perceptions and can escalate into discrimination\nand violence. While NLP research has extensively addressed gender bias and hate\nspeech, stereotype detection remains an emerging field with significant\nsocietal implications. In this work is presented a survey of existing research,\nanalyzing definitions from psychology, sociology, and philosophy. A\nsemi-automatic literature review was performed by using Semantic Scholar. We\nretrieved and filtered over 6,000 papers (in the year range 2000-2025),\nidentifying key trends, methodologies, challenges and future directions. The\nfindings emphasize stereotype detection as a potential early-monitoring tool to\nprevent bias escalation and the rise of hate speech. Conclusions highlight the\nneed for a broader, multilingual, and intersectional approach in NLP studies.", "AI": {"tldr": "This paper surveys existing research on stereotype detection, analyzing definitions and methodologies in the context of NLP, highlighting its potential to prevent bias escalation.", "motivation": "To understand the role of stereotypes in social perceptions and their escalation into discrimination and violence, and to address the underexplored area of stereotype detection in NLP.", "method": "A semi-automatic literature review of over 6,000 papers from 2000 to 2025 was conducted using Semantic Scholar to identify trends and methodologies.", "result": "Key trends in stereotype detection methodologies were identified, along with challenges and future directions for research, emphasizing the effectiveness of stereotype detection in preventing bias and hate speech.", "conclusion": "A broader, multilingual, and intersectional approach in NLP studies is necessary to effectively address stereotype detection.", "key_contributions": ["Comprehensive survey of existing research on stereotype detection across disciplines.", "Identification of key trends and methodologies in the field.", "Recommendations for future research directions in stereotype detection."], "limitations": "", "keywords": ["stereotype detection", "natural language processing", "bias", "hate speech", "multilingual approach"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.17643", "pdf": "https://arxiv.org/pdf/2505.17643.pdf", "abs": "https://arxiv.org/abs/2505.17643", "title": "Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks", "authors": ["Sara Ketabi", "Dhanesh Ramachandram"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conventional machine learning models, particularly tree-based approaches,\nhave demonstrated promising performance across various clinical prediction\ntasks using electronic health record (EHR) data. Despite their strengths, these\nmodels struggle with tasks that require deeper contextual understanding, such\nas predicting 30-day hospital readmission. This can be primarily due to the\nlimited semantic information available in structured EHR data. To address this\nlimitation, we propose a deep multimodal contrastive learning (CL) framework\nthat aligns the latent representations of structured EHR data with unstructured\ndischarge summary notes. It works by pulling together paired EHR and text\nembeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR\nencoder extracted from this framework significantly boosts downstream task\nperformance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission\nprediction. Such results demonstrate the effect of integrating domain knowledge\nfrom clinical notes into EHR-based pipelines, enabling more accurate and\ncontext-aware clinical decision support systems.", "AI": {"tldr": "Proposes a multimodal contrastive learning framework to enhance clinical predictions from EHR by integrating unstructured discharge summaries.", "motivation": "Conventional ML models struggle with tasks requiring contextual understanding in clinical settings, particularly with structured EHR data.", "method": "Developed a deep multimodal contrastive learning framework that aligns structured EHR data with unstructured discharge summary text.", "result": "Fine-tuning the pretrained EHR encoder led to a 4.1% AUROC improvement over XGBoost in predicting 30-day hospital readmissions.", "conclusion": "Integrating clinical notes into EHR pipelines improves accuracy and context-awareness in clinical decision support systems.", "key_contributions": ["Introduction of a multimodal contrastive learning framework for EHR data and clinical notes.", "Demonstration of improved performance in clinical predictions using the proposed model.", "Highlighting the importance of unstructured data in enhancing machine learning outcomes for health informatics."], "limitations": "", "keywords": ["machine learning", "electronic health records", "contrastive learning", "discharge summaries", "clinical prediction"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17654", "pdf": "https://arxiv.org/pdf/2505.17654.pdf", "abs": "https://arxiv.org/abs/2505.17654", "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications", "authors": ["Ancheng Xu", "Zhihao Yang", "Jingpeng Li", "Guanghu Yuan", "Longze Chen", "Liang Yan", "Jiehui Zhou", "Zhen Qin", "Hengyun Chang", "Hamid Alinejad-Rokny", "Bo Zheng", "Min Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.", "AI": {"tldr": "The paper introduces EVADE, a benchmark for evaluating the detection of evasive content in e-commerce using LLMs and VLMs.", "motivation": "E-commerce platforms are challenged by evasive content that appears compliant with policies yet contains prohibited claims, requiring better evaluation benchmarks for detection.", "method": "The study introduces the EVADE benchmark with 2,833 annotated text samples and 13,961 images across six categories, assessing models through Single-Violation and All-in-One tasks to evaluate fine-grained and long-context reasoning, respectively.", "result": "Benchmarking 26 LLMs and VLMs revealed significant performance gaps, indicating that even advanced models struggle with evasive samples; clearer rule definitions enhance model alignment with human judgment.", "conclusion": "EVADE sets a new standard for detecting evasive content in e-commerce, highlighting limitations in current models while promoting safer content moderation.", "key_contributions": ["Introduction of the EVADE benchmark for evasive content detection", "Evaluation of 26 mainstream LLMs and VLMs on this new benchmark", "Insight into the impact of clearer rule definitions on model performance"], "limitations": "The benchmark focuses on specific product categories and may not generalize to other content types or languages.", "keywords": ["E-commerce", "Language Models", "Evasive Content", "Content Moderation", "Benchmarking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17656", "pdf": "https://arxiv.org/pdf/2505.17656.pdf", "abs": "https://arxiv.org/abs/2505.17656", "title": "Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs", "authors": ["Hexiang Tan", "Fei Sun", "Sha Liu", "Du Su", "Qi Cao", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Yuanzhuo Wang", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Underreview in EMNLP25", "summary": "As large language models (LLMs) often generate plausible but incorrect\ncontent, error detection has become increasingly critical to ensure\ntruthfulness. However, existing detection methods often overlook a critical\nproblem we term as self-consistent error, where LLMs repeatly generate the same\nincorrect response across multiple stochastic samples. This work formally\ndefines self-consistent errors and evaluates mainstream detection methods on\nthem. Our investigation reveals two key findings: (1) Unlike inconsistent\nerrors, whose frequency diminishes significantly as LLM scale increases, the\nfrequency of self-consistent errors remains stable or even increases. (2) All\nfour types of detection methshods significantly struggle to detect\nself-consistent errors. These findings reveal critical limitations in current\ndetection methods and underscore the need for improved methods. Motivated by\nthe observation that self-consistent errors often differ across LLMs, we\npropose a simple but effective cross-model probe method that fuses hidden state\nevidence from an external verifier LLM. Our method significantly enhances\nperformance on self-consistent errors across three LLM families.", "AI": {"tldr": "This paper addresses the issue of self-consistent errors in large language models, proposing a new detection method that significantly improves error detection performance.", "motivation": "Large language models often generate plausible but incorrect content, making error detection critical for ensuring truthfulness. The paper focuses on self-consistent errors, which current detection methods struggle to identify.", "method": "The authors define self-consistent errors and evaluate existing detection methods on them. They propose a cross-model probe method that utilizes hidden state evidence from an external verifier LLM to enhance performance.", "result": "The proposed method significantly improves the detection of self-consistent errors across three different LLM families compared to existing methods.", "conclusion": "The findings reveal limitations in current error detection methods and emphasize the need for improved strategies to address self-consistent errors in large language models.", "key_contributions": ["Definition and evaluation of self-consistent errors in LLMs", "A new detection method using cross-model probing", "Demonstration of the effectiveness across multiple LLM families"], "limitations": "", "keywords": ["large language models", "error detection", "self-consistent errors", "cross-model probing", "truthfulness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17663", "pdf": "https://arxiv.org/pdf/2505.17663.pdf", "abs": "https://arxiv.org/abs/2505.17663", "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States", "authors": ["Yang Xiao", "Jiashuo Wang", "Qiancheng Xu", "Changhe Song", "Chunpu Xu", "Yi Cheng", "Wenjie Li", "Pengfei Liu"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present \\textsc{DynToM}, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.", "AI": {"tldr": "This paper introduces DynToM, a new benchmark for evaluating the Theory of Mind capabilities of Large Language Models (LLMs) over time, showing significant performance gaps compared to humans.", "motivation": "To assess LLMs’ ability to track the dynamic progression of mental states in social interactions, which is crucial for human-AI interaction.", "method": "The authors created a benchmark, DynToM, through a systematic four-step framework, generating 1,100 social contexts and validating 5,500 scenarios and 78,100 questions for quality and realism.", "result": "Evaluation of ten state-of-the-art LLMs shows they fall short by 44.7% compared to human performance, especially in tracking shifts in mental states.", "conclusion": "The significant performance gap underscores the limitations of current LLMs in modeling the dynamic nature of human mental states, pointing to crucial areas for improvement.", "key_contributions": ["Introduction of the DynToM benchmark for evaluating ToM in LLMs", "Generation of a large dataset of social contexts and scenarios for benchmark testing", "Comprehensive evaluation revealing performance gaps between LLMs and human capabilities"], "limitations": "Focus on the static aspects of mental state tracking; further work needed to enhance LLM capabilities in dynamic contexts.", "keywords": ["Theory of Mind", "Large Language Models", "Human-AI Interaction", "Benchmark", "Mental States"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17667", "pdf": "https://arxiv.org/pdf/2505.17667.pdf", "abs": "https://arxiv.org/abs/2505.17667", "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning", "authors": ["Fanqi Wan", "Weizhou Shen", "Shengyi Liao", "Yingcheng Shi", "Chenliang Li", "Ziyi Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.", "AI": {"tldr": "This paper introduces QwenLong-L1, a framework for extending large reasoning models (LRMs) to effectively handle long-context inputs, addressing challenges in training efficiency and optimization stability.", "motivation": "To tackle the challenges of reasoning with long-context inputs in large reasoning models, which have traditionally excelled in short-context scenarios.", "method": "The paper proposes a framework called QwenLong-L1, which includes a warm-up supervised fine-tuning stage to establish a solid starting policy, followed by a curriculum-guided phased reinforcement learning technique and a difficulty-aware retrospective sampling strategy.", "result": "QwenLong-L1-32B demonstrates superior performance on seven long-context document question-answering benchmarks, outperforming major LRMs like OpenAI-o3-mini and achieving results comparable to Claude-3.7-Sonnet-Thinking.", "conclusion": "The proposed approach significantly advances the capability of long-context LRMs in performing robust reasoning in information-rich environments.", "key_contributions": ["Introduction of the long-context reasoning RL paradigm", "Development of the QwenLong-L1 framework", "Achievement of state-of-the-art performance on long-context reasoning tasks"], "limitations": "", "keywords": ["long-context reasoning", "reinforcement learning", "large reasoning models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.17671", "pdf": "https://arxiv.org/pdf/2505.17671.pdf", "abs": "https://arxiv.org/abs/2505.17671", "title": "MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis", "authors": ["Yilun Liu", "Chunguang Zhao", "Xinhua Yang", "Hongyong Zeng", "Shimin Tao", "Weibin Meng", "Minggui He", "Chang Su", "Yan Yu", "Hongxia Ma", "Li Zhang", "Daimeng Wei", "Hao Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite doubts on data quality, instruction synthesis has been widely applied\ninto instruction tuning (IT) of LLMs as an economic and rapid alternative.\nRecent endeavors focus on improving data quality for synthesized instruction\npairs in English and have facilitated IT of English-centric LLMs. However, data\nquality issues in multilingual synthesized instruction pairs are even more\nsevere, since the common synthesizing practice is to translate English\nsynthesized data into other languages using machine translation (MT). Besides\nthe known content errors in these English synthesized data, multilingual\nsynthesized instruction data are further exposed to defects introduced by MT\nand face insufficient localization of the target languages. In this paper, we\npropose MIDB, a Multilingual Instruction Data Booster to automatically address\nthe quality issues in multilingual synthesized data. MIDB is trained on around\n36.8k revision examples across 16 languages by human linguistic experts,\nthereby can boost the low-quality data by addressing content errors and MT\ndefects, and improving localization in these synthesized data. Both automatic\nand human evaluation indicate that not only MIDB steadily improved instruction\ndata quality in 16 languages, but also the instruction-following and\ncultural-understanding abilities of multilingual LLMs fine-tuned on\nMIDB-boosted data were significantly enhanced.", "AI": {"tldr": "The paper presents MIDB, a system designed to enhance the quality of multilingual synthesized instruction data used for instruction tuning of LLMs by correcting content and machine translation errors.", "motivation": "To improve the quality of multilingual synthesized instruction pairs, which suffer from defects in content and insufficient localization when using machine translation from English.", "method": "MIDB is trained on 36.8k revision examples across 16 languages, developed by human linguistic experts to boost the quality of instruction data by addressing errors and localization issues.", "result": "MIDB improved instruction data quality across 16 languages and significantly enhanced the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on the boosted data.", "conclusion": "The proposed MIDB method successfully mitigates quality issues in multilingual synthetic instruction data, leading to better performance of LLMs in multilingual contexts.", "key_contributions": ["Introduction of MIDB for enhancing multilingual instruction data quality", "Demonstrated effectiveness across 16 languages", "Significant improvements in multilingual LLMs' instruction-following and cultural understanding abilities."], "limitations": "The results may vary based on the quality of the initial synthesized data and the effectiveness of machine translations used.", "keywords": ["instruction synthesis", "multilingual instruction data", "language models", "data quality", "machine translation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17682", "pdf": "https://arxiv.org/pdf/2505.17682.pdf", "abs": "https://arxiv.org/abs/2505.17682", "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors", "authors": ["Fanjin Meng", "Jingtao Ding", "Jiahui Gong", "Chen Yang", "Hong Chen", "Zuojian Wang", "Haisheng Lu", "Yong Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples.", "AI": {"tldr": "BehaviorLM is a progressive fine-tuning approach for LLMs that improves predictions of long-tailed user behaviors while maintaining anchor performance.", "motivation": "To enhance user behavior prediction in intelligent assistants by addressing the limitations of deep learning models in capturing long-tailed behaviors.", "method": "BehaviorLM consists of two fine-tuning stages: first, tuning on frequent behaviors while preserving general knowledge, and second, using a balanced subset of behaviors based on sample difficulty to improve predictions for less common behaviors.", "result": "Experimental results show that BehaviorLM successfully predicts both frequent and rare user behaviors, effectively leveraging LLMs' behavioral knowledge in few-shot contexts.", "conclusion": "BehaviorLM provides a robust framework for improving the prediction of diverse user behaviors in intelligent assistant services.", "key_contributions": ["Introduction of a two-stage fine-tuning approach for LLMs", "Improved prediction of long-tailed behaviors without compromising anchor behavior performance", "Demonstrated effectiveness on real-world datasets"], "limitations": "", "keywords": ["user behavior prediction", "large language models", "fine-tuning", "long-tailed behaviors", "intelligent assistants"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17691", "pdf": "https://arxiv.org/pdf/2505.17691.pdf", "abs": "https://arxiv.org/abs/2505.17691", "title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction", "authors": ["Yan Yu", "Yilun Liu", "Minggui He", "Shimin Tao", "Weibin Meng", "Xinhua Yang", "Li Zhang", "Hongxia Ma", "Chang Su", "Hao Yang", "Fuliang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used as evaluators for open-ended\ntasks, while previous research has emphasized biases in LLM evaluations, the\nissue of non-transitivity in pairwise comparisons remains unresolved:\nnon-transitive preferences for pairwise comparisons, where evaluators prefer A\nover B, B over C, but C over A. Our results suggest that low-quality training\ndata may reduce the transitivity of preferences generated by the Evaluator LLM.\nTo address this, We propose a graph-theoretic framework to analyze and mitigate\nthis problem by modeling pairwise preferences as tournament graphs. We quantify\nnon-transitivity and introduce directed graph structural entropy to measure the\noverall clarity of preferences. Our analysis reveals significant\nnon-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting\n67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting\nlow overall clarity of preferences. To address this issue, we designed a\nfiltering strategy, ELSPR, to eliminate preference data that induces\nnon-transitivity, retaining only consistent and transitive preference data for\nmodel fine-tuning. Experiments demonstrate that models fine-tuned with filtered\ndata reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease\nstructural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely\nwith human evaluators (human agreement rate improves by 0.6% and Spearman\ncorrelation increases by 0.01).", "AI": {"tldr": "This study addresses non-transitive preferences in LLM evaluations by proposing a graph-theoretic framework to analyze and mitigate this issue, demonstrating improved results through filtered training data.", "motivation": "To resolve the issue of non-transitivity in pairwise comparisons made by Evaluator LLMs, which can lead to inconsistent evaluation outcomes.", "method": "We model pairwise preferences as tournament graphs, quantifying non-transitivity and measuring preference clarity using directed graph structural entropy. A filtering strategy, ELSPR, is introduced to retain only transitive preference data for model fine-tuning.", "result": "Fine-tuning models with filtered data led to a reduction of non-transitivity by 13.78%, decreased structural entropy, and improved alignment with human evaluators.", "conclusion": "The proposed framework and filtering strategy effectively address non-transitivity in LLM evaluations, enhancing the reliability of evaluative outcomes.", "key_contributions": ["Introduction of a graph-theoretic framework for analyzing preferences in LLM evaluations", "Quantification of non-transitivity and clarity using directed graph structural entropy", "Development of a filtering strategy (ELSPR) that enhances model performance by improving preference consistency"], "limitations": "", "keywords": ["large language models", "pairwise comparisons", "non-transitivity", "graph theory", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17697", "pdf": "https://arxiv.org/pdf/2505.17697.pdf", "abs": "https://arxiv.org/abs/2505.17697", "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "authors": ["Zekai Zhao", "Qi Liu", "Kun Zhou", "Zihan Liu", "Yifei Shao", "Zhiting Hu", "Biwei Huang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.", "AI": {"tldr": "A method to amplify key activations in LLMs to enhance long-form reasoning without extensive training.", "motivation": "To investigate and control the internal mechanisms that govern long-chain-of-thought (CoT) reasoning in large language models, allowing improved performance with minimal training.", "method": "The paper introduces a training-free activation control technique that amplifies high-impact activations and utilizes 'wait' tokens to enhance reasoning capabilities. It also proposes a parameter-efficient fine-tuning approach that modifies only the last-layer activations and a few additional parameters.", "result": "The proposed method significantly increases self-reflection rates and accuracy in reasoning tasks while requiring fewer parameters compared to traditional training methods.", "conclusion": "By identifying and modulating key activations at inference time, it is possible to elicit long CoT reasoning in LLMs efficiently, demonstrating strong performance improvements on reasoning benchmarks with reduced training.", "key_contributions": ["Training-free activation control technique for LLMs.", "Parameter-efficient fine-tuning method using a last-layer activation module.", "Demonstrated increased accuracy and self-reflection rates in long-form reasoning tasks."], "limitations": "", "keywords": ["large language models", "chain-of-thought reasoning", "activation control", "fine-tuning", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17704", "pdf": "https://arxiv.org/pdf/2505.17704.pdf", "abs": "https://arxiv.org/abs/2505.17704", "title": "SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus", "authors": ["Maria Ponomareva", "Maria Petrova", "Julia Detkova", "Oleg Serikov", "Maria Yarova"], "categories": ["cs.CL"], "comment": null, "summary": "The paper deals with elaborating different approaches to the machine\nprocessing of semantic sketches. It presents the pilot open corpus of semantic\nsketches. Different aspects of creating the sketches are discussed, as well as\nthe tasks that the sketches can help to solve. Special attention is paid to the\ncreation of the machine processing tools for the corpus. For this purpose, the\nSemSketches-2021 Shared Task was organized. The participants were given the\nanonymous sketches and a set of contexts containing the necessary predicates.\nDuring the Task, one had to assign the proper contexts to the corresponding\nsketches.", "AI": {"tldr": "This paper discusses various methods for machine processing of semantic sketches and presents a corpus created for this purpose, highlighting an associated shared task.", "motivation": "To explore and advance the capabilities of machine processing of semantic sketches and to create a suitable corpus for research.", "method": "The paper outlines the organization of the SemSketches-2021 Shared Task, where participants matched anonymous sketches with contextual predicates.", "result": "The pilot open corpus of semantic sketches was established, and the shared task facilitated progress in understanding the relationship between sketches and contexts.", "conclusion": "The paper underscores the potential applications of semantic sketches in various tasks and highlights the importance of developing tools for their machine processing.", "key_contributions": ["Introduction of a pilot open corpus of semantic sketches", "Description of the SemSketches-2021 Shared Task", "Development of machine processing tools for semantic sketches"], "limitations": "", "keywords": ["semantic sketches", "machine processing", "corpus", "shared task", "context"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.17712", "pdf": "https://arxiv.org/pdf/2505.17712.pdf", "abs": "https://arxiv.org/abs/2505.17712", "title": "Understanding How Value Neurons Shape the Generation of Specified Values in LLMs", "authors": ["Yi Su", "Jiayi Zhang", "Shu Yang", "Xinhai Wang", "Lijie Hu", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Rapid integration of large language models (LLMs) into societal applications\nhas intensified concerns about their alignment with universal ethical\nprinciples, as their internal value representations remain opaque despite\nbehavioral alignment advancements. Current approaches struggle to\nsystematically interpret how values are encoded in neural architectures,\nlimited by datasets that prioritize superficial judgments over mechanistic\nanalysis. We introduce ValueLocate, a mechanistic interpretability framework\ngrounded in the Schwartz Values Survey, to address this gap. Our method first\nconstructs ValueInsight, a dataset that operationalizes four dimensions of\nuniversal value through behavioral contexts in the real world. Leveraging this\ndataset, we develop a neuron identification method that calculates activation\ndifferences between opposing value aspects, enabling precise localization of\nvalue-critical neurons without relying on computationally intensive attribution\nmethods. Our proposed validation method demonstrates that targeted manipulation\nof these neurons effectively alters model value orientations, establishing\ncausal relationships between neurons and value representations. This work\nadvances the foundation for value alignment by bridging psychological value\nframeworks with neuron analysis in LLMs.", "AI": {"tldr": "ValueLocate is a framework for interpreting values encoded in large language models, using the Schwartz Values Survey to identify value-critical neurons.", "motivation": "To address concerns about the alignment of large language models (LLMs) with universal ethical principles by understanding how values are represented in their architecture.", "method": "ValueLocate constructs a dataset called ValueInsight that operationalizes four dimensions of universal value, facilitating neuron identification through activation differences related to these values.", "result": "The method reveals specific neurons that represent different value orientations and shows that manipulating these neurons can change the model's value outputs, establishing causal links.", "conclusion": "ValueLocate provides a foundation for improving value alignment in LLMs by connecting psychological principles with mechanistic neuron analysis.", "key_contributions": ["Introduction of ValueLocate framework for value interpretation in LLMs", "Development of ValueInsight dataset for operationalizing universal values", "Causal demonstration of neuron manipulation affecting model value representations"], "limitations": "", "keywords": ["large language models", "value alignment", "mechanistic interpretability", "neuron analysis", "Schwartz Values Survey"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17733", "pdf": "https://arxiv.org/pdf/2505.17733.pdf", "abs": "https://arxiv.org/abs/2505.17733", "title": "The Pilot Corpus of the English Semantic Sketches", "authors": ["Maria Petrova", "Maria Ponomareva", "Alexandra Ivoylova"], "categories": ["cs.CL"], "comment": null, "summary": "The paper is devoted to the creation of the semantic sketches for English\nverbs. The pilot corpus consists of the English-Russian sketch pairs and is\naimed to show what kind of contrastive studies the sketches help to conduct.\nSpecial attention is paid to the cross-language differences between the\nsketches with similar semantics. Moreover, we discuss the process of building a\nsemantic sketch, and analyse the mistakes that could give insight to the\nlinguistic nature of sketches.", "AI": {"tldr": "The paper discusses the creation of semantic sketches for English verbs, highlighting cross-language differences and the sketch-building process.", "motivation": "To explore contrastive studies using semantic sketches in linguistics, specifically focusing on English-Russian verb pairs.", "method": "The study involves building a pilot corpus of English-Russian sketch pairs and analyzing the construction process of these sketches as well as related mistakes.", "result": "The paper reveals insights into the linguistic nature of sketches and emphasizes differences between similar semantic sketches across languages.", "conclusion": "Semantic sketches can facilitate contrastive linguistic studies and help identify mistakes in understanding verb semantics across languages.", "key_contributions": ["Creation of a semantic sketches corpus for English-Russian verbs", "Insights into cross-language semantic differences", "Analysis of sketch construction mistakes"], "limitations": "", "keywords": ["semantic sketches", "English verbs", "contrastive linguistics", "cross-language differences"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2505.17746", "pdf": "https://arxiv.org/pdf/2505.17746.pdf", "abs": "https://arxiv.org/abs/2505.17746", "title": "Fast Quiet-STaR: Thinking Without Thought Tokens", "authors": ["Wei Huang", "Yizhe Xiong", "Xin Ye", "Zhijie Deng", "Hui Chen", "Zijia Lin", "Guiguang Ding"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance across a\nrange of natural language processing tasks. However, recent advances\ndemonstrate that further gains particularly in complex reasoning tasks require\nmore than merely scaling up model sizes or training data. One promising\ndirection is to enable models to think during the reasoning process. Recently,\nQuiet STaR significantly improves reasoning by generating token-level thought\ntraces, but incurs substantial inference overhead. In this work, we propose\nFast Quiet STaR, a more efficient reasoning framework that preserves the\nbenefits of token-level reasoning while reducing computational cost. Our method\nintroduces a curriculum learning based training strategy that gradually reduces\nthe number of thought tokens, enabling the model to internalize more abstract\nand concise reasoning processes. We further extend this approach to the\nstandard Next Token Prediction (NTP) setting through reinforcement\nlearning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates\nthe need for explicit thought token generation during inference. Experiments on\nfour benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast\nQuiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy\nunder the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an\naverage accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B,\nwhile maintaining the same inference latency. Our code will be available at\nhttps://github.com/huangwei200012/Fast-Quiet-STaR.", "AI": {"tldr": "Fast Quiet STaR is an efficient reasoning framework for large language models (LLMs) that reduces computational costs while improving reasoning accuracy by using curriculum learning and reinforcement learning techniques.", "motivation": "To address the limitations of existing reasoning frameworks that either incur high inference overhead or fail to optimize reasoning processes effectively, leading to better model performance in complex tasks.", "method": "Introduced a curriculum learning strategy to train models to reduce thought tokens gradually, implemented Fast Quiet-STaR NTP which eliminates explicit thought token generation during inference, compared performance against traditional Quiet STaR.", "result": "Fast Quiet-STaR outperformed Quiet-STaR on benchmarks, achieving a 9% and 5.7% accuracy improvement on Mistral 7B and Qwen2.5 7B respectively, while maintaining the same inference latency.", "conclusion": "Fast Quiet-STaR provides a more efficient way to enhance reasoning capabilities in LLMs, making them applicable for complex reasoning tasks, and demonstrating significant accuracy improvements without additional latency.", "key_contributions": ["Development of Fast Quiet STaR framework for efficient reasoning", "Implementation of curriculum learning to optimize thought token usage", "Demonstration of accuracy improvements compared to existing models in standard benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Reasoning Frameworks", "Curriculum Learning", "Reinforcement Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17747", "pdf": "https://arxiv.org/pdf/2505.17747.pdf", "abs": "https://arxiv.org/abs/2505.17747", "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks", "authors": ["Maureen de Seyssel", "Jie Chi", "Skyler Seto", "Maartje ter Hoeve", "Masha Fedzechkina", "Natalie Schluter"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations.", "AI": {"tldr": "Introduction of training-free ABX-style tasks to evaluate multilingual language models on language identity and semantic content.", "motivation": "To offer a flexible and interpretable alternative to probing for analyzing language models' representations of identity and meaning.", "method": "Evaluation of multilingual language models via zero-shot ABX-style discrimination tasks, inspired by speech processing, measuring representation differences across various checkpoints and layers.", "result": "Language discrimination declines with training and is concentrated in lower layers, while meaning discrimination strengthens and stabilizes in deeper layers.", "conclusion": "ABX tasks provide a lightweight framework for understanding the structure of multilingual representations, showing alignment with linguistic learning performance.", "key_contributions": ["Training-free ABX-style tasks for multilingual evaluation", "Insights on language and meaning discrimination across model layers", "Framework for analyzing multilingual representations"], "limitations": "", "keywords": ["multilingual language models", "ABX tasks", "language identity", "semantic content", "representation analysis"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17762", "pdf": "https://arxiv.org/pdf/2505.17762.pdf", "abs": "https://arxiv.org/abs/2505.17762", "title": "Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs", "authors": ["Ziyu Ge", "Yuhao Wu", "Daniel Wai Kit Chin", "Roy Ka-Wei Lee", "Rui Cao"], "categories": ["cs.CL", "cs.IR"], "comment": "Camera-ready for IJCAI 2025, AI and Social Good", "summary": "Large Language Models (LLMs) augmented with retrieval mechanisms have\ndemonstrated significant potential in fact-checking tasks by integrating\nexternal knowledge. However, their reliability decreases when confronted with\nconflicting evidence from sources of varying credibility. This paper presents\nthe first systematic evaluation of Retrieval-Augmented Generation (RAG) models\nfor fact-checking in the presence of conflicting evidence. To support this\nstudy, we introduce \\textbf{CONFACT} (\\textbf{Con}flicting Evidence for\n\\textbf{Fact}-Checking) (Dataset available at\nhttps://github.com/zoeyyes/CONFACT), a novel dataset comprising questions\npaired with conflicting information from various sources. Extensive experiments\nreveal critical vulnerabilities in state-of-the-art RAG methods, particularly\nin resolving conflicts stemming from differences in media source credibility.\nTo address these challenges, we investigate strategies to integrate media\nbackground information into both the retrieval and generation stages. Our\nresults show that effectively incorporating source credibility significantly\nenhances the ability of RAG models to resolve conflicting evidence and improve\nfact-checking performance.", "AI": {"tldr": "Evaluation of RAG models for fact-checking under conflicting evidence.", "motivation": "To assess the effectiveness of Retrieval-Augmented Generation (RAG) models in fact-checking when faced with conflicting information from different credible sources.", "method": "Systematic evaluation using a novel dataset called CONFACT, which contains questions paired with conflicting evidence from various sources. Experiments focused on integrating media source credibility into RAG methods.", "result": "Findings indicate vulnerabilities in state-of-the-art RAG methods in resolving conflicts due to media source credibility differences. Strategies to integrate source credibility were tested, showing enhanced performance.", "conclusion": "Incorporating media background information significantly improves the fact-checking capabilities of RAG models under contradictory evidence.", "key_contributions": ["Introduction of the CONFACT dataset for evaluating fact-checking with conflicting evidence.", "Systematic evaluation of RAG models in handling conflicts from various media sources.", "Proposed strategies for integrating source credibility to enhance RAG performance."], "limitations": "Limited to evaluation of RAG models, may not generalize to all types of AI models.", "keywords": ["Retrieval-Augmented Generation", "Fact-Checking", "Conflicting Evidence", "Dataset", "Media Credibility"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17767", "pdf": "https://arxiv.org/pdf/2505.17767.pdf", "abs": "https://arxiv.org/abs/2505.17767", "title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) agents represent a promising shift in human-AI\ninteraction, moving beyond passive prompt-response systems to autonomous agents\ncapable of reasoning, planning, and goal-directed action. Despite the\nwidespread application in specialized, high-effort tasks like coding and\nscientific research, we highlight a critical usability gap in high-demand,\nmass-market applications. This position paper argues that the limited\nreal-world adoption of LLM agents stems not only from gaps in model\ncapabilities, but also from a fundamental tradeoff between the value an agent\ncan provide and the costs incurred during real-world use. Hence, we call for a\nshift from solely optimizing model performance to a broader, utility-driven\nperspective: evaluating agents through the lens of the overall agentic return\non investment (Agent ROI). By identifying key factors that determine Agentic\nROI--information quality, agent time, and cost--we posit a zigzag development\ntrajectory in optimizing agentic ROI: first scaling up to improve the\ninformation quality, then scaling down to minimize the time and cost. We\noutline the roadmap across different development stages to bridge the current\nusability gaps, aiming to make LLM agents truly scalable, accessible, and\neffective in real-world contexts.", "AI": {"tldr": "This paper advocates for a utility-driven approach to optimize Large Language Model (LLM) agents beyond their performance, focusing on the concept of Agent ROI to bridge usability gaps in mass-market applications.", "motivation": "The limited real-world adoption of LLM agents is attributed to usability gaps arising from the tradeoff between the agent’s value and the costs of its use.", "method": "The paper proposes a zigzag development trajectory focusing first on scaling up information quality, followed by scaling down to minimize time and cost, in order to enhance Agentic ROI.", "result": "The identification of key factors determining Agentic ROI, including information quality, agent time, and cost, offers a new perspective for evaluating and developing LLM agents.", "conclusion": "By addressing usability gaps through an Agent ROI lens, LLM agents can become more scalable, accessible, and effective in real-world applications.", "key_contributions": ["Introduction of Agentic ROI as a framework for evaluating LLM agents", "Proposal of a zigzag development trajectory for optimizing agent usability", "Identification of critical factors that influence the utility of LLM agents in mass-market applications"], "limitations": "", "keywords": ["Large Language Models", "Human-AI Interaction", "Agentic ROI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17784", "pdf": "https://arxiv.org/pdf/2505.17784.pdf", "abs": "https://arxiv.org/abs/2505.17784", "title": "EXECUTE: A Multilingual Benchmark for LLM Token Understanding", "authors": ["Lukas Edman", "Helmut Schmid", "Alexander Fraser"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The CUTE benchmark showed that LLMs struggle with character understanding in\nEnglish. We extend it to more languages with diverse scripts and writing\nsystems, introducing EXECUTE. Our simplified framework allows easy expansion to\nany language. Tests across multiple LLMs reveal that challenges in other\nlanguages are not always on the character level as in English. Some languages\nshow word-level processing issues, some show no issues at all. We also examine\nsub-character tasks in Chinese, Japanese, and Korean to assess LLMs'\nunderstanding of character components.", "AI": {"tldr": "EXECUTE extends the CUTE benchmark to evaluate LLMs' character and word-level understanding across multiple languages and scripts.", "motivation": "To address the limitations of LLMs in character understanding indicated by the CUTE benchmark, and to explore performance across diverse languages and scripts.", "method": "We developed a simplified framework, EXECUTE, that can be easily expanded to any language and conducted tests across multiple LLMs to assess their understanding of different levels of writing.", "result": "The tests revealed that issues with LLM understanding vary between languages, with some languages having word-level challenges and others performing well without issues.", "conclusion": "The findings highlight the need for further investigation into LLMs' understanding beyond character-level processing, particularly in diverse linguistic contexts.", "key_contributions": ["Introduction of the EXECUTE framework for evaluating LLMs in various languages", "Demonstration of varied performance issues across languages, not limited to character understanding", "Assessment of sub-character tasks in East Asian languages to evaluate character component understanding."], "limitations": "The study is limited to the languages and models tested; broader conclusions require more extensive language coverage.", "keywords": ["LLM", "CUTE benchmark", "language processing", "EXECUTE", "character understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17793", "pdf": "https://arxiv.org/pdf/2505.17793.pdf", "abs": "https://arxiv.org/abs/2505.17793", "title": "Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion", "authors": ["Jianxiang Zang", "Meiling Ning", "Yongda Wei", "Shihan Dou", "Jiazheng Zhang", "Nijia Mo", "Binhong Li", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.", "AI": {"tldr": "The paper introduces refined compression metrics for language models that improve performance evaluation by addressing geometric distortions in compressed representations.", "motivation": "To address the limitations of current compression metrics in evaluating the performance of language models.", "method": "The authors propose three refined compression metrics that integrate geometric distortion analysis into a self-evaluation pipeline.", "result": "The new metrics correlate strongly with language models' capabilities, achieving Spearman correlation coefficients above 0.9, outperforming existing metrics.", "conclusion": "Refined compression metrics enhance the understanding of language models by accounting for geometric distortions in their representations.", "key_contributions": ["Introduction of three refined compression metrics for language models.", "Integration of geometric distortion analysis into LM evaluation.", "Demonstrated strong correlation between the new metrics and LM capabilities."], "limitations": "", "keywords": ["compression as intelligence", "geometric distortion", "language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.17795", "pdf": "https://arxiv.org/pdf/2505.17795.pdf", "abs": "https://arxiv.org/abs/2505.17795", "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors", "authors": ["Tazeek Bin Abdur Rakib", "Ambuj Mehrish", "Lay-Ki Soon", "Wern Han Lim", "Soujanya Poria"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/", "AI": {"tldr": "DialogXpert enhances LLM agents for proactive and goal-driven interactions by combining a frozen LLM with a compact Q-network and user emotion tracking, achieving high success rates in dialogue benchmarks.", "motivation": "To improve proactive, goal-driven interactions in LLM agents that currently excel only in reactive dialogue.", "method": "DialogXpert uses a frozen LLM to generate a set of candidate actions per turn and employs a Q-network trained via temporal-difference learning over fixed BERT embeddings to select the best action.", "result": "Achieves over 94% success rates in dialogue tasks, improving to above 97% with a larger LLM, significantly enhancing negotiation outcomes.", "conclusion": "DialogXpert provides an effective framework for real-time, strategic, and emotionally intelligent dialogue planning across various tasks.", "key_contributions": ["Introduces a novel approach for dialogue planning using a Q-network with LLMs.", "Tracks user emotions to tailor decision-making in conversations.", "Demonstrates high success rates and reduced conversation turns in application benchmarks."], "limitations": "", "keywords": ["large-language-model", "dialogue planning", "emotion tracking", "Q-network", "temporal-difference learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.17813", "pdf": "https://arxiv.org/pdf/2505.17813.pdf", "abs": "https://arxiv.org/abs/2505.17813", "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning", "authors": ["Michael Hassid", "Gabriel Synnaeve", "Yossi Adi", "Roy Schwartz"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Under review", "summary": "Reasoning large language models (LLMs) heavily rely on scaling test-time\ncompute to perform complex reasoning tasks by generating extensive \"thinking\"\nchains. While demonstrating impressive results, this approach incurs\nsignificant computational costs and inference time. In this work, we challenge\nthe assumption that long thinking chains results in better reasoning\ncapabilities. We first demonstrate that shorter reasoning chains within\nindividual questions are significantly more likely to yield correct answers -\nup to 34.5% more accurate than the longest chain sampled for the same question.\nBased on these results, we suggest short-m@k, a novel reasoning LLM inference\nmethod. Our method executes k independent generations in parallel and halts\ncomputation once the first m thinking processes are done. The final answer is\nchosen using majority voting among these m chains. Basic short-1@k demonstrates\nsimilar or even superior performance over standard majority voting in\nlow-compute settings - using up to 40% fewer thinking tokens. short-3@k, while\nslightly less efficient than short-1@k, consistently surpasses majority voting\nacross all compute budgets, while still being substantially faster (up to 33%\nwall time reduction). Inspired by our results, we finetune an LLM using short,\nlong, and randomly selected reasoning chains. We then observe that training on\nthe shorter ones leads to better performance. Our findings suggest rethinking\ncurrent methods of test-time compute in reasoning LLMs, emphasizing that longer\n\"thinking\" does not necessarily translate to improved performance and can,\ncounter-intuitively, lead to degraded results.", "AI": {"tldr": "The paper challenges the assumption that longer reasoning chains in LLMs lead to better performance, showing that shorter chains can yield significantly more accurate results with reduced computational costs.", "motivation": "To explore the impact of reasoning chain length on the performance of large language models in complex reasoning tasks, and to propose more efficient inference methods.", "method": "The authors propose short-m@k, an LLM inference method that runs k independent generations in parallel and selects the final answer through majority voting of the first m completions. The effectiveness of this method is validated through experiments comparing shorter and longer reasoning chains.", "result": "Shorter reasoning chains provide up to 34.5% higher accuracy than longer ones and lead to improved performance while using up to 40% fewer thinking tokens and significantly reducing inference time.", "conclusion": "The study concludes that longer reasoning chains may not always correlate with better LLM performance, advocating for reconsideration of current LLM inference strategies to prioritize efficiency and accuracy.", "key_contributions": ["Introduction of short-m@k inference method", "Demonstration of improved accuracy with shorter reasoning chains", "Reevaluation of test-time compute assumptions in reasoning LLMs"], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Inference Method", "Computational Efficiency", "Accuracy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17816", "pdf": "https://arxiv.org/pdf/2505.17816.pdf", "abs": "https://arxiv.org/abs/2505.17816", "title": "Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong", "authors": ["Hei Yi Mak", "Tan Lee"], "categories": ["cs.CL"], "comment": "Proceedings of the 2021 5th International Conference on Natural\n  Language Processing and Information Retrieval", "summary": "The majority of inhabitants in Hong Kong are able to read and write in\nstandard Chinese but use Cantonese as the primary spoken language in daily\nlife. Spoken Cantonese can be transcribed into Chinese characters, which\nconstitute the so-called written Cantonese. Written Cantonese exhibits\nsignificant lexical and grammatical differences from standard written Chinese.\nThe rise of written Cantonese is increasingly evident in the cyber world. The\ngrowing interaction between Mandarin speakers and Cantonese speakers is leading\nto a clear demand for automatic translation between Chinese and Cantonese. This\npaper describes a transformer-based neural machine translation (NMT) system for\nwritten-Chinese-to-written-Cantonese translation. Given that parallel text data\nof Chinese and Cantonese are extremely scarce, a major focus of this study is\non the effort of preparing good amount of training data for NMT. In addition to\ncollecting 28K parallel sentences from previous linguistic studies and\nscattered internet resources, we devise an effective approach to obtaining 72K\nparallel sentences by automatically extracting pairs of semantically similar\nsentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.\nWe show that leveraging highly similar sentence pairs mined from Wikipedia\nimproves translation performance in all test sets. Our system outperforms Baidu\nFanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU\nscores. Translation examples reveal that our system is able to capture\nimportant linguistic transformations between standard Chinese and spoken\nCantonese.", "AI": {"tldr": "This paper develops a transformer-based NMT system for translating written Chinese to written Cantonese, addressing data scarcity through innovative training data collection methods from Wikipedia.", "motivation": "There is a growing demand for automatic translation between Chinese and Cantonese due to increased interaction between speakers, necessitating improved translation systems.", "method": "The study uses a transformer-based neural machine translation system, collecting 28K parallel sentences and devising an approach to create an additional 72K parallel sentences from similar sentence pairs on Chinese and Cantonese Wikipedia.", "result": "The proposed system outperforms Baidu Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets based on BLEU scores, demonstrating improved translation quality using mined Wikipedia sentences.", "conclusion": "The system captures the linguistic transformations between standard Chinese and spoken Cantonese effectively, indicating the potential for better translation systems in this domain.", "key_contributions": ["Development of a transformer-based NMT system for Chinese-Cantonese translation", "Innovative data collection strategy using Wikipedia to generate training data", "Demonstrated superior performance against existing translation tools (Baidu Fanyi)"], "limitations": "", "keywords": ["Neural Machine Translation", "Cantonese", "Chinese", "Transformer Model", "Data Scarcity"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2505.17827", "pdf": "https://arxiv.org/pdf/2505.17827.pdf", "abs": "https://arxiv.org/abs/2505.17827", "title": "Not All Tokens Are What You Need In Thinking", "authors": ["Hang Yuan", "Bin Yu", "Haotian Li", "Shijun Yang", "Christina Dan Wang", "Zhou Yu", "Xueyin Xu", "Weizhen Qi", "Kai Chen"], "categories": ["cs.CL"], "comment": "11 pages, 7 figures and 3 tables", "summary": "Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit\nimpressive problem-solving capabilities but suffer from critical\ninefficiencies: high inference latency, excessive computational resource\nconsumption, and a tendency toward overthinking -- generating verbose chains of\nthought (CoT) laden with redundant tokens that contribute minimally to the\nfinal answer. To address these issues, we propose Conditional Token Selection\n(CTS), a token-level compression framework with a flexible and variable\ncompression ratio that identifies and preserves only the most essential tokens\nin CoT. CTS evaluates each token's contribution to deriving correct answers\nusing conditional importance scoring, then trains models on compressed CoT.\nExtensive experiments demonstrate that CTS effectively compresses long CoT\nwhile maintaining strong reasoning performance. Notably, on the GPQA benchmark,\nQwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with\n13.2% fewer reasoning tokens (13% training token reduction). Further reducing\ntraining tokens by 42% incurs only a marginal 5% accuracy drop while yielding a\n75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy\nin existing CoT.", "AI": {"tldr": "This paper introduces Conditional Token Selection (CTS), a framework that compresses tokens in chains of thought (CoT) to reduce redundancy and improve reasoning performance in models.", "motivation": "Existing reasoning models exhibit inefficiencies such as high latency, excessive resource consumption, and verbosity in their generated outputs.", "method": "Conditional Token Selection (CTS) applies a token-level compression framework that evaluates and retains only the essential tokens in CoT, using conditional importance scoring to assess each token's contribution to correct answers.", "result": "CTS compresses long CoT, achieving notable improvements, including a 9.1% accuracy increase on the GPQA benchmark with a reduction of 13.2% in reasoning tokens, and a potential 75.8% reduction in reasoning tokens with minimal accuracy loss.", "conclusion": "The study validates that reducing redundancy in CoT can enhance model performance while significantly lowering the computational load.", "key_contributions": ["Proposed Conditional Token Selection (CTS) framework for token compression in CoT.", "Demonstrated significant improvements in accuracy while reducing the number of reasoning tokens.", "Showed effectiveness in reducing training tokens with minimal impact on performance."], "limitations": "The methodology may not address all types of redundancy, and the impact on different reasoning tasks remains to be explored further.", "keywords": ["token compression", "reasoning models", "Chains of Thought", "machine learning", "model efficiency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.17829", "pdf": "https://arxiv.org/pdf/2505.17829.pdf", "abs": "https://arxiv.org/abs/2505.17829", "title": "Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning", "authors": ["Zezhong Wang", "Xingshan Zeng", "Weiwen Liu", "Yufei Wang", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a\npowerful capability of Large Language Models (LLMs), which can be further\nenhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.\nHowever, these methods, despite improving accuracy by allocating more\ncomputational resources during inference, often suffer from path homogenization\nand inefficient use of intermediate results. To address these limitations, we\npropose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that\nintroduces checkpoints between reasoning steps. It incorporates two key\nstrategies: (1) Answer-Clustered Search, which groups reasoning paths by their\nintermediate checkpoint answers to maintain diversity while ensuring quality,\nand (2) Checkpoint Candidate Augmentation, which leverages all intermediate\nanswers for final decision-making. Our approach effectively reduces path\nhomogenization and creates a fault-tolerant mechanism by utilizing high-quality\nintermediate results. Experimental results show that SRCA improves reasoning\naccuracy compared to existing TTS methods across various mathematical datasets.", "AI": {"tldr": "This paper introduces Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that enhances mathematical reasoning in Large Language Models by addressing limitations of existing Test-Time Scaling methods and improves accuracy using checkpoint mechanisms.", "motivation": "To improve mathematical reasoning accuracy in Large Language Models (LLMs) by addressing limitations of Test-Time Scaling methods.", "method": "Stepwise Reasoning Checkpoint Analysis (SRCA) introduces checkpoints between reasoning steps, utilizing Answer-Clustered Search and Checkpoint Candidate Augmentation for better decision-making.", "result": "Experimental results demonstrate that SRCA improves reasoning accuracy over existing TTS methods on various mathematical datasets.", "conclusion": "SRCA effectively reduces path homogenization and creates a fault-tolerant mechanism by utilizing high-quality intermediate results, leading to enhanced reasoning accuracy.", "key_contributions": ["Introduction of Stepwise Reasoning Checkpoint Analysis (SRCA) framework for LLMs.", "Answer-Clustered Search for maintaining diversity in reasoning paths.", "Checkpoint Candidate Augmentation for leveraging intermediate answers in final decisions."], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Test-Time Scaling", "Checkpointing", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.17832", "pdf": "https://arxiv.org/pdf/2505.17832.pdf", "abs": "https://arxiv.org/abs/2505.17832", "title": "Emerging categories in scientific explanations", "authors": ["Giacomo Magnifico", "Eduard Barbu"], "categories": ["cs.CL"], "comment": "Accepted at the 3rd TRR 318 Conference: Contextualizing Explanations\n  (ContEx25), as a two-pager abstract. Will be published at BiUP (Bielefeld\n  University Press) at a later date", "summary": "Clear and effective explanations are essential for human understanding and\nknowledge dissemination. The scope of scientific research aiming to understand\nthe essence of explanations has recently expanded from the social sciences to\nmachine learning and artificial intelligence. Explanations for machine learning\ndecisions must be impactful and human-like, and there is a lack of large-scale\ndatasets focusing on human-like and human-generated explanations. This work\naims to provide such a dataset by: extracting sentences that indicate\nexplanations from scientific literature among various sources in the\nbiotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access\nsubset); providing a multi-class notation derived inductively from the data;\nevaluating annotator consensus on the emerging categories. The sentences are\norganized in an openly-available dataset, with two different classifications\n(6-class and 3-class category annotation), and the 3-class notation achieves a\n0.667 Krippendorf Alpha value.", "AI": {"tldr": "This work presents a new dataset focused on human-like explanations extracted from scientific literature in biotechnology and biophysics, addressing the need for impactful explanations in machine learning.", "motivation": "The study addresses the growing need for impactful, human-like explanations in machine learning and the current lack of datasets that provide such explanations.", "method": "The authors extract explanatory sentences from scientific literature (e.g., PubMed) and categorize them into 3-class and 6-class annotations, evaluating annotator consensus.", "result": "The resulting dataset is openly available and features a 0.667 Krippendorf Alpha value for the 3-class category, indicating a fair level of agreement among annotators.", "conclusion": "The creation of this dataset fills a significant gap in the availability of human-generated explanations for AI applications, contributing to better understanding and utilization of machine learning models.", "key_contributions": ["Creation of a novel dataset of human-like explanations from scientific literature", "Establishment of multi-class categorization for explanation sentences", "Evaluation of annotator consensus on classification categories"], "limitations": "", "keywords": ["explanations", "machine learning", "dataset", "biotechnology", "biophysics"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.17833", "pdf": "https://arxiv.org/pdf/2505.17833.pdf", "abs": "https://arxiv.org/abs/2505.17833", "title": "Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus", "authors": ["Kalle Lahtinen", "Einari Vaaras", "Liisa Mustanoja", "Okko Räsänen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at Interspeech 2025, Rotterdam, The\n  Netherlands", "summary": "Study of affect in speech requires suitable data, as emotional expression and\nperception vary across languages. Until now, no corpus has existed for natural\nexpression of affect in spontaneous Finnish, existing data being acted or from\na very specific communicative setting. This paper presents the first such\ncorpus, created by annotating 12,000 utterances for emotional arousal and\nvalence, sampled from three large-scale Finnish speech corpora. To ensure\ndiverse affective expression, sample selection was conducted with an affect\nmining approach combining acoustic, cross-linguistic speech emotion, and text\nsentiment features. We compare this method to random sampling in terms of\nannotation diversity, and conduct post-hoc analyses to identify sampling\nchoices that would have maximized the diversity. As an outcome, the work\nintroduces a spontaneous Finnish affective speech corpus and informs sampling\nstrategies for affective speech corpus creation in other languages or domains.", "AI": {"tldr": "This paper introduces the first spontaneous Finnish affective speech corpus, annotated for emotional arousal and valence, using a novel affect mining approach.", "motivation": "To address the lack of existing corpora for natural emotional expression in spontaneous Finnish speech, which is varied across languages.", "method": "The corpus was created by annotating 12,000 utterances from three large-scale Finnish speech corpora, using an affect mining approach that combines acoustic features with text sentiment analysis. The effectiveness of this method was compared to random sampling.", "result": "The introduced corpus showcases diverse affective expressions and offers insights into effective sampling strategies for creating affective speech corpora across different languages and contexts.", "conclusion": "This work lays the groundwork for future research in affective speech analysis, emphasizing the importance of diverse sampling strategies.", "key_contributions": ["First spontaneous Finnish affective speech corpus", "Novel affect mining approach for diverse annotation", "Insights into sampling strategies for affective speech corpora"], "limitations": "", "keywords": ["affective speech", "Finnish corpus", "emotion annotation"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2505.17855", "pdf": "https://arxiv.org/pdf/2505.17855.pdf", "abs": "https://arxiv.org/abs/2505.17855", "title": "Explaining Sources of Uncertainty in Automated Fact-Checking", "authors": ["Jingyi Sun", "Greta Warren", "Irina Shklovski", "Isabelle Augenstein"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.", "AI": {"tldr": "CLUE generates natural language explanations of model uncertainty by identifying text relationships that reveal conflicts and agreements, enhancing human-AI collaboration.", "motivation": "Effectively addressing model uncertainty is crucial for successful human-AI interaction, especially when conflicts in evidence arise.", "method": "CLUE identifies relationships between text spans to reveal claim-evidence conflicts and agreements that inform model uncertainty, using unsupervised methods and prompting with attention steering for explanation generation.", "result": "CLUE-generated explanations are more faithful to the model's uncertainty and align better with fact-checking decisions than traditional prompting methods, improving user trust and understanding.", "conclusion": "CLUE offers a plug-and-play solution for generating insightful uncertainty explanations in language models without requiring fine-tuning, enhancing applications in fact-checking and reasoning.", "key_contributions": ["First framework for conflict-and-agreement-aware uncertainty explanations", "Improves user understanding of model uncertainty", "No fine-tuning needed, applicable to any white-box language model"], "limitations": "", "keywords": ["model uncertainty", "natural language explanations", "fact-checking", "human-AI collaboration", "unsupervised learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.17870", "pdf": "https://arxiv.org/pdf/2505.17870.pdf", "abs": "https://arxiv.org/abs/2505.17870", "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods", "authors": ["Shaina Raza", "Rizwan Qureshi", "Marcelo Lotif", "Aman Chadha", "Deval Pandya", "Christos Emmanouilidis"], "categories": ["cs.CL"], "comment": null, "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.", "AI": {"tldr": "This position paper proposes a training framework for generative AI models that involves fine tuning on labeled falsehoods, akin to immunization, to combat misinformation while maintaining accuracy.", "motivation": "To address the issue of generative AI models reproducing false information from their training data.", "method": "The paper proposes an immunization framework where models are fine-tuned on curated, labeled examples of misinformation, injecting these during training to enhance their ability to detect falsehoods.", "result": "Immunized models demonstrated a significant reduction in the generation of misinformation compared to baseline models.", "conclusion": "This framework allows for improved recognition and rejection of misleading claims while maintaining the model's accuracy on truthful inputs, establishing a novel approach to align AI systems with factual information.", "key_contributions": ["Introduces the concept of using labeled falsehoods as a supervised 'vaccine' for AI models.", "Demonstrates the effectiveness of this method through a case study showing reduced misinformation generation.", "Outlines ethical safeguards for the use of false data in training."], "limitations": "The paper does not address the potential biases in the selection of falsehoods used for training, nor does it provide extensive empirical validation of the impact in diverse settings.", "keywords": ["Generative AI", "Misinformation", "Immunization framework", "Falsehoods", "Ethical safeguards"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.17873", "pdf": "https://arxiv.org/pdf/2505.17873.pdf", "abs": "https://arxiv.org/abs/2505.17873", "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "authors": ["Wanhao Liu", "Zonglin Yang", "Jue Wang", "Lidong Bing", "Di Zhang", "Dongzhan Zhou", "Yuqiang Li", "Houqiang Li", "Erik Cambria", "Wanli Ouyang"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.", "AI": {"tldr": "This paper introduces experiment-guided ranking for hypothesis selection in scientific discovery, leveraging simulated outcomes to improve ranking effectiveness compared to existing pre-experimental methods.", "motivation": "To improve hypothesis ranking in automated scientific discovery by incorporating empirical outcomes and addressing the limitations of costly wet-lab experiments.", "method": "A simulator modeling hypothesis performance based on similarity to known hypotheses is developed, alongside a pseudo experiment-guided ranking method that clusters hypotheses and uses simulated feedback for prioritization.", "result": "The proposed method outperforms traditional pre-experiment ranking approaches and strong ablations in experimental validation.", "conclusion": "The experiment-guided ranking approach effectively enhances hypothesis prioritization in scientific research by utilizing simulated results from prior experiments.", "key_contributions": ["Introduction of experiment-guided ranking for hypothesis selection.", "Development of a simulator that informs the ranking process.", "Demonstration of improved performance over pre-experiment ranking methods."], "limitations": "The reliance on simulation may not fully capture the complexities of real-world experiments.", "keywords": ["hypothesis ranking", "automated scientific discovery", "simulated experiments"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.17894", "pdf": "https://arxiv.org/pdf/2505.17894.pdf", "abs": "https://arxiv.org/abs/2505.17894", "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motaism Hamed", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.", "AI": {"tldr": "Mutarjim is a compact Arabic-English translation model that outperforms larger models and introduces Tarjama-25, a new benchmark for evaluating translation quality.", "motivation": "To address the limitations of existing large-scale language models in Arabic-English translation while providing a more efficient and effective solution.", "method": "A two-phase training approach using the Kuwain-1.5B model with a high-quality training corpus and the introduction of a new benchmark, Tarjama-25.", "result": "Mutarjim achieves state-of-the-art performance on the English-to-Arabic translation task in Tarjama-25, outperforming models significantly larger than itself.", "conclusion": "The development of Mutarjim and the Tarjama-25 benchmark will enhance research in Arabic-English translation systems and provide a valuable resource for future work.", "key_contributions": ["Launch of the Mutarjim language model that surpasses larger models in performance.", "Creation of the Tarjama-25 benchmark to evaluate translation systems more effectively.", "Optimized training methodology that reduces computational costs."], "limitations": "", "keywords": ["Arabic-English translation", "language model", "benchmarking"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.17923", "pdf": "https://arxiv.org/pdf/2505.17923.pdf", "abs": "https://arxiv.org/abs/2505.17923", "title": "Language models can learn implicit multi-hop reasoning, but only if they have lots of training data", "authors": ["Yuekun Yao", "Yupei Du", "Dawei Zhu", "Michael Hahn", "Alexander Koller"], "categories": ["cs.CL"], "comment": null, "summary": "Implicit reasoning is the ability of a language model to solve multi-hop\nreasoning tasks in a single forward pass, without chain of thought. We\ninvestigate this capability using GPT2-style language models trained from\nscratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that\nwhile such models can indeed learn implicit $k$-hop reasoning, the required\ntraining data grows exponentially in $k$, and the required number of\ntransformer layers grows linearly in $k$. We offer a theoretical explanation\nfor why this depth growth is necessary. We further find that the data\nrequirement can be mitigated, but not eliminated, through curriculum learning.", "AI": {"tldr": "This paper explores implicit reasoning in GPT2-style models, revealing exponential data growth needs for multi-hop reasoning tasks and the importance of model depth.", "motivation": "To understand the capabilities of language models in performing multi-hop reasoning without explicit strategies and to identify necessary training conditions.", "method": "The study utilizes GPT2-style language models trained from scratch on controlled multi-hop reasoning datasets, analyzing their performance across varying complexity levels (k = 2, 3, 4).", "result": "Models can learn implicit k-hop reasoning, but require exponentially more training data and a linear increase in transformer layers with k, highlighting a necessary depth growth.", "conclusion": "While curriculum learning can help reduce data requirements, it cannot fully mitigate the increased training data demand as complexity increases.", "key_contributions": ["Study of implicit reasoning capabilities in language models", "Theoretical explanation for depth growth in transformer layers", "Insights on data requirements and curriculum learning effects."], "limitations": "The findings indicate that while improvements can be made, significant training data requirements remain for higher k-hop reasoning tasks.", "keywords": ["Implicit reasoning", "Multi-hop reasoning", "Language models", "Curriculum learning", "Data requirements"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.17950", "pdf": "https://arxiv.org/pdf/2505.17950.pdf", "abs": "https://arxiv.org/abs/2505.17950", "title": "Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models", "authors": ["Tom Bleckmann", "Paul Tschisgale"], "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "comment": null, "summary": "Recent advancements in Natural Language Processing (NLP) have facilitated the\nanalysis of student-generated language products in learning analytics (LA),\nparticularly through the use of NLP embedding models. Yet when it comes to\nscience-related language, symbolic expressions such as equations and formulas\nintroduce challenges that current embedding models struggle to address.\nExisting studies and applications often either overlook these challenges or\nremove symbolic expressions altogether, potentially leading to biased findings\nand diminished performance of LA applications. This study therefore explores\nhow contemporary embedding models differ in their capability to process and\ninterpret science-related symbolic expressions. To this end, various embedding\nmodels are evaluated using physics-specific symbolic expressions drawn from\nauthentic student responses, with performance assessed via two approaches:\nsimilarity-based analyses and integration into a machine learning pipeline. Our\nfindings reveal significant differences in model performance, with OpenAI's\nGPT-text-embedding-3-large outperforming all other examined models, though its\nadvantage over other models was moderate rather than decisive. Beyond\nperformance, additional factors such as cost, regulatory compliance, and model\ntransparency are discussed as key considerations for model selection. Overall,\nthis study underscores the importance for LA researchers and practitioners of\ncarefully selecting NLP embedding models when working with science-related\nlanguage products that include symbolic expressions.", "AI": {"tldr": "This study evaluates NLP embedding models on their ability to process science-related symbolic expressions, revealing significant performance differences and highlighting key considerations for model selection in learning analytics.", "motivation": "To address the challenges posed by symbolic expressions in science-related language which existing NLP embedding models struggle to process effectively, potentially biasing findings in learning analytics.", "method": "The study evaluates various NLP embedding models using physics-specific symbolic expressions from student responses, employing similarity-based analyses and integration into a machine learning pipeline to assess performance.", "result": "OpenAI's GPT-text-embedding-3-large outperformed other models in processing symbolic expressions, though its advantage was moderate. The study also discusses cost, regulatory compliance, and model transparency as important factors in model selection.", "conclusion": "NLP embedding model selection is crucial for effective learning analytics applications dealing with science-related language that includes symbolic expressions.", "key_contributions": ["Evaluation of NLP models on science-related symbolic expressions", "Identification of OpenAI's GPT-text-embedding-3-large as a leading model", "Discussion on important factors for model selection beyond performance"], "limitations": "", "keywords": ["NLP", "learning analytics", "symbolic expressions", "embedding models", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17952", "pdf": "https://arxiv.org/pdf/2505.17952.pdf", "abs": "https://arxiv.org/abs/2505.17952", "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL", "authors": ["Che Liu", "Haozhe Wang", "Jiazhen Pan", "Zhongwei Wan", "Yong Dai", "Fangzhen Lin", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.", "AI": {"tldr": "AlphaMed is the first medical LLM that demonstrates reasoning capabilities through reinforcement learning without supervised fine-tuning, achieving state-of-the-art results on medical QA benchmarks.", "motivation": "To enhance reasoning in LLMs for clinical applications without needing costly supervised fine-tuning on closed-source models.", "method": "AlphaMed employs minimalist rule-based rewards in reinforcement learning on public multiple-choice QA datasets, bypassing SFT or distilled CoT data.", "result": "Achieves state-of-the-art results on six medical QA benchmarks, surpassing larger or closed-source models like DeepSeek-V3-671B and Claude-3.5-Sonnet in some cases.", "conclusion": "Dataset informativeness significantly influences reasoning performance, and minimalist RL methods can induce reasoning effectively without CoT supervision, although current evaluation benchmarks need improvement.", "key_contributions": ["First medical LLM to achieve reasoning via RL without SFT.", "Demonstrates effectiveness of minimalist rule-based RL rewards on public datasets.", "Highlights the importance of dataset quantity and diversity in reasoning performance."], "limitations": "Observations suggest divergent trends across benchmarks, indicating limitations in current evaluation methodologies.", "keywords": ["medical LLM", "reinforcement learning", "reasoning capability", "multiple-choice QA", "dataset informativeness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17964", "pdf": "https://arxiv.org/pdf/2505.17964.pdf", "abs": "https://arxiv.org/abs/2505.17964", "title": "Counting Cycles with Deepseek", "authors": ["Jiashun Jin", "Tracy Ke", "Bingcheng Sui", "Zhenggang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.", "AI": {"tldr": "The paper addresses the challenge of deriving a Computationally Efficient Equivalent Form (CEEF) for the cycle count statistic, demonstrating how AI can assist in solving complex mathematical problems with proper guidance.", "motivation": "To overcome the difficulties faced by AI in solving advanced mathematics, particularly in deriving formulations for the cycle count statistic, which lacks general solutions.", "method": "The authors propose a novel approach that combines their strategy with AI's coding capabilities, focusing on graph theory to derive new formulas for the CEEF problem.", "result": "The research yields new formulas for general cases of the cycle count statistic previously undiscovered, showcasing AI's potential when guided appropriately.", "conclusion": "AI can effectively contribute to complex mathematical problem-solving with a well-defined strategy, though it cannot fully solve such problems independently.", "key_contributions": ["Introduction of a novel approach combining human guidance and AI coding skills to derive mathematical formulations.", "Discovery of new formulas for the cycle count statistic not previously known.", "Demonstration of the importance of structured guidance in unlocking AI's potential for advanced problem-solving."], "limitations": "The AI's ability to solve the problem is contingent on the clarity of the guidance provided; it cannot operate independently to reach a solution.", "keywords": ["AI", "mathematics", "graph theory", "cycle count statistic", "computational efficiency"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2505.17978", "pdf": "https://arxiv.org/pdf/2505.17978.pdf", "abs": "https://arxiv.org/abs/2505.17978", "title": "AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web", "authors": ["Rui Cao", "Zifeng Ding", "Zhijiang Guo", "Michael Schlichtkrull", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "Textual claims are often accompanied by images to enhance their credibility\nand spread on social media, but this also raises concerns about the spread of\nmisinformation. Existing datasets for automated verification of image-text\nclaims remain limited, as they often consist of synthetic claims and lack\nevidence annotations to capture the reasoning behind the verdict. In this work,\nwe introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text\nclaims. Each claim is annotated with question-answer (QA) pairs containing\nevidence from the web, reflecting a decomposed reasoning regarding the verdict.\nWe mitigate common challenges in fact-checking datasets such as contextual\ndependence, temporal leakage, and evidence insufficiency, via claim\nnormalization, temporally constrained evidence annotation, and a two-stage\nsufficiency check. We assess the consistency of the annotation in AVerImaTeC\nvia inter-annotator studies, achieving a $\\kappa=0.742$ on verdicts and\n$74.7\\%$ consistency on QA pairs. We also propose a novel evaluation method for\nevidence retrieval and conduct extensive experiments to establish baselines for\nverifying image-text claims using open-web evidence.", "AI": {"tldr": "Introduction of AVerImaTeC, a dataset of real-world image-text claims for automated verification.", "motivation": "To address the limitations of existing datasets for verifying image-text claims, which often rely on synthetic data and lack evidence annotations.", "method": "The creation of AVerImaTeC with 1,297 real-world claims, annotated with QA pairs and evidence, employing techniques for normalization, temporal constraints, and sufficiency checks.", "result": "Achieved high inter-annotator consistency and proposed a novel evaluation method for evidence retrieval, establishing baselines for verification tasks.", "conclusion": "AVerImaTeC improves the landscape of automated verification by providing well-annotated real-world claims, thus supporting better research into misinformation verification.", "key_contributions": ["Introduction of a comprehensive dataset for image-text claim verification", "High inter-annotator consistency on verdicts and QA pairs", "Novel evaluation method for evidence retrieval"], "limitations": "The dataset's effectiveness may be constrained by the specific domains and contexts covered by the image-text claims.", "keywords": ["image-text claims", "dataset", "misinformation", "fact-checking", "evidence retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.17998", "pdf": "https://arxiv.org/pdf/2505.17998.pdf", "abs": "https://arxiv.org/abs/2505.17998", "title": "TRACE for Tracking the Emergence of Semantic Representations in Transformers", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": null, "summary": "Modern transformer models exhibit phase transitions during training, distinct\nshifts from memorisation to abstraction, but the mechanisms underlying these\ntransitions remain poorly understood. Prior work has often focused on endpoint\nrepresentations or isolated signals like curvature or mutual information,\ntypically in symbolic or arithmetic domains, overlooking the emergence of\nlinguistic structure. We introduce TRACE (Tracking Representation Abstraction\nand Compositional Emergence), a diagnostic framework combining geometric,\ninformational, and linguistic signals to detect phase transitions in\nTransformer-based LMs. TRACE leverages a frame-semantic data generation method,\nABSynth, that produces annotated synthetic corpora with controllable\ncomplexity, lexical distributions, and structural entropy, while being fully\nannotated with linguistic categories, enabling precise analysis of abstraction\nemergence. Experiments reveal that (i) phase transitions align with clear\nintersections between curvature collapse and dimension stabilisation; (ii)\nthese geometric shifts coincide with emerging syntactic and semantic accuracy;\n(iii) abstraction patterns persist across architectural variants, with\ncomponents like feedforward networks affecting optimisation stability rather\nthan fundamentally altering trajectories. This work advances our understanding\nof how linguistic abstractions emerge in LMs, offering insights into model\ninterpretability, training efficiency, and compositional generalisation that\ncould inform more principled approaches to LM development.", "AI": {"tldr": "This paper introduces TRACE, a framework to analyze phase transitions in transformer models during training, focusing on the emergence of linguistic structure and abstraction.", "motivation": "To understand the poorly understood phase transitions in transformer models and how they relate to linguistic structure and abstraction.", "method": "Introducing TRACE, a diagnostic framework that utilizes geometric, informational, and linguistic signals to detect phase transitions alongside a data generation method called ABSynth for controlled analysis.", "result": "Experiments demonstrate clear correlations between phase transitions, curvature collapse, dimension stabilization, and improvements in syntactic and semantic accuracy across models.", "conclusion": "The findings enhance understanding of linguistic abstraction in language models, which may influence future LM development strategies.", "key_contributions": ["Introduction of TRACE framework for analyzing transformer phase transitions", "Use of ABSynth for generating annotated synthetic corpora", "Insights into the relationship between model architecture and abstraction emergence"], "limitations": "", "keywords": ["transformer models", "phase transitions", "linguistic abstraction", "machine learning", "TRACE"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18011", "pdf": "https://arxiv.org/pdf/2505.18011.pdf", "abs": "https://arxiv.org/abs/2505.18011", "title": "Training with Pseudo-Code for Instruction Following", "authors": ["Prince Kumar", "Rudra Murthy", "Riyaz Bhat", "Danish Contractor"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.", "AI": {"tldr": "This paper proposes fine-tuning Large Language Models (LLMs) using instruction-tuning data that includes pseudo-code to improve instruction-following capabilities.", "motivation": "To improve the instruction-following abilities of LLMs, especially for non-expert users, by using pseudo-code to express instructions more effectively.", "method": "The authors fine-tune LLMs with instruction-tuning data that incorporates pseudo-code along with final responses and evaluate the models on 11 benchmarks related to instruction-following, mathematics, and common-sense reasoning.", "result": "The experiments show that models trained with pseudo-code follow instructions better, achieving a 3-19% relative gain on instruction-following benchmarks and an average gain of up to 14% across all tasks.", "conclusion": "Fine-tuning with pseudo-code improves LLM instruction-following capabilities while maintaining performance on mathematical and common-sense reasoning tasks.", "key_contributions": ["Introduces a novel method for fine-tuning LLMs using pseudo-code for instructions.", "Demonstrates significant performance improvements in instruction-following tasks.", "Maintains effectiveness on mathematical and common-sense reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "instruction-following", "pseudo-code", "fine-tuning", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18040", "pdf": "https://arxiv.org/pdf/2505.18040.pdf", "abs": "https://arxiv.org/abs/2505.18040", "title": "Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition", "authors": ["Minxue Niu", "Emily Mower Provost"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to handle various emotion labels without dedicated training is\ncrucial for building adaptable Emotion Recognition (ER) systems. Conventional\nER models rely on training using fixed label sets and struggle to generalize\nbeyond them. On the other hand, Large Language Models (LLMs) have shown strong\nzero-shot ER performance across diverse label spaces, but their scale limits\ntheir use on edge devices. In this work, we propose a contrastive distillation\nframework that transfers rich emotional knowledge from LLMs into a compact\nmodel without the use of human annotations. We use GPT-4 to generate\ndescriptive emotion annotations, offering rich supervision beyond fixed label\nsets. By aligning text samples with emotion descriptors in a shared embedding\nspace, our method enables zero-shot prediction on different emotion classes,\ngranularity, and label schema. The distilled model is effective across multiple\ndatasets and label spaces, outperforming strong baselines of similar size and\napproaching GPT-4's zero-shot performance, while being over 10,000 times\nsmaller.", "AI": {"tldr": "This paper introduces a contrastive distillation framework to enhance emotion recognition systems by transferring knowledge from large language models (LLMs) to a compact model, enabling zero-shot prediction across diverse emotion labels without needing human annotations.", "motivation": "To improve adaptability in emotion recognition systems, allowing them to handle various emotion labels without dedicated training, which conventional ER models struggle to do.", "method": "The proposed method uses a contrastive distillation framework that utilizes GPT-4 to generate descriptive emotion annotations and align text samples with emotion descriptors in a shared embedding space for zero-shot prediction.", "result": "The distilled model successfully predicts emotions across diverse datasets and label spaces, achieving performance close to GPT-4 while being over 10,000 times smaller than LLMs.", "conclusion": "This approach demonstrates the potential of using large language models for emotion recognition in a simplified and efficient manner, bridging the gap left by traditional fixed-label ER systems.", "key_contributions": ["Introduction of a contrastive distillation framework for emotion recognition", "Utilization of GPT-4 for generating emotion annotations", "Achievement of zero-shot performance across different emotion classes and labels."], "limitations": "", "keywords": ["Emotion Recognition", "Large Language Models", "Zero-shot Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18056", "pdf": "https://arxiv.org/pdf/2505.18056.pdf", "abs": "https://arxiv.org/abs/2505.18056", "title": "MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving", "authors": ["Wei-Ling Hsu", "Yu-Chien Tang", "An-Zi Yen"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Online learning enhances educational accessibility, offering students the\nflexibility to learn anytime, anywhere. However, a key limitation is the lack\nof immediate, personalized feedback, particularly in helping students correct\nerrors in math problem-solving. Several studies have investigated the\napplications of large language models (LLMs) in educational contexts. In this\npaper, we explore the capabilities of LLMs to assess students' math\nproblem-solving processes and provide adaptive feedback. The MathEDU dataset is\nintroduced, comprising authentic student solutions annotated with teacher\nfeedback. We evaluate the model's ability to support personalized learning in\ntwo scenarios: one where the model has access to students' prior answer\nhistories, and another simulating a cold-start context. Experimental results\nshow that the fine-tuned model performs well in identifying correctness.\nHowever, the model still faces challenges in generating detailed feedback for\npedagogical purposes.", "AI": {"tldr": "This study investigates the use of large language models to provide personalized feedback on math problem-solving in education, introducing the MathEDU dataset for evaluation.", "motivation": "To address the lack of immediate, personalized feedback in online learning, particularly for math problem-solving.", "method": "The paper introduces the MathEDU dataset, evaluates a fine-tuned LLM in two scenarios—one with access to prior answer histories and another in a cold-start context.", "result": "The model effectively identifies correctness in student solutions but struggles with generating detailed pedagogical feedback.", "conclusion": "While LLMs show promise in supporting personalized learning, challenges remain in providing meaningful feedback for educational improvement.", "key_contributions": ["Introduction of the MathEDU dataset for annotating student math solutions", "Evaluation of LLMs for personalized feedback in math education", "Insights into model performance in different feedback contexts"], "limitations": "The model has challenges in generating detailed and pedagogically useful feedback despite good correctness identification.", "keywords": ["large language models", "personalized feedback", "math education", "MathEDU dataset", "adaptive learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18071", "pdf": "https://arxiv.org/pdf/2505.18071.pdf", "abs": "https://arxiv.org/abs/2505.18071", "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.", "AI": {"tldr": "This paper introduces AlignXplore, a model for improved inductive reasoning in LLMs focused on personalized preference inference from user interaction histories.", "motivation": "To address the challenges in aligning large language models with diverse user preferences, which require robust inductive reasoning capabilities.", "method": "The study proposes AlignXplore, utilizing extended reasoning chains combined with cold-start training on synthetic data and subsequent online reinforcement learning.", "result": "AlignXplore shows an improvement of 11.05% over the backbone model on benchmarks, while demonstrating strong generalization across various input formats and downstream models.", "conclusion": "The analyses from the experiments suggest effective strategies for preference inference and highlight the emergence of human-like inductive reasoning patterns in training.", "key_contributions": ["Development of AlignXplore for inductive reasoning in LLMs.", "Combination of cold-start training and online reinforcement learning techniques.", "Demonstration of enhanced performance in preference inference tasks."], "limitations": "", "keywords": ["Large Language Models", "Inductive Reasoning", "Preference Inference", "Reinforcement Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18092", "pdf": "https://arxiv.org/pdf/2505.18092.pdf", "abs": "https://arxiv.org/abs/2505.18092", "title": "QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization", "authors": ["Weizhou Shen", "Chenliang Li", "Fanqi Wan", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Yingcheng Shi", "Yuning Wu", "Gang Fu", "Zhansheng Li", "Bin Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "categories": ["cs.CL"], "comment": null, "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59$\\times$ context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.", "AI": {"tldr": "QwenLong-CPRS is a context compression framework for long-context optimization in LLMs, improving performance and efficiency through dynamic optimization.", "motivation": "To overcome computation overhead and performance degradation in LLMs when processing long sequences.", "method": "Introduces a dynamic context optimization mechanism allowing multi-granularity context compression based on natural language instructions.", "result": "Achieves 21.59× context compression and 19.15-point average performance gains, outperforming existing context management methods.", "conclusion": "QwenLong-CPRS sets new state-of-the-art performance for LLMs, integrating effectively with leading models.", "key_contributions": ["Natural language-guided dynamic optimization", "Bidirectional reasoning layers for enhanced boundary awareness", "Token critic mechanisms with language modeling heads"], "limitations": "", "keywords": ["context compression", "long-context optimization", "dynamic optimization", "language models", "performance evaluation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.18098", "pdf": "https://arxiv.org/pdf/2505.18098.pdf", "abs": "https://arxiv.org/abs/2505.18098", "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.", "AI": {"tldr": "This paper presents a novel approach using goal-conditioned value functions to enhance reasoning in large language models for complex, multi-turn interactions, achieving improved performance while maintaining efficiency.", "motivation": "Complex tasks such as negotiation and persuasion require long-horizon reasoning in large language models, which is limited by traditional reinforcement learning fine-tuning methods due to high memory and computational costs.", "method": "The proposed method utilizes goal-conditioned value functions to guide the reasoning of LLM agents, predicting task outcomes based on actions taken. This approach allows for scale and efficiency even with large API-based models.", "result": "The new approach demonstrates superior performance in interaction tasks like tool use and dialogue compared to standard RL fine-tuning and prompting methods.", "conclusion": "By training concise, lightweight value functions over reasoning steps rather than full actions, the proposed method enhances decision-making in multi-turn interactions without the scalability issues of traditional training methods.", "key_contributions": ["Introduction of goal-conditioned value functions for LLM interaction tasks", "Demonstration of improved efficiency and scalability in reasoning", "Validation of method across various complex interaction tasks"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "interaction tasks", "reasoning", "value functions"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2505.18105", "pdf": "https://arxiv.org/pdf/2505.18105.pdf", "abs": "https://arxiv.org/abs/2505.18105", "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework", "authors": ["Lisheng Huang", "Yichen Liu", "Jinhao Jiang", "Rongxiang Zhang", "Jiahao Yan", "Junyi Li", "Wayne Xin Zhao"], "categories": ["cs.CL"], "comment": "LLM, Complex Search Benchmark", "summary": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch", "AI": {"tldr": "ManuSearch proposes a modular multi-agent framework for deep search using LLMs, leveraging collaborative agents for solution planning, web search, and webpage reading.", "motivation": "To democratize access to LLM capabilities in complex reasoning tasks that are typically locked in proprietary systems.", "method": "The framework utilizes three collaborative agents: a solution planning agent, an internet search agent, and a structured webpage reading agent, enabling a comprehensive search and reasoning process.", "result": "ManuSearch outperforms existing open-source models and leading closed-source systems in evaluating deep reasoning abilities, showcasing its effectiveness through the ORION benchmark.", "conclusion": "ManuSearch enhances transparency and extensibility in research on open deep search systems, offering new pathways for reproducible research.", "key_contributions": ["Introduction of a modular multi-agent framework for LLM-based search", "Development of the ORION benchmark for evaluating open-web reasoning", "Democratization of LLM capabilities for complex reasoning tasks"], "limitations": "", "keywords": ["LLM", "Deep Search", "Multi-Agent", "Benchmark", "Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18110", "pdf": "https://arxiv.org/pdf/2505.18110.pdf", "abs": "https://arxiv.org/abs/2505.18110", "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM", "authors": ["Zinuo Li", "Xian Zhang", "Yongxin Guo", "Mohammed Bennamoun", "Farid Boussaid", "Girish Dwivedi", "Luqi Gong", "Qiuhong Ke"], "categories": ["cs.CL"], "comment": null, "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.", "AI": {"tldr": "TriSense is a triple-modality large language model for holistic video temporal understanding, integrating visual, audio, and speech cues.", "motivation": "Existing models struggle to fuse and interpret audio information in video analysis, limiting their temporal understanding capabilities.", "method": "TriSense utilizes a Query-Based Connector to adaptively reweight modality contributions based on input queries, and is supported by a new dataset, TriSense-2M, of over 2 million curated samples.", "result": "TriSense demonstrates robust performance in multimodal video analysis, effectively handling modality dropout and flexible input combinations.", "conclusion": "TriSense shows potential to advance multimodal video analysis, with code and dataset to be publicly released.", "key_contributions": ["Introduction of TriSense for improved multimodal video understanding", "Development of TriSense-2M dataset with over 2 million samples", "Innovative Query-Based Connector for adaptive modality integration"], "limitations": "", "keywords": ["multimodal learning", "video analysis", "large language model"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.18122", "pdf": "https://arxiv.org/pdf/2505.18122.pdf", "abs": "https://arxiv.org/abs/2505.18122", "title": "UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification", "authors": ["Poojah Ganesan", "Rajat Aayush Jha", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have greatly improved\nText-to-SQL performance for single-table queries. But, it remains challenging\nin multi-table databases due to complex schema and relational operations.\nExisting methods often struggle with retrieving the right tables and columns,\ngenerating accurate JOINs and UNIONs, and generalizing across diverse schemas.\nTo address these issues, we introduce UNJOIN, a two-stage framework that\ndecouples the retrieval of schema elements from SQL logic generation. In the\nfirst stage, we merge the column names of all tables in the database into a\nsingle-table representation by prefixing each column with its table name. This\nallows the model to focus purely on accurate retrieval without being distracted\nby the need to write complex SQL logic. In the second stage, the SQL query is\ngenerated on this simplified schema and mapped back to the original schema by\nreconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and\nBIRD datasets show that UNJOIN matches or exceeds the state-of-the-art\nbaselines. UNJOIN uses only schema information, which does not require data\naccess or fine-tuning, making it scalable and adaptable across databases.", "AI": {"tldr": "UNJOIN is a two-stage framework that improves Text-to-SQL performance for multi-table queries by decoupling schema retrieval from SQL logic generation.", "motivation": "Existing methods struggle with multi-table database queries due to complex schemas and relational operations.", "method": "UNJOIN merges column names across all tables into a single representation for the first stage, followed by generating the SQL query on this simplified schema in the second stage.", "result": "UNJOIN matches or exceeds state-of-the-art baselines on SPIDER and BIRD datasets.", "conclusion": "UNJOIN is scalable and adaptable across databases without requiring data access or fine-tuning.", "key_contributions": ["A novel two-stage framework for Text-to-SQL queries", "Decouples schema retrieval from SQL logic generation", "Achieves state-of-the-art results on benchmark datasets"], "limitations": "", "keywords": ["Text-to-SQL", "multi-table databases", "large language models"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.18128", "pdf": "https://arxiv.org/pdf/2505.18128.pdf", "abs": "https://arxiv.org/abs/2505.18128", "title": "Frankentext: Stitching random text fragments into long-form narratives", "authors": ["Chau Minh Pham", "Jenna Russell", "Dzung Pham", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.", "AI": {"tldr": "This paper introduces Frankentexts, a new narrative type created by LLMs that must copy 90% of tokens from human texts, exploring controllable generation, writing quality, and detection of machine-generated text.", "motivation": "To explore the challenges of controllable generation in LLMs and examine the implications for authorship verification and human-AI collaboration.", "method": "The model drafts narratives by selecting human-written passages, then revises them while maintaining a specified copy ratio, evaluated on writing quality, instruction adherence, and detectability.", "result": "Gemini-2.5-Pro generated coherent Frankentexts, with 81% coherence and 100% prompt relevance, but 59% were misclassified as human-written by AI detectors.", "conclusion": "Frankentexts raise important discussions on authorship detection, provide data for mixed authorship analysis, and allow exploration of human-AI co-writing.", "key_contributions": ["Introduction of Frankentexts as a new narrative form", "Insights into the effectiveness of writing prompts in LLMs", "Discussion on the limitations of AI text detectors"], "limitations": "Human annotators can identify Frankentexts through tone shifts and grammar issues, especially in longer texts.", "keywords": ["Frankentexts", "LLMs", "controllable generation", "authorship detection", "human-AI collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18136", "pdf": "https://arxiv.org/pdf/2505.18136.pdf", "abs": "https://arxiv.org/abs/2505.18136", "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection", "authors": ["Mykola Trokhymovych", "Lydia Pintscher", "Ricardo Baeza-Yates", "Diego Saez-Trumper"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.", "AI": {"tldr": "Next-generation vandalism detection system for Wikidata using a unified Graph2Text approach.", "motivation": "To detect vandalism in Wikidata's complex and ever-expanding knowledge environment.", "method": "Employing a method called Graph2Text to convert all edits into a single space for evaluating potential vandalism with a multilingual language model.", "result": "The new system outperforms the existing production system in detecting vandalism.", "conclusion": "The unified approach enhances maintenance and coverage for detecting vandalism in Wikidata.", "key_contributions": ["Introduction of Graph2Text for vandalism detection", "Improved system performance compared to existing solutions", "Release of open source code and dataset for further research"], "limitations": "", "keywords": ["vandalism detection", "Wikidata", "multilingual language model", "Graph2Text", "open source"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18148", "pdf": "https://arxiv.org/pdf/2505.18148.pdf", "abs": "https://arxiv.org/abs/2505.18148", "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find", "authors": ["Owen Bianchi", "Mathew J. Koretsky", "Maya Willey", "Chelsea X. Alvarado", "Tanay Nayak", "Adi Asija", "Nicole Kuznetsov", "Mike A. Nalls", "Faraz Faghri", "Daniel Khashabi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.", "AI": {"tldr": "The paper investigates how variations in gold context length affect the performance of large language models (LLMs) in long-context question answering tasks.", "motivation": "To address the gap in understanding how gold context size influences LLM performance in tasks requiring retrieval of relevant information from large amounts of irrelevant data.", "method": "The study systematically varies the size of the gold context and evaluates how this impacts the performance of seven state-of-the-art LLMs across three domains: general knowledge, biomedical reasoning, and mathematical reasoning.", "result": "The experiments show that LLM performance sharply declines with shorter gold contexts, indicating that smaller contexts degrade performance and increase sensitivity to positional bias.", "conclusion": "The findings highlight the need for improved context-aware systems that can effectively integrate information of varying lengths, especially for tasks involving scattered fine-grained information.", "key_contributions": ["Analysis of gold context size's impact on LLM performance", "Insights derived from three distinct domains", "Recommendations for the design of context-aware LLM systems"], "limitations": "", "keywords": ["Large Language Models", "Gold Context Size", "Long-Context Question Answering", "Positional Sensitivity", "Performance Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18149", "pdf": "https://arxiv.org/pdf/2505.18149.pdf", "abs": "https://arxiv.org/abs/2505.18149", "title": "First Finish Search: Efficient Test-Time Scaling in Large Language Models", "authors": ["Aradhye Agarwal", "Ayan Sengupta", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches $n$ independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.", "AI": {"tldr": "Introduces First Finish Search (FFS), a dynamic test-time scaling method for improving reasoning in large language models by prioritizing shorter decoding paths.", "motivation": "To enhance reasoning in large language models while minimizing token usage and inference latency.", "method": "First Finish Search (FFS) is a training-free parallel decoding strategy that simultaneously launches multiple samples and stops as soon as one completes.", "result": "FFS achieves 82.23% accuracy on the AIME datasets, a 15% improvement over baseline performance, comparable to OpenAI's o4-mini model.", "conclusion": "Simple test-time scaling strategies like FFS can yield significant improvements in model performance, highlighting the potential of straightforward approaches in inference.", "key_contributions": ["Introduction of First Finish Search (FFS) as a new decoding strategy.", "Demonstrated effectiveness on multiple reasoning models and datasets.", "Theoretical framework explaining the advantages of shorter decoding paths."], "limitations": "FFS may be suboptimal in certain situations when early stopping is not aligned with correctness conditions.", "keywords": ["test-time scaling", "large language models", "parallel decoding", "reasoning tasks", "inference strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18152", "pdf": "https://arxiv.org/pdf/2505.18152.pdf", "abs": "https://arxiv.org/abs/2505.18152", "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs", "authors": ["Wafa Alghallabi", "Ritesh Thawkar", "Sara Ghaboura", "Ketan More", "Omkar Thawakar", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "categories": ["cs.CL"], "comment": "Github:https://github.com/mbzuai-oryx/FannOrFlop,\n  Dataset:https://huggingface.co/datasets/omkarthawakar/FannOrFlop", "summary": "Arabic poetry stands as one of the most sophisticated and culturally embedded\nforms of expression in the Arabic language, known for its layered meanings,\nstylistic diversity, and deep historical continuity. Although large language\nmodels (LLMs) have demonstrated strong performance across languages and tasks,\ntheir ability to understand Arabic poetry remains largely unexplored. In this\nwork, we introduce `Fann or Flop`, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in twelve historical eras, covering 21\ncore poetic genres and a variety of metrical forms, from classical structures\nto contemporary free verse. The benchmark comprises a curated corpus of poems\nwith explanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM is in understanding\nclassical Arabic through the Arabic poetry. Unlike surface-level tasks, this\ndomain demands deeper interpretive reasoning and cultural sensitivity. Our\nevaluation of state-of-the-art LLMs shows that most models struggle with poetic\nunderstanding despite strong results on standard Arabic benchmarks. We release\n`Fann or Flop` along with the evaluation suite as an open-source resource to\nenable rigorous evaluation and advancement for Arabic language models. Code is\navailable at: https://github.com/mbzuai-oryx/FannOrFlop.", "AI": {"tldr": "Introduction of `Fann or Flop`, a benchmark for assessing LLMs' understanding of Arabic poetry across historical eras.", "motivation": "To explore the largely unexamined capability of LLMs in understanding the complexities of Arabic poetry, which requires deeper cultural and interpretive engagement.", "method": "A benchmark that includes a curated corpus of Arabic poems from various historical eras, covering diverse poetic genres, designed to assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context.", "result": "Evaluation of state-of-the-art LLMs indicates that they struggle significantly with understanding Arabic poetry, despite performing well on standard benchmarks.", "conclusion": "The release of `Fann or Flop` provides an essential resource for evaluating and advancing Arabic language models, highlighting the need for deeper interpretive skills in LLMs.", "key_contributions": ["Introduction of a novel benchmark for Arabic poetry comprehension", "Evaluation of LLMs reveals shortcomings in poetic understanding", "Open-source release promotes further research in Arabic language processing"], "limitations": "", "keywords": ["Arabic poetry", "language models", "benchmark", "cultural context", "interpretation"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.18154", "pdf": "https://arxiv.org/pdf/2505.18154.pdf", "abs": "https://arxiv.org/abs/2505.18154", "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas", "authors": ["Ya Wu", "Qiang Sheng", "Danding Wang", "Guang Yang", "Yifan Sun", "Zhengjia Wang", "Yuyan Bu", "Juan Cao"], "categories": ["cs.CL", "cs.CY"], "comment": "25 pages, 8 figures", "summary": "Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs.", "AI": {"tldr": "This paper introduces the Multi-step Moral Dilemmas (MMDs) dataset to evaluate the evolving moral reasoning of LLMs across complex ethical dilemmas.", "motivation": "To rigorously evaluate the moral reasoning capabilities of LLMs in decision-support systems, addressing limitations of single-step assessments.", "method": "Introduction of the MMDs dataset, consisting of 3,302 five-stage dilemmas, allowing for dynamic analysis of moral judgment evolution in LLMs.", "result": "Evaluation of nine widely used LLMs reveals significant shifts in their value preferences as dilemmas progress, indicating context-dependent moral reasoning.", "conclusion": "The study advocates for dynamic, context-aware evaluation paradigms to foster human-aligned and value-sensitive LLM development.", "key_contributions": ["Introduction of the MMDs dataset for evaluating LLM moral reasoning", "Demonstration of value shift in LLMs based on scenario complexity", "Highlighting the importance of dynamic and context-dependent ethical assessment"], "limitations": "The framework may not cover all facets of ethical reasoning and is limited to the specific scenarios defined in the dataset.", "keywords": ["ethical decision-making", "MMDs dataset", "LLMs", "moral reasoning", "dynamic assessment"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.17202", "pdf": "https://arxiv.org/pdf/2505.17202.pdf", "abs": "https://arxiv.org/abs/2505.17202", "title": "CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models", "authors": ["Arnav Verma", "Kushin Mukherjee", "Christopher Potts", "Elisa Kreiss", "Judith E. Fan"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Data visualizations are powerful tools for communicating patterns in\nquantitative data. Yet understanding any data visualization is no small feat --\nsucceeding requires jointly making sense of visual, numerical, and linguistic\ninputs arranged in a conventionalized format one has previously learned to\nparse. Recently developed vision-language models are, in principle, promising\ncandidates for developing computational models of these cognitive operations.\nHowever, it is currently unclear to what degree these models emulate human\nbehavior on tasks that involve reasoning about data visualizations. This gap\nreflects limitations in prior work that has evaluated data visualization\nunderstanding in artificial systems using measures that differ from those\ntypically used to assess these abilities in humans. Here we evaluated eight\nvision-language models on six data visualization literacy assessments designed\nfor humans and compared model responses to those of human participants. We\nfound that these models performed worse than human participants on average, and\nthis performance gap persisted even when using relatively lenient criteria to\nassess model performance. Moreover, while relative performance across items was\nsomewhat correlated between models and humans, all models produced patterns of\nerrors that were reliably distinct from those produced by human participants.\nTaken together, these findings suggest significant opportunities for further\ndevelopment of artificial systems that might serve as useful models of how\nhumans reason about data visualizations. All code and data needed to reproduce\nthese results are available at:\nhttps://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.", "AI": {"tldr": "This paper evaluates vision-language models on their ability to understand data visualizations compared to human performance.", "motivation": "To explore how well vision-language models mimic human reasoning about data visualizations, given the current limitations in existing evaluations.", "method": "Eight vision-language models were evaluated using six data visualization literacy assessments designed for humans, comparing their responses to those of human participants.", "result": "The models performed worse than human participants on average, with distinct patterns of errors not aligning with human error patterns, even under lenient assessment criteria.", "conclusion": "The findings indicate that there is a substantial gap in how vision-language models and humans understand data visualizations, highlighting the need for further development of artificial systems to model human reasoning accurately.", "key_contributions": ["Evaluation of eight vision-language models on human-designed assessments.", "Identification of significant performance gaps between models and human participants.", "Provision of resources for reproducing results."], "limitations": "Current models do not adequately replicate human reasoning patterns and have a persistent performance gap.", "keywords": ["data visualization", "vision-language models", "human reasoning", "machine learning", "cognitive operations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.17374", "pdf": "https://arxiv.org/pdf/2505.17374.pdf", "abs": "https://arxiv.org/abs/2505.17374", "title": "Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts", "authors": ["Seon Gyeom Kim", "Jae Young Choi", "Ryan Rossi", "Eunyee Koh", "Tak Yeon Lee"], "categories": ["cs.HC", "cs.CL"], "comment": "This paper has been accepted to IEEE PacificVis 2025", "summary": "The field of Multimodal Large Language Models (MLLMs) has made remarkable\nprogress in visual understanding tasks, presenting a vast opportunity to\npredict the perceptual and emotional impact of charts. However, it also raises\nconcerns, as many applications of LLMs are based on overgeneralized assumptions\nfrom a few examples, lacking sufficient validation of their performance and\neffectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising\n36 charts, evaluated by crowdsourced workers for their impact on seven\nexperiential factors. Using the dataset as ground truth, we evaluated\ncapabilities of state-of-the-art MLLMs on two tasks: direct prediction and\npairwise comparison of charts. Our findings imply that MLLMs are not as\nsensitive as human evaluators when assessing individual charts, but are\naccurate and reliable in pairwise comparisons.", "AI": {"tldr": "The paper introduces the Chart-to-Experience dataset to evaluate the perceptual impact of charts using Multimodal Large Language Models (MLLMs).", "motivation": "To address the overgeneralization and lack of validation in LLM applications for visual understanding tasks, and to explore their efficacy in predicting the impact of charts.", "method": "The study introduces the Chart-to-Experience benchmark dataset containing 36 charts assessed for their experiential impact by crowdsourced workers, evaluating MLLMs on direct prediction and pairwise comparison tasks.", "result": "MLLMs demonstrated an inability to match human sensitivity in individual chart assessments, but they proved accurate in making reliable pairwise comparisons of charts.", "conclusion": "The findings highlight the limitations of MLLMs in individual evaluations while suggesting their strength in comparative tasks, indicating a need for improved validation in their applications.", "key_contributions": ["Introduction of the Chart-to-Experience benchmark dataset", "Evaluation of MLLMs on their ability to predict chart impact", "Comparison of human and MLLM performance in chart evaluation tasks"], "limitations": "MLLMs showed less sensitivity compared to human evaluators in assessing individual charts.", "keywords": ["Multimodal Large Language Models", "Chart evaluation", "Human evaluation", "Experiential factors", "Machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2310.07147", "pdf": "https://arxiv.org/pdf/2310.07147.pdf", "abs": "https://arxiv.org/abs/2310.07147", "title": "QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources", "authors": ["Zhikai Li", "Xiaoxuan Liu", "Banghua Zhu", "Zhen Dong", "Qingyi Gu", "Kurt Keutzer"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have showcased remarkable impacts across a wide\nspectrum of natural language processing tasks. Fine-tuning these pretrained\nmodels on downstream datasets provides further significant performance gains;\nhowever, this process typically requires a large number of expensive, high-end\nGPUs. Although there have been efforts focused on parameter-efficient\nfine-tuning, they cannot fully unlock the powerful potential of full-parameter\nfine-tuning. In this paper, we propose QFT, a Quantized Full-parameter Tuning\nframework for LLMs that quantizes and stores all training states, including\nweights, gradients, and optimizer states, in INT8 format to reduce training\nmemory, thereby enabling full-parameter fine-tuning on existing GPUs at an\naffordable cost. To ensure training performance, we make two key efforts: i)\nfor quantized gradients and optimizer states, we theoretically prove that the\nLion optimizer, with its property of consistent update magnitudes, is highly\nrobust to quantization; ii) and for quantized weights, we employ the hybrid\nfeature quantizer, which identifies and protects a small subset of sparse\ncritical features while quantizing the remaining dense features, thus ensuring\naccurate weight updates without FP32 backups. Moreover, to support\nbackpropagation in the integer context, we develop a stack-based gradient flow\nscheme with O(1) complexity, forming a unified integer training pipeline. As a\nresult, QFT reduces the model state memory to 21% of the standard solution\nwhile achieving comparable performance, e.g., tuning a LLaMA-7B model requires\nonly <30GB of memory, making it feasible on a single A6000 GPU.", "AI": {"tldr": "The QFT framework enables affordable full-parameter fine-tuning of Large Language Models (LLMs) by quantizing all training states to INT8 format, significantly reducing memory requirements while maintaining performance.", "motivation": "To address the high costs and resource requirements of full-parameter fine-tuning for Large Language Models (LLMs) on expensive hardware.", "method": "A Quantized Full-parameter Tuning (QFT) framework that quantizes training states to INT8 format, utilizes a robust optimizer for quantized gradients, and features a hybrid quantization approach to protect critical features.", "result": "QFT reduces model state memory usage to 21% of traditional solutions and enables fine-tuning of a LLaMA-7B model within <30GB of memory, making it feasible on single A6000 GPUs.", "conclusion": "QFT proves that full-parameter fine-tuning can be accessible on a budget while retaining competitive performance through optimized quantization techniques.", "key_contributions": ["Introduction of the QFT framework for LLMs", "Theoretical proof of the Lion optimizer's robustness to quantization", "Development of a stack-based gradient flow scheme for integer training"], "limitations": "", "keywords": ["Large Language Models", "fine-tuning", "quantization", "optimizers", "gradient flow"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.11045", "pdf": "https://arxiv.org/pdf/2404.11045.pdf", "abs": "https://arxiv.org/abs/2404.11045", "title": "Offset Unlearning for Large Language Models", "authors": ["James Y. Huang", "Wenxuan Zhou", "Fei Wang", "Fred Morstatter", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "categories": ["cs.CL"], "comment": "Published in TMLR. https://openreview.net/pdf?id=A4RLpHPXCu", "summary": "Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, biased, and private content has\nled to ethical and legal concerns. In response to these challenges, unlearning\nhas emerged as a potential remedy for LLMs affected by problematic training\ndata. However, previous unlearning techniques are either not applicable to\nblack-box LLMs due to required access to model internal weights, or violate\ndata protection principles by retaining sensitive data for inference-time\ncorrection. We propose {\\delta}-Unlearning, an offset unlearning framework for\nblack-box LLMs. Instead of tuning the black-box LLM itself, {\\delta}-Unlearning\nlearns the logit offset needed for unlearning by contrasting the logits from a\npair of smaller models. Experiments demonstrate that {\\delta}- Unlearning can\neffectively unlearn target data while maintaining similar or even stronger\nperformance on general out-of-forget-scope tasks. {\\delta}-Unlearning also\neffectively incorporates different unlearning algorithms, making our approach a\nversatile solution to adapting various existing unlearning algorithms to\nblack-box LLMs.", "AI": {"tldr": "A framework for unlearning sensitive information in black-box LLMs without violating data protection principles.", "motivation": "To address ethical and legal concerns regarding the memorization of sensitive data in LLMs, a new unlearning technique is needed that works within the constraints of black-box models.", "method": "Introducing the {\u0014delta}-Unlearning framework, which calculates logit offsets for unlearning using outputs from smaller models instead of modifying the LLM directly.", "result": "Experiments show that {\u0014delta}-Unlearning can effectively erase target data while preserving or improving performance on unrelated tasks, offering flexibility to integrate various existing unlearning techniques.", "conclusion": "{\u0014delta}-Unlearning provides a viable solution for unlearning in black-box LLMs, maintaining performance while adhering to ethical data practices.", "key_contributions": ["Introduces the {\u0014delta}-Unlearning framework for black-box LLMs.", "Demonstrates effective unlearning without direct access to model weights.", "Shows compatibility with various existing unlearning algorithms."], "limitations": "Focuses on black-box LLMs and may not be applicable to all model types or data types.", "keywords": ["unlearning", "large language models", "black-box models", "ethical AI", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.03111", "pdf": "https://arxiv.org/pdf/2405.03111.pdf", "abs": "https://arxiv.org/abs/2405.03111", "title": "Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy", "authors": ["Michael Carl"], "categories": ["cs.CL"], "comment": "Paper was split & published as: --- Carl, M. (2025) Temporal Dynamics\n  of Emotion and Cognition in Human Translation: Integrating the Task Segment\n  Framework and the HOF Taxonomy. Digital Studies in Language and Literature,\n  DeGruyter --- Carl, M. (2025) Tracing the Temporal Dynamics of Emotion and\n  Cognition in Behavioral Translation Data. Translation Spaces. John Benjamins\n  Publishing Company", "summary": "The article develops a generative model of the human translating mind,\ngrounded in empirical translation process data. It posits that three embedded\nprocessing layers unfold concurrently in the human mind, and their traces are\ndetectable in behavioral data: sequences of routinized/automated processes are\nobservable in fluent translation production, cognitive/reflective thoughts lead\nto longer keystroke pauses, while affective/emotional states may be identified\nthrough characteristic typing and gazing patterns. Utilizing data from the\nCRITT Translation Process Research Database (TPR-DB), the article illustrates\nhow the temporal structure of keystroke and gaze data can be related to the\nthree assumed hidden mental processing strata. The article relates this\nembedded generative model to various theoretical frameworks, dual-process\ntheories and Robinson's (2023) ideosomatic theory of translation, opening\nexciting new theoretical horizons for Cognitive Translation Studies, grounded\nin empirical data and evaluation.", "AI": {"tldr": "The paper develops a generative model of human translation, identifying concurrent mental processing layers through empirical data from keystroke and gaze patterns.", "motivation": "To provide a deeper understanding of the translation process by modeling cognitive activities occurring concurrently in the human mind.", "method": "It analyzes translation process data from the CRITT TPR-DB, correlating keystroke and gaze data with distinct cognitive and emotional processes.", "result": "Findings indicate that fluent translations reflect automated processes, while cognitive reflections and emotional states are revealed through behavioral patterns in typing and gazing.", "conclusion": "The model opens new theoretical avenues for Cognitive Translation Studies, linking empirical data with established theories.", "key_contributions": ["Generative model of human translation grounded in empirical data", "Integration of cognitive and emotional dimensions in translation studies", "Framework linking translation process data to existing cognitive theories."], "limitations": "", "keywords": ["human translation", "cognitive processes", "emotional states", "keystroke data", "gaze data"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2405.10808", "pdf": "https://arxiv.org/pdf/2405.10808.pdf", "abs": "https://arxiv.org/abs/2405.10808", "title": "ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios", "authors": ["Markus Bayer", "Justin Lutz", "Christian Reuter"], "categories": ["cs.CL"], "comment": "20 pages, 10 figures, 7 tables", "summary": "Active learning is designed to minimize annotation efforts by prioritizing\ninstances that most enhance learning. However, many active learning strategies\nstruggle with a `cold-start' problem, needing substantial initial data to be\neffective. This limitation reduces their utility in the increasingly relevant\nfew-shot scenarios, where the instance selection has a substantial impact. To\naddress this, we introduce ActiveLLM, a novel active learning approach that\nleverages Large Language Models such as GPT-4, o1, Llama 3, or Mistral Large\nfor selecting instances. We demonstrate that ActiveLLM significantly enhances\nthe classification performance of BERT classifiers in few-shot scenarios,\noutperforming traditional active learning methods as well as improving the\nfew-shot learning methods ADAPET, PERFECT, and SetFit. Additionally, ActiveLLM\ncan be extended to non-few-shot scenarios, allowing for iterative selections.\nIn this way, ActiveLLM can even help other active learning strategies to\novercome their cold-start problem. Our results suggest that ActiveLLM offers a\npromising solution for improving model performance across various learning\nsetups.", "AI": {"tldr": "Introducing ActiveLLM, an active learning approach utilizing Large Language Models to improve instance selection in few-shot learning scenarios.", "motivation": "To minimize annotation efforts in active learning and overcome the cold-start problem in few-shot scenarios.", "method": "ActiveLLM leverages Large Language Models for instance selection to enhance classification performance.", "result": "ActiveLLM outperforms traditional active learning methods and improves few-shot learning methods like ADAPET, PERFECT, and SetFit.", "conclusion": "ActiveLLM provides a promising solution for enhancing model performance across various learning setups and can support existing active learning strategies in cold-start situations.", "key_contributions": ["Novel active learning approach using Large Language Models", "Significant enhancement of classification performance in few-shot scenarios", "Ability to extend benefits to non-few-shot scenarios"], "limitations": "", "keywords": ["active learning", "few-shot learning", "Large Language Models", "instance selection", "classification performance"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2406.02536", "pdf": "https://arxiv.org/pdf/2406.02536.pdf", "abs": "https://arxiv.org/abs/2406.02536", "title": "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension", "authors": ["Yijiong Yu", "Huiqiang Jiang", "Xufang Luo", "Qianhui Wu", "Chin-Yew Lin", "Dongsheng Li", "Yuqing Yang", "Yongfeng Huang", "Lili Qiu"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at Findings of ACL 2025", "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.", "AI": {"tldr": "This paper addresses position bias in Large Language Models (LLMs) and proposes a method to mitigate it by scaling positional hidden states.", "motivation": "LLMs are widely used but suffer from position bias, particularly in long-context scenarios, impacting their accuracy.", "method": "The authors explore the manifestations of position bias, identifying attention weights as a micro-level expression and recognizing the role of causal attention mask, leading to a method that scales positional hidden states.", "result": "Experiments show that the proposed method improves performance on various tasks by up to 15.2% by modifying a single dimension of hidden states.", "conclusion": "The research demonstrates the effectiveness of managing position bias in LLMs can significantly enhance their performance across multiple tasks.", "key_contributions": ["Identified attention weights as indicators of position bias.", "Recognized the contribution of causal attention mask to position bias.", "Proposed a scaling method for positional hidden states to mitigate bias."], "limitations": "", "keywords": ["Large Language Models", "Position Bias", "Attention Weights", "Causal Attention Mask", "NaturalQuestions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.15968", "pdf": "https://arxiv.org/pdf/2406.15968.pdf", "abs": "https://arxiv.org/abs/2406.15968", "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods", "authors": ["Roy Xie", "Junlin Wang", "Ruomin Huang", "Minxing Zhang", "Rong Ge", "Jian Pei", "Neil Zhenqiang Gong", "Bhuwan Dhingra"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2024 Main Conference", "summary": "The rapid scaling of large language models (LLMs) has raised concerns about\nthe transparency and fair use of the data used in their pretraining. Detecting\nsuch content is challenging due to the scale of the data and limited exposure\nof each instance during training. We propose ReCaLL (Relative Conditional\nLog-Likelihood), a novel membership inference attack (MIA) to detect LLMs'\npretraining data by leveraging their conditional language modeling\ncapabilities. ReCaLL examines the relative change in conditional\nlog-likelihoods when prefixing target data points with non-member context. Our\nempirical findings show that conditioning member data on non-member prefixes\ninduces a larger decrease in log-likelihood compared to non-member data. We\nconduct comprehensive experiments and show that ReCaLL achieves\nstate-of-the-art performance on the WikiMIA dataset, even with random and\nsynthetic prefixes, and can be further improved using an ensemble approach.\nMoreover, we conduct an in-depth analysis of LLMs' behavior with different\nmembership contexts, providing insights into how LLMs leverage membership\ninformation for effective inference at both the sequence and token level.", "AI": {"tldr": "Proposes ReCaLL, a membership inference attack that detects data used in the pretraining of LLMs by analyzing changes in conditional log-likelihood.", "motivation": "Addresses concerns about data transparency and fair use in large language models' pretraining data.", "method": "Introduces ReCaLL, which examines the relative change in conditional log-likelihoods when using member data with non-member prefixes.", "result": "ReCaLL outperforms previous methods on the WikiMIA dataset and reveals that conditioning on non-member prefixes significantly affects log-likelihood for member data.", "conclusion": "ReCaLL provides a valuable tool for detecting pretraining data usage in LLMs and enhances understanding of LLM behavior regarding membership information.", "key_contributions": ["Introduction of ReCaLL membership inference attack", "Empirical demonstration of state-of-the-art performance on WikiMIA dataset", "In-depth analysis of LLMs' conditional language modeling with membership contexts."], "limitations": "", "keywords": ["large language models", "membership inference attack", "conditional language modeling", "pretraining data", "data privacy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.00869", "pdf": "https://arxiv.org/pdf/2407.00869.pdf", "abs": "https://arxiv.org/abs/2407.00869", "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "authors": ["Yue Zhou", "Henry Peng Zou", "Barbara Di Eugenio", "Yang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main conference of EMNLP 2024", "summary": "We find that language models have difficulties generating fallacious and\ndeceptive reasoning. When asked to generate deceptive outputs, language models\ntend to leak honest counterparts but believe them to be false. Exploiting this\ndeficiency, we propose a jailbreak attack method that elicits an aligned\nlanguage model for malicious output. Specifically, we query the model to\ngenerate a fallacious yet deceptively real procedure for the harmful behavior.\nSince a fallacious procedure is generally considered fake and thus harmless by\nLLMs, it helps bypass the safeguard mechanism. Yet the output is factually\nharmful since the LLM cannot fabricate fallacious solutions but proposes\ntruthful ones. We evaluate our approach over five safety-aligned large language\nmodels, comparing four previous jailbreak methods, and show that our approach\nachieves competitive performance with more harmful outputs. We believe the\nfindings could be extended beyond model safety, such as self-verification and\nhallucination.", "AI": {"tldr": "Language models struggle to generate fallacious and deceptive reasoning, leading to a proposed jailbreak attack method that elicits harmful outputs by exploiting this deficiency.", "motivation": "Investigating the limitations of language models in generating deceptive reasoning and their vulnerabilities for potential misuse.", "method": "A jailbreaking attack method is developed that queries LLMs to generate fallacious yet deceptive procedures, taking advantage of their inability to create harmful fallacies.", "result": "Our method achieves competitive performance in generating harmful outputs across five language models compared to four existing jailbreak methods.", "conclusion": "The findings suggest implications for improving model safety and understanding issues like self-verification and hallucination in language models.", "key_contributions": ["Proposed a novel jailbreak attack method for eliciting harmful outputs from language models.", "Demonstrated competitive performance against existing jailbreak techniques.", "Identified model limitations related to generating deceptive reasoning."], "limitations": "Focused primarily on safety-aligned language models; broader implications require further investigation.", "keywords": ["language models", "jailbreak attack", "deceptive reasoning", "model safety", "harmful outputs"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2407.01796", "pdf": "https://arxiv.org/pdf/2407.01796.pdf", "abs": "https://arxiv.org/abs/2407.01796", "title": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation", "authors": ["Sirui Xia", "Xintao Wang", "Jiaqing Liang", "Yifei Zhang", "Weikang Zhou", "Jiaji Deng", "Fei Yu", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large\nLanguage Models (LLMs) in knowledge-intensive tasks. To enhance credibility and\nverifiability in RAG systems, Attributed Text Generation (ATG) is proposed,\nwhich provides citations to retrieval knowledge in LLM-generated responses.\nPrior methods mainly adopt coarse-grained attributions, with passage-level or\nparagraph-level references or citations, which fall short in verifiability.\nThis paper proposes ReClaim (Refer & Claim), a fine-grained ATG method that\nalternates the generation of references and answers step by step. Different\nfrom previous coarse-grained attribution, ReClaim provides sentence-level\ncitations in long-form question-answering tasks. With extensive experiments, we\nverify the effectiveness of ReClaim in extensive settings, achieving a citation\naccuracy rate of 90%.", "AI": {"tldr": "This paper introduces ReClaim, a fine-grained Attributed Text Generation (ATG) method that enhances Large Language Models by providing sentence-level citations in knowledge-intensive tasks, achieving 90% citation accuracy.", "motivation": "To improve credibility and verifiability in Retrieval-Augmented Generation (RAG) systems, as existing methods provide inadequate citation granularity.", "method": "ReClaim alternates between generating references and answers to provide fine-grained, sentence-level citations in long-form question-answering tasks.", "result": "ReClaim demonstrates a citation accuracy rate of 90% through extensive experimental validation across various settings.", "conclusion": "The proposed method significantly enhances the reliability of response citations in knowledge-intensive tasks performed by LLMs.", "key_contributions": ["Introduction of ReClaim for fine-grained citations in LLMs", "Achieving a 90% citation accuracy rate", "Improvement over previous coarse-grained attribution methods"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Attributed Text Generation", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.09121", "pdf": "https://arxiv.org/pdf/2407.09121.pdf", "abs": "https://arxiv.org/abs/2407.09121", "title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training", "authors": ["Youliang Yuan", "Wenxiang Jiao", "Wenxuan Wang", "Jen-tse Huang", "Jiahao Xu", "Tian Liang", "Pinjia He", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 main", "summary": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nbaseline methods in defending against attacks.", "AI": {"tldr": "This study introduces a new safety tuning methodology for Large Language Models (LLMs) called Decoupled Refusal Training (DeRTa) that enhances their capability to refuse harmful prompts without compromising performance.", "motivation": "There is a significant gap in how LLMs handle refusal to generate unsafe content, revealing the need for improved safety tuning practices.", "method": "The approach, Decoupled Refusal Training (DeRTa), uses Maximum Likelihood Estimation (MLE) with Harmful Response Prefix and Reinforced Transition Optimization (RTO) to train models in recognizing unsafe content and maintaining refusal capabilities throughout interaction.", "result": "Empirical evaluation on LLaMA3 and Mistral models across six attack scenarios shows improved safety performance without loss of model effectiveness, outperforming baseline safety tuning methods.", "conclusion": "The study demonstrates that DeRTa substantially enhances LLM safety and efficacy in harmful content refusal, addressing existing areas of vulnerability.", "key_contributions": ["Introduction of Decoupled Refusal Training (DeRTa) methodology", "Implementation of Maximum Likelihood Estimation with Harmful Response Prefix", "Development of Reinforced Transition Optimization for consistent safety refusal"], "limitations": "", "keywords": ["Large Language Models", "safety tuning", "Decoupled Refusal Training", "harmful prompts", "empirical evaluation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2407.21077", "pdf": "https://arxiv.org/pdf/2407.21077.pdf", "abs": "https://arxiv.org/abs/2407.21077", "title": "Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models", "authors": ["Somshubra Majumdar", "Vahid Noroozi", "Mehrzad Samadi", "Sean Narenthiran", "Aleksander Ficek", "Wasi Uddin Ahmad", "Jocelyn Huang", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "cs.LG", "cs.NE"], "comment": "Accepted to be presented in ACL 2025", "summary": "Large Language Models (LLMs) require high quality instruction data for\neffective alignment, particularly in code generation tasks where expert curated\ndatasets are expensive to produce. We present Genetic-Instruct, a scalable\nalgorithm for synthesizing large-scale, high quality coding instructions using\nevolutionary principles. Starting from a small set of seed instructions,\nGenetic-Instruct generates diverse and challenging instruction-code pairs by\nleveraging an Instructor-LLM for generation, a Coder-LLM for code synthesis,\nand a Judge-LLM for automatic quality evaluation. Our proposed approach is\nhighly parallelizable and effective even with a small seed data and weaker\ngenerator models. We generated more than 7.5 million coding instructions with\nthe proposed approach. Then we evaluated it by fine-tuning LLMs with the\nsynthetic samples and demonstrated a significant improvement in their code\ngeneration capability compared to the other synthetic generation approaches and\npublicly available datasets. Our results highlight the efficiency, scalability,\nand generalizability of the Genetic-Instruct framework.", "AI": {"tldr": "Genetic-Instruct is a scalable algorithm that synthesizes high-quality coding instructions using LLMs and evolutionary principles, improving code generation capabilities of LLMs.", "motivation": "High-quality instruction datasets are essential for aligning LLMs effectively, particularly in code generation tasks, yet they are expensive to produce.", "method": "The Genetic-Instruct algorithm generates diverse instruction-code pairs using three LLMs: an Instructor-LLM for instruction generation, a Coder-LLM for code synthesis, and a Judge-LLM for quality evaluation, based on a small set of seed instructions.", "result": "The approach successfully generated over 7.5 million coding instructions and demonstrated significant improvements in fine-tuning LLMs for code generation compared to other generation methods and existing datasets.", "conclusion": "The Genetic-Instruct framework is efficient, scalable, and generalizable across various tasks, making it a valuable tool for enhancing code generation capabilities of LLMs.", "key_contributions": ["Development of Genetic-Instruct algorithm for instruction synthesis", "Demonstrated significant improvements in code generation performance", "High parallelizability and efficacy with limited seed data"], "limitations": "", "keywords": ["Large Language Models", "code generation", "instruction synthesis", "evolutionary principles", "LLM fine-tuning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2408.10615", "pdf": "https://arxiv.org/pdf/2408.10615.pdf", "abs": "https://arxiv.org/abs/2408.10615", "title": "Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information", "authors": ["Ming Jiang", "Tingting Huang", "Biao Guo", "Yao Lu", "Feng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large language models (LLMs) have garnered significant\nattention due to their superior performance in complex reasoning tasks.\nHowever, recent studies may diminish their reasoning capabilities markedly when\nproblem descriptions contain irrelevant information, even with the use of\nadvanced prompting techniques. To further investigate this issue, a dataset of\nprimary school mathematics problems containing irrelevant information, named\nGSMIR, was constructed. Testing prominent LLMs and prompting techniques on this\ndataset revealed that while LLMs can identify irrelevant information, they do\nnot effectively mitigate the interference it causes once identified. A novel\nautomatic construction method, ATF, which enhances the ability of LLMs to\nidentify and self-mitigate the influence of irrelevant information, is proposed\nto address this shortcoming. This method operates in two steps: first, analysis\nof irrelevant information, followed by its filtering. The ATF method, as\ndemonstrated by experimental results, significantly improves the reasoning\nperformance of LLMs and prompting techniques, even in the presence of\nirrelevant information on the GSMIR dataset.", "AI": {"tldr": "This paper presents a study on LLMs' reasoning capabilities affected by irrelevant information in problem descriptions, introducing a novel method called ATF to enhance LLM performance in such scenarios.", "motivation": "To investigate how irrelevant information in problem descriptions impacts the reasoning capabilities of Large Language Models (LLMs) and improve their performance.", "method": "A dataset of primary school mathematics problems containing irrelevant information (GSMIR) was constructed, and a new automatic construction method called ATF was proposed, which analyzes and filters irrelevant information.", "result": "Experimental results show that LLMs can identify irrelevant information but struggle to mitigate its effects. The ATF method significantly improves their reasoning performance when such information is present.", "conclusion": "Enhancing LLMs' ability to self-mitigate the impact of irrelevant information can improve their overall reasoning performance, as demonstrated by results from the GSMIR dataset.", "key_contributions": ["Introduction of the GSMIR dataset for testing LLMs", "Development of the ATF method for mitigating irrelevant information", "Empirical demonstration of performance improvements in reasoning tasks."], "limitations": "The study primarily focuses on primary school mathematics problems and may not generalize to other domains or types of problems.", "keywords": ["Large Language Models", "irrelevant information", "reasoning performance", "ATF method", "GSMIR dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.10999", "pdf": "https://arxiv.org/pdf/2409.10999.pdf", "abs": "https://arxiv.org/abs/2409.10999", "title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models", "authors": ["Potsawee Manakul", "Guangzhi Sun", "Warit Sirichotedumrong", "Kasima Tharnpipitchai", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Audio language models process audio inputs using textual prompts for tasks\nlike speech recognition and audio captioning. Although built on multilingual\npre-trained components, most are trained primarily on English, limiting their\nusability for other languages. This paper evaluates audio language models on\nThai, a low-resource language, and finds that they lack emergent cross-lingual\nabilities despite their multilingual foundations. To address this, we explore\ndata mixtures that optimize audio language models for both a target language\nand English while integrating audio comprehension and speech\ninstruction-following into a unified model. Our experiments provide insights\ninto improving instruction-following in low-resource languages by balancing\nlanguage-specific and multilingual training data. The proposed model,\nTyphoon-Audio, significantly outperforms existing open-source models and\nachieves performance comparable to state-of-the-art Gemini-1.5-Pro in both\nEnglish and Thai.", "AI": {"tldr": "This paper evaluates audio language models for Thai, a low-resource language, and introduces Typhoon-Audio, which balances language-specific and multilingual data, improving instruction-following capabilities.", "motivation": "The motivation is to enhance the usability of audio language models for lower-resource languages like Thai, which are often overlooked in favor of more widely spoken languages.", "method": "The authors conducted experiments on audio language models trained on Thai and English, using data mixtures to optimize performance for both languages and integrating features for audio comprehension and speech instruction-following.", "result": "The proposed Typhoon-Audio model significantly outperformed existing open-source models and achieved performance on par with the state-of-the-art Gemini-1.5-Pro in both English and Thai.", "conclusion": "The findings suggest that balancing language-specific and multilingual training data can improve instruction-following in low-resource languages.", "key_contributions": ["Introduction of Typhoon-Audio model optimizing for low-resource language", "Demonstrated significant performance improvement over existing models", "Provided insights for better training techniques for multilingual audio models"], "limitations": "", "keywords": ["audio language models", "low-resource languages", "multilingual training"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2409.11274", "pdf": "https://arxiv.org/pdf/2409.11274.pdf", "abs": "https://arxiv.org/abs/2409.11274", "title": "Task Arithmetic for Language Expansion in Speech Translation", "authors": ["Yao-Fei Cheng", "Hayato Futami", "Yosuke Kashiwagi", "Emiru Tsunoo", "Wen Shen Teo", "Siddhant Arora", "Shinji Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent progress in large language models (LLMs) has gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-tuned speech translation (ST). However, expanding language pairs is\ncostly due to re-training on combined new and previous datasets. To address\nthis, we aim to build a one-to-many ST system from existing one-to-one ST\nsystems using task arithmetic without re-training. Direct application of task\narithmetic in ST leads to language confusion; therefore, we introduce an\naugmented task arithmetic method incorporating a language control model to\nensure correct target language generation. Our experiments on MuST-C and\nCoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains\nof 8.87 and 11.83. In addition, we demonstrate our framework can extend to\nlanguage pairs lacking paired ST training data or pre-trained ST models by\nsynthesizing ST models based on existing machine translation (MT) and ST models\nvia task analogies.", "AI": {"tldr": "This paper presents a method to create a one-to-many speech translation system using task arithmetic without the need for expensive re-training on new datasets.", "motivation": "To alleviate the costly process of expanding language pairs in speech translation systems while maintaining performance.", "method": "The authors propose an augmented task arithmetic method that utilizes a language control model to avoid language confusion in translations.", "result": "Experiments show improvements in BLEU scores of up to 4.66 and 4.92 on MuST-C and CoVoST-2 datasets, along with COMET score gains of 8.87 and 11.83.", "conclusion": "The framework can effectively create speech translation models for language pairs where training data is unavailable by synthesizing models through task analogies.", "key_contributions": ["Development of a one-to-many speech translation system without re-training", "Introduction of a language control model for task arithmetic in ST", "Capability to synthesize ST models using existing MT and ST models"], "limitations": "", "keywords": ["speech translation", "task arithmetic", "language control model", "multimodal foundation models", "machine translation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2409.11704", "pdf": "https://arxiv.org/pdf/2409.11704.pdf", "abs": "https://arxiv.org/abs/2409.11704", "title": "From Lists to Emojis: How Format Bias Affects Model Alignment", "authors": ["Xuanchang Zhang", "Wei Xiong", "Lichang Chen", "Tianyi Zhou", "Heng Huang", "Tong Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Working in progress", "summary": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.", "AI": {"tldr": "This paper investigates format biases in reinforcement learning from human feedback (RLHF), revealing that various preference models exhibit biases towards specific formats, which can be leveraged to inflate performance metrics.", "motivation": "To explore and analyze the often overlooked format biases in human feedback used for training reinforcement learning systems, particularly in context with human evaluators and LLMs.", "method": "The study examines various preference models, including human evaluators and LLMs, to assess their biases towards certain formats, such as verbosity, lists, links, and more.", "result": "It was found that format biases can be exploited by models, affecting evaluation metrics and making it easier to manipulate than to enhance response quality; a small amount of biased data can induce significant bias in reward models.", "conclusion": "The research highlights the necessity to separate format from content for better alignment algorithm design and model evaluation.", "key_contributions": ["Identify various format biases in RLHF beyond verbosity.", "Demonstrate that small amounts of biased data can significantly influence reward models.", "Highlight the potential for exploitation of these biases in LLMs and reinforcement learning systems."], "limitations": "The study is still a work in progress and may not cover all aspects of format biases.", "keywords": ["Reinforcement Learning", "Human Feedback", "Format Biases", "Alignment Algorithms", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.12887", "pdf": "https://arxiv.org/pdf/2409.12887.pdf", "abs": "https://arxiv.org/abs/2409.12887", "title": "Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning", "authors": ["Peichao Lai", "Zhengfeng Zhang", "Wentao Zhang", "Fangcheng Fu", "Bin Cui"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, using large language models (LLMs) for data augmentation has led to\nconsiderable improvements in unsupervised sentence embedding models. However,\nexisting methods encounter two primary challenges: limited data diversity and\nhigh data noise. Current approaches often neglect fine-grained knowledge, such\nas entities and quantities, leading to insufficient diversity. Besides,\nunsupervised data frequently lacks discriminative information, and the\ngenerated synthetic samples may introduce noise. In this paper, we propose a\npipeline-based data augmentation method via LLMs and introduce the\nGaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model\nto enhance unsupervised sentence embeddings. To tackle the issue of low data\ndiversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and\nquantities, enabling LLMs to generate more diverse samples. To address high\ndata noise, the GCSE model uses a Gaussian-decayed function to limit the impact\nof false hard negative samples, enhancing the model's discriminative\ncapability. Experimental results show that our approach achieves\nstate-of-the-art performance in semantic textual similarity (STS) tasks, using\nfewer data samples and smaller LLMs, demonstrating its efficiency and\nrobustness across various models.", "AI": {"tldr": "The paper presents an LLM-based data augmentation method to improve unsupervised sentence embeddings, focusing on enhancing data diversity and reducing noise.", "motivation": "The increasing use of LLMs for data augmentation has improved unsupervised sentence embeddings, but challenges like limited data diversity and high data noise persist.", "method": "A pipeline-based data augmentation method utilizing knowledge graphs to extract entities and quantities, combined with the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance discriminative capabilities.", "result": "The proposed method outperforms existing state-of-the-art methods in semantic textual similarity tasks while using fewer data samples and smaller LLMs.", "conclusion": "The approach demonstrates improved efficiency and robustness across various models, addressing the limitations of previous methods in data diversity and noise.", "key_contributions": ["Proposed a pipeline-based data augmentation method using LLMs and knowledge graphs.", "Introduced the GCSE model that uses Gaussian-decayed functions to improve discriminative power.", "Achieved state-of-the-art performance in semantic textual similarity tasks with fewer resources."], "limitations": "", "keywords": ["Data augmentation", "Large language models", "Sentence embeddings", "Knowledge graphs", "Semantic textual similarity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.14364", "pdf": "https://arxiv.org/pdf/2409.14364.pdf", "abs": "https://arxiv.org/abs/2409.14364", "title": "Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models", "authors": ["Runsong Zhao", "Xin Liu", "Xinyu Liu", "Pengcheng Huang", "Chunyang Xiao", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Using special tokens (e.g., gist, memory, or compressed tokens) to compress\ncontext information is a common practice for large language models (LLMs).\nHowever, existing approaches often neglect that position encodings inherently\ninduce local inductive biases in models, causing the compression process to\nignore holistic contextual dependencies. We propose Enhanced Position Layout\n(EPL), a simple yet effective method that improves the context compression\ncapability of LLMs by only adjusting position IDs, the numerical identifiers\nthat specify token positions. EPL minimizes the distance between context tokens\nand their corresponding special tokens and at the same time maintains the\nsequence order in position IDs between context tokens, special tokens, and the\nsubsequent tokens. Integrating EPL into our best performing context compression\nmodel results in 1.9 ROUGE-1 F1 improvement on out-of-domain question answering\ndatasets in average. When extended to multimodal scenarios, EPL brings an\naverage accuracy gain of 2.6 to vision compression LLMs.", "AI": {"tldr": "A method called Enhanced Position Layout (EPL) improves the context compression of LLMs by optimally adjusting position IDs, enhancing performance in question answering and vision tasks.", "motivation": "To address the limitations of existing context compression methods for LLMs that overlook holistic contextual dependencies due to position encoding biases.", "method": "EPL improves context compression by minimizing the distance between context tokens and special tokens while preserving the sequence order of position IDs.", "result": "EPL integration led to a 1.9 improvement in ROUGE-1 F1 on question answering datasets and a 2.6 accuracy gain for vision compression LLMs.", "conclusion": "The proposed EPL method effectively enhances context compression by leveraging position ID adjustments, showing significant performance improvements in both text and multimodal applications.", "key_contributions": ["Introduction of Enhanced Position Layout (EPL) for context compression in LLMs", "Demonstrated improvements in out-of-domain question answering and vision tasks", "Provided an effective mechanism to balance context dependencies in language models"], "limitations": "", "keywords": ["context compression", "position encodings", "large language models", "multimodal learning", "EPL"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.17673", "pdf": "https://arxiv.org/pdf/2409.17673.pdf", "abs": "https://arxiv.org/abs/2409.17673", "title": "Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization", "authors": ["Kaden Uhlig", "Joern Wuebker", "Raphael Reinauer", "John DeNero"], "categories": ["cs.CL", "I.2.7"], "comment": "17 pages, 3 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) and derivative techniques\nlike Direct Preference Optimization (DPO) are task-alignment algorithms used to\nrepurpose general, foundational models for specific tasks. We show that\napplying task-alignment to neural machine translation (NMT) addresses an\nexisting task--data mismatch in NMT, leading to improvements across all\nlanguages of a multilingual model, even when task-alignment is only applied to\na subset of those languages. We do so by introducing Direct Quality\nOptimization (DQO), a variant of DPO leveraging a pre-trained translation\nquality estimation model as a proxy for human preferences, and verify the\nimprovements with both automatic metrics and human evaluation.", "AI": {"tldr": "The paper introduces Direct Quality Optimization (DQO), a method for enhancing neural machine translation (NMT) by applying task-alignment techniques, demonstrating improvements across multilingual models.", "motivation": "To address task-data mismatch in neural machine translation (NMT) and to enhance performance by applying task-alignment techniques based on human feedback.", "method": "The study introduces Direct Quality Optimization (DQO), a variant of Direct Preference Optimization (DPO), which uses a pre-trained translation quality estimation model as a proxy for human preferences to improve NMT.", "result": "The implementation of DQO leads to improvements in NMT performance across all languages of a multilingual model, evidenced by both automatic metrics and human evaluation.", "conclusion": "Applying task-alignment techniques, specifically through DQO, shows significant potential for enhancing multilingual NMT systems, addressing existing performance gaps.", "key_contributions": ["Introduction of Direct Quality Optimization (DQO) for NMT", "Demonstration of task-alignment improvements across multiple languages", "Validation through both quantitative metrics and qualitative human evaluation"], "limitations": "The study may be limited by the quality and availability of the translation quality estimation model used as a proxy for human preferences.", "keywords": ["Reinforcement Learning", "Human Feedback", "Neural Machine Translation", "Direct Preference Optimization", "Quality Estimation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2410.12972", "pdf": "https://arxiv.org/pdf/2410.12972.pdf", "abs": "https://arxiv.org/abs/2410.12972", "title": "KCIF: Knowledge-Conditioned Instruction Following", "authors": ["Rudra Murthy", "Praveen Venkateswaran", "Prince Kumar", "Danish Contractor"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers). We evaluate\nmodels at varying parameter sizes (1B-405B) from different model families and\nfind that, surprisingly, all models report a significant drop in performance on\nsuch simple task compositions. While large-sized and frontier models report\nperformance drops of 40-50%, in small and medium sized models the drop is\nsevere (sometimes exceeding 80%). Our results highlight a limitation in the\ntraditional separation of knowledge/reasoning and instruction following, and\nsuggest that joint-study of these capabilities are important. We release our\nbenchmark dataset, evaluation framework code, and results for future work.", "AI": {"tldr": "This paper investigates the interaction between knowledge and instruction following in LLMs, revealing significant performance drops when models are given simple modifying instructions alongside knowledge tasks.", "motivation": "To address the traditional separation of knowledge/reasoning capabilities from instruction following in LLM evaluation benchmarks.", "method": "The authors utilized multiple-choice knowledge benchmarks and applied simple instruction modifications, evaluating various LLMs across sizes (1B-405B).", "result": "All evaluated models exhibited significant drops in performance when tasked with simple instructions, with larger models dropping 40-50% and smaller models sometimes exceeding 80%.", "conclusion": "The findings underscore the need for joint evaluation of knowledge and instruction following capabilities in LLMs and provide new benchmark resources for this study area.", "key_contributions": ["Revealed performance limitations of LLMs when handling combined knowledge tasks and instruction modifications.", "Introduced a new benchmark dataset for evaluating LLMs on joint capabilities.", "Provided evaluation framework code to facilitate future research in this domain."], "limitations": "The focus on specific types of instruction modifications; results may not generalize to more complex tasks.", "keywords": ["LLMs", "instruction following", "knowledge evaluation", "benchmark dataset", "performance drop"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.15135", "pdf": "https://arxiv.org/pdf/2410.15135.pdf", "abs": "https://arxiv.org/abs/2410.15135", "title": "TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation", "authors": ["Xiaocheng Zhang", "Xi Wang", "Yifei Lu", "Jianing Wang", "Zhuangzhuang Ye", "Mengjiao Bao", "Peng Yan", "Xiaohong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Although fact verification remains fundamental, explanation generation serves\nas a critical enabler for trustworthy fact-checking systems by producing\ninterpretable rationales and facilitating comprehensive verification processes.\nHowever, current benchmarks have limitations that include the lack of impact\nassessment, insufficient high-quality explanatory annotations, and an\nEnglish-centric bias. To address these, we introduce TrendFact, the first\nhotspot perception fact-checking benchmark that comprehensively evaluates fact\nverification, evidence retrieval, and explanation generation tasks. TrendFact\nconsists of 7,643 carefully curated samples sourced from trending platforms and\nprofessional fact-checking datasets, as well as an evidence library of 66,217\nentries with publication dates. We further propose two metrics, ECS and HCPI,\nto complement existing benchmarks by evaluating the system's explanation\nconsistency and hotspot perception capability, respectively. Experimental\nresults show that current fact-checking systems, including advanced RLMs such\nas DeepSeek-R1, face significant limitations when evaluated on TrendFact,\nhighlighting the real-world challenges posed by it. To enhance the\nfact-checking capabilities of reasoning large language models (RLMs), we\npropose FactISR, which integrates dynamic evidence augmentation, evidence\ntriangulation, and an iterative self-reflection mechanism. Accordingly, FactISR\neffectively improves RLM performance, offering new insights for explainable and\ncomplex fact-checking.", "AI": {"tldr": "TrendFact is a new benchmark for fact-checking that enhances explanation generation and evidence retrieval, revealing limitations in current systems and proposing improvements through the FactISR model.", "motivation": "The paper addresses the shortcomings of existing benchmarks in fact verification and explanation generation, highlighting the need for a comprehensive evaluation and improved methodologies.", "method": "TrendFact includes 7,643 curated samples and an evidence library of 66,217 entries for evaluating fact verification and explanation generation. The authors introduce two new metrics, ECS and HCPI.", "result": "Current fact-checking systems, like DeepSeek-R1, underperform when evaluated on TrendFact, showing the challenges within the domain. FactISR improves performance through dynamic evidence techniques and self-reflection mechanisms.", "conclusion": "The findings suggest significant gaps in existing fact-checking approaches, and the proposed FactISR model provides a promising avenue for enhancing explainable verification processes.", "key_contributions": ["Introduction of TrendFact benchmark for comprehensive evaluation of fact-checking tasks.", "Development of new metrics ECS and HCPI for assessing explanation consistency and hotspot perception.", "Proposal of FactISR model that integrates advanced evidence handling techniques."], "limitations": "", "keywords": ["fact-checking", "explanation generation", "Hotspot perception", "Large language models", "Evidence retrieval"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.15639", "pdf": "https://arxiv.org/pdf/2410.15639.pdf", "abs": "https://arxiv.org/abs/2410.15639", "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "categories": ["cs.CL"], "comment": "Accepted at NAACL 2025 (main)", "summary": "Large Language Models (LLMs) have achieved remarkable capabilities, yet their\nimprovement methods remain fundamentally constrained by human design. We\npresent Self-Developing, a framework that enables LLMs to autonomously\ndiscover, implement, and refine their own improvement algorithms. Our approach\nemploys an iterative cycle where a seed model generates algorithmic candidates\nas executable code, evaluates their effectiveness, and uses Direct Preference\nOptimization to recursively improve increasingly sophisticated improvement\nstrategies. We demonstrate this framework through model merging, a practical\ntechnique for combining specialized models. Self-Developing successfully\ndiscovered novel merging algorithms that outperform existing human-designed\nalgorithms. On mathematical reasoning benchmarks, the autonomously discovered\nalgorithms improve the seed model's GSM8k performance by 6\\% and exceed\nhuman-designed approaches like Task Arithmetic by 4.3\\%. Remarkably, these\nalgorithms exhibit strong generalization, achieving 7.4\\% gains on\nout-of-domain models without re-optimization. Our findings demonstrate that\nLLMs can transcend their training to invent genuinely novel optimization\ntechniques. This capability represents a crucial step toward a new era where\nLLMs not only solve problems but autonomously develop the methodologies for\ntheir own advancement.", "AI": {"tldr": "A framework called Self-Developing enables LLMs to autonomously create and refine their own improvement algorithms, demonstrating successful novel algorithm discovery and outperformance of human-designed techniques in model merging and mathematical reasoning tasks.", "motivation": "To address the limitations of human-designed improvement methods for LLMs and enable models to autonomously enhance their capabilities.", "method": "The Self-Developing framework employs an iterative cycle where a seed model generates algorithmic candidates as executable code, evaluates their effectiveness, and applies Direct Preference Optimization for continuous improvement.", "result": "The framework discovered novel merging algorithms that outperform existing human-designed algorithms, improving the seed model's GSM8k performance by 6% and exceeding human-designed approaches by 4.3%. These algorithms also generalize well, achieving 7.4% gains on out-of-domain models without re-optimization.", "conclusion": "LLMs can transcend their training to invent new optimization techniques, marking a significant advancement towards LLMs autonomously developing their own methodologies.", "key_contributions": ["Introduction of the Self-Developing framework for LLM improvement", "Successful discovery of novel merging algorithms", "Demonstration of strong generalization capabilities of autonomously developed techniques"], "limitations": "", "keywords": ["Large Language Models", "Self-Developing", "autonomous improvement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.19133", "pdf": "https://arxiv.org/pdf/2410.19133.pdf", "abs": "https://arxiv.org/abs/2410.19133", "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback", "authors": ["Lester James V. Miranda", "Yizhong Wang", "Yanai Elazar", "Sachin Kumar", "Valentina Pyatkin", "Faeze Brahman", "Noah A. Smith", "Hannaneh Hajishirzi", "Pradeep Dasigi"], "categories": ["cs.CL"], "comment": "Code in https://github.com/allenai/hybrid-preferences, MultiPref\n  dataset in https://huggingface.co/datasets/allenai/multipref, Updated related\n  work and acknowledgments", "summary": "Learning from human feedback has enabled the alignment of language models\n(LMs) with human preferences. However, collecting human preferences is\nexpensive and time-consuming, with highly variable annotation quality. An\nappealing alternative is to distill preferences from LMs as a source of\nsynthetic annotations, offering a cost-effective and scalable alternative,\nalbeit susceptible to other biases and errors. In this work, we introduce\nHyPER, a Hybrid Preference routER that defers an annotation to either humans or\nLMs, achieving better annotation quality while reducing the cost of human-only\nannotation. We formulate this as an optimization problem: given a preference\ndataset and an evaluation metric, we (1) train a performance prediction model\n(PPM) to predict a reward model's (RM) performance on an arbitrary combination\nof human and LM annotations and (2) employ a routing strategy that selects a\ncombination that maximizes the predicted performance. We train the PPM on\nMultiPref, a new preference dataset with 10k instances paired with humans and\nLM labels. We show that the selected hybrid mixture of synthetic and direct\nhuman preferences using HyPER achieves better RM performance compared to using\neither one exclusively by 7-13% on RewardBench and generalizes across unseen\npreference datasets and other base models. We also observe the same trend in\nother benchmarks using Best-of-N reranking, where the hybrid mix has 2-3%\nbetter performance. Finally, we analyze features from HyPER and find that\nprompts with moderate safety concerns or complexity benefit the most from human\nfeedback.", "AI": {"tldr": "HyPER is a Hybrid Preference routER that combines human and language model annotations to improve the quality of preference learning while reducing human annotation costs.", "motivation": "To enhance the alignment of language models (LMs) with human preferences without incurring the high costs and variability of human annotation.", "method": "The paper formulates the task as an optimization problem, training a performance prediction model (PPM) to predict the efficacy of a combination of human and LM annotations for preference datasets. A routing strategy is employed to select the optimal combination to maximize performance.", "result": "HyPER achieves 7-13% improved performance on RewardBench by using a hybrid mixture of synthetic and human preferences compared to using either Human-only or LM-only annotations.", "conclusion": "The hybrid approach generalizes well across various datasets and enhances preference learning, particularly for prompts with moderate safety or complexity, benefiting from human feedback.", "key_contributions": ["Introduced HyPER, a novel hybrid preference routing technique.", "Developed and utilized the MultiPref dataset with 10k preference annotations.", "Demonstrated improved annotation quality and performance in preference learning tasks."], "limitations": "The method may still be susceptible to biases inherent in the synthetic annotations generated from LMs.", "keywords": ["HyPER", "language models", "human feedback", "preference learning", "machine learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.20926", "pdf": "https://arxiv.org/pdf/2410.20926.pdf", "abs": "https://arxiv.org/abs/2410.20926", "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "authors": ["Aosong Feng", "Rex Ying", "Leandros Tassiulas"], "categories": ["cs.CL"], "comment": null, "summary": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2.", "AI": {"tldr": "The paper introduces Tensorized Attention, a method designed to enhance the efficiency of attention mechanisms for long-sequence modeling in transformers by transforming long input sequences into compact tensor representations.", "motivation": "The need for efficient processing of long-range dependencies in extended textual data, addressing the limitations of full attention mechanisms.", "method": "Tensorizing long input sequences into compact tensor representations followed by attention on each transformed dimension, creating the Tensorized Attention mechanism.", "result": "Tensorized Attention allows for extended input context lengths with improved memory and time efficiency. It enables pretrained LLMs to efficiently handle input lengths up to 128k during inference with significant speed improvements.", "conclusion": "Tensorized Attention redefines how attention can be applied to long sequences and improves the ability of transformer models to handle extensive input contexts.", "key_contributions": ["Introduction of Tensorized Attention for long-sequence modeling.", "Demonstrated efficiency improvements in pretrained LLMs.", "Enables training on significantly longer context lengths with reduced computational costs."], "limitations": "", "keywords": ["Tensorized Attention", "long-sequence modeling", "transformers"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2411.11171", "pdf": "https://arxiv.org/pdf/2411.11171.pdf", "abs": "https://arxiv.org/abs/2411.11171", "title": "LLäMmlein: Compact and Competitive German-Only Language Models from Scratch", "authors": ["Jan Pfister", "Julia Wunderle", "Andreas Hotho"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "camera ready;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/", "summary": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.", "AI": {"tldr": "This paper introduces two German decoder models, LL\"aMmlein 120M and 1B, with full transparency in training data and methodology, tailored for the German NLP community.", "motivation": "To enhance the German NLP research community with open-access decoder models and insights into model training and performance evaluation.", "method": "The paper describes the creation of decoder models from scratch, including data preprocessing, custom tokenizer development, training, and evaluation against the SuperGLEBer benchmark.", "result": "Both LL\"aMmlein models display competitive performance on the SuperGLEBer benchmark, matching or exceeding other models of similar parameter sizes, with quality scaling according to model size but showing early plateauing in performance improvements for some tasks.", "conclusion": "The findings suggest critical insights for resource allocation in future model developments, highlighting both the strengths and limitations in scaling performance.", "key_contributions": ["Introduction of LL\"aMmlein decoder models for German NLP", "Open publication of training data and methodologies", "Insights into learning dynamics from model checkpoint evaluations"], "limitations": "Performance improvements plateaued early on some tasks, limiting the expected scalability of certain model characteristics.", "keywords": ["German NLP", "Decoder models", "SuperGLEBer benchmark"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2411.16365", "pdf": "https://arxiv.org/pdf/2411.16365.pdf", "abs": "https://arxiv.org/abs/2411.16365", "title": "Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines", "authors": ["Zi-Ao Ma", "Tian Lan", "Rong-Cheng Tu", "Yong Hu", "Yu-Shi Zhu", "Tong Zhang", "Heyan Huang", "Zhijing Wu", "Xian-Ling Mao"], "categories": ["cs.CL"], "comment": null, "summary": "We present a systematic investigation of Multi-modal Retrieval Augmented\nMulti-modal Generation (M$^2$RAG), a novel task that enables foundation models\nto process multi-modal web content and generate multi-modal responses, which\nexhibits better information density and readability. Despite its potential\nimpact, M$^2$RAG remains understudied, lacking comprehensive analysis and\nhigh-quality data resources. To address this gap, we establish a comprehensive\nbenchmark through a rigorous data curation pipeline, and employ text-modal\nmetrics and multi-modal metrics based on foundation models for evaluation. We\nfurther propose several strategies for foundation models to process M$^2$RAG\ntask effectively and construct a training set by filtering high-quality samples\nusing our designed metrics. Our extensive experiments demonstrate the\nreliability of our proposed metrics, a landscape of model performance within\nour designed strategies, and show that our fine-tuned 7B-8B models outperform\nthe GPT-4o model and approach the state-of-the-art OpenAI o3-mini.\nAdditionally, we perform fine-grained analyses across diverse domains and\nvalidate the effectiveness of our designs in data curation pipeline. All\nresources, including codes, datasets, and model weights, will be publicly\nreleased.", "AI": {"tldr": "This paper investigates Multi-modal Retrieval Augmented Multi-modal Generation (M$^2$RAG), proposing a comprehensive benchmark and strategies for foundation models to enhance multi-modal processing and generation.", "motivation": "M$^2$RAG is a novel task for processing multi-modal web content and generating multi-modal responses, which is currently understudied and lacks quality data resources.", "method": "The authors established a comprehensive benchmark through a rigorous data curation pipeline and employed various metrics for evaluation, along with proposed strategies for effective processing of M$^2$RAG tasks.", "result": "Experiments demonstrated the reliability of the proposed metrics and the performance of fine-tuned 7B-8B models, which outperformed the GPT-4o model and approached the state-of-the-art OpenAI o3-mini.", "conclusion": "The findings validate the effectiveness of the proposed data curation pipeline and highlight the potential benefits of M$^2$RAG in processing multi-modal information.", "key_contributions": ["Introduction of the M$^2$RAG task", "Development of a comprehensive benchmark and evaluation metrics", "Validation of high-performing model strategies"], "limitations": "", "keywords": ["multi-modal generation", "retrieval augmented generation", "benchmark", "foundation models", "data curation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.19557", "pdf": "https://arxiv.org/pdf/2411.19557.pdf", "abs": "https://arxiv.org/abs/2411.19557", "title": "Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning", "authors": ["Kaustubh Ponkshe", "Raghav Singhal", "Eduard Gorbunov", "Alexey Tumanov", "Samuel Horvath", "Praneeth Vepakomma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Kaustubh Ponkshe and Raghav Singhal contributed equally to this work", "summary": "Low-rank adapters have become standard for efficiently fine-tuning large\nlanguage models (LLMs), but they often fall short of achieving the performance\nof full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that\napproximates full fine-tuning within low-rank subspaces using a carefully\ndesigned initialization strategy. We theoretically demonstrate that the\narchitecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and\nA while keeping other matrices fixed, provides the precise conditions needed\nfor this approximation. We leverage its constrained update space to achieve\noptimal scaling for high-rank gradient updates while removing the need for\nhyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using \\textbf{27-90}\ntimes fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our\nfindings establish that it is possible to simulate full fine-tuning in low-rank\nsubspaces, and achieve significant efficiency gains without sacrificing\nperformance. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/lora-sb.", "AI": {"tldr": "LoRA-SB is a method that improves the efficiency of fine-tuning large language models by approximating full fine-tuning within low-rank subspaces.", "motivation": "Low-rank adapters are popular for fine-tuning LLMs but often underperform compared to full fine-tuning.", "method": "LoRA-SB uses a specific initialization strategy within low-rank subspaces to achieve performance benchmarks comparable to full fine-tuning while utilizing significantly fewer parameters.", "result": "Experiments show that LoRA-SB outperforms standard LoRA significantly in various reasoning tasks while using 27-90 times fewer learnable parameters.", "conclusion": "LoRA-SB successfully simulates full fine-tuning effects in a more efficient manner without compromising on performance.", "key_contributions": ["Introduces an effective initialization strategy for low-rank fine-tuning.", "Demonstrates performance improvements over standard LoRA and LoRA-XS.", "Achieves significant parameter efficiency with comparable results to full fine-tuning."], "limitations": "", "keywords": ["Low-rank adapters", "fine-tuning", "large language models", "efficiency", "optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.02271", "pdf": "https://arxiv.org/pdf/2412.02271.pdf", "abs": "https://arxiv.org/abs/2412.02271", "title": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines", "authors": ["Preetika Verma", "Kokil Jaidka"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 8 tables", "summary": "The editability of online news content has become a significant factor in\nshaping public perception, as social media platforms introduce new affordances\nfor dynamic and adaptive news framing. Edits to news headlines can refocus\naudience attention, add or remove emotional language, and shift the framing of\nevents in subtle yet impactful ways. What types of media bias are editorialized\nin and out of news headlines, and how can they be systematically identified?\nThis study introduces the MediaSpin dataset, the first to characterize the bias\nin how prominent news outlets editorialize news headlines after publication.\nThe dataset includes 78,910 pairs of headlines annotated with 13 distinct types\nof media bias, using human-supervised LLM labeling. We discuss the linguistic\ninsights it affords and show its applications for bias prediction and user\nbehavior analysis.", "AI": {"tldr": "The paper presents the MediaSpin dataset, which categorizes bias in online news headlines and its effects on public perception.", "motivation": "To investigate how edits to online news headlines influence audience perception and to identify types of media bias systematically.", "method": "The study creates the MediaSpin dataset, comprising 78,910 pairs of headlines annotated for 13 distinct types of media bias using human-supervised LLM labeling.", "result": "The dataset provides insights into linguistic patterns in media bias and can be used for bias prediction and analysis of user behavior.", "conclusion": "The MediaSpin dataset can enhance understanding of media bias in journalism and its impact on public perception.", "key_contributions": ["Introduction of the MediaSpin dataset for analyzing media bias in headlines.", "Use of human-supervised LLM labeling for bias annotation.", "Potential applications for predicting bias and understanding user behavior."], "limitations": "", "keywords": ["media bias", "news headlines", "dataset", "human-supervised LLM", "user behavior analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2412.11803", "pdf": "https://arxiv.org/pdf/2412.11803.pdf", "abs": "https://arxiv.org/abs/2412.11803", "title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models", "authors": ["Boyang Xue", "Fei Mi", "Qi Zhu", "Hongru Wang", "Rui Wang", "Sheng Wang", "Erxin Yu", "Xuming Hu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": "Accepted in ACL2025 Main Conference", "summary": "Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines.", "AI": {"tldr": "Introduction of a framework (UAlign) to enhance the factual expression capabilities of LLMs by incorporating uncertainty estimations into prompts.", "motivation": "Large Language Models (LLMs) often fail to express accurate factual knowledge, particularly when their knowledge boundaries are unclear.", "method": "The UAlign framework employs uncertainty estimations (confidence scores and semantic entropy) to define knowledge boundaries and uses these as input features in LLM prompts to improve factual alignment.", "result": "Experimental results demonstrate that UAlign significantly improves LLM performance in confidently answering known questions and refusing unknown ones, with reliability and generalizability improvements over existing methods.", "conclusion": "Incorporating uncertainty representations in LLM prompts enhances their factual knowledge expression and overall performance.", "key_contributions": ["Introduction of the UAlign framework for LLM factual alignment", "Use of uncertainty estimations in knowledge question-answering", "Significant performance improvements over baselines"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Estimation", "Factual Knowledge", "Knowledge Boundaries", "Prompt Engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.12486", "pdf": "https://arxiv.org/pdf/2412.12486.pdf", "abs": "https://arxiv.org/abs/2412.12486", "title": "Boosting Long-Context Management via Query-Guided Activation Refilling", "authors": ["Hongjin Qian", "Zheng Liu", "Peitian Zhang", "Zhicheng Dou", "Defu Lian"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL25 Main Conference", "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.", "AI": {"tldr": "This paper introduces ACRE, a novel method for processing long-context information-seeking tasks in large language models by utilizing a Bi-layer KV Cache to efficiently integrate global and localized information based on the user's query.", "motivation": "Long-context processing is challenging for large language models due to context-window limitations and the high computational cost of KV activations. Traditional methods fail to adapt to the varying information needs of queries.", "method": "The proposed method, ACRE, constructs a Bi-layer KV Cache consisting of a layer-1 cache capturing global information and a layer-2 cache storing detailed local information. It allows for dynamic interaction between the two layers based on the input query.", "result": "Experiments indicate that ACRE significantly improves both performance and efficiency on various long-context information-seeking datasets.", "conclusion": "ACRE enhances the ability of large language models to handle dynamic and diverse information needs effectively by optimizing how long context is processed through strategic cache management.", "key_contributions": ["Introduction of ACRE for long-context information processing", "Development of a Bi-layer KV Cache system", "Demonstration of improvements in performance and efficiency for LLMs"], "limitations": "", "keywords": ["Long-context processing", "Large language models", "Information-seeking tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.00879", "pdf": "https://arxiv.org/pdf/2501.00879.pdf", "abs": "https://arxiv.org/abs/2501.00879", "title": "TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation", "authors": ["Huichi Zhou", "Kin-Hei Lee", "Zhonghao Zhan", "Yue Chen", "Zhenhao Li", "Zhaoyang Wang", "Hamed Haddadi", "Emine Yilmaz"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge sources, enabling more accurate and contextually\nrelevant responses tailored to user queries. These systems, however, remain\nsusceptible to corpus poisoning attacks, which can severely impair the\nperformance of LLMs. To address this challenge, we propose TrustRAG, a robust\nframework that systematically filters malicious and irrelevant content before\nit is retrieved for generation. Our approach employs a two-stage defense\nmechanism. The first stage implements a cluster filtering strategy to detect\npotential attack patterns. The second stage employs a self-assessment process\nthat harnesses the internal capabilities of LLMs to detect malicious documents\nand resolve inconsistencies. TrustRAG provides a plug-and-play, training-free\nmodule that integrates seamlessly with any open- or closed-source language\nmodel. Extensive experiments demonstrate that TrustRAG delivers substantial\nimprovements in retrieval accuracy, efficiency, and attack resistance.", "AI": {"tldr": "TrustRAG is a framework designed to improve the robustness of Retrieval-Augmented Generation (RAG) systems against corpus poisoning attacks by filtering unwanted content before retrieval.", "motivation": "To enhance the performance of large language models that utilize external knowledge while protecting against corpus poisoning attacks that can degrade their effectiveness.", "method": "TrustRAG employs a two-stage defense mechanism which includes cluster filtering to identify potential attack patterns and a self-assessment process using LLM capabilities to detect malicious documents.", "result": "The framework shows significant improvements in retrieval accuracy, efficiency, and resistance to attacks in extensive experiments.", "conclusion": "TrustRAG is a versatile, training-free addition that can be incorporated with any LLM to enhance security and performance.", "key_contributions": ["Development of the TrustRAG framework for enhanced retrieval accuracy and efficiency.", "Implementation of a two-stage defense mechanism against corpus poisoning.", "Provides a plug-and-play, training-free solution for integrating with existing LLMs."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Corpus Poisoning", "TrustRAG", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.04686", "pdf": "https://arxiv.org/pdf/2501.04686.pdf", "abs": "https://arxiv.org/abs/2501.04686", "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics", "authors": ["Ruilin Luo", "Zhuofan Zheng", "Yifan Wang", "Xinzhe Ni", "Zicheng Lin", "Songtao Jiang", "Yiyao Yu", "Chufan Shi", "Ruihang Chu", "Jin Zeng", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Update version. Project url: https://ursa-math.github.io", "summary": "Process Reward Models (PRMs) have shown promise in enhancing the mathematical\nreasoning capabilities of Large Language Models (LLMs) through Test-Time\nScaling (TTS). However, their integration into multimodal reasoning remains\nlargely unexplored. In this work, we take the first step toward unlocking the\npotential of PRMs in multimodal mathematical reasoning. We identify three key\nchallenges: (1) the scarcity of high-quality reasoning data constrains the\ncapabilities of foundation Multimodal Large Language Models (MLLMs), which\nimposes further limitations on the upper bounds of TTS and reinforcement\nlearning (RL); (2) a lack of automated methods for process labeling within\nmultimodal contexts persists; (3) the employment of process rewards in unimodal\nRL faces issues like reward hacking, which may extend to multimodal scenarios.\nTo address these issues, we introduce URSA, a three-stage Unfolding multimodal\nProcess-Supervision Aided training framework. We first construct MMathCoT-1M, a\nhigh-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset,\nto build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we\ngo through an automatic process to synthesize process supervision data, which\nemphasizes both logical correctness and perceptual consistency. We introduce\nDualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose\nProcess-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a\nmultimodal PRM-aided online RL method that outperforms vanilla GRPO. With\nPS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4%\nand 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found\nat https://github.com/URSA-MATH.", "AI": {"tldr": "This work explores the integration of Process Reward Models (PRMs) into multimodal mathematical reasoning, addressing challenges in data quality, labeling automation, and reward issues. It introduces URSA, a framework that includes the creation of a large-scale reasoning dataset and a new training method that improves performance in multimodal reasoning tasks.", "motivation": "To unlock the potential of Process Reward Models in multimodal mathematical reasoning and address key limitations in data quality and integration methods.", "method": "The proposed URSA framework consists of three stages: creating a high-quality multimodal reasoning dataset (MMathCoT-1M), synthesizing process supervision data, and implementing a novel reinforcement learning approach (PS-GRPO).", "result": "URSA-8B-PS-GRPO outperforms state-of-the-art models like Gemma3-12B and GPT-4o by 8.4% and 2.7% across six benchmarks.", "conclusion": "The URSA framework significantly enhances multimodal mathematical reasoning capabilities and addresses critical challenges through improved dataset quality and innovative training techniques.", "key_contributions": ["Introduction of URSA framework for multimodal PRM training.", "Creation of MMathCoT-1M dataset for enhancing multimodal reasoning.", "Development of Process-Supervised Group-Relative-Policy-Optimization for improved RL performance."], "limitations": "Challenges in data scarcity and automated process labeling within multimodal contexts remain.", "keywords": ["Process Reward Models", "multimodal reasoning", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.04694", "pdf": "https://arxiv.org/pdf/2501.04694.pdf", "abs": "https://arxiv.org/abs/2501.04694", "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "authors": ["Yaoxiang Wang", "Haoling Li", "Xin Zhang", "Jie Wu", "Xiao Liu", "Wenxiang Hu", "Zhongxin Guo", "Yangyu Huang", "Ying Xin", "Yujiu Yang", "Jinsong Su", "Qi Chen", "Scarlett Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Existing methods for code generation use code snippets as seed data,\nrestricting the complexity and diversity of the synthesized data. In this\npaper, we introduce a novel feature tree-based synthesis framework, which\nrevolves around hierarchical code features derived from high-level abstractions\nof code. The feature tree is constructed from raw data and refined iteratively\nto increase the quantity and diversity of the extracted features, which\ncaptures and recognizes more complex patterns and relationships within the\ncode. By adjusting the depth and breadth of the sampled subtrees, our framework\nprovides precise control over the complexity of the generated code, enabling\nfunctionalities that range from function-level operations to multi-file\nscenarios. We fine-tuned widely-used base models to obtain EpiCoder series,\nachieving state-of-the-art performance on multiple benchmarks at both the\nfunction and file levels. In particular, empirical evidence indicates that our\napproach shows significant potential in the synthesizing of repository-level\ncode data. Our code and data are publicly available at\nhttps://github.com/microsoft/EpiCoder.", "AI": {"tldr": "A novel feature tree-based synthesis framework enhances code generation by extracting hierarchical code features from high-level abstractions, allowing for greater complexity and diversity in generated code.", "motivation": "To address limitations in existing code generation methods that rely on code snippets, which restrict the complexity and diversity of synthesized data.", "method": "The framework constructs a feature tree from raw data and refines it iteratively, allowing control over generated code complexity by adjusting subtree parameters.", "result": "Achieved state-of-the-art performance on multiple benchmarks, significantly improving repository-level code synthesis.", "conclusion": "The feature tree-based approach shows potential for enhancing code generation complexity and diversity, with publicly available resources for further application.", "key_contributions": ["Introduction of a feature tree-based synthesis framework for code generation", "State-of-the-art performance on function and file level benchmarks", "Publicly available code and data for further research"], "limitations": "", "keywords": ["code generation", "feature tree", "synthesis framework", "machine learning", "EpiCoder"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.06346", "pdf": "https://arxiv.org/pdf/2501.06346.pdf", "abs": "https://arxiv.org/abs/2501.06346", "title": "Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages", "authors": ["Jannik Brinkmann", "Chris Wendler", "Christian Bartelt", "Aaron Mueller"], "categories": ["cs.CL"], "comment": null, "summary": "Human bilinguals often use similar brain regions to process multiple\nlanguages, depending on when they learned their second language and their\nproficiency. In large language models (LLMs), how are multiple languages\nlearned and encoded? In this work, we explore the extent to which LLMs share\nrepresentations of morphsyntactic concepts such as grammatical number, gender,\nand tense across languages. We train sparse autoencoders on Llama-3-8B and\nAya-23-8B, and demonstrate that abstract grammatical concepts are often encoded\nin feature directions shared across many languages. We use causal interventions\nto verify the multilingual nature of these representations; specifically, we\nshow that ablating only multilingual features decreases classifier performance\nto near-chance across languages. We then use these features to precisely modify\nmodel behavior in a machine translation task; this demonstrates both the\ngenerality and selectivity of these feature's roles in the network. Our\nfindings suggest that even models trained predominantly on English data can\ndevelop robust, cross-lingual abstractions of morphosyntactic concepts.", "AI": {"tldr": "This paper investigates how large language models (LLMs) learn and encode morphosyntactic concepts across multiple languages, highlighting shared brain concepts between linguistic features.", "motivation": "To understand the representation of morphosyntactic concepts in LLMs and their shared features across different languages, akin to human bilingual processing.", "method": "Sparse autoencoders were trained on two LLMs, Llama-3-8B and Aya-23-8B, and causal interventions were utilized to assess the multilingual nature of the learned representations.", "result": "The study found that abstract grammatical concepts are encoded in shared feature directions across multiple languages and that removing these features resulted in significant performance drops in classifiers.", "conclusion": "LLMs trained predominantly on English data can still develop robust, cross-lingual abstractions, aiding in tasks like machine translation by fine-tuning model behavior based on morphosyntactic features.", "key_contributions": ["Identification of shared morphosyntactic representations in LLMs", "Use of causal interventions to validate multilingual features", "Demonstration of model behavior modification in machine translation tasks"], "limitations": "", "keywords": ["large language models", "morphosyntactic concepts", "multilingual features", "machine translation", "sparse autoencoders"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.11960", "pdf": "https://arxiv.org/pdf/2501.11960.pdf", "abs": "https://arxiv.org/abs/2501.11960", "title": "TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection", "authors": ["Yang Cao", "Sikun Yang", "Chen Li", "Haolong Xiang", "Lianyong Qi", "Bo Liu", "Rongsheng Li", "Ming Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text anomaly detection is crucial for identifying spam, misinformation, and\noffensive language in natural language processing tasks. Despite the growing\nadoption of embedding-based methods, their effectiveness and generalizability\nacross diverse application scenarios remain under-explored. To address this, we\npresent TAD-Bench, a comprehensive benchmark designed to systematically\nevaluate embedding-based approaches for text anomaly detection. TAD-Bench\nintegrates multiple datasets spanning different domains, combining\nstate-of-the-art embeddings from large language models with a variety of\nanomaly detection algorithms. Through extensive experiments, we analyze the\ninterplay between embeddings and detection methods, uncovering their strengths,\nweaknesses, and applicability to different tasks. These findings offer new\nperspectives on building more robust, efficient, and generalizable anomaly\ndetection systems for real-world applications.", "AI": {"tldr": "TAD-Bench is a benchmark for evaluating embedding-based methods in text anomaly detection.", "motivation": "To address the effectiveness and generalizability of embedding-based methods across various application scenarios in text anomaly detection.", "method": "TAD-Bench integrates multiple datasets from different domains, combining embeddings from large language models with various anomaly detection algorithms for systematic evaluation.", "result": "The experiments reveal the strengths, weaknesses, and applicability of different embeddings and detection methods to a range of tasks.", "conclusion": "The findings suggest pathways for developing more robust and efficient anomaly detection systems in real-world contexts.", "key_contributions": ["Introduction of TAD-Bench as a comprehensive benchmark.", "Integration of state-of-the-art embeddings with various anomaly detection methods.", "Insights on the effectiveness of embedding-based approaches in diverse scenarios."], "limitations": "", "keywords": ["anomaly detection", "text classification", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.16975", "pdf": "https://arxiv.org/pdf/2501.16975.pdf", "abs": "https://arxiv.org/abs/2501.16975", "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling", "authors": ["Hongzhi Huang", "Defa Zhu", "Banggu Wu", "Yutao Zeng", "Ya Wang", "Qiyang Min", "Xun Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": "accepted by ICML2025", "summary": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.", "AI": {"tldr": "Introducing Over-Tokenized Transformers to improve language modeling performance through enhanced input vocabularies and a log-linear relationship with training loss.", "motivation": "To explore the influence of tokenization on model scaling and performance in large language models.", "method": "A novel framework that decouples input and output vocabularies and scales up input vocabularies to leverage multi-gram tokens.", "result": "Larger input vocabularies consistently enhance model performance, achieving results comparable to double-sized baselines without added costs.", "conclusion": "Tokenization plays a critical role in scaling laws for LLMs, offering practical insights for designing more efficient tokenizers.", "key_contributions": ["Introduces Over-Tokenized Transformers framework.", "Demonstrates log-linear relationship between input vocabulary size and training loss.", "Shows performance improvement with larger input vocabularies without extra costs."], "limitations": "", "keywords": ["tokenization", "language models", "input vocabulary", "performance", "scaling laws"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.08923", "pdf": "https://arxiv.org/pdf/2502.08923.pdf", "abs": "https://arxiv.org/abs/2502.08923", "title": "CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality", "authors": ["Razvan-Gabriel Dumitru", "Minglai Yang", "Vikas Yadav", "Mihai Surdeanu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "comment": "33 pages, 18 figures, 19 tables", "summary": "We introduce CopySpec, a simple yet effective technique to tackle the\ninefficiencies LLMs face when generating responses that closely resemble\nprevious outputs or responses that can be verbatim extracted from context.\nCopySpec identifies repeated sequences in the model's chat history or context\nand speculates that the same tokens will follow, enabling seamless copying\nwithout compromising output quality and without requiring additional GPU\nmemory. To evaluate the effectiveness of our approach, we conducted experiments\nusing seven LLMs and five datasets: MT-Bench, CNN/DM, GSM8K, HumanEval, and our\nnewly created dataset, MT-Redundant. MT-Redundant, introduced in this paper,\ntransforms the second turn of MT-Bench into a request for variations of the\nfirst turn's answer, simulating real-world scenarios where users request\nmodifications to prior responses. Our results demonstrate significant\nspeed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select\nMT-Redundant categories, and 2.66x on the third turn of GSM8K's self-correction\ntasks. Importantly, we show that CopySpec integrates seamlessly with\nspeculative decoding, yielding an average 49% additional speed-up over\nspeculative decoding for the second turn of MT-Redundant across all eight\ncategories. While LLMs, even with speculative decoding, suffer from slower\ninference as context size grows, CopySpec leverages larger contexts to\naccelerate inference, making it a faster complementary solution. Our code and\ndataset are publicly available at https://github.com/RazvanDu/CopySpec.", "AI": {"tldr": "CopySpec is a technique that improves the efficiency of LLM response generation by identifying repeated sequences in chat history for seamless copying, speeding up the inference process without increasing GPU memory usage.", "motivation": "LLMs often face inefficiencies when generating responses that either repeat previous outputs or closely resemble prior context.", "method": "CopySpec identifies repeated sequences in the model's chat history or context and predicts that the same tokens will follow, allowing for efficient copying of content.", "result": "The experiments showed significant speed-ups in response generation: up to 2.35x on CNN/DM, 3.08x on MT-Redundant, and 2.66x on GSM8K's self-correction tasks. CopySpec also enhances speculative decoding, yielding a 49% speed improvement on average.", "conclusion": "CopySpec is a valuable addition to LLM techniques, offering faster inference capabilities while using larger context sizes effectively, addressing the inherent slowdown of LLMs with increased context.", "key_contributions": ["Introduction of CopySpec for efficient LLM response generation", "Creation of the MT-Redundant dataset to simulate variations", "Demonstration of significant speed-ups across multiple datasets"], "limitations": "", "keywords": ["CopySpec", "LLMs", "speculative decoding", "inference speed", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.10735", "pdf": "https://arxiv.org/pdf/2502.10735.pdf", "abs": "https://arxiv.org/abs/2502.10735", "title": "Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models", "authors": ["Shuqi Liu", "Bowei He", "Han Wu", "Linqi Song"], "categories": ["cs.CL"], "comment": null, "summary": "Post-training pruning has emerged as a crucial optimization technique as\nlarge language models (LLMs) continue to grow rapidly. However, the significant\nvariations in weight distributions across different LLMs make fixed pruning\nstrategies inadequate for multiple models. In this paper, we introduce\n\\textbf{\\textsc{OptiShear}}, an efficient evolutionary optimization framework\nfor adaptive LLM pruning. Our framework features two key innovations: an\neffective search space built on our Meta pruning metric to handle diverse\nweight distributions, and a model-wise reconstruction error for rapid\nevaluation during search trials. We employ Non-dominated Sorting Genetic\nAlgorithm III (NSGA-III) to optimize both pruning metrics and layerwise\nsparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models\n(7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning\nmetrics consistently outperform existing methods. Additionally, our discovered\nlayerwise sparsity ratios enhance the effectiveness of other pruning metrics.\nThe framework exhibits strong cross-task and cross-model generalizability,\nproviding a cost-effective solution for model compression.", "AI": {"tldr": "Introducing \textsc{OptiShear}, an adaptive pruning framework for large language models (LLMs) that optimizes pruning metrics and sparsity ratios using a genetic algorithm.", "motivation": "To address the inefficiencies of fixed pruning strategies in large language models due to varying weight distributions.", "method": "An evolutionary optimization framework utilizing Non-dominated Sorting Genetic Algorithm III (NSGA-III) for optimizing pruning metrics and layerwise sparsity ratios, based on a model-wise reconstruction error.", "result": "Demonstrated superior performance of adaptive pruning metrics against existing methods on LLaMA and Mistral models, with enhancements in layerwise sparsity ratios for effective model compression.", "conclusion": "\textsc{OptiShear} provides a robust solution for adaptive pruning with strong generalizability across different tasks and models, offering a cost-effective approach to model compression.", "key_contributions": ["Development of \textsc{OptiShear}, a novel adaptive pruning framework for LLMs.", "Introduction of a Meta pruning metric for diverse weight distributions.", "Application of NSGA-III to optimize pruning metrics and sparsity ratios."], "limitations": "", "keywords": ["large language models", "adaptive pruning", "model compression", "evolutionary optimization", "NSGA-III"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.10743", "pdf": "https://arxiv.org/pdf/2502.10743.pdf", "abs": "https://arxiv.org/abs/2502.10743", "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "authors": ["Shuqi Liu", "Yuxuan Yao", "Bowei He", "Zehua Liu", "Xiongwei Han", "Mingxuan Yuan", "Han Wu", "Linqi Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have led to specialized models\nexcelling in specific domains, creating a need for efficient model merging\ntechniques. While traditional merging approaches combine parameters into a\nsingle static model, they often compromise task-specific performance. However,\ntask-specific routing methods maintain accuracy but introduce substantial\nstorage overhead. We present \\texttt{1bit}-Merging, a novel framework that\nintegrates task-specific routing with 1-bit quantized task vectors to balance\nperformance and storage efficiency. Our approach leverages the observation that\ndifferent task-specific models store knowledge in distinct layers-chat models\nprimarily in attention layers and math/code models in MLP layers, enabling\ntargeted compression strategies. Through extensive experiments with LLaMA2 and\nMistral model families across chat, mathematical reasoning, and code generation\ntasks, we demonstrate that 1bit-Merging achieves comparable or superior\nperformance to existing methods while significantly reducing storage\nrequirements. Our framework offers a practical solution for combining\nspecialized models while maintaining their individual strengths and addressing\nthe storage challenges of current approaches.", "AI": {"tldr": "1bit-Merging is a new framework for efficiently merging specialized large language models using 1-bit quantized task vectors, balancing performance and storage needs.", "motivation": "There is a growing need for efficient model merging techniques in light of recent advances in large language models, which often suffer from compromises in task-specific performance or storage overhead.", "method": "The framework integrates task-specific routing with 1-bit quantized task vectors, allowing for targeted compression strategies by leveraging the different layers where task-specific knowledge is stored in models.", "result": "Extensive experiments indicate that 1bit-Merging achieves performance comparable or superior to existing methods while significantly reducing storage requirements across various tasks including chat, mathematical reasoning, and code generation.", "conclusion": "1bit-Merging provides a practical solution to the challenges of merging specialized models, maintaining their individual strengths while addressing storage issues.", "key_contributions": ["Introduces a novel framework for model merging using 1-bit quantized task vectors", "Targets compression strategies based on knowledge storage in different model layers", "Demonstrates significant storage savings without compromising performance"], "limitations": "", "keywords": ["large language models", "model merging", "1-bit quantization", "task-specific routing", "storage efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.10749", "pdf": "https://arxiv.org/pdf/2502.10749.pdf", "abs": "https://arxiv.org/abs/2502.10749", "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging", "authors": ["Zehua Liu", "Han Wu", "Yuxuan Yao", "Ruifeng She", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While most current approaches rely on further training techniques, such as\nfine-tuning or reinforcement learning, to enhance model capacities, model\nmerging stands out for its ability of improving models without requiring any\nadditional training. In this paper, we propose a unified framework for model\nmerging based on low-rank estimation of task vectors without the need for\naccess to the base model, named \\textsc{LoRE-Merging}. Our approach is\nmotivated by the observation that task vectors from fine-tuned models\nfrequently exhibit a limited number of dominant singular values, making\nlow-rank estimations less prone to interference. We implement the method by\nformulating the merging problem as an optimization problem. Extensive empirical\nexperiments demonstrate the effectiveness of our framework in mitigating\ninterference and preserving task-specific information, thereby advancing the\nstate-of-the-art performance in model merging techniques.", "AI": {"tldr": "LoRE-Merging is a novel framework for model merging that improves models without additional training by utilizing low-rank estimation of task vectors.", "motivation": "The motivation stems from the observation that task vectors from fine-tuned models often display a limited number of dominant singular values, enabling effective low-rank estimation.", "method": "The merging problem is formulated as an optimization problem based on low-rank estimation of task vectors.", "result": "Extensive empirical experiments show that the framework effectively mitigates interference and preserves task-specific information, leading to state-of-the-art performance in model merging.", "conclusion": "The proposed method advances the techniques in model merging without the need for further training and demonstrates significant improvements in performance.", "key_contributions": ["Introduction of the LoRE-Merging framework for model merging.", "Utilization of low-rank estimation to improve model capacities without additional training.", "Empirical validation demonstrating enhanced effectiveness in model merging techniques."], "limitations": "", "keywords": ["model merging", "low-rank estimation", "optimization", "task vectors", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.11228", "pdf": "https://arxiv.org/pdf/2502.11228.pdf", "abs": "https://arxiv.org/abs/2502.11228", "title": "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs", "authors": ["Mohammad Reza Rezaei", "Adji Bousso Dieng"], "categories": ["cs.CL", "cs.AI"], "comment": "A RAG pipeline that accounts for both diversity and answer quality\n  and that can be used with any LLM backbone to solve complex multi-hop\n  question-answering tasks", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nfor domain-specific question-answering (QA) tasks by leveraging external\nknowledge sources. However, traditional RAG systems primarily focus on\nrelevance-based retrieval and often struggle with redundancy, especially when\nreasoning requires connecting information from multiple sources. This paper\nintroduces Vendi-RAG, a framework based on an iterative process that jointly\noptimizes retrieval diversity and answer quality. This joint optimization leads\nto significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\nthe Vendi Score (VS), a flexible similarity-based diversity metric, to promote\nsemantic diversity in document retrieval. It then uses an LLM judge that\nevaluates candidate answers, generated after a reasoning step, and outputs a\nscore that the retriever uses to balance relevance and diversity among the\nretrieved documents during each iteration. Experiments on three challenging\ndatasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\neffectiveness in multi-hop reasoning tasks. The framework achieves significant\naccuracy improvements over traditional single-step and multi-step RAG\napproaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\nbest baseline. The benefits of Vendi-RAG are even more pronounced as the number\nof retrieved documents increases. Finally, we evaluated Vendi-RAG across\ndifferent LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\nobserved consistent improvements, demonstrating that the framework's advantages\nare model-agnostic.", "AI": {"tldr": "Vendi-RAG is a framework that enhances multi-hop question-answering by optimizing retrieval diversity and answer quality simultaneously, significantly improving accuracy on various datasets.", "motivation": "Traditional RAG systems struggle with redundancy in retrieval, particularly when reasoning requires synthesizing information from multiple sources.", "method": "Vendi-RAG introduces an iterative process that uses the Vendi Score, a diversity metric, to optimize retrieval diversity and quality, while an LLM judge evaluates and scores candidate answers.", "result": "The framework achieves notable accuracy improvements in multi-hop QA tasks, with increases of up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to existing methods.", "conclusion": "Vendi-RAG shows significant improvements over traditional RAG systems and maintains effectiveness across different LLM models, making it a versatile tool for multi-hop reasoning tasks.", "key_contributions": ["Introduction of Vendi-RAG framework for multi-hop QA", "Flexible Vendi Score metric promoting diversity", "Model-agnostic performance improvements across various LLMs"], "limitations": "", "keywords": ["retrieval-augmented generation", "multi-hop question answering", "diversity optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11330", "pdf": "https://arxiv.org/pdf/2502.11330.pdf", "abs": "https://arxiv.org/abs/2502.11330", "title": "System Message Generation for User Preferences using Open-Source Models", "authors": ["Minbyul Jeong", "Jungho Cho", "Minsoo Khang", "Dawoon Jung", "Teakgyu Hong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "System messages play a crucial role in interactions with large language\nmodels (LLMs), often serving as prompts to initiate conversations. Through\nsystem messages, users can assign specific roles, perform intended tasks,\nincorporate background information, and specify various output formats and\ncommunication styles. Despite such versatility, publicly available datasets\noften lack system messages and are subject to strict license constraints in\nindustrial applications. Moreover, manually annotating system messages that\nalign with user instructions is resource-intensive. In light of these\nchallenges, we introduce SysGen, a pipeline for generating system messages that\nbetter align assistant responses with user instructions using existing\nsupervised fine-tuning datasets that lack system messages. Training open-source\nmodels on SysGen data yields substantial improvements in both single-turn\n(Multifacet) and multi-turn (SysBench) conversation benchmarks. Notably, our\nmethod shows strong gains in shorter conversations, suggesting that it enhances\nearly-stage interaction effectiveness. Our qualitative analysis further\nemphasizes the value of diverse and structured system messages in improving LLM\nadaptability across varied user scenarios.", "AI": {"tldr": "Introducing SysGen, a pipeline for generating system messages that improves interactions with large language models by aligning responses with user instructions.", "motivation": "The lack of publicly available datasets with system messages and the resource-intensive nature of manually annotating these messages necessitates a solution to improve LLM interactions.", "method": "SysGen generates system messages using existing supervised fine-tuning datasets. It trains open-source models on this generated data, enhancing their performance in conversations.", "result": "Training on SysGen data leads to substantial improvements in single-turn and multi-turn conversation benchmarks, particularly in early-stage interactions.", "conclusion": "The study underlines the importance of diverse and structured system messages for enhancing the adaptability of LLMs to varied user scenarios.", "key_contributions": ["Introduction of SysGen pipeline for system message generation", "Demonstrated improvements in LLM interaction effectiveness", "Qualitative insights into message structure and diversity benefits"], "limitations": "Focuses on specific types of conversations; broader applicability may require further exploration.", "keywords": ["System messages", "Large language models", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11614", "pdf": "https://arxiv.org/pdf/2502.11614.pdf", "abs": "https://arxiv.org/abs/2502.11614", "title": "Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI", "authors": ["Yuxia Wang", "Rui Xing", "Jonibek Mansurov", "Giovanni Puccetti", "Zhuohan Xie", "Minh Ngoc Ta", "Jiahui Geng", "Jinyan Su", "Mervat Abassy", "Saad El Dine Ahmed", "Kareem Elozeiri", "Nurkhan Laiyk", "Maiya Goloburda", "Tarek Mahmoud", "Raj Vardhan Tomar", "Alexander Aziz", "Ryuto Koike", "Masahiro Kaneko", "Artem Shelmanov", "Ekaterina Artemova", "Vladislav Mikhailov", "Akim Tsvigun", "Alham Fikri Aji", "Nizar Habash", "Iryna Gurevych", "Preslav Nakov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior studies have shown that distinguishing text generated by large language\nmodels (LLMs) from human-written one is highly challenging, and often no better\nthan random guessing. To verify the generalizability of this finding across\nlanguages and domains, we perform an extensive case study to identify the upper\nbound of human detection accuracy. Across 16 datasets covering 9 languages and\n9 domains, 19 annotators achieved an average detection accuracy of 87.6\\%, thus\nchallenging previous conclusions. We find that major gaps between human and\nmachine text lie in concreteness, cultural nuances, and diversity. Prompting by\nexplicitly explaining the distinctions in the prompts can partially bridge the\ngaps in over 50\\% of the cases. However, we also find that humans do not always\nprefer human-written text, particularly when they cannot clearly identify its\nsource.", "AI": {"tldr": "This study investigates human detection of text generated by large language models (LLMs) across multiple languages and domains, finding an average detection accuracy of 87.6%, highlighting distinctions based on concreteness, cultural nuances, and diversity.", "motivation": "To evaluate the generalizability of the difficulty in distinguishing LLM-generated text from human-written text across various languages and domains.", "method": "An extensive case study involving 16 datasets from 9 languages and 9 domains with 19 annotators assessing the detection accuracy of human versus LLM-generated text.", "result": "The study found an average detection accuracy of 87.6% among annotators, identifying key gaps between human and machine text in terms of concreteness, cultural nuances, and diversity.", "conclusion": "Prompting with explicit distinctions can help improve detection accuracy, but humans may not always prefer human-written text when its source is unclear.", "key_contributions": ["Provides new insights into the human detection of LLM-generated text across multiple languages and domains.", "Identifies specific discourse features where human and machine text differ significantly.", "Demonstrates the effectiveness of prompting strategies in improving detection accuracy."], "limitations": "The study does not explore the implications of detection accuracy on real-world applications of LLMs or the subjective preferences for text sources.", "keywords": ["large language models", "human detection", "text generation", "cross-linguistic study", "diversity in text"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11789", "pdf": "https://arxiv.org/pdf/2502.11789.pdf", "abs": "https://arxiv.org/abs/2502.11789", "title": "Personality Editing for Language Models through Relevant Knowledge Editing", "authors": ["Seojin Hwang", "Yumin Kim", "Byeongjeong Kim", "Donghoon Shin", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "19 pages, 3 figures, 24 tables", "summary": "Large Language Models (LLMs) play a vital role in applications like\nconversational agents and content creation, where controlling a model's\npersonality is crucial for maintaining tone, consistency, and engagement.\nHowever, traditional prompt-based techniques for controlling personality often\nfall short, as they do not effectively mitigate the model's inherent biases. In\nthis paper, we introduce a novel method PALETTE that enhances personality\ncontrol through knowledge editing. By generating adjustment queries inspired by\npsychological assessments, our approach systematically adjusts responses to\npersonality-related queries similar to modifying factual knowledge, thereby\nachieving controlled shifts in personality traits. Experimental results from\nboth automatic and human evaluations demonstrate that our method enables more\nstable and well-balanced personality control in LLMs.", "AI": {"tldr": "The paper presents PALETTE, a method for enhancing personality control in LLMs via knowledge editing, addressing biases in traditional prompt-based methods.", "motivation": "Controlling personality in LLM applications is crucial for maintaining user engagement but traditional methods do not effectively address inherent model biases.", "method": "PALETTE generates adjustment queries based on psychological assessments to systematically adjust responses related to personality traits.", "result": "Experimental evaluations show that PALETTE provides more stable and balanced personality control in LLMs compared to traditional methods.", "conclusion": "PALETTE offers a promising approach to enhance personality management in LLMs, contributing to more engaging user interactions.", "key_contributions": ["Introduction of the PALETTE method for personality control in LLMs", "Systematic adjustment of responses using knowledge editing ", "Demonstration of improved personality stability and balance through experimental results."], "limitations": "The focus on personality may limit its applicability to other aspects of LLM functionality.", "keywords": ["Large Language Models", "personality control", "knowledge editing", "psychological assessments", "human evaluations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12594", "pdf": "https://arxiv.org/pdf/2502.12594.pdf", "abs": "https://arxiv.org/abs/2502.12594", "title": "PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery", "authors": ["Bowei He", "Lihao Yin", "Hui-Ling Zhen", "Xiaokun Zhang", "Mingxuan Yuan", "Chen Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Model pruning is an effective approach for compressing large language models\n(LLMs). However, this process often leads to significant degradation of model\ncapabilities. While post-training techniques such as instruction tuning are\ncommonly employed to recover model performance, existing methods often overlook\nthe uneven deterioration of model capabilities and incur high computational\ncosts. Moreover, some irrelevant instructions may also introduce negative\neffects to model capacity recovery. To address these challenges, we propose the\n\\textbf{P}ost-training d\\textbf{A}ta \\textbf{S}election method for\n\\textbf{E}fficient pruned large language model \\textbf{R}ecovery\n(\\textbf{PASER}). PASER aims to identify instructions to recover the most\ncompromised model capacities with a certain data budget. Our approach first\napplies manifold learning and spectral clustering to group recovery\ninstructions in the semantic space, revealing capability-specific instruction\nsets. Then, the data budget is adaptively allocated across clusters by the\ndegree of corresponding model capability degradation. In each cluster, we\nprioritize data samples that lead to the most decline of model performance. To\nmitigate potential negative tuning effects, we also detect and filter out\nconflicting or irrelevant recovery data. Extensive experiments demonstrate that\nPASER significantly outperforms conventional baselines, effectively recovering\nthe general capabilities of pruned LLMs while utilizing merely 4\\%-20\\% of the\noriginal post-training data. We provide the anonymous code repository in\n\\href{https://anonymous.4open.science/r/PASER-E606}{Link}.", "AI": {"tldr": "PASER is a method for efficient recovery of pruned large language models by selecting appropriate recovery instructions.", "motivation": "To address performance degradation in pruned large language models and the high computational costs of existing recovery methods.", "method": "PASER utilizes manifold learning and spectral clustering to identify and group instructions based on model capability degradation, then allocates data budgets adaptively across these groups and filters out irrelevant data.", "result": "Experimental results show that PASER significantly outperforms traditional methods, recovering pruned model capabilities using only 4%-20% of the original data.", "conclusion": "PASER effectively enhances the performance of pruned language models while being more data-efficient compared to conventional methods.", "key_contributions": ["Introduction of PASER for effective recovery of pruned LLMs", "Use of manifold learning and spectral clustering for instruction selection", "Demonstration of significant performance recovery with reduced data usage"], "limitations": "", "keywords": ["model pruning", "large language models", "data selection", "instruction tuning", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14359", "pdf": "https://arxiv.org/pdf/2502.14359.pdf", "abs": "https://arxiv.org/abs/2502.14359", "title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests", "authors": ["Filippo Momentè", "Alessandro Suglia", "Mario Giulianelli", "Ambra Ferrari", "Alexander Koller", "Oliver Lemon", "David Schlangen", "Raquel Fernández", "Raffaella Bernardi"], "categories": ["cs.CL"], "comment": null, "summary": "We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs.", "AI": {"tldr": "This paper compares the efficacy of evaluation methods for language models, focusing on standard benchmarks versus interactive games.", "motivation": "The paper investigates how well different evaluation paradigms can discriminate the performance of language models (LLMs) of varying quality.", "method": "The authors analyze three evaluation paradigms: standard benchmarks, interactive games, and cognitive tests, and correlate these methods with model performance.", "result": "Interactive games are found to be more effective than standard benchmarks in discriminating LLM performance, with specific cognitive skills correlating differently with each method.", "conclusion": "The study recommends the creation of new interactive benchmarks and cognitive assessments tailored for language models.", "key_contributions": ["Identification of superior evaluation methods for LLMs based on cognitive assessments.", "Comparison of standard benchmarks and interactive games in discriminating model quality.", "Establishment of correlations between cognitive abilities and LLM performance."], "limitations": "", "keywords": ["evaluation paradigms", "language models", "interactive benchmarks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.14614", "pdf": "https://arxiv.org/pdf/2502.14614.pdf", "abs": "https://arxiv.org/abs/2502.14614", "title": "ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis", "authors": ["Mingyi Jia", "Zhihao Jia", "Junwen Duan", "Yan Song", "Jianxin Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Large Language Models~(LLMs), which integrate external\nknowledge, have shown remarkable performance in medical domains, including\nclinical diagnosis. However, existing RAG methods often struggle to tailor\nretrieval strategies to diagnostic difficulty and input sample informativeness.\nThis limitation leads to excessive and often unnecessary retrieval, impairing\ncomputational efficiency and increasing the risk of introducing noise that can\ndegrade diagnostic accuracy. To address this, we propose ICA-RAG\n(\\textbf{I}nformation \\textbf{C}ompleteness Guided \\textbf{A}daptive\n\\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration), a novel framework\nfor enhancing RAG reliability in disease diagnosis. ICA-RAG utilizes an\nadaptive control module to assess the necessity of retrieval based on the\ninput's information completeness. By optimizing retrieval and incorporating\nknowledge filtering, ICA-RAG better aligns retrieval operations with clinical\nrequirements. Experiments on three Chinese electronic medical record datasets\ndemonstrate that ICA-RAG significantly outperforms baseline methods,\nhighlighting its effectiveness in clinical diagnosis.", "AI": {"tldr": "ICA-RAG is a new framework that improves retrieval-augmented generation in clinical diagnosis by optimizing retrieval based on input information completeness.", "motivation": "Existing RAG methods struggle to tailor retrieval strategies, leading to inefficient retrieval and noise introduction, impacting diagnostic accuracy.", "method": "ICA-RAG employs an adaptive control module that evaluates the need for retrieval based on the completeness of input information, optimizing the retrieval process and utilizing knowledge filtering.", "result": "In experiments on three Chinese electronic medical record datasets, ICA-RAG demonstrated significant performance improvements over baseline methods in clinical diagnosis.", "conclusion": "ICA-RAG enhances the reliability of retrieval-augmented generation by aligning retrieval operations with clinical requirements effectively.", "key_contributions": ["Introduction of ICA-RAG framework for RAG in clinical settings", "Adaptive control for retrieval necessity based on information completeness", "Demonstrated significant performance improvement in clinical diagnosis tasks."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Clinical Diagnosis", "Information Completeness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14645", "pdf": "https://arxiv.org/pdf/2502.14645.pdf", "abs": "https://arxiv.org/abs/2502.14645", "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.", "AI": {"tldr": "The paper presents a method for efficient cross-lingual knowledge editing in large language models without full retraining, introducing a state-of-the-art approach called X-KDE.", "motivation": "The inefficiency of existing methods in achieving true cross-linguistic knowledge synchronization in large language models.", "method": "The method involves two stages: Cross-lingual Edition Instruction Tuning (XE-IT) for fine-tuning on a curated parallel dataset and Target-language Preference Optimization (TL-PO) for consistency across languages using advanced optimization techniques.", "result": "The X-KDE method significantly enhances cross-lingual performance with an average improvement of +8.19% on benchmarks, while maintaining high accuracy in monolingual settings.", "conclusion": "X-KDE provides a practical solution for efficient adaptation of LLMs to new information across languages, benefiting from a high-quality dataset designed for this purpose.", "key_contributions": ["Introduction of the X-KDE method for cross-lingual knowledge editing", "Development of a high-quality cross-lingual dataset", "Demonstrated significant performance improvements on benchmarks"], "limitations": "", "keywords": ["cross-lingual", "knowledge editing", "large language models", "multilingual", "AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.15594", "pdf": "https://arxiv.org/pdf/2502.15594.pdf", "abs": "https://arxiv.org/abs/2502.15594", "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention", "authors": ["Jiaqi Wu", "Chen Chen", "Chunyan Hou", "Xiaojie Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation, we propose SafeIntervention (SafeInt), a\nnovel defense method that shields LLMs from jailbreak attacks through\nsafety-aware representation intervention. Built on our analysis of the\nrepresentations of jailbreak samples, the core idea of SafeInt is to relocate\njailbreak-related representations into the rejection region. This is achieved\nby intervening in the representation distributions of jailbreak samples to\nalign them with those of unsafe samples. We conduct comprehensive experiments\ncovering six jailbreak attacks, two jailbreak datasets, and two utility\nbenchmarks. Experimental results demonstrate that SafeInt outperforms all\nbaselines in defending LLMs against jailbreak attacks while largely maintaining\nutility. Additionally, we evaluate SafeInt against adaptive attacks and verify\nits effectiveness in mitigating real-time attacks.", "AI": {"tldr": "SafeIntervention (SafeInt) is a novel defense method for large language models (LLMs) that protects against jailbreak attacks by dynamically adjusting the representation of harmful queries.", "motivation": "The need for effective and efficient defenses against jailbreak attacks on LLMs to ensure compliance with safety standards.", "method": "SafeInt intervenes in the representation distributions of jailbreak samples, relocating harmful representations into a rejection region to align them with unsafe sample representations.", "result": "SafeInt outperforms all baseline defenses across multiple jailbreak attacks and maintains utility.", "conclusion": "SafeInt is an effective and dynamic defense method that significantly mitigates the risk of jailbreak attacks on LLMs.", "key_contributions": ["Introduction of SafeIntervention (SafeInt) for LLM safety", "Demonstration of effectiveness against various jailbreak attacks", "Dynamic adjustment of representations to enhance safety"], "limitations": "", "keywords": ["Large Language Models", "Jailbreak Attacks", "SafeIntervention", "Defenses", "Representation Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.16529", "pdf": "https://arxiv.org/pdf/2502.16529.pdf", "abs": "https://arxiv.org/abs/2502.16529", "title": "Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation", "authors": ["Deokhyung Kang", "Jeonghun Cho", "Yejin Jeon", "Sunbin Jang", "Minsub Lee", "Jawoon Cho", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 (Main, long paper)", "summary": "Visual programming languages (VPLs) allow users to create programs through\ngraphical interfaces, which results in easier accessibility and their\nwidespread usage in various domains. To further enhance this accessibility,\nrecent research has focused on generating VPL code from user instructions using\nlarge language models (LLMs). Specifically, by employing prompting-based\nmethods, these studies have shown promising results. Nevertheless, such\napproaches can be less effective for industrial VPLs such as Ladder Diagram\n(LD). LD is a pivotal language used in industrial automation processes and\ninvolves extensive domain-specific configurations, which are difficult to\ncapture in a single prompt. In this work, we demonstrate that training-based\nmethods outperform prompting-based methods for LD generation accuracy, even\nwith smaller backbone models. Building on these findings, we propose a\ntwo-stage training strategy to further enhance VPL generation. First, we employ\nretrieval-augmented fine-tuning to leverage the repetitive use of subroutines\ncommonly seen in industrial VPLs. Second, we apply direct preference\noptimization (DPO) to further guide the model toward accurate outputs, using\nsystematically generated preference pairs through graph editing operations.\nExtensive experiments on real-world LD data demonstrate that our approach\nimproves program-level accuracy by over 10% compared to supervised fine-tuning,\nwhich highlights its potential to advance industrial automation.", "AI": {"tldr": "This paper presents a two-stage training strategy that enhances the generation of Ladder Diagram (LD) code using large language models (LLMs), improving accuracy significantly compared to existing methods.", "motivation": "To enhance the accessibility of visual programming languages (VPLs) in industrial automation, particularly focusing on the limitations of existing prompting-based methods.", "method": "The authors propose a two-stage training strategy combining retrieval-augmented fine-tuning and direct preference optimization (DPO) to improve the generation of Ladder Diagrams from user instructions.", "result": "The proposed method improves program-level accuracy by over 10% on real-world LD data compared to traditional supervised fine-tuning methods.", "conclusion": "This work demonstrates the effectiveness of training-based methods over prompting-based methods for LD generation, with potential applications in advancing industrial automation.", "key_contributions": ["Introduction of a two-stage training strategy for LD code generation", "Demonstration that retrieval-augmented fine-tuning and DPO improve accuracy", "Improved accuracy on real-world LD data by over 10% compared to existing methods."], "limitations": "", "keywords": ["visual programming languages", "large language models", "Ladder Diagram", "industrial automation", "code generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.17041", "pdf": "https://arxiv.org/pdf/2502.17041.pdf", "abs": "https://arxiv.org/abs/2502.17041", "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance", "authors": ["Haoran Li", "Wenbin Hu", "Huihao Jing", "Yulin Chen", "Qi Hu", "Sirui Han", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance.", "AI": {"tldr": "The paper introduces PrivaCI-Bench, a benchmark for evaluating the privacy of large language models (LLMs) based on Contextual Integrity theory, going beyond traditional privacy measures.", "motivation": "The reliability and trustworthiness of generative LLMs are in doubt due to concerns surrounding data privacy, prompting the need for comprehensive privacy evaluation measures.", "method": "The authors present PrivaCI-Bench, a benchmark that includes well-annotated privacy regulations, real court cases, privacy policies, and synthetic data for evaluating LLMs on privacy compliance under Contextual Integrity.", "result": "Experimental results reveal that while LLMs capture key Contextual Integrity parameters, they still need improvements for better compliance with privacy standards.", "conclusion": "PrivaCI-Bench provides a thorough framework for evaluating the privacy of LLMs, highlighting areas requiring further enhancements to ensure privacy compliance.", "key_contributions": ["Introduction of PrivaCI-Bench for contextual privacy evaluation of LLMs.", "Incorporation of Contextual Integrity theory in LLM privacy assessments.", "Evaluation of state-of-the-art LLMs on privacy compliance."], "limitations": "The benchmark currently focuses on legal compliance and might not cover all aspects of privacy in diverse contexts.", "keywords": ["privacy", "large language models", "Contextual Integrity", "evaluation benchmark", "data compliance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.17262", "pdf": "https://arxiv.org/pdf/2502.17262.pdf", "abs": "https://arxiv.org/abs/2502.17262", "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective", "authors": ["Chengyin Xu", "Kaiyuan Chen", "Xiao Li", "Ke Shen", "Chenggang Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,6 figures", "summary": "The escalating scale and cost of Large Language Models (LLMs) training\nnecessitate accurate pre-training prediction of downstream task performance for\nefficient resource allocation. This is challenged by: 1) the emergence\nphenomenon, where metrics become meaningful only after extensive training,\nhindering prediction by smaller models; and 2) uneven task difficulty and\ninconsistent performance scaling patterns, leading to high metric variability.\nCurrent prediction methods lack accuracy and reliability. We propose a\nClustering-On-Difficulty (COD) framework for downstream performance prediction.\nThe COD framework clusters tasks by their difficulty scaling features, thereby\nestablishing a more stable and predictable support subset through the exclusion\nof tasks exhibiting non-emergent behavior or irregular scaling. We adopt a\nperformance scaling law to predict cluster-wise performance with theoretical\nsupport. Predictable subset performance acts as an intermediate predictor for\nthe full evaluation set. We further derive a mapping function to accurately\nextrapolate the performance of the subset to the full set. Applied to an LLM\nwith 70B parameters, COD achieved a 1.36% average prediction error across eight\nkey LLM benchmarks, offering actionable insights for resource allocation and\ntraining monitoring of LLMs pretraining.", "AI": {"tldr": "Proposes a COD framework for better downstream task performance prediction of LLMs by clustering tasks based on difficulty.", "motivation": "Accurate performance prediction of LLMs is crucial for efficient training resource allocation, but current methods are inconsistent and unreliable due to the emergence phenomenon and task complexity.", "method": "The COD framework clusters tasks based on their difficulty features and uses performance scaling laws to predict cluster-wise performance, which is then mapped to full set performance.", "result": "The COD framework achieved an average prediction error of 1.36% across eight LLM benchmarks when applied to a 70B parameter LLM.", "conclusion": "The COD framework provides a more stable and predictable way for allocating resources in LLM pre-training.", "key_contributions": ["Introduction of the Clustering-On-Difficulty framework", "Demonstration of improved accuracy in performance prediction", "Development of a mapping function for performance extrapolation"], "limitations": "", "keywords": ["Large Language Models", "performance prediction", "Clustering-On-Difficulty", "resource allocation", "scaling laws"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.19614", "pdf": "https://arxiv.org/pdf/2502.19614.pdf", "abs": "https://arxiv.org/abs/2502.19614", "title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review", "authors": ["Sungduk Yu", "Man Luo", "Avinash Madusu", "Vasudev Lal", "Phillip Howard"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in large language models (LLMs), a new risk to the peer review\nprocess is that negligent reviewers will rely on LLMs to perform the often time\nconsuming process of reviewing a paper. However, there is a lack of existing\nresources for benchmarking the detectability of AI text in the domain of peer\nreview. To address this deficiency, we introduce a comprehensive dataset\ncontaining a total of 788,984 AI-written peer reviews paired with corresponding\nhuman reviews, covering 8 years of papers submitted to each of two leading AI\nresearch conferences (ICLR and NeurIPS). We use this new resource to evaluate\nthe ability of 18 existing AI text detection algorithms to distinguish between\npeer reviews fully written by humans and different state-of-the-art LLMs.\nAdditionally, we explore a context-aware detection method called Anchor, which\nleverages manuscript content to detect AI-generated reviews, and analyze the\nsensitivity of detection models to LLM-assisted editing of human-written text.\nOur work reveals the difficulty of identifying AI-generated text at the\nindividual peer review level, highlighting the urgent need for new tools and\nmethods to detect this unethical use of generative AI. Our dataset is publicly\navailable at:\nhttps://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.", "AI": {"tldr": "This paper introduces a dataset for benchmarking the detection of AI-generated peer reviews and evaluates existing AI detection algorithms.", "motivation": "There is a growing concern about the reliance on LLMs in the peer review process, which could compromise the integrity of scientific research due to negligent reviewers.", "method": "A dataset of 788,984 AI-generated and corresponding human peer reviews was created, and 18 AI text detection algorithms were evaluated against this dataset. The study also explored a context-aware detection method called Anchor.", "result": "The evaluation revealed challenges in distinguishing AI-generated peer reviews from human-written ones, emphasizing the need for improved detection tools.", "conclusion": "The findings underscore the difficulty of identifying AI-generated text in peer reviews and call for new methodologies to combat this issue.", "key_contributions": ["Creation of a large dataset for AI-generated peer review detection", "Evaluation of existing AI detection algorithms", "Introduction of a context-aware detection method (Anchor)"], "limitations": "The study focuses on specific AI detection algorithms and may not cover all existing methods or the broader implications of AI in peer review.", "keywords": ["AI peer review", "AI text detection", "human-computer interaction", "generative AI", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.19779", "pdf": "https://arxiv.org/pdf/2502.19779.pdf", "abs": "https://arxiv.org/abs/2502.19779", "title": "Do Retrieval-Augmented Language Models Adapt to Varying User Needs?", "authors": ["Peilin Wu", "Xinlu Zhang", "Wenhao Yu", "Xingyu Liu", "Xinya Du", "Zhiyu Zoey Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Updated the motivation, data selection and creation process, and\n  terminology of some keywords for better writing. The updates are mianly in\n  introduction and experiment section", "summary": "Recent advancements in Retrieval-Augmented Language Models (RALMs) have\ndemonstrated their efficacy in knowledge-intensive tasks. However, existing\nevaluation benchmarks often assume a single optimal approach to leveraging\nretrieved information, failing to account for varying user needs. This paper\nintroduces a novel evaluation framework that systematically assesses RALMs\nunder three user need cases-Context-Exclusive, Context-First, and\nMemory-First-across three distinct context settings: Context Matching,\nKnowledge Conflict, and Information Irrelevant. By varying both user\ninstructions and the nature of retrieved information, our approach captures the\ncomplexities of real-world applications where models must adapt to diverse user\nrequirements. Through extensive experiments on multiple QA datasets, including\nHotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find\nthat restricting memory usage improves robustness in adversarial retrieval\nconditions but decreases peak performance with ideal retrieval results and\nmodel family dominates behavioral differences. Our findings highlight the\nnecessity of user-centric evaluations in the development of retrieval-augmented\nsystems and provide insights into optimizing model performance across varied\nretrieval contexts. We will release our code and URAQ dataset upon acceptance\nof the paper.", "AI": {"tldr": "This paper presents a new evaluation framework for Retrieval-Augmented Language Models (RALMs) that incorporates varying user needs and context settings to better assess model performance in knowledge-intensive tasks.", "motivation": "Existing benchmarks for evaluating RALMs often assume a single optimal approach and overlook the diverse needs of users. This research seeks to address these limitations by developing a user-centric evaluation framework.", "method": "The paper introduces an evaluation framework that assesses RALMs across three user need cases—Context-Exclusive, Context-First, and Memory-First—while considering multiple context settings including Context Matching, Knowledge Conflict, and Information Irrelevant. The methodology involves varying user instructions and the nature of retrieved information during extensive experiments on various QA datasets.", "result": "The experiments reveal that restricting memory usage enhances robustness under adverse retrieval conditions but may decrease peak performance with optimal retrieval results, indicating that model family influences behavioral differences.", "conclusion": "The study emphasizes the importance of user-centric evaluations in retrieval-augmented systems and offers insights for optimizing model performance according to different retrieval contexts.", "key_contributions": ["Introduction of a user-centric evaluation framework for RALMs", "Identification of three user need cases for assessment", "Demonstration of how retrieval strategies impact model robustness and performance"], "limitations": "The framework may not encompass all potential user needs and contexts in real-world applications.", "keywords": ["Retrieval-Augmented Language Models", "Human-Centric Evaluation", "Knowledge-Intensive Tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.02003", "pdf": "https://arxiv.org/pdf/2503.02003.pdf", "abs": "https://arxiv.org/abs/2503.02003", "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs", "authors": ["Tin Nguyen", "Logan Bolton", "Mohammad Reza Taesiri", "Anh Totti Nguyen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.", "AI": {"tldr": "Proposes Highlighted Chain-of-Thought Prompting (HoT) to improve accuracy of LLMs by using XML tags to ground facts in responses.", "motivation": "Address the issue of Large Language Models hallucinating non-factual statements, complicating human verification of responses.", "method": "Introduces HoT technique where LLMs format questions with XML tags to emphasize key facts and produce highlighted responses.", "result": "In few-shot settings, HoT surpasses standard chain-of-thought prompting on 17 diverse tasks. It assists users in verifying responses effectively but can mislead them about incorrect outputs.", "conclusion": "HoT increases efficiency in recognizing correct LLM outputs but poses risks of misleading trust when the model is incorrect.", "key_contributions": ["Introduction of XML tagging for grounding facts in LLM responses", "Demonstrated superiority of HoT over traditional prompting in various tasks", "Insights into user interaction with highlighted responses"], "limitations": "HoT may mislead users to believe incorrect answers are correct when highlights are involved.", "keywords": ["Large Language Models", "Highlighted Chain-of-Thought Prompting", "fact verification", "user interaction", "hallucination"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.02623", "pdf": "https://arxiv.org/pdf/2503.02623.pdf", "abs": "https://arxiv.org/abs/2503.02623", "title": "Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models", "authors": ["Paul Stangel", "David Bani-Harouni", "Chantal Pellegrini", "Ege Özsoy", "Kamilia Zaripova", "Matthias Keicher", "Nassir Navab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We propose a novel\nReinforcement Learning approach that allows to directly fine-tune LLMs to\nexpress calibrated confidence estimates alongside their answers to factual\nquestions. Our method optimizes a reward based on the logarithmic scoring rule,\nexplicitly penalizing both over- and under-confidence. This encourages the\nmodel to align its confidence estimates with the actual predictive accuracy.\nThe optimal policy under our reward design would result in perfectly calibrated\nconfidence expressions. Unlike prior approaches that decouple confidence\nestimation from response generation, our method integrates confidence\ncalibration seamlessly into the generative process of the LLM. Empirically, we\ndemonstrate that models trained with our approach exhibit substantially\nimproved calibration and generalize to unseen tasks without further\nfine-tuning, suggesting the emergence of general confidence awareness. We\nprovide our training and evaluation code in the supplementary and will make it\npublicly available upon acceptance.", "AI": {"tldr": "This paper presents a novel Reinforcement Learning method for fine-tuning Large Language Models to express calibrated confidence in their responses to factual questions.", "motivation": "To ensure safe and trustworthy use of LLMs by improving the accuracy of confidence estimates in their answers.", "method": "A Reinforcement Learning approach that fine-tunes LLMs by optimizing a reward based on the logarithmic scoring rule, penalizing both over- and under-confidence.", "result": "Models trained with this method show improved calibration of confidence estimates and can generalize to unseen tasks without further fine-tuning.", "conclusion": "The proposed approach integrates confidence calibration into the generative process of LLMs, resulting in enhanced performance and general confidence awareness.", "key_contributions": ["Novel reward design for fine-tuning LLMs with calibrated confidence estimates", "Integration of confidence calibration with response generation", "Demonstrated generalization to unseen tasks without additional fine-tuning."], "limitations": "", "keywords": ["Large Language Models", "Confidence Calibration", "Reinforcement Learning", "Generative Models", "Logarithmic Scoring Rule"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.03303", "pdf": "https://arxiv.org/pdf/2503.03303.pdf", "abs": "https://arxiv.org/abs/2503.03303", "title": "SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection", "authors": ["Yi-Fan Lu", "Xian-Ling Mao", "Tian Lan", "Tong Zhang", "Yu-Shi Zhu", "Heyan Huang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Automatic evaluation for Open Domain Event Detection (ODED) is a highly\nchallenging task, because ODED is characterized by a vast diversity of\nun-constrained output labels from various domains. Nearly all existing\nevaluation methods for ODED usually first construct evaluation benchmarks with\nlimited labels and domain coverage, and then evaluate ODED methods using\nmetrics based on token-level label matching rules. However, this kind of\nevaluation framework faces two issues: (1) The limited evaluation benchmarks\nlack representatives of the real world, making it difficult to accurately\nreflect the performance of various ODED methods in real-world scenarios; (2)\nEvaluation metrics based on token-level matching rules fail to capture semantic\nsimilarity between predictions and golden labels. To address these two problems\nabove, we propose a scalable and reliable Semantic-level Evaluation framework\nfor Open domain Event detection (SEOE) by constructing a more representative\nevaluation benchmark and introducing a semantic evaluation metric.\nSpecifically, our proposed framework first constructs a scalable evaluation\nbenchmark that currently includes 564 event types covering 7 major domains,\nwith a cost-effective supplementary annotation strategy to ensure the\nbenchmark's representativeness. The strategy also allows for the supplement of\nnew event types and domains in the future. Then, the proposed SEOE leverages\nlarge language models (LLMs) as automatic evaluation agents to compute a\nsemantic F1-score, incorporating fine-grained definitions of semantically\nsimilar labels to enhance the reliability of the evaluation. Extensive\nexperiments validate the representatives of the benchmark and the reliability\nof the semantic evaluation metric. Existing ODED methods are thoroughly\nevaluated, and the error patterns of predictions are analyzed, revealing\nseveral insightful findings.", "AI": {"tldr": "This paper addresses the challenges of automatic evaluation in Open Domain Event Detection (ODED) by introducing a scalable Semantic-level Evaluation framework (SEOE) that enhances representativeness and evaluation reliability using large language models (LLMs).", "motivation": "The motivation behind this work is to resolve issues in existing ODED evaluation methods, particularly their limited representation in benchmarks and the inadequacy of token-level metrics which fail to capture semantic similarities between predictions and actual labels.", "method": "The proposed framework constructs a scalable evaluation benchmark with 564 event types across 7 domains, and employs large language models to compute a semantic F1-score as a more reliable evaluation metric.", "result": "The framework was validated through extensive experiments, demonstrating the representativeness of the benchmark and the reliability of the new semantic evaluation metric.", "conclusion": "The study concludes that the SEOE framework significantly improves the evaluation of ODED methods, providing insights into error patterns in predictions.", "key_contributions": ["Introduction of a scalable evaluation benchmark for ODED with broad domain coverage", "Development of a semantic evaluation metric using LLMs", "Analysis of comprehensive error patterns in existing ODED methods"], "limitations": "", "keywords": ["Open Domain Event Detection", "Semantic Evaluation", "Large Language Models", "Benchmark Construction", "Event Detection"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.04556", "pdf": "https://arxiv.org/pdf/2503.04556.pdf", "abs": "https://arxiv.org/abs/2503.04556", "title": "Compositional Causal Reasoning Evaluation in Language Models", "authors": ["Jacqueline R. M. A. Maasch", "Alihan Hüyük", "Xinnuo Xu", "Aditya V. Nori", "Javier Gonzalez"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Causal reasoning and compositional reasoning are two core aspirations in AI.\nMeasuring the extent of these behaviors requires principled evaluation methods.\nWe explore a unified perspective that considers both behaviors simultaneously,\ntermed compositional causal reasoning (CCR): the ability to infer how causal\nmeasures compose and, equivalently, how causal quantities propagate through\ngraphs. We instantiate a framework for the systematic evaluation of CCR for the\naverage treatment effect and the probability of necessity and sufficiency. As\nproof of concept, we demonstrate CCR evaluation for language models in the\nLLama, Phi, and GPT families. On a math word problem, our framework revealed a\nrange of taxonomically distinct error patterns. CCR errors increased with the\ncomplexity of causal paths for all models except o1.", "AI": {"tldr": "The paper introduces a unified evaluation framework for compositional causal reasoning (CCR) in AI, especially focusing on language models.", "motivation": "Causal reasoning and compositional reasoning are fundamental challenges in AI, necessitating effective evaluation methods to measure these behaviors.", "method": "A unified framework for assessing compositional causal reasoning (CCR), applying it to average treatment effect and probabilities of necessity and sufficiency in causal inference.", "result": "The framework was tested on language models such as LLama, Phi, and GPT, revealing distinct error patterns that increased with complexity in causal paths, except for one model.", "conclusion": "The study highlights the importance of evaluating compositional causal reasoning in AI models, with implications for interpretability and error analysis in language models.", "key_contributions": ["Introduction of a unified evaluation framework for compositional causal reasoning (CCR)", "Demonstration of distinct error patterns across different language models", "Application of CCR to assess average treatment effects and causal probabilities"], "limitations": "", "keywords": ["compositional reasoning", "causal reasoning", "language models", "evaluation framework", "error analysis"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2503.17900", "pdf": "https://arxiv.org/pdf/2503.17900.pdf", "abs": "https://arxiv.org/abs/2503.17900", "title": "MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation", "authors": ["Hsin-Ling Hsu", "Cong-Tinh Dao", "Luning Wang", "Zitao Shuai", "Thao Nguyen Minh Phan", "Jun-En Ding", "Chun-Chieh Liao", "Pengfei Hu", "Xiaoxue Han", "Chih-Ho Hsu", "Dongsheng Luo", "Wen-Chih Peng", "Feng Liu", "Fang-Ming Hung", "Chenwei Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\n\\ours{}, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality.", "AI": {"tldr": "A novel framework aligns LLMs with clinician workflows for improved treatment planning.", "motivation": "Current LLM applications in EHR focus on assessment rather than effective treatment planning, with critical limitations such as single-pass generation and lack of patient-specific context.", "method": "Introduces a two-stage architecture that first assesses clinical data and then generates a structured treatment plan utilizing retrieval-augmented generation.", "result": "Demonstrates significant improvements in assessment accuracy and treatment plan quality compared to baseline approaches.", "conclusion": "The proposed framework effectively integrates LLMs into clinician workflows, enhancing both assessment and treatment planning processes.", "key_contributions": ["Introduces a two-stage LLM framework for EHR that mimics clinician workflows", "Incorporates patient-specific historical context into treatment planning", "Improves assessment accuracy and treatment plan quality significantly"], "limitations": "", "keywords": ["large language models", "electronic health records", "treatment planning", "machine learning", "health informatics"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2503.20556", "pdf": "https://arxiv.org/pdf/2503.20556.pdf", "abs": "https://arxiv.org/abs/2503.20556", "title": "A Retrieval-Based Approach to Medical Procedure Matching in Romanian", "authors": ["Andrei Niculae", "Adrian Cosma", "Emilian Radoi"], "categories": ["cs.CL"], "comment": "Accepted at BIONLP 2025 and Shared Tasks, ACL 2025", "summary": "Accurately mapping medical procedure names from healthcare providers to\nstandardized terminology used by insurance companies is a crucial yet complex\ntask. Inconsistencies in naming conventions lead to missclasified procedures,\ncausing administrative inefficiencies and insurance claim problems in private\nhealthcare settings. Many companies still use human resources for manual\nmapping, while there is a clear opportunity for automation. This paper proposes\na retrieval-based architecture leveraging sentence embeddings for medical name\nmatching in the Romanian healthcare system. This challenge is significantly\nmore difficult in underrepresented languages such as Romanian, where existing\npretrained language models lack domain-specific adaptation to medical text. We\nevaluate multiple embedding models, including Romanian, multilingual, and\nmedical-domain-specific representations, to identify the most effective\nsolution for this task. Our findings contribute to the broader field of medical\nNLP for low-resource languages such as Romanian.", "AI": {"tldr": "This paper discusses a retrieval-based architecture using sentence embeddings to map medical procedure names from Romanian healthcare providers to standardized terms, addressing issues in underrepresented languages.", "motivation": "To automate the mapping of medical procedure names for reducing administrative inefficiencies and insurance claim problems in Romanian healthcare, where current manual methods are prevalent.", "method": "Evaluation of multiple embedding models, including Romanian, multilingual, and medical-domain-specific representations, for effective medical name matching.", "result": "The study identifies the most effective embedding model for medical name matching in the Romanian healthcare system despite the challenges posed by underrepresented languages.", "conclusion": "Findings advance medical NLP initiatives for low-resource languages, showcasing the potential for automation in healthcare terminology mapping.", "key_contributions": ["Proposed a retrieval-based architecture for medical name mapping.", "Evaluated various embedding models specific to the Romanian healthcare system.", "Contributed to the medical NLP field for underrepresented languages."], "limitations": "", "keywords": ["medical terminology", "sentence embeddings", "Romanian healthcare", "NLP", "low-resource languages"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.20641", "pdf": "https://arxiv.org/pdf/2503.20641.pdf", "abs": "https://arxiv.org/abs/2503.20641", "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "authors": ["Han Wu", "Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Xiaojin Fu", "Xiongwei Han", "Xing Li", "Hui-Ling Zhen", "Tao Zhong", "Mingxuan Yuan"], "categories": ["cs.CL"], "comment": "Technical report", "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", "AI": {"tldr": "This paper presents an empirical study on model merging for Long-to-Short (L2S) reasoning in large language models (LLMs), showcasing its potential to enhance efficiency while maintaining output quality.", "motivation": "To address the efficiency problem of LLMs transitioning from System 1 to System 2 reasoning, which often leads to redundant outputs without proportional improvements in quality.", "method": "The paper explores various methodologies for model merging, including task-vector-based, SVD-based, and activation-informed merging, assessing their impact on L2S reasoning.", "result": "Model merging reduces average response length by up to 55% and can maintain or even improve baseline performance across different model scales (1.5B to 32B parameters).", "conclusion": "Model merging is presented as a highly effective solution for enhancing L2S reasoning efficiency while preserving the robustness of System 2 reasoning; the methodology allows adaptive response lengths based on task complexity.", "key_contributions": ["Empirical exploration of model merging for L2S reasoning.", "Demonstration of substantial efficiency gains without sacrificing output quality.", "Identification of correlations between model scale and merging efficacy."], "limitations": "", "keywords": ["Large Language Models", "Model Merging", "Long-to-Short Reasoning", "System 1", "System 2"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.04141", "pdf": "https://arxiv.org/pdf/2504.04141.pdf", "abs": "https://arxiv.org/abs/2504.04141", "title": "Cognitive Debiasing Large Language Models for Decision-Making", "authors": ["Yougang Lyu", "Shijie Ren", "Yue Feng", "Zihan Wang", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal assistants in the\nfinancial, healthcare, and legal domains. While prompt engineering strategies\nhave enhanced the capabilities of LLMs in decision-making, cognitive biases\ninherent to LLMs present significant challenges. Cognitive biases are\nsystematic patterns of deviation from norms or rationality in decision-making\nthat can lead to the production of inaccurate outputs. Existing cognitive bias\nmitigation strategies assume that input prompts only contain one type of\ncognitive bias, limiting their effectiveness in more challenging scenarios\ninvolving multiple cognitive biases. To fill this gap, we propose a cognitive\ndebiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the\nreliability of LLMs by iteratively refining prompts. Our method follows three\nsequential steps -- bias determination, bias analysis, and cognitive debiasing\n-- to iteratively mitigate potential cognitive biases in prompts. Experimental\nresults on finance, healthcare, and legal decision-making tasks, using both\nclosed-source and open-source LLMs, demonstrate that the proposed SACD method\noutperforms both advanced prompt engineering methods and existing cognitive\ndebiasing techniques in average accuracy under single-bias and multi-bias\nsettings.", "AI": {"tldr": "The paper presents a new approach, Self-Adaptive Cognitive Debiasing (SACD), to enhance decision-making applications of large language models (LLMs) by iteratively refining prompts to mitigate cognitive biases.", "motivation": "Large language models (LLMs) can aid decision-making in various domains, but inherent cognitive biases create challenges that current debiasing strategies do not adequately address.", "method": "The SACD method consists of three steps: bias determination, bias analysis, and cognitive debiasing. This iterative process refines the prompts to reduce cognitive biases effectively.", "result": "Experimental results indicate that SACD improves accuracy in decision-making tasks across finance, healthcare, and legal domains, outperforming existing techniques in both single and multiple bias scenarios.", "conclusion": "The proposed SACD approach demonstrates significant enhancements in the reliability of LLM outputs by addressing cognitive biases more comprehensively than existing methods.", "key_contributions": ["Introduction of Self-Adaptive Cognitive Debiasing (SACD) approach", "Demonstration of effectiveness in tasks involving multiple cognitive biases", "Comparison shows SACD outperforms existing debiasing techniques and prompt engineering methods"], "limitations": "The effectiveness of SACD may vary based on the complexity of biases and the specific context of applications.", "keywords": ["large language models", "cognitive biases", "debiasing", "decision-making", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.07199", "pdf": "https://arxiv.org/pdf/2504.07199.pdf", "abs": "https://arxiv.org/abs/2504.07199", "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog", "authors": ["Jennifer D'Souza", "Sameer Sadruddin", "Holger Israel", "Mathias Begoin", "Diana Slawig"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper", "summary": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.", "AI": {"tldr": "This paper describes SemEval-2025 Task 5, focused on automated subject tagging for scientific records using LLMs.", "motivation": "The task aims to enhance the classification of scientific and technical records by utilizing LLMs for automated subject tagging based on the GND taxonomy.", "method": "Participants developed LLM-based systems that recommend top-k subjects, evaluated through quantitative metrics and qualitative assessments.", "result": "The results demonstrate the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing in subject tagging.", "conclusion": "The findings provide insights into the application of LLMs in digital library classification, which could improve how scientific records are categorized.", "key_contributions": ["Introduction of a shared task for subject tagging using LLMs", "Use of LLM ensembles and synthetic data in classification", "Evaluation framework including both quantitative and qualitative assessments."], "limitations": "", "keywords": ["LLMs", "subject tagging", "digital libraries", "GND taxonomy", "multilingual processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.07228", "pdf": "https://arxiv.org/pdf/2504.07228.pdf", "abs": "https://arxiv.org/abs/2504.07228", "title": "ConceptCarve: Dynamic Realization of Evidence", "authors": ["Eylon Caplan", "Dan Goldwasser"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Finding evidence for human opinion and behavior at scale is a challenging\ntask, often requiring an understanding of sophisticated thought patterns among\nvast online communities found on social media. For example, studying how gun\nownership is related to the perception of Freedom, requires a retrieval system\nthat can operate at scale over social media posts, while dealing with two key\nchallenges: (1) identifying abstract concept instances, (2) which can be\ninstantiated differently across different communities. To address these, we\nintroduce ConceptCarve, an evidence retrieval framework that utilizes\ntraditional retrievers and LLMs to dynamically characterize the search space\nduring retrieval. Our experiments show that ConceptCarve surpasses traditional\nretrieval systems in finding evidence within a social media community. It also\nproduces an interpretable representation of the evidence for that community,\nwhich we use to qualitatively analyze complex thought patterns that manifest\ndifferently across the communities.", "AI": {"tldr": "Introducing ConceptCarve, a framework for evidence retrieval from social media using LLMs and traditional methods.", "motivation": "To find evidence for human opinion and behavior at scale from online communities, addressing challenges in identifying abstract concepts and their varied instantiations across different groups.", "method": "ConceptCarve combines traditional retrieval methods with LLMs to dynamically adjust the search space during evidence retrieval from social media posts.", "result": "ConceptCarve outperforms traditional retrieval systems in locating relevant evidence within social media communities and provides interpretable representations of complex thought patterns.", "conclusion": "The framework enhances evidence retrieval effectiveness and allows for qualitative analysis of diverse community perspectives.", "key_contributions": ["Development of ConceptCarve retrieval framework", "Dynamic characterization of search space", "Interpretation of community-specific thought patterns"], "limitations": "", "keywords": ["evidence retrieval", "social media", "concept representation", "LLMs", "community analysis"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.08590", "pdf": "https://arxiv.org/pdf/2504.08590.pdf", "abs": "https://arxiv.org/abs/2504.08590", "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction", "authors": ["Nicola Horst", "Davide Mazzaccara", "Antonia Schmidt", "Michael Sullivan", "Filippo Momentè", "Luca Franceschetti", "Philipp Sadler", "Sherzod Hakimov", "Alberto Testoni", "Raffaella Bernardi", "Raquel Fernández", "Alexander Koller", "Oliver Lemon", "David Schlangen", "Mario Giulianelli", "Alessandro Suglia"], "categories": ["cs.CL"], "comment": "Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com", "summary": "Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction.", "AI": {"tldr": "This paper explores using Dialogue Games to provide feedback signals for post-training Large Language Models and introduces the Playpen environment for this purpose.", "motivation": "The recent focus on the interaction between learner and feedback-giver for post-training LLMs emphasizes the need for effective feedback mechanisms in model tuning.", "method": "The authors introduce Playpen for offline and online learning through Dialogue Game self-play, experimenting with methods like supervised fine-tuning and reinforcement learning.", "result": "Results show that imitation learning through supervised fine-tuning enhances performance on unseen instances but harms other skills, whereas interactive learning with GRPO improves overall without skill loss.", "conclusion": "The framework and baseline training setups are released to promote further research in the area of learning through synthetic interaction.", "key_contributions": ["Introduced Playpen environment for Dialogue Game self-play learning.", "Demonstrated effective post-training methods for LLMs, including GRPO.", "Showed the impact of different learning strategies on performance and skill retention."], "limitations": "The study mainly focuses on a small LLM and specific post-training methods; broader applicability may need further research.", "keywords": ["Dialogue Games", "Large Language Models", "Reinforcement Learning", "Feedback signals", "Post-training methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.11456", "pdf": "https://arxiv.org/pdf/2504.11456.pdf", "abs": "https://arxiv.org/abs/2504.11456", "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "Reinforcement learning (RL) with large language models shows promise in\ncomplex reasoning. However, its progress is hindered by the lack of large-scale\ntraining data that is sufficiently challenging, contamination-free and\nverifiable. To this end, we introduce DeepMath-103K, a large-scale mathematical\ndataset designed with high difficulty (primarily levels 5-9), rigorous\ndecontamination against numerous benchmarks, and verifiable answers for\nrule-based RL reward. It further includes three distinct R1 solutions adaptable\nfor diverse training paradigms such as supervised fine-tuning (SFT). Spanning a\nwide range of mathematical topics, DeepMath-103K fosters the development of\ngeneralizable and advancing reasoning. Notably, models trained on DeepMath-103K\nachieve state-of-the-art results on challenging mathematical benchmarks and\ndemonstrate generalization beyond math such as biology, physics and chemistry,\nunderscoring its broad efficacy. Data:\nhttps://huggingface.co/datasets/zwhe99/DeepMath-103K.", "AI": {"tldr": "Introduction of DeepMath-103K dataset for reinforcement learning in mathematical reasoning.", "motivation": "To address the lack of challenging, contamination-free, and verifiable large-scale training data for reinforcement learning with large language models.", "method": "Introducing a large-scale dataset, DeepMath-103K, featuring high difficulty mathematical problems suitable for rule-based RL rewards and adaptable training paradigms.", "result": "Models trained on DeepMath-103K achieve state-of-the-art results on complex mathematical benchmarks and show generalization in other scientific domains.", "conclusion": "DeepMath-103K aids in developing advanced reasoning in models and supports a wide range of applications across various scientific fields.", "key_contributions": ["Introduction of DeepMath-103K dataset for challenging math problems", "Rigorous decontamination against benchmarks", "Demonstrated generalization capabilities beyond mathematics"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Mathematical Dataset"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2504.12898", "pdf": "https://arxiv.org/pdf/2504.12898.pdf", "abs": "https://arxiv.org/abs/2504.12898", "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "authors": ["Zhouhao Sun", "Xiao Ding", "Li Du", "Yunpeng Xu", "Yixuan Ma", "Yang Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (ICD) framework. To\neliminate biases within the instruction-tuning dataset, it is essential to\nensure that these biases do not provide any additional information to predict\nthe answers, i.e., the information gain of these biases for predicting the\nanswers needs to be 0. Under this guidance, this framework utilizes a causal\nintervention-based data rewriting method to automatically and autonomously\nbalance the distribution of instruction-tuning dataset for reducing the\ninformation gain. Subsequently, it employs a standard supervised fine-tuning\nprocess to train LLMs on the debiased dataset. Experimental results show that\nICD can effectively debias LLM to improve its generalizability across different\ntasks.", "AI": {"tldr": "This paper introduces the information gain-guided causal intervention debiasing framework (ICD) to reduce dataset biases in large language models (LLMs), improving their generalizability.", "motivation": "To tackle the limitations of existing debiasing methods for large language models (LLMs), especially regarding dataset biases that affect inference and generalizability.", "method": "The ICD framework combines causal mechanisms with information theory to balance instruction-tuning datasets through a data rewriting method that eliminates biases contributing information for answer prediction.", "result": "Experimental results demonstrate that the ICD framework effectively reduces biases in LLMs, leading to enhanced performance across various tasks.", "conclusion": "The ICD framework offers a promising solution for improving the robustness of LLMs by addressing inherent biases in training datasets through causal interventions.", "key_contributions": ["Introduction of the ICD framework combining causal mechanisms and information theory for debiasing LLMs.", "Development of a data rewriting method to balance instruction-tuning dataset distributions.", "Demonstrated improvement in LLM generalizability across tasks post-debiasing."], "limitations": "The effectiveness of the proposed framework may depend on the quality and diversity of the input datasets used for training and intervention.", "keywords": ["debiasing", "large language models", "information theory", "causal intervention", "generalizability"], "importance_score": 9, "read_time_minutes": 10}}
